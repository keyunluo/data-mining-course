Finito: A Faster, Permutable Incremental Gradient Method for Big Data
Problems
Aaron J. Defazio
Tibério S. Caetano
Justin Domke
NICTA and Australian National University

Abstract
Recent advances in optimization theory have
shown that smooth strongly convex finite sums
can be minimized faster than by treating them as
a black box ”batch” problem. In this work we
introduce a new method in this class with a theoretical convergence rate four times faster than existing methods, for sums with sufficiently many
terms. This method is also amendable to a sampling without replacement scheme that in practice gives further speed-ups. We give empirical
results showing state of the art performance.

1. Introduction
Many recent advances in the theory and practice of numerical optimization have come from the recognition and exploitation of structure. Perhaps the most common structure
is that of finite sums. In machine learning when applying
empirical risk minimization we almost always end up with
an optimization problem involving the minimization of a
sum with one term per data point.
The recently developed SAG algorithm (Schmidt et al.,
2013) has shown that even with this simple form of structure, as long as we have sufficiently many data points we
are able to do significantly better than black-box optimization techniques in expectation for smooth strongly convex
problems. In practical terms the difference is often a factor
of 10 or more.
The requirement of sufficiently large datasets is fundamental to these methods. We describe the precise form of this
as the big data condition. Essentially, it is the requirement
that the amount of data is on the same order as the condition
number of the problem. The strong convexity requirement
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

AARON . DEFAZIO @ ANU . EDU . AU
TIBERIO . CAETANO @ NICTA . COM . AU
JUSTIN . DOMKE @ NICTA . COM . AU

is not as onerous. Strong convexity holds in the common
case where a quadratic regularizer is used together with a
convex loss.
The SAG method and the Finito method we describe in this
work are similar in their form to stochastic gradient descent
methods, but with one crucial difference: They store additional information about each data point during optimization. Essentially, when they revisit a data point, they do not
treat it as a novel piece of information every time.
Methods for the minimization of finite sums have classically been known as Incremental gradient methods (Bertsekas, 2010). The proof techniques used in SAG differ fundamentally from those used on other incremental gradient
methods though. The difference hinges on the requirement
that data be accessed in a randomized order. SAG does
not work when data is accessed sequentially each epoch,
so any proof technique which shows even non-divergence
for sequential access cannot be applied.
A remarkable property of Finito is the tightness of the theoretical bounds compared to the practical performance of the
algorithm. The practical convergence rate seen is at most
twice as good as the theoretically predicted rate. This sets
it apart from methods such as LBFGS where the empirical
performance is often much better than the relatively weak
theoretical convergence rates would suggest.
The lack of tuning required also sets Finito apart from
stochastic gradient descent (SGD). In order to get good
performance out of SGD, substantial laborious tuning of
multiple constants has traditionally been required. A multitude of heuristics have been developed to help choose these
constants, or adapt them as the method progresses. Such
heuristics are more complex than Finito, and do not have
the same theoretical backing. SGD has application outside
of convex problems of course, and we do not propose that
Finito will replace SGD in those settings. Even on strongly
convex problems SGD does not exhibit linear convergence
like Finito does.

Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems

There are many similarities between SAG, Finto and
stochastic dual coordinate descent (SDCA) methods
(Shalev-Shwartz & Zhang, 2013). SDCA is only applicable to linear predictors. When it can be applied, it has linear convergence with theoretical rates similar to SAG and
Finito.

2. Algorithm
We consider differentiable convex functions of the form
n
1X
fi (w).
f (w) =
n i=1
We assume that each fi has Lipschitz continuous gradients
with constant L and is strongly convex with constant s.
Clearly if we allow n = 1, virtually all smooth, strongly
convex problems are included. So instead, we will restrict
ourselves to problems satisfying the big data condition.
Big data condition: Functions of the above form satisfy
the big data condition with constant β if
L
s
Typical values of β are 1-8. In plain language, we are considering problems where the amount of data is of the same
order as the condition number (L/s) of the problem.
n≥β

2.1. Additional Notation
We superscript with (k) to denote the value of the scripted
quantity at iteration k. We omit the n superscript on summations, and subscript with i with the implication that indexing starts at 1. When we use separate arguments for
each fi , we denote them φi . Let φ̄(k) denote the average
Pn (k)
φ̄(k) = n1 i φi . Our step length constant, which depends on β, is denoted α. We use angle bracket notation
for dot products h·, ·i.
2.2. The Finito algorithm
(0)

We start with a table of known φi values, and a table of
(0)
known gradients fi0 (φi ), for each i. We will update these
two tables during the course of the algorithm. The step for
iteration k, is as follows:

1. Update w using the step:
w(k) = φ̄(k) −

1 X 0 (k)
f (φ ).
αsn i i i

2. Pick an index j uniformly at random, or using without-replacement sampling as discussed

in Section 3.
(k+1)

3. Set φj

= w(k) in the table and leave the other
(k+1)

variables the same (φi

(k+1)

4. Calculate and store fj0 (φj

(k)

= φi

for i 6= j).

) in the table.

Our main theoretical result is a convergence rate proof for
this method.
Theorem 1. When the big data condition holds with β = 2,
α = 2 may be used. In that setting, if we have initialized
(0)
all φi the same, the convergence rate is:

k 

h
i
3
1
 0 (0) 2
(k)
∗
E f (φ̄ ) − f (w ) ≤
1−
f (φ̄ ) .
4s
2n
See Section
 5 for the proof. In contrast, SAG achieves a
1
1 − 8n
rate when β = 2. Note that on a per epoch basis,

1 n
the Finito rate is 1 − 2n
≈ exp(−1/2) = 0.606. To
put that into context, 10 epochs will see the error bound
reduced by more than 148x.
One notable feature of our method is the fixed step size.
In typical machine learning problems the strong convexity
constant is given by the strength constant of the quadratic
regularizer used. Since this is a known quantity, as long as
the big data condition holds α = 2 may be used without
any tuning or adjustment of Finito required. This lack of
tuning is a major feature of Finito.
In cases where the big data condition does not hold, we
conjecture that the step size must be reduced proportionally to the violation of the big data condition. In practice,
the most effective step size can be found by testing a number of step sizes, as is usually done with other stochastic
optimisation methods.
A simple way of satisfying the big data condition is to duplicate your data enough times so then holds. This is not as
effective in practice as just changing the step size, and of
course it uses more memory. However it does fall within
the current theory.
Another difference compared to the SAG method is that
we store both gradients and points φi . We do not actually
need twice as much memory however as they can be stored
summed together. In particular we store the quantities
Ppi =
1
fi0 (φi ) − αsφi , and use the update rule w = − αsn
i pi .
This trick does not work when step lengths are adjusted
during optimization however. The storage of φi is also a
disadvantage when the gradients fi0 (φi ) are sparse but φi
are not sparse, as it can cause significant additional memory
usage. We do not recommend the usage of Finito when
gradients are sparse.
The SAG algorithm differs from Finito only in the w update

Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems

and step lengths:
w(k) = w(k−1) −

1
16Ln

X

(k)

fi0 (φi ).

i

3. Randomness is key
By far the most interesting aspect of the SAG and Finito
methods is the random choice of index at each iteration.
We are not in an online setting, so there is no inherent
randomness in the problem. Yet it seems that a randomized method is required. Neither method works in practice
when the same ordering is used each pass, or in fact with
any non-random access scheme we have tried. It is hard
to emphasize enough the importance of randomness here.
The technique of pre-permuting the data, then doing in order passes after that, also does not work. Reducing the step
size in SAG or Finito by 1 or 2 orders of magnitude does
not fix the convergence issues either.
Other methods, such as standard SGD, have been noted
by various authors to exhibit speed-ups when random
sampling is used instead of in order passes, but the differences are not as extreme as convergence v.s. nonconvergence. Perhaps the most similar problem is that of
coordinate descent on smooth convex functions. Coordinate descent cannot diverge when non-random orderings
are used, but convergence rates are substantially worse in
the non-randomized setting (Nesterov 2010, Richtarik &
Takac 2011).
Reducing the step size α by a much larger amount, namely
by a factor of n, does allow for non-randomized orderings
to be used. This gives an extremely slow method however.
This is the case covered by the MISO (Mairal, 2013). A
similar reduction in step size gives convergence under nonrandomized orderings for SAG also. Convergence rates for
incremental sub-gradient methods with a variety of orderings appear in the literature also (Nedic & Bertsekas, 2000).
Sampling without replacement is much faster
Other sampling schemes, such as sampling without replacement, should be considered. In detail, we mean the
case where each ”pass” over the data is a set of sampling
without replacement steps, which continue until no data remains, after which another ”pass” starts afresh. We call
this the permuted case for simplicity, as it is the same as
re-permuting the data after each pass. In practice, this approach does not give any speedup with SAG, however it
works spectacularly well with Finito. We see speedups of
up to a factor of two using this approach. This is one of
the major differences in practice between SAG and Finito.
We should note that we have no theory to support this case
however. We are not aware of any analysis that proves
faster convergence rates of any optimization method under

a sampling without replacement scheme. An interesting
discussion of SGD under without-replacement sampling
appears in Recht & Re (2012).
The SDCA method is also sometimes used with a permuted ordering (Shalev-Shwartz & Zhang, 2013), our experiments in Section 7 show that this sometimes results in
a large speedup over uniform random sampling, although it
does not appear to be as reliable as with Finito.

4. Proximal variant
We now consider composite problems of the form
1X
fi (w) + λr(w),
f (w) =
n i
where r is convex but not necessarily smooth or strongly
convex. Such problems are often addressed using proximal
algorithms, particularly when the proximal operator for r:
1
2
kx − zk + λr(x)
2
has a closed form solution. An example would be the use
of L1 regularization. We now describe the Finito update for
this setting. First notice that when we set w in the Finito
method, it can be interpreted as minimizing the quantity:
1X
1X 0
B(x) =
fi (φi ) +
hfi (φi ), x − φi i
n i
n i
αs X
2
+
kx − φi k ,
2n i
proxrλ (z) = argminx

with respect to x, for fixed φi . This is related to the upper
bound minimized by MISO, where αs is instead L. It is
straight forward to modify this for the composite case:
1X
1X 0
Bλr (x) = λr(x) +
fi (φi ) +
hfi (φi ), x − φi i
n i
n i
αs X
2
+
kx − φi k .
2n i
The minimizer of the modified Bλr can be expressed using
the proximal operator as:
!
1 X 0
r
w = proxλ/αs φ̄ −
f (φi ) .
αsn i i
This strongly resembles the update in the standard gradient
descent setting, which for a step size of 1/L is


1 0 (k−1)
r
(k−1)
w = proxλ/L w
− f (w
) .
L
We have not yet developed any theory supporting the proximal variant of Finito, although empirical evidence suggests
it has the same convergence rate as in the non-proximal
case.

Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems

5. Convergence proof

T3 = −

We start by stating two simple lemmas. All expectations
in the following are over the choice of index j at step k.
Quantities without superscripts are at their values at iteration k.
Lemma 1. The expected step is
1 0
f (w).
αsn
I.e. the w step is a gradient descent step in expectation
1
∝ L1 ). A similar equality also holds for SGD, but not
( αsn
for SAG.
E[w(k+1) ] − w = −

T4 =

(k+1)

Proof.

LemmaP2. (Decomposition of variance) We can decom2
pose n1 i kw − φi k as

2 1 X 

1X
2
φ̄ − φi 2 .
kw − φi k = w − φ̄ +
n i
n i
Proof.
1X
n

2

kw − φi k

i

X

2 1 X 



φ̄ − φi 2 + 2
= w − φ̄ +
w − φ̄, φ̄ − φi
n i
n i
2 1 X 





φ̄ − φi 2 + 2 w − φ̄, φ̄ − φ̄
= w − φ̄ +
n i


2 1 X 
φ̄ − φi 2 .
= w − φ̄ +
n i

Main proof
Our proof proceeds by construction of a Lyapunov function
T ; that is, a function that bounds a quantity of interest, and
that decreases each iteration in expectation. Our Lyapunov
function T = T1 + T2 + T3 + T4 is composed of the sum
of the following terms,
T1 = f (φ̄),
X
1
1X 0
T2 = −
fi (φi ) −
hfi (φi ), w − φi i ,
n i
n i


s X
φ̄ − φi 2 .
2n i

We now state how each term changes between steps k + 1
and k. Proofs are found in the appendix in the supplementary material:
 L X
1
 0
(k+1)
2
kw − φi k ,
E[T1
]−T1 ≤
f (φ̄), w − φ̄ + 3
n
2n i
E[T2

E[w(k+1) ] − w



1
1
0
0
(w − φj ) −
f (w) − fj (φj )
=E
n
αsn j
1 0
1 X 0
1
f (φi )
f (w) +
= (w − φ̄) −
n
αsn
αsn2 i i
P 0
1
Now simplify n1 (w − φ̄) as − αsn
2
i fi (φi ), so the only
1
term that remains is − αsn
f 0 (w).

s X
2
kw − φi k ,
2n i

1
1
] − T2 ≤ − T2 − f (w)
n
n
1
β 1 X 0
2
+( − ) 3
kfi (w) − fi0 (φi )k
α n sn i

1

φ̄ − w, f 0 (w)
n
1 X 0
hfi (w) − fi0 (φi ), w − φi i ,
− 3
n i

+

(k+1)

E[T3

(k+1)

E[T4


1
1
1 
 0
+ 2 )T3 +
f (w), w − φ̄
n n
αn
X
1
2
0
− 2 3
kfi (φi ) − fi0 (w)k ,
2α sn i

] − T3 = −(




s X
φ̄ − φi 2 + s φ̄ − w2
2
2n i
2n
s X
2
kw − φi k .
− 3
2n i

] − T4 = −

β
Theorem 2. Between steps k and k+1, if α2 − α12 −β+ α
≤
0, α ≥ 2 and β ≥ 2 then

E[T (k+1) ] − T ≤ −

1
T.
αn

Proof. We take the three lemmas above and group like
terms to get

1
 0
1 X
E[T (k+1) ] − T ≤
f (φ̄), w − φ̄ + 2
fi (φi )
n
n i
1
1 X 0
− f (w) + 2
hfi (φi ), w − φi i
n
n i

1 1
 0
) f (w), φ̄ − w
α n
L
s X
2
+(
+ 1) 2
kw − φi k
sn
2n i
1 X 0
− 3
hfi (w) − fi0 (φi ), w − φi i
n i
1
1 X 0
2
+ (1 −
)
kfi (φi ) − fi0 (w)k
2α αsn3 i
+ (1 −

Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems

X


s 
w − φ̄2 − s
φ̄ − φi 2 .
2
2n
2n i

P
2
pendix) to partially cancel i kfi (φi ) − fi (w)k :



1
1
1
1
1− −
− f (w) − T2
Next we cancel part of the first line using
α β
n
n




1
s 
1
1 
 0
1
1
β X 0
2
w − φ̄2 ,
f (φ̄), w − φ̄ ≤
f (w)− f (φ̄)−
≤
−
1
−
−
kfi (φi ) − fi0 (w)k .
αn
αn
αn
2αn
α β 2sn3 i
based on B3 in the Appendix. We then pull terms occurring
1
Leaving us with
in − αn
T together, giving E[T (k+1) ] − T ≤
1

1
1 1

E[T (k+1) ] − T ≤ − T
−
T + (1 − ) f 0 (φ̄) − f 0 (w), w − φ̄
αn
αn
α n


β 1 X 0
2
1
2
1
1
1
kfi (φi ) − fi0 (w)k .
+( − 2 −β+ )
+ (1 − ) − f (w) − T2
α α
α 2sn3 i
α
n
n
L
1 s X
The remaining gradient norm term is non-positive under
2
+(
+1− ) 2
kw − φi k
the conditions specified in our assumptions.
sn
α 2n i
1 X 0
Theorem 3. The Lyapunov function bounds f (φ̄) − f (w∗ )
hfi (w) − fi0 (φi ), w − φi i
− 3
n i
as follows:
1
1 X 0
2
f (φ̄(k) ) − f (w∗ ) ≤ αT (k) .
+ (1 −
)
kfi (φi ) − fi0 (w)k
3
2α αsn i
X


1 s 
Proof. Consider the following function, which we will call
w − φ̄2 − (1 − 1 ) s
φ̄ − φi 2 .
+ (1 − )
2
α 2n
α 2n i
R(x):
1X 0
1X
Next we use the standard inequality (B5)
fi (φi ) +
hfi (φi ), x − φi i
R(x) =
n i
n i
2

1 s
1 1
 0
0
(1− ) f (φ̄) − f (w), w − φ̄ ≤ −(1− ) w − φ̄ ,
s X
2
α n
α n
+
kx − φi k .


2n
2
s 
i
which changes the bottom row to −(1 − α1 ) 2n
w − φ̄ −
2
P 
s
1
When evaluated at its minimum
(1 − α ) 2n2 i φ̄ − φi  . These two terms can then be
P 0 with respect to x, which
1
we denote w0 = φ̄ − sn
grouped using Lemma 2, to give
i fi (φi ), it is a lower bound on
f (w∗ ) by strong
convexity.
However, we are evaluating at
X
P 0
1
L
2
1
w = φ̄ − αsn
E[T (k+1) ] − T ≤ − T + 3
kw − φi k
i fi (φi ) instead in the (negated) Lyapunv
αn
2n i
function. R is convex with respect to x, so by definition


1
1
1
+ (1 − ) − f (w) − T2



α
n
n
1
1 0
X
R(w)
=
R
1
−
w
φ̄
+
1
α
α
− 3
hfi0 (w) − fi0 (φi ), w − φi i


n i
1
1
≤ 1−
R(φ̄) + R(w0 ).
1
1 X 0
2
0
α
α
+ (1 −
)
kfi (φi ) − fi (w)k .
2α αsn3 i
Therefore by the lower bounding property


We use the followingP
inequality (Corollary 6 in Appendix)
1
1
2
f (φ̄) − R(w) ≥ f (φ̄) − 1 −
R(φ̄) − R(w0 )
to cancel against the i kw − φi k term:
α
α




1
1
1 X 0
1
1
1
0
− f (w) − T2 ≤ 3
hfi (w) − fi (φi ), w − φi i
≥ f (φ̄) − 1 −
f (φ̄) − f (w∗ )
β
n
n
n i
α
α

1
L X
2
=
f (φ̄) − f (w∗ ) .
− 3
kw − φi k
α
2n
+

i

1 X 0
2
−
kfi (w) − fi0 (φi )k ,
2sn3 i

Now note that T ≥ f (φ̄) − R(w). So
f (φ̄) − f (w∗ ) ≤ αT.

and then apply the following similar inequality (B7 in Ap(0)

Theorem 4. If the Finito method is initialized with all φi
the same,and the assumptions of Theorem 2 hold, then the

Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems

convergence rate is:

k 

h
i
1
c
 0 (0) 2
E f (φ̄(k) ) − f (w∗ ) ≤
1−
f (φ̄ ) ,
s
αn

1
with c = 1 − 2α .
Proof. By unrolling Theorem 2, we get

k
1
T (0) .
E[T (k) ] ≤ 1 −
αn
Now using Theorem 3

k
h
i
1
E f (φ̄(k) ) − f (w∗ ) ≤ α 1 −
T (0) .
αn
We need to control T (0) also. Since we are assuming that
all φ0i start the same, we have that
1X
fi (φ̄(0) )
T (0) = f (φ̄(0) ) −
n i
2
E s
1 X D 0 (0)


−
fi (φ̄ ), w(0) − φ̄(0) − w(0) − φ̄(0) 
n i
2

D
E s
 1 0 (0) 2
0 (0)
(0)
(0)

= 0 − f (φ̄ ), w − φ̄
− − f (φ̄ )

2
αs




1  0 (0) 2
1 
2
=
f (φ̄ ) − 2 f 0 (φ̄(0) )
αs
2α
s



1
1 
 0 (0) 2
= 1−
f (φ̄ ) .
2α αs

6. Lower complexity bounds and exploiting
problem structure
The theory for the class of smooth, strongly convex problems with Lipschitz continuous gradients under first order
1,1
optimization methods (known as Ss,L
) is well developed.
These results require the technical condition that the dimensionality of the input space Rm is much larger than the
number of iterations we will take. For simplicity we will
assume this is the case in the following discussions.
1,1
It is known that problems exist in Ss,L
for which the iterate
convergence rate is bounded by:
!2k
p

2

2
L/s − 1
 (k)
 (0)

∗
w − w  ≥ p
w − w∗  .
L/s + 1

In fact, when s and L are known in advance, this rate is
achieved up to a small constant factor by several methods, most notably by Nesterov’s accelerated gradient descent method (Nesterov 1988, Nesterov 1998). In order
to achieve convergence rates faster than this, additional assumptions must be made on the class of functions considered.

Recent advances have shown that all that is required to
achieve significantly faster rates is a finite sum structure,
such as in our problem setup. When the big data condition holds our method achieves a rate 0.6065 per epoch
in expectation. This rate only depends on the condition
number indirectly, through the big data condition. For example, with L/s = 1, 000, 000, the fastest possible rate for
a black box method is a 0.996, whereas Finito achieves a
rate of 0.6065 in expectation for n ≥ 4, 000, 000, or 124x
faster. The required amount of data is not unusual in modern machine learning problems. In practice, when quasinewton methods are used instead of accelerated methods, a
speedup of 10-20x is more common.
6.1. Oracle class
We now describe the (stochastic) oracle class
m
F S 1,1
s,L,n (R ) for which SAG and Finito most naturally fit.
Pn
Function class: f (w) = n1 i=1 fi (w), with fi ∈
1,1
(Rm ).
Ss,L
Oracle: Each query takes a point x ∈ Rm , and returns j,
fj (w) and fj0 (w), with j chosen uniformly at random.

2
Accuracy: Find w such that E[w(k) − w∗  ] ≤ .
The main choice made in formulating this definition is
putting the random choice in the oracle. This restricts
the methods allowed quite strongly. The alternative case,
where the index j is input to the oracle in addition to
x, is also interesting. Assuming that the method has access to a source of true random indices, we call that class
m
DS 1,1
s,L,n (R ). In Section 3 we discuss empirical evidence
m
that suggests that faster rates are possible in DS 1,1
s,L,n (R )
1,1
m
than for F S s,L,n (R ).
It should first be noted that there is atrivial lower bound
1,1
rate for f ∈ SS s,L,β
(Rm ) of 1 − n1 reduction per step.
Its not clear if this can be achieved for any finite β. Finito

1
is only a factor of 2 off this rate, namely 1 − 2n
at
β = 2, and asymptotes towards this rate for very large β.
SDCA, while not applicable to all problems in this class,
also achieves the rate asymptotically.
Another case to consider is the smooth convex but nonstrongly convex setting. We still assume Lipschitz continuous gradients. In this setting we will show that for sufficiently high dimensional input spaces, the (non-stochastic)
lower complexity bound is the same for the finite sum case
and cannot be better than that given by treating f as a single
black box function.
The full proof is in the Appendix, but the idea is as follows:
when the fi are not strongly convex, we can choose them
such that they do not interact with each other, as long as the

Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems

dimensionality is much larger than k. More precisely, we
may choose them so that for any x and y and any i 6= j,
hfi0 (x), fj0 (y)i = 0 holds. When the functions do not interact, no optimization scheme may reduce the iterate error
faster than by just handling each fi separately. Doing so
in an in-order fashion gives the same rate as just treating f
using a black box method.
For strongly convex fi , it is not possible for them to not
interact in the above sense. By definition strong convexity
requires a quadratic component in each fi that acts on all
dimensions.

7. Experiments
In this section we compare Finito, SAG, SDCA and
LBFGS. We only consider problems where the regularizer
is large enough so that the big data condition holds, as this
is the case our theory supports. However, in practice our
method can be used with smaller step sizes in the more
general case, in much the same way as SAG.
Since we do not know the Lipschitz constant for these problems exactly, the SAG method was run for a variety of step
sizes, with the one that gave the fastest rate of convergence
plotted. The best step-size for SAG is usually not what the
theory suggests. Schmidt et al. (2013) suggest using L1 in1
stead of the theoretical rate 16L
. For Finito, we find that
using α = 2 is the fastest rate when the big data condition
holds for any β > 1. This is the step suggested by our theory when β = 2. Interestingly, reducing α to 1 does not
improve the convergence rate. Instead we see no further
improvement in our experiments.
For both SAG and Finito we used a differing step rule than
suggested by the theory for the first pass. For Finito, during
the first pass, since we do not have derivatives for each φi
yet, we simply sum over the k terms seen so far
w(k) =

k
k
1 X (k)
1 X 0 (k)
φi −
f (φ ),
k i
αsk i i i

where we process data points in index order for the first
pass only. A similar trick is suggested by Schmidt et al.
(2013) for SAG.
Since SDCA only applies to linear predictors, we are restricted in possible test problems. We choose log loss for
3 binary classification datasets, and quadratic loss for 2 regression tasks. For classification, we tested on the ijcnn1
and covtype datasets1 , as well as MNIST2 classifying 04 against 5-9. For regression, we choose the two datasets
from the UCI repository: the million song year regression
1

http://www.csie.ntu.edu.tw/˜cjlin/
libsvmtools/datasets/binary.html
2
http://yann.lecun.com/exdb/mnist/

dataset, and the slice-localization dataset. The training portion of the datasets are of size 5.3×105 , 5.0×104 , 6.0×104 ,
4.7 × 105 and 5.3 × 104 respectively.
Figure 6 shows the results of our experiments. Firstly we
can see that LBFGS is not competitive with any of the incremental gradient methods considered. Secondly, the nonpermuted SAG, Finito and SDCA often converge at very
similar rates. The observed differences are usually down
to the speed of the very first pass, where SAG and Finito
are using the above mentioned trick to speed their convergence. After the first pass, the slopes of the line are usually
comparable. When considering the methods with permutation each pass, we see a clear advantage for Finito. Interestingly, it gives very flat lines, indicating very stable
convergence.

8. Related work
Traditional incremental gradient methods (Bertsekas,
2010) have the same form as SGD, but applied to finite
sums. Essentially they are the non-online analogue of
SGD. Applying SGD to strongly convex problems does not
yield linear convergence, and in practice it is slower than
the linear-converging methods we discuss in the remainder
of this section.
Besides the methods that fall under the classical Incremental gradient moniker, SAG and MISO (Mairal, 2013) methods are also related. MISO method falls into the class of upper bound minimization methods, such as EM and classical
gradient descent. MISO is essentially the Finito method,
but with step sizes n times smaller. When using these
larger step sizes, the method is no longer a upper bound
minimization method. Our method can be seen as MISO,
but with a step size scheme that gives neither a lower nor
upper bound minimisation method. While this work was
under peer review, a tech report (Mairal (2014)) was put on
arXiv that establishes the convergence rate of MISO with
1
per step. This similar
step α = 1 and with β = 2 as 1 − 3n
1
but not quite as good as the 1 − 2n rate we establish.
Stochastic Dual Coordinate descent (Shalev-Shwartz &
Zhang, 2013) also gives fast convergence rates on problems
for which it is applicable. It requires computing the convex
conjugate of each fi , which makes it more complex to implement. For the best performance it has to take advantage
of the structure of the losses also. For simple linear classification and regression problems it can be effective. When
using a sparse dataset, it is a better choice than Finito due
to the memory requirements.
its theo For linear predictors,

β
per step is a little
retical convergence rate of 1 − (1+β)n
faster than what we establish for Finito, however it does not
appear to be faster in our experiments.

10−2
10−3
10−4
10−5
10−6
10−7
10−8
10−9
10−10
10−11

full gradient norm
0

2

4

6

8
Epoch
SAG
Finito perm
1. MNIST
FinitoFigureSDCA

10

12

14

0

2

4

6

8
Epoch
SAG
Finito perm
3. Covtype
FinitoFigure SDCA

100
10−1
10−2
10−3
10−4
10−5
10−6
10−7
10−8
10−9
10−10

0

2

SDCA perm
LBFGS
full gradient norm

100
10−1
10−2
10−3
10−4
10−5
10−6
10−7
10−8
10−9
10−10

full gradient norm

full gradient norm

full gradient norm

Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems

10

12

14

10−1
10−2
10−3
10−4
10−5
10−6
10−7
10−8
10−9
10−10
10−11

0

2

SDCA perm
LBFGS

4

6

8
Epoch
SAG
Finito perm
2. ijcnn1
Finito FigureSDCA

4

10

12

14

SDCA perm
LBFGS

6

8
10
12
14
Epoch
SAG
Finito perm
SDCA perm
Figure 4. SDCA
Million Song
Finito
LBFGS

100
10−1
10−2
10−3
10−4
10−5
0

2

4
SAG
Finito

6

8
Epoch
Finito perm
SDCA

10

12

14

SDCA perm
LBFGS

Figure 5. slice
Figure 6. Convergence rate plots for test problems

9. Conclusion
We have presented a new method for minimization of finite
sums of smooth strongly convex functions, when there is a
sufficiently large number of terms in the summation. We
additionally develop some theory for the lower complexity
bounds on this class, and show the empirical performance
of our method.

References
Bertsekas, Dimitri P. Incremental gradient, subgradient,
and proximal methods for convex optimization: A survey. Technical report, 2010.
Mairal, Julien. Optimization with first-order surrogate
functions. ICML, 2013.
Mairal, Julien. Incremental majorization-minimization optimization with application to large-scale machine learning. Technical report, INRIA Grenoble Rhne-Alpes /
LJK Laboratoire Jean Kuntzmann, 2014.

Nedic, Angelia and Bertsekas, Dimitri. Stochastic Optimization: Algorithms and Applications, chapter Convergence Rate of Incremental Subgradient Algorithms.
Kluwer Academic, 2000.
Nesterov, Yu. On an approach to the construction of optimal methods of minimization of smooth convex functions. Ekonomika i Mateaticheskie Metody, 24:509–517,
1988.
Nesterov, Yu. Introductory Lectures On Convex Programming. Springer, 1998.
Nesterov, Yu. Efficiency of coordinate descent methods
on huge-scale optimization problems. Technical report,
CORE, 2010.
Recht, Benjamin and Re, Christopher. Beneath the valley of the noncommutative arithmetic-geometric mean
inequality: conjectures, case-studies, and consequences.
Technical report, University of Wisconsin-Madison,
2012.

Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems

Richtarik, Peter and Takac, Martin. Iteration complexity of
randomized block-coordinate descent methods for minimizing a composite function. Technical report, University of Edinburgh, 2011.
Schmidt, Mark, Roux, Nicolas Le, and Bach, Francis. Minimizing finite sums with the stochastic average gradient.
Technical report, INRIA, 2013.
Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual coordinate ascent methods for regularized loss minimization. JMLR, 2013.

