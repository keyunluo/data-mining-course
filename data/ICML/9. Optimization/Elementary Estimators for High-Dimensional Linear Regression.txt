Elementary Estimators for High-Dimensional Linear Regression

Eunho Yang
Department of Computer Science, The University of Texas, Austin, TX 78712, USA

EUNHO @ CS . UTEXAS . EDU

Aurélie C. Lozano
IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA

ACLOZANO @ US . IBM . COM

Pradeep Ravikumar
Department of Computer Science, The University of Texas, Austin, TX 78712, USA

Abstract
We consider the problem of structurally constrained high-dimensional linear regression. This
has attracted considerable attention over the last
decade, with state of the art statistical estimators
based on solving regularized convex programs.
While these typically non-smooth convex programs can be solved by the state of the art optimization methods in polynomial time, scaling
them to very large-scale problems is an ongoing
and rich area of research. In this paper, we attempt to address this scaling issue at the source,
by asking whether one can build simpler possibly
closed-form estimators, that yet come with statistical guarantees that are nonetheless comparable
to regularized likelihood estimators. We answer
this question in the affirmative, with variants
of the classical ridge and OLS (ordinary least
squares estimators) for linear regression. We analyze our estimators in the high-dimensional setting, and moreover provide empirical corroboration of its performance on simulated as well as
real world microarray data.

1. Introduction
We consider the problem of high-dimensional linear regression, where the number of variables p could potentially be
even larger than the number of observations n. Under such
high-dimensional regimes, it is now well understood that
consistent estimation is typically not possible unless one
imposes low-dimensional structural constraints upon the
regression parameter vector. Popular structural constraints
include that of sparsity, where very few entries of the highProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

PRADEEPR @ CS . UTEXAS . EDU

dimensional regression parameter are assumed to be nonzero, group-sparse constraints, and low-rank structure with
matrix-structured parameters, among others.
The development of consistent estimators for such structurally constrained high-dimensional linear regression has
attracted considerable recent attention. A key class of estimators are based on regularized maximum likelihood estimators; in the case of linear regression with Gaussian
noise, these take the form of regularized least squares estimators. For the case of sparsity, a popular instance is constrained basis pursuit or LASSO (Tibshirani, 1996), which
solves an `1 regularized (or equivalently `1 -constrained)
least squares problem, and has been shown to have strong
statistical guarantees, including prediction error consistency (van de Geer & Buhlmann, 2009), consistency of the
parameter estimates in `2 or some other norm (van de Geer
& Buhlmann, 2009; Meinshausen & Yu, 2009; Candes &
Tao, 2006), as well as variable selection consistency (Meinshausen & Bühlmann, 2006; Wainwright, 2009; Zhao &
Yu, 2006). For the case of group-sparse structured linear
regression, `1 /`q regularized least squares (with q ≥ 2)
has been proposed (Tropp et al., 2006; Zhao et al., 2009;
Yuan & Lin, 2006; Jacob et al., 2009), and shown to have
strong statistical guarantees, including convergence rates
in `2 -norm (Lounici et al., 2009; Baraniuk et al., 2008))
as well as model selection consistency (Obozinski et al.,
2008; Negahban & Wainwright, 2009). For the matrixstructured least squares problem, nuclear norm regularized
estimators have been studied for instance in (Recht et al.,
2010; Bach, 2008). For other structurally constrained least
squares problems, see (Huang et al., 2011; Bach et al.,
2012; Negahban et al., 2012) and references therein. All
of these estimators solve convex programs, though with
non-smooth components due to the respective regularization functions. The state of the art optimization methods for
solving these programs are iterative, and can approach the
optimal solution within any finite accuracy with computational complexity that scales polynomially with the number

Elementary Estimators for High-Dimensional Linear Regression

of variables and number of samples, rendering these very
expensive for very large-scale problems.
In another line of work, the Dantzig estimator (Candes &
Tao, 2007) solves a linear program to estimate the sparse
linear regression parameter with stronger statistical guarantees when compared to the LASSO (Bickel et al., 2009).
But here again, the linear program while convex, is computationally expensive for very large-scale problems. Other
class of estimators for structured linear regression include
greedy methods. These include forward-backward selection (Zhang et al., 2008b), matching pursuit (Mallat &
Zhang, 1993), and orthogonal matching pursuit (Zhang,
2010), among others. The caveat with such greedy methods however is that some of these require considerably
more stringent conditions to hold when compared to regularized convex programs noted above, and either require the
knowledge of the exact structural complexity such as sparsity level, or are otherwise unstable i.e. sensitive to tuning
parameters such as their stopping thresholds. Thus overall,
the class of regularized convex programs have proved more
popular for large-scale structured linear regression, and accordingly there has been a strong line of recent research on
large-scale optimization methods for such programs (see
e.g. Friedman et al. (2007); Hsieh et al. (2011) and references therein), including parallel and distributed variants.

are set to soft-thresholded values of the correlation of the
covariates with the response. However, as (Genovese et al.,
2012) showed, as a flip side of the simplicity of marginal
regression, this method requires very stringent conditions
(loosely, extremely weak correlations between the covariates) in order to select a relevant subset of covariates.
In this paper, we are surprisingly able to answer our question in the affirmative, by building on the OLS and ridge estimators: we provide close variants that are not only available in closed form, but also come with strong statistical
guarantees similar to those of the regularized convex program based estimators. The estimators are reminiscent of
the Dantzig estimator, but in contrast are actually available
in closed form, are thus much more scalable. As we show,
for the sparse structure case the ridge-variant comes with
slightly worse guarantees when compared to the LASSO,
but the OLS variant actually has comparable guarantees to
that of the LASSO. We also provide a unified statistical
analysis for general structures beyond sparsity. We corroborate these results via simulations as well as on a realworld microarray dataset. Overall, the estimators and analyses in the paper thus motivate a new line of research on
simpler and possibly closed form estimators for very highdimensional statistical estimation.

In this paper, we consider the following simple question:

2. Setup

“If we restrict ourselves to estimators with closed-form solutions; can we nonetheless obtain consistent estimators
that have the sharp convergence rates of the regularized
convex programs and other estimators noted above?”

We consider the linear regression model,

A natural closed-form estimator for the high-dimensional
linear regression problem is the ordinary least squares
(OLS) estimator. This is the maximum likelihood solution under a Gaussian noise assumption, and is thus consistent with optimal convergence rates under classical statistical settings where the number of variables p is fixed
as a function of the number of samples n. However under high-dimensional regimes, it is not only inconsistent
but the least squares estimation problem does not even
have a unique minimum. Another classical estimator is the
ridge, or `2 -regularized least squares estimator, which is
also available in closed form; but unlike the OLS estimator,
the ridge-regularized estimation problem has a unique solution. While simple, it is however not known to be for instance `2 -norm consistent under high-dimensional settings.
Another prominent closed-form estimator and an OLS variant is marginal regression: this regresses each covariate
separately on the response to get the corresponding regression parameter. Correlation or sure screening (Fan & Lv,
2008) (also termed simple thresholding (Donoho, 2006)) is
a closely related method for the high-dimensional sparse
linear regression setting, where the regression parameters

∗
yi = x>
i θ + wi ,

i = 1, . . . , n,

(1)

where θ∗ ∈ Rp is the fixed unknown regression parameter
of interest, yi ∈ R is a real-valued response, xi ∈ Rp is
a known observation vector, and wi ∈ R is an unknown
noise term. For technical simplicity, we assume that these
noise terms are independent zero-mean Gaussian random
variables, wi ∈ N (0, σ 2 ), for some σ > 0. Suppose we
collate the n observations from the linear regression model
in (1) in vector and matrix form. Let y ∈ Rn denote the
vector of n responses, X ∈ Rn×p denote the design matrix
consisting of the linear regression observation vectors, and
w ∈ Rn the vector of n noise terms. The linear regression
model thus entails: y = Xθ∗ + w.
We are interested in the high-dimensional setting, where
the number of variables p may be of the same order as,
or even substantially larger than the sample size n, so that
p  n. Under such high-dimensional settings, it is now
well understood that it is typically necessary to impose
structural constraints on the model parameters θ∗ .
2.1. Unified Framework of Negahban et al. (2012)
We follow the unified statistical framework of Negahban
et al. (2012) to formalize the notion of structural con-

Elementary Estimators for High-Dimensional Linear Regression
⊥

straints. There, they use subspace pairs (M, M ) such
that M ⊆ M where M is the model subspace in which
the model parameter θ∗ and similarly structured parame⊥
ters lie, and which is typically low-dimensional, while M
is the perturbation subspace of parameters that represents
perturbations away from the model subspace.
They also define the property of decomposability of a regularization function, which captures the suitablity of a regularization function R to particular structure. Specifically,
a regularization function R is said to be decomposable
⊥
with respect to a subspace pair (M, M ), if R(u + v) =
⊥
R(u) + R(v), for all u ∈ M, v ∈ M . Note that when
R(·) is a norm, by the triangle inequality, the LHS is always
less than or equal to the RHS, so that the equality indicates
the largest possible value for the LHS. In other words, the
decomposable regularization function R(·) heavily penalizes perturbations v from structured parameters u.
For any structure such as sparsity, low-rank, etc., we
can define the corresponding low-dimensional model subspaces, as well as regularization functions that are decomposable with respect to the corresponding subspace pairs.
Example 1. Given any subset S ⊆ {1, . . . , p} of the coordinates, let M(S) be the subspace of vectors in Rp that
have support contained in S. It can be seen that any parameter θ ∈ M(S) would be atmost |S|-sparse. For this case,
⊥
we use M(S) = M(S), so that M (S) = M⊥ (S). Negahban et al. (2012) show that the `1 norm R(θ) = kθk1 ,
commonly used as a sparsity-encouraging regularization
function, is decomposable with respect to subspace pairs
⊥
(M(S), M (S)).
Also of note is the subspace compatibility constant that
captures the relationship between the regularization function R(·) and the error norm k · k, over vectors in subspace
M: Ψ(M, k · k) := supu∈M\{0} R(u)
kuk . We also define the
projection operator ΠM̄ (u) := argminv∈M̄ ku − vk2 .
2.2. Regularized Convex Program Estimators
We now briefly review key regularized convex program
based estimators for high-dimensional linear regression focused on the sparse structure case, where the underlying
true parameter θ∗ is sparse,
so that denoting the

	 non-zero
indices by S(θ∗ ) := i ∈ {1, . . . , p} | θi∗ 6= 0 , the sparsity assumption constrains the cardinality k = |S(θ∗ )| as a
function of the problem size p.
The LASSO estimator (Tibshirani, 1996) solves the following `1 regularized least squares problem:
n 1
o
minimize
ky − Xθk22 + λn kθk1 .
θ
2n
The Dantzig estimator (Candes & Tao, 2007) solves the fol-

lowing linear program:
minimize kθk1
θ

s. t.

1
kX > (Xθ − y)k∞ ≤ λn .
n

(2)

This seeks a parameter θb with minimum `1 norm, and that
yet satisfies a key component of the stationary conditions
of the LASSO optimization problem.
2.3. Classical Closed-Form Estimators
When n > p, and (X > X) is full-rank (and hence invertible), the OLS estimator is then given as follows:
−1

θb = X > X
X > y . However, since the p × p matrix (X > X) can have rank atmost n, the requirement that
it is full-rank cannot be satisfied when p > n, hence the
OLS estimator is no longer well-defined.
Ridge regularized least squares estimator solves the following problem:

	
θb = arg min ky − Xθk22 + kθk22 .
(3)
θ

It can be seen that this has a unique minimum and is welldefined even in high-dimensional regimes when p > n.
The unique solution moreover is available in closed-form
as: θb = (X > X + I)−1 X > y.
2.4. Outline
In the next two sections, we derive variants of the ridge and
OLS estimators for general structurally constrained highdimensional linear regression models. We also provide a
unified statistical analysis of these two estimators for general structural constraints, deriving corollaries for specific
structures. Finally in the last section, we experimentally
corroborate the performance of our estimators.

3. The Elem-Ridge Estimator
We will now propose a variant of the standard ridgeregularized least squares estimator (3), where we incorporate the ridge estimator within a structural constraint.
Our estimator is specified by any regularization function
R : Rp 7→ R. We assume that for the regularization function R(·), the following “dual” function R∗ : Rp 7→ R is
u> θ
. When R(·) is a
well-defined: R∗ (u) = supθ:R(θ)6=0 R(θ)
norm for instance, it can be seen that R∗ (·) would be the
corresponding dual norm. Armed with this notation, we
consider the following general class of what we call ElemRidge estimators:
minimize R(θ)
θ


s. t. R∗ θ − (X > X + I)−1 X > y ≤ λn .

(4)

Elementary Estimators for High-Dimensional Linear Regression

The estimator bears similarities to the Dantzig estimator (2)
since both of these minimize some structural complexity
of the parameter subject to certain constraints. However,
unlike the Dantzig estimator, the estimator above is available in closed form for typical settings of the regularization
function R(·). For instance, when R(·) is set to the `1
norm, the estimator (4) is given by
minimize kθk1
θ


s. t. θ − (X > X + I)−1 X > y ∞ ≤ λn .

(5)

This can be seen to have a unique
solution available


>
b
in closed form as: θ = Sλn (X X + I)−1 X > y ,
where [Sλ (u)]i = sign(ui ) max(|ui | − λ, 0) is the softthresholding function.
Another interesting instantiation of R(·) would be the
group structured `1 /`α norm defined as kθkG,α :=
PL
g=1 kθGg kα , where G := {G1 , G2 , . . . , GL } is a set of
disjoint subsets/groups of the index-set {1, . . . , p} and α is
a constant between 2 and ∞. With respect to this group
norm, the estimator (4) will have the form of

the estimation problem (4) setting the constraint bound λn
such that λn ≥ R∗ (θ∗ − (X > X + I)−1 X > y). Then the
optimal solution θb satisfies the following error bounds:

R∗ θb − θ∗ ≤ 2λn ,
kθb − θ∗ k2 ≤ 4Ψ(M)λn ,

R θb − θ∗ ≤ 8[Ψ(M)]2 λn .
We note that Theorem 1 is a non-probabilistic result, and
holds deterministically for any selection of λn or any distributional setting of the covariates X. It is also worthwhile to
note that the conditions of the theorem entail that the “initial estimator” consisting of the standard ridge-regularized
least squares estimator is in turn consistent with respect to
the R∗ (·) norm. However, embedding this initial estimator
within a structural constraint as in our Elem-Ridge estimator (4) allows us to guarantee additional error bounds in
terms of R(·) and `2 norms, which do not hold for the initial ridge-regularized estimator. While the statement of the
theorem is a bit abstract, we derive its consequences under
specific structural settings as corollaries.
3.2. Sparse Linear Models

minimize kθkG,α
θ
∗

s. t. θ − (X > X + I)−1 X > y G,α ≤ λn
where kθk∗G,α := maxg kθGg kα∗ for a constant α∗ satisfying α1 + α1∗ = 1. At the time same time, the
soft-thresholding operator for the group, SG,λ can be extended as follows: for any group g in G, [SG,λ (u)]g =
u
max(kug kα − λ, 0) kuggkα , and hence the optimal solub
tion will
 have a closed-form as previous `1 case: θ =
>
−1 >
SG,λn (X X + I) X y .
3.1. Error Bounds
We now provide a unified statistical analysis of the class
of estimators in (4), for general structures, and general regularization functions R(·). We follow the structural constraint notation of Negahban et al. (2012) detailed in the
background section and assume the following:
(C1) The norm in the objective R(·) is decomposable with
⊥
respect to the subspace-pair (M, M ).
⊥

(C2) There exists a structured subspace-pair (M, M )
such that the regression parameter satisfies ΠM⊥ (θ∗ ) = 0.
In (C2), we consider the case where θ∗ is exactly sparse
with respect to the subspace pair for technical simplicity.
Theorem 1. Consider the linear regression model (1)
where the conditions (C1) and (C2) hold. Suppose we solve

We now derive a corollary of Theorem 1 for the specific
case where θ∗ is sparse with k non-zero entries. The condition described in (C2) can be written in this case as:
(C3) The regression parameter θ∗ is exactly sparse with k
non-zero entries. As discussed in Example 1, it is natural
to select M(S) equal to the support set of θ∗ .
We analyze the variant (5) which sets the regularization
function R(·) in (4) to the `1 norm. Note that the condition (C1) is automatically satisfied with this selection of
regularization function since `1 norm is decomposable with
respect to M(S) and its orthogonal complement, as discussed in Example 1. The only remaining issue to appeal
to Theorem 1, is to set λn satisfying the condition in the
statement: λn ≥ kθ∗ − (X > X + I)−1 X > yk∞ . To do so,
we leverage the analysis of the classical ridge-regression
estimator from Zhang et al. (2008a), where they impose
the following assumption:
(C-Ridge) Let e1 , . . . , eq , eq+1 , . . . , ep be the singular
vectors of n1 X > X corresponding to the singular values
d1 ≥ . . . ≥ dq > dq+1 = . . . =Pdp = 0 where
p
qP
is the rank of n1 X > X. Let θ∗ =
i=1 θi ei . Then,
p
k i=q+1 θi ei k∞ = O(ξ) with some sequence ξ → 0.
Note that this assumption is trivially satisfied if n ≥ p and
X > X has full rank. When p  n, however, this assumption plays a role as an identifiable condition for `∞ consistency so that the penalty term favors true parameter over
any others (see Zhang et al. (2008a) for details).

Elementary Estimators for High-Dimensional Linear Regression

Appealing to Theorem 3.1 in Zhang et al. (2008a), under
Condition (C-Ridge), the classical ridge-estimator satisfies
the following error bound:
 √

k log p 1/3
∗
>
−1 >
kθ − (X X + I) X yk∞ ≤ O
ndq

Suppose we apply this element-wise operator Tν to the

>
>
sample covariance matrix X n X , to obtain Tν X n X . We

>
will now show that Tν X n X will be invertible with high
probability, even under high-dimensional settings, provided the following conditions hold:

where k is the sparsity level of θ∗ in (C3). Note that this
does not entail that the classical ridge-estimator would also
be `2 or `1 norm consistent. But we can provide such
bounds for the estimator in (4) by deriving the following
corollary of Theorem 1.
Corollary 1. Consider the optimization problem in (5),
and suppose that all the conditions in (C3) and (C-Ridge)
are satisfied. Furthermore, suppose also that we select

 √
k log p 1/3
.
λn := O
ndq

(C-OLS1) (Σ-Gaussian ensemble) Each row of the design
matrix X ∈ Rn×p is i.i.d. sampled from N (0, Σ).
(C-OLS2) The design matrix X is column normalized.

Then, there are universal positive constants (c1 , c2 ) such
that any optimal solution θb of (5) satisfies
 √

k log p 1/3
∗
b
kθ − θ k∞ ≤ O
,
ndq

 2
k log p 1/3
,
kθb − θ∗ k2 ≤ O
ndq
 3 √

k k log p 1/3
kθb − θ∗ k1 ≤ O
ndq
with probability at least 1 − c1 exp(−c2 p).

Lastly, we impose the following condition on the population matrix Σ:
(C-OLS3) The covariance Σ of Σ-Gaussian ensemble is
strictly
diagonally dominant: for all row i, δi := Σii −
P
|Σ
ij | ≥ δmin > 0 where δmin is a large enough conj6=i
1
stant so that |||Σ|||∞ ≤ δmin
.
Condition (C-OLS3) that Σ be strictly diagonally dominant
is different from (and possibly stronger) than the conventional restricted eigenvalue assumption for the LASSO. As
we will show in the sequel, this assumption guarantees that
(a) the matrix Tν [(X T X)/n] is invertible, and (b) its induced `∞ norm is well bounded. We note that there could
be more general cases under which Tν [(X T X)/n] satisfies
these two conditions, and defer relaxing the assumption to
future work.
We then have the following proposition.
Proposition 1. Suppose conditions (C-OLS1)
q and (C0

Even though our estimator in (5) is consistent in `2 and
`1 norms, its convergences rate can be seen to be inferior
when compared with those of the LASSO; if we compare
`2 error bounds for instance,
estimator satisq the LASSO

k log p
∗
b
fies kθLASSO −θ k2 ≤ O
under some standard
n

conditions such as restricted eigenvalue condition (Negahban et al., 2012). For reasons of space, we thus defer deriving corollaries of Theorem 1 for other structures such as
group-sparsity, and focus on a variant of the OLS estimator
with faster convergence rates in the next section.

OLS3) hold. Then for any ν ≥ 8(maxi Σii ) 10τ nlog p ,

>
the matrix Tν X n X is invertible with probability at least
1 − 4/p0τ −2 for p0 := max{n, p} and any constant τ > 2.

>
Our key idea is to then use this invertible matrix Tν X n X
to modify the OLS estimator, and embed this within a structural constraint, so as to obtain the following class of what
we call “Elem-OLS” estimators:
minimize R(θ)
(6)
θ

h  X > X i−1 X > y 
s. t. R∗ θ − Tν
≤ λn .
n
n

4. The Elem-OLS Estimator
As noted in our discussion of the classical OLS estimator in
Section 2.3, in the high-dimensional regime p > n, the matrix X > X is rank-deficient, and the classical OLS estimator (X > X)−1 X > y is no longer well-defined since X > X
is not invertible. In this section, we thus consider the following simple variant of the OLS estimator.
For any matrix A, we first define the following elementwise operator Tν :



Aii + ν
if i = j
Tν (A) ij =
sign(Aij )(|Aij | − ν)
otherwise, i 6= j

As in Elem-Ridge estimators discussed in the beginning of
Section (4), the estimators from (6) are also available in
closed form for typical settings of the regularization function R(·).
4.1. Error Bounds
As for our earlier Elem-Ridge estimators, here too we provide a unified statistical analysis of Elem-OLS estimators
in (6), for general structures, and general regularization
functions R(·). Specifically, we obtain the following counterpart of Theorem 1:

Elementary Estimators for High-Dimensional Linear Regression

Theorem 2. Consider the linear regression model (1)
where the conditions (C1) and (C2) hold. Suppose we solve
the estimation problem
 bound λn
 (6)setting>theconstraint
−1 X > y
. Then the
such that λn ≥ R∗ θ∗ − Tν X n X
n
optimal solution θb satisfies the following error bounds:

R∗ θb − θ∗ ≤ 2λn ,
kθb − θ∗ k2 ≤ 4Ψ(M)λn ,

R θb − θ∗ ≤ 8[Ψ(M)]2 λn .
As in the class of Elem-Ridge estimators, here we have an
“initial estimator” consisting of a (novel) OLS-esque estih  > i−1 >
X y
mator Tν X n X
n . The conditions of our theorem guarantee that even this initial estimator is consistent
with respect to R∗ (·) norm. However, embedding this initial estimator within a structural constraint as in our ElemOLS estimator (6) allows us to guarantee additional error
bounds in terms of R(·) and `2 norms, which do not hold
for our initial OLS-esque estimator.
We now derive the consequences of our abstract theorem
under specific structural settings.
4.2. Sparse Linear Models
Consider the case when the true parameter θ∗ is sparse with
k non-zero elements. We then consider the variant of (6)
with the regularization function R(·) set to the `1 norm:
minimize kθk1
(7)
θ


h  X > X i−1 X > y 

 ≤ λn .
s. t. 
θ − Tν
n
n ∞
We can then derive the convergence rate of this estimator (7) as a corollary of Theorem 2:
Corollary 2. Under the condition (C3), consider the
optimization
problem
(7), setting ν := 8(maxi Σii )
q
q
10τ log p0
n

with probability at least 1 − c1 exp(−c2 p0 ).
The rates in Corollary 2 are the almost same as those
by q
standard LASSO; for instance, kθbLASSO − θ∗ k2 ≤


k log p
O
analyzed in Negahban et al. (2012) even
n
though the rates here have an additional kθ∗ k1 term, which
are negligible if k and kθ∗ k∞ are bounded by constants.
4.3. Group-sparse Linear Models
Another interesting structural constraint arises when θ∗ is
group-sparse. Consider a set of disjoint subsets/groups
G := {G1 , G2 , . . . , GL } of the index-set {1, . . . , p}, each
of size at most |Gj | ≤ m. For any vector u ∈ Rp , let uGj
denote the subvector with indices restricted to the set Gj .
We assume that the model parameter θ∗ is group-sparse
with respect to these groups G so that the condition described in (C2) can be written in this case as:
(C4) The linear regression parameter θ∗ has group∗
support of size atmost k, so that |{j ∈ {1, . . . , L} : θG
6=
j
0}| ≤ k.
A natural regularization function for such a setting is
the following group-structured `1 /`α norm defined as
PL
kθkG,α := g=1 kθGg kα , where α is a constant between 2
and ∞.
In this case, the assumption of column normalization in
(C-OLS2) can be naturally generalized (Negahban et al.,
2012):
(C-OLS4) Define the operator norm |||XG |||α→2
maxkwkα =1 kXG wk2 . Then,
1, . . . , L.

|||XGj |||α→2
√
n

:=

≤ 1 for all j =

We then consider the following variant of (6), with the regularization function R(·) set to the above group-structured
norm:

0

:= a lognp , and p0 := max{n, p}. Suppose
that all the conditions in (C-OLS1)-(C-OLS3) are satisfied.
Furthermore, suppose also that we select
r
 r

1
log p0
log p0 ∗
λn :=
2σ
+a
kθ k1 .
δmin
n
n
Then, there are universal positive constants (c1 , c2 ) such
that any optimal solution θb of (7) satisfies
r
 r

2
log p0
log p0 ∗
∗
b
kθ − θ k∞ ≤
2σ
+a
kθ k1 ,
δmin
n
n
r
r


4
k log p0
k log p0 ∗
kθb − θ∗ k2 ≤
2σ
+a
kθ k1 ,
δmin
n
n
r
r


0
8
log p
log p0 ∗
∗
b
kθ − θ k1 ≤
2σk
+ ak
kθ k1
δmin
n
n

minimize kθkG,α
(8)
θ

h  X > X i−1 X > y 

∗

s. t. 
θ
−
T
≤ λn .
ν

n
n G,α
where kθk∗G,α := maxg kθGg kα∗ for a constant α∗ satisfying α1 + α1∗ = 1.
Now we can derive the convergence rates of this estimator
(8) as a corollary of Theorem 2:
Corollary 3. Under the condition (C4), consider the
optimization
problem
q
q (8), setting ν := 8(maxi Σii )
10τ log p0
n

0

:= a lognp , and p0 := max{n, p}. Suppose that all the conditions in (C-OLS1), (C-OLS3) and
(C-OLS4) are satisfied. Furthermore, suppose also that we

Elementary Estimators for High-Dimensional Linear Regression
Table 1. Average performance measure and standard deviation in parenthesis for `1 -penalized comparison methods on simulated data
for sparse linear models.

n=1000,p=1000

n=1000,p=2000

Method
Elem-OLS
Elem-Ridge
LASSO
Thr-LASSO
OMP
Elem-OLS
Elem-Ridge
LASSO
Thr-LASSO
OMP

TP
100.00 (0.00)
100.00 (0.00)
100.00 (0.00)
100.00 (0.00)
98.24 (0.64)
100.00 (0.00)
100.00 (0.00)
100.00 (0.00)
99.59(0.36)
96.36(1.00)

select
∗

λn :=

m1/α
δmin

r
 r

log p0
m log p0 ∗
2σ
+a
kθ k1 .
n
n

Then, there are universal positive constants (c1 , c2 ) such
that any optimal solution θb of (8) satisfies
r
 r

1/α∗
0
log
p
m log p0 ∗
2m
∗
∗
+a
kθ k1 ,
2σ
kθb − θ kG,α ≤
δmin
n
n
r
√ 1/α∗  r

log p0
m log p0 ∗
4 km
∗
b
kθ − θ k2 ≤
+a
kθ k1 ,
2σ
δmin
n
n
r
r

∗ 
log p0
m log p0 ∗
8km1/α
+a
kθ k1
kθb − θ∗ kG,α ≤
2σ
δmin
n
n
with probability at least 1 − c1 exp(−c2 p0 ).

5. Experiments
We demonstrate the performance of our elementary estimators on simulated as well as real-world datasets.
5.1. Simulation study
We first provide simulation studies that corroborate our theoretical results, and furthermore compares the finite sample
performance of various methods.
Sparse linear models For our first set of simulations, we
generate data according to the linear model y = Xθ∗ + w.
We construct the n × p design matrices X by sampling
the rows independently from a multivariate Gaussian distribution N (0, Σ) where Σi,j = 0.5|i−j| . We then set
the error term as w ∼ N (0, 1). For each simulation, the
entries of the true model coefficient vector θ∗ are set to
be 0 everywhere, except for a randomly chosen subset of
10 coefficients, which are chosen independently and uniformly in the interval (1, 3). We set the number of samples to n = 1000, and the number of covariates among
p ∈ {1000, 2000}. We compare the performance of ElemOLS: our OLS-variant elementary estimator in (7), with the

FP
2.05 (1.15)
2.44 (2.12)
9.84 (2.45)
8.33 (1.14)
3.20 (1.38)
2.22 (2.02)
11.94 (4.48)
18.88 (6.93)
14.35(2.66)
10.25 (4.24)

`2
0.551 (0.071)
0.741 (0.411)
0.563 (0.067)
0.560 (0.066)
0.559 (0.113)
0.656 (0.111)
3.8834 (0.411)
0.657(0.110)
0.656 (0.099)
0.735(0.222)

`∞
0.255 (0.041)
0.435 (0.064)
0.270 (0.039)
0.274 (0.071)
0.282 (0.055)
0.314( 0.071)
1.678 (0.349)
0.316(0.075)
0.315(0.052)
0.536(0.136)

operator Tν and the regularization function R(·) set to the
`1 norm; Elem-Ridge: our ridge-variant elementary estimator in (5) with the regularization function R(·) set to the `1
norm, LASSO: the standard LASSO estimator, Thr-LASSO:
the LASSO estimator followed by post hard-thresholding
and OLS refitting (van de Geer et al., 2011), and OMP:
the Orthogonal Matching Pursuit estimator (Zhang, 2010).
As performance measures, we used the True Positive Rate
(TP), False Positive Rate (FP), `∞ and `2 error between the
estimated and true parameter vectors. While our theorem
specified an optimal setting of the regularization parameter
λ, this optimal setting depended on unknown model parameters. Thus, as is standard with high-dimensional regularized convex programs, we set the tuning parameters in a
holdout-validated fashion, as those that minimize the average squared error on an independent validation set of sample size n. For each setting, we present the average of the
performance measures based on 100 simulations in Table 1.
As can be seen from the table, the performance of ElemOLS in term of `2 and `∞ errors is competitive with that
of LASSO, which corroborates our statistical analyses of
Section 4.2. In addition, the simulation results confirm the
suboptimal performance of Elem-Ridge – especially as p
becomes larger than n. Elem-OLS even achieves superior
variable selection accuracy when compared to the LASSO.
This is further illustrated in Figure 1, which depicts ROC
curves for the various methods, i.e., how TP and FP evolve
as the amount of regularization (stopping point for OMP)
is varied. We find this remarkable, given that Elem-OLS is
available in closed-form, whereas LASSO, OMP and variants require an iterative optimization procedure.
Group-sparse linear models The data is generated following the lines of the third model of Yuan & Lin
(2006). We compare Group-LASSO, Thr-Group-LASSO
(Group-LASSO followed by hard-thresholding), GroupOMP (Lozano et al., 2009), with our Elem-OLS estimator
in (8) for group sparsity, which we refer to as Group-ElemOLS. We present the average of the performance measures
based on 100 simulations in Table 2. As can be seen from

Elementary Estimators for High-Dimensional Linear Regression
Table 2. Average performance measure and standard deviation in parenthesis for `1 -penalized comparison methods on simulated data
for group-sparse linear models.
TP
100 (0.00)
100 (0.00)
99 (0.14)
99 (0.16)

FP
0.9 (0.10)
1.8 (0.18)
1.2 (0.16)
1.9 (0.15)

100

Method
Group-Elem-OLS
Group-LASSO
Thr-Group-LASSO
Group-OMP

99
98
97
96

True Positive %

Table 4. Top 20 selected genes for the microarray study, along
with their associated pathways.
Elem-OLS
LASSO

95

2

4

6

`∞
0.613 (0.027)
0.642 (0.031)
0.628 (0.029)
0.642(0.030)

Table 3. Average test MSE on microarray dataset. (Smaller values
indicate higher predictive accuracy).
Elem-OLS
LASSO
10.52 ± 0.39 11.59 ± 0.39

Elem-OLS
LASSO
Thr-LASSO
OMP
Elem-Ridge

0

`2
1.300 (0.045)
1.269 (0.095)
1.296 (0.075)
1.984(0.080)

8

False Positive %

Figure 1. ROC curves for the Elem-OLS, Elem-Ridge, LASSO,
Thresholded-LASSO and OMP estimators under sparse linear
model.

the table, the performance of Group-Elem-OLS is comparable to the other methods in term of `2 and `∞ errors are
comparable. Group-Elem-OLS achieves superior variable
selection accuracy.
5.2. Real Data Analysis
We employed our estimators toward the analysis of gene
expression data. We used microarray data pertaining to isoprenoid biosynthesis in Arabidopsis thaliana (A. thaliana)
provided by Wille et al. (2004). A. thaliana is a plant
widely used as model organism in genetics. Isoprenoids
play key roles in major plant processes including photosynthesis, respiration and defense against pathogens. Here
we focus on identifying the genes that exhibit significant
association with the specific isoprenoid gene GGPPS11
(AGI code AT4G36810), which is known as a precursor to chlorophils, carotenoids, tocopherols and abscisic
acids. Thus, the response is the expression level of the
isoprenoid gene GGPPS11, while as covariates, we have
the expression levels from 833 genes, coming from 58 different metabolic pathways. There are 131 samples. All
variables are log transformed. Due to space limitations, we
only report results for our OLS-variant elementary estimator with `1 regularization (Elem-OLS) and for the LASSO
estimator. We evaluate the predictive accuracy of the methods by randomly partitioning the data into training and test
sets, using 90 observations for training and the remainder

Pathway
Carote
Cytoki
Flavon
Flavon
Folate
Glycer
Glycol
Inosit
Inosit
Non-Mev
Phenyl
Phytos
Porphy
Ribofl
Starch
Starch
Threo
Toco
Trypto

Gene (AGI)
AT1G57770
AT3G63110
AT2G38240
AT5G08640
AT1G78670
AT2G44810
AT5G50850
AT2G31830
AT3G56960
AT1G17050
AT2G27820
AT1G58440
AT1G09940
AT4G13700
AT2G45880
AT5G03650
AT1G72810
AT2G18950
AT5G48220

Pathway
Carote
Citrate
Ethyl
Ethyl
Flavon
Folate
Inosit
Inosit
Phenyl
Porphy
Pyrimi
Ribofl
SGC
Starch
Starch
Starch
Toco
Trypto
Trypto

Gene (AGI)
AT1G57770
AT1G54340
AT1G01480
AT4G11280
AT5G08640
AT1G78670
AT3G56960
AT2G41210
AT2G27820
AT3G51820
AT5G23300
AT4G13700
AT5G28030
AT2G45880
AT2G25540
AT5G29958
AT2G18950
AT5G17980
AT3G03680

for testing. The tuning parameters were selected using 5
fold cross-validation. We computed the prediction MSE
for the testing set. The results for 20 random train/test partitions are presented in Table 3. Overall, our elementary
estimator achieves superior prediction accuracy. We now
look into the genes selected by the comparison methods
on the full dataset. Out of 833 candidate genes, Elem-OLS
and LASSO selected selected 79 and 73 genes respectively.
For each method we ranked the selected genes according to
the amplitude of their regression coefficients. The top 20
genes for each method are listed in Table 4 along with their
associated pathways (the pathway names are abbreviated).
From table 4 we can see Elem-OLS and LASSO have 7
genes in common among their respective top 20 genes. Interestingly, the gene AT1G17050 (a.k.a. PPDS1), which
is known to belong to the same pathway as target gene
GGPPS11 (isoprenoid non-mevalonate), is included in the
top-20 genes of Elem-OLS, but not of LASSO.
Acknowledgments
E.Y and P.R. acknowledge the support of ARO via
W911NF-12-1-0390 and NSF via IIS-1149803, IIS1320894, DMS-1264033.

Elementary Estimators for High-Dimensional Linear Regression

References
Bach, F. Consistency of trace norm minimization. Journal of
Machine Learning Research, 9:1019–1048, June 2008.

Meinshausen, N. and Yu, B. Lasso-type recovery of sparse representations for high-dimensional data. Annals of Statistics, 37
(1):246–270, 2009.

Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. Structured
sparsity through convex optimization. Statistical Science, 27
(4):450–468, 2012.

Negahban, S. and Wainwright, M. J. Simultaneous support recovery in high-dimensional regression: Benefits and perils of
`1,∞ -regularization. Technical report, Department of Statistics, UC Berkeley, April 2009.

Baraniuk, R. G., Cevher, V., Duarte, M. F., and Hegde, C. Modelbased compressive sensing. Technical report, Rice University,
2008. Available at arxiv:0808.3572.

Negahban, S., Ravikumar, P., Wainwright, M. J., and Yu, B.
A unified framework for high-dimensional analysis of Mestimators with decomposable regularizers. Statistical Science,
27(4):538–557, 2012.

Bickel, P., Ritov, Y., and Tsybakov, A. Simultaneous analysis of
lasso and dantzig selector. 37(4):1705–1732, 2009. Annals of
Statistics.
Candes, E. and Tao, T. The Dantzig selector: Statistical estimation
when p is much larger than n. Annals of Statistics, 2006.
Candes, E. J. and Tao, T. The dantzig selector: Statistical estimation when p is much larger than n. Annals of Statistics, 35(6):
2313–2351, 2007.
Donoho, D. For most large underdetermined systems of linear
equations, the minimal `1 -norm solution is also the sparsest
solution. Communications on Pure and Applied Mathematics,
59(6):797–829, June 2006.
Fan, J. and Lv, J. Sure independence screening for ultra-high dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology) (JRSSB), 70:849–911,
2008.
Friedman, J., Hastie, T., Hofling, H., and Tibshirani, R. Pathwise
coordinate optimization. Annals of Applied Statistics, 2007.
Genovese, C. R., Jin, J., Wasserman, L., and Yao, Z. A comparison of the lasso and marginal regression. Journal of Machine
Learning Research (JMLR), 13:2107–2143, 2012.
Hsieh, C. J., Sustik, M., Dhillon, I., and Ravikumar, P. Sparse
inverse covariance matrix estimation using quadratic approximation. In Neur. Info. Proc. Sys. (NIPS), 24, 2011.
Huang, J., Zhang, T., and Metaxas, D. Learning with structured
sparsity. Journal of Machine Learning Research (JMLR), 12:
3371–3412, 2011.
Jacob, L., Obozinski, G., and Vert, J. P. Group Lasso with Overlap and Graph Lasso. In International Conference on Machine
Learning (ICML), pp. 433–440, 2009.
Lounici, K., Pontil, M., Tsybakov, A. B., and van de Geer, S.
Taking advantage of sparsity in multi-task learning. Technical
Report arXiv:0903.1468, ETH Zurich, March 2009.

Obozinski, G., Wainwright, M. J., and Jordan, M. I. Union
support recovery in high-dimensional multivariate regression.
Technical report, Department of Statistics, UC Berkeley, August 2008.
Recht, B., Fazel, M., and Parrilo, P. Guaranteed minimum-rank
solutions of linear matrix equations via nuclear norm minimization. SIAM Review, Vol 52(3):471–501, 2010.
Tibshirani, R. Regression shrinkage and selection via the lasso.
Journal of the Royal Statistical Society, Series B, 58(1):267–
288, 1996.
Tropp, J. A., Gilbert, A. C., and Strauss, M. J. Algorithms for simultaneous sparse approximation. Signal Processing, 86:572–
602, April 2006. Special issue on ”Sparse approximations in
signal and image processing”.
van de Geer, S. and Buhlmann, P. On the conditions used to prove
oracle results for the lasso. Electronic Journal of Statistics, 3:
1360–1392, 2009.
van de Geer, S., Bühlmann, P., and Zhou, S. The adaptive and
the thresholded lasso for potentially misspecified models (and
a lower bound for the lasso). Electronic Journal of Statistics,
5:688–749, 2011.
Wainwright, M. J. Sharp thresholds for high-dimensional and
noisy sparsity recovery using `1 -constrained quadratic programming (Lasso). IEEE Trans. Information Theory, 55:2183–
2202, May 2009.
Wille, A., Zimmermann, P., and Vranova, E. [and others]. Sparse
graphical gaussian modeling of the isoprenoid gene network in
arabidopsis thaliana. Genome Biology, 5, 2004.
Yuan, M. and Lin, Y. Model selection and estimation in regression
with grouped variables. Journal of the Royal Statistical Society
B, 1(68):49, 2006.
Zhang, J., Jeng, X. J., and Liu, H. Some two-step procedures for
variable selection in high-dimensional linear regression. Arxiv
preprint arXiv:0810.1644, 2008a.
Zhang, T. Sparse recovery with orthogonal matching pursuit under rip. Tech Report arXiv:1005.2249, May 2010.

Lozano, A. C., Swirszcz, G., and Abe, N. Group orthogonal
matching pursuit for variable selection and prediction. In Neur.
Info. Proc. Sys (NIPS), 2009.

Zhang, Z., Dolecek, L., Nikolic, B., Anantharam, V., and Wainwright, M. J. Lowering LDPC error floors by post-processing.
In Proc. IEEE GLOBECOM, September 2008b.

Mallat, S. and Zhang, Z. Matching pursuits with time-frequency
dictionaries. IEEE Trans. Signal Processing, December 1993.

Zhao, P. and Yu, B. On model selection consistency of Lasso.
Journal of Machine Learning Research, 7:2541–2567, 2006.

Meinshausen, N. and Bühlmann, P. High-dimensional graphs and
variable selection with the Lasso. Annals of Statistics, 34:
1436–1462, 2006.

Zhao, P., Rocha, G., and Yu, B. Grouped and hierarchical model
selection through composite absolute penalties. Annals of
Statistics, 37(6A):3468–3497, 2009.

