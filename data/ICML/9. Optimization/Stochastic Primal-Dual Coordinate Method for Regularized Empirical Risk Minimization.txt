Stochastic Primal-Dual Coordinate Method for Regularized
Empirical Risk Minimization

Yuchen Zhang
University of California Berkeley, Berkeley, CA 94720, USA
Lin Xiao
Microsoft Research, Redmond, WA 98053, USA

Abstract
We consider a generic convex optimization problem associated with regularized empirical risk
minimization of linear predictors. The problem structure allows us to reformulate it as a
convex-concave saddle point problem. We propose a stochastic primal-dual coordinate method,
which alternates between maximizing over one
(or more) randomly chosen dual variable and
minimizing over the primal variable. We also
develop an extension to non-smooth and nonstrongly convex loss functions, and an extension with better convergence rate on unnormalized data. Both theoretically and empirically, we
show that the SPDC method has comparable or
better performance than several state-of-the-art
optimization methods.

1. Introduction
We consider a generic convex optimization problem in
machine learning: regularized empirical risk minimization (ERM) of linear predictors. More specifically, let
a1 , . . . , an ∈ Rd be the feature vectors of n data samples,
φi : R → R be a convex loss function associated with the
linear prediction aTi x, for i = 1, . . . , n, and g : Rd → R be
a convex regularization function for the predictor x ∈ Rd .
Our goal is to solve the following optimization problem:
)
(
n
X
def 1
T
min
P (x) =
φi (ai x) + g(x) . (1)
n i=1
x∈Rd
Examples of the above formulation include many wellknown classification and regression problems. For binary
nd

Proceedings of the 32
International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

YUCZHANG @ EECS . BERKELEY. EDU

LIN . XIAO @ MICROSOFT. COM

classification, each feature vector ai is associated with a label bi ∈ {±1}. We obtain the linear SVM (support vector
machine) by setting φi (z) = max{0, 1 − bi z} (the hinge
loss) and g(x) = (λ/2)kxk22 , where λ > 0 is a regularization parameter. Regularized logistic regression is obtained
by setting φi (z) = log(1 + exp(−bi z)). For linear regression problems, each feature vector ai is associated with a
dependent variable bi ∈ R, and φi (z) = (1/2)(z − bi )2 .
Then we get ridge regression with g(x) = (λ/2)kxk22 , and
the Lasso with g(x) = λkxk1 . Further backgrounds on
regularized ERM in machine learning and statistics can be
found, e.g., in the book by Hastie et al. (2009).
We are especially interested in developing efficient algorithms for solving problem (1) when the number of samples n is very large. In this case, evaluating the full gradient or subgradient of the function P (x) is expensive,
thus incremental methods that operate on a single component function φi at each iteration can be very attractive. There have been extensive research on incremental
(sub)gradient methods (e.g., Tseng, 1998; Nedić & Bertsekas, 2001; Blatt et al., 2007; Bertsekas, 2011) as well
as variants of the stochastic gradient method (e.g., Zhang,
2004; Bottou, 2010; Duchi & Singer, 2009; Langford et al.,
2009; Xiao, 2010). While the computational cost per iteration of these methods is only a small fraction, say 1/n, of
that of the batch gradient methods, their iteration complexities are much higher (it takes many more iterations for them
to reach the same precision). In order to better quantify the
complexities of various algorithms and position our contributions, we need to make some concrete assumptions and
introduce the notion of condition number and batch complexity.
1.1. Condition number and batch complexity
Let γ and λ be two positive real parameters. We make the
following assumption:
Assumption A. Each φi is convex and differentiable, and
its derivative is (1/γ)-Lipschitz continuous (same as φi be-

Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization

ing (1/γ)-smooth), i.e. for i = 1, . . . , n,
|φ0i (α)

−

φ0i (β)|

≤ (1/γ)|α − β|,

dient methods are the same as their iteration complexities.
∀ α, β ∈ R.

In addition, the regularization function g is λ-strongly convex, i.e. for any x, y ∈ Rn and any g 0 (y) ∈ ∂g(y), we have
g(y) ≥ g(x) + g 0 (y)T (x − y) +

λ
kx − yk22 .
2

For example, the logistic loss φi (z) = log(1 + exp(−bi z))
is (1/4)-smooth, the squared error φi (z) = (1/2)(z − bi )2
is 1-smooth, and the squared `2 -norm g(x) = (λ/2)kxk22 is
λ-strongly convex. The hinge loss and the `1 -regularization
do not satisfy Assumption A. Nevertheless, we can treat
them using smoothing and strongly convex perturbations,
respectively, so that our algorithm and theoretical framework still apply (see Section 3.1).
Under Assumption A, the gradient of each component
function, ∇φi (aTi x), is also Lipschitz continuous, with
constant Li = kai k22 /γ ≤ R2 /γ, where R = maxi kai k2 .
In other words, each φi (aTi x) is (R2 /γ)-smooth. We define a condition number κ = R2 /(λγ), and focus on illconditioned problems where κ  1. In the statistical learning context,√
the regularization parameter λ is usually on the
order of 1/ n or 1/n (e.g.
√ Bousquet & Elisseeff, 2002),
thus κ is on the order of n or n. It can be even larger
if the strong convexity in g is added purely for numerical
regularization purposes (see Section 3.1).

1.2. Our Contribution
In this paper, we present a new algorithm with batch complexity
p

(2)
O (1 + κ/n) log(1/) ,
This complexity has much weaker dependence on n than
the full gradient methods, and also much weaker dependence on  than the stochastic gradient methods.
Our approach is based on reformulating problem (1) as a
convex-concave saddle point problem, and then devising
a primal-dual algorithm to approximate the saddle point.
More specifically, we replace each component function
φi (aTi x) through convex conjugation, i.e.,
φi (aTi x) = sup {yi hai , xi − φ∗i (yi )} ,
yi ∈R

where φ∗i (yi ) = supα∈R {αyi −φi (α)}, and hai , xi denotes
the inner product of ai and x (which is the same as aTi x, but
is more convenient for later presentation). This leads to a
convex-concave saddle point problem
min max f (x, y),

x∈Rd y∈Rn

where
def

f (x, y) =

?

Let P = minx∈Rd P (x) be the optimal value of problem (1). In order to find an approximate solution x̂ satisfying P (x̂) − P ? ≤ , the classical full gradient method
and its proximal variants require O((1 + κ) log(1/)) iterations (e.g., Nesterov, 2004). Accelerated full gradient
(AFG) methods (Nesterov,√2004) enjoy the improved iteration complexity O((1 + κ) log(1/)). However, each
iteration of these batch methods requires a full pass over
the dataset, which cost O(nd) operations. In contrast, the
stochastic gradient method and its variants operate on one
single component φi (aTi x) (chosen randomly) at each iteration, which only costs O(d). But their iteration complexities are far worse. Under Assumption A, it takes them
O(κ/) iterations to find an x̂ such that E[P (x̂) − P ? ] ≤ ,
where the expectation is with respect to the random choices
made at all the iterations (e.g., Polyak & Juditsky, 1992;
Nemirovski et al., 2009).

n

1X
yi hai , xi − φ∗i (yi ) + g(x).
n i=1

(4)

Under Assumption A, each φ∗i is γ-strongly convex (since
φi is (1/γ)-smooth (e.g., Hiriart-Urruty & Lemaréchal,
2001, Theorem 4.2.2)) and g is λ-strongly convex. As a
consequence, the saddle point problem (3) has a unique
solution, which we denote by (x? , y ? ). The Stochastic
Primal-Dual Coordinate (SPDC) method we propose in this
paper achieves the batch complexity in (2) for solving the
primal-dual problem (3).
1.3. Comparing with Dual Coordinate Ascent Methods
It is worth comparing our method with the family of dual
coordinate ascent methods, which solves the primal problem (1) via its dual:
(
maxn

y∈R

To make fair comparisons with batch methods, we measure
the complexity of stochastic or incremental gradient methods in terms of the number of equivalent passes over the
dataset required to reach an expected precision . We call
this measure the batch complexity, which are usually obtained by dividing their iteration complexities by n. For
example, the batch complexity of the stochastic gradient
method is O(κ/(n)). The batch complexities of full gra-

(3)


)
n
n
1X
1X
−φ∗i (yi ) − g ∗ −
yi ai
,
n i=1
n i=1

(9)

where g ∗ (u) = supx∈Rd {xT u − g(x)} is the conjugate
function of g. Recent work show that dual coordinate ascent methods are typically more efficient than primal full
gradient methods (e.g., Hsieh et al., 2008; Shalev-Shwartz
& Zhang, 2013a). In the stochastic dual coordinate ascent
(SDCA) method, a dual coordinate yi is picked at random
during each iteration and updated to increase the dual ob-

Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization

Algorithm 1 The Stochastic Primal-Dual Coordinate (SPDC) method
Input: mini-batch size m, parameters τ, σ, θ ∈ R+ , number of iterations T , and x(0) and y (0) .
Pn
(0)
Initialize: x(0) = x(0) , u(0) = (1/n) i=1 yi ai . for t = 0, 1, 2, . . . , T − 1 do
Randomly pick a subset of indices K ⊂ {1, 2, . . . , n} of size m, such that the probability of each index being picked
is equal to m/n. Execute the following updates:
n
o
(
(t)
1
arg maxβ∈R βhai , x(t) i − φ∗i (β) − 2σ
(β − yi )2
if i ∈ K,
(t+1)
yi
=
(5)
(t)
yi
if i ∈
/ K,
)
(


kx − x(t) k22
1 X (t+1)
(t)
(t+1)
(t)
(yk
− yk )ak , x +
,
(6)
x
= arg min g(x) + u +
m
2τ
x∈Rd
k∈K
1 X (t+1)
(t)
u(t+1) = u(t) +
(yk
− yk )ak ,
(7)
n
k∈K

x

(t+1)

=x

(t+1)

+ θ(x(t+1) − x(t) ).

(8)

end
Output: x(T ) and y (T )

jective value. Shalev-Shwartz & Zhang (2013a) showed
e + κ/n). The
that the batch complexity of SDCA is O(1
p
e
SPDC method, which has batch complexity O(1+
κ/n),
can be much better when κ > n, i.e., for ill-conditioned
problems.
In addition, Shalev-Shwartz & Zhang (2013b) developed
an accelerated proximal SDCA method which achieves
the same batch complexity in (2). Their method is an
inner-outer iteration procedure, where the outer loop is a
full-dimensional accelerated gradient method in the primal
space x ∈ Rd . At each iteration of the outer loop, the
SDCA method (Shalev-Shwartz & Zhang, 2013a) is called
to solve the dual problem (9) with customized regularization parameter and precision. In contrast, SPDC is a singleloop primal-dual coordinate method.
More recently, Lin et al. (2014) developed an accelerated
proximal coordinate gradient (APCG) method for solving a
more general class of composite convex optimization problems. When applied to the dual problem
p (9), APCG enjoys
e 1 + κ/n as of SPDC.
the same batch complexity O
However, it needs an extra primal proximal-gradient step to
have theoretical guarantees on the convergence of primaldual gap (Lin et al., 2014). This in unnecessary for the
SPDC method.

2. The SPDC method
In this section, we describe and analyze the SPDC method.
The basic idea of SPDC is quite simple: to approach the
saddle point of f (x, y) defined in (4), we alternatively maximize f with respect to y, and minimize f with respect to x.
Since the dual vector y has n coordinates and each coordinate is associated with a feature vector ai ∈ Rd , maximiz-

ing f with respect to y takes O(nd) computation, which
can be very expensive if n is large. We reduce the computational cost by randomly picking m coordinates of y at a
time, and maximizing f only with respect to the selected
coordinates. Consequently, the computational cost of each
iteration is O(md). Here m is called the mini-batch size;
in the simplest case, we have m = 1.
We give the details of the SPDC method in Algorithm 1.
The dual coordinate update and primal vector update are
given in equations (5) and (6) respectively. Instead of maximizing f over yk and minimizing f over x directly, we add
(t+1)
two quadratic regularization terms to penalize yk
and
(t)
(t+1)
(t)
x
from deviating from yk and x . The parameters σ
and τ control their regularization strength, which we will
specify in the convergence analysis (Theorem 1). Moreover, we introduce two auxiliary variables u(t) and x(t) .
Pn
(0)
From the initialization u(0) = (1/n) i=1 yi ai and the
Pn
(t)
update rules (5) and (7), we have u(t) = n1 i=1 yi ai .
(t+1)
Equation (8) obtains x
based on extrapolation from
x(t) and x(t+1) . This step is similar to Nesterov’s acceleration technique (Nesterov, 2004), and yields faster convergence rate.
With a single processor, each iteration of Algorithm 1 takes
O(md) time to accomplish. Since the updates of each coordinate yk are independent of each other, we can use parallel computing to accelerate the Mini-Batch SPDC method.
Concretely, we can use m processors to update the m coordinates in the subset K in parallel, then aggregate them
to update x(t+1) . Such a procedure can be achieved by
a single round of communication, for example, using the
Allreduce operation in MPI or MapReduce. If we ignore the communication delay, then each iteration takes

Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization

O(d) time. Not surprisingly, we will show that the SPDC
algorithm converges faster with larger m, because it processes multiple dual coordinates in a single iteration.
2.1. Convergence analysis
We present a convergence theorem for the SPDC algorithm.
Theorem 1. Assume that each φi is (1/γ)-smooth and g is
λ-strongly convex (Assumption A). Let R = max{kai k2 :
i = 1, . . . , n}. If the parameters τ, σ and θ in Algorithm 1
are chosen such that
s
r
mγ
nλ
1
1
, σ=
and
τ=
2R nλ
2R mγ
θ =1−

(n/m) + R

1
p

(n/m)/(λγ)

,

(10)

then for each t ≥ 1, the Mini-Batch SPDC algorithm
achieves
 1
 
 1
 Eky (t) − y ? k2 
2
(t)
? 2
+ λ E kx − x k2 +
+γ
2τ
4σ
m


 1
 ky (0) − y ? k2 
1
2
+ λ kx(0) − x? k22 +
+γ
.
≤θt
2τ
2σ
m

The proof of Theorem 1 is given in the long version of this
paper (Zhang & Xiao, 2014). The following corollary establishes the expected iteration complexity for obtaining an
-accurate solution.
Corollary 1. Suppose Assumption A holds and the parameters τ , σ and θ are set as in (10). In order for Algorithm 1
to obtain
E[kx(T ) − x? k22 ] ≤ ,

E[ky (T ) − y ? k22 ] ≤ ,

(11)

it suffices to have the number of iterations T satisfy


 
r
n
n
C
+R
log
,
T ≥
m
mλγ

where
C=

1
2τ


+ λ kx(0) − x? k22 +
1
min 2τ
+ λ,

1
2σ
1
4σ


+ γ ky (0) − y ? k22 /m
	
.
+ γ)/m

Proof. By Theorem 1, we have E[kx(T ) − x? k22 ] ≤ θT C
and E[ky (T ) − y ? k22 ] ≤ θT C. To obtain (11), it suffices to
ensure that θT C ≤ , which is equivalent to
T ≥

log(C/)
=
− log(θ)

log(C/)


−1  .
q
n
n
− log 1 − m
+ R mλγ

Applying the inequality − log(1 − x) ≥ x to the denominator above completes the proof.
Recall the definition of the condition number κ = R2 /(λγ)
in Section 1.1. Corollary 1 establishes that the iteration

complexity of the SPDC method for achieving (11) is


p

O (n/m) + κ(n/m) log(1/) .
So a larger batch size m leads to less number of iterations. In the extreme case of n = m, we obtain a full
batch algorithm,
which has iteration or batch complex√
ity O((1 + κ) log(1/)). This complexity is shared by
the AFG methods (Nesterov, 2004), as well as the batch
primal-dual algorithm of Chambolle & Pock (2011).
Since an equivalent pass over the dataset corresponds to
n/m iterations, the batch complexity of SPDC is


p

O 1 + κ(m/n) log(1/) .
This expression implies that a smaller batch size m leads
to less number of passes through the data. In this sense, the
basic SPDC method with m = 1 is the most efficient one.
However, if we prefer the least amount of wall-clock time,
then the best choice is to choose a mini-batch size m that
matches the number of parallel processors available.

3. Extensions of SPDC
In this section, we derive two extensions of the SPDC
method. The first one handles problems for which Assumption A does not hold. The second one employs a nonuniform sampling scheme to improve the iteration complexity when the feature vectors ai are unnormalized.
3.1. Non-smooth or non-strongly convex functions
The complexity bounds established in Section 2 require
each φ∗i to be γ-strongly convex, which corresponds to the
condition that the first derivative of φi is (1/γ)-Lipschitz
continuous. In addition, the function g needs to be λstrongly convex. For general loss functions where either
or both of these conditions fail (e.g., the hinge loss and
`1 -regularization), we can slightly perturb the saddle-point
function f (x, y) so that SPDC can still be applied.
For simplicity, here we consider the case where neither φi
is smooth nor g is strongly convex. Formally, we assume
that each φi and g are convex and Lipschitz continuous,
and f (x, y) has a saddle point (x? , y ? ). We choose a scalar
δ > 0 and consider the modified saddle-point function:

n 

X
δy 2 
def 1
fδ (x, y) =
yi hai , xi − φ∗i (yi ) + i
n i=1
2
δ
+ g(x) + kxk22 .
2
Denote by (x?δ , yδ? ) the saddle-point of fδ . We employ
the SPDC method (Algorithm 1) to approximate (x?δ , yδ? ),
treating φ∗i + 2δ (·)2 as φ∗i and g + 2δ k·k22 as g, which now are
all δ-strongly convex. We note that adding strongly convex

Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization

Algorithm 2 SPDC method with weighted sampling
Input: parameters τ, σ, θ ∈ R+ , number of iterations T , and initial points x(0) and y (0) .
Pn
(0)
Initialize: x(0) = x(0) , u(0) = (1/n) i=1 yi ai . for t = 0, 1, 2, . . . , T − 1 do
1
k k2
. Execute the following updates:
Randomly pick k ∈ {1, 2, . . . , n}, with probability pk = 2n
+ 2 Pka
n
i=1 kai k2
n
o
(
(t) 2
pi n
(t)
∗
arg
max
βha
,
(β
−
y
)
i = k,
x
i
−
φ
(β)
−
β∈R
i
(t+1)
i
i
2σ
yi
=
(t)
yi
i 6= k,




1
kx − x(t) k22
(t+1)
(t)
x(t+1) = arg min g(x) + u(t) +
(yk
,
− yk )ak , x +
pk n
2τ
x∈Rd
1 (t+1)
(t)
u(t+1) = u(t) + (yk
− yk )ak ,
n
x(t+1) = x(t+1) + θ(x(t+1) − x(t) ).
end
Output: x(T ) and y (T )

perturbation on φ∗i is equivalent to smoothing φi , which becomes (1/δ)-smooth. Letting γ = λ = δ, the parameters
τ , σ and θ in (10) become
1
τ =
2R

r

m
1
,σ=
n
2R

r

r −1

n
n R n
, and θ = 1−
+
.
m
m δ m

Although (x?δ , yδ? ) is not exactly the saddle point of f , the
following corollary shows that applying the SPDC method
to the perturbed function fδ effectively minimizes the original loss function P . See the long version of this paper (Zhang & Xiao, 2014) for the proof.
Corollary 2. Assume that each φi is convex and Gφ Lipschitz continuous, and g is convex and Gg -Lipschitz
continuous. Define two constants:
C1 = (kx? k22 + G2φ ), and

C2 = (Gφ R + Gg )2 kx(0) − x?δ k22 +

1
2σ
1
2τ


+ δ ky (0) − yδ? k22
.
m
+δ

If we choose δ = /C1 , and run the SPDC algorithm for T
iterations where
r 



n
C1 R n
4C2
T ≥
+
log
,
m

m
2
then E[P (x(T ) ) − P (x? )] ≤ .
e
When m = 1, the corresponding batch complexity is O(1+
2 −1/2
( n)
). Under the same condition, the batch complexity of full accelerated gradient method and that of stochastic
e + (2 n)−1 ) regradient descent are O(1 + −1 ) and O(1
spectively (Nesterov, 2005; Shamir & Zhang, 2012), both
slower than the SPDC method.
There are two other cases that can be considered: when
φi is not smooth but g is strongly convex, and when φi is
smooth but g is not strongly convex. They can be handled
with the same technique described above, and we omit the

(12)
(13)

details here. In Table 1, we list the batch complexities of
the SPDC method for finding an -optimal solution of problem (1) under various assumptions.
3.2. SPDC with non-uniform sampling
One potential drawback of the SPDC algorithm is that,
its convergence rate depends on a problem-specific constant R, which is the largest `2 -norm of the feature vectors ai . As a consequence, the algorithm may perform
badly on unnormalized data, especially if the `2 -norms of
some feature vectors are substantially larger than others. In
this section, we propose an extension of the SPDC method
to mitigate this problem, which is given in Algorithm 2.
For simplicity of presentation, we described in Algorithm 2
with single dual coordinate update, i.e., the case of m = 1.
The basic idea is to use non-uniform sampling in picking
the dual coordinate to update at each iteration. In particular, instances with large feature norms should be sampled
more frequently. Simultaneously, we adopt an adaptive regularization in step (12), imposing stronger regularization
on such instances. In addition, we adjust the weight of ak
in (13) for updating the primal variable. As a consequence,
the convergence rate of Algorithm 2 depends on the aver-

φi

g

(1/γ)-smooth

λ-strongly convex

(1/γ)-smooth

non-strongly convex

non-smooth
non-smooth

λ-strongly convex
non-strongly convex

batch complexity
q
m
log(1/)
λγn
q
m
1 + γn
p m
1 + λn
p
1 + m
2n

1+

Table 1. Batch complexities of the SPDC method under different
assumptions on the functions φi and g. For the last three cases,
we solve the perturbed saddle-point problem with δ = /C1 .

Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization

age norm of feature vectors.
Theorem
2. Suppose Assumption A holds. Let R̄ =
Pn
1
ka
i k2 . If the parameters τ, σ, θ in Algorithm 2 are
i=1
n
chosen such that
1
τ =
4R̄

r

γ
,
nλ

1
σ=
4R̄

s


r −1
nλ
n
, θ = 1 − 2n + 2R̄
,
γ
λγ

then for each t ≥ 1, we have
 1
 
  7

2γ   (t)
+ λ E kx(t) − x? k22 +
+
E ky − y ? k22
2τ
16σ
n



 1

1
t
(0)
? 2
≤θ
+ λ kx − x k2 +
+ 2γ ky (0) − y ? k22 .
2τ
2σ

Comparing the constant θ in Theorem 2 to that of Theorem 1, the constant R̄ here is determined by the average
norm of features, instead of the largest one. It makes the algorithm more robust to unnormalized feature vectors. For
example, if the ai ’s are sampled i.i.d. from a multivariate
normal distribution, then maxi {kai k2 } almostP
surely goes
n
to infinity as n → ∞, but the average norm n1 i=1 kai k2
converges to E[kai k2 ].

4. Efficient Implementation with Sparse Data
During each iteration of the SPDC method, the updates
of primal variables (i.e., computing x(t+1) ) require full ddimensional vector operations; see the step (6) of Algorithm 1 and the step (13) of Algorithm 2. So the computational cost per iteration is O(d), and this can be too
expensive if the dimension d is very high. In this section,
we show how to exploit problem structure to avoid highdimensional vector operations when the feature vectors ai
are sparse. We illustrate the efficient implementation for
two popular cases: when g is an squared-`2 penalty and
when g is an `1 + `2 penalty. For both cases, we show
that the computation cost per iteration only depends on the
number of non-zero components of the feature vector.
4.1. Squared `2 -norm penalty
Suppose that g(x) = λ2 kxk22 . For this case, the updates
for each coordinate of x are independent of each other.
More specifically, x(t+1) can be computed coordinate-wise
in closed form:
1
(t)
(t)
(t+1)
xj
=
(x − τ uj − τ ∆uj ),
(14)
1 + λτ j
P
(t+1)
(t)
1
where ∆u denotes m
− yk )ak in Algok∈K (yk
(t+1)
(t)
rithm 1, or (yk
− yk )ak /(pk n) in Algorithm 2, and
∆uj represents the j-th coordinate of ∆u.
Although the dimension d can be very large, we assume
that each feature vector ak is sparse. We denote by J (t)
the set of non-zero coordinates at iteration t, that is, if for
some index k ∈ K picked at iteration t we have akj 6=

0, then j ∈ J (t) . If j ∈
/ J (t) , then the SPDC algorithm
(and its variants) updates y (t+1) without using the value
(t)
(t)
of xj or xj . This can be seen from the updates in (5)
and (12), where the value of the inner product hak , x(t) i
(t)
does not depend on the value of xj . As a consequence,
we can delay the updates on xj and xj whenever j ∈
/ J (t)
(t)
without affecting the updates on y , and process all the
missing updates at the next time when j ∈ J (t) .
Such a delayed update can be carried out very efficiently.
We assume that t0 is the last time when j ∈ J (t) , and t1
is the current iteration where we want to update xj and xj .
Since j ∈
/ J (t) implies ∆uj = 0, for t = t0 + 1, t0 +
2, . . . , t1 − 1 we have
xt+1
=
j
(t)

Notice that uj
J

(t)

1
(t)
(t)
(x − τ uj ).
1 + λτ j

(15)

is updated only at iterations where j ∈
(t)

. The value of uj

doesn’t change during iterations

(t)
uj

(t +1)

[t0 + 1, t1 ], so we have
≡ uj 0
for t ∈ [t0 + 1, t1 ].
Substituting this equation into the formula (15), we obtain
(t )

xj 1 =

(t +1) 


uj 0
1
(t0 +1)
x
+
(1 + λτ )t1 −t0 −1 j
λ

(t +1)

−

uj 0
λ

.

This update takes O(1) time to compute. Using the same
(t −1)
formula, we can compute xj 1
and subsequently com(t )

(t )

(t )

(t −1)

pute xj 1 = xj 1 + θ(xj 1 − xj 1 ). Thus, the computational complexity of a single iteration in SPDC is proportional to |J (t) |, independent of the dimension d.
4.2. (`1 + `2 )-norm penalty
Suppose that g(x) = λ1 kxk1 + λ22 kxk22 . Since both the `1 norm and the squared `2 -norm are decomposable, the updates for each coordinate of x(t+1) are independent. More
specifically,
n
λ2 α 2
(t+1)
xj
= arg min λ1 |α| +
α∈R
2
(t)
(α − xj )2 o
(t)
+ (uj + ∆uj )α +
,
(16)
2τ
where ∆uj follows the definition in Section 4.1. If j ∈
/
J (t) , then ∆uj = 0 and equation (16) can be simplified as
(t+1)

xj






=

(t)
1
1+λ2 τ (xj
(t)
1
1+λ2 τ (xj

0

(t)

(t)

(t)

− τ uj − τ λ1 ) if xj − τ uj > τ λ1 ,
(t)
(t)
(t)
− τ uj + τ λ1 ) if xj − τ uj < −τ λ1 ,
otherwise.

Similar to the approach of Section 4.1, we delay the update
of xj until j ∈ J (t) . We assume t0 to be the last iteration when j ∈ J (t) , and let t1 be the current iteration when

Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization
λ

RCV1

Covtype

0

News20

0

0
−2

−15

AFG
L−BFGS
SPDC
5
10
15
20
Number of Passes

−10
−15
−20

25

Log Loss

−10

Log Loss

10−4

Log Loss

−5
−5

0

AFG
L−BFGS
SPDC
5
10
15
20
Number of Passes

−10

25

0

25

0

AFG
L−BFGS
SPDC
20
40
60
80
Number of Passes

100

−5

−10

−15

0

Log Loss

Log Loss

Log Loss

−6

−10

AFG
L−BFGS
SPDC
5
10
15
20
Number of Passes

−2

−4

−8

−6
−8

−2

10−6

−4

AFG
L−BFGS
SPDC
20
40
60
80
Number of Passes

−4
−6
−8

100

−10

0

AFG
L−BFGS
SPDC
20
40
60
80
Number of Passes

100

0

−2
−2

−4
−6
−8

AFG
L−BFGS
SPDC
100 200 300 400 500 600
Number of Passes

−4

Log Loss

Log Loss

10−8

Log Loss

−2
−6
−8
−10
−12

AFG
L−BFGS
SPDC
100 200 300 400 500 600
Number of Passes

−4
−6
−8

AFG
L−BFGS
SPDC
100 200 300 400 500 600
Number of Passes

Figure 1. Comparing SPDC with AFG and L-BFGS on three real datasets. The horizontal axis is the number of passes through the entire
dataset, and the vertical axis is the logarithmic optimality gap log(P (x(t) ) − P (x? )).

we want to update xj . During iterations [t0 + 1, t1 ], the
(t)
(t)
(t +1)
value of uj doesn’t change, so we have uj ≡ uj 0
for t ∈ [t0 + 1, t1 ]. Using the above equation and the in(t)
variance of uj for t ∈ [t0 + 1, t1 ], we have an O(1) time
(t )

algorithm to calculate xj 1 . See Zhang & Xiao (2014, Ap(t )

pendix C) for the algorithm details. The vector xj 1 can be
updated by the same algorithm since it is a linear combina(t )
(t −1)
tion of xj 1 and xj 1 . As a consequence, the computational complexity of each iteration in SPDC is proportional
to |J (t) |, independent of the dimension d.

5. Experiments
In this section, we compare the SPDC method (Algorithm 1
with m = 1) with several state-of-the-art optimization algorithms for solving problem (1). They include two batchupdate algorithms: the accelerated full gradient (FAG)
method (Nesterov, 2004), and the limited-memory quasiNewton method L-BFGS (e.g., Nocedal & Wright, 2006).
For the AFG method, we adopt an adaptive line search
scheme (Nesterov, 2013) to improve its efficiency. For the

L-BFGS method, we use the memory size 30 as suggested
by Nocedal & Wright (2006). We also compare SPDC with
three stochastic algorithms: the stochastic average gradient (SAG) method (Roux et al., 2012), the stochastic dual
coordinate descent (SDCA) method (Shalev-Shwartz &
Zhang, 2013a) and the accelerated stochastic dual coordinate descent (ASDCA) method (Shalev-Shwartz & Zhang,
2013b).
The datasets are obtained from LIBSVM data (Fan & Lin,
2011) and summarized in Table 2. The three datasets are
selected to reflect different relations between the sample
size n and the feature dimensionality d, which cover n  d
(Covtype), n ≈ d (RCV1) and n  d (News20). For
all tasks, the data points take the form of (ai , bi ), where
Dataset name
Covtype
RCV1
News20

# samples n
581,012
20,242
19,996

# features d
54
47,236
1,355,191

Table 2. Characteristics of three real datasets.

Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization
λ

RCV1

Covtype

0

News20

0

0

−15

SAG
SDCA
SPDC
5
10
15
20
Number of Passes

−10
−15
−20

25

0

Log Loss

−10

Log Loss

10

−4

Log Loss

−5
−5

SAG
SDCA
SPDC
5
10
15
20
Number of Passes

−5

−10

−15

25

0

SAG
SDCA
SPDC
5
10
15
20
Number of Passes

25

0

-5
SAG
SDCA
ASDCA
SPDC

-10

20
40
60
80
Number of Passes

-10
-15

100

Log Loss

Log Loss

10−6

Log Loss

-5

-20

SAG
SDCA
ASDCA
SPDC

20
40
60
80
Number of Passes

100

-5
SAG
SDCA
ASDCA
SPDC

-10

0

0

SAG
SDCA
ASDCA
SPDC

-8

-5

-10

200
400
Number of Passes

600

Log Loss

Log Loss

Log Loss

-2

-4
-6

100

0

-2

10−8

20
40
60
80
Number of Passes

SAG
SDCA
ASDCA
SPDC

-4
-6

200
400
Number of Passes

600

-8

SAG
SDCA
ASDCA
SPDC

200
400
Number of Passes

600

Figure 2. Comparing SPDC with SAG, SDCA and ASDCA on three real datasets. The horizontal axis is the number of passes through
the entire dataset, and the vertical axis is the logarithmic optimality gap log(P (x(T ) ) − P (x? )).

ai ∈ Rd is the feature vector, and bi ∈ {−1, 1} is the
binary class label. Our goal is to minimize the regularized
empirical risk:
n

P (x) =

1X
λ
φi (aTi x) + kxk22 .
n i=1
2

Here, φi is the smoothed hinge loss (see e.g., ShalevShwartz & Zhang, 2013a):


if bi z ≥ 1
 0
1
if bi z ≤ 0
φi (z) =
2 − bi z

 1 (1 − b z)2 otherwise.
i
2
It is easy to verify that the conjugate function of φi is
φ∗i (β) = bi β + 12 β 2 for bi β ∈ [−1, 0] and +∞ otherwise.
The performance of the five algorithms are plotted in Figure 1 and Figure 2. In Figure 1, we compare SPDC with the
two batch methods: AFG and L-BFGS. The results show
that SPDC is substantially faster than AFG and L-BFGS
for relatively large λ, illustrating the advantage of stochastic methods over batch methods on well-conditioned problems. As λ dropping to 10−8 , the batch methods (especially

L-BFGS) become comparable to SPDC.
In Figure 2, we compare SPDC with the three stochastic
methods: SAG, SDCA and ASDCA. The ASDCA specification (Shalev-Shwartz & Zhang, 2013b) requires the regR2
ularization coefficient λ satisfies λ ≤ 10n
where R is the
maximum `2 -norm of feature vectors. To satisfy this constraint, we run ASDCA with λ ∈ {10−6 , 10−8 }. Here,
the observations are just the opposite to that of Figure 1.
All stochastic algorithms have comparable performance
on relatively large λ, but the two accelerated algorithms
SPDC and ASDCA becomes substantially faster when λ
gets closer to zero.
Among these the two accelerated algorithms, ASDCA converges faster than SPDC on the Covtype dataset (which has
a very small feature dimension) and slower on the remaining two datasets. In addition, due to the outer-inner loop
structure of the ASDCA algorithm, its objective gap my
increase at the beginning of each outer iteration. This oscillation can be undesirable, especially at early iterations.
In contrast, the convergence of SPDC is almost linear and
more stable than ASDCA.

Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization

References
Bertsekas, D. P. Incremental proximal methods for large
scale convex optimization. Mathematical Programming,
Ser. B, 129:163–195, 2011.
Blatt, D., Hero, A. O., and Gauchman, H. A convergent
incremental gradient method with a constant step size.
SIAM Journal on Optimization, 18(1):29–51, 2007.
Bottou, L. Large-scale machine learning with stochastic
gradient descent. In Lechevallier, Y. and Saporta, G.
(eds.), Proceedings of the 19th International Conference
on Computational Statistics, pp. 177–187, Paris, France,
August 2010. Springer.
Bousquet, O. and Elisseeff, A. Stability and generalization. Journal of Machine Learning Research, 2:499–526,
2002.
Chambolle, A. and Pock, T. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of Mathematical Imaging and Vision, 40(1):
120–145, 2011.
Duchi, J. and Singer, Y. Efficient online and batch learning
using forward backward splitting. Journal of Machine
Learning Research, 10:2873–2898, 2009.
Fan, R.-E. and Lin, C.-J.
LIBSVM data: Classification, regression and multi-label.
URL:
http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets,
2011.
Hastie, T., Tibshirani, R., and Friedman, J. The Elements of
Statistical Learning: Data Mining, Inference, and Prediction. Springer, New York, 2nd edition, 2009.
Hiriart-Urruty, J.-B. and Lemaréchal, C. Fundamentals of
Convex Analysis. Springer, 2001.
Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S., and
Sundararajan, S. A dual coordinate descent method for
large-scale linear svm. In Proceedings of the 25th International Conference on Machine Learning (ICML), pp.
408–415, 2008.
Langford, J., Li, L., and Zhang, T. Sparse online learning via truncated gradient. Journal of Machine Learning
Research, 10:777–801, 2009.
Lin, Q., Lu, Z., and Xiao, L. An accelerated proximal
coordinate gradient method and its application to regularized empirical risk minimization. Technical Report
MSR-TR-2014-94, Microsoft Research, 2014.
Nedić, A. and Bertsekas, D. P. Incremental subgradient
methods for nondifferentiable optimization. SIAM Journal on Optimization, 12(1):109–138, 2001.

Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
Robust stochastic approximation approach to stochastic
programming. SIAM Journal on Optimization, 19(4):
1574–1609, 2009.
Nesterov, Y. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer, Boston, 2004.
Nesterov, Y. Smooth minimization of nonsmooth functions. Mathematical Programming, 103:127–152, 2005.
Nesterov, Y. Gradient methods for minimizing composite
functions. Mathematical Programming, Ser. B, 140:125–
161, 2013.
Nocedal, J. and Wright, S. J. Numerical Optimization.
Springer, New York, 2nd edition, 2006.
Polyak, B. T. and Juditsky, A. Acceleration of stochastic
approximation by averaging. SIAM Journal on Control
and Optimization, 30:838–855, 1992.
Roux, N. L., Schmidt, M., and Bach, F. A stochastic gradient method with an exponential convergence rate for
finite training sets. In Advances in Neural Information
Processing Systems 25, pp. 2672–2680. 2012.
Shalev-Shwartz, S. and Zhang, T. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14:567–
599, 2013a.
Shalev-Shwartz, S. and Zhang, T. Accelerated proximal
stochastic dual coordinate ascent for regularized loss
minimization. arXiv:1309.2375, 2013b.
Shamir, O. and Zhang, T. Stochastic gradient descent for
non-smooth optimization: Convergence results and optimal averaging schemes. arXiv:1212.1824, 2012.
Tseng, P. An incremental gradient(-projection) method
with momentum term and adaptive stepsiz rule. SIAM
Journal on Optimization, 8(2):506–531, 1998.
Xiao, L. Dual averaging methods for regularized stochastic
learning and online optimization. Journal of Machine
Learning Research, 11:2534–2596, 2010.
Zhang, T. Solving large scale linear prediction problems
using stochastic gradient descent algorithms. In Proceedings of the 21st International Conference on Machine Learning (ICML), pp. 116–123, Banff, Alberta,
Canada, 2004.
Zhang, Y. and Xiao, L. Stochastic primal-dual coordinate method for regularized empirical risk minimization.
arXiv preprint arXiv:1409.3257, 2014.

