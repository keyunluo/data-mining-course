Stochastic Optimization with Importance Sampling for Regularized Loss
Minimization
Peilin Zhao†,‡
ZHAOP @ I 2 R . A - STAR . EDU . SG
Tong Zhang‡
TZHANG @ STAT. RUTGERS . EDU
†
Data Analytics Department, Institute for Infocomm Research, A*STAR, Singapore
‡
Department of Statistics & Biostatistics, Rutgers University, USA; and Big Data Lab, Baidu Research, China

Abstract
Uniform sampling of training data has been commonly used in traditional stochastic optimization
algorithms such as Proximal Stochastic Mirror
Descent (prox-SMD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although
uniform sampling can guarantee that the sampled
stochastic quantity is an unbiased estimate of the
corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying
optimization procedure. In this paper we study stochastic optimization, including prox-SMD and
prox-SDCA, with importance sampling, which
improves the convergence rate by reducing the
stochastic variance. We theoretically analyze the
algorithms and empirically validate their effectiveness.

1. Introduction
Stochastic optimization has been extensively studied in the
machine learning community (Zhang, 2004; Rakhlin et al.,
2011;
Shamir & Zhang,
2013;
Duchi & Singer,
2009; Luo & Tseng, 1992; Mangasarian & Musicant,
1999; Hsieh et al., 2008; Shalev-Shwartz & Tewari,
2011; Lacoste-Julien et al., 2012; Nesterov, 2012b;
Shalev-Shwartz & Zhang, 2012a; 2013; 2012b). At every
step, a traditional stochastic optimization method will
sample one training example or one dual coordinate uniformly at random from the training data, and then update
the model parameter using the sampled example or dual
coordinate. In this paper we focus on Proximal Stochastic
Mirror Descent (prox-SMD) (Duchi & Singer, 2009;
Duchi et al., 2010) and Proximal Stochastic Dual CoorProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

dinate Ascent (prox-SDCA) (Shalev-Shwartz & Zhang,
2012b) methods.
For prox-SMD, the traditional algorithms such as Stochastic Gradient Descent (SGD) sample training examples uniformly at random during the entire learning process,
so that the stochastic gradient is an unbiased estimation
of the true gradient (Zhang, 2004; Rakhlin et al., 2011;
Shamir & Zhang, 2013; Duchi & Singer, 2009). However,
the variance of the resulting stochastic gradient estimator
may be large since the stochastic gradient can vary significantly over different examples. In order to improve convergence, this paper proposes a sampling distribution and
the corresponding unbiased importance weighted gradient estimator that minimizes the variance. To this end, we
analyze the relationship between the variance of stochastic gradient and the sampling distribution. We show that
to minimize the variance, the optimal sampling distribution
should be roughly proportional to the norm of the stochastic gradient. To simplify computation, we also consider the
use of upper bounds for the norms. Our theoretical analysis
shows that under certain conditions, the proposed sampling
method can significantly improve the convergence rate, and
our results include the existing theoretical results for uniformly sampled prox-SGD and SGD as special cases.
Similarly for prox-SDCA, the traditional approach
such as Stochastic Dual Coordinate Ascent (SDCA) (Shalev-Shwartz & Zhang, 2013) picks a coordinate
to update by sampling the training data uniformly at
random (Luo & Tseng, 1992; Mangasarian & Musicant,
1999; Hsieh et al., 2008; Shalev-Shwartz & Tewari,
2011; Lacoste-Julien et al., 2012; Nesterov, 2012b;
Shalev-Shwartz & Zhang, 2012a; 2013; 2012b). It was
shown recently that the SDCA and prox-SDCA algorithms
with uniform random sampling converge much faster than
a fixed cyclic ordering (Shalev-Shwartz & Zhang, 2013;
2012b). However, this paper shows that if we employ
an appropriately defined importance sampling strategy,
the convergence can be further improved. To optimize
sampling distribution, we analyze the connection between

Stochastic Optimization with Importance Sampling for Regularized Loss Minimization

the expected increase of dual objective and the sampling
distribution, and obtain the optimal solution that depends
on the smoothness or Lipschitz constants of the loss
functions. Our analysis shows that under certain conditions, the proposed sampling method can significantly
improve the convergence rate. In addition, our theoretical
results include the existing results for uniformly sampled
prox-SDCA and SDCA as special cases.
The rest of this paper is organized as follows. Section 2 reviews the related work. In section 3, we will study stochastic optimization with importance sampling. Section 4 gives
our empirical evaluations. Section 5 concludes the paper.
The detailed proofs of the theoretical results can be found
in the full version of the paper (Zhao & Zhang, 2014).

2. Related Work
After finishing the work, we noticed that Needell et al.
(2014) also considered importance sampling for stochastic gradient descent, where they suggested ideas similar to
ours. Moreover Strohmer & Vershynin (2009) proposed a
variant of the Kaczmarz method (an iterative method for
solving systems of linear equations) which selects rows
with probability proportional to their squared norms. It was
pointed out in (Needell et al., 2014) that this algorithm is
actually a SGD algorithm with importance sampling. Our
paper studies importance sampling for more general composite objectives and the more general proximal stochastic
mirror descent method, covering their algorithms as special cases. Our paper also studies prox-SDCA with importance sampling, which is not covered by previous studies.
Another related work is (Xiao & Zhang, 2014), where the
authors studied importance sampling for the prox-SVRG
procedure, and obtained results similar to those of proxSDCA considered in this work. The main concern of this
work is on the effectiveness of importance sampling, which
could be applied to many gradient based algorithms. Therefore we include the study of the standard SGD procedure
for comparison, although for smooth and strongly convex
objective functions it does not achieve the linear rates of
SVRG, SDCA, and SAG (Roux et al., 2012).
For the primal coordinate descent procedures, some researchers have recently considered non-uniform sampling
strategies (Nesterov, 2012a; Lee & Sidford, 2013). However their results cannot be directly applied to obtain
duality-gap convergence for proximal SDCA which we
are interested in here. In contrast, the primal-dual analysis of prox-SDCA in this paper is analogous to that of
(Shalev-Shwartz & Zhang, 2013), which directly bounds
the duality gap. The proof technique relies on the structure of the regularized loss minimization, which differs from the traditional primal coordinate descent analysis.
The suggested distribution for the primal coordinate de-

scent is propositional to the smoothness constant of every
coordinate, while the distribution of prox-SDCA is propositional to a constant plus the smoothness constant of the
primal individual loss function. These two distributions are
quite different. In addition, we also provide an importance
sampling distribution when the individual loss functions
are Lipschitz. Finally we note that an accelerated version
of prox-SDCA was proposed by Shalev-Shwartz & Zhang
(2014). The procedure employs an inner-outer-iteration
strategy, where the inner iteration is the standard proxSDCA procedure. The importance sampling result of this
paper can be directly applied to the accelerated prox-SDCA
in that the convergence of inner iteration becomes faster
than that of the uniform sampling. Therefore in this paper
we only consider the unaccelerated prox-SDCA.

3. Stochastic Optimization with Importance
Sampling
Let ϕ1 , ϕ2 , . . . , ϕn be n vector functions from Rd to R. Our
goal is to find an approximate solution of the following optimization problem
min P (w) := f (w) + λr(w),

w∈Rd

(1)

∑n
where f (w) = n1 i=1 ϕi (w), λ > 0 is a regularization parameter, and r is a regularizer. For example, given examples (xi , yi ) where xi ∈ Rd and yi ∈ {−1, +1},
the Support Vector Machine problem is obtained by setting
ϕi (w) = [1 − yi x⊤
i w]+ , [z]+ = max(0, z), and r(w) =
1
2
.
Regression
problems also fall into the above. For
∥w∥
2
2
2
example, lasso is obtained by setting ϕi (w) = (yi −x⊤
i w)
and r(w) = ∥w∥1 .
Let w∗ be the optimal solution of (1). We say that a solution w is ϵP -sub-optimal if P (w) − P (w∗ ) ≤ ϵP . We
analyze the convergence rates of the proposed algorithms
with respect to the number of iterations.
3.1. prox-SMD with Importance Sampling
In this subsection, we consider the proximal stochastic mirror descent method with importance sampling. Proximal
Stochastic Mirror Descent works in iterations. At each iteration t = 1, 2, . . ., a sample it will be uniformly drawn
from {1, 2, . . . , n}, and the iterative solution will be updated by setting wt+1 as
[
]
1
arg min ⟨∇ϕit (wt ), w⟩ + λr(w) + Bψ (w, wt ) , (2)
w
ηt
where Bψ is a Bregman divergence and ∇ϕit (wt ) denotes
an arbitrary (sub-)gradient of ϕit . Intuitively, this method
works by minimizing a first-order approximation of the
function ϕit at the current iterate wt plus the regularizer

Stochastic Optimization with Importance Sampling for Regularized Loss Minimization

λr(w), and forcing the next iterate wt+1 to lie close to wt .
The step size ηt is a trade-off between these two objectives.
We assume that the exact solution of the above optimization (2) can be efficiently obtained. For example, when
ψ(w) = 12 ∥w∥22 , we have Bψ (u, v) = 12 ∥u − v∥22 , and
the above optimization will produce the t + 1-th iterate as:
wt+1 = proxηt λr (wt − ηt ∇ϕit (wt )), where proxh (x) =
(
)
arg minw h(w) + 12 ∥w − x∥22 . Furthermore, it is also assumed that the proximal mapping of ηt λr(w), i.e.,
proxηt λr (x), is easy to compute. For example, when
r(w) = ∥w∥1 , the proximal mapping of λr(w) is the
shrinkage operation proxλr (x) = sign(x) ⊙ [|x| − λ]+ ,
where ⊙ is the element-wise vector product.
A disadvantage of this method is that the randomness introduces variance - this is caused by the fact that ∇ϕit (wt )
equals the gradient ∇f (wt ) in expectation, but ∇ϕi (wt )
varies with i. In particular, if the stochastic gradient has
a large variance, then the convergence will become slow.
This paper studies prox-SMD with importance sampling to
reduce the variance of stochastic gradient. The idea of importance sampling can be described as follows: at the t-th
step, we assign
i ∈ {1, . . . , n} a probability pti ≥ 0
∑n each
t
such that i=1 pi = 1; we then sample it from {1, . . . , n}
based on the probability pt = (pt1 , . . . , ptn )⊤ . If we adopt
this distribution, then proximal SMD with importance sampling is obtained by setting wt+1 as the solution of
[ ∇ϕ (wt )
]
1
it
t
,
w⟩
+
λr(w)
+
min ⟨
B
(w,
w
)
, (3)
ψ
w
nptit
ηt

have. In the next subsection, we will study how to adopt
importance sampling to reduce the variance. This observation will be made more rigorous below.
3.1.1. A LGORITHM
According to Lemma 1 , in order to maximize the reduction
on the objective value, we should choose pt as the solution
of the following optimization
min
V(
t
n

p ∈△

n
∇ϕit (wt )
1 ∑ ∥∇ϕi (wt )∥2∗
, (4)
)
⇔
min
pt ∈△n n2
nptit
pti
i=1

where △n is the n-dimensional simplex. It is easy to verify,
that the solution of the above optimization is
∥∇ϕi (wt )∥∗
pti = ∑n
,
t
j=1 ∥∇ϕj (w )∥∗

∀i ∈ {1, 2, . . . , n}.

(5)

Although, this distribution can minimize the variance of
the t-th stochastic gradient, it requires the calculation of
n derivatives at each step, which is clearly inefficient. To
solve this issue, a potential remedy is to calculate the n
derivatives at some steps and then keep it for use for a relatively long time period. In addition, the true derivatives
will changes every step, and thus it is beneficial to add a
small constant to the sampling probability. Another practical solution is to relax the previous optimization (4) as
follows
n
n
1 ∑ G2i
1 ∑ ∥∇ϕi (wt )∥2∗
≤
min
min
(6)
pt ∈△n n2
pt ∈△n n2
pti
pti
i=1
i=1

which is another unbiased estimation of the optimization
problem for prox-MD (or composite objective mirror descent), because E[(nptit )−1 ∇ϕit (wt )|wt ] = ∇f (wt ).

by introducing upperbounds

The main question is: what choice of pt can optimally reduce the variance of the stochastic gradient. To answer this
question, we first prove a lemma that establishes a relationship between pt and the convergence rate of prox-SMD
with importance sampling.
Lemma 1. Define wt+1 by the update (3). Assume that
ψ(·) is σ-strongly convex with respect to a norm ∥ · ∥ (its
dual norm is ∥ · ∥∗ ), and f is µ-strongly convex and (1/γ)smooth with respect to ψ. If r(w) is convex and ηt ∈ (0, γ],
then wt+1 satisfies the following inequality for any t ≥ 1,
1
E[P (wt+1 ) − P (w∗ )] ≤ E[Bψ (w∗ , wt )−Bψ (w∗ , wt+1 )]
ηt
(
)
ηt
∗
t
−µEBψ (w , w ) + EV (nptit )−1 ∇ϕit (wt ) ,
σ
where the variance is defined as V((nptit )−1 ∇ϕit (wt )) =
E∥(nptit )−1 ∇ϕit (wt ) − ∇f (wt )∥2∗ , and the expectation is
taken with the distribution pt .

Using this approach, we can approximate the distribution
in (5) by solving the the right hand side of (6) as

From the above analysis, we can observe that the smaller
the variance, the more reduction on objective function we

Gi ≥ ∥∇ϕi (wt )∥∗ ,

Gi
pti = ∑n
j=1

Gj

,

∀t.

∀i ∈ {1, 2, . . . , n},

which is independent of t.
Based on the above solution, we will suggest distributions
for two kinds of loss functions - Lipschitz functions and
smooth functions. First, if each ϕi (w) is Li -Lipschitz w.r.t.
∥ · ∥∗ , then ∥∇ϕi (w)∥∗ ≤ Li for any w ∈ Rd , and the
suggested distribution is
Li
pti = ∑n
j=1

Lj

,

∀i ∈ {1, 2, . . . , n}.

Second, if ϕi (w) is (1/γi )-smooth and ∥wt ∥ ≤ R for any
t (this is possible when the feasible domain is bounded),
then ∥∇ϕi (wt )∥∗ ≤ R/γi , and the distribution becomes
1
γi

pti = ∑n

1
j=1 γj

,

∀i ∈ {1, 2, . . . , n}.

Stochastic Optimization with Importance Sampling for Regularized Loss Minimization

Finally, we can summarize the proposed Proximal SMD
with importance sampling in Algorithm 1.
Algorithm 1 Proximal Stochastic Mirror Descent with Importance Sampling (Iprox-SMD)
Input: λ ≥ 0, the learning rates η1 , . . . , ηT > 0.
i
Initialize: w1 = 0, pi = ∑LiLj or pi = ∑1/γ
, ∀i.
j
j 1/γj
for t = 1, . . . , T do
Sample it from {1, . . . , n} based on p;
[⟨
⟩
wt+1 = arg min (npit )−1 ∇ϕit (wt ), w + λr(w)
w
]
1
+ Bψ (w, wt ) ;
ηt
end for

3.1.2. A NALYSIS
Before presenting the results, we make some general assumptions: r(0) = 0, and r(w) ≥ 0, for all w. It is
easy to see that these two assumptions are generally satisfied by most of the well-known regularizers.
Under the above assumptions, we first prove a convergence
result for Proximal SMD with importance sampling using
the previous Lemma 1.
Theorem 1. Assume that ψ(·) is σ-strongly convex with
respect to a norm ∥ · ∥, f is µ-strongly convex and (1/γ)1
smooth with respect to ψ, r(w) is convex and ηt = α+µt
with α ≥ 1/γ − µ. If we further assume ϕi (w) is (1/γi )smooth, ∥wt ∥ ≤ R for any t, and the distribution is set
i
as pti = ∑nR/γR/γ
, then the following inequality holds for
j
j=1
any T ≥ 1,
[ ∑n
]
T
( i=1 R/γi )2 ln(α + µT )
1 ∑
EP (wt+1 ) − P (w∗ ) ≤ O
.
T t=1
σµn2
T

∑n

(R/γi )2

ed by i=1 n
per( bound for

. Hence Theorem 1 results in an up∑T
t+1
∗
t=1 EP
) (w ) − P (w ) of the form
∑n
2
(R/γ
)
ln(α+µT
)
i
i=1
O
for strongly convex f , and of
σµn
T
√
∑n
2
∗
1
Bψ (w ,w )
i=1 (R/γi ) √1
for general convex
the form 2
σ
n
T
f . According to the Cauchy-Schwarz inequality,
∑n

2
i=1 (R/γi )

n

1
T

( ∑n
/

R/γi
n

)2

i=1

∑
2
n n
i=1 (R/γi )
∑
=
≥ 1.
n
2
( i=1 R/γi )

It implies that importance sampling
always improves the
∑
( n R/γ )2
convergence rate, especially when ∑ni=1(R/γii)2 ≪ n.
i=1

If f is convex, we can provide the following convergence
results using the analysis of (Duchi et al., 2010).
Theorem 2. Assume that ψ(·) is σ-strongly convex with
respect to a norm ∥ · ∥, f and r(w) are convex, and ηt = η.
If we further assume ϕi (w) is (1/γi )-smooth, ∥wt ∥ ≤ R
i
for any t, and the distribution is set as pti = ∑nR/γR/γ
,
j
j=1
∑
√
√
n
R/γ
then when ηt is set as 2σBψ (w∗ , w1 )/( i=1n i T ),
the following inequality holds for any T ≥ 1,
T
1 ∑
EP (wt ) − P (w∗ ) ≤
T t=1

√

2
Bψ (w∗ , w1 ) (
σ

∑n

R/γi 1
)√ .
n
T

i=1

If ϕi (w) is L∑
i -Lipschitz, and the distribution is set
n
as pi = Li / j=1 Lj , ∀i, then when ηt is set as
∑n
√
Li √
2σBψ (w∗ , w1 )/( i=1
T ), the following inequality
n
holds for any T ≥ 1,
T
1 ∑
EP (wt ) − P (w∗ ) ≤
T t=1

√

2
Bψ (w∗ , w1 ) (
σ

∑n
i=1

n

Li

1
)√ .
T

Remark: If the uniform distribution is adopted, it is
easy to observe
that the variance of stochastic gradient is
∑n
(R/γ )2
bounded by i=1 n i for smooth ϕi (·), and bounded
∑n

(L )2

by i=1n i for Lipschitz ϕi (·). Theorem 2 results in an
∑T
upper bound for T1 t=1 EP (wt ) − P (w∗ ) of the form
In addition, if µ = 0, the above bound∑is invalid, howev√
∑
√
√
n
2
2Bψ (w∗ ,w1 ) n
R/γ
i=1 (R/γi )
for smooth ϕi , and of the forer if ηt is set as σBψ (w∗ , w1 )/( T i=1n i ), we can
σnT
√
∑n
prove the following inequality for any T ≥ 1,
2
∗
1
2Bψ (w ,w ) i=1 (Li )
m
for Lipschitz ϕi . However, acσnT
√
∑n
T
∑
∗
1
cording
to
the
Cauchy-Schwarz
inequality,
R/γ
Bψ (w , w ) i=1
1
i 1
√ .
EP (wt+1 ) − P (w∗ ) ≤ 2
∑
∑n
n
T t=1
σ
n
T
n i=1 (R/γi )2
n i=1 L2i
∑n
∑
≥
1,
≥ 1,
n
( i=1 R/γi )2
( i=1 Li )2
Remark: If ψ(w) = 21 ∥w∥22 and r(w) = 0, then
implies that importance sampling
improves the conver∑
Bψ (u, v) = 21 ∥u − v∥22 , and the proposed algorithm be( n
R/γi )2
∑ni=1
gence
bound,
especially
when
≪ n, and
2
comes SGD with importance sampling. Under these as(R/γ
i)
i=1
∑n
2
(
L )
sumptions, it is known that one may get rid of the ln T facwhen ∑ni=1(Lii)2 ≪ n.
i=1
tor in the convergence bound, when the objective function
is strongly convex. For simplicity, we do not provide the
3.2. prox-SDCA with Importance Sampling
details.
In this section, we study the Proximal Stochastic Dual
Remark. If the uniform distribution is adopted, it is easy
to observe that the variance of stochastic gradient is boundCoordinate Ascent method (prox-SDCA) with importance

Stochastic Optimization with Importance Sampling for Regularized Loss Minimization

sampling. Prox-SDCA deals with the dual problem of (1):
max D(θ) :=
θ

1
n

n
∑

−ϕ∗i (−θi ) − λr∗ (

i=1

1
λn

n
∑

θi ).

(7)

i=1

We assume that r∗ (·) is continuously differentiable; the relationship between the primal variable w∑and dual variable
n
1
θ is w = ∇r∗ (v(θ)) , v(θ) = λn
i=1 θi . We also
assume that r(w) is 1-strongly convex with respect to a
norm ∥ · ∥P ′ , i.e., r(w + ∆w) ≥ r(w) + ∇r(w)⊤ ∆w +
1
2
∗
2 ∥∆w∥P ′ , which means that r (w) is 1-smooth with respect to its dual norm ∥ · ∥D′ . Namely, r∗ (v + ∆v) ≤
h(v; ∆v), where h(v; ∆v) := r∗ (v) + ∇r∗ (v)⊤ ∆v +
1
2
2 ∥∆v∥D ′ .
At the t-th step, the Proximal Stochastic Dual Coordinate
Ascent method (prox-SDCA) picks i ∈ {1, . . . , n} uniformly at random, and update the dual variable θit−1 as:
θit

=

θit−1

+

For many interesting cases, it is easy to estimate R =
supu̸=0 ∥u∥D′ /∥u∥D . For example, if p > r > 0, then
∥w∥p ≤ ∥w∥r ≤ d(1/r−1/p) ∥w∥p for any w ∈ Rd .

∆θit−1 ,

3.2.1. A LGORITHM
According to Lemma 2, to maximize the dual ascent for the
t-th update, we should choose s and p as the solution of the
following optimization
max

s/(pi n)∈[0,1],p∈△n

where △n is the n-dimensional simplex. However, because
this optimization problem is difficult to solve, we choose to
relax it as follows:
max

s
n
pi n ∈[0,1],p∈△

≥

where ∆θit−1 is the solution of

[
]
1
max −ϕ∗i (−(θit−1 + ∆θi ))−(wt−1)⊤∆θi −
∥∆θi ∥2D′ ,(8)
∆θi
2λn

s
s Gt
E[P (wt−1 ) − D(θt−1 )] − 2
.
n
n 2λ

≥

s
s Gt
E[P (wt−1 ) − D(θt−1 )] − 2
n
n 2λ

],p∈△n

s
s Gt
E[P (wt−1 ) − D(θt−1 )] − 2
n
n 2λ

],p∈△n

s
E[P (wt−1 ) − D(θt−1 )].
n

max

λnγi
s
pi n ∈[0, R2 +λnγ
i

max

λnγi
s
pi n ∈[0, R2 +λnγ
i

which is equivalent to maximizing∑
a lower bound of the
n
t−1
1
following problem with vt−1 = λn
i=1 θi
[
]
1 ∗
1
t−1
∗
t−1
max − ϕi (−(θi + ∆θi )) − λr (v
∆θi ) .
+
∆θi
n
λn

∑n
where the last inequality has used Gt = n1 i=1 (si R2 −
γi (1−si )λn)E∥ut−1
−θit−1 ∥2D ≤ 0, since si = s/(pi n) ≤
i
λnγi
R2 +λnγi . To optimize the final relaxation, we have the following proposition

However, the optimization (8) may not have a closed form
solution, and in prox-SDCA we may adopt other update
rules ∆θi = s(u − θit−1 ) for an appropriately chosen step
size parameter s > 0 and any vector u ∈ Rd such that
−u ∈ ∂ϕi (wt−1 ). Note that when r(w) = 12 ∥w∥22 , proxSDCA is also known as SDCA.

Proposition 1. The solution to the optimization problem
s.t. s/(pi n) ∈ [0,

max s
s,p

R2

λnγi
], p ∈ △n
+ λnγi

is given by
2

In the following, we study prox-SDCA with importance
sampling, which is to allow the algorithm to randomly pick
i according
∑to probability pi , which is the i-th element of
p ∈ Rn+ , i pi = 1. Once we pick the coordinate i, θi is
updated as traditional prox-SDCA. The main question we
are interested in here is which p = (p1 , . . . , pn )⊤ can optimally accelerate the convergence rate of prox-SDCA. To
answer this question, we will introduce a lemma which will
state the relationship between p and the convergence rate
of prox-SDCA with importance sampling.
Lemma 2. Given a distribution p, if assume ϕi is (1/γi )smooth with norm ∥ · ∥P , then for any iteration t and any s
such that si = s/(pi n) ∈ [0, 1], ∀i, we have
E[D(θt )−D(θt−1 )] ≥

s
sGt
E[P (wt−1 )−D(θt−1 )]−
,
n
2λn2

(9)

∑n
where Gt = n1 i=1 (si R2 − γi (1 − si )λn)E∥uit−1 −
θit−1 ∥2D , R = supu̸=0 ∥u∥D′ /∥u∥D , and −uit−1 ∈
∂ϕi (wt−1 ).

s=

n+

n
∑n
i=1

R
1 + λnγ
, pi =
∑n i R 2 .
R2
n + j=1 λnγj
λnγi

(10)

We omit the proof since it is simple. Given that ϕi is (1/γi )smooth, ∀i ∈ {1, . . . , n}, the sampling distribution should
be set as in (10).
When γi = 0, the above distribution in the equation (10) is
not valid. To solve this problem, we combine the facts
P (wt−1 ) − D(θt−1 ) ≥ D(θ∗ ) − D(θt−1 ) := ϵt−1
D ,
where θ∗ is the optimal solution of the dual problem
t
maxθ D(θ), D(θt ) − D(θt−1 ) = ϵt−1
D − ϵD , and the inequality (9), to obtain
s
s
Gt .
E[ϵtD ] ≤ (1 − )E[ϵt−1
(11)
D ]+
n
2λn2
According to this inequality, although every γi = 0, if we

Stochastic Optimization with Importance Sampling for Regularized Loss Minimization

further assume every ϕi is Li -Lipschitz, then
Gt

3.2.2. A NALYSIS

=

n
1∑
(si R2 − γi (1 − si )λn)E∥ut−1
− θit−1 ∥2D
i
n i=1

≤

n
4R2 s ∑ 1 2
Li ,
2
n i=1 pi

(12)

where we use si = s/(npi ), ∥ut−1
∥ ≤ Li and ∥θit−1 ∥ ≤
i
t−1
t−1
Li , since −ui , −θi
∈ ∂ϕi (wt−1 ). Combining the
above two inequalities results in
s
s 4R2 s ∑ 1 2
)E[ϵt−1
L . (13)
D ]+
n
2λn2 n2 i=1 pi i
n

E[ϵtD ] ≤ (1 −

According to the above inequality, to minimize the t-th duality gap, we should choose∑a proper distribution to optin
mize the problem minp∈△n i=1 p1i L2i , for which the optimal distribution is obviously
pi = Li /

n
∑

Lj .

j=1

Because si = s/(npi ) ∈ [0, 1], the above distribution further implies
]
[
n
∩
nLmin
s∈
[0, npi ] = 0, ∑n
:= [0, ρ],
j=1 Lj
i=1
where Lmin = min{L1 , L2 , . . . , Ln } and ρ ≤ 1.
In summary, prox-SDCA with importance sampling can be
described in Algorithm 2.
Algorithm 2 Proximal Stochastic Dual Coordinate Ascent
with Importance Sampling (Iprox-SDCA)
Input: λ > 0, R = supu̸=0 ∥u∥D′ /∥u∥D , norms ∥ · ∥D ,
∥ · ∥D′ , γ1 , . . . , γn > 0, or L1 , . . . , Ln ≥ 0.
2

Initialize: θi0 = 0, w0 = ∇r∗ (0), pi =

R
1+ λnγ
i
∑n
R2
n+ j=1 λnγ

,
j

or pi = ∑nLi Lj , ∀i ∈ {1, . . . , n}.
j=1
for t = 1, . . . , T do
Sample it from {1, . . . , n} based on p;
[
∆θit−1
=arg max −ϕ∗it (−(θit−1
+∆θit ))−(wt−1 )⊤∆θit
t
t
∆θit

−

]
1
∥∆θit ∥2D′ ;
2λn

θitt = θit−1
+ ∆θit−1
;
t
t
1
t
t−1
v =v
+ λn
∆θit−1
;
t
wt = ∇r∗ (vt );
end for

Before presenting the theoretical results, we will make several assumptions without loss of generality: a) for the loss
functions: ϕi (0) ≤ 1, and ∀w, ϕi (w) ≥ 0, and b) for the
regularizer: r(0) = 0 and ∀w, r(w) ≥ 0. Then, we have
the following theorem for the expected duality gap when
the loss functions are smooth.
Theorem 3. Assume ϕi is (1/γi )-smooth ∀i ∈ {1, . . . , n}
∑n
R2
R2
and set pi = (1 + λnγ
)/(n + j=1 λnγ
), for ali
j
l i ∈ {1, . . . , n}. To obtain an expected duality gap of
E[P (wt ) − D(θT )] ≤ ϵP for the proposed Proximal SDCA with importance sampling, it suffices to have a total
number of iterations of
)
(
n
n
∑
∑
R2
R2 1
T ≥ (n +
) log (n +
)
.
λnγi
λnγi ϵP
i=1
i=1
Remark: If we employ uniform sampling, i.e., pi =
1/n ∀i, then we have to use the same γ for all ϕi
by choosing γmin = min{γ1 , . . . , γn }. By replacing γi with γmin , the theorem recovers a related result
of (Shalev-Shwartz & Zhang, 2012b)
under uniform
(
) samR2
R2
1
pling, i.e., T ≥ (n + λγmin ) log (n + λγmin ) ϵP . Since
2

n + λγRmin
nλγmin + R2
=
∑n
2 ∑n
R2
n + i=1 λγi n
nλγmin + Rn
i=1

γmin
γi

≥ 1,

the bound for
∑nimportance sampling is always better, especially when i=1 γmin
γi ≪ n.
For non-smooth loss functions, the convergence rate for
Proximal SDCA with importance sampling is given below.
Theorem 4. Consider the proposed proximal SDCA with
importance sampling.
Assume that ϕi is Li -Lipschitz and
∑n
set pi = Li / j=1 Lj , ∀i ∈ {1, . . . , n}. To obtain an
¯ ≤ ϵP where
expected duality gap of E[P (w̄) − D(θ)]
∑T
∑T
1
1
t−1
w̄ = T −T0 t=T0 +1 w
and θ̄ = T −T0 t=T0 +1 θt−1 ,
it suffices to have a total number of iterations of
∑n
4R2 ( i=1 Li )2
T ≥ T0 + n/ρ +
n2 λϵP
∑n
2
20R ( i=1 Li )2
,
≥ ω + n/ρ +
n2 λϵP
where ω = max(0, ⌈ nρ log( ρ2R2 (∑nλn Li )2 /n2 )⌉), and ρ =
i=1

Moreover, when t ≥ T0 , we have dual suboptimality bound of E[D(θ∗ ) − D(θt )] ≤ ϵP /2.
nLmin
∑
.
n
i=1 Li

Remark:
If we replace all Li by Lmax
=
max{L1 , . . . , Ln }, the theorem is still valid, and
the sampling distribution becomes the uniform distribution.
In this case we recover a related result of (Shalev-Shwartz & Zhang, 2012b), i.e.,

Stochastic Optimization with Importance Sampling for Regularized Loss Minimization

T ≥ max(0, 2⌈n log( 2R2λn
L2max )⌉) − n +
However, the ratio of the leading terms is

20R2 (Lmax )2
.
λϵP

(L
)2
n
∑n max 2 2 = ( ∑n
)2 ≥ 1,
( i=1 Li ) /n
i=1 Li /Lmax

which again implies that the importance
sampling bound
∑n
Li
)2 ≪ n2 .
is always better, especially when ( i=1 Lmax

4. Experimental Results
4.1. Experimental Testbed and Setup
For simplicity, in the experiments we only consider the task
of optimizing squared hinge loss based SVM with ℓ2 reg)2
∑n (
ularization: minw n1 i=1 [1 − yi w⊤ xi ]+ + λ2 ∥w∥22 .
Moreover we set ψ = 12 ∥ · ∥22 . In this case, we compare importance sampling versus the standard uniform sampling
using Pegasos of Shalev-Shwartz et al. (2007)) for SGD,
and using SDCA (Shalev-Shwartz & Zhang, 2013).
For Iprox-SGD, using√the inequality P (w∗ ) = D(θ∗ ), we
can get ∥w∗ ∥2 ≤ 1/ λ. Thus the theoretical analysis is
still valid if we project
the iterative solutions onto {w ∈
√
Rd |∥w∥2 ≤ 1/ λ} using Euclidean distance. Setting
(
)2
ϕi (w) = [1 − yi w⊤ xi ]+ + λ2 ∥w∥22 so that ∇ϕi (w) =
−2[1−y√i w⊤ xi ]+ yi x√
i +λw. Because ∥∇ϕi (w)∥2 ≤ 2(1+
∥xi ∥2 / λ)∥xi ∥2 + λ, according to our√analysis, √
the optimal distribution is pi =

∑n

2(1+∥xi ∥2 / λ)∥xi ∥2 + λ
√
√ .
λ)∥xj ∥2 + λ]

j=1 [2(1+∥xj ∥2 /

Finally, r(w) = 0 and proxλr (x) = x.
For Iprox-SDCA, we set r(w) = 12 ∥w∥22 , which is 1strongly convex with ∥ · ∥P ′ = ∥ · ∥2 ; we also have
(
)2
ϕi (w) = [1 − yi w⊤ xi ]+ , which is (2∥xi ∥22 )-smooth
with respect to ∥ · ∥P = ∥ · ∥2 . As a result, the optimal
distribution for proximal SDCA with importance sampling
∑n 2∥x ∥2
2∥x ∥2
should be pi = (1 + λni 2 )/(n + j=1 λnj 2 ), where
we used the fact R = supu̸=0 ∥u∥D′ /∥u∥D = 1. It can be
derived that the dual function of ϕ(·) is
ϕ∗i (−θ) =

{

−α + α2 /4
∞

θ = αyi xi , α ≥ 0
.
otherwise

The Iprox-SDCA method
may employ the closed-form
so(
)
1−yi w⊤ xi −αi /2
lution: ∆θi = max 1/2+∥xi ∥2 /(λn) , −αi yi xi .

our experiments. In particular, the regularization parameter λ of SVM is set to 10−4 , 10−6 , 10−4 for ijcnn1, kdd2010(algebra), and w8a, respectively. For prox-SGD and
Iprox-SGD, the step size is set to ηt = 1/(λt) for all the
datasets.
Given these parameters, we estimated the ratios between
the constants in the convergence bounds for uniform sampling and the proposed importance sampling strategies,
which are listed in Table 2. These ratios imply that the importance sampling will be effective for SGD on kdd2010
and w8a, but not very effective for ijcnn1, which will be
verified by empirical results. In addition, these ratios imply that importance sampling accelerates SDCA for all the
datasets, which will also be demonstrated empirically.
Table 2. Theoretical Constant Ratios for The Datasets.

Constant Ratio

∑
n n
(G )2
∑ i=1 i 2
( n
i=1 Gi )
nλγmin +R2
2 ∑n
nλγmin + Rn
i=1

γmin
γi

ijcnn1

kdd2010

w8a

1.0643

1.4667

1.9236

1.1262

1.1404

1.3467

All experiments were conducted by fixing five different random seeds for each dataset, and the reported results
were averaged over these five runs. We evaluated the learning performance by measuring the primal objective value
(P (wt )) for SGD, and the duality gap (P (wt ) − D(θt ))
for SDCA. In addition, to examine the generalization ability of the learning algorithms, we evaluated the test error
rates. Moreover, we report the variances of the stochastic
gradients of the two algorithms to check the effectiveness
of importance sampling. Finally, for Iprox-SGD and IproxSDCA, the uniform sampling is adopted at the first epoch,
so that the performance is the same with SGD and SDCA
at the first epoch, respectively.
4.2. Evaluation on Iprox-SGD
Figure 1 summarizes results in terms of primal objective
values, test error rates and variances of the stochastic gradients varying over the learning process on all the datasets
for SGD and Iprox-SGD. Epoch for the horizontal axis is
the number of iterations divided by dataset size.

2

To evaluate the performance of our algorithms, the
experiments were performed on several real world
datasets downloaded from the LIBSVM website
www.csie.ntu.edu.tw/˜cjlin/libsvmtools/.
The dataset characteristics are provided in the Table 1.
Table 1. Datasets used in the experiments.

Dataset
Dataset Size
Features
ijcnn1
49990
22
kdd2010(algebra)
8407752
20216830
w8a
49749
300
For fair comparison, all algorithms use the same setup in

First, the left column summarized the primal objective values of Iprox-SGD in comparison to SGD with uniform
sampling on all the datasets. On the last two datasets, the
proposed Iprox-SGD algorithm achieved the fastest convergence rates. Because these two algorithms adopted the
same learning rates, this observation implies that the proposed importance sampling does sampled more informative
stochastic gradient during the learning process. Second, the
central column summarized the test error rates of the two
algorithms, where Iprox-SGD achieves significantly smaller test error rates than those of SGD on the last two dataset.
This indicates that the proposed importance sampling ap-

Stochastic Optimization with Importance Sampling for Regularized Loss Minimization

1.4

0.08
10
Epoch

15

20

0

5

10
Epoch

15

20

0

(b) ijcnn1

5

10
Epoch

15

10

20

(c) ijcnn1

50
Epoch

100

0

(a) ijcnn1

15

0.1
0

20

(d) kdd2010
2

10
Epoch

15

20

0

(e) kdd2010

Test error rate

0

10

−5

10

5

10
Epoch

15

10

20

0

(f) kdd2010

Pegasos
Iprox−SGD

0.12

50
Epoch

Pegasos
Iprox−SGD

10

5

10
Epoch

(g) w8a

15

20

0

5

10
Epoch

(h) w8a

15

20

10

0

10
Epoch

15

20

(i) w8a

50
Epoch

0.106

SDCA
Iprox−SDCA

−5

10

−10

10

50
40

100

20
0

100

(f) kdd2010
8

SDCA
Iprox−SDCA

0.104

50
Epoch

0.102
0.1

SDCA
Iprox−SDCA

6

4

0.098
−15

5

100

SDCA
Iprox−SDCA

60

(e) kdd2010

0.1
0

70

SDCA
Iprox−SDCA

0.12

0.1
0

100

0

0.14

50
Epoch

(c) ijcnn1

0.14

(d) kdd2010

5

10

1
0

100

30
−10

5

0.16

Pegasos
Iprox−SGD

10

0.16

SDCA
Iprox−SDCA
Test error rate

0.15

50
Epoch

(b) ijcnn1

Test error rate

10
Epoch

0.2

5

10

10
Duality gap value

10

Pegasos
Iprox−SGD
Variance

Test error rate
0

5

Pegasos
Iprox−SGD

0.3
0.25

Duality gap value

Pegasos
Iprox−SGD

Variance

10
Primal objective value

0

0

0

1.5

0.08

Variance

5

(a) ijcnn1

0

0.1
0.09

SDCA
Iprox−SDCA

2

−15

5

Primal objective value

−10

10

2.5

SDCA
Iprox−SDCA

0.11

Variance

0.22
0

1.6

0.12

SDCA
Iprox−SDCA

−5

10

Variance

0.1
0.09

10

Test error rate

0.24

Pegasos
Iprox−SGD

1.8
Variance

0.26

0

2

Pegasos
Iprox−SGD

0.11

Duality gap value

0.12

Pegasos
Iprox−SGD
Test error rate

Primal objective value

0.3
0.28

10

0

50
Epoch

100

0.096
0

(g) w8a

50
Epoch

(h) w8a

100

2
0

50
Epoch

100

(i) w8a

Figure 1. Comparison between Pegasos with Iprox-SGD.

Figure 2. Comparison between SDCA with Iprox-SDCA.

proach is effective in improving generalization ability. In
addition, the right column shows the variances of stochastic
gradients for the Iprox-SGD and SGD algorithms, where
we can observe Iprox-SGD enjoys much smaller variances
than SGD on the last two dataset. This again demonstrates
that the proposed importance sampling strategy is effective
in reducing the variance of the stochastic gradients. Finally, on the first dataset, the proposed Iprox-SGD algorithm
achieved a convergence rate comparable to that of the traditional prox-SGD, which indicates that Iprox-SGD may degenerate into the traditional prox-SGD when the variance
is not reduced.

column shows the variances of stochastic gradients for the
Iprox-SDCA and SDCA algorithms, where we can observe
Iprox-SDCA enjoys slightly smaller variances than those
of SDCA on all datasets. However the improvement is not
large enough to significantly reduce the test error. This is
because SDCA by itself is already a stochastic variance reduction gradient method (Johnson & Zhang, 2013). We believe if distributed prox-SDCA adopts this importance sampling strategy, then the corresponding improvement can be
more significant.

4.3. Evaluation on Iprox-SDCA
Figure 2 summarizes experimental results in terms of duality gap values, test error rates and variances of the stochastic gradients varying over the learning process on all the
datasets for SDCA and Iprox-SDCA.

This paper studies stochastic optimization with importance
sampling, including importance sampling strategies for
prox-SMD and prox-SDCA. For prox-SMD with importance sampling, our analysis shows that in order to reduce variance, the sample distribution should depend on
the norms of the gradients of the loss functions, which can
be relaxed to the smooth constants or the Lipschitz constants of all the loss functions; for prox-SDCA with importance sampling, our analysis shows that the sampling
distribution should rely on the smooth constants or Lipschitz constants of all the loss functions. Compared to the
traditional prox-SGD and prox-SDCA methods, we have
shown that the proposed importance sampling methods can
significantly improve the convergence rate under suitable
conditions. Finally, we performed a set of empirical experiments to confirm the theoretical analysis.

We have several observations from these empirical results.
First, the left column summarized the dual gap values of
Iprox-SDCA in comparison to SDCA with uniform sampling on all the datasets. According to the dual gap values on all the datasets, the proposed Iprox-SDCA algorithm converges faster than the standard SDCA algorithm,
which indicates that the proposed importance sampling strategy is more effective than uniform sampling. Second,
the central column summarized the test error rates of the
two algorithms, where the test error rates of Iprox-SDCA
is comparable with those of SDCA on all the dataset. The
results indicate that SDCA is quite fast at the first few epochs so that the importance sampling does not improve
the test accuracy, although importance sampling can accelerate the minimization of duality gap. In addition, the right

5. Conclusion

Acknowledgments
The research of Peilin Zhao and Tong Zhang is partially
supported by NSF-IIS-1407939 and NSF-IIS 1250985.

Stochastic Optimization with Importance Sampling for Regularized Loss Minimization

References
Duchi, John and Singer, Yoram. Efficient online and batch
learning using forward backward splitting. The Journal
of Machine Learning Research, 10:2899–2934, 2009.
Duchi, John C., Shalev-Shwartz, Shai, Singer, Yoram, and
Tewari, Ambuj. Composite objective mirror descent. In
COLT, pp. 14–26, 2010.
Hsieh, Cho-Jui, Chang, Kai-Wei, Lin, Chih-Jen, Keerthi,
S Sathiya, and Sundararajan, Sellamanickam. A dual
coordinate descent method for large-scale linear svm. In
Proceedings of the 25th international conference on Machine learning, pp. 408–415. ACM, 2008.
Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
NIPS, 2013.
Lacoste-Julien, Simon, Jaggi, Martin, Schmidt, Mark W.,
and Pletscher, Patrick. Stochastic block-coordinate
frank-wolfe optimization for structural svms. CoRR, abs/1207.4747, 2012.
Lee, Yin Tat and Sidford, Aaron. Efficient accelerated coordinate descent methods and faster algorithms for solving
linear systems. arXiv preprint arXiv:1305.1922, 2013.
Luo, Zhi-Quan and Tseng, Paul. On the convergence of
the coordinate descent method for convex differentiable
minimization. Journal of Optimization Theory and Applications, 72(1):7–35, 1992.
Mangasarian, Olvi L and Musicant, David R. Successive
overrelaxation for support vector machines. Neural Networks, IEEE Transactions on, 10(5):1032–1037, 1999.
Needell, Deanna, Srebro, Nathan, and Ward, Rachel. Stochastic gradient descent, weighted sampling, and the
randomized kaczmarz algorithm. arXiv preprint arXiv:1310.5715v3, 2014.
Nesterov, Yu. Efficiency of coordinate descent methods
on huge-scale optimization problems. SIAM Journal on
Optimization, 22(2):341–362, 2012a.
Nesterov, Yurii. Efficiency of coordinate descent methods
on huge-scale optimization problems. SIAM Journal on
Optimization, 22(2):341–362, 2012b.
Rakhlin, Alexander, Shamir, Ohad, and Sridharan, Karthik.
Making gradient descent optimal for strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647,
2011.
Roux, Nicolas L, Schmidt, Mark, and Bach, Francis R. A
stochastic gradient method with an exponential convergence rate for finite training sets. In Advances in Neural
Information Processing Systems, pp. 2663–2671, 2012.

Shalev-Shwartz, Shai and Tewari, Ambuj. Stochastic methods for l1 -regularized loss minimization. Journal of Machine Learning Research, 12:1865–1892, 2011.
Shalev-Shwartz, Shai and Zhang, Tong. Proximal stochastic dual coordinate ascent. CoRR, abs/1211.2717, 2012a.
Shalev-Shwartz, Shai and Zhang, Tong. Proximal stochastic dual coordinate ascent. arXiv preprint arXiv:1211.2717, 2012b.
Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual coordinate ascent methods for regularized loss minimization. JMLR, pp. 567–599, 2013.
Shalev-Shwartz, Shai and Zhang, Tong. Accelerated proximal stochastic dual coordinate ascent for regularized loss
minimization. In ICML, 2014.
Shalev-Shwartz, Shai, Singer, Yoram, and Srebro, Nathan.
Pegasos: Primal estimated sub-gradient solver for svm.
In ICML, pp. 807–814, 2007.
Shamir, Ohad and Zhang, Tong. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In Proceedings of
the 30th International Conference on Machine Learning
(ICML-13), pp. 71–79, 2013.
Strohmer, Thomas and Vershynin, Roman. A randomized kaczmarz algorithm with exponential convergence.
Journal of Fourier Analysis and Applications, 15(2):
262–278, 2009.
Xiao, Lin and Zhang, Tong. A proximal stochastic gradient method with progressive variance reduction. Siam
Journal on Optimization, 24:2057–2075, 2014.
Zhang, Tong. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In
ICML, 2004.
Zhao, Peilin and Zhang, Tong. Stochastic optimization with importance sampling. arXiv preprint arXiv:1401.2753, 2014.

