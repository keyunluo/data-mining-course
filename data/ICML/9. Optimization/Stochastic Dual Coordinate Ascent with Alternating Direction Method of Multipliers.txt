Stochastic Dual Coordinate Ascent
with Alternating Direction Method of Multipliers

Taiji Suzuki
S - TAIJI @ IS . TITECH . AC . JP
Department of Mathematical and Computing Sciences, Tokyo Institute of Technology, Tokyo 152-8552, JAPAN

Abstract
We propose a new stochastic dual coordinate ascent technique that can be applied to a wide range
of regularized learning problems. Our method is
based on alternating direction method of multipliers (ADMM) to deal with complex regularization functions such as structured regularizations.
Although the original ADMM is a batch method,
the proposed method offers a stochastic update
rule where each iteration requires only one or few
sample observations. Moreover, our method can
naturally afford mini-batch update and it gives
speed up of convergence. We show that, under
mild assumptions, our method converges exponentially. The numerical experiments show that
our method actually performs efficiently.

1. Introduction
This paper proposes a new stochastic optimization method
that shows exponential convergence and can be applied to
wide range of regularization functions using the techniques
of stochastic dual coordinate ascent with alternating direction method of multipliers. Recently, it is getting more and
more important to develop an efficient optimization method
which can handle large amount of samples. One of the
most successful approaches is a stochastic optimization approach. Indeed, a lot of stochastic methods have been proposed to deal with large amount of samples. Among them,
the (online) stochastic gradient method is the most basic
and successful one. This can be naturally applied to the
regularized learning frame-work. Such a method is called
several different names including online proximal gradient
descent, forward-backward splitting and online mirror descent (Duchi and Singer, 2009). Basically, these methods
are intended to process sequentially coming data. They
update the parameter using one new observation and disProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

card the observed sample. Therefore, they don’t need large
memory space to store the whole observed
√ data. The convergence rate of those methods is O(1/ T ) for general
settings and O(1/T ) for strongly convex losses, which are
minimax optimal (Nemirovskii and Yudin, 1983).
On the other hand, recently it was shown that, if it is allowed to reuse the observed data several times, it is possible to develop a stochastic method with exponential convergence rate for a strongly convex objective (Le Roux et al.,
2013; Shalev-Shwartz and Zhang, 2013c;a). These methods are still stochastic in a sense that one sample or small
mini-batch is randomly picked up to be used for each update. The main difference from the stochastic gradient
method is that these methods are intended to process data
with a fixed number of training samples. stochastic average gradient (SAG) method (Le Roux et al., 2013) utilizes
an averaged gradient to show an exponential convergence.
stochastic dual coordinate ascent (SDCA) method solves
the dual problem using a stochastic coordinate ascent technique (Shalev-Shwartz and Zhang, 2013c;a). These methods have favorable properties of both online-stochastic approach and batch approach. That is, they show fast decrease of the objective function in the early stage of the
optimization as online-stochastic approaches, and shows
exponential convergence after the “burn in” time as batch
approaches. However, these methods have some drawbacks. SAG needs to maintain all gradients computed on
each training sample in memory which amount to dimension times sample size. SDCA method can be applied only
to a simple regularization function for which the dual function is easily computed, thus it is hard to apply the method
to a complex regularization function such as structured regularization.
In this paper, we propose stochastic dual coordinate ascent method for alternating direction method of multipliers (SDCA-ADMM). Our method is similar to SDCA, but
inherits a favorable property of ADMM. By combining
SDCA and ADMM, our method can be applied to a wide
range of regularized learning problems. ADMM is an effective optimization method to solve a composite optimization problem described as minx f (x) + g(y) s.t. Ax +

Stochastic Dual Coordinate Ascent with ADMM

By = 0 (Gabay and Mercier, 1976; Boyd et al., 2010;
Qin and Goldfarb, 2012). This formulation is quite flexible and fit wide range of applications such as structured
regularization, dictionary learning, convex tensor decomposition and so on (Qin and Goldfarb, 2012; Jacob et al.,
2009; Tomioka et al., 2011; Rakotomamonjy, 2013). However, ADMM is a batch optimization method. Our approach transforms ADMM to a stochastic one by utilizing stochastic coordinate ascent technique. Our method,
SDCA-ADMM, does not require large amount of memory because it observes only one or few samples for each
iteration. SDCA-ADMM can be naturally adapted to a
sub-batch situation where a block of few samples is utilized for each iteration. Moreover, it is shown that our
method shows exponential convergence for risk functions
with some strong convexity and smoothness property. The
convergence rate is affected by the size of sub-batch. If
the samples are not strongly correlated, sub-batch gives a
better convergence rate than one-sample update.

The purpose of this paper is to give an efficient stochastic
optimization method to solve this problem (2). For this purpose, we employ the dual formulation. Using the Fenchel’s
duality theorem, we have the following dual formulation.
Lemma 1.
n

1X
fi (zi⊤ w) + ψ(B ⊤ w)
w∈R n
i=1
)
( n

1X ∗
∗ y
| Zx+By = 0 , (3)
f (xi )+ψ
=− minn
x∈R
n i=1 i
n
d
minp

y∈R

where fi∗ and ψ ∗ are the convex conjugates of fi
and ψ respectively (Rockafellar, 1970)1 , and Z =
[z1 , z2 , . . . , zn ] ∈ Rp×n . Moreover w∗ , x∗ and y ∗ are optimal solutions of both sides if and only if
zi⊤ w∗ ∈ ∂fi∗ (x∗i ),

1 ∗
y ∈ ∂ψ(u)|u=B ⊤ w∗ ,
n

Zx∗ + By ∗ = 0.

2. Structured Regularization and its Dual
Formulation
In this section, we give the problem formulation of structured regularization and its dual formulation. The standard
regularized risk minimization is described as follows:
n

minp

w∈R

1X
fi (zi⊤ w) + ψ̃(w),
n i=1

(1)

where z1 , z2 , . . . , zn are vectors in Rp , w is the weight vector that we want to learn, fi is a loss function for the ith sample, and ψ̃ is the regularization function which is
used to avoid over-fitting. For example, the loss function fi
can be taken as a classification surrogate loss fi (zi⊤ w) =
ℓ(yi , zi⊤ w) where yi is the training label of the i-th sample. With regard to ψ̃, we are interested in a sparsity inducing regularization, e.g., ℓ1 -regularization, group lasso
regularization, trace-norm regularization, and so on. Our
motivation in this paper is to deal with a “complex” regularization ψ̃ where it is not easy to directly minimize the
regularization function (more precisely the proximal operation determined by ψ̃ is not easily computed, see Eq. (5)).
This kind of regularization appears in, for example, structured sparsity such as overlapped group lasso and graph
regularization (Jacob et al., 2009; Signoretto et al., 2010).
In many cases, such a “complex” regularization function
can be decomposed into a “simple” regularization ψ and a
linear transformation B, that is, ψ̃(w) = ψ(B ⊤ w) where
B ∈ Rp×d . Using this formulation, the optimization problem (Eq. (1)) is equivalent to
n

minp

w∈R

1X
fi (zi⊤ w) + ψ(B ⊤ w).
n i=1

(2)

Proof. By Fenchel’s duality theorem (Corollary 31.2.1 of
Rockafellar (1970)), we have that
Pn
minw∈Rp n1 i=1 fi (zi⊤ w) + ψ̃(w)
n P
o
n
= − minx∈Rn n1 i=1 fi∗ (xi ) + ψ̃ ∗ (−Zx/n) . (4)

Moreover x∗ , w∗ are optimal in each side if and only
if zi⊤ w∗ ∈ ∂fi∗ (x∗i ) and −Zx∗ /n ∈ ∂ ψ̃(w∗ ) =
B∂ψ(u∗ )|u=B ⊤ w∗ (Corollary 31.3 of Rockafellar (1970)).
Now, Theorem 16.3 of Rockafellar (1970) gives that
ψ̃ ∗ (u) = (ψ ◦ B ⊤ )∗ (u) = inf{ψ ∗ (y) | By = u}.

Thus ψ̃ ∗ (−Zx/n) = inf{ψ ∗ (y/n) | By = −Zx},
and substituting this into the RHS of Eq. (4) we obtain Eq. (3). Now, y ∗ satisfying Zx∗ + By ∗ = 0
is the optimal solution if and only if ψ ∗ (y ∗ /n) =
ψ̃ ∗ (−Zx∗ /n) for the optimal x∗ . Thus, if (w∗ , x∗ , y ∗ )
is optimal, then we have −Zx∗ /n ∈ ∂ ψ̃(w∗ ) and thus
ψ ∗ (y ∗ /n) = ψ̃ ∗ (−Zx∗ /n) = hw∗ , −Zx∗ /ni − ψ̃(w∗ ) =
hB ⊤ w∗ , y ∗ /ni − ψ(B ⊤ w∗ ) which implies y ∗ /n ∈
∂ψ(u)|u=B ⊤ w∗ . Contrary, if y ∗ /n ∈ ∂ψ(u)|u=B ⊤ w∗ , then
it is obvious that −Zx∗ /n ∈ ∂ ψ̃(w∗ ) because Zx∗ +
By ∗ = 0. Therefore, we obtain the optimality conditions.
The dual problem is a composite objective function optimization with a linear constraint Zx + By = 0. In the next
section, we give an efficient stochastic method to solve this
dual problem. A nice property of the dual formulation is
that, in many machine learning applications, the dual loss
1
The convex conjugate function f ∗ of f is defined by
f ∗ (y) := supx {x⊤ y − f (x)}.

Stochastic Dual Coordinate Ascent with ADMM

function fi∗ becomes strongly convex. For example, for the
logistic loss fi (x) = log(1+exp(−yi x)), the dual function
is fi∗ (−u) = yi u log(yi u) + (1 − yi u) log(1 − yi u) (yi u ∈
[0, 1]) and its modulus of strong convexity is much better
than the primal one. More importantly, each sample (zi , yi )
directly affects only each coordinate xi of dual variable. In
other words, if xi is fixed the i-th sample (zi , yi ) has no
influence to the objective value. This enables us to utilize
the stochastic coordinate ascent technique in the dual problem because update of single coordinate xi requires only
the information of the i-th sample (zi , yi ).
Finally, we give the precise notion of the “complex”
and “simple” regularizations. This notion is defined by
the computational complexity of proximal operation corresponding to the regularization function (Rockafellar,
1970). The proximal operation corresponding to a convex
function ψ is defined by


1
2
kq − uk + ψ(u) .
(5)
prox(q|ψ) := arg min
2
u
For example, the proximal operation corresponding to
ℓ1 -regularization ψ(w) = kwkℓ1 is easily computed as
prox(q|ψ) = (sign(wi ) max{|wi | − 1, 0})i which is the
so-called soft-thresholding operation. More generally, the
proximal operation for group lasso regularization with
non-overlapped groups can also be analytically computed.
On the other hand, for overlapped group regularization,
the proximal operation is no longer analytically obtained.
However, by choosing B appropriately, we can split the
overlap and obtain ψ for which the proximal operation is
easily computed (see Section 6 for concrete examples).

3. Proposed Method: Stochastic Dual
Coordinate Ascent with ADMM
In this section, we present our proposal, stochastic dual coordinate ascent type ADMM. For√a positive semidefinite
matrix S, we denote by kxkS := x⊤ Sx. Zi denotes the
i-th column of Z, which is zi , and Z\i is a matrix obtained
by subtracting i-th column from Z. Similarly, for a vector
x, x\i is a vector obtained by subtracting i-th component
from x.
3.1. One Sample Update of SDCA for ADMM
The basic update rule of our proposed method in the tth step is given as follows: Each update step, choose
i ∈ {1, . . . , n} uniformly at random, and update as
n
y (t)← arg min nψ ∗ (y/n) − hw(t−1), Zx(t−1) +Byi
y

(t)

o
ρ
1
+ kZx(t−1) + Byk2 + ky − y (t−1) k2Q , (6a)
2
2
n

xi ← arg min fi∗ (xi ) − hw(t−1) , Zi xi + By (t) i
xi

ρ
(t−1)
+ kZi xi + Z\i x\i
+ By (t) k2
2
o
1
(t−1) 2
kGii ,
+ kxi − xi
2
w(t)←w(t−1) − γρ{n(Zx(t) + By (t) )
− (n − 1)(Zx(t−1) + By (t−1) )},

(6b)

(6c)

where w(t) ∈ Rp is the primal variable at the t-th step,
Q and G are arbitrary positive semidefinite matrices, and
γ, ρ > 0 are parameters we give beforehand.
The optimization procedure looks a bit complicated, To
simplify the procedure, we set Q as
Q = ρ(ηB Id − B ⊤ B)

(7)

where ηB are chosen so that ηB Id ≻ B ⊤ B. Then, by carrying out simple calculations and denoting ηZ,i = Gii /ρ +
kzi k2 , the update rule of x(t) and y (t) is rewritten as

B⊤
y (t)←prox y (t−1) +
{w(t−1)
ρηB

 nψ ∗ (·/n) 

,
− ρ(Zx(t−1) + By (t−1) )}
ρηB

Z⊤
(t)
(t−1)
xi ←prox xi
+ i {w(t−1)
ρηZ,i
 f∗ 

.
− ρ(Zx(t−1) + By (t) )}  i
ρηZ,i

(8a)

(8b)

Note that the update (8b) of x(t) is just a one dimensional
optimization, thus it is quite easily computed. Moreover,
for some loss functions such as the smoothed hinge loss
used in Section 6, we have an analytic form of the update.
The update rule (8a) of y (t) can be rewritten by the proximal operation corresponding to the primal function ψ while
the rule (8a) is given by that corresponding to the dual function ψ ∗ . Indeed, there is a clear relation between primal and
dual (Theorem 31.5 of Rockafellar (1970)):
prox(q|ψ) + prox(q|ψ ∗ ) = q.
⊤

B
Using this, for q (t) = y (t−1) + ρη
{w(t−1) − ρ(Zx(t−1) +
B
(t−1)
By
)}, we have that

y (t) ← q (t) − prox(q (t) |nψ(ρηB · )/(ρηB )),

(9)

because (cf (·))∗ (y) = cf ∗ (y/c) for a convex function f
and c > 0. This is efficiently computed because we assumed the proximal operation corresponding to ψ can be
efficiently computed.
During the update, we need Zx(t−1) which seems to require O(n) computation at the first glance. However, it can
(t)
be incrementally updated as Zx(t) = Zx(t−1) + Zi (xi −

Stochastic Dual Coordinate Ascent with ADMM
(t−1)

xi
). Thus we don’t need to load all the samples to compute Zx(t−1) at each iteration.
In the above, the update rule of our algorithm is based on
one sample observation. Next, we give a mini-batch extension of the algorithm where more than one samples could
be used for each iteration.
3.2. Mini-Batch Extension
Here, we generalize our method to mini-batch situation
where, at each iteration, we observe a small number of samples {(xi1 , yi1 ), . . . , (xik , yik )} instead of one sample observation. At each iteration, we randomly choose an index
set I ⊆ {1, . . . , n} so that each index i is included in I with
probability 1/K; P (i ∈ I) = 1/K for all i = 1, . . . , n. To
do so, we suggest the following procedure. We split the index set {1, . . . , n} into K groups (I1 , I2 , . . . , IK ) beforehand, and then pick up uniformly k ∈ {1, . . . , K} and set
I = Ik for each iteration. Each sub-batch Ik can have different cardinality from others, but the probability P (i ∈ I)
should be uniform for all i = 1, . . . , n. The update rule
using sub-batch is given as follows: Update y (t) as before
(6a), and update x(t) and w(t) by
nX
(t)
xI ← arg min
fi∗ (xi ) − hw(t−1) , ZI xI + By (t) i
xI

i∈I

o
ρ
1
(t−1)
(t−1) 2
+ kZI xI +Z\I x\I +By (t) k2 + kxI −xI
kGI,I ,
2
2
(10a)

where xI is a vector consisting of components with indexes
i ∈ I, xI = (xi )i∈I , and ZI is a sub-matrix of Z consisting
of columns P
with indexes i ∈ I, ZI = [Zi1 , . . . , Zi|I| ]. Note
that, since i∈I fi∗ (xi ) is sum of single variable convex
functions fi∗ (xi ), the proximal operation in Eq. (12) can be
split into the proximal operation with respect to each single
variable xi . This is advantageous for not only the simpleness of the computation but also parallel computation. That
(t−1)
ZI⊤
{w(t−1) − ρ(Zx(t−1) + By (t) )},
is, for pI = xI
+ ρηZ,I
(t)

f∗

i
)
the update rule (12) is reduced to xi ← prox(pi | ρηZ,I
for each i ∈ I, which is easily parallelizable. In summary,
our proposed algorithm is given in Algorithm 1.

Algorithm 1 SDCA-ADMM
Input: ρ, η > 0
Initialize x0 = 0, y0 = 0, w0 = 0 and {I1 , . . . , IK }.
for t = 1 to T do
Choose k ∈ {1, . . . , K} uniformly at random, set I =
Ik , and observe the training samples {(xi , yi )}i∈I .
B⊤
Set q (t)= y (t−1)+ ρη
{w(t−1)−ρ(Zx(t−1)+By (t−1) )}.
B
(t)
(t)
(t)
Update y ← q − prox(q
|nψ(ρηB · )/(ρηB ))

(t)

Update xI

(t−1)

← prox xI
+
P
∗
f

i∈I i
ρ(Zx(t−1) + By (t) )}  ρη
.
Z,I

ZI⊤
(t−1)
ρηZ,I {w

−

Update w(t) ← w(t−1) −γρ{n(Zx(t) +By (t) )−(n−
n/K)(Zx(t−1) + By (t−1) )}.
end for
Output: w(T ) .

w(t) ← w(t−1) − γρ{n(Zx(t) + By (t) )

− (n − n/K)(Zx(t−1) + By (t−1) )}.

(10b)
(t)

Using Q given in Eq. (7), the update rule of y can be
replaced by Eq. (9) as in one-sample update situation. The
update rule of x(t) can also be simplified by choosing G appropriately. Because sub-batches have no overlap between
each other, we can construct a positive semi-definite matrix
G such that the block-diagonal element GI,I has the form
GI,I = ρ(ηZ,I I|I| − ZI⊤ ZI )

(11)

where ηZ,I is a positive real satisfying ηZ,I ≥ kZI⊤ ZI k.
The reason why we split the index sets into K sets is to
construct this kind of G which “diagonalizes” the quadratic
function in (10a). The choice of I and G could be replaced with another one for which we could compute the
update efficiently, as long as P (i ∈ I) is uniform for all
i = 1, . . . , n. Using G given in (11), the update rule (10a)
of x(t) is rewritten as

Z⊤
(t)
(t−1)
xI ← prox xI
+ I {w(t−1)
ρηZ,I
P
∗
 i∈I fi
(t−1)
(t)
, (12)
− ρ(Zx
+ By )} 
ρηZ,I

Finally, we would like to highlight the connection between
our method and the original batch ADMM (Hestenes,
1969; Powell, 1969; Rockafellar, 1976). The batch ADMM
utilizes the following update rule
y
n
y (t) ← arg min nψ ∗
− hw(t−1) , Zx(t−1) + Byi
n
y
o
ρ
+ kZx(t−1) + Byk2 ,
(13a)
2

P
n
(t−1)
∗
, Zx + By (t) i
x(t) ← arg min
i=1 fi (xi ) − hw
x

w(t)

	
ρ
+ kZx + By (t) k2 ,
2
←w(t−1) − γρ(Zx(t) + By (t) ).

(13b)
(13c)

One can see that the update rule of our algorithm is reduced
to that of the batch ADMM (13) if we set K = 1 except the
term related to G and Q (the terms 21 k · k2Q and 21 k · k2GI,I ).
These terms related to G and Q are used also in batch situation to eliminate cross terms in BB ⊤ and ZZ ⊤ . This
technique is called linearization. The linearization technique makes the update rule simple and parallelizable, and
in some situations makes it possible to obtain an analytic
form of the update.

Stochastic Dual Coordinate Ascent with ADMM

4. Linear Convergence of SDCA-ADMM
In this section, the convergence rate of our proposed algorithm is given. Indeed, the convergence rate is exponential
(R-linear). To show the convergence rate, we assume some
conditions. First, we assume that there exits an unique optimal solution w∗ and B ⊤ is injective (on the other hand,
B is not necessarily injective). Moreover, we assume the
uniqueness of the dual solution x∗ , but don’t assume the
uniqueness of y ∗ . We denote by the set of dual optimum of
y as Y ∗ and assume that Y ∗ is compact. Then, by Lemma
1, we have that
zi⊤ w∗ ∈ ∂fi∗ (x∗i ), y ∗ /n ∈ ∂ψ(u)|u=B ⊤ w∗ .

(14)

By the convex duality arguments, this implies that x∗i ∈
∂fi (u)|u=zi⊤ w∗ , B ⊤ w∗ ∈ ∂ψ ∗ (u)|u=y∗ /n .
Moreover, we suppose that each (dual) loss function fi is
locally v-strongly convex and ψ, h-smooth around the optimal solution and ψ ∗ is also locally strongly convex in a
weak sense as follows.
Assumption 1. There exits v > 0 such that, ∀xi ∈ R,
fi∗ (xi ) − fi∗ (x∗i ) ≥ hzi⊤ w∗ , xi − x∗i i +

vkxi − x∗i k2
.
2

There exit h > 0 and vψ > 0 such that, for all y, u and
y ∗ ∈ Y ∗ , there exists yb∗ ∈ Y ∗ (depending on y) and we
have
ψ ∗ (y/n) − ψ ∗ (b
y ∗ /n) ≥ hB ⊤ w∗ , y/n − yb∗ /ni
v
+ 2ψ kPKer(B) (y/n − yb∗ /n)k2 ,
⊤

∗

∗

⊤

ψ(u) − ψ(B w ) ≥ hy /n, u − B w i
+ h2 ku − B ⊤ w∗ k2 ,

(15)

∗

(16)

where PKer(B) is the projection matrix to the kernel of B.
Note that these conditions should be satisfied only around
the optimal solutions (x∗ , y ∗ ) and w∗ . It does not need to
hold for every point, thus is much weaker than the ordinary strong convexity. Moreover, the inequalities need to
be satisfied only for the solution sequence (w(t) , x(t) , y (t) )
of our algorithm. The strong convexity of the dual loss
fi∗ implies that the primal loss fi is smooth around the
optimal. The condition (15) is satisfied, for example, by
ℓ1 -regularization because the dual of ℓ1 -regularization is
an indicator function with a compact support and, outside
the optimal solution set Y ∗ , the indicator function is lower
bounded by a quadratic function. In addition, the quadratic
term in the right hand side of this condition (15) is restricted on Ker(B). This makes it possible to include several types of regularization functions. Indeed, if B = Ip ,
this condition is always satisfied. The assumption (16) is
the strongest assumption. This is satisfied for elastic-net

regularization. ℓ1 -regularization could satisfy this condition depending on the optimum w∗ and the solution sequence. If one wants to make this condition always hold,
just adding a small square term, then the condition is satisfied and we obtain an approximated solution which is sufficiently close to the true one within a precision.
Define the primal and dual objectives as
Pn
FP (w) := n1 i=1 fi (zi⊤ w) + ψ(B ⊤ w),
Pn
FD (x, y) := n1 i=1 fi∗ (xi ) + ψ ∗ ( ny ) − hw∗ , Z nx − B ny i.

Note that, by Eq. (14), FP (w) − FP (w∗ ) and FD (x, y) −
FD (x∗ , y ∗ ) are always non-negative. Define the block diagonal matrix H as HI,I = ρZI⊤ ZI + GI,I for all I ∈
{I1 , . . . , IK } and Hi,j = 0 for (i, j) ∈
/ Ik × Ik (∀k). Let
ky − Y ∗ kQ := min{ky − y ∗ kQ | y ∗ ∈ Y ∗ }. We define
RD (x, y, w) as
RD (x, y, w) := FD (x, y) − FD (x∗ , y ∗ ) +
+

ρ(1−γ)
2n kZx

+ Byk2 +

1
2n kx

kw − w∗ k2
2n2 γρ

− x∗ k2vIp +H +

ky−Y ∗ k2Q
.
2nK

For a symmetric matrix S, we define σmax (S) and σmin (S)
as the maximum and minimum singular value respectively.
1
Theorem 2. Suppose that γ = 4n
, ηZ,I > {1 + 2γn(1 −
⊤
1/K)}σmax (ZI ZI ) for all I ∈ {I1 , . . . , IK } and B ⊤ is
injective. Then, under Assumption 1, the dual objective
function converges R-linearly: We have that, for C1 =
RD (x(0) , y (0) , w(0) ),

µ T
C1 ,
E[RD (x(T ) , y (T ) , w(T ) )] ≤ 1 −
K

hρσmin (B ⊤ B)
v
where µ = min 4(v+σmax
(H)) , 2 max{1/n,4hρ,4hσmax(Q) } ,
	
Kvψ /n
Kvσmin (BB ⊤ )
4σmax (Q) , 4σmax(Q) (ρσmax (Z ⊤ Z)+4v) . In particular,

E[kw(T ) − w∗ k2 ] ≤

µ T
nρ 
1−
C1 .
2
K

If we further assume ψ(B ⊤ w) ≤ ψ(B ⊤ w∗ ) +
hy ∗ /n, B ⊤ (w − w∗ )i + l1 kw − w∗ k + l2 kw − w∗ k2 (∀w),
then this implies that
E[FP (w(T ) ) − FP (w∗ )]

 nρ 
⊤
µ T
Z/n)
≤ σmax (Z
+
l
1
−
C1
2
2v
2
K
r
nρ 
µ T
1−
C1 .
+ l1
2
K
Since the proof is technical, it is deferred to the supplementary material. This theorem shows that the primal and
dual objective values converge R-linearly. Moreover, the
primal variable w also converges R-linearly to the optimal value. The number K of sub-batches controls the

Stochastic Dual Coordinate Ascent with ADMM

convergence rate. If all samples are nearly orthogonal to
each other, σmax (H) is bounded by a constant for all K,
and thus convergence rate gets faster and faster as K decreases (the size of each sub-batch grows up). On the
other hand, if samples are strongly correlated to each other,
σmax (H) grows linearly against 1/K and then the convergence rate is not improved by decreasing K. As for
batch settings, the linear convergence of batch ADMM
has been shown by Deng and Yin (2012). However, their
proof can not be directly applied to our stochastic setting.
Our proof requires a technique specialized to stochastic
coordinate ascent technique. We would like to point out
that the exponential convergence is not guaranteed if the
choice of index set I at each update is cyclic. The index I is supposed to be chosen randomly. It is reported
in Shalev-Shwartz and Zhang (2013a) that cyclic choice of
I yields slower convergence. As described in the introduction, the mini-max optimal rate of stochastic optimization
is at least O(1/T ). This corresponds to the convergence
rate of the test loss in machine learning terminology. However, our analysis focuses on the training loss. Thus we can
obtain the linear convergence.
The statement can be described in terms of the number of
iterations required to achieve a precision ǫ, i.e. smallest T
satisfying E[FP (w(T ) ) − FP (w∗ )] ≤ ǫ:
T ≤ C ′ K max

 4(v+σmax (H))
v

,

2 max{1/(nh),4ρ,4σmax(Q) }
,
ρσmin (B ⊤ B)

4σmax (Q) 4σmax (Q)(ρσmax (Z ⊤ Z)+4v) 	
log
Kvψ /n ,
Kvσmin (BB ⊤ )



nC ′′
ǫ



,

where C ′ and C ′′ are an absolute constant. This says that
dependency of ǫ is log-order. An interesting point is that
the influence of h, the modulus of local strong convexity
of ψ. Usually the regularization function is made weaker
as the number of samples increases. In that situation, h decreases as n goes up. However, even if we set h = 1/n (and
n
), we still have T = O(K log(n/ǫ)) instead of
vψ ≥ K
O(nK log(n/ǫ)). Thus, the convergence rate is hardly affected by the setting of h. This point is same as the ordinary
SDCA algorithm (Shalev-Shwartz and Zhang, 2013a).

5. Related Works
In this section, we present some related works and discuss
differences from our method.
The most related work is a recent study by
Shalev-Shwartz and Zhang (2013c) in which stochastic
dual coordinate ascent (SDCA) method for a regularized risk minimization is proposed. Their method also
deals with the dual problem (3) with B = Ip in our
setting, and apply a stochastic coordinate ascent technique. This method converges linearly. At each iteration,
the method solves the following one-dimensional opti(t)
mization problem, ∆xi ← arg min∆xi ∈R fi∗ (∆xi +

(t−1)

1
kzi ∆xi k2 , and updates
) + zi⊤ w(t−1) ∆xi + 2n
xi
(t)
(t)
(t−1)
xi
← ∆xi + xi
and w(t) ← ∂ ψ̃ ∗ (−Zx(t) ).
The most important difference from our method is the
computation of ∂ψ ∗ . In a “simple” regularization function,
it is often easy to compute the (sub-)gradient of ψ̃ ∗ .
However, in a “complex” regularization such as structured
regularization, the computation is not efficiently carried
out. To overcome this difficulty, our method utilizes a
linearly transformed one ψ(B·) = ψ̃(·), and split the
optimization with respect to fi∗ and ψ ∗ by applying
ADMM technique. Thus, our method is applicable to
much more general regularization functions. A mini-batch
extension of SDCA is a recent hot topic (Takáč et al., 2013;
Shalev-Shwartz and Zhang, 2013b). Our approach realizes
the mini-batch extension using the linearlization technique
in ADMM which is naturally derived in the frame-work of
ADMM. Although the proof technique is quite different,
the convergence analysis of normal mini-batch SDCA
given by Shalev-Shwartz and Zhang (2013b) is parallel to
our theorem.

The second method related to ours is stochastic average gradient (SAG) method (Le Roux et al., 2013). The
method is a modification of stochastic gradient descent
method, but utilizes an averaged gradient. A good point
of their method is that we only need to deal with the primal problem. Thus the computation is easy, and we don’t
need to look at the convex conjugate function. Moreover,
their method also converges linearly. However, the linear
convergence of SAG is guaranteed for smoothed loss and
regularization functions. Thus non-smooth structured regularization is not included in the scope of the naive SAG procedure. It is conjectured by Schmidt et al. (2013) that SAG
could be combined with proximal gradient frame-work and
even ADMM. Our work gives a particular answer to this
question in the setting of SDCA.
The third method is online version of ADMM. Recently
some online variants of ADMM have been proposed by
Wang and Banerjee (2012); Suzuki (2013); Ouyang et al.
(2013). These methods are effective for complex regularizations as discussed in this paper. Thus they are applicable to wide range of situations. However, those methods are basically online methods, thus they discard the
samples once observed. They are not adapted to a situation where the training samples are observed
√ several times.
Therefore, the convergence rate is O(1/ T ) in general and
O(log(T )/T ) for a strongly convex loss (possibly O(1/T )
with some modification). On the other hand, our method
converges linearly.

6. Numerical Experiments
In this section, we give numerical experiments on
artificial and real data to demonstrate the effective-

Stochastic Dual Coordinate Ascent with ADMM

Empirical Risk

Empirical Risk

n = 5120

CPU time (s)

Test Loss

Test Loss

CPU time (s)

CPU time (s)

CPU time (s)
RDA
OPG-ADMM
RDA-ADMM
OL-ADMM
Batch-ADMM
SDCA-ADMM

Classification Error

Then the proximal operation with respect to its dual function is analytically given as follows (see the supplementary
material for the derivation):

Cu−yi
i −1

(−1 ≤ Cuy
≤ 0),
 1+C
1+C
Cuyi −1
∗
prox(u|fi /C) = −yi
(−1 > 1+C ),


0
(otherwise).

n = 512

Classification Error

ness of our proposed algorithm2 . We compare our
SDCA-ADMM with the existing stochastic optimization methods such as regularized dual averaging (RDA)
(Duchi and Singer, 2009; Xiao, 2009), online ADMM
(OL-ADMM) (Wang and Banerjee, 2012), online proximal
gradient descent ADMM (OPG-ADMM) (Ouyang et al.,
2013; Suzuki, 2013) and RDA-ADMM (Suzuki, 2013).
We also compared our method with batch ADMM (BatchADMM) in the artificial data sets. We used sub-batch with
size 50 for all the methods including ours (|Ik | = 50, but
|IK | could be less than 50). We employed the parameter
settings γ = 1/n and ρ = 0.13 . As for ηZ,I and ηB , we
used ηZ,I = 1.1σmax (ZI⊤ ZI ) and ηB = σmax (BB ⊤ ) + 1.
All of the experiments are classification problems with
structured sparsity. We employed the smoothed hinge loss:


(yi u ≥ 1),
0,
fi (u) = 21 − yi u,
(yi u < 0),

1
2
2 (1 − yi u) , (otherwise).

CPU time (s)

CPU time (s)

Figure 1. Excess empirical risk, exected loss on the test data and
test classification error averaged over 10 independent iteration
against CPU time in artificial data with n = 512, 5120. The error
bar indicates the standard deviation.

6.1. Artificial Data

Here we execute numerical experiments on artificial data
sets. The problem is a classification problem with overlapped group regularization as performed in Suzuki (2013).
We generated n input feature vectors {zi }ni=1 with dimension d = 32 × 32 = 1024 where each feature is generated from i.i.d. standard normal distribution. Then the true
weight vector w0 is generated as follows: First we generate
a random matrix which has non-zero elements on its first
column (distributed from i.i.d. standard normal) and zeros
on other columns, and vectorize the matrix to obtain w0 .
The training label yi is given by yi = sign(zi⊤ w0 + ǫi )
where ǫi is distributed from normal distribution with mean
0 and standard deviation 0.1.
The group regularization is given as ψ̃(x)
=
P32
P32
P
2
C( i=1 kX:,i k + j=1 kXj,: k + 0.01 × i,j Xi,j /2)
where X is the 32 × 32 matrix obtained by reshaping x.
The quadratic term is added to make the regularization
function strongly convex4 . Since there exist overlaps
2
All the experiments were carried out on Intel Core i7
2.93GHz with 8GB RAM.
3
In our experiments, ρ = 0.1 gave nice performances on all
datasets. Too large or too small rho does not give a proper performance, but ρ = 0.1 gave stable performances in our experiments.
4
Even if there is no quadratic term, our method converged with
almost the same speed.

between groups, the proximal operation can not be
straightforwardly computed (Jacob et al., 2009). To deal
with this regularization function in our frame-work, we
let B ⊤ x = [x; x](= [x⊤ x⊤ ]⊤ ), that is B = [Ip Ip ], and
P32
P32
′
ψ([x; x′ ]) = C( i=1 kX:,i k + j=1 kXj,:
k). Then we
⊤
can see that ψ̃(x) = ψ(B x) and the proximal operation
with respect to ψ is analytically obtained; indeed it is
easily checked that prox([q; q ′ ]|ψ) = [STC ′ (Q:,1 /(1 +
0.01C)); . . . ; STC ′ (Q:,32 /(1 + 0.01C)); STC ′ (Q′1,: /(1 +
0.01C)); . . . ; STC ′ (Q′32,: /(1
+
0.01C))]
where
STC (q) = q max(1−C/kqk, 0) and C ′ = C/(1+0.01C).
The original RDA requires a direct computation of the
proximal operation for the overlapped group penalty. To
compute that, we employed the dual formulation proposed
by Yuan et al. (2011).
We independently repeated the experiments 10
times and averaged the excess empirical risk
(FP (w(t) ) − minw FP (w)), the expected loss on the
test data (E(z,y) [f (y, z ⊤ w(t) )]) and the classification error
(E(z,y) [1{y 6= sign(z ⊤ w(t) )}). Figure 1 shows these three
values against CPU time with the standard deviation
for
√
n = 512 and n = 5120. We employed C1 = 0.1/ n.
We observe that the excess empirical risk of our method,

Stochastic Dual Coordinate Ascent with ADMM

a9a

Empirical Risk

Empirical Risk

20news

CPU time (s)

CPU time (s)

Test Loss

Test Loss

SDCA-ADMM, actually converges linearly while other
stochastic methods don’t show linear convergence. Although Batch-ADMM also shows linear convergence and
its convergence speed is comparable to SDCA-ADMM for
small sample situation (n = 512), SDCA-ADMM is much
faster than Batch-ADMM when the number of samples is
large (n = 5120). As for the classification error, existing stochastic methods also show nice performances despite the poor convergence of the empirical risk. On the
other hand, SDCA-ADMM rapidly converges to a stable
state and shows comparable or better classification accuracy than existing methods.
6.2. Real Data

ψ̃(w) = C1
+ 0.01 ×

Pp

i=1 |wi | + C2
Pp
(C1 i=1 |wi |2 +

P

(i,j)∈E

C2

P

|wi − wj |

(i,j)∈E

2

|wi − wj | ).

Now let F be |E|×p matrix where Fe,i = 1 and Fe,j = −1,
if (i, j) = e ∈ E, and Fe,i = 0 otherwise.
Then
Pp
by letting B ⊤ = [Ip ; F ] and ψ(u) = C1 i=1 |ui | +
P|E|
Pp
P|E|
C2 i=p+1 |ui |+0.01(C1 i=1 |ui |2 +C2 i=p+1 |ui |2 )
for u ∈ Rp+|E| , we have ψ̃(w) = ψ(B ⊤ w). Note that
the proximal operation with respect to ψ is just the softthresholding operation. In our experiments,
we employed
√
C2 = C1 |E|/p and C1 = 0.01/ n.
We computed the empirical risk on the training data, the
averaged loss on the test data, and the test classification
error (Figure 2). We observe that the empirical risk on the
training data of SDCA-ADMM converges much faster than
other methods. Although other methods also performs well
on the test loss and the classification error, SDCA-ADMM
still converges faster than existing methods with respect to
the two quantities measured on the test data.

CPU time (s)
OPG-ADMM
RDA-ADMM
OL-ADMM
SDCA-ADMM

Classification Error

CPU time (s)

Classification Error

Here we execute numerical experiments on real data; ‘20
Newsgroups’5 and ‘a9a’6 . ‘20 Newsgroups’ contains 100
dimensional 12,995 training samples and 3,247 test samples. ‘a9a’ contains 123 dimensional 32,561 training samples and 16,281 test samples. We constructed a similarity graph between features using graph Lasso and applied
graph guided regularization as in Ouyang et al. (2013).
That is, we applied graph Lasso to the training samples,
and obtain a sparse inverse variance-covariance matrix F̂ .
Based on the similarity matrix F̂ , we connect all index pairs
(i, j) with F̂i,j 6= 0 on edges. We denote by E the set of
edges. Then we impose the following graph guided regularization:

CPU time (s)

CPU time (s)

Figure 2. Empirical risk, average loss on the test data and test classification error averaged over 5 independent iteration against CPU
time in real data. The error bar indicates the standard deviation.

7. Conclusion
We proposed a new stochastic dual coordinate ascent technique with alternating direction method of multipliers. The
proposed method can be applied to wide range of regularization functions. Moreover, we proposed a mini-batch extension of our method. It is shown that, under some strong
convexity conditions, our method converges exponentially.
According to our analysis, the mini-batch method improves
the convergence rate if the input features don’t have strong
correlation between each other. The numerical experiments
showed that our method actually converges exponentially,
and the convergence is fast in terms of both empirical and
expected risk.
Future work includes that the determination of ηZ,I . In
Theorem 2, the exponential convergence is guaranteed if
ηZ,I >= {1 + 2γn(1 − 1/K)}σmax (ZI⊤ ZI ). However,
in our preliminary numerical experiments, an aggressive
method like the one suggested in Takáč et al. (2013) performed effectively in some data sets. Developing more sophisticated determination of ηZ,I (and G) would be a potentially promising future work.

5

Available at http://www.cs.nyu.edu/˜roweis/data.html. We
converted the four class classification task into binary classification by grouping category 1,2 and category 3,4 respectively.
6
Available
at
‘LIBSVM
data
sets’
http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets.

Acknowledgement TS was partially supported by
MEXT Kakenhi 25730013, and the Aihara Project, the
FIRST program from JSPS, initiated by CSTP.

Stochastic Dual Coordinate Ascent with ADMM

References
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein.
Distributed optimization and statistical learning via the
alternating direction method of multipliers. Foundations
and Trends in Machine Learning, 3:1–122, 2010.
W. Deng and W. Yin. On the global and linear convergence of the generalized alternating direction method of
multipliers. Technical report, Rice University CAAM
TR12-14, 2012.
J. Duchi and Y. Singer. Efficient online and batch learning
using forward backward splitting. Journal of Machine
Learning Research, 10:2873–2908, 2009.
D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems via finite-element
approximations. Computers & Mathematics with Applications, 2:17–40, 1976.
M. Hestenes. Multiplier and gradient methods. Journal of
Optimization Theory & Applications, 4:303–320, 1969.
L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with
overlap and graph lasso. In Proceedings of the 26th International Conference on Machine Learning, 2009.
N. Le Roux, M. Schmidt, and F. Bach. A stochastic gradient method with an exponential convergence rate for
strongly-convex optimization with finite training sets. In
Advances in Neural Information Processing Systems 25,
2013.
A. Nemirovskii and D. Yudin. Problem complexity and
method efficiency in optimization. John Wiley, New
York, 1983.
H. Ouyang, N. He, L. Q. Tran, and A. Gray. Stochastic
alternating direction method of multipliers. In Proceedings of the 30th International Conference on Machine
Learning, 2013.
M. Powell. A method for nonlinear constraints in minimization problems. In R. Fletcher, editor, Optimization,
pages 283–298. Academic Press, London, New York,
1969.
Z. Qin and D. Goldfarb. Structured sparsity via alternating direction methods. Journal of Machine Learning Research, 13:1435–1468, 2012.
A. Rakotomamonjy. Applying alternating direction method
of multipliers for constrained dictionary learning. Neurocomputing, 106:126–136, 2013.
R. T. Rockafellar. Convex Analysis. Princeton University
Press, Princeton, 1970.

R. T. Rockafellar. Augmented Lagrangians and applications of the proximal point algorithm in convex programming. Mathematics of Operations Research, 1:97–116,
1976.
M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite
sums with the stochastic average gradient. Technical report, 2013. arXiv:1309.2388.
S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14:567–
599, 2013a.
S. Shalev-Shwartz and T. Zhang. Accelerated mini-batch
stochastic dual coordinate ascent. In Advances in Neural
Information Processing Systems 26, 2013b.
S. Shalev-Shwartz and T. Zhang. Proximal stochastic dual coordinate ascent. Technical report, 2013c.
arXiv:1211.2717.
M. Signoretto, L. D. Lathauwer, and J. Suykens. Nuclear norms for tensors and their use for convex multilinear estimation. Technical Report 10-186, ESAT-SISTA,
K.U.Leuven, 2010.
T. Suzuki. Dual averaging and proximal gradient descent
for online alternating direction multiplier method. In
Proceedings of the 30th International Conference on
Machine Learning, 2013.
M. Takáč, A. Bijral, P. Richtárik, and N. Srebro. Mini-batch
primal and dual methods for SVMs. In Proceedings of
the 30th International Conference on Machine Learning,
2013.
R. Tomioka, T. Suzuki, K. Hayashi, and H. Kashima. Statistical performance of convex tensor decomposition. In
Advances in Neural Information Processing Systems 25,
2011.
H. Wang and A. Banerjee. Online alternating direction
method. In Proceedings of the 29th International Conference on Machine Learning, 2012.
L. Xiao. Dual averaging methods for regularized stochastic
learning and online optimization. In Advances in Neural
Information Processing Systems 23, 2009.
L. Yuan, J. Liu, and J. Ye. Efficient methods for overlapping group lasso. In Advances in Neural Information
Processing Systems 24, 2011.

