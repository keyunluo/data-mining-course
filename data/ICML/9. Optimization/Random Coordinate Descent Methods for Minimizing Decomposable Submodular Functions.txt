Random Coordinate Descent Methods for
Minimizing Decomposable Submodular Functions
Alina Ene
Department of Computer Science and DIMAP, University of Warwick
Huy L. Nguyen
Simons Institute, University of California, Berkeley

Abstract
Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have
used convex optimization techniques to obtain
very practical algorithms for minimizing functions that are sums of “simple” functions. In this
paper, we use random coordinate descent methods to obtain algorithms with faster linear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly
fewer iterations.

1. Introduction
Over the past few decades, there has been a significant
progress on minimizing submodular functions, leading
to several polynomial time algorithms for the problem
(Grötschel et al., 1981; Schrijver, 2000; Iwata, 2003; Fleischer & Iwata, 2003; Orlin, 2009). Despite this intense focus, the running times of these algorithms are high-order
polynomials in the size of the data and designing faster
algorithms remains a central and challenging direction in
submodular optimization.
At the same time, technological advances have made it possible to capture and store data at an ever increasing rate and
level of detail. A natural consequence of this “big data”
phenomenon is that machine learning applications need to
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

A.E NE @ DCS . WARWICK . AC . UK
HLNGUYEN @ CS . PRINCETON . EDU

cope with data that is quite large and is growing at a fast
pace. Thus there is an increasing need for algorithms that
are fast and scalable.
The general purpose algorithms for submodular minimization are designed to provide worst-case guarantees even in
settings where the only structure that one can exploit is submodularity. At the other extreme, graph cut algorithms are
very efficient but they cannot handle more general submodular functions. In many applications, the functions strike a
middle ground between these two extremes and it is becoming increasingly more important to use their special structure to obtain significantly faster algorithms.
Following (Kolmogorov, 2012; Stobbe & Krause, 2010;
Jegelka et al., 2013; Nishihara et al., 2014a), we consider the problem of minimizing decomposable submodular functions that can be expressed as a sum of simple functions. We use the term simple to refer to functions F for
which there is an efficient algorithm for minimizing F + w,
where w is a linear function. We assume that we are given
black-box access to these minimization procedures for simple functions.
Decomposable functions are a fairly rich class of functions
and they arise in several applications in machine learning
and computer vision. For example, they model higherorder potential functions for MAP inference in Markov random fields, the cost functions in SVM models for which
the examples have only a small number of features, and the
graph and hypergraph cut functions in image segmentation.
The recent work of (Jegelka et al., 2013; Kolmogorov,
2012; Stobbe & Krause, 2010) has developed several algorithms with very good empirical performance that exploit
the special structure of decomposable functions. In particular, (Jegelka et al., 2013) have shown that the problem
of minimizing decomposable submodular functions can be
formulated as a distance minimization problem between
two polytopes. This formulation, when coupled with powerful convex optimization techniques such as gradient de-

scent or projection methods, yields algorithms that are very
fast in practice and very simple to implement (Jegelka et al.,
2013).

w 2 Rn .

In this paper, we consider the problem of minimizing
Pra submodular function F : 2V ! R of the form F = i=1 Fi ,
where each function Fi is a simple submodular set function:
r
X
min F (A) ⌘ min
Fi (A).
(DSM)

On the theoretical side, the convergence behaviour of these
methods is not very well understood. Very recently, Nishihara et al. (2014a) have made a significant progress in this
direction. Their work shows that the classical alternating projections method, when applied to the distance minimization formulation, converges at a linear rate.

A✓V

A✓V

i=1

We assume without loss of generality that the function F
is normalized, i.e., F (;) = 0. Additionally, we assume
we are given black-box access to oracles for minimizing
Fi + w for each function Fi in the decomposition and each
w 2 Rn .

Our contributions. In this work, we use random coordinate descent methods in order to obtain algorithms for minimizing decomposable submodular functions with faster
convergence rates and cheaper iteration costs. We analyze a
standard and an accelerated random coordinate descent algorithm and we show that they achieve linear convergence
rates. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations
and they are faster by a factor equal to the number of simple
functions. Moreover, our accelerated algorithm converges
in a much smaller number of iterations. We experimentally evaluate our algorithms on image segmentation tasks
and we show that they perform very well and they converge
much faster than the alternating projection method.

The base polytope B(F ) of F is defined as follows.

B(F ) = {w 2 Rn | w(A)  F (A) for all A ✓ V,
w(V ) = F (V )}.

The discrete problem (DSM)1 admits an exact convex programming relaxation based on the Lovász extension of a
submodular function. The Lovász extension f of F can be
written as the support function of the base polytope B(F ):

Submodular minimization. The first polynomial time
algorithm for submodular optimization was obtained by
Grötschel et al. (1981) using the ellipsoid method. There
are several combinatorial algorithms for the problem
(Schrijver, 2000; Iwata, 2003; Fleischer & Iwata, 2003;
Orlin, 2009). Among the combinatorial methods, Orlin’s algorithm (2009) achieves the best time complexity
of O(n5 T + n6 ), where n is the size of the ground set
and T is the maximum amount of time it takes to evaluate the function. Several algorithms have been proposed
for minimizing decomposable submodular functions (Stobbe & Krause, 2010; Kolmogorov, 2012; Jegelka et al.,
2013; Nishihara et al., 2014a). Stobbe and Krause (2010)
use gradient descent methods with sublinear convergence
rates for minimizing sums of concave functions applied to
linear functions. Nishihara et al. (2014a) give an algorithm
based on alternating projections that achieves a linear convergence rate.

f (x) = max hw, xi 8x 2 Rn .
w2B(F )

Even though the base polytope B(F ) has exponentially
many vertices, the Lovász extension f can be evaluated efficiently using the greedy algorithm of Edmonds (see for
example (Schrijver, 2003)). Given any point x 2 Rn , Edmonds’ algorithm evaluates f (x) using O(n log n + nT )
time, where T is the time needed to evaluate the submodular function F .
Lovász showed that a set function F is submodular if and
only if its Lovász extension f is convex (Lovász, 1983).
Thus we can relax the problem of minimizing F to the following non-smooth convex optimization problem:
min n f (x) ⌘ min n

x2[0,1]

x2[0,1]

r
X

fi (x),

i=1

where fi is the Lovász extension of Fi .

1.1. Preliminaries and Background

The relaxation above is exact. Given a fractional solution
x to the Lovász Relaxation, the best threshold set of x has
cost at most f (x).

Let V be a finite ground set of size n; without loss of genn
erality, V = {1, 2, . . . , n}. We view
P each point w 2 R
as a modular set function w(A) = i2A wi on the ground
set V .

An important drawback of the Lovász relaxation is that its
objective function is not smooth. Following previous work
(Jegelka et al., 2013; Nishihara et al., 2014a), we consider a
proximal version of the problem (k· k denotes the `2 -norm):

A set function F : 2V ! R is submodular if F (A) +
F (B) F (A\B)+F (A[B) for any two sets A, B ✓ V .
A set function Fi : 2V ! R is simple if there is a fast
subroutine for minimizing Fi + w for any modular function
2

1
DSM stands for decomposable submodular function minimization.

RCDM Algorithm for (Prox-DSM)
hhWe can take the initial point y0 to be 0ii
(1)
(r)
Start with y0 = (y0 , . . . , y0 ) 2 Y
In each iteration k (k 0)
Pick an index ik 2 {1, 2, . . . , r} uniformly at random
hhUpdate the block⇣ iDk ii
E
(ik )
yk+1

+

L ik
2

y

In the remainder of this section, we analyze the convergence rate of the RCDM algorithm. We emphasize that the
objective function of (Prox-DSM) is not strongly convex
and thus we cannot use as a black-box Nesterov’s analysis of the RCDM method for minimizing strongly convex functions. Instead, we exploit the special structure of
the problem to achieve convergence guarantees that match
the rate achievable for strong convex objectives with strong
convexity parameter 1/(n2 r). Our analysis shows that the
RCDM algorithm is faster by a factor of r (same convergence rate but faster iterations) than the alternating projections algorithm from (Nishihara et al., 2014a).

(i )
yk k

rik g(yk ), y

arg min

y2B(Fik )

implement and
⇣ it uses oracles
⌘ for problems of the form
2
miny2B(Fi ) hy, ai + kyk , where i 2 [r] and a 2 Rn .
Since each function Fi is simple, we have such oracles that
are very efficient.

(i )

yk k

2

⌘

Figure 1. Random block coordinate descent method for (ProxDSM). It finds a solution to (Prox-DSM) given access to an oracle
for miny2B(Fi ) hy, ai + kyk2 .

✓

◆
1
2
min
⌘ minn
fi (x) +
kxk .
x2Rn
x2R
2r
i=1
(Proximal)
Given an optimal solution x to the proximal problem
2
minx2Rn f (x)+ 12 kxk , we can construct an optimal solution to the discrete problem (DSM) by thresholding x at
zero; more precisely, the set {v 2 V : x(v) 0} is an optimal solution to (DSM) (Proposition 8.6 in (Bach, 2011)).
Lemma 1 ((Jegelka et al., 2013)). The dual of the proximal
problem
◆
r ✓
X
1
2
min
fi (x) +
kxk
x2Rn
2r
i=1
1
2
f (x) + kxk
2

◆

r ✓
X

Outline of the analysis: Our analysis has two main components. First, we build on the work of (Nishihara et al.,
2014a) in order to prove a key theorem (Theorem 2). This
theorem exploits the special structure of the (Prox-DSM)
problem and it allows us to overcome the fact that the
objective function of (Prox-DSM) is not strongly convex.
Second, we modify Nesterov’s analysis of the RCDM algorithm for minimizing strongly convex functions and we replace the strong convexity guarantee by the guarantee given
by Theorem 2.
We start by introducing some notation; for the most part,
we follow the notation of (Nesterov,
2012) and (Nishihara
Nr
n
et al., 2014a). Let Rnr =
R
. We write a vector
i=1
(i)
y 2 Rnr as y = (y (1) , . . . , y (r) ), where
Nr each block y
is an n-dimensional vector. Let Y =
i=1 B(Fi ) be the
constraint set of (Prox-DSM). Let g : Rnr ! R be the
Pr
(i) 2
objective function of (Prox-DSM): g(y) =
.
i=1 y
We use rg to denote the gradient of g, i.e., the (nr)dimensional vector of partial derivatives. For each i 2
{1, . . . , r}, we use ri g(y) 2 Rn to denote the i-th block
of coordinates of rg(y).

is the problem

max

y (1) 2B(F1 ),...,y (r) 2B(Fr )

r
1 X (i)
y
2 i=1

2

.

The
and dual variables are linked as x
Prprimal
(i)
y
.
i=1

=

We write the dual proximal problem in the following equivalent form:
min

y (1) 2B(F1 ),...,y (r) 2B(Fr )

r
X

Let S 2 Rn⇥nr be the following matrix:

2

y (i)

.

i
1 h
S = p In In · · · In .
r | {z }

(Prox-DSM)

i=1

r times

It follows from the discussion above that, given an optimal solution y = (y (1) , . . . , y (r) ) to (Prox-DSM), we
can recover
optimal solution to (DSM) by thresholding
Pr an (i)
x=
y
at zero.
i=1

2

Note that g(y) = r kSyk and rg(y) = 2rS T Sy. Additionally, for each i 2 {1, 2, . . . , r}, ri g is Lipschitz continuous with constant Li = 2:

2. Random Coordinate Descent Algorithm

In this section, we give an algorithm for the problem (ProxDSM) that is based on the random coordinate gradient descent method (RCDM) of (Nesterov, 2012). The algorithm is given in Figure 1. The algorithm is very easy to

kri g(x)

ri g(y)k  Li x(i)

y (i) ,

for all vectors x, y 2 Rnr that differ only in block i.
3

(1)

Our first step is to prove the following key theorem that
builds on the work of (Nishihara et al., 2014a).

Theorem 2. Let y 2 Y be a feasible solution to (ProxDSM). Let y ⇤ be an optimal solution to (Prox-DSM) that
minimizes ky y ⇤ k. We have
1
ky
nr

y ⇤ )k

kS(y

Theorem 2 now follows from Theorem 3.
In the remainder of this section, we use Nesterov’s analysis
(Nesterov, 2012) in conjunction with Theorem 2 in order to
show that the RCDM algorithm converges at a linear rate.
Recall that E is the set of all optimal solutions to (ProxDSM).

y⇤ k .

The proof of Theorem 2 uses the following key result from
(Nishihara et al., 2014b). We will need the following definitions from (Nishihara et al., 2014b).

Theorem 4. After (k + 1) iterations of the RCDM algorithm, we have
⇥
⇤
E d(yk , E)2 + g(yk+1 ) g(y ⇤ ) 
✓
◆k+1
2
1
d(y0 , E)2 + g(y0 ) g(y ⇤ ) ,
n2 r 2 + r

Let d(K1 , K2 ) = inf {kk1 k2 k : k1 2 K1 , k2 2 K2 } be
the distance between sets K1 and K2 . Let P and Q be two
closed convex sets in Rd . Let E ✓ P and H ✓ Q be the
sets of closest points

where y ⇤ 2 E is an arbitrary optimal solution to (ProxDSM).

E = {p 2 P : d(p, Q) = d(P, Q)} ,

H = {q 2 Q : d(q, P) = d(P, Q)} .

We devote the rest of this section to the proof of Theorem 4.
We recall the following well-known lemma, which we refer
to as the first-order optimality condition.

Since P and Q are convex, for each point in p 2 E, there
is a unique point q 2 H such that d(p, q) = d(P, Q) and
vice versa. Let v = ⇧Q P 0, where ⇧Q P is the projection
operator onto Q P; note that H = E+v. Let Q0 = Q v;
Q0 is a translated version of Q and it intersects P at E. Let
⇤ =

sup
x2(P[Q0 )\E

Lemma 5 (Theorem 2.2.5 in (Nesterov, 2004)). Let f :
Rd ! R be a differentiable convex function and let Q ✓
Rd be a closed convex set. A point x⇤ 2 Rd is a solution to
the problem minx2Q f (x) if and only if

d(x, E)
.
max {d(x, P), d(x, Q0 )}

hrf (x⇤ ), x

By combining Corollary 5 and Proposition 11 from (Nishihara et al., 2014b), we obtain the following theorem.

(i )

k
It follows from the first-order optimality condition for yk+1
that, for any z 2 B(Fik ),
D
⇣
⌘
E
(ik )
(i )
(ik )
rik g(yk ) + Lik yk+1
yk k , z yk+1
0. (2)

Now we are ready to prove Theorem 2. Let
Or
P=
B(Fi ) = Y,
n i=1
o
Xr
Q = y 2 Rnr :
y (i) = 0 = {y 2 Rnr : Sy = 0} .

We show in the supplement that

g(yk+1 ) = g(yk )+
D
E L
i
(ik )
(i )
(ik )
rik g(yk ), yk+1
yk k + k yk+1
2

i=1

We define Q0 and ⇤ as above.

Let y and y ⇤ be the two points in the statement of the theorem. Note that y 2 P and y ⇤ 2 E, since E is the set of
all optimal solutions to (Prox-DSM). We may assume that
y 2
/ E, since otherwise the theorem trivially holds. Since
y 2 P \ E, we have

⇤

d(y, E)
.
d(y, Q0 )

ky y ⇤ k
.
kS(y y ⇤ )k

(i )

yk k

2

. (3)

Let y ⇤ = arg miny2E ky yk k be the optimal solution
that is closest to yk . Using (2) and (3), we show in the
supplement that
2

kyk+1

y⇤ k

 kyk

y⇤ k +

2

2
(g(yk+1 )
L ik

Since y ⇤ is an optimal solution that is closest to y, we have
d(y, E) = ky y ⇤ k. Using the fact that the rows of S form
a basis for the orthogonal complement of Q, we can show
that d(y, Q0 ) = kS(y y ⇤ )k.
Therefore

0

for all x 2 Q.

Theorem 3 N
((Nishihara et al., 2014a)). If P is the
r
polyhedron P i=1 B(Fi ) and Q is the polyhedron
r
nr
(i)
y2R :
= 0 , we have ⇤  nr.
i=1 y

⇤

x⇤ i

4

2 D
rik g(yk ), (y ⇤ )(ik )
L ik
g(yk )) .

(i )

yk k

E

(4)

If we rearrange the terms of the inequality (4), take expectation over ik , and substitute Lik = 2, we obtain
h
i
2
Eik kyk+1 y ⇤ k + g(yk+1 ) g(y ⇤ )

2

y ⇤ k + g(yk )

 kyk

1
g(y ⇤ ) + hrg(yk ), y ⇤
r

yk i.

(5)

We can upper bound hrg(yk ), y ⇤ yk i as follows.
⌦
↵
hrg(yk ), y ⇤ yk i = 2r S T Syk , y ⇤ yk
⌦
↵
= r S T Syk + S T Sy ⇤ , y ⇤ yk +
⌦
↵
r S T Syk S T Sy ⇤ , y ⇤ yk
⌦
↵
2
= r S T Syk + S T Sy ⇤ , y ⇤ yk
r kS(yk y ⇤ )k
= r hS(yk + y ⇤ ), S(y ⇤
= (g(y ⇤ )

g(yk ))

 (g(y ⇤ )

g(yk ))

(i)

2

2

y ⇤ )k

r kS(yk
1
kyk
n2 r

2

y⇤ k .

(By Theorem 2)
(6)

On the first and fifth lines, we have used the fact that
2
rg(z) = 2rS T Sz and g(z) = r kSzk for any z 2 Rnr .
On the last line, we have used Theorem 2.
Since y ⇤ is an optimal solution to (Prox-DSM), the firstorder optimality condition gives us that
hrg(y ⇤ ), y ⇤

yk i = 2rhS T Sy ⇤ , y ⇤

(7)

yk i  0.

Using the inequality above, we can also upper bound
hrg(yk ), y ⇤ yk i as follows.
hrg(yk ), y ⇤
T

⇤

yk i = 2rhS T Syk , y ⇤

= 2rhS Sy , y

⇤

= 2rhS T Sy ⇤ , y ⇤
(7)





T

yk i + 2rhS Syk
yk i

2r kS(yk y ⇤ )k
2
2
kyk y ⇤ k .
n2 r

By taking

2

n2 r+1

2r kS(yk

yk i

S T Sy ⇤ , y ⇤

yk i

2

y ⇤ )k

2

(By Theorem 2)

⇣
⇥ (6) + 1

2

n2 r+1

⌘

(8)

⇥ (8), we obtain

hrg(yk ), y ⇤ yk i 
⇣
2
g(yk ) g(y ⇤ ) + kyk
n2 r + 1

By (5) and (9),
h
i
2
Eik kyk+1 y ⇤ k + g(yk+1 ) g(y ⇤ )
✓
◆⇣
2
 1
g(yk ) g(y ⇤ ) + kyk
n2 r 2 + r
2

2

y⇤ k

⌘

t+zk 2B(Fik )
(i)
(i)
zk + t k
(i)
1 r✓k (i)
u
t
2
p 4k 2 ✓2k k
✓k +4✓k ✓k
✓k+1 =
2
Return ✓k2 uk+1 + zk+1
(i)
zk+1
(i)
uk+1

y ⇤ )k

r kS(yk

yk )i

APPROX algorithm applied to (Prox-DSM)
(1)
(r)
Start with z0 = (z0 , . . . , z0 ) 2 Y, ✓0 = 1r , u0 = 0
In each iteration k (k 0)
Generate a random set of blocks Rk where each
block is included independently with probability 1r
uk+1
uk , zk+1
zk
For each i 2 Rk
⌦
↵
(i)
2
tk
arg min ri g ✓k2 uk + zk , t + 2r✓k ktk

Figure 2. The APPROX algorithm of (Fercoq & Richtárik, 2013)
applied to (Prox-DSM). It finds a solution to (Prox-DSM) given
access to an oracle for miny2B(Fi ) hy, ai + kyk2 .

ACDM Algorithm for (Prox-DSM)
hhWe can take the initial point y0 to be 0ii
(1)
(r)
Start with y0 = (y0 , . . . , y0 ) 2 Y
In each epoch ` (` 0)
Run the algorithm in Figure 2 for (4nr3/2 + 1)
iterations with y` as its starting point (z0 = y` )
Let y`+1 be the vector returned by the algorithm
Figure 3. Accelerated block coordinate descent method for (ProxDSM). It finds a solution to (Prox-DSM) given access to an oracle
for miny2B(Fi ) hy, ai + kyk2 .

By taking expectation over ⇠ = (i1 , . . . , ik ), we get
⇥
⇤
E⇠ d(yk+1 , E)2 + g(yk+1 ) g(y ⇤ )
✓
◆k+1
2
 1
d(y0 , E)2 + g(y0 ) g(y ⇤ ) .
n2 r 2 + r
Therefore the proof of Theorem 4 is complete.

(9)

.

2

y⇤ k

⌘

.

Note that d(yk+1 , E)2  kyk+1 y ⇤ k and d(yk , E)2 =
2
kyk y ⇤ k . Therefore
⇥
⇤
Eik d(yk+1 , E)2 + g(yk+1 ) g(y ⇤ )
✓
◆
2
 1
d(yk , E)2 + g(yk ) g(y ⇤ ) .
n2 r 2 + r
5

3. Accelerated Coordinate Descent Algorithm
In this section, we give an accelerated random coordinate
descent (ACDM) algorithm for (Prox-DSM). The algorithm uses the APPROX algorithm of Fercoq and Richtárik
(2013) as a subroutine. The APPROX algorithm (Algorithm 2 in (Fercoq & Richtárik, 2013)), when applied to
the (Prox-DSM) problem, yields the algorithm in Figure 2.
The ACDM algorithm runs in a sequence of epochs (see
Figure 3). In each epoch, the algorithm starts with the solution of the previous epoch and it runs the APPROX algorithm for ⇥(nr3/2 ) iterations. The solution constructed
by the APPROX algorithm will be the starting point of the
next epoch. Note that, for each i, the gradient ri g(y) =

✓✓

P
2 j y (j) can be easily maintained at a cost of O(n) per
block update, and thus the iteration cost is dominated by
the time to compute the projection.

E[g(y`+1 )

1

g(y )] 

2`+1

(g(y0 )

g(y )).

g(y` ) = g(y ⇤ ) + hrg(y ⇤ ), y` y ⇤ i+
Z 1
hrg(y ⇤ + t(y` y ⇤ )) rg(y ⇤ ), y`
0

h
i
2
E [g (x + hR )] = E r kS(x + hR )k
h
i
2
2
= E r kSxk + r kShR k + 2r hSx, ShR i
h
⌦
↵i
2
2
= E r kSxk + r kShR k + 2r S T Sx, hR
2
3
2
r
X
(i)
= E 4g(x) +
h
+ hrg(x), hR i5

g(y ⇤ ) +
= g(y ⇤ ) +

R

+

i6=j

1
hrg(x), hi
r
r
2 X (i)
 g(x) +
h
r i=1

2

g(y ⇤ )] 

1
+ hrg(x), hi.
r

and hence

(k

Z

1

0

hrg(y ⇤ + t(y`

1
0

y ⇤ ))

rg(y ⇤ ), y`

y ⇤ idt

2

y ⇤ )k dt

2tr kS(y`
2

2

y ⇤ k  n2 r(g(y` )

ky`

g(y ⇤ )),

2n2 r + 1
(g(y` ) g(y ⇤ ))
(2nr1/2 + 1)2
1
 (g(y` ) g(y ⇤ )).
2
Let ⇠ = (⇠0 , . . . , ⇠` ) be the random choices made during
the epochs 0 to `. We have

⇤

4r2
·
1 + 2r)2

Z

y ⇤ idt

Therefore

E⇠` [g(y`+1 )

Lemma 7 together with Theorem 3 in (Fercoq & Richtárik,
2013) give us the following theorem.
Theorem 8 (Theorem 3 of (Fercoq & Richtárik, 2013)).
Consider iteration k of the APPROX algorithm (see Figure 2). Let yk = ✓k2 uk+1 + zk+1 . Let y ⇤ =
arg miny2E ky z0 k is the optimal solution that is closest to z0 . We have
E[g(yk )

.

= g(y ⇤ ) + r kS(y` y ⇤ )k
1
2
g(y ⇤ ) + 2 ky` y ⇤ k .
(By Theorem 2)
n r
In the second line, we have used the first-order optimality
condition for y ⇤ (Lemma 5). In the last line, we have used
Theorem 2.

i=1

2

y k

◆

We also have

1
2
2
hrg(x), hi + khk .
r
r

r
1 X (i) (j)
1 X (i)
hh
,
h
i
+
h
r2
r i=1

g(y )) + 2 kz0

⇤ 2

4r2
E⇠` [g(y`+1 ) g(y ⇤ )] 
·
(4nr3/2 + 2r)2
✓✓
◆
◆
1
⇤
⇤ 2
1
(g(y` ) g(y )) + 2 ky` y k
r
⇣
⌘
1
⇤
⇤ 2

g(y
)
g(y
)
+
2
ky
y
k
.
`
`
(2nr1/2 + 1)2

Proof: We have

= g(x) +

(g(z0 )

⇤

Consider an epoch `. Let y`+1 be the solution constructed
by the APPROX algorithm after 4nr3/2 +1 iterations, starting with y` . Let y ⇤ = arg miny2E ky y` k be the optimal solution that is closest to y` . Let ⇠` denote the random
choices made during epoch `. By Theorem 8,

⇤

In the following lemma, we show that the objective function of (Prox-DSM) satisfies Assumption 1 in (Fercoq &
Richtárik, 2013) and thus the convergence analysis given
in (Fercoq & Richtárik, 2013) can be applied to our setting.
Lemma 7. Let R ✓ {1, 2, . . . , r} be a random subset of coordinate blocks with the property that each i 2
{1, 2, . . . , r} is in R independently at random with probability 1/r. Let x and h be two vectors in Rnr . Let hR be
the vector in Rnr such that (hR )(i) = h(i) for each block
i 2 R and (hR )(i) = 0 otherwise. We have
E [g (x + hR )]  g(x) +

1

◆

Proof: It follows from Lemma 7 that the objective function g of (Prox-DSM) and the random blocks Rk used by
the APPROX algorithm satisfy Assumption 1 in (Fercoq
& Richtárik, 2013) with ⌧ = 1 and ⌫i = 4 for each
i 2 {1, 2, . . . , r}. Thus we can apply Theorem 3 in (Fercoq
& Richtárik, 2013).
⇤

In the remainder of this section, we use the analysis of (Fercoq & Richtárik, 2013) together with Theorem 2 in order to
show that the ACDM algorithm converges at a linear rate.
We follow the notation used in Section 2.
Theorem 6. After ` epochs of the ACDM algorithm (equivalently, (4nr3/2 + 1)` iterations), we have
⇤

1
r

g(y ⇤ )] 

E⇠ [g(y`+1 )

6

g(y ⇤ )] 

1
2`+1

(g(y0 )

g(y ⇤ )).

This completes the proof of Theorem 6 and the convergence analysis for the ACDM algorithm.

(a) Smooth gaps - Octopus

(b) Smooth gaps - Penguin

(c) Smooth gaps - Plane

(d) Smooth gaps - Small plant

(e) Discrete gaps - Octopus

(f) Discrete gaps - Penguin

(g) Discrete gaps - Plane

(h) Discrete gaps - Small plant

(i) Penguin

Figure 4. Comparison of the convergence of the three algorithms (UCDM, ACDM, AP) on four image segmentation instances.

(a) ACDM 1
⌫s = 8.74 · 106
⌫d = 1.12 · 105

(b) ACDM 20
⌫s = 7.65 · 106
⌫d = 8.86 · 104

(c) ACDM 100
⌫s = 1.78 · 106
⌫d = 1.65 · 104

(d) AP 1
⌫s = 8.04 · 106
⌫d = 1.06 · 105

(e) AP 20
⌫s = 7.65 · 106
⌫d = 1.05 · 105

(f) AP 100
⌫s = 6.72 · 106
⌫d = 8.31 · 104

Figure 5. Penguin segmentation results for the fastest (ACDM) and slowest (AP) algorithms, after 1, 20, and 100 projections. The ⌫s
and ⌫d values are the smooth and discrete duality gaps.

7

4. Experiments

Since the ACDM algorithm is randomized, we have run the
algorithm several times. We have found that the difference
in the duality gaps between different runs is very small and
we have chosen to report the results of a single run instead
of the averages.

Algorithms. We empirically evaluate and compare the following algorithms: the RCDM described in Section 2, the
ACDM described in Section 3, and the alternating projections (AP) algorithm of (Nishihara et al., 2014a). The AP
algorithm solves the following best approximation problem
that is equivalent to (Prox-DSM):
min

a2A,y2Y

ka

2

yk

(1) (2)
(r)
nr
where A =
Nr (a , a , . . . , a ) 2 R :
and Y = i=1 B(Fi ).

Function decomposition: We partition the edges of the grid
into a small number of matchings and we decompose the
function using the cut functions of these matchings. Note
that it is straightforward to project onto the base polytopes
of such functions using a sequence of projections onto line
segments.

(Best-Approx)
Pr

i=1

a(i) = 0

Duality gaps: We evaluate the convergence behaviours
of the algorithms using the following measures. Let y
be a feasible solution to the dual of the
probPr proximal
(i)
lem (Proximal). The solution x =
is a feai=1 y
sible solution for the proximal problem. We define the
smooth duality gap to be the difference between the objective values
⇣ of the primal⌘solution
⇣ x and the
⌘ dual solution
2
2
r
y: ⌫s = f (x) + 12 kxk
kSyk
. We use the
2
pool adjacent violators algorithm to search for an improvement to the smooth duality gap; we use the same approach
as the one described in (Jegelka et al., 2013). Additionally,
we compute a discrete duality gap for the discrete problem
(DSM) and the dual of its Lovász relaxation; the latter is the
problem maxz2B(F ) (z) (V ), where (z) = min {z, 0}
applied elementwise (Jegelka et al., 2013).
Pr The(i)best level
set Sx of the proximal solution x =
is a soi=1 y
lution to
the
discrete
problem
(DSM).
The
solution
z =
Pr
x = i=1 y (i) is a feasible solution for the dual of the
Lovász relaxation. We define the discrete duality gap to be
the difference between the objective values of these solutions: ⌫d (x) = F (Sx ) ( x) (V ).

The AP algorithm starts with a point a0 2 A and it iteratively constructs a sequence {(ak , yk )}k 0 by projecting
onto A and Y: yk = ⇧Y (ak ), ak+1 = ⇧A (yk ).
⇧K (· ) is the projection operator onto K, that is, ⇧K (x) =
arg minz2K kx zk. Since A is a subspace, it is straightforward to project onto A. The projection onto Y can be
implemented using the oracles for the projections ⇧B(Fi )
onto the base polytopes of the functions Fi .
For all three algorithms, the iteration cost is dominated
by the cost of projecting onto the base polytopes B(Fi ).
Therefore the total number of such projections is a suitable measure for comparing the algorithms. In each iteration, the RCDM algorithm performs a single projection for
a random block i and the ACDM algorithm performs a single projection in expectation. The AP algorithm performs
r projections in each iteration, one for each block.
Image Segmentation Experiments. We evaluate the algorithms on graph cut problems that arise in image segmentation or MAP inference tasks in Markov Random Fields.
Our experimental setup is similar to that of (Jegelka et al.,
2013). We set up the image segmentation problems on a
8-neighbor grid graph with unary potentials derived from
Gaussian Mixture Models of color features (Rother et al.,
2004). The weight of a graph edge (i, j) between pixels i
2
and j is a function of exp( kvi vj k ), where vi is the
RGB color vector of pixel i. The optimization problem that
we solve for each segmentation task is a cut problem on the
grid graph.

We evaluated the algorithms on four image segmentation
instances2 (Jegelka & Bilmes, 2011; Rother et al., 2004).
Figure 4 shows the smooth and discrete duality gaps on the
four instances. Figure 5 shows some segmentation results
for one of the instances.
Acknowledgements. We thank Stefanie Jegelka for providing us with some of the data used in our experiments.

Remarks on the ACDM algorithm: We emphasize that, in
our experiments, the number of iterations is smaller than
the size of an epoch of the ACDM algorithm, and thus
there are no restarts. We have run experiments where we
restarted the ACDM algorithm after a much smaller number of iterations, and we have found that this approach leads
to a slower convergence rate.
Our implementation of the ACDM algorithm uses random
permutations of the blocks instead of picking a block independently and uniformly at random in each iteration.

2

8

The data is available at http://melodi.ee.
washington.edu/˜jegelka/cc/index.html
and
http://research.microsoft.com/en-us/um/
cambridge/projects/visionimagevideoediting/
segmentation/grabcut.htm

References

Orlin, James B. A faster strongly polynomial time algorithm for submodular function minimization. Mathematical Programming, 118(2):237–251, 2009.

Bach, Francis. Learning with submodular functions:
A convex optimization perspective. ArXiv preprint
arXiv:1111.6453, 2011.

Rockafellar, R Tyrrell. Convex analysis. Number 28
in Princeton Mathematical Series. Princeton university
press, 1970.

Fercoq, Olivier and Richtárik, Peter. Accelerated, parallel and proximal coordinate descent. ArXiv preprint
arXiv:1312.5799, 2013.

Rother, Carsten, Kolmogorov, Vladimir, and Blake, Andrew. Grabcut: Interactive foreground extraction using iterated graph cuts. ACM Transactions on Graphics
(TOG), 23(3):309–314, 2004.

Fleischer, Lisa and Iwata, Satoru. A push-relabel framework for submodular function minimization and applications to parametric optimization. Discrete Applied Mathematics, 131(2):311–322, 2003.

Schrijver, Alexander. A combinatorial algorithm minimizing submodular functions in strongly polynomial time.
Journal of Combinatorial Theory, Series B, 80(2):346–
355, 2000.

Grötschel, Martin, Lovász, László, and Schrijver, Alexander. The ellipsoid method and its consequences in combinatorial optimization. Combinatorica, 1(2):169–197,
1981.

Schrijver, Alexander. Combinatorial optimization: polyhedra and efficiency, volume 24. Springer Science &
Business Media, 2003.

Iwata, Satoru. A faster scaling algorithm for minimizing
submodular functions. SIAM Journal on Computing, 32
(4):833–840, 2003.

Stobbe, Peter and Krause, Andreas. Efficient minimization
of decomposable submodular functions. In Advances
in Neural Information Processing Systems (NIPS), pp.
2208–2216, 2010.

Jegelka, Stefanie and Bilmes, Jeff. Submodularity beyond
submodular energies: coupling edges in graph cuts. In
Computer Vision and Pattern Recognition (CVPR), pp.
1897–1904. IEEE, 2011.
Jegelka, Stefanie, Bach, Francis, and Sra, Suvrit. Reflection methods for user-friendly submodular optimization.
In Advances in Neural Information Processing Systems
(NIPS), pp. 1313–1321, 2013.
Kolmogorov, Vladimir. Minimizing a sum of submodular functions. Discrete Applied Mathematics, 160(15):
2246–2258, 2012.
Lovász, László. Submodular functions and convexity. In
Mathematical Programming The State of the Art, pp.
235–257. Springer, 1983.
Nesterov, Yurii. Introductory lectures on convex optimization: A basic course, volume 87. Springer, 2004.
Nesterov, Yurii. Efficiency of coordinate descent methods
on huge-scale optimization problems. SIAM Journal on
Optimization, 22(2):341–362, 2012.
Nishihara, Robert, Jegelka, Stefanie, and Jordan, Michael I.
On the convergence rate of decomposable submodular
function minimization. In Advances in Neural Information Processing Systems (NIPS), pp. 640–648, 2014a.
Nishihara, Robert, Jegelka, Stefanie, and Jordan, Michael I.
On the convergence rate of decomposable submodular
function minimization. ArXiv preprint arXiv:1406.6474,
2014b.

9

