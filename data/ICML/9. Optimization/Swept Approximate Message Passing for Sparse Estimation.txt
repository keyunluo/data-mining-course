Swept Approximate Message Passing for Sparse Estimation

Andre Manoel
AMANOEL @ IF. USP. BR
Institute of Physics, University of São Paulo, R. do Matão 187, São Paulo, SP 05508-090, Brazil
Florent Krzakala
KRZAKALA @ ENS . FR
Université Pierre et Marie Curie and École Normale Supérieure, 24 rue Lhomond, 75005 Paris, France
Eric W. Tramel
École Normale Supérieure, 24 rue Lhomond, 75005 Paris, France

ERIC . TRAMEL @ LPS . ENS . FR

Lenka Zdeborová
LENKA . ZDEBOROVA @ GMAIL . COM
Institut de Physique Théorique, CEA Saclay, and CNRS URA 2306, 91191 Gif-sur-Yvette, France

Abstract
Approximate Message Passing (AMP) has been
shown to be a superior method for inference
problems, such as the recovery of signals from
sets of noisy, lower-dimensionality measurements, both in terms of reconstruction accuracy
and in computational efficiency. However, AMP
suffers from serious convergence issues in contexts that do not exactly match its assumptions.
We propose a new approach to stabilizing AMP
in these contexts by applying AMP updates to individual coefficients rather than in parallel. Our
results show that this change to the AMP iteration can provide expected, but hitherto unobtainable, performance for problems on which the
standard AMP iteration diverges. Additionally,
we find that the computational costs of this swept
coefficient update scheme is not unduly burdensome, allowing it to be applied efficiently to signals of large dimensionality.

1. Introduction
Belief Propagation (BP) is a powerful iterative message passing algorithm for graphical models (Pearl, 1988;
Mézard & Montanari, 2009; Opper & Saad, 2001). However, it presents two main drawbacks when applied to
highly connected continuous variable problems: first, the
need to work with continuous probability distributions; and
second, the necessity to iterate over one such probability
distribution for each pair of variables.
nd

Proceedings of the 32
International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

The first problem can be addressed by projecting the distributions onto a finite number of moments (Sudderth et al.,
2010) and the second by utilizing the Thouless-AndresonPalmer (TAP) approach (Mézard & Montanari, 2009; Opper & Saad, 2001) where only single variable marginals are
required. Approximate message passing (AMP), first introduced in (Donoho et al., 2009), is one such relaxation of
BP that utilizes both of the aforementioned approximations
in order to solve sparse estimation problems. In AMP’s
more general setting, as is considered in Generalized AMP
(GAMP) (Rangan, 2011a), the goal of the algorithm is the
reconstruction of an N -dimensional sparse vector x given
the knowledge of an M -dimensional vector y obtained via
a possibly non-linear and/or probabilistic output function
h(z) performed on a set of linear projections. Specifically,
yµ = h(zµ ),

where zµ =

N
X

Φµi xi .

1

(1)

i=1

For example, if h(z) = z + ξ where ξ is a zero-mean
i.i.d. Gaussian random variable, then h(z) represents an additive white Gaussian noise (AWGN) channel. With this
output function, in the setting M  N , (1) is simply
the application of Compressed Sensing (CS) (Candès &
Romberg, 2005) under noise. AMP is currently acknowledged as one of the foremost algorithms for such problems in terms of both its computational efficiency and in
the number of measurements required for exact reconstruction of x. In fact, with properly chosen measurement matrices (Krzakala et al., 2012b;a; Donoho et al., 2012), one
1

In the present work, we use subscript notation to denote the
individual coefficients of vectors, i.e. yµ refers to the µth coefficient of y where µ ∈ {1, 2, . . . , M }, and the double-subscript
notation to refer to individual matrix elements in row-column order.

Swept Approximate Message Passing

can achieve information-theoretically optimal reconstruction performance for CS, a hitherto unachievable bound
with standard convex optimization approaches.
Just as with any iterative algorithm, the convergence properties of AMP are of chief analytical concern. Many rigorous results have been obtained on the performance of AMP
in the case of i.i.d. and block i.i.d. matrices (Bayati & Montanari, 2011; Donoho et al., 2012). Unfortunately, while
AMP performs well for zero-mean i.i.d. projections, performance tends to drastically decline if one moves away
from these simple scenarios. In fact, even for i.i.d. matrices with a small positive mean, the algorithm may violently
diverge, leading to poor reconstruction results (Caltagirone
et al., 2014). This instability to slight variations from these
strict assumptions on the projections is a serious problem
for many practical applications of AMP.
The main theoretical reason for these convergence issues
has been identified in (Caltagirone et al., 2014) using a rigorous tool called State Evolution. The issue stems from
AMP’s use of a parallel update, instead of a sequential one,
on the BP variables at each iteration. This result motivates
the present work.
Three strategies have been proposed in recent literature to
avoid this problem. First, one can highly damp the AMP
iterations, as in (Krzakala et al., 2012b; Vila & Schniter,
2012). However, this often requires a damping factor so
large that the cost, in terms of the number of iterations until
convergence, is prohibitive. Additionally, it is not entirely
clear how to determine an optimal damping factor to ensure
convergence in general.
Second, one can modify the problem a posteriori in order
to come back to a more favorable situation. For instance,
one might remove the mean of the matrix and of the measurements (Caltagirone et al., 2014), or one might modify
the algorithm according to the theoretical spectrum of the
operator Φ (Rangan et al., 2014; Çakmak et al., 2014), if it
is known. This knowledge about the operator may be prohibitive and could therefore present a strong limitation in
practice. This procedure may work for non-zero mean projectors, but for more complicated operators it is not clear
what one should do.
A third solution was proposed in (Caltagirone et al., 2014)
where it was shown that one can take one step backward
in approximation from AMP to a come back to a BP-style
iteration (Caltagirone et al., 2014), using BP with a sequential update rather than the parallel one. It is indeed rather
natural when using BP to work with a sequential update,
sweeping through all variables, and this usually gives better
convergence performance empircally. Going back to BP,
however, amounts to a huge cost in terms of both memory
and computational efficiency as there are O(N 2 ) variables

to update per-iteration with BP as opposed to the the O(N )
utilized in AMP.
In this contribution, we solve these problems by deriving a slightly modified and efficient AMP algorithm with
greatly improved convergence properties while preserving
the O(N ) iteration and memory cost of AMP. We accomplish this by a careful analysis of the relaxation leading
from BP to AMP where we preserve the sequential, or
swept, variable update pattern of BP in our AMP approach,
and by paying great attention to the “time indices”. This
leads to a subtly modified set of update rules for the AMP
and GAMP algorithms without affecting the fixed point
in any way. The resulting algorithm, which we denote
as Swept AMP (SwAMP), possesses impressive empirical
convergence properties.

2. Belief-Propagation for Signal Recovery
2.1. Signal Recovery as Statistical Estimation
To describe AMP, we focus on the CS signal recovery problem with real valued signals in terms of statistical inference. Given an unknown signal x ∈ RN , a linear projection Φ ∈ RM ×N , and a set of observations y ∈ RM
generated from x and Φ, we write the posterior distribution
for the unknown signal according to Bayes’ rule,
P (x |Φ, y) ∝ P (y |Φ, x) P0 (x),

(2)

where we write ∝ as we neglect the normalization constant.
The likelihood P (y |Φ, x) is determined according to the
constraints one wishes to enforce, which we consider to be
of form y = h(Φ x), with h being, in general, any stochastic function. We consider h to be an AWGN channel 2 ,
yµ = h(Φµ x) = Φµ x +N (0, ∆),

(3)

where ∆ is the variance of the AWGN and Φµ is the µth
row-vector of Φ. Hence,

P (y |Φ, x) =

M
Y
µ=1

√

P


(yµ − i Φµi xi )2
1
.
exp −
2∆
2π∆
(4)

The prior P0 (x) is determined from the information we
have on the structure of x. For CS, we are concerned with
the recovery of sparse signals, i.e. ones with few non-zero
values. Unstructured sparse signals can be modeled well
2
One can generalize h to be a more complicated output function. This generalization constitutes the change of AMP to GAMP
(Rangan, 2011a). For example, we examine the case of 1-bit CS
in Sec. 4.3 where h is a non-linear sign function.

Swept Approximate Message Passing

by an i.i.d. Bernoulli sparse prior,
P0 (x) ∝

N
Y

P0 (xi ),

This leads (see (Krzakala et al., 2012a)) to the following
closed recursion sometimes called relaxed BP (r-BP),
where

(5)

P0 (xi ) = ρψ(xi ) + (1 − ρ)δ(xi ),

(6)

where ψ(xi ) can be any distribution, e.g. ψ(xi ) =
N (xi ; x̄, σ 2 ), δ(xi ) is the Dirac delta function and the degree of sparsity is controlled by the value ρ ∈ [0, 1]. Notice
that, in this usual setting, both distributions are factorized,
that is, the likelihood is in M terms relative to the constraint
over each yµ , and the prior is in N terms relative to what is
expected of each xi . Factorized distributions such as these
are well represented by graphical models (Wainwright &
Jordan, 2008) , specifically, bipartite graphs in which the
M + N factors are represented by one type of node and the
N variables xi by another. Once the posterior distribution
is written down, the estimate x̂ may be assigned in different
ways, according to what loss function one wishes to minimize. In this work, we are chiefly concerned with the minimum mean-squared error (MMSE) estimate, which can be
shown to be the average of xi with respect to the posterior P (x |Φ, y); if one were able to compute the posterior’s
marginals, the MMSE estimate would read
x̂MMSE
=
i

Z

dxi xi P (xi |Φ, y),

∀i.

(7)

The strategy employed by AMP is to infer the marginals
of the posterior by using a relaxed version of the BP algorithm (Pearl, 1988; Mézard & Montanari, 2009) , and thus
to arrive at the MMSE estimate of the unknown signal x.
2.2. Relaxed Belief-Propagation
BP implements a message-passing scheme between nodes
in a graphical model, ultimately allowing one to compute approximations of the posterior marginals. Messages
mi→µ are sent from the variables nodes to the factor nodes
and subsequent messages mµ→i are sent from factor nodes
back to variable nodes that corresponds to algorithm’s current “beliefs” about the probabilistic distribution of the
variables xi . Since these distributions are continuous, the
first relaxation step is to move to a projected version of
these distributions, as described in (Rangan, 2011a; Krzakala et al., 2012a) . Here, we shall follow the notation of
reference and use the following parametrization
Z
ai→µ ,
Z
vi→µ ,

dxi xi mi→µ (xi ) ,

(8)

dxi x2i mi→µ (xi ) − a2i→µ ,

(9)

mµ→i (xi ) ∝ e−

x2
i
2

Φ2µi
,
∆ + j6=i Φ2µj vj→µ
P
Φµi (yµ − j6=i Φµj aj→µ )
P
,
=
∆ + j6=i Φ2µj vj→µ
!
P
1
ν6=µ Bν→i
= f1 P
,P
,
ν6=µ Aν→i
ν6=µ Aν→i
!
P
1
ν6=µ Bν→i
,
,P
= f2 P
ν6=µ Aν→i
ν6=µ Aν→i

Aµ→i =

i=1

Aµ→i +Bµ→i xi

.

(10)

Bµ→i
ai→µ
vi→µ

P

(11)
(12)
(13)

(14)

where the functions f are defined by the following priordependent integrals
Z

2

f1 (Σ , R) ,

dx x P0 (x) √

(x−R)2
1
e− 2Σ2 ,
2πΣ

(15)

and
f2 (Σ2 , R) ,

Z

dx x2 P0 (x) √

= Σ2

(x−R)2
1
e− 2Σ2 − f12 (Σ2 , R)
2πΣ

df1 2
(Σ , R) .
dR

(16)

After convergence, the single point marginals are given by
P

Bν→i
1
, Pν
,
ν Aν→i
ν Aν→i
P


Bν→i
1
, Pν
.
vi = f2 P
ν Aν→i
ν Aν→i


ai = f1

P

(17)
(18)

We intentionally write r-BP without specifying time indices
since the updates can be performed in one of two ways. The
first approach is to update in parallel, where all variables
are updated at time t given the state at time t − 1. The
second is the random sequential update where one picks a
single index i and updates all messages corresponding to it.
A time-step is completed once all indices have been visited
and updated once.
2.3. The AMP algorithm
We consider the AMP algorithm in the form that was derived in (Donoho et al., 2010; Rangan, 2011b; Krzakala
et al., 2012a). The main steps are a) going from belief
propagation (BP) to a relaxed BP (r-BP) where only the
two first moments of all messages are kept and b) using N
sites marginals instead of N × M messages and adding the
compensating Onsager terms (Thouless et al., 1977). Fi-

Swept Approximate Message Passing

3. Swept Approximate Message Passing

nally, AMP reads
Vµt+1 =

X

Φ2µi vit ,

(19)

i

(yµ − ωµt ) X 2 t
Φ v ,
ωµt+1 =
Φµi ati −
∆ + Vµt i µi i
i
"
#−1
X
Φ2µi
t+1 2
(Σi ) =
,
∆ + Vµt+1
µ
X

P
Rit+1 = ati +

Φµi
P

µ

(20)

(21)

t+1
(yµ −ωµ
)

∆+Vµt+1

Φ2µi
µ ∆+Vµt+1

,


at+1
= f1 (Σt+1
)2 , Rit+1 ,
i
i

vit+1 = f2 (Σt+1
)2 , Rit+1 .
i

(22)
(23)
(24)

where fk (Σ2 , R), here and in what follows, are the k-th
connected cumulants w.r.t. the probability measure
(x−R)2

e− 2Σ2
1
√
P
(x)
,
Q(x) =
Z(Σ2 , R)
2πΣ2

(25)

N
1 X t
V =
v .
N i=1 i
t

t
= at+1
− Bµ→i
(Σ2i )t
i


∂f1
(Σ2i )t , Rit ,
∂R
(27)

making the expansion for ωµ

The variables ai and vi are the AMP estimators for the
mean and variance of the component i of the signal. The
quality of the reconstruction can be evaluated by computing the mean squared error (MSE) and the average variance
N
1 X
E =
(si − ati )2 ,
N i=1

In the message-passing described in the previous section,
2(M × N ) messages are sent, one between each variable
component and each measurement at each iteration. This
creates a very large computational and memory burden for
applications with large N , M . It is possible to rewrite the
BP equations in terms of only N + M messages by making
the assumption that
√ Φ is dense and that its elements are of
magnitude O(1/ N ). In statistical physics, this assumption leads to the TAP equations (Thouless et al., 1977) used
in the study of spin glasses. For graphical models, such
strategies have been discussed in (Opper & Saad, 2001).
The use of TAP with r-BP provides the standard AMP iteration. Now let us investigate the expansion of the factor ωµ
from Eq. (20) of the AMP iteartion as we include the time,
or iteration, indices t. First one has
!
P t
t
1
ν Bν→i − Bµ→i
t+1
,P t
ai→µ = f1 P t
t
t
ν Aν→i − Aµ→i
ν Aν→i − Aµ→i
t
= at+1
− Bµ→i
vit+1 ,
i

with Z(Σ2 , R) as the normalization constant.

t

3.1. Rederiving the Time Indices

(26)

where si is the original signal component. When γ = 0,
the performance of the AMP algorithm was analyzed rigorously in the limit of large system size via the state evolution (E t+1 , V t+1 ) = G(E t , V t ), where G is a function specified in (Bayati & Montanari, 2011; Donoho et al.,
2010; Rangan, 2011b; Krzakala et al., 2012a). An important property of the Bayes optimal inference (i.e. when the
signal was indeed generated from the assumed prior distribution) is that the two paramaters are equal in the large
size limit, E t = V t , and the state evolution hence reduces
to an iterative equation of a single real number, which is
amenable to rigorous analysis (Bayati & Montanari, 2011).
In statistical physics E t = V t is called the Nishimori condition and is discussed in the context of compressed sensing in detail in (Krzakala et al., 2012a). In general, when
γ = 0 we observed by analyzing the state evolution equations that even when at initial times E t=0 6= V t=0 the equality E t = V t is restored after a sufficient number of iterations.

ωµt+1 =

X

=

X

i

i

Φµi at+1
−
i

(yµ − ωµt ) X 2 t+1
Φ v
∆µ + Vµt i µi i

Φµi at+1
−
i

(yµ − ωµt ) t+1
V
,
∆µ + Vµt µ

(28)

which allows us to close the equations on the set of
a, v, R, Σ, V and ω. Iterating all relations in parallel (i.e.
updating all R, Σ’s, then a, v’s and then the ω, V ’s) provides the AMP iteration.
The implementation of the sequential update is not a
straightforward task as many otherwise intuitive attempts
lead to non-convergent algorithms. The key observation
in the derivation of SwAMP is that (28) mixes different
time indices: while the “a” and “V ” are the “new ones”,
the expression in the fraction is the “old” one, i.e. the
one before P
the last iteration. The implication of this is
that while
i Φµi ai and Vµ should be recalculated as
the updates sweep over i at a single time-step, the term
(yµ − ωµ )/(∆µ + Vµ ) (which we denote as gµ later on)
should not. A corresponding bookkeeping then leads to
the SwAMP algorithm for the evolution of ωµ , Σ2i , Vµ and
Ri described in Alg. 1. At this point, the difference between AMP and SwAMP appears minimal, but, as we shall
see, the differences in convergence properties turn out to be
spectacular.

Swept Approximate Message Passing
3

Algorithm 1 Swept AMP
Input: y, Φ, ∆, θprior , tmax , ε
t←0 
	
Initialize a(0) , v(0) , {ω (0; N +1) , V(0; N +1) }
while t < tmax and || a(t+1) − a(t) || > ε do
for µ = 1 to M do
(t)

gµ ←

. For calculating `1 recoveries, we utilize an implementation of the SPGL1 (van den Berg & Friedlander, 2008)
algorithm.
4.1. Compressed Sensing with Troublesome Projections

(t; N +1)
yµ −ωµ
(t; N +1)
∆+Vµ

(t+1; 1)

Vµ

(t+1; 1)

←

P

Pi

(t)

Φ2µi vi

(t)

(t+1; 1) (t)
gµ

ωµ
← i Φµi ai − Vµ
end for
S ← Permute([1, 2, . . . , N ])
for k = 1 to N do
i ← Sk

−1
P
Φ2µi
2 (t+1)
Σi
←
(t+1; k)
µ
∆+Vµ

(t+1)
Ri
(t+1)
ai
(t+1)
vi

←

(t)
ai

+ Σ2i

(t+1)

P

µ

Φµi

(t+1; k)
yµ −ωµ
(t+1; k)

∆+Vµ
(t+1)
(t+1)
f1 (Ri
, Σ2i
; θprior )
(t+1)
(t+1)
f2 (Ri
, Σ2i
; θprior )

←
←
for µ = 1, m do
(t+1)
(t)
(t+1; k)
(t+1; k+1)
+ Φ2µi (vi
− vi )
← Vµ
Vµ
(t+1; k+1)

(t+1; k)

(t+1)

+ Φµi (ai
← ωµ
ωµ
(t+1; k)
(t+1; k+1)
(t)
)
− Vµ
gµ (Vµ
end for
end for
t←t+1
end while

(t)

− ai ) −

3.2. Generalized Swept AMP
We note that this procedure can also be generalized, a
la GAMP, for output channels other than the AWGN.
The required change is minimal (Rangan, 2011a): one
should replace the term (yµ − ωµ )/(∆µ + Vµ ) in the
Ri and ωµ updates with gout (ωµ , Vµ ), a generic function
which depends on the channel. Specifically, gout (ω, V ) =

R
(z−ω)2
z−ω
1
dz P (y|z) e− 2V
. Additionally, the ∆µ +V
V
µ
out
term in the Σ2i update should be replaced by − ∂g
∂ω . Notice
that all AWGN specific terms are recovered for P (y|z) ∝

e−

(y−z)2
2∆

.

As discussed earlier, using projections of non-zero mean
to sample x is one of the simplest cases for which AMP
can fail to converge. However, by using the proposed
SwAMP approach, accurate estimates of x can be obtained
even when the mean of the projections is non-negligible.
While it may be possible to use mean subtraction, our
proposed approach does not require such preprocessing.
Additionally, as we will show later, not all problems are
amenable to such mean subtraction. To evaluate the effectiveness of SwAMP as compared to the standard parallelupdate AMP iteration, we draw i.i.d. projections according
to


γ 1
,
,
(29)
Φµi ∼ N
N N
where the magnitude of the projector mean is controlled
by the term γ. For a given signal x and noise variance
∆, as γ increases from 0, we expect to see AMP failing
to converge. This behavior can be observed in the numerical experiments presented in Fig. 1. Here, we observe that
SwAMP is robust to values of γ over an order of magnitude larger than the standard AMP, converging to a lowMSE solution even for γ ≈ 140 while AMP fails already
at γ = 2. Additionally, for the tested parameters, `1 minimization fails to provide a meaningful reconstruction for
any value of γ.
We also consider an even more troublesome case, namely,
strongly correlated projections. Such problems are of interest as they arise naturally in machine learning and biomedical applications, where the practitioner does not design
the projections for optimal signal recovery, but rather the
projections represent sampled observations from which the
practitioner desires to predict some response variables by
finding an interpretable, i.e. sparse, set of regression coefficients. Such observations can be highly correlated and
thus represent a significant impediment to the use of parallel AMP for regression tasks.
For these tests, we draw
Φ=

4. Numerical Results
Here, we present a range of numerical results demonstrating the effectiveness of the SwAMP algorithm for problems on which both standard AMP and `1 minimization via
convex optimization fail to provide desirable reconstruction
performance. All experiments were conducted on a computer with an i7-3930K processor and run via Matlab. We
have provided demonstrations of the SwAMP code on-line

1
P Q,
N

where Pµk , Qki ∼ N (0, 1)

(30)

with P ∈ RM ×R , Q ∈ RR×N and R , ηN . That is,
Φ is rank-deficient for η < α, where α = M
N . In our
experiments, we use η to denote the level of independence
of the rows of Φ, with lower values of η representing a
more difficult problem. We observe that the elements of
3
https://github.com/eric-tramel/
SwAMP-Demo

Swept Approximate Message Passing

We can see that SwAMP provides more accurate estimates
across all η while also possessing a more robust transition between successful and unsuccessful reconstructions
in terms of η. Over the tested η, SwAMP averaged 2 seconds of computation time, BPDN averaged 8 seconds, and
`p and adaptive Lasso required on average 40 and 14.88
seconds of compute time respectively. We point out these
run times to demonstrate that despite the potential complexity of the SwAMP fixed-point iteration, its computational burden is on par with, or an improvement on, other
well known techniques.
These two experiments demonstrate how the proposed
SwAMP iteration allows for AMP-like performance while
remaining robust to conditions outside of the TAP assumptions about the projector.
4.2. Group Testing
Group testing, also known as pooling in molecular biology, is an approach to designing experiments so as to reduce the number of tests required to identify rare events
or faulty items. In the most naive approach to this problem, the number of tests is equal to the number of items,
as each item is tested individually. However, since only a
small fraction of the items may be faulty, the number of
tests can be significantly reduced via pooling, i.e. testing
many items simultaneously and allowing items to be included within multiple different tests. The nature of this
linear combination of tests allows for a CS-type approach

MSE at iter.

As we cannot guarantee that SwAMP provides Bayes optimal estimation for problems exhibiting strong correlations
in Φ, we provide a comparison to other well-known approaches in Fig. 2. Here, we compare the recovery performance of SwAMP to adaptive Lasso (Zou, 2006), basis
pursuit denoising (BPDN) (Chen et al., 2001), as well as
the `p generalization of Lasso (Tibshirani, 1996) for the
non-convex case p < 1 (Chartrand, 2007). In order to address the free-parameter of the regularization strength in
both `p and adaptive Lasso, we compute the entire solution path and report only the most accurate solution, providing best-case oracle results for these approaches. Additionally, for adaptive Lasso we choose a weight exponent of
0.1. The tests are conducted over 500 independent realizations of the sparse reconstruction problem for N = 1024,
α = 0.6, and ρ = 0.2 with a noise variance ∆ = 10−8 . We
assume that the value of ∆ is known to all the tested approaches a posteriori. For the implemenation of the `p regression we utilized the SparseReg Matlab toolbox (Zhou,
2013), while we use the SpaSM Matlab toolbox (Sjöstrand
et al., 2012) for the implementation of adaptive Lasso.

0.15
0.10

0.15
γ = 1.8
γ = 2.0
γ = 2.4

γ = 10
γ = 30
γ = 50

0.10

0.05
0.00
0

0.05

5
10
AMP iter.

15

0.00
0

5
10
SwAMP iter.

10−1
final MSE

Φ are neither normal nor i.i.d. for these experiments. In
Fig. 2 we see that SwAMP is robust to even these extremely
troublesome projections while AMP fails to converge.

15

L1
SwAMP

10−3
10−5
10−7
10−9
0

60

120

180

γ

Figure 1. AMP, SwAMP, and `1 solvers compared for CS signal
reconstruction for sensing matrices with positive mean on sparse
signals of size N = 104 and sparsity ρ = 0.2 with noise variance
∆ = 10−8 . The projections for have been created following (29)
using M = αN measurements with α = 0.5. Finally, a comparison between the reconstruction error obtained by SwAMP and
`1 -minimization is given at the bottom for the same experimental
settings.

to faulty item detection, but with a few important caveats.
First, the operator is extremely sparse since the number of
pools, and the number of items in them, may be limited due
to physical testing constraints. Second, the elements of this
operator are commonly 0/1. Group testing is therefore a
very challenging application for AMP since the properties
of the group testing operator do not match AMP’s assumptions.
In one recent work (Zhang et al., 2013), the authors
use both BP and AMP for group testing and found that
while basic AMP would not converge, very good results—
optimal ones, in fact—could be obtained by using a BP
approach. This came at a large computational cost, however. Here, we have repeated the experiment of (Zhang
et al., 2013) using the SwAMP approach instead of AMP
and BP. In fact, for SwAMP, a sparse operator is a very advantageous situation in terms of computational efficiency.
Since the projector is extremely sparse by construction, we
may explicitly ignore operations involving null elements,
thus considerably improving the algorithm’s speed, as seen
in Fig. 3(b). Here, we also see that SwAMP’s computational complexity is on the order of O(N 2 ), as is AMP’s.
Group testing experiments are shown in Fig. 3(a) where we
use random 0/1 projections, under the constraint that each
projection should sum to 7, to sample sparse 0/1 signals
with K  N non-zero elements, where N is the signal
dimensionality. While AMP diverges when attempting to

Swept Approximate Message Passing
0.15
η=1
η=3
η=5

0.10

0.15

0.05

0.00
0

0.00
0

5
10
AMP iter.

15

0.5

20
40
SwAMP iter.

60

10−3

0.4
0.3
0.2

BPDN
adalasso
`p (p = 0.5)

10−1
average MSE

SwAMP
BP
L1
Bayes opt.

0.6

M
N

0.30

0.7

η = 0.5
η = 0.7
η = 0.9

α=

MSE at iter.

0.45

0.1

SwAMP

0.0

0.1

10−7
10−9

0.2

ρ=

10−5

0.3

K
N

0.4

0.5

11

12

(a)
0.4

0.6

0.8

1.0

101

Figure 2. At the top, convergence behavior of AMP and
SwAMP are compared for CS signal reconstruction for correlated sensing matrices on sparse signals of size N = 104 and
sparsity ρ = 0.2 with noise variance ∆ = 10−8 . The projectors have been created according to (30) and are rank-deficient
for η < α = 0.6. At the bottom, a comparison between logscale average reconstruction MSE obtained by SwAMP , BPDN,
adaptive Lasso, and `p regularization is given for signals of size
N = 1024 for ∆ = 10−8 , ρ = 0.2, and α = 0.6.

recover these signals, SwAMP converges to the correct solution in few iterations. Additionally, SwAMP very closely
matches the BP transition, thus providing recovery performance better than convex optimization, just as BP does, but
with much less computational complexity.
4.3. 1-bit Compressed Sensing
One of the confounding factors regarding the practical implementation of CS in hardware devices is the treatment of
measurement quantization. The original CS analysis provides recovery bounds based upon the assumption of realvalued measurements. However, in practice, hardware devices cannot capture such values with infinite precision,
and so some kind of quantization on the measurements
must be implemented. Specifically, if Q(·) is a uniform
scalar quantizer, then y = Q(Φ x, B), where B is the number of bits used to represent the measurement. If signal recoverability is significantly impacted by small B, then the
dimensionality reduction provided by CS may be lost by
the requirement for many bits to encode each measurement.
Thankfully, recent works have shown CS recovery to be robust to quantization and the non-linear error it introduces.
In fact, CS has been shown (Boufounos & Baraniuk, 2008;
Jacques et al., 2012) to be robust even in the extreme case
B = 1 known as 1-bit CS. In this case, the quantized mea-

time (s)

η

SwAMP (dense)
SwAMP (sparse)
AMP

100

10−1
t∝N2

10−2
7

8

9

10

log2 N

(b)
Figure 3. (a) Group testing phase transition diagram between successful and unsuccessful signal recovery over M , the number of
pools, and K, the number of non-zero signal elements. Successful
recovery means the correct identification of all signal elements.
The top-left of the diagram represents the easiest problems while
the bottom-right the most difficult. The transition lines are drawn
along the contour of 50% of recoveries succeeding for many trials. (b) Execution times for both SwAMP and AMP using a sparse
matrix with 25% of its elements having non-zero value. The reported times are measured for 500 iterations of the algorithms for
each value of N for the parameters ρ = 0.25 and α = 0.75.

surements are given by
y = sign(Φ x).

(31)

The non-linearity and severity of 1-bit CS requires special
treatment from the CS recovery procedure. In (Boufounos
& Baraniuk, 2008), a renormalized fixed-point continuation (RFPC) algorithm was proposed. Later, (Jacques et al.,
2012) analyzed the sensitivity of 1-bit CS to sign flips and
proposed a noise-robust recovery algorithm, binary iterative hard thresholding (BIHT).
Recognizing the capability of GAMP to handle non-linear
output channels, (Kamilov et al., 2012) proposed the use
of GAMP for signal recovery from quantized CS measurements. Further analysis of message-passing approaches to
the 1-bit CS problem from the perspective of statistical me-

Both methods (Kamilov et al., 2012) and (Xu &
Kabashima, 2013) show the effectiveness of algorithms
grounded in statistical mechanics for quantized CS reconstruction. However, both assume an amenable set of projections. Even projections possessing small mean can cause
large degradations in performance. While mean removal is
occasionally effective in the usual CS setting, it cannot be
used for 1-bit CS due to the nature of the sign operation in
(31). An algorithm that can handle troublesome projectors
can therefore be of great use. In Sec. 2.3, we show how the
SwAMP can be modified to the general-channel setting, as
was done in GAMP. This generalization allows for 1-bit CS
recovery with SwAMP under much more relaxed requirements for Φ.
In Fig. 4(a), we see Generalized
 SwAMP (G-SwAMP)
1
,
We observe that Gresults for Φµi ∼ N 20
N N .
SwAMP performs admirably even for this non-neglible
mean on the projectors. In terms of recovery performance,
it does not quite meet the theoretical Bayes optimal performance (Xu et al., 2014), however, this is expected as
the Bayes optimal performance is calculated for γ = 0.
Additionally, we see that even for this non-zero mean, GSwAMP outperforms both the BIHT’s empirical performance for the same mean, as well as the best-case theoretical `1 performance for zero mean (Xu & Kabashima,
2013). Finally, in Fig. 4(b), we see that GAMP fails to provide any meaningful signal recovery for γ small, while GSwAMP continues to converge to low-MSE even for large
values of γ.

5. Conclusion
While the AMP algorithm has been shown to be a very desirable approach for signal recovery and statistical inference problems in terms of both computational efficiency
and accuracy, it is also very sensitive to problems which
deviate from its fundamental assumptions. In this work,
we propose the SwAMP algorithm which matches AMP’s
accuracy while remaining robust to such variations, all
without unduly increasing computation or memory requirements. We also demonstrate how SwAMP can be used to
solve practical problems for which AMP and GAMP cannot be applied, namely, group testing and 1-bit CS with

0

ρ = 1/4

−10

ρ = 1/8
ρ = 1/16

−20
−30

final MSE (dB)

0

1

2

3

4

5

6

4

5

6

0
−10

BIHT
G-SwAMP
L1
Bayes opt.

−20
−30
0

1

2

3

α=

M
N

(a)
0

0
γ=1
γ=3
γ=5

MSE at iter. (dB)

chanics was given in (Xu & Kabashima, 2013) where a
modified fixed-point iteration was derived via the cavity
method which provided both improved recovery accuracy
and reconstruction time as compared to the RFPC. Additionally, the authors used replica analysis to estimate the
optimal MSE performance of `1 -minimization based 1-bit
CS reconstruction. Finally, this analysis is extended in (Xu
et al., 2014) to include the theoretical Bayesian optimal performance, which we will use as a baseline of comparison
in Fig. 4(a).

final MSE (dB)

Swept Approximate Message Passing

γ = 10
γ = 20
γ = 30

−10

−10

−20

−20

0

3

6

9

GAMP iter.

0

10

20

30

G-SwAMP iter.

(b)
Figure 4. Results for 1-bit CS. (a) Top: Comparison between the
Bayes optimal MSE for zero-mean projectors (Xu et al., 2014)
(dashed lines) and that obtained by SwAMP for projectors with
γ = 20 (markers) for three different levels of signal sparsity. The
reported empirical results were obtained by averaging over 200
instances of size N = 512. Bottom: Comparison of SwAMP and
BIHT for ρ = 1/8 for experiment conditions identical to the figure above; theoretical results for zero-mean projectors are also
presented for completeness, including theoretical `1 performance
(Xu & Kabashima, 2013). (b) Single instance comparison between GAMP and G-SwAMP for 1-bit CS with N = 2048,
ρ = 1/8, and α = 3.

troublesome projections. In all cases, SwAMP provides
superior accuracy as compared to `1 - minimization, as well
as convergence properties superior to AMP and GAMP, and
all with less computational and memory burden than BP or
r-BP.
Exact analysis of the asymptotic state evolution of SwAMP,
as well as a thorough analytical proof of its convergence,
remains a challenging open problem for future work. However, the results are promising for many difficult applications such as sparse logistic regression for identifying significant biological markers from among many thousands of
potential features when very few test samples are available.
Such applications would be impossible for GAMP due to
the sensitivity of its convergence, but are a quite natural
application for G-SwAMP.

Swept Approximate Message Passing

References
Bayati, M. and Montanari, A. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764, 2011.
Boufounos, P. T. and Baraniuk, R. G. 1-bit compressive
sensing. In Proceedings of the 42nd Annual Conference
on Information Sciences and Systems, pp. 16–21, Princeton, NJ, 2008.

Krzakala, F., Mézard, M., Sausset, F., Sun, Y. F., and Zdeborová, L. Probabilistic reconstruction in compressed
sensing: Algorithms, phase diagrams, and threshold
achieving matrices. J. Stat. Mech.: Th. and Exp., (8):
P08009, 2012a.
Krzakala, F., Mézard, M., Sausset, F., Sun, Y. F., and Zdeborová, L. Statistical-physics-based reconstruction in
compressed sensing. Physical Review X, 2(2):021005,
2012b.

Caltagirone, F., Krzakala, F., and Zdeborová, L. On convergence of approximate message passing. In Information Theory Proceedings (ISIT), 2014 IEEE International Symposium on, 2014. URL http://arxiv.
org/abs/1401.6384.

Mézard, M. and Montanari, A. Information, Physics, and
Computation. OUP, 2009. ISBN 9780198570837.

Candès, E. J. and Romberg, J. Signal recovery from random projections. In Computational Imaging III, pp. 76–
86, San Jose, CA, 2005. Proc. SPIE 5674.

Pearl, J. Probabilistic Reasoning in Intelligent Systems.
Morgan Kaufmann, 1988. ISBN 9781558604797.

Çakmak, B., Winther, O., and Fleury, B. H. S-amp: Approximate message passing for general matrix ensembles. arXiv preprint 1405.2767, 2014.

Rangan, S. Generalized approximate message passing for
estimation with random linear mixing. In Information
Theory Proceedings, IEEE Internaional Symposium on,
pp. 2168, 2011a.

Chartrand, Rick. Exact reconstruction of sparse signals via
nonconvex minimization. IEEE Signal Processing Letters, 14(10):707–710, 2007.
Chen, Scott Shaobing, Donoho, David L., and Saunders,
Michael A. Atomic decomposition by basis pursuit.
SIAM Review, 43(1):129–159, March 2001.
Donoho, D. L., Maleki, A., and Montanari, A. Messagepassing algorithms for compressed sensing. Proc. National Academy of Sciences of the United States of America, 106(45):18914, 2009.
Donoho, D. L., Maleki, A., and Montanari, A. Message
passing algorithms for compressed sensing: I. motivation and construction. In Proc. IEEE Information Theory
Workshop, pp. 1–5, 2010.
Donoho, D. L., Javanmard, A., and Montanari, A.
Information-theoretically optimal compressed sensing
via spatial coupling and approximate message passing.
In Information Theory Proceedings (ISIT), 2012 IEEE
International Symposium on, pp. 1231. IEEE, 2012.
Jacques, L., Laska, J. N., Boufounos, P. T., and Baraniuk, R. G. Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors. arXiv preprint
1104.3160v3, 2012.
Kamilov, U. S., Goyal, V. K., and Rangan, S. Messagepassing de-quantization with applications to compressed
sensing. IEEE Transactions on Image Processing, 60
(12):6270, 2012.

Opper, M. and Saad, D. Advanced Mean Field Methods: Theory and Practice. MIT Press, 2001. ISBN
0585417725. NIPS workshop series.

Rangan, S. Generalized approximate message passing for
estimation with random linear mixing. In Proc. IEEE
International Symposium on Information Theory, pp.
2168–2172, 2011b.
Rangan, S., Schniter, P., and Fletcher, A. K. On the
convergence of approximate message passing with arbitrary matrices. arXiv preprint 1402.3210, 2014. URL
http://arxiv.org/abs/1402.3210.
Sjöstrand, Karl, Clemmensen, Line Harder, Larsen, Rasmus, and Ersbøll, Bjarne. Spasm: A matlab toolbox for
sparse statistical modeling. Journal of Statistical Software, pp. 1–24, 2012.
Sudderth, E. B., Ihler, A. T., Isard, M., Freeman, W. T.,
and Willsky, A. S. Nonparametric belief propagation.
Communications of the ACM, 53(10):95, 2010.
Thouless, D. J., Anderson, P. W., and Palmer, R. G. Solution of ‘solvable model of a spin glass’. Philosophical
Magazine, 35(3):593, 1977.
Tibshirani, Robert. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society, Series
B, 58(1):267–288, 1996.
van den Berg, E. and Friedlander, M. P. Probing the pareto
frontier for basis pursuit solutions. SIAM Journal on Scientific Computing, 31(2):890–912, 2008. doi: 10.1137/
080714488. URL http://link.aip.org/link/
?SCE/31/890.

Swept Approximate Message Passing

Vila, J. P. and Schniter, P. Expectation-maximization
gaussian-mixture approximate message passing. In
Proc. 46th Annual Conference on Information Sciences
and Systems, pp. 1, 2012.
Wainwright, M. J. and Jordan, M. I. Graphical models,
exponential families, and variational inference. Foundations and Trends in Machine Learning, 1, 2008. ISSN
1935-8237. doi: 10.1561/2200000001.
Xu, Y. and Kabashima, Y. Statistical mechanics approach
to 1-bit compressed sensing. Journal of Statistical Mechanics: Theory and Experiment, (2):P02041, 2013.
Xu, Y., Kabashima, Y., and Zdeborová, L. Bayesian signal
reconstruction for 1-bit compressed sensing. arXiv to
appear, 2014.
Zhang, P., Krzakala, F., Mézard, M., and Zdeborová, L.
Non-adaptive pooling strategies for detection of rare
faulty items. In Communications Workshops, Proc. IEEE
International Conference on, pp. 1409, Budapest, Hungary, 2013.
Zhou, Hua. Matlab SparseReg toolbox version 0.0.1.
Available online: http://hua-zhou.github.
io/softwares/sparsereg, July 2013.
Zou, Hui. The adaptive lasso and its oracle properties.
Journal of the American Statistical Association, 101
(476):1418–1429, 2006.

