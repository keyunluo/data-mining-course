Communication-Efficient Distributed Optimization
using an Approximate Newton-type Method

Ohad Shamir
OHAD . SHAMIR @ WEIZMANN . AC . IL
Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot, Israel
Nathan Srebro
NATI @ TTIC . EDU
Toyota Technological Institute at Chicago and the Department of Computer Science, Technion, Haifa, Israel
Tong Zhang
TZHANG @ STAT. RUTGERS . EDU
Department of Statistics, Rutgers University, Piscataway NJ, USA, and Baidu Inc., Beijing, China

Abstract

N = nm independent samples evenly and randomly distributed among the machines. Each machine i can construct
a local empirical (sample) estimate of F (w):

We present a novel Newton-type method for distributed optimization, which is particularly well
suited for stochastic optimization and learning
problems. For quadratic objectives, the method
enjoys a linear rate of convergence which provably improves with the data size, requiring an
essentially constant number of iterations under
reasonable assumptions. We provide theoretical
and empirical evidence of the advantages of our
method compared to other approaches, such as
one-shot parameter averaging and ADMM.

n

Ï†i (w) = FÌ‚i (w) =

1X
f (w, zij )
n j=1

and the overall empirical objective is then:
m

Ï†(w) = FÌ‚ (w) =

1 X
1 X
FÌ‚i (w) =
f (w, zij ). (4)
m i=1
nm i,j

We can then use the empirical risk minimizer (ERM)
wÌ‚ = arg min FÌ‚ (w)

1. Introduction
We consider the problem of distributed optimization, where
each of m machines has access to a function Ï†i : Rd â†’ R,
i = 1, . . . , m, and we would like to minimize their average
Ï†(w) =

1
m

m
X

Ï†i (w).

(1)

i=1

We are particularly interested in a stochastic optimization
(learning) setting, where the ultimate goal is to minimize
some stochastic (population) objective (e.g. expected loss
or generalization error)
F (w) = E [f (w, z)]
zâˆ¼D

(2)

and each of the m machines has access to n i.i.d. samples
zi1 , . . . , zin from the source distribution D, for a total of
st

(3)

Proceedings of the 31 International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

(5)

as an approximate minimizer of F (w). Since our interest lies mostly with this stochastic optimization setting, we
will denote wÌ‚ = arg min Ï†(w) even when the optimization objective Ï†(w) is not an empirical approximation to a
stochastic objective.
When considering distributed optimization, two resources
are at play: the amount of processing on each machine, and
the communication between machines. In this paper, we
focus on algorithms which alternate between a local optimization procedure at each machine, and a communication round involving simple map-reduce operations such as
distributed averaging of vectors in Rd . Since the cost of
communication is very high in practice (Bekkerman et al.,
2011), our goal is to develop methods which quickly optimize the empirical objective FÌ‚ (Â·), using a minimal number
of such iterations.
One-Shot Averaging A straight-forward single-iteration
approach is for each machine to optimize its own local objective, obtaining
wÌ‚i = arg min Ï†i (w),

(6)

Newton-type Distributed Optimization

and then to compute their average:
m

wÌ„ =

1 X
wÌ‚i .
m i=1

(7)

This approach, which we refer to as â€œone-shot parameter averagingâ€, was recently considered in Zinkevich et al.
(2010) and further analyzed by Zhang et al. (2013). The
latter also proposed a bias-corrected improvement which
perturbs each wÌ‚i using the optimum on a bootstrap sample. This approach gives only an approximate minimizer of
Ï†(w) with some finite suboptimality, rather then allowing
us converge to wÌ‚ (i.e. to obtain solutions with any desired
suboptimality ). Although approximate solutions are often
sufficient for stochastic optimization, we prove in Section
2 that the one-shot solution wÌ„ can be much worse in terms
of minimizing the population objective F (w), compared to
the actual empirical minimizer wÌ‚. It does not seem possible
to address this suboptimality by more clever averaging, and
instead additional rounds of communications appear necessary.
Gradient Descent One possible multi-round approach to
distributed optimization is a distributed implementation of
gradient descent: at each iteration each machine calculates
âˆ‡Ï†i (w(t) ) at the current iterate w(t) , and then these are averaged to obtain the overall gradient âˆ‡Ï†(w(t) ), and a gradient step is taken. As the iterates are then standard gradient
descent iterates, the number of iterations, and so also number of communication rounds, is linear in the conditioning
of the problem â€“ or, if accelerated gradient descent is used,
proportional to the square root of the condition number: If
Ï†(w) is L-smooth and Î»-strongly convex, then
r
 !
1
L
log
(8)
O
Î»

iterations are needed to attain an -suboptimal solution.
The polynomial dependence on the condition number may
be disappointing, as in many problems the parameter of
strong convexity Î» might be very small. E.g., when strong
convexity arises from regularization, as in many stochastic
optimization problems, Î» decreases with the
âˆš overall sample
size N = nm, and is typically at most 1/ nm (Sridharan
et al. 2008; Shalev-Shwartz et al. 2009; and see also Section 4.3 below). The number of iterations / communication
rounds needed âˆš
for distributed accelerated gradient descent
then scales as 4 nm, i.e. increases polynomially with the
sample size.
Instead of gradient descent, one may also consider more
sophisticated methods which utilize gradient information,
such as quasi-Newton methods. For example, a distributed implementation using L-BFGS has been proposed
in (Agarwal et al., 2011). However, no guarantee better

then (8) can be ensured for gradient-based methods (Nemirovsky & Yudin, 1983), and we thus may still get a polynomial dependence on the sample size.
ADMM and other approaches Another popular approach is distributed alternating direction method of multipliers (ADMM, e.g. Boyd et al. 2011), where the machines
alternate between computing shared dual variables in a distributed manner, and solving augmented Lagrangian problems with respect to their local data. However, the convergence of ADMM can be slow. Although recent works
proved a linear convergence rate under favorable assumptions (Deng & Yin, 2012; Hong & Luo, 2012), we are
not aware of any analysis where the number of iterations
/ communication rounds doesnâ€™t scale strongly with the
condition number, and hence the sample size, for learning applications. A similar dependence occurs with other
recently-proposed algorithms for distributed optimization
(e.g. Yang, 2013; Mahajan et al., 2013; Dekel et al., 2012;
Cotter et al., 2011; Duchi et al., 2012). We also mention
that our framework is orthogonal to much recent work on
distributed coordinate descent methods (e.g. Recht et al.,
2011; RichtaÌrik & TakaÌc, 2013), which assume the data is
split feature-wise rather than instance-wise.
Our Method The method we propose can be viewed as
an approximate Newton-like method, where at each iteration, instead of a gradient step, we take a step appropriate for the geometry of the problem, as estimated on
each machine separately. In particular, for quadratic objectives, the method can be seen as taking approximate
Newton steps, where each machine i implicitly uses its local Hessian âˆ‡2 Ï†i (w) (although no Hessians are explicitly
computed!). Unlike ADMM, our method can take advantage of the fact that for machine learning applications, the
sub-problems are usually similar: Ï†i â‰ˆ Ï†. We refer to our
method as DANEâ€”Distributed Approximate NEwton.
DANE is applicable to any smooth and strongly convex
problem. However, as is typical of Newton and Newtonlike methods, its generic analysis is not immediately apparent. For general functions, we can show convergence,
but cannot rigorously prove improvement over gradient descent. Instead, in order to demonstrate DANEâ€™s advantages
and give a sense of its benefits, we focus our theoretical
analysis on quadratic objectives. For stochastic quadratic
objectives, where f (w, z) is L-smooth and Î»-strongly convex in w âˆˆ Rd , we show that


1
(L/Î»)2
log(dm) log( )
(9)
O
n

iterations are sufficient for DANE to find wÌƒ such that with
high probability FÌ‚ (wÌƒ) â‰¤ FÌ‚ (wÌ‚) + . When L/Î» is fixed
and the number of examples n per machine is large (the

Newton-type Distributed Optimization

regime considered by Zhang et al. 2013), (9) establishes
convergence after a constant number ofâˆš
iterations / communication rounds. When Î» scales as 1/ nm, as discussed
above, (9) yields convergence to the empirical minimizer
in a number of iterations that scales roughly linearly with
the number of machines m, but not with the sample size
N = nm. To the best of our knowledge, this is the first
algorithm which provably has such a behavior. We also
provide evidence for similar behavior on non-quadratic objectives.
Notation and Definitions For vectors, kvk is always the
Euclidean norm, and for matrices kAk2 is the spectral
norm. We use Î» 4 A 4 L to indicate that the eigenvalues of A are bounded between Î» and L. We say that a
twice differentiable1 function f (w) is Î»-strongly convex or
L-smooth, iff for all w, its Hessian is bounded from below
by Î» (i.e. Î» 4 âˆ‡2 f (w)), or above by L (i.e. âˆ‡2 f (w) 4 L)
respectively.

2. Stochastic Optimization and One-shot
Parameter Averaging
In a stochastic optimization setting, where the true objective is the population objective F (w), there is a limit to the
accuracy with which we can minimize F (w) given only
N = nm samples, even using the exact empirical minimizer wÌ‚. It is thus reasonable to compare the suboptimality
of F (w) when using the exact wÌ‚ to what can be attained using distributed optimization with limited communication.
When f (w, z) has gradients with bounded second moments, namely when âˆ€w Ez kâˆ‡w f (w, z)k2 â‰¤ G2 , and
F (w) is Î»-strongly convex, then (Shalev-Shwartz et al.,
2009)2
 2 
 2
G
G
= inf F (w)+O
E[F (wÌ‚)] â‰¤ F (wâˆ— )+O
w
Î»N
Î»nm
(10)
where wâˆ— = arg min F (w) is the population minimizer
and the expectation is with respect to the random sample
of size N = nm. One might
then ask whether a subop
G2
timality of  = O Î»nm
can be also be achieved using
a few, perhaps only one, round of communication. This
is different from seeking a distributed optimization method
that achieves any arbitrarily small empirical suboptimality,
and thus converges to wÌ‚, but might be sufficient in terms of
stochastic optimization.
For one-shot parameter averaging, Zhang et al. (2013,
1

All our results hold also for weaker definitions of smoothness
and strong convexity which do not require twice differentiability.
2
More precisely, (Shalev-Shwartz et al., 2009) shows this assuming kâˆ‡w f (w, z)k2 â‰¤ G2 for all w, z, but the proof easily
carries over to this case.

Corollary 2) recently showed that for Î»-strongly convex
objectives, and when moments of the first, second and third
derivatives of f (w, z) are bounded by G, L, and M respectively3 , then


G2
L2 G2 log d
G4 M 2
âˆ— 2
E kwÌ„ âˆ’ w k â‰¤ OÌƒ
,
+ 6 2 +
Î»2 nm
Î» n
Î»4 n2
(11)
where wÌ„ is the one-shot average estimator defined in (7).
This implies that the population suboptimality E[F (wÌ„)] âˆ’
F (wâˆ— ) is bounded by

OÌƒ

L3 G2 log d
LG4 M 2
LG2
+
+
Î»2 nm
Î» 6 n2
Î»4 n2


.

(12)

Zhang et al. (2013) argued that the dependence on the sample size mn above is essentially optimal: the dominant
term (as n â†’ âˆž, and in particular when n  m) scales
as 1/(nm), which is the same as for the empirical minimizer wÌ‚ (as in eq. 10), and so one-shot parameter averaging achieves the same population suboptimality rate, using only a single round of communication, as the best rate
we can hope for using unlimited communication, or if all
N = nm samples were on the same machine. Moreover,
the O(nâˆ’2 ) terms can be replaced by a O(nâˆ’3 ) term using
an appropriate bias-correction procedure.
However, this view ignores the dependence on the other
parameters, and in particular the strong convexity parameter Î», which is much worse in (12) relative to (10). The
strong convexity parameter often arises from an explicit
regularization, and decays as the sample size increases.
E.g., in regularized loss minimization and SVM-type problems (Sridharan et al., 2008), as well as more generally
for stochastic convex optimization (Shalev-Shwartz et al.,
2009), the regularization parameter, and hence the strong
1
convexity parameter, decreases as âˆš1N = âˆšnm
. In practice, Î» is often chosen even smaller, possibly
as
small as
âˆš
1
.
Unfortunately,
substituting
Î»
=
O(1/
nm)
in
(12) reN
sults in a useless bound, where even the first term does not
decrease with the sample size.
Of course, this strong dependence on Î» might be an artifact of the analysis of Zhang et al.. However, in Theorem 1
below, we show that even
âˆš in a simple one-dimensional example, when Î» â‰¤ O(1/ n), the population sub-optimality
of the one-shot estimator (using m machines and a total
of nm samples), can be no better then the population suboptimality using just n samples, and much worse than what
3

The exact conditions in Zhang et al. (2013) refer to various high order moments, but are in any case satisfied when
kâˆ‡w f (w, z)k â‰¤ G, kâˆ‡2w f (w, z)k2 â‰¤ L and âˆ‡2 f (w, z) is M Lipschitz in the spectral norm. For learning problems, all derivatives of the objective can be bounded in terms of a bound on the
data and bounds on the derivative of a scalar loss function, and
are less of a concern to us.

Newton-type Distributed Optimization

can be attained using nm samples. In other words, one-shot
averaging does not give any benefit over using only the data
on a single machine, and ignoring all other (m âˆ’ 1)n data
points.
Theorem1. For any
 per-machine sample size n â‰¥ 9, and
1
any Î» âˆˆ 0, 9âˆšn , there exists a distribution D over examples and a stochastic optimization problem on a convex
set4 W âŠ‚ R, such that:
â€¢ f (w; z) is Î»-strongly convex, infinitely differentiable,
and âˆ€wâˆˆW Ez [kâˆ‡f (w; z)k2 ] â‰¤ 9.
â€¢ For any number of machines m, if we run one-shot
parameter averaging to compute wÌ„, it holds for some
universal constants C1 , C2 , C3 , C4 that
E[kwÌ„ âˆ’ wâˆ— k2 ] â‰¥

C2
C1
, E[kwÌ‚ âˆ’ wâˆ— k2 ] â‰¤ 2
2
Î» n
Î» nm

E[F (wÌ„)]âˆ’F (wâˆ— ) â‰¥

The crux of the method is the local optimization performed
on each machine at each iteration:
(t)

wi = arg min[Ï†i (w)

3. Distributed Approximate Newton-type
Method
We now describe a new iterative method for distributed
optimization. The method performs two distributed averaging computations per iteration, and outputs a predictor
w(t) which, under suitable parameter choices, converges
to the optimum wÌ‚. The method, which we refer to as
DANE (Distributed Approximate NEwton-type Method) is
described in Figure 1.
DANE maintains an agreed-upon iterate w(t) , which is
synchronized among all machines at the end of each iteration. In each iteration, we first compute the gradient
âˆ‡Ï†(w(tâˆ’1) ) at the current iterate, by averaging the local
gradients âˆ‡Ï†i (w(tâˆ’1) ). Each machine then performs a separate local optimization, based on its own local objective
Following the framework of Zhang et al., we present an example where the optimization is performed on a bounded set,
which ensures that the gradient moments are bounded. However,
this is not essential and the same result can be shown when the
domain of optimization is R.

(13)

w

âˆ’ (âˆ‡Ï†i (w(tâˆ’1) ) âˆ’ Î·âˆ‡Ï†(w(tâˆ’1) ))> w +

Âµ
kw âˆ’ w(tâˆ’1) k22 ]
2

To understand this local optimization, recall the definition
of the Bregman divergence corresponding to a strongly
convex function Ïˆ:
DÏˆ (w0 ; w) = Ïˆ(w0 ) âˆ’ Ïˆ(w) âˆ’ hâˆ‡Ïˆ(w), w0 âˆ’ wi.
Now, for each local objective Ï†i , consider the regularized
local objective, defined as

C3
C4
, E[F (wÌ‚)]âˆ’F (wâˆ— ) â‰¤
Î»n
Î»nm

The intuition behind the construction of Theorem 1 is that
when Î» is small, the deviation of each machine output wÌ‚i
from wâˆ— is large, and its expectation is biased away from
wâˆ— . The exact bias amount is highly problem-dependent,
and cannot be eliminated by any fixed averaging scheme.
Since bias is not reduced by averaging, the optimization
error does not scale down with the number of machines
m. The full construction and proof appear in appendix A.
In the appendix we also show that the bias correction proposed by Zhang et al. to reduce the lower-order terms in
equation (11) does not remedy this problem.

4

Ï†i (w) and the computed global gradient âˆ‡Ï†(w(t) ), to ob(t)
tain a local iterate wi . These local iterates are averaged to
obtain the centralized iterate w(t) .

hi (w) = Ï†i (w) +

Âµ
kwk2
2

and its corresponding Bregman divergence:
Di (w0 ; w) = Dhi (w0 ; w) = DÏ†i (w0 ; w) +

Âµ 0
kw âˆ’ wk2 .
2

It is not difficult to check that the local optimization problem (13) can be written as
(t)

wi = arg min Ï†(w(tâˆ’1) ) + hâˆ‡Ï†(w(tâˆ’1) ), w âˆ’ w(tâˆ’1) i
w

1
+ Di (w; w(tâˆ’1) ),
Î·

(14)

where we also added the terms Ï†(w(tâˆ’1) ) +
hâˆ‡Ï†(w(tâˆ’1) ), w(tâˆ’1) i which do not depend on w
and do not affect the optimization. The first two terms
in (14) are thus a linear approximation of the overall
objective Ï†(w) about the current iterate w(tâˆ’1) , and do not
depend on the machine i. What varies from machine to
machine is the potential function used to localize the linear
approximation. The update (14) is in-fact a mirror descent
update (Nemirovski & Yudin, 1978; Beck & Teboulle,
2003) using the potential function hi , and step size Î·.
Let us examine this form of update. When Âµ â†’ âˆž the
potential function essentially becomes a squared Euclidean
norm, as in gradient descent updates. In fact, when Î·, Âµ â†’
.
âˆž as Î·Ìƒ = ÂµÎ· remains constant, the update (14) becomes a
standard gradient descent update on Ï†(w) with stepsize Î·Ìƒ.
In this extreme, the update does not use the local objective
Ï†i (w), beyond the centralized calculation of âˆ‡Ï†(w), the
updates (14) are the same on all machines, and the second
round of communication is not needed. DANE reduces to
distributed gradient
 descent, with its iteration complexity
.
of O L
log(1/)
Î»

Newton-type Distributed Optimization

Procedure DANE
Parameter: learning rate Î· > 0 and regularizer Âµ > 0
Initialize: Start at some w(0) , e.g. w(0) = 0
Iterate: for t = 1, 2, . . .
Pm
1
(tâˆ’1)
Compute âˆ‡Ï†(w(tâˆ’1) ) = m
i=1 âˆ‡Ï†i (w  ) and distribute to all machines

(t)
For each machine i, solve wi = arg minw Ï†i (w) âˆ’ (âˆ‡Ï†i (w(tâˆ’1) ) âˆ’ Î·âˆ‡Ï†(w(tâˆ’1) ))> w + Âµ2 kw âˆ’ w(tâˆ’1) k22
P
(t)
m
1
(âˆ—)
Compute w(t) = m
i=1 wi and distribute to all machines
end
Figure 1. Distributed Approximate NEwton-type method (DANE)

At the other extreme, consider the case where Âµ = 0 and
all local objectives are equal, i.e. hi (w) = Ï†i (w) = Ï†(w).
Substituting the definition of the Bregman divergence into
(t)
(14), or simply investigating (13), we can see that wi =
arg min Ï†i (w) = arg min Ï†(w) = wÌ‚. That is, DANE converges in a single iteration to the overall empirical optimum. This is an ideal Newton-type iteration, where the
potential function is perfectly aligned with the objective.
Of course, if Ï†i (w) = Ï†(w) for all machines i, we would
not need to perform distributed optimization in the first
place. Nevertheless, as n â†’ âˆž, we can hope that Ï†i (w) are
similar enough to each other, such that (14) approximates
such an ideal Newton-type iteration, gets us very close to
the optimum, and very few such iterations are sufficient.
In particular, consider the case where Ï†i (w), and hence also
Ï†(w) are quadratic. In this case, the Bregman divergence
Di (w; w(tâˆ’1) ) takes the form:
1
(w âˆ’ w(tâˆ’1) )> (âˆ‡2 Ï†i (w(tâˆ’1) ) + ÂµI)(w âˆ’ w(tâˆ’1) ), (15)
2
and the update (14) can be solved in closed form:
(t)

wi = w(tâˆ’1) âˆ’ Î·(âˆ‡2 Ï†i (w(tâˆ’1) ) + ÂµI)âˆ’1 âˆ‡Ï†(w(tâˆ’1) )
w(t) = w(tâˆ’1)
âˆ’Î·

1 X 2
(âˆ‡ Ï†i (w(tâˆ’1) ) + ÂµI)âˆ’1
m i

!
âˆ‡Ï†(w(tâˆ’1) ).
(16)

Contrast this with the true Newton update:
!âˆ’1
1 X 2
(t)
(tâˆ’1)
(tâˆ’1)
w =w
âˆ’Î·
âˆ‡ Ï†i (w
)
âˆ‡Ï†(w(tâˆ’1) ).
m i
(17)
The difference here is that in (16) we approximate the inverse of the average of the local Hessians with the average
of the inverse of the Hessians (plus a possible regularizer).
Again we see that the DANE update (16) approximates the
true Newton update (17), which can be performed in a distributed fashion without communicating the Hessians.

For a quadratic objective, a single Newton update is enough
to find the exact optimum. In Section 4 we rigorously analyze the effects of the distributed approximation, and quantify the number of DANE iterations (and thus rounds of
communication) required.
For a general convex, but non-quadratic, objective, the
standard Newton approach is to use a quadratic approximation to the ideal Bregman divergence DÏ† . This leads to
the familiar quadratic Newton update in terms of the Hessian. DANE uses a different sort of approximation to DÏ† :
we use a non-quadratic approximation, based on the entire
objective and not just a local quadratic approximation, but
approximate the potential on each node separately. In the
stochastic setting, this approximation becomes better and
better, and thus the required number of iterations decrease,
as n â†’ âˆž.
Since it is notoriously difficult to provide good global
analysis for Newton-type methods, we will investigate the
global convergence behavior of DANE carefully in the next
Section but only for quadratic objective functions. This
analysis can also be viewed as indicative for non-quadratic
objectives, as locally they can be approximated by quadratics and so should enjoy the same behavior, at least asymptotically. For non-quadratics, we provide a rigorous convergence guarantee when the stepsize Î· is sufficiently small
or the regularization parameter Âµ is sufficiently large (in
Section 5). However, this analysis does not show a benefit over distributed gradient descent for non-quadratics. We
partially bridge this gap by showing that even in the nonquadratic case, the convergence rate improves as the local
problems Ï†i become more similar.

4. DANE for Quadratic Objectives
In this Section, we analyze the performance of DANE on
quadratic objectives. We begin in Section 4.1 with an
analysis of DANE for arbitrary quadratic objectives Ï†i (w),
without stochastic assumptions, deriving a guarantee in
terms of the approximation error of the true Hessian. Then
in Section 4.2 we consider the stochastic setting where the

Newton-type Distributed Optimization

instantaneous objective f (w, z) is quadratic in w, utilizing a bound on the approximation error of the Hessian to
obtain a performance guarantee for DANE in terms of the
smoothness and strong convexity of f (w, z) . In Section
4.3 we also consider the behavior for stochastic optimization problems, where Î» is set as a function of the sample
size N = nm.

The proof appears in appendix D. Combining Lemma 2,
Lemma 1 and Theorem 2, we can conclude:
Theorem 3. In the stochastic setting, and when the instantaneous losses are quadratic with Î» 4 âˆ‡f (w, z) 4 L, then
after





(L/Î»)2
dm
Lkw0 âˆ’ wÌ‚k2
t=O
log
log
n
Î´


4.1. Quadratic Ï†i (w)

iterations of DANE, we have, with probability at least 1âˆ’Î´,
that FÌ‚ (w(t) ) â‰¤ FÌ‚ (wÌ‚) + .

We begin by considering the case where each local objective Ï†i (w) is quadratic, i.e. has a fixed Hessian. The overall
objective Ï†(w) is then of course also quadratic.
Theorem 2. After t iterations of DANE on quadratic objectives with Hessians Hi = âˆ‡2 Ï†i (w), we have:
kw(t) âˆ’ wÌ‚k â‰¤ kI âˆ’ Î· HÌƒ âˆ’1 Hkt2 kw(0) âˆ’ wÌ‚k,
Pm
1
âˆ’1
=
where
H = âˆ‡2 Ï†(w) = m
i=1 Hi and HÌƒ
P
n
1
âˆ’1
.
i=1 (Hi + ÂµI)
m
The proof appears in Appendix B. The theorem implies that
if kI âˆ’ Î· HÌƒ âˆ’1 Hk2 is smaller than 1, we get a linear convergence rate. Indeed, we would expect kI âˆ’ Î· HÌƒ âˆ’1 Hk2  1
as long as Î· is close to 1 and HÌƒ is a good approximation for
the true Hessian H, hence HÌƒ âˆ’1 H â‰ˆ I. In particular, if H
is not too ill-conditioned, and all Hi are sufficiently close
to their average H, we can indeed ensure HÌƒ â‰ˆ H. This is
captured by the following lemma (whose proof appears in
Appendix C):
Lemma 1. If 0 < Î» 4 H 4 L and for all i, kHi âˆ’ Hk2 â‰¤
2
Î², then setting Î· = 1 and Âµ = max{0, 8Î²Î» âˆ’ Î»}, we have:
( 2
2
4Î²
1
if 4Î²
2
2 â‰¤ 2
âˆ’1
Î»
Î»
kI âˆ’ HÌƒ Hk2 â‰¤
2
Î»
otherwise.
1 âˆ’ 16Î²
2
In the next Section, we consider the stochastic setting,
where we can obtain bounds for kHi âˆ’ Hk2 that improve
with the sample size, and plug these into Lemma 1 and
Theorem 2 to obtain a performance guarantee for DANE.
4.2. Stochastic Quadratic Problems
We now turn to a stochastic quadratic setting, where
Ï†i (w) = FÌ‚i (w) as in (3), and the instantaneous losses are
smooth and strongly convex quadratics. That is, for all z,
f (w, z) is quadratic in w and Î» 4 âˆ‡2w f (w, z) 4 L.
We first use a matrix concentration bound to establish that
all Hessians Hi = âˆ‡2 FÌ‚i (w) are close to each other, and
hence also to their average:
Lemma 2. If 0 4 âˆ‡2w f (w, z) 4 L for all z, then with
probability
q at least 1 âˆ’ Î´ over the samples, maxi kHi âˆ’

Hk2 â‰¤

âˆ‡2 FÌ‚ (w).

32L2 log(dm/Î´)
,
n

where Hi = âˆ‡2 FÌ‚i (w) and H =

The proof appears in Appendix E. From the theorem, we
see that if the condition number L/Î» is fixed, then as
n â†’ âˆž the number of required iterations decreases. In
fact, for any target sub-optimality , as long as the sample size is at least logarithmically
large, namely n =

â„¦ (L/Î»)2 log(dm) log( 1 ) , we can obtain the desired accuracy after a constant or even a single DANE iteration!
This is a mild requirement on the sample size, since N generally increases at least linearly with 1/.
We next turn to discuss the more challenging case where
the condition number decays with the sample size.
4.3. Analysis for Regularized Objectives
Consider a stochastic convex optimization scenario where
the instantaneous objectives f (w, z) are not strongly convex. For example, this is the case in linear prediction (including linear and kernel classification and regression, support vector machines, etc.), and more generally
for generalized linear objectives of the form f (w, z) =
`z (hw, Î¨(z)i). For such generalized linear objectives, the
Hessian âˆ‡2w f (w, z) is rank-1, and so certainly not strongly
convex, even if `z (Â·) is strongly convex.
Confronted with such non-strongly-convex objectives, a
standard approach is to perform empirical minimization on
a regularized objective (Shalev-Shwartz et al., 2009). That
is, to define the regularized instantaneous objective
fÎ» (w, z) = f (w, z) +

Î»
kwk2
2

(18)

and minimize the corresponding empirical objective FÌ‚Î» .
The instantaneous objective fÎ» (w, z) of the modified
stochastic optimization problem is now Î»-strongly convex.
If f (w, z) are G-Lipschitz in w, then we have (ShalevShwartz et al., 2009):
 2
G
âˆ—
F (wÌ‚Î» ) â‰¤ FÎ» (wÌ‚Î» ) â‰¤ FÎ» (wÎ» ) + O
Î»N


 2
Î»
G
2
= inf F (w) + kwk + O
w
2
Î»N


G2
2
,
â‰¤ inf F (w) + O Î»B +
Î»N
kwkâ‰¤B

Newton-type Distributed Optimization

where wÌ‚Î» = arg min FÌ‚Î» (w) and wÎ»âˆ— = arg
q min FÎ» (w).
2
The optimal choice of Î» in the above is Î» = BG2 N , where
B is a bound on the predictors we would like to compete
with, and with this Î» we get the optimal rate:
!
r
B 2 G2
F (wÌ‚Î» ) â‰¤ inf F (w) + O
.
(19)
N
kwkâ‰¤B
It is thus instructive
to consider
of DANE
q
qthe behavior

G2
G2
when Î» = Î˜
=Î˜
B2 N
B 2 nm . Plugging this
choice of Î» into Theorem 3, we get that the number of
DANE iterations behaves as:
 2 2

L B
O
Â·
m
Â·
log(dm)
log(1/)
.
(20)
G2
That is, unlike distributed gradient descent, or any other
relevant method we are aware of, the number of required
iterations / communication rounds does not increase with
the sample size, and only scales linearly with the number
of machines.

5. Convergence Analysis for Non-Quadratic
Objectives
As discussed above, it is notoriously difficult to obtain
generic global analysis of Newton-type methods. Our main
theoretical result in this paper is the analysis for quadratic
objectives, which we believe is also instructive for nonquadratics. Nevertheless, we complement this with a convergence analysis for generic objectives.
We therefore return to considering generic convex objectives Ï†i (w). We also do not make any stochastic assumptions. We only assume that each Ï†i (w) is Li -smooth and
Î»i strongly convex, and that the combined objective Ï†(w)
is L-smooth and Î»-strongly convex.
Theorem 4. Assume that for all i, w, z, Î»i 4 âˆ‡2 Ï†i (w) 4
Li and Î» 4 âˆ‡2 Ï†(w) 4 L. Let

m 
1 X
1
Î·L
Ï=
âˆ’
Î·Î».
m i=1 Âµ + Li
2(Âµ + Î»i )2
If Ï > 0, then the DANE iterates satisfy Ï†(w(t) ) âˆ’ Ï†(wÌ‚) â‰¤
(1 âˆ’ Ï)t [Ï†(w(0) ) âˆ’ Ï†(wÌ‚)].
The proof appears in Appendix F. The theorem establishes
that with any Âµ > 0 and small enough step-size Î·, DANE
converges to wÌ‚. If each Ï†i (w) is strongly convex, we can
also take Âµ = 0 and sufficiently small Î· and ensure convergence to wÌ‚. However, the optimal setting of Î· and Âµ
above is to take Âµ â†’ âˆž and set Î· = Âµ/L, in which case
Ï â†’ Î»/L, and we recover distributed gradient descent,
with the familiar gradient descent guarantee.

m=4

m=16

0

0
3

N=6*10

N=10*103

âˆ’2

âˆ’2

N=14*103

âˆ’4

âˆ’6

âˆ’4

5

10

15

20

âˆ’6

0

0

âˆ’2

âˆ’2

âˆ’4

âˆ’4

âˆ’6

5

10

15

20

âˆ’6

5

10

15

20

5

10

15

20

Figure 2. Synthetic dataset: Convergence rate for different number of machines m and sample sizes N . The top row presents
results for DANE, and the bottom row for ADMM. The x-axis is
the iteration number, and the y-axis is the logarithm (in base 10)
of the suboptimality.

We again emphasize that the analysis above is weak and
does not take into account the relationship between the local objectives Ï†i (w). We believe that the quadratic analysis of Section 4 better captures the true behavior of DANE.
Moreover, we can partially bridge this gap by the following
result, which shows that a variant of DANE enjoys a linear
convergence rate which improves as the local objectives Ï†i
become more similar to Ï† (the proof is in Appendix G):
Theorem 5. Assume that in the DANE procedure, we
(t)
replace step (âˆ—) by w(t) = w1 , and define h(Â·) =
h1 (Â·). If there exists Î³ > 0 such that âˆ€w, w0 , we have
Î³Dh (w; w0 ) â‰¤ DÏ† (w; w0 ) â‰¤ Î· âˆ’1 Dh (w; w0 ), then
Dh (wÌ‚; w(t) ) â‰¤ (1 âˆ’ Î·Î³)t Dh (wÌ‚; w(0) ).
If Âµ is small and Ï†i â‰ˆ Ï†, then we expect Î³ â‰ˆ 1 and Î· â‰ˆ 1.
In this case, Î·Î³ â‰ˆ 1, leading to fast convergence.

6. Experiments
In this section, we present preliminary experimental results on our proposed method. In terms of tuning Î·, Âµ,
we discovered that simply picking Î· = 1, Âµ = 0 (which
makes DANE closest to a Newton-type iteration, as discussed in Section 3) often results in the fastest convergence.
However, in unfavorable situations (such as when the data
size per machine is very small), this can also lead to nonconvergence. In those cases, convergence can be recovered
by slightly increasing Âµ to a small positive number. In the
experiments, we considered Âµ = 0, 3Î». These are considerably smaller than what our theory indicates, and we leave
the question of the best parameter choice to future research.
We begin by considering a simple quadratic problem us-

Newton-type Distributed Optimization
m
Âµ=0
Âµ = 3Î»
ADMM

2
2
9
3

4
2
9
3

8
2
9
5

COV1
16
2
9
9

32
2
9
16

64
3
9
31

2
6
14
24

4
6
14
20

ASTRO
8
16
6
6
14
14
16
16

32
12
14
14

64
*
14
20

2
5
10
23

4
5
10
23

MNIST-47
8
16
5
5
10
10
27
21

32
6
10
31

64
*
10
28

Figure 3. Number of iterations required to reach < 10âˆ’6 accuracy on 3 datasets, for varying number of machines m. Results are for
DANE using Î· = 1 and Âµ = 0, Î», 3Î», and for ADMM. * Indicates non-convergence after 100 iterations.
COV1

MNISTâˆ’47

ASTRO

0.231

DANE
ADMM
OSA
Opt

0.06

0.07

0.05

0.23

0.06

0.229

0.05

0.04
0.03
0

5
t

10

0.04

0

5
t

10

0

5
t

10

Figure 4. Average regularized smooth-hinge loss on the test set as a function of the iteration number. OSA represents bias-corrected
one-shot parameter averaging, which requires a single iteration. â€˜Optâ€™ is the average loss of the exact regularized loss minimizer.

ing a synthetic dataset, where all parameters can be explicitly controlled. We generated N i.i.d. training examples
(x, y) according to the model y = hx, wâˆ— i + Î¾ , x âˆ¼
N (0, Î£), Î¾ âˆ¼ N (0, 1), where x âˆˆ R500 , the covariance
matrix Î£ is diagonal with Î£i,i = iâˆ’1.2 , and wâˆ— is the
all-ones vector. Given a set of examples {x, y} which is
assumed to be randomly split to different machines, we
then solved
a standard ridge regression problem of the form
PN
minw N1 i=1 (hx, wiâˆ’y)2 +0.005w2 , using DANE (with
Î· = 1, Âµ = 0). Figure 2 shows the convergence behavior of
the algorithm for different number of machines m as the total number of examples N (and hence also the data size per
machine) increases. For comparison, we also implemented
distributed ADMM (Boyd et al., 2011), which is a standard method for distributed optimization but does not take
advantage of the statistical similarity between problems at
different machines. The results for DANE clearly indicate a
linear convergence rate, and moreover, that the rate of convergence improves with the data size, as predicted by our
analysis. In contrast, while more data improves the ADMM
accuracy after a fixed number of iterations, the convergence
rate is slower and does not improve with the data size5 .
We now turn to present results for solving a smooth nonquadratic problem, this time using non-synthetic datasets.
Specifically, we solved a regularized
loss minimization
PN
problem of the form minw N1 i=1 `(yi hxi , wi) + Î»2 kwk2 ,
where ` is the smooth hinge loss (as in (Shalev-Shwartz &
Zhang, 2013)) and the training examples {(xi , yi )} are randomly split among different machines. We experimented
on 3 datasets: COV1 and ASTRO-PH (as used in e.g.
5
To be fair, ADMM performs a single distributed averaging
computation per iteration, while DANE performs two. However,
counting iterations is a more realistic measure of performance,
since both methods also perform a full-scale local optimization at
each iteration.

(Shalev-Shwartz & Zhang, 2013; Rakhlin et al., 2012)), as
well as a subset of the MNIST digit recognition dataset
which focuses on discriminating the 4 from the 7 digits6 .
In figure 3, we present the number of iterations required for
DANE to reach accuracy < 10âˆ’6 for Î· = 1 and Âµ = 0, 3Î»,
and for different number of machines. We also report results for ADMM on the same datasets. As in the synthetic case, DANE explicitly takes advantage of the similarity between problems on different machines, and we
indeed observe that it tends to converge in less iterations
than ADMM. Finally, note that for Âµ = 0 and many machines (i.e. few data points per machine), DANE may not
converge, and increasing Âµ fixes this at the cost of slowing
down the average convergence rate.
Finally, we examine the convergence on these datasets in
terms of the average loss on the test set. In figure 4, we
present the results for m = 64 machines on the three
datasets, using DANE (with Âµ = 3Î») and ADMM. We
also present for comparison the objective value obtained
using one-shot parameter averaging (OSA), using bias correction as proposed in (Zhang et al., 2013). The figure
highlights the practical importance of multi-round communication algorithms: while DANE and ADMM converge
to the value achieved by the regularized loss minimizer,
the single-round OSA algorithm may return a significantly
suboptimal result.
Acknowledgements: Ohad Shamir and Nathan Srebro
are supported by the Intel ICRI-CI Institute. Ohad Shamir
is further supported by an Israel Science Foundation grant
425/13 and an FP7 Marie Curie CIG grant.
6
We used Î» = 10âˆ’5 for COV1, Î» = 0.0005 for ASTRO and
Î» = 0.001 for MNIST-47. For MNIST-47, we randomly chose
10,000 examples as the training set, and the rest of the examples
as a test set.

Newton-type Distributed Optimization

References
Agarwal, A., Chapelle, O., DudÄ±Ìk, M., and Langford, J. A
reliable effective terascale linear learning system. CoRR,
abs/1110.4198, 2011.
Beck, A. and Teboulle, M. Mirror descent and nonlinear
projected subgradient methods for convex optimization.
Oper. Res. Lett., 31(3):167â€“175, 2003.
Bekkerman, R., Bilenko, M., and Langford, J. Scaling up
machine learning: Parallel and distributed approaches.
Cambridge University Press, 2011.
Boyd, S.P., Parikh, N., Chu, E., Peleato, B., and Eckstein,
J. Distributed optimization and statistical learning via the
alternating direction method of multipliers. Foundations
and Trends in Machine Learning, 3(1):1â€“122, 2011.

Recht, B., Re, C., Wright, S., and Niu, F. Hogwild: A
lock-free approach to parallelizing stochastic gradient
descent. In NIPS, 2011.
RichtaÌrik, P. and TakaÌc, M. Distributed coordinate descent method for learning with big data.
CoRR,
abs/1310.2059, 2013.
Shalev-Shwartz, S. and Zhang, T. Stochastic dual coordinate ascent methods for regularized loss. Journal of
Machine Learning Research, 14(1):567â€“599, 2013.
Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan,
K. Stochastic convex optimization. In COLT, 2009.
Sridharan, K., Shalev-Shwartz, S., and Srebro, N. Fast rates
for regularized objectives. In Advances in Neural Information Processing Systems, pp. 1545â€“1552, 2008.

Cotter, A., Shamir, O., Srebro, N., and Sridharan, K. Better
mini-batch algorithms via accelerated gradient methods.
In NIPS, 2011.

Tropp, J. User-friendly tail bounds for sums of random
matrices. Foundations of Computational Mathematics,
12(4):389â€“434, 2012.

Dekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao,
L. Optimal distributed online prediction using minibatches. Journal of Machine Learning Research, 13:
165â€“202, 2012.

Yang, T. Trading computation for communication: Distributed stochastic dual coordinate ascent. In NIPS,
2013.

Deng, W. and Yin, W. On the global and linear convergence of the generalized alternating direction method of
multipliers. Technical report, Rice University Technical
Report TR12-14, 2012.
Duchi, J., Agarwal, A., and Wainwright, M. Dual averaging
for distributed optimization: Convergence analysis and
network scaling. IEEE Trans. Automat. Contr., 57(3):
592â€“606, 2012.
Hong, M. and Luo, Z.-Q. On the linear convergence of
the alternating direction method of multipliers. CoRR,
abs/1208.3922, 2012.
Mahajan, D., Keerthy, S., Sundararajan, S., and Bottou, L.
A parallel sgd method with strong convergence. CoRR,
abs/1311.0636, 2013.
Nemirovski, A. and Yudin, D. On cesaroâ€™s convergence
of the gradient descent method for finding saddle points
of convex-concave functions. Doklady Akademii Nauk
SSSR, 239(4), 1978.
Nemirovsky, A. and Yudin, D. Problem Complexity and
Method Efficiency in Optimization. Wiley-Interscience,
1983.
Rakhlin, A., Shamir, O., and Sridharan, K. Making gradient descent optimal for strongly convex stochastic optimization. In ICML, 2012.

Zhang, Y., Duchi, J., and Wainwright, M. Communicationefficient algorithms for statistical optimization. Journal
of Machine Learning Research, 14:3321â€“3363, 2013.
Zinkevich, M., Weimer, M., Smola, A., and Li, L. Parallelized stochastic gradient descent. In NIPS, pp. 2595â€“
2603, 2010.

