Communication-Efficient Distributed Optimization
using an Approximate Newton-type Method

Ohad Shamir
OHAD . SHAMIR @ WEIZMANN . AC . IL
Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot, Israel
Nathan Srebro
NATI @ TTIC . EDU
Toyota Technological Institute at Chicago and the Department of Computer Science, Technion, Haifa, Israel
Tong Zhang
TZHANG @ STAT. RUTGERS . EDU
Department of Statistics, Rutgers University, Piscataway NJ, USA, and Baidu Inc., Beijing, China

Abstract

N = nm independent samples evenly and randomly distributed among the machines. Each machine i can construct
a local empirical (sample) estimate of F (w):

We present a novel Newton-type method for distributed optimization, which is particularly well
suited for stochastic optimization and learning
problems. For quadratic objectives, the method
enjoys a linear rate of convergence which provably improves with the data size, requiring an
essentially constant number of iterations under
reasonable assumptions. We provide theoretical
and empirical evidence of the advantages of our
method compared to other approaches, such as
one-shot parameter averaging and ADMM.

n

œÜi (w) = FÃÇi (w) =

1X
f (w, zij )
n j=1

and the overall empirical objective is then:
m

œÜ(w) = FÃÇ (w) =

1 X
1 X
FÃÇi (w) =
f (w, zij ). (4)
m i=1
nm i,j

We can then use the empirical risk minimizer (ERM)
wÃÇ = arg min FÃÇ (w)

1. Introduction
We consider the problem of distributed optimization, where
each of m machines has access to a function œÜi : Rd ‚Üí R,
i = 1, . . . , m, and we would like to minimize their average
œÜ(w) =

1
m

m
X

œÜi (w).

(1)

i=1

We are particularly interested in a stochastic optimization
(learning) setting, where the ultimate goal is to minimize
some stochastic (population) objective (e.g. expected loss
or generalization error)
F (w) = E [f (w, z)]
z‚àºD

(2)

and each of the m machines has access to n i.i.d. samples
zi1 , . . . , zin from the source distribution D, for a total of
st

(3)

Proceedings of the 31 International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

(5)

as an approximate minimizer of F (w). Since our interest lies mostly with this stochastic optimization setting, we
will denote wÃÇ = arg min œÜ(w) even when the optimization objective œÜ(w) is not an empirical approximation to a
stochastic objective.
When considering distributed optimization, two resources
are at play: the amount of processing on each machine, and
the communication between machines. In this paper, we
focus on algorithms which alternate between a local optimization procedure at each machine, and a communication round involving simple map-reduce operations such as
distributed averaging of vectors in Rd . Since the cost of
communication is very high in practice (Bekkerman et al.,
2011), our goal is to develop methods which quickly optimize the empirical objective FÃÇ (¬∑), using a minimal number
of such iterations.
One-Shot Averaging A straight-forward single-iteration
approach is for each machine to optimize its own local objective, obtaining
wÃÇi = arg min œÜi (w),

(6)

Newton-type Distributed Optimization

and then to compute their average:
m

wÃÑ =

1 X
wÃÇi .
m i=1

(7)

This approach, which we refer to as ‚Äúone-shot parameter averaging‚Äù, was recently considered in Zinkevich et al.
(2010) and further analyzed by Zhang et al. (2013). The
latter also proposed a bias-corrected improvement which
perturbs each wÃÇi using the optimum on a bootstrap sample. This approach gives only an approximate minimizer of
œÜ(w) with some finite suboptimality, rather then allowing
us converge to wÃÇ (i.e. to obtain solutions with any desired
suboptimality ). Although approximate solutions are often
sufficient for stochastic optimization, we prove in Section
2 that the one-shot solution wÃÑ can be much worse in terms
of minimizing the population objective F (w), compared to
the actual empirical minimizer wÃÇ. It does not seem possible
to address this suboptimality by more clever averaging, and
instead additional rounds of communications appear necessary.
Gradient Descent One possible multi-round approach to
distributed optimization is a distributed implementation of
gradient descent: at each iteration each machine calculates
‚àáœÜi (w(t) ) at the current iterate w(t) , and then these are averaged to obtain the overall gradient ‚àáœÜ(w(t) ), and a gradient step is taken. As the iterates are then standard gradient
descent iterates, the number of iterations, and so also number of communication rounds, is linear in the conditioning
of the problem ‚Äì or, if accelerated gradient descent is used,
proportional to the square root of the condition number: If
œÜ(w) is L-smooth and Œª-strongly convex, then
r
 !
1
L
log
(8)
O
Œª

iterations are needed to attain an -suboptimal solution.
The polynomial dependence on the condition number may
be disappointing, as in many problems the parameter of
strong convexity Œª might be very small. E.g., when strong
convexity arises from regularization, as in many stochastic
optimization problems, Œª decreases with the
‚àö overall sample
size N = nm, and is typically at most 1/ nm (Sridharan
et al. 2008; Shalev-Shwartz et al. 2009; and see also Section 4.3 below). The number of iterations / communication
rounds needed ‚àö
for distributed accelerated gradient descent
then scales as 4 nm, i.e. increases polynomially with the
sample size.
Instead of gradient descent, one may also consider more
sophisticated methods which utilize gradient information,
such as quasi-Newton methods. For example, a distributed implementation using L-BFGS has been proposed
in (Agarwal et al., 2011). However, no guarantee better

then (8) can be ensured for gradient-based methods (Nemirovsky & Yudin, 1983), and we thus may still get a polynomial dependence on the sample size.
ADMM and other approaches Another popular approach is distributed alternating direction method of multipliers (ADMM, e.g. Boyd et al. 2011), where the machines
alternate between computing shared dual variables in a distributed manner, and solving augmented Lagrangian problems with respect to their local data. However, the convergence of ADMM can be slow. Although recent works
proved a linear convergence rate under favorable assumptions (Deng & Yin, 2012; Hong & Luo, 2012), we are
not aware of any analysis where the number of iterations
/ communication rounds doesn‚Äôt scale strongly with the
condition number, and hence the sample size, for learning applications. A similar dependence occurs with other
recently-proposed algorithms for distributed optimization
(e.g. Yang, 2013; Mahajan et al., 2013; Dekel et al., 2012;
Cotter et al., 2011; Duchi et al., 2012). We also mention
that our framework is orthogonal to much recent work on
distributed coordinate descent methods (e.g. Recht et al.,
2011; RichtaÃÅrik & TakaÃÅc, 2013), which assume the data is
split feature-wise rather than instance-wise.
Our Method The method we propose can be viewed as
an approximate Newton-like method, where at each iteration, instead of a gradient step, we take a step appropriate for the geometry of the problem, as estimated on
each machine separately. In particular, for quadratic objectives, the method can be seen as taking approximate
Newton steps, where each machine i implicitly uses its local Hessian ‚àá2 œÜi (w) (although no Hessians are explicitly
computed!). Unlike ADMM, our method can take advantage of the fact that for machine learning applications, the
sub-problems are usually similar: œÜi ‚âà œÜ. We refer to our
method as DANE‚ÄîDistributed Approximate NEwton.
DANE is applicable to any smooth and strongly convex
problem. However, as is typical of Newton and Newtonlike methods, its generic analysis is not immediately apparent. For general functions, we can show convergence,
but cannot rigorously prove improvement over gradient descent. Instead, in order to demonstrate DANE‚Äôs advantages
and give a sense of its benefits, we focus our theoretical
analysis on quadratic objectives. For stochastic quadratic
objectives, where f (w, z) is L-smooth and Œª-strongly convex in w ‚àà Rd , we show that


1
(L/Œª)2
log(dm) log( )
(9)
O
n

iterations are sufficient for DANE to find wÃÉ such that with
high probability FÃÇ (wÃÉ) ‚â§ FÃÇ (wÃÇ) + . When L/Œª is fixed
and the number of examples n per machine is large (the

Newton-type Distributed Optimization

regime considered by Zhang et al. 2013), (9) establishes
convergence after a constant number of‚àö
iterations / communication rounds. When Œª scales as 1/ nm, as discussed
above, (9) yields convergence to the empirical minimizer
in a number of iterations that scales roughly linearly with
the number of machines m, but not with the sample size
N = nm. To the best of our knowledge, this is the first
algorithm which provably has such a behavior. We also
provide evidence for similar behavior on non-quadratic objectives.
Notation and Definitions For vectors, kvk is always the
Euclidean norm, and for matrices kAk2 is the spectral
norm. We use Œª 4 A 4 L to indicate that the eigenvalues of A are bounded between Œª and L. We say that a
twice differentiable1 function f (w) is Œª-strongly convex or
L-smooth, iff for all w, its Hessian is bounded from below
by Œª (i.e. Œª 4 ‚àá2 f (w)), or above by L (i.e. ‚àá2 f (w) 4 L)
respectively.

2. Stochastic Optimization and One-shot
Parameter Averaging
In a stochastic optimization setting, where the true objective is the population objective F (w), there is a limit to the
accuracy with which we can minimize F (w) given only
N = nm samples, even using the exact empirical minimizer wÃÇ. It is thus reasonable to compare the suboptimality
of F (w) when using the exact wÃÇ to what can be attained using distributed optimization with limited communication.
When f (w, z) has gradients with bounded second moments, namely when ‚àÄw Ez k‚àáw f (w, z)k2 ‚â§ G2 , and
F (w) is Œª-strongly convex, then (Shalev-Shwartz et al.,
2009)2
 2 
 2
G
G
= inf F (w)+O
E[F (wÃÇ)] ‚â§ F (w‚àó )+O
w
ŒªN
Œªnm
(10)
where w‚àó = arg min F (w) is the population minimizer
and the expectation is with respect to the random sample
of size N = nm. One might
then ask whether a subop
G2
timality of  = O Œªnm
can be also be achieved using
a few, perhaps only one, round of communication. This
is different from seeking a distributed optimization method
that achieves any arbitrarily small empirical suboptimality,
and thus converges to wÃÇ, but might be sufficient in terms of
stochastic optimization.
For one-shot parameter averaging, Zhang et al. (2013,
1

All our results hold also for weaker definitions of smoothness
and strong convexity which do not require twice differentiability.
2
More precisely, (Shalev-Shwartz et al., 2009) shows this assuming k‚àáw f (w, z)k2 ‚â§ G2 for all w, z, but the proof easily
carries over to this case.

Corollary 2) recently showed that for Œª-strongly convex
objectives, and when moments of the first, second and third
derivatives of f (w, z) are bounded by G, L, and M respectively3 , then


G2
L2 G2 log d
G4 M 2
‚àó 2
E kwÃÑ ‚àí w k ‚â§ OÃÉ
,
+ 6 2 +
Œª2 nm
Œª n
Œª4 n2
(11)
where wÃÑ is the one-shot average estimator defined in (7).
This implies that the population suboptimality E[F (wÃÑ)] ‚àí
F (w‚àó ) is bounded by

OÃÉ

L3 G2 log d
LG4 M 2
LG2
+
+
Œª2 nm
Œª 6 n2
Œª4 n2


.

(12)

Zhang et al. (2013) argued that the dependence on the sample size mn above is essentially optimal: the dominant
term (as n ‚Üí ‚àû, and in particular when n  m) scales
as 1/(nm), which is the same as for the empirical minimizer wÃÇ (as in eq. 10), and so one-shot parameter averaging achieves the same population suboptimality rate, using only a single round of communication, as the best rate
we can hope for using unlimited communication, or if all
N = nm samples were on the same machine. Moreover,
the O(n‚àí2 ) terms can be replaced by a O(n‚àí3 ) term using
an appropriate bias-correction procedure.
However, this view ignores the dependence on the other
parameters, and in particular the strong convexity parameter Œª, which is much worse in (12) relative to (10). The
strong convexity parameter often arises from an explicit
regularization, and decays as the sample size increases.
E.g., in regularized loss minimization and SVM-type problems (Sridharan et al., 2008), as well as more generally
for stochastic convex optimization (Shalev-Shwartz et al.,
2009), the regularization parameter, and hence the strong
1
convexity parameter, decreases as ‚àö1N = ‚àönm
. In practice, Œª is often chosen even smaller, possibly
as
small as
‚àö
1
.
Unfortunately,
substituting
Œª
=
O(1/
nm)
in
(12) reN
sults in a useless bound, where even the first term does not
decrease with the sample size.
Of course, this strong dependence on Œª might be an artifact of the analysis of Zhang et al.. However, in Theorem 1
below, we show that even
‚àö in a simple one-dimensional example, when Œª ‚â§ O(1/ n), the population sub-optimality
of the one-shot estimator (using m machines and a total
of nm samples), can be no better then the population suboptimality using just n samples, and much worse than what
3

The exact conditions in Zhang et al. (2013) refer to various high order moments, but are in any case satisfied when
k‚àáw f (w, z)k ‚â§ G, k‚àá2w f (w, z)k2 ‚â§ L and ‚àá2 f (w, z) is M Lipschitz in the spectral norm. For learning problems, all derivatives of the objective can be bounded in terms of a bound on the
data and bounds on the derivative of a scalar loss function, and
are less of a concern to us.

Newton-type Distributed Optimization

can be attained using nm samples. In other words, one-shot
averaging does not give any benefit over using only the data
on a single machine, and ignoring all other (m ‚àí 1)n data
points.
Theorem1. For any
 per-machine sample size n ‚â• 9, and
1
any Œª ‚àà 0, 9‚àön , there exists a distribution D over examples and a stochastic optimization problem on a convex
set4 W ‚äÇ R, such that:
‚Ä¢ f (w; z) is Œª-strongly convex, infinitely differentiable,
and ‚àÄw‚ààW Ez [k‚àáf (w; z)k2 ] ‚â§ 9.
‚Ä¢ For any number of machines m, if we run one-shot
parameter averaging to compute wÃÑ, it holds for some
universal constants C1 , C2 , C3 , C4 that
E[kwÃÑ ‚àí w‚àó k2 ] ‚â•

C2
C1
, E[kwÃÇ ‚àí w‚àó k2 ] ‚â§ 2
2
Œª n
Œª nm

E[F (wÃÑ)]‚àíF (w‚àó ) ‚â•

The crux of the method is the local optimization performed
on each machine at each iteration:
(t)

wi = arg min[œÜi (w)

3. Distributed Approximate Newton-type
Method
We now describe a new iterative method for distributed
optimization. The method performs two distributed averaging computations per iteration, and outputs a predictor
w(t) which, under suitable parameter choices, converges
to the optimum wÃÇ. The method, which we refer to as
DANE (Distributed Approximate NEwton-type Method) is
described in Figure 1.
DANE maintains an agreed-upon iterate w(t) , which is
synchronized among all machines at the end of each iteration. In each iteration, we first compute the gradient
‚àáœÜ(w(t‚àí1) ) at the current iterate, by averaging the local
gradients ‚àáœÜi (w(t‚àí1) ). Each machine then performs a separate local optimization, based on its own local objective
Following the framework of Zhang et al., we present an example where the optimization is performed on a bounded set,
which ensures that the gradient moments are bounded. However,
this is not essential and the same result can be shown when the
domain of optimization is R.

(13)

w

‚àí (‚àáœÜi (w(t‚àí1) ) ‚àí Œ∑‚àáœÜ(w(t‚àí1) ))> w +

¬µ
kw ‚àí w(t‚àí1) k22 ]
2

To understand this local optimization, recall the definition
of the Bregman divergence corresponding to a strongly
convex function œà:
Dœà (w0 ; w) = œà(w0 ) ‚àí œà(w) ‚àí h‚àáœà(w), w0 ‚àí wi.
Now, for each local objective œÜi , consider the regularized
local objective, defined as

C3
C4
, E[F (wÃÇ)]‚àíF (w‚àó ) ‚â§
Œªn
Œªnm

The intuition behind the construction of Theorem 1 is that
when Œª is small, the deviation of each machine output wÃÇi
from w‚àó is large, and its expectation is biased away from
w‚àó . The exact bias amount is highly problem-dependent,
and cannot be eliminated by any fixed averaging scheme.
Since bias is not reduced by averaging, the optimization
error does not scale down with the number of machines
m. The full construction and proof appear in appendix A.
In the appendix we also show that the bias correction proposed by Zhang et al. to reduce the lower-order terms in
equation (11) does not remedy this problem.

4

œÜi (w) and the computed global gradient ‚àáœÜ(w(t) ), to ob(t)
tain a local iterate wi . These local iterates are averaged to
obtain the centralized iterate w(t) .

hi (w) = œÜi (w) +

¬µ
kwk2
2

and its corresponding Bregman divergence:
Di (w0 ; w) = Dhi (w0 ; w) = DœÜi (w0 ; w) +

¬µ 0
kw ‚àí wk2 .
2

It is not difficult to check that the local optimization problem (13) can be written as
(t)

wi = arg min œÜ(w(t‚àí1) ) + h‚àáœÜ(w(t‚àí1) ), w ‚àí w(t‚àí1) i
w

1
+ Di (w; w(t‚àí1) ),
Œ∑

(14)

where we also added the terms œÜ(w(t‚àí1) ) +
h‚àáœÜ(w(t‚àí1) ), w(t‚àí1) i which do not depend on w
and do not affect the optimization. The first two terms
in (14) are thus a linear approximation of the overall
objective œÜ(w) about the current iterate w(t‚àí1) , and do not
depend on the machine i. What varies from machine to
machine is the potential function used to localize the linear
approximation. The update (14) is in-fact a mirror descent
update (Nemirovski & Yudin, 1978; Beck & Teboulle,
2003) using the potential function hi , and step size Œ∑.
Let us examine this form of update. When ¬µ ‚Üí ‚àû the
potential function essentially becomes a squared Euclidean
norm, as in gradient descent updates. In fact, when Œ∑, ¬µ ‚Üí
.
‚àû as Œ∑ÃÉ = ¬µŒ∑ remains constant, the update (14) becomes a
standard gradient descent update on œÜ(w) with stepsize Œ∑ÃÉ.
In this extreme, the update does not use the local objective
œÜi (w), beyond the centralized calculation of ‚àáœÜ(w), the
updates (14) are the same on all machines, and the second
round of communication is not needed. DANE reduces to
distributed gradient
 descent, with its iteration complexity
.
of O L
log(1/)
Œª

Newton-type Distributed Optimization

Procedure DANE
Parameter: learning rate Œ∑ > 0 and regularizer ¬µ > 0
Initialize: Start at some w(0) , e.g. w(0) = 0
Iterate: for t = 1, 2, . . .
Pm
1
(t‚àí1)
Compute ‚àáœÜ(w(t‚àí1) ) = m
i=1 ‚àáœÜi (w  ) and distribute to all machines

(t)
For each machine i, solve wi = arg minw œÜi (w) ‚àí (‚àáœÜi (w(t‚àí1) ) ‚àí Œ∑‚àáœÜ(w(t‚àí1) ))> w + ¬µ2 kw ‚àí w(t‚àí1) k22
P
(t)
m
1
(‚àó)
Compute w(t) = m
i=1 wi and distribute to all machines
end
Figure 1. Distributed Approximate NEwton-type method (DANE)

At the other extreme, consider the case where ¬µ = 0 and
all local objectives are equal, i.e. hi (w) = œÜi (w) = œÜ(w).
Substituting the definition of the Bregman divergence into
(t)
(14), or simply investigating (13), we can see that wi =
arg min œÜi (w) = arg min œÜ(w) = wÃÇ. That is, DANE converges in a single iteration to the overall empirical optimum. This is an ideal Newton-type iteration, where the
potential function is perfectly aligned with the objective.
Of course, if œÜi (w) = œÜ(w) for all machines i, we would
not need to perform distributed optimization in the first
place. Nevertheless, as n ‚Üí ‚àû, we can hope that œÜi (w) are
similar enough to each other, such that (14) approximates
such an ideal Newton-type iteration, gets us very close to
the optimum, and very few such iterations are sufficient.
In particular, consider the case where œÜi (w), and hence also
œÜ(w) are quadratic. In this case, the Bregman divergence
Di (w; w(t‚àí1) ) takes the form:
1
(w ‚àí w(t‚àí1) )> (‚àá2 œÜi (w(t‚àí1) ) + ¬µI)(w ‚àí w(t‚àí1) ), (15)
2
and the update (14) can be solved in closed form:
(t)

wi = w(t‚àí1) ‚àí Œ∑(‚àá2 œÜi (w(t‚àí1) ) + ¬µI)‚àí1 ‚àáœÜ(w(t‚àí1) )
w(t) = w(t‚àí1)
‚àíŒ∑

1 X 2
(‚àá œÜi (w(t‚àí1) ) + ¬µI)‚àí1
m i

!
‚àáœÜ(w(t‚àí1) ).
(16)

Contrast this with the true Newton update:
!‚àí1
1 X 2
(t)
(t‚àí1)
(t‚àí1)
w =w
‚àíŒ∑
‚àá œÜi (w
)
‚àáœÜ(w(t‚àí1) ).
m i
(17)
The difference here is that in (16) we approximate the inverse of the average of the local Hessians with the average
of the inverse of the Hessians (plus a possible regularizer).
Again we see that the DANE update (16) approximates the
true Newton update (17), which can be performed in a distributed fashion without communicating the Hessians.

For a quadratic objective, a single Newton update is enough
to find the exact optimum. In Section 4 we rigorously analyze the effects of the distributed approximation, and quantify the number of DANE iterations (and thus rounds of
communication) required.
For a general convex, but non-quadratic, objective, the
standard Newton approach is to use a quadratic approximation to the ideal Bregman divergence DœÜ . This leads to
the familiar quadratic Newton update in terms of the Hessian. DANE uses a different sort of approximation to DœÜ :
we use a non-quadratic approximation, based on the entire
objective and not just a local quadratic approximation, but
approximate the potential on each node separately. In the
stochastic setting, this approximation becomes better and
better, and thus the required number of iterations decrease,
as n ‚Üí ‚àû.
Since it is notoriously difficult to provide good global
analysis for Newton-type methods, we will investigate the
global convergence behavior of DANE carefully in the next
Section but only for quadratic objective functions. This
analysis can also be viewed as indicative for non-quadratic
objectives, as locally they can be approximated by quadratics and so should enjoy the same behavior, at least asymptotically. For non-quadratics, we provide a rigorous convergence guarantee when the stepsize Œ∑ is sufficiently small
or the regularization parameter ¬µ is sufficiently large (in
Section 5). However, this analysis does not show a benefit over distributed gradient descent for non-quadratics. We
partially bridge this gap by showing that even in the nonquadratic case, the convergence rate improves as the local
problems œÜi become more similar.

4. DANE for Quadratic Objectives
In this Section, we analyze the performance of DANE on
quadratic objectives. We begin in Section 4.1 with an
analysis of DANE for arbitrary quadratic objectives œÜi (w),
without stochastic assumptions, deriving a guarantee in
terms of the approximation error of the true Hessian. Then
in Section 4.2 we consider the stochastic setting where the

Newton-type Distributed Optimization

instantaneous objective f (w, z) is quadratic in w, utilizing a bound on the approximation error of the Hessian to
obtain a performance guarantee for DANE in terms of the
smoothness and strong convexity of f (w, z) . In Section
4.3 we also consider the behavior for stochastic optimization problems, where Œª is set as a function of the sample
size N = nm.

The proof appears in appendix D. Combining Lemma 2,
Lemma 1 and Theorem 2, we can conclude:
Theorem 3. In the stochastic setting, and when the instantaneous losses are quadratic with Œª 4 ‚àáf (w, z) 4 L, then
after





(L/Œª)2
dm
Lkw0 ‚àí wÃÇk2
t=O
log
log
n
Œ¥


4.1. Quadratic œÜi (w)

iterations of DANE, we have, with probability at least 1‚àíŒ¥,
that FÃÇ (w(t) ) ‚â§ FÃÇ (wÃÇ) + .

We begin by considering the case where each local objective œÜi (w) is quadratic, i.e. has a fixed Hessian. The overall
objective œÜ(w) is then of course also quadratic.
Theorem 2. After t iterations of DANE on quadratic objectives with Hessians Hi = ‚àá2 œÜi (w), we have:
kw(t) ‚àí wÃÇk ‚â§ kI ‚àí Œ∑ HÃÉ ‚àí1 Hkt2 kw(0) ‚àí wÃÇk,
Pm
1
‚àí1
=
where
H = ‚àá2 œÜ(w) = m
i=1 Hi and HÃÉ
P
n
1
‚àí1
.
i=1 (Hi + ¬µI)
m
The proof appears in Appendix B. The theorem implies that
if kI ‚àí Œ∑ HÃÉ ‚àí1 Hk2 is smaller than 1, we get a linear convergence rate. Indeed, we would expect kI ‚àí Œ∑ HÃÉ ‚àí1 Hk2  1
as long as Œ∑ is close to 1 and HÃÉ is a good approximation for
the true Hessian H, hence HÃÉ ‚àí1 H ‚âà I. In particular, if H
is not too ill-conditioned, and all Hi are sufficiently close
to their average H, we can indeed ensure HÃÉ ‚âà H. This is
captured by the following lemma (whose proof appears in
Appendix C):
Lemma 1. If 0 < Œª 4 H 4 L and for all i, kHi ‚àí Hk2 ‚â§
2
Œ≤, then setting Œ∑ = 1 and ¬µ = max{0, 8Œ≤Œª ‚àí Œª}, we have:
( 2
2
4Œ≤
1
if 4Œ≤
2
2 ‚â§ 2
‚àí1
Œª
Œª
kI ‚àí HÃÉ Hk2 ‚â§
2
Œª
otherwise.
1 ‚àí 16Œ≤
2
In the next Section, we consider the stochastic setting,
where we can obtain bounds for kHi ‚àí Hk2 that improve
with the sample size, and plug these into Lemma 1 and
Theorem 2 to obtain a performance guarantee for DANE.
4.2. Stochastic Quadratic Problems
We now turn to a stochastic quadratic setting, where
œÜi (w) = FÃÇi (w) as in (3), and the instantaneous losses are
smooth and strongly convex quadratics. That is, for all z,
f (w, z) is quadratic in w and Œª 4 ‚àá2w f (w, z) 4 L.
We first use a matrix concentration bound to establish that
all Hessians Hi = ‚àá2 FÃÇi (w) are close to each other, and
hence also to their average:
Lemma 2. If 0 4 ‚àá2w f (w, z) 4 L for all z, then with
probability
q at least 1 ‚àí Œ¥ over the samples, maxi kHi ‚àí

Hk2 ‚â§

‚àá2 FÃÇ (w).

32L2 log(dm/Œ¥)
,
n

where Hi = ‚àá2 FÃÇi (w) and H =

The proof appears in Appendix E. From the theorem, we
see that if the condition number L/Œª is fixed, then as
n ‚Üí ‚àû the number of required iterations decreases. In
fact, for any target sub-optimality , as long as the sample size is at least logarithmically
large, namely n =

‚Ñ¶ (L/Œª)2 log(dm) log( 1 ) , we can obtain the desired accuracy after a constant or even a single DANE iteration!
This is a mild requirement on the sample size, since N generally increases at least linearly with 1/.
We next turn to discuss the more challenging case where
the condition number decays with the sample size.
4.3. Analysis for Regularized Objectives
Consider a stochastic convex optimization scenario where
the instantaneous objectives f (w, z) are not strongly convex. For example, this is the case in linear prediction (including linear and kernel classification and regression, support vector machines, etc.), and more generally
for generalized linear objectives of the form f (w, z) =
`z (hw, Œ®(z)i). For such generalized linear objectives, the
Hessian ‚àá2w f (w, z) is rank-1, and so certainly not strongly
convex, even if `z (¬∑) is strongly convex.
Confronted with such non-strongly-convex objectives, a
standard approach is to perform empirical minimization on
a regularized objective (Shalev-Shwartz et al., 2009). That
is, to define the regularized instantaneous objective
fŒª (w, z) = f (w, z) +

Œª
kwk2
2

(18)

and minimize the corresponding empirical objective FÃÇŒª .
The instantaneous objective fŒª (w, z) of the modified
stochastic optimization problem is now Œª-strongly convex.
If f (w, z) are G-Lipschitz in w, then we have (ShalevShwartz et al., 2009):
 2
G
‚àó
F (wÃÇŒª ) ‚â§ FŒª (wÃÇŒª ) ‚â§ FŒª (wŒª ) + O
ŒªN


 2
Œª
G
2
= inf F (w) + kwk + O
w
2
ŒªN


G2
2
,
‚â§ inf F (w) + O ŒªB +
ŒªN
kwk‚â§B

Newton-type Distributed Optimization

where wÃÇŒª = arg min FÃÇŒª (w) and wŒª‚àó = arg
q min FŒª (w).
2
The optimal choice of Œª in the above is Œª = BG2 N , where
B is a bound on the predictors we would like to compete
with, and with this Œª we get the optimal rate:
!
r
B 2 G2
F (wÃÇŒª ) ‚â§ inf F (w) + O
.
(19)
N
kwk‚â§B
It is thus instructive
to consider
of DANE
q
qthe behavior

G2
G2
when Œª = Œò
=Œò
B2 N
B 2 nm . Plugging this
choice of Œª into Theorem 3, we get that the number of
DANE iterations behaves as:
 2 2

L B
O
¬∑
m
¬∑
log(dm)
log(1/)
.
(20)
G2
That is, unlike distributed gradient descent, or any other
relevant method we are aware of, the number of required
iterations / communication rounds does not increase with
the sample size, and only scales linearly with the number
of machines.

5. Convergence Analysis for Non-Quadratic
Objectives
As discussed above, it is notoriously difficult to obtain
generic global analysis of Newton-type methods. Our main
theoretical result in this paper is the analysis for quadratic
objectives, which we believe is also instructive for nonquadratics. Nevertheless, we complement this with a convergence analysis for generic objectives.
We therefore return to considering generic convex objectives œÜi (w). We also do not make any stochastic assumptions. We only assume that each œÜi (w) is Li -smooth and
Œªi strongly convex, and that the combined objective œÜ(w)
is L-smooth and Œª-strongly convex.
Theorem 4. Assume that for all i, w, z, Œªi 4 ‚àá2 œÜi (w) 4
Li and Œª 4 ‚àá2 œÜ(w) 4 L. Let

m 
1 X
1
Œ∑L
œÅ=
‚àí
Œ∑Œª.
m i=1 ¬µ + Li
2(¬µ + Œªi )2
If œÅ > 0, then the DANE iterates satisfy œÜ(w(t) ) ‚àí œÜ(wÃÇ) ‚â§
(1 ‚àí œÅ)t [œÜ(w(0) ) ‚àí œÜ(wÃÇ)].
The proof appears in Appendix F. The theorem establishes
that with any ¬µ > 0 and small enough step-size Œ∑, DANE
converges to wÃÇ. If each œÜi (w) is strongly convex, we can
also take ¬µ = 0 and sufficiently small Œ∑ and ensure convergence to wÃÇ. However, the optimal setting of Œ∑ and ¬µ
above is to take ¬µ ‚Üí ‚àû and set Œ∑ = ¬µ/L, in which case
œÅ ‚Üí Œª/L, and we recover distributed gradient descent,
with the familiar gradient descent guarantee.

m=4

m=16

0

0
3

N=6*10

N=10*103

‚àí2

‚àí2

N=14*103

‚àí4

‚àí6

‚àí4

5

10

15

20

‚àí6

0

0

‚àí2

‚àí2

‚àí4

‚àí4

‚àí6

5

10

15

20

‚àí6

5

10

15

20

5

10

15

20

Figure 2. Synthetic dataset: Convergence rate for different number of machines m and sample sizes N . The top row presents
results for DANE, and the bottom row for ADMM. The x-axis is
the iteration number, and the y-axis is the logarithm (in base 10)
of the suboptimality.

We again emphasize that the analysis above is weak and
does not take into account the relationship between the local objectives œÜi (w). We believe that the quadratic analysis of Section 4 better captures the true behavior of DANE.
Moreover, we can partially bridge this gap by the following
result, which shows that a variant of DANE enjoys a linear
convergence rate which improves as the local objectives œÜi
become more similar to œÜ (the proof is in Appendix G):
Theorem 5. Assume that in the DANE procedure, we
(t)
replace step (‚àó) by w(t) = w1 , and define h(¬∑) =
h1 (¬∑). If there exists Œ≥ > 0 such that ‚àÄw, w0 , we have
Œ≥Dh (w; w0 ) ‚â§ DœÜ (w; w0 ) ‚â§ Œ∑ ‚àí1 Dh (w; w0 ), then
Dh (wÃÇ; w(t) ) ‚â§ (1 ‚àí Œ∑Œ≥)t Dh (wÃÇ; w(0) ).
If ¬µ is small and œÜi ‚âà œÜ, then we expect Œ≥ ‚âà 1 and Œ∑ ‚âà 1.
In this case, Œ∑Œ≥ ‚âà 1, leading to fast convergence.

6. Experiments
In this section, we present preliminary experimental results on our proposed method. In terms of tuning Œ∑, ¬µ,
we discovered that simply picking Œ∑ = 1, ¬µ = 0 (which
makes DANE closest to a Newton-type iteration, as discussed in Section 3) often results in the fastest convergence.
However, in unfavorable situations (such as when the data
size per machine is very small), this can also lead to nonconvergence. In those cases, convergence can be recovered
by slightly increasing ¬µ to a small positive number. In the
experiments, we considered ¬µ = 0, 3Œª. These are considerably smaller than what our theory indicates, and we leave
the question of the best parameter choice to future research.
We begin by considering a simple quadratic problem us-

Newton-type Distributed Optimization
m
¬µ=0
¬µ = 3Œª
ADMM

2
2
9
3

4
2
9
3

8
2
9
5

COV1
16
2
9
9

32
2
9
16

64
3
9
31

2
6
14
24

4
6
14
20

ASTRO
8
16
6
6
14
14
16
16

32
12
14
14

64
*
14
20

2
5
10
23

4
5
10
23

MNIST-47
8
16
5
5
10
10
27
21

32
6
10
31

64
*
10
28

Figure 3. Number of iterations required to reach < 10‚àí6 accuracy on 3 datasets, for varying number of machines m. Results are for
DANE using Œ∑ = 1 and ¬µ = 0, Œª, 3Œª, and for ADMM. * Indicates non-convergence after 100 iterations.
COV1

MNIST‚àí47

ASTRO

0.231

DANE
ADMM
OSA
Opt

0.06

0.07

0.05

0.23

0.06

0.229

0.05

0.04
0.03
0

5
t

10

0.04

0

5
t

10

0

5
t

10

Figure 4. Average regularized smooth-hinge loss on the test set as a function of the iteration number. OSA represents bias-corrected
one-shot parameter averaging, which requires a single iteration. ‚ÄòOpt‚Äô is the average loss of the exact regularized loss minimizer.

ing a synthetic dataset, where all parameters can be explicitly controlled. We generated N i.i.d. training examples
(x, y) according to the model y = hx, w‚àó i + Œæ , x ‚àº
N (0, Œ£), Œæ ‚àº N (0, 1), where x ‚àà R500 , the covariance
matrix Œ£ is diagonal with Œ£i,i = i‚àí1.2 , and w‚àó is the
all-ones vector. Given a set of examples {x, y} which is
assumed to be randomly split to different machines, we
then solved
a standard ridge regression problem of the form
PN
minw N1 i=1 (hx, wi‚àíy)2 +0.005w2 , using DANE (with
Œ∑ = 1, ¬µ = 0). Figure 2 shows the convergence behavior of
the algorithm for different number of machines m as the total number of examples N (and hence also the data size per
machine) increases. For comparison, we also implemented
distributed ADMM (Boyd et al., 2011), which is a standard method for distributed optimization but does not take
advantage of the statistical similarity between problems at
different machines. The results for DANE clearly indicate a
linear convergence rate, and moreover, that the rate of convergence improves with the data size, as predicted by our
analysis. In contrast, while more data improves the ADMM
accuracy after a fixed number of iterations, the convergence
rate is slower and does not improve with the data size5 .
We now turn to present results for solving a smooth nonquadratic problem, this time using non-synthetic datasets.
Specifically, we solved a regularized
loss minimization
PN
problem of the form minw N1 i=1 `(yi hxi , wi) + Œª2 kwk2 ,
where ` is the smooth hinge loss (as in (Shalev-Shwartz &
Zhang, 2013)) and the training examples {(xi , yi )} are randomly split among different machines. We experimented
on 3 datasets: COV1 and ASTRO-PH (as used in e.g.
5
To be fair, ADMM performs a single distributed averaging
computation per iteration, while DANE performs two. However,
counting iterations is a more realistic measure of performance,
since both methods also perform a full-scale local optimization at
each iteration.

(Shalev-Shwartz & Zhang, 2013; Rakhlin et al., 2012)), as
well as a subset of the MNIST digit recognition dataset
which focuses on discriminating the 4 from the 7 digits6 .
In figure 3, we present the number of iterations required for
DANE to reach accuracy < 10‚àí6 for Œ∑ = 1 and ¬µ = 0, 3Œª,
and for different number of machines. We also report results for ADMM on the same datasets. As in the synthetic case, DANE explicitly takes advantage of the similarity between problems on different machines, and we
indeed observe that it tends to converge in less iterations
than ADMM. Finally, note that for ¬µ = 0 and many machines (i.e. few data points per machine), DANE may not
converge, and increasing ¬µ fixes this at the cost of slowing
down the average convergence rate.
Finally, we examine the convergence on these datasets in
terms of the average loss on the test set. In figure 4, we
present the results for m = 64 machines on the three
datasets, using DANE (with ¬µ = 3Œª) and ADMM. We
also present for comparison the objective value obtained
using one-shot parameter averaging (OSA), using bias correction as proposed in (Zhang et al., 2013). The figure
highlights the practical importance of multi-round communication algorithms: while DANE and ADMM converge
to the value achieved by the regularized loss minimizer,
the single-round OSA algorithm may return a significantly
suboptimal result.
Acknowledgements: Ohad Shamir and Nathan Srebro
are supported by the Intel ICRI-CI Institute. Ohad Shamir
is further supported by an Israel Science Foundation grant
425/13 and an FP7 Marie Curie CIG grant.
6
We used Œª = 10‚àí5 for COV1, Œª = 0.0005 for ASTRO and
Œª = 0.001 for MNIST-47. For MNIST-47, we randomly chose
10,000 examples as the training set, and the rest of the examples
as a test set.

Newton-type Distributed Optimization

References
Agarwal, A., Chapelle, O., Dudƒ±ÃÅk, M., and Langford, J. A
reliable effective terascale linear learning system. CoRR,
abs/1110.4198, 2011.
Beck, A. and Teboulle, M. Mirror descent and nonlinear
projected subgradient methods for convex optimization.
Oper. Res. Lett., 31(3):167‚Äì175, 2003.
Bekkerman, R., Bilenko, M., and Langford, J. Scaling up
machine learning: Parallel and distributed approaches.
Cambridge University Press, 2011.
Boyd, S.P., Parikh, N., Chu, E., Peleato, B., and Eckstein,
J. Distributed optimization and statistical learning via the
alternating direction method of multipliers. Foundations
and Trends in Machine Learning, 3(1):1‚Äì122, 2011.

Recht, B., Re, C., Wright, S., and Niu, F. Hogwild: A
lock-free approach to parallelizing stochastic gradient
descent. In NIPS, 2011.
RichtaÃÅrik, P. and TakaÃÅc, M. Distributed coordinate descent method for learning with big data.
CoRR,
abs/1310.2059, 2013.
Shalev-Shwartz, S. and Zhang, T. Stochastic dual coordinate ascent methods for regularized loss. Journal of
Machine Learning Research, 14(1):567‚Äì599, 2013.
Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan,
K. Stochastic convex optimization. In COLT, 2009.
Sridharan, K., Shalev-Shwartz, S., and Srebro, N. Fast rates
for regularized objectives. In Advances in Neural Information Processing Systems, pp. 1545‚Äì1552, 2008.

Cotter, A., Shamir, O., Srebro, N., and Sridharan, K. Better
mini-batch algorithms via accelerated gradient methods.
In NIPS, 2011.

Tropp, J. User-friendly tail bounds for sums of random
matrices. Foundations of Computational Mathematics,
12(4):389‚Äì434, 2012.

Dekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao,
L. Optimal distributed online prediction using minibatches. Journal of Machine Learning Research, 13:
165‚Äì202, 2012.

Yang, T. Trading computation for communication: Distributed stochastic dual coordinate ascent. In NIPS,
2013.

Deng, W. and Yin, W. On the global and linear convergence of the generalized alternating direction method of
multipliers. Technical report, Rice University Technical
Report TR12-14, 2012.
Duchi, J., Agarwal, A., and Wainwright, M. Dual averaging
for distributed optimization: Convergence analysis and
network scaling. IEEE Trans. Automat. Contr., 57(3):
592‚Äì606, 2012.
Hong, M. and Luo, Z.-Q. On the linear convergence of
the alternating direction method of multipliers. CoRR,
abs/1208.3922, 2012.
Mahajan, D., Keerthy, S., Sundararajan, S., and Bottou, L.
A parallel sgd method with strong convergence. CoRR,
abs/1311.0636, 2013.
Nemirovski, A. and Yudin, D. On cesaro‚Äôs convergence
of the gradient descent method for finding saddle points
of convex-concave functions. Doklady Akademii Nauk
SSSR, 239(4), 1978.
Nemirovsky, A. and Yudin, D. Problem Complexity and
Method Efficiency in Optimization. Wiley-Interscience,
1983.
Rakhlin, A., Shamir, O., and Sridharan, K. Making gradient descent optimal for strongly convex stochastic optimization. In ICML, 2012.

Zhang, Y., Duchi, J., and Wainwright, M. Communicationefficient algorithms for statistical optimization. Journal
of Machine Learning Research, 14:3321‚Äì3363, 2013.
Zinkevich, M., Weimer, M., Smola, A., and Li, L. Parallelized stochastic gradient descent. In NIPS, pp. 2595‚Äì
2603, 2010.

