Safe Subspace Screening for Nuclear Norm Regularized Least Squares
Problems
Qiang Zhou
ZHOUQIANG @ U . NUS . EDU
Department of Electrical and Computer Engineering, National University of Singapore, Singapore 117583
Qi Zhao∗
ELEQIZ @ NUS . EDU . SG
Department of Electrical and Computer Engineering, National University of Singapore, Singapore 117583

Abstract
Nuclear norm regularization has been shown
very promising for pursing a low rank matrix
solution in various machine learning problems.
Many efforts have been devoted to develop
efficient algorithms for solving the optimization
problem in nuclear norm regularization. Solving
it for large-scale matrix variables, however, is
still a challenging task since the complexity
grows fast with the size of matrix variable.
In this work, we propose a novel method
called safe subspace screening (SSS), to improve
the efficiency of the solver for nuclear norm
regularized least squares problems. Motivated
by the fact that the low rank solution can be
represented by a few subspaces, the proposed
method accurately discards a predominant
percentage of inactive subspaces prior to
solving the problem to reduce problem size.
Consequently, a much smaller problem is
required to solve, making it more efficient than
optimizing the original problem. The proposed
SSS is safe, in that its solution is identical
to the solution from the solver. In addition,
the proposed SSS can be used together with
any existing nuclear norm solver since it is
independent of the solver. Extensive results on
several synthetic and real data sets show that
the proposed SSS is very effective in inactive
subspace screening.

1. Introduction
To obtain a low rank matrix solution, many machine
learning problems are formulated as minimizing nuclear
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37.
Copyright 2015 by the author(s). ∗ Corresponding author

norm regularized least squares problem (Yuan et al., 2007;
Argyriou et al., 2008; Kang et al., 2011; Favaro et al.,
2011). In the past several years, a number of efficient
algorithms have been developed to solve the optimization
problem arise from this formulation (Ji & Ye, 2009; Toh
& Yuan, 2010; Jaggi & Sulovský, 2010; Mazumder et al.,
2010; Shalev-Shwartz et al., 2011; Avron et al., 2012;
Mishra et al., 2013; Hsieh & Olsen, 2014). Solving the
problem for large-scale matrix variables, however, is still a
challenging task since the computational complexity grows
fast with the size of the matrix variable. On the other side,
in many real applications, the size of matrix variable is
becoming larger and larger in the big data era.
In the optimization of Lasso (Tibshirani, 1996), Ghaoui
et al. lay the groundwork on safe screening method
to identify features that corresponding to zero coefficient
in the solution and discard them prior to solving the
optimization problem (Ghaoui et al., 2012). Their method
has been further improved by a large body of work on
screening performance (Xiang et al., 2011; Tibshirani et al.,
2012; Wang et al., 2013; Liu et al., 2014) and extended
to discard features for more general `1 norm regularized
sparse problems (Wang et al., 2014b; Wang & Ye, 2014).
In addition, the idea of screening has also been studied
for discarding non-support vectors in the support vector
machine (SVM) (Ogawa et al., 2013; Wang et al., 2014a)
since there are only sparse support vectors used in the
solution of SVM. Previous screening methods can be
considered in two categories, one is safe screening method
like (Ghaoui et al., 2012; Xiang et al., 2011; Wang et al.,
2013; Ogawa et al., 2013), in which the discarded features
are guaranteed to have zero coefficients in the solution,
or vectors guaranteed to be non-support vectors. Another
category is heuristic screening method such as strong rules
(Tibshirani et al., 2012), sure independence screening (SIS)
(Fan & Lv, 2008; Fan & Song, 2010). In these methods,
since features are screened out by several heuristic criteria,
some features corresponding to nonzero coefficients may
be mistakenly discarded.

Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems

In this work, we propose a method called safe subspace
screening (SSS) for discarding subspaces in nuclear norm
regularized least squares problem. Suppose W ∈ Rd×m is
the matrix variable, let us represent W as the sum of rank
one matrices
d X
m
X
W=
Θij ui vjT
(1)
i=1 j=1

where Θ ∈ Rd×m , {ui ∈ Rd }di=1 and {vj ∈ Rm }m
j=1 are
d×d
m×m
orthogonal bases in R
and R
, respectively. It is
easy to verify that any matrix in Rd×m can be represented
in this form as both {ui }di=1 and {vj }m
j=1 are orthogonal
bases. Given ui and vj , we aim to identify inactive
subspaces that {ui vjT |Θij = 0} in the solution prior to
solving the problem. This allows to solve an equivalent
problem on a lower-dimensional subspace corresponding to
Θij that are likely to be nonzero, thus reducing to a smaller
problem and can be more efficiently solved.
Although nuclear norm can be considered as the `1
norm of singular values, a number of key differences
between `1 norm and nuclear norm regularization make our
work a nontrivial extension of previous feature screening
works. Essentially, the feature screening rules for `1
norm regularization mainly make use of the KarushKuhn-Tucker (KKT) condition at the optimal solution.
Specifically, the subgradient of `1 norm at zero and nonzero
points have different ranges: {−1, 1} at nonzero points,
and [−1, 1] at zero points. Therefore, one component
of the solution will be zero if its subgradient belongs
to (−1, 1) and it is a natural approach in such cases to
discard the corresponding feature. Methods along this line,
however, are not applicable for subspace screening because
the subgradient of nuclear norm at both zero and nonzero
Θij are [−1, 1] (Watson, 1992). Therefore, the subgradient
at Θij can not be used to determine whether Θij in the
solution is zero or not. More detailed technical derivation
for this is provided in the Supplementary Materials.
To address this problem, we propose a novel subspace
screening rule by making use of the property of orthogonal
subspaces. Specifically, one subspace will not appear in
the solution and can be safely discarded if the solution
is orthogonal to that subspace. In other words, for each
subspace, we can evaluate the cosine for the angle between
the solution and that subspace, and it can be screened out
if the value is 0 meaning the solution is orthogonal to the
subspace.
To utilize the aforementioned feature screening rule for
identifying inactive features, we need to know the solution,
which however is unknown before solving the problem.
Therefore, previous feature screening methods usually
construct a feasible set for the solution by using some prior
knowledge. One common prior knowledge is that, for `1
norm regularization, there exists a particular regularization

parameter which is the smallest one such that all elements
of the solution to be zero. Although this also holds for
nuclear norm as shown in Sec. 3.2, it is not surprising
that this prior knowledge does not work well for subspace
screening. In fact, the prior knowledge cannot even
identify any inactive subspace. The reason for that is,
unlike the features that are fixed in feature screening,
we need to choose {ui }di=1 and {vj }m
j=1 in subspace
screening, which are quite important for the performance
of subspace screening and can be chosen appropriately by
utilizing the prior knowledge. On the other hand, if the
same strategy as feature screening is adopted, the prior
knowledge in this case is a zero matrix solution at that
particular regularization parameter. Then, {ui }di=1 and
{vj }m
j=1 can be only chosen as standard basis, which leads
to Θ = W. As we know, it is possible that a low rank
W with all its elements being nonzero, then all elements
of Θ are also nonzero. In the proposed method, to provide
more informative {ui }di=1 and {vj }m
j=1 , we seek to utilize
the solution at a very small regularization parameter, which
can be easily obtained by exploiting a smart initialization
strategy and it can provide a more appropriate choice for
{ui }di=1 and {vj }m
j=1 as it has many singular vectors with
nonzero singular values.
As the name indicates, the proposed method is safe in
the sense that the discarded subspaces definitely do not
appear in the solution. In addition, it can be used in
conjunction with any existing nuclear norm solver as it is
independent of the solver. To the best of our knowledge, the
proposed method is the first work to identify and discard
the subspaces that will not appear in the solution prior to
solving the problem.
Notations: Throughout the paper, vectors and matrices
will be denoted by lower and upper case boldface
characters (e.g. a and A), respectively. We use the notation
Aij to refer to the (i, j)th entry of A. Moreover, the ith
row and jth column of A are denoted by Ai· and A·j . Let
k · k2 denote the Euclidean norm for a vector. For matrix
norm, the Frobenius norm is denoted by k · kF . In addition,
k · k∗ and k · k2 denote the nuclear norm and spectral norm,
respectively. The trace of a matrix is denoted by Tr [·]. Let
A−1 denote the inverse of A if A is invertible, otherwise it
denotes the Moore-Penrose generalized inverse of A. 0
is used to denote a zero vector or matrix and its size is
determined by the context. Let I denote an identity matrix
with approximate size.

2. Motivation of Safe Subspace Screening
Specifically, we consider the following nuclear norm
regularized least squares problem (Toh & Yuan, 2010)
min

W∈Rd×m

1
2
kXW − YkF + λ kWk∗
2

(2)

Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems

where X ∈ Rn×d is the input data and Y ∈ Rn×m is the
corresponding output, W ∈ Rd×m is the matrix variable,
and λ is a regularization parameter. Many machine
learning problems can be formulated as this form, e.g.
multivariate learning regression (Lu et al., 2012), multitask learning (Argyriou et al., 2008; Kang et al., 2011),
subspace clustering (Favaro et al., 2011). Suppose we are
given {ui }di=1 and {vj }m
j=1 ,substituting W in Eq. (1) into
Eq. (2), we obtain the following equivalent problem

2


d X
m
X


1
T

X
Θ
u
v
−
Y
min
ij i j


Θ∈Rd×m 2 

i=1 j=1
F


 d m

X X

(3)
+ λ
Θij ui vjT 


 i=1 j=1


b m
d×
b
b =Θ b
b and V
b are
where Θ
. Since both U
1:d,1:m
b ∈ R
bΘ
bV
b T k∗ = kΘk
b ∗ . Then
orthogonal bases, it implies kU
the problem in Eq. (7) can be rewritten as
2
1
 b b bT

b ∗
(8)
UΘV − Y + λkΘk
min
X
b m 2
d×c
b
F
Θ∈R

In the following, we use Wλ∗ and Θ∗λ to denote the
solutions to Eq. (2) and Eq. (3) when the value of
regularization parameter is λ, respectively. It is easy to
verify that {ui vjT }d,m
i=1,j=1 is orthogonal to each other.
Therefore, for a particular subspace ui vjT , the value of
(Θ∗λ )ij will be 0 if and only if
 h

i 

T
(4)
Tr (Wλ∗ ) ui vjT  = uTi Wλ∗ vj  = 0

since uTi Wλ∗ vj / kWλ∗ kF is the cosine of the angle
between Wλ∗ and ui vjT . In other words, ui vjT can be
safely discarded in the representation of Wλ∗ and (Θ∗λ )ij
can be safely set as 0 even prior to optimizing Eq. (3). We
only need to focus on Θ∗ij such that

3.1. Overview of the Proposed Method

In Eq. (8), we only need to solve the optimization problem
with a db × m
b matrix variable instead of d × m as in
Eq. (2), leading to potentially substantial improvement in
efficiency.

3. The Proposed Safe Subspace Screening
In this section, we present the details of the proposed safe
subspace screening rule for the problem in Eq. (3).

∗

uTi Wλ∗ vj 6= 0

(5)

b = [· · · , ui , · · · ] and V
b = [· · · , vj , · · · ] be all the ui
Let U
b ⊥ and V
b⊥
and vj that satisfy Eq. (5), respectively. Let U
b and
denote the set of ui and vj that do not appear in U
b
V, respectively. Based on these definitions, we can form a
e = [U,
b U
b ⊥ ] ∈ Rd×d and row basis V
e =
column basis U
⊥
m×m
b
b
[V, V ] ∈ R
. Then, W can be re-parameterized as
e V
e T . By using this representation, Eq. (3) can be
W = UΘ
rewritten as
2


1
 e eT

e eT
min
UΘV − Y + λ UΘ
V 
(6)
X
F
∗
Θ∈Rd×m 2
b and V
b have db and m
Suppose U
b columns, respectively.
According to previous discussions, we only need to solve
the db × m
b leading upper-left corner submatrix of Θ since
all other Θij corresponding to the subspaces can be safely
discarded and their values are zero in the solution.
After applying safe subspace screening, the problem
Eq. (6) reduces to the following equivalent problem
2


1
 b b bT

b b bT
UΘV − Y + λ U
ΘV 
(7)
min
X
b m 2
d×c
b
F
∗
Θ∈R

To utilize the rule developed in Eq. (4) to identify inactive
subspaces, we need the solution Wλ∗ , which is unknown
prior to solving the Eq. (2). Therefore, we seek to
∗
construct a feasible set
 for Wλ and estimate the upper
bound for uTi Wλ∗ vj . In particular, the technique used
to construct the feasible set is the so called variational
inequality, which is a necessary condition for the optimal
solution of a constrained optimization problem (Güler,
2010). Therefore, in Sec. 3.2, we first introduce the dual
problem of Eq. (2) to obtain a constrained optimization
problem. By using the relationship between primal and
dual optimal solutions, the upper bound problem can be
reformulated as a function of the dual optimal solution.
Then, in Sec. 3.3, a feasible set is constructed for the dual
optimal solution. For each pair of ui and vj , Sec. 3.4
discusses how to estimate the upper bound over the feasible
set. In fact, as we shall see, the upper bound problem
has a closed form solution due to special structure of the
objective function and constraints. The proposed safe
subspace screening rule for Eq. (3) based on Eq. (4) is
presented in Sec. 3.5. Due to space limitation, all technical
derivations and proofs are provided in the Supplementary
Materials.
3.2. The Dual Problem
The dual problem of Eq. (2) can be written as

2


1
Y

 s.t. XT P ≤ 1
min
P
−


2
λ F
P∈Rn×m 2

(9)

where P ∈ Rn×m is the dual variable. Similarly, let
P∗λ denote the solution to Eq. (9) when the value of
regularization parameter is λ. By using the KKT condition,
we can establish the following relationship for the primal
solution Wλ∗ and the dual solution P∗λ
λP∗λ = Y − XWλ∗

(10)

Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems



According to this relationship, uTi Wλ∗ vj  can be
reformulated as
 
−1
 
 T
(11)
XT X
XT Y − λXT P∗λ vj 
ui
In addition, it is easy to verify that there exists a specific
parameter value λmax such that the primal optimal solution
Wλ∗ is 0 for any λ ≥ λmax . According to Eq. (9)
and Eq.(10), the
 λmax can be analytically computed and
λmax = XT Y2 which is the largest singular value (a.k.a.
spectral norm) of XT Y.
3.3. The Feasible Set of Dual Optimal Solution
In the following, we will make use of the variational
inequality as in Lemma 1 to construct a feasible set for the
dual optimal solution P∗λ .
Lemma 1. (Güler, 2010) Let G ∈ Rd×m be a convex set
and let f be a Gâteaux differentiable function on an open
set containing G. If Z∗ is a local minimizer of f on G, then
Tr[∇f (Z∗ )T (Z − Z∗ )] ≥ 0, ∀Z ∈ G

(12)

As we can see, to construct a feasible set for Z∗ by Eq. (12),
we need to find a known Z from G. Therefore, to construct
the feasible set for P∗λ with λ ∈ (0, λmax ), we assume that
there exists another parameter λ0 with λ0 ∈ (0, λ) and
its dual solution P∗λ0 is known. To make this assumption
reasonable, we need to find an appropriate λ0 such that
its solution can be obtained trivially. Indeed, when λ0 is
close to zero, the solution Wλ∗ 0 can be easily obtained
−1 T
by using W = XT X
X Y as initialization, which
is the solution at λ = 0. In addition, in many scenarios,
the solution at λ0 can be freely obtained. For instance, an
appropriate value of λ for Eq. (2) needs to be determined
since the optimal value of λ is generally unknown in real
applications. Therefore, we usually need to solve Eq. (2)
over a grid of regularization parameters λ1 < λ2 < · · · <
λk and choose the optimal λ under certain criterion. After
obtaining the solution Wλ∗ t−1 at λt−1 , it can be freely used
to screen out inactive subspaces for Eq. (2) at λt .
Now, we describe how to construct a feasible set for
the dual optimal solution P∗λ by using the variational
inequality. Since P∗λ0 and P∗λ are the solutions to Eq. (9) at
λ0 and λ, respectively, we can apply Lemma 1 to Eq. (9)
and obtain
"
#
T

Y
∗
∗
Tr Pλ0 −
P − Pλ0 ≥ 0
(13)
λ0
"
#
T
Y
∗
∗
Tr Pλ −
(P − Pλ ) ≥ 0
(14)
λ


which holds for ∀P : XT P2 ≤ 1. Since P = P∗λ
and P = P∗λ0 are also feasible for Eq. (13) and Eq. (14),

respectively, substituting them into Eq. (13) and Eq. (14)
leads to
"
#
T

Y
Tr P∗λ0 −
P∗λ − P∗λ0 ≥ 0
(15)
λ0
#
"
T

Y
∗
∗
∗
(16)
Pλ0 − Pλ ≥ 0
Tr Pλ −
λ
From inequalities in Eq. (15) and Eq. (16), we obtain the
feasible set for P∗λ
(
"
#
T

Y
∗
∗
∗
F(Pλ ) = P : Tr Pλ0 −
P − Pλ0 ≥ 0,
λ0
#
)
"
T

Y
∗
Pλ0 − P ≥ 0
(17)
Tr P −
λ
3.4. Estimating the Upper Bound
Given the feasible set F(P∗λ ), we seek to estimate the
upper bound of Eq. (11) over the feasible set for each pair
of ui and vj . Formally, we need to solve the following
optimization problem
 
−1
 
 T
T
T
T
max
X
X
X
Y
−
λX
P
vj  (18)
u
i
n×m
P∈R

s.t. P ∈ F(P∗λ )
As mentioned before, the performance of subspace
screening also relies on the choice of ui and vj . In the
proposed method, ui and vj are chosen as the singular
vectors of Wλ0 . Specifically, suppose the singular value
decomposition (SVD) of Wλ∗ 0 is
Wλ∗ 0 = UΣVT

(19)

Then, we let ui = U·i and vj = V·j .
For reformulating the upper bound estimation problem in
Eq. (18), we first introduce three variables
XWλ∗ 0
Y
− P∗λ0 =
λ0
λ0


Y
Y
Y
∗
B=
− Pλ0 = A +
−
λ
λ
λ0


Y
R = 2P − P∗λ0 +
λ
A=

(20)
(21)
(22)

where A can be considered as the scaled prediction based
on Wλ∗ 0 by λ0 , and B is obtained by translating A with
the difference between the scaled Y by λ0 and λ. The
following lemma shows that both A and B are nonzero
matrices.
Lemma
 T  2. For any λ0 and λ such that 0 < λ0 < λ <
X Y , and Y 6= 0, we have both A 6= 0 and B 6= 0.
2

Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems

Next, we reformulate the upper bound problem in Eq. (18)
Since we have obtained
the optimal value of Eq. (26), the

upper bound of uTi Wλ∗ vj  is also ready to obtain. Here,
by using the variables defined in Eq. (20), Eq. (21) and
we use Φ ∈ Rd×m and Ψ ∈ Rd×m to represent the
Eq. (22) and obtain the following equivalent problem
 upper bounds for all subspaces. Specifically, Φij and Ψij
−1 T
−1 T
λ  T
 denote the upper bounds of −uT W∗ v and uT W∗ v ,
T
T
T
X
X
X
Bv
−
u
X
X
X
Rv
max
u

j
j
i
i
λ j
λ j
i
i
R∈Rn×m 2
respectively. The values of Φ and Ψ are summarized in
 T

2
2
s.t. Tr A (R + B) ≤ 0, kRkF ≤ kBkF
(23)
the following corollary.
Let us define SC ∈ Rd×m and SR ∈ Rd×m such that
−1 T
−1 T
USC VT = XT X
X B, USR VT = XT X
X R
(24)
Then, the objective function in Eq. (23) can be further
reformulated as

 λ 
λ  T

ui USC VT vj − uTi USR VT vj  = (SC )ij − (SR )ij 
2
2


λ
= max (SC )ij − (SR )ij , − (SC )ij + (SR )ij
(25)
2
which means we can solve the optimization problem by
maximizing − (SR )ij and (SR )ij over the constraint set.
They are further equivalent to minimizing (SR )ij and
− (SR )ij over the constraint set, which can be unified as
the following problem


2
2
min e (SR )ij s.t. Tr AT (R + B) ≤ 0, kRkF ≤ kBkF
(26)
where e = ±1. For convenience, we introduce a new
−1
matrix variable D defined as D = X XT X
U.
According to Eq. (24), (SR )ij can be represented as
T

(SR )ij = (D·i ) RV·j .
Eq. (26) should admit a closed form solution since the
objective function is linear and the constrain set is the
intersection of a linear and quadratic function (Bertsimas
& Tsitsiklis, 1997). The following theorem provides the
optimal solution for Eq. (26).
Theorem

 1. For any λ0 and λ such that 0 < λ0 < λ <
XT Y , and both X and Y are not equal to 0. The
2
optimal solution to Eq. (26) is
(SR )ij = −e kD·i k2 kBkF
if the following holds


b ij
λ0 Tr AT B kD·i k2 ≤ e kBkF Σ

(27)

(28)

otherwise
(SR )ij =



b ij
−eGij − Tr AT B Σ
2

λ0 kAkF

(29)

Corollary
 T  1. For any λ0 and λ such that 0 < λ0 < λ <
X Y , and Y 6= 0. We have
2




0.5λ kBkF kD·i k2 − (SC )ij







b ij ≤ −λ0 Tr AT B kD·i k

if kBkF Σ
2


Φij =
b ij
Gij −Tr[AT B]Σ

− (SC )ij
 0.5λ

λ0 kAk2F




otherwise


0.5λ kBkF kD·i k2 + SC ij







b ij ≥ λ0 Tr AT B kD·i k

if kBkF Σ
2


b ij
Ψij =
Gij +Tr[AT B]Σ
 0.5λ
+ SC ij

λ0 kAk2F




otherwise
3.5. Safe Subspace Screening Rule
In view of Eq. (4), we are now ready to construct the safe
subspace screening rule for Eq. (3). Let us introduce a
new matrix Ω ∈ R d×m with its (i, j)th entry denoting
the upper bound of uTi Wλ∗ vj  meaning the value of Ωij
is max(Φij , Ψij ). If Ωij = 0, it implies that both
−uTi Wλ∗ vj and uTi Wλ∗ vj are equal to zero, then the
value of (Θ∗λ )ij must be zero and the subspace ui vjT
can be discarded prior to solving Eq. (3). Formally, the
proposed subspace screening method can be summarized
in the following theorem.
Theorem 2. For nuclear norm regularized least squares
problem, suppose the solution Wλ∗ 0 is known and the SVD
b = U, V
b = V,
of Wλ∗ 0 as represented in Eq. (19). Let U
ui = U·i and vj = V·j . For any λ > λ0
1. If λ ≥ λmax , then Wλ∗ = 0.
2. If λ < λmax , for 1 ≤ i ≤ d, if kΩi· k∞ = 0, then
b ·i can be removed from U.
b Similarly,
(Θ∗λ )i· = 0 and U
∗
b ·j
for 1 ≤ j ≤ m, if kΩ·j k∞ = 0, then (Θλ )·j = 0 and V
b Then, solving Eq. (8) will get the
can be removed from V.
identical result as optimizing Eq. (3).

4. Experiments

In this section, we perform experiments on several

b = UT XT X −1 XT XΣ and Gij is defined as
where Σ
synthetic and real data sets to evaluate the performance of
r

 the proposed SSS. Since there is no existing method on
2
2
2
2
2
2
safe subspace screening prior to solving the problem, we
2
T
b
kAkF kBkF − (Tr [A B])
λ0 kAkF kD·i k2 − Σ
ij
evaluate the proposed SSS by comparing the performance

Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems

(a) Synthetic Set 1

(b) Synthetic Set 2

(c) Synthetic Set 3

Figure 1. The subspace rejection ratio of the proposed SSS on three synthetic data sets.

of the nuclear norm solver with SSS and without SSS. For
the nuclear norm solver, we use the popular accelerated
proximal gradient (APG) algorithm (Toh & Yuan, 2010; Ji
& Ye, 2009). On each data set, we run the solver without
and with SSS to optimize Eq. (2) along a sequence of
100 values of λ equally spaced on the logarithmic scale of
λ/λmax from 0.001 to 0.95. To reduce statistical variability,
all reported results are averaged over 10 trials. All
experiments are performed on a workstation with Intel(R)
Core(TM) i7-4930K 3.40 GHz CPU and 64G RAM
Suppose the 100 values of λ are indexed by λt , 1 ≤ t ≤
100 in ascending order of value. In our experiments, the
warm-start strategy is used for the solver. Specifically,
for solving the optimization problem at λt with t ≥
2, the solution Wλ∗ t−1 at λt−1 will be used as the
initialization. To solve the problem for the smallest
regularization parameter λ1 , we use the solution at λ = 0
−1 T
that is XT X
X Y as initialization. In order to apply
the proposed SSS for λ1 , we first solve the problem for a
very small regularization parameter λ0 = (1e − 6)λmax by
−1 T
using XT X
X Y as initialization.
Since the proposed SSS is safe, the solution obtained
by the solver with SSS is the same as the solution
directly from the solver. In other words, their predictive
performances are the same to each other. To quantify the
performance of the proposed method, similarly to (Wang
et al., 2013), two measures are used in our experiments:
(a) subspace rejection ratio: the ratio of the number of
subspaces discarded by the proposed SSS to the total
number of subspaces that can be safely discarded in the
ground truth. More precisely, suppose the rank of ground
truth is r, by using the notation in Sec. 2, we have
b m
d×
b
subspace rejection ratio = d×m−
d×m−r 2 . (b) speedup: this
value is the ratio of the computational time of the solver
without the proposed SSS to the computational time of the
solver with the proposed SSS.
4.1. Synthetic Data Sets
In this subsection, we evaluate the proposed method in the
problem of multivariate linear regression on three synthetic

Table 1. Computational time (in minutes) for solving nuclear
norm regularized least squares problem along a sequence of 100
parameter values of λ equally spaced on the logarithmic scale of
λ/λmax from 0.001 to 0.95 on the three synthetic data sets by (a)
“Solver” (solver without subspace screening); (b) “Solver with
SSS” (solver in conjunction with the proposed SSS). “Prep.” is
the running time for solving the problem at λ0 . “SSS” is the
total computational time used to perform the proposed subspace
screening.

Data Set
Solver
Solver with SSS
Speedup (times)

Prep.
SSS
Total

Set 1
659.12
2.27
13.39
28.81
22.88

Set 2
212.79
0.60
4.37
19.73
10.78

Set 3
182.38
0.64
8.63
12.86
14.18

data sets. Suppose the input X ∈ Rn×d is n samples
with d-dimensional features for each and the output Y ∈
Rn×m is m responses for all samples, then it can be
formulated as Y = XW∗ + E where W∗ ∈ Rd×m is the
model coefficient matrix and E ∈ Rn×m is the regression
noise. To generate the synthetic data sets, we use a similar
procedure as reported in (Jacob et al., 2008). Specifically,
the ith observation is generated from a multivariate normal
distribution Xi· ∼ N (0, I) and the output of the jth
response is obtained by Yij = Xi· W·j + N (0, 16). 200
samples are generated for each data set.
In data set 1, all m = 5000 models are assumed from
100 clusters each consisting of 50 models. All d =
5000 dimensions are randomly divided into 100 disjoint
groups and each group is assigned to only one cluster.
The coefficients for each model from a particular cluster
are nonzero only for corresponding dimensions, and are
zero for all other dimensions. For each cluster, a specific
model coefficient is the cluster mean plus a model specific
component N (0, 4I). Data set 2 and data set 3 are the same
as data set 1 except we change d = 2500 and m = 2500
for data set 2 and data set 3, respectively.
Fig. 1 shows the subspace rejection ratio of the proposed
SSS on the three synthetic data sets. As observed, the
proposed method consistently discards more than 90%

Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems

(a) PIE

(b) MNIST

(c) Yahoo Stock

Figure 2. The subspace rejection ratio of the proposed SSS on three real data sets.

inactive subspaces on all three data sets. Table 1 reports
the computational time of the solver without or with the
proposed SSS for solving the 100 nuclear norm regularized
least squares problems, as well as the computational time
used to perform the proposed SSS. Since most inactive
subspaces have been screened out prior to solving the
problem, the proposed SSS significantly improves the
efficiency of the solver. The lowest speedup achieved
by the proposed SSS on the three data sets is still up to
10.78. Moreover, as shown in the table, more significant
improvement can be achieved for larger problem size.
Especially, on the synthetic data set 1, the size of matrix
variable is 5000 × 5000 and the solver spends 659.12
minutes to solve the 100 problems. In contrast, by
enhancing the solver with the proposed SSS, only 28.81
minutes is used for the 100 problems, which leads to
substantial saving in the computational time. The proposed
SSS is not only effective in identifying inactive subspaces
as shown in Fig. 1, but also efficient. As observed in
Table 1, on the three data sets, the computational times of
performing the proposed SSS are only 2.03%, 2.05% and
4.73% that of the solver without subspace screening. In
addition, compared with the computational time of solver
without subspace screening, the preparation procedure
of the proposed SSS is also very efficient since it only
occupies 0.34%, 0.28% and 0.35% on the three data sets,
respectively.
4.2. Real Data Sets
In this subsection, we perform experiments on three real
data sets to evaluate the performance of the proposed SSS.
The details of the three data sets as follows.
PIE Face Image Data Set This data set used in this
experiment consist of 11554 gray face images from
68 people, which were captured under various poses,
illumination conditions and expressions (Sim et al., 2003;
Cai et al., 2007). The size of each image is 32 × 32 pixels.
We consider the subspace clustering task on it. Specifically,
in each trial, we first randomly pick 70 images from each
people and put them together as the dictionary X. Then,
another 70 images are picked from each people used as

the target clustering subspace Y. The feature dimension
is reduced to 80 by performing PCA on the vectorized
raw features. Then, then we get the dictionary X ∈
R80×4760 and targeted clustering subspace Y ∈ R80×4760 .
Therefore, we have W ∈ R4760×4760 .
MNIST Handwritten Digit Data Set This data set
consists of 70, 000 grey images of scanned handwritten
digits (LeCun et al., 1998). The sample sizes of training
and testing are 60, 000 and 10, 000 respectively. We still
consider a subspace clustering task. Specifically, in each
trial, we randomly pick 600 images from training and
testing for each digit to form the dictionary X and the target
clustering subspace Y, respectively. The feature dimension
is reduced to 100 by performing PCA on the vectorized raw
features. Finally, we obtain a dictionary X ∈ R100×6000
and a target clustering subspace Y ∈ R100×6000 . Then, the
problem is to learn W ∈ R6000×6000 .
Yahoo Stock Data In this data set, we consider the
application of multivariate linear regression on the financial
econometrics. Specifically, we aim to predict the future
return of stock via multivariate linear regression by using
the daily closing price. Let yt−1 ∈ Rd and yt ∈ Rd denote
the stock prices at day (t − 1) and t, respectively. Then,
T
W, where
the problem can be formulated as ytT = yt−1
d×d
W∈R
and we have d = m in this case. To perform
the experiment, in each trial, we download the daily closing
prices for m = 4676 stocks during 101 days in 2013
form Yahoo Fiance. Then X and Y are formed as X =
T
T
[y1 · · · y100 ] ∈ R100×4676 and Y = [y2 · · · y101 ] ∈
100×4676
4676×4676
R
, which implies W ∈ R
.
The subspace rejection ratios of the proposed SSS on the
three real data sets are shown in Fig. 2. As observed,
the proposed method is very effective on screening out
inactive subspaces on real data sets in the sense that it
successfully identifies more than 97% inactive subspaces
on all three real data sets. As can be seen in Table 2,
compared with results on synthetic data sets, the proposed
SSS achieves better performance on real data sets in terms
of speedup. Specifically, even the lowest speedup is up
to 53.57 on the MNIST data set while it achieves around
80 speedup on both other two data sets. In addition, the

Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems
Table 2. Computational time (in minutes) for solving nuclear
norm regularized least squares problem along a sequence of 100
parameter values of λ equally spaced on the logarithmic scale
of λ/λmax from 0.001 to 0.95 on the three real data sets by (a)
“Solver” (solver without subspace screening); (b) “Solver with
SSS” (solver in conjunction with the proposed SSS). “Prep.” is
the running time for solving the problem at λ0 . “SSS” is the
total computational time used to perform the proposed subspace
screening.

Data Set
Solver

PIE
MNIST Yahoo Stock
2395.54 2968.87
3075.09
Prep. 1.88
3.69
2.24
Solver with SSS SSS 11.21
22.04
10.93
Total 31.22
55.42
37.26
Speedup (times)
76.72
53.57
82.53
computational time of performing the proposed SSS and
running the preparation procedure are also much less than
that of synthetic data sets. In particular, the percentage
of computational time of the preparation procedure over
that of the solver without subspace screening is 0.08%,
0.12% and 0.07% on three real data sets, respectively. Thus
the time for preparation is quite negligible. Moreover,
the largest value of percentage of performing the proposed
SSS is 0.74% which shows that the proposed SSS is very
efficient. One reason for the better performance of the
proposed SSS on real data sets is that they are generally
more complicated thus requiring more time for the solver
to convergence. On the other hand, the proposed SSS
only goes through the data once, whose computational time
depends solely on the size of the matrix variable.
4.3. Comparison on Forward and Backward Solution
Paths for the Solver
As mentioned at the beginning of this section, we can
make use of the warm-start strategy to efficiently obtain the
solutions for a sequence of value of λ. In our experiment,
for a given λt , 1 ≤ t ≤ 100 in ascending ordering of value,
we obtain the solution path by solving the problem from
λ1 to λ100 . We call this method as a forward solution
path for solver. In contrast, there is an alternative method
called backward solution path method, in which we solve
the problem from λ100 to λ1 . In this method, we can
only use 0 that is the solution of λmax as initialization for
λ100 . Intuitively, there is no clear theoretical proof as of
which one is more efficient since the result should depend
on the choice of λt . Here, we experimentally compare
the performances of forward and backward solution paths.
Specifically, we run the solver on the three synthetic data
sets by using both the forward and backward methods
and compare their computational time. The results are
reported in Table 3 and they are averaged over 10 trials. As
observed, the computational time of two paths on synthetic
set 1 and set 3 are almost the same to each other, and the

Table 3. Computational time (in minutes) of forward and
backward solution path for the solver on three synthetic data sets.

Data Set
Forward
Backward

Set 1
659.12
660.16

Set 2
212.79
230.12

Set 3
182.38
185.22

Table 4. Computational time (in minutes) for solving nuclear
norm regularized least squares problem along a sequence of
100 parameter values of λ equally spaced on the logarithmic
scale of λ/λmax from 0.001 to 0.95 on the three synthetic data
sets by (a) “ADMM” (ADMM without subspace screening); (b)
“ADMM with SSS” (ADMM in conjunction with the proposed
SSS). “Prep.” is the running time for solving the problem at
λ0 . “SSS” is the total computational time used to perform the
proposed subspace screening.

Data Set
ADMM
ADMM with SSS
Speedup (times)

Prep.
SSS
Total

Set 1
590.63
1.97
13.40
26.05
22.67

Set 2
221.12
0.60
4.43
16.93
13.06

Set 3
227.68
0.63
9.74
14.58
15.61

forward path is a little faster than the backward path.
4.4. Results of Safe Subspace Screening for ADMM
Further as we mentioned before, the proposed SSS can
be used in conjunction with any nuclear norm solver.
In this subsection, we evaluate the performance of the
proposed SSS for another popular nuclear norm solver, i.e.
the alternating direction method of multipliers (ADMM)
(Boyd et al., 2011). Specifically, we perform experiments
on the three synthetic data sets with the same setting as
previous experiments except using ADMM as the solver
here. The results are shown in Table 4. Compared with
Table 1, the proposed SSS has shown similar improvements
for ADMM as APG. This shows that the proposed SSS
can extensively used to improve the efficiency of existing
nuclear norm solvers.

5. Conclusions
In this work, we present a safe subspace screening method
to improve the efficiency of the solver for nuclear norm
regularized least squares problems. Essentially, the idea
of subspace screening is to identify the subspaces that are
orthogonal to the solution. The proposed method is able to
effectively and efficiently discard inactive subspaces prior
to solving the problem, thus greatly reducing the size of the
optimization problem. Moreover, the proposed method can
be used in conjunction with any nuclear norm solver since
the it is independent of solver. Extensive experiments on
three synthetic and three real data sets have shown that the
proposed method significantly improves the efficiency of
existing solvers.

Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems

Acknowledgments
The research was supported by the Defense Innovative
Research Programme (No. 9014100596) and the Young
Investigator Award (No. R-263-000-A29-133).

Jaggi, Martin and Sulovský, Marek. A simple algorithm
for nuclear norm regularized problems. In Proc. ICML,
2010.

References

Ji, Shuiwang and Ye, Jieping. An accelerated gradient
method for trace norm minimization. In Proc. ICML,
2009.

Argyriou, Andreas, Evgeniou, Theodoros, and Pontil,
Massimiliano.
Convex multi-task feature learning.
Machine Learning, 73(3):243–272, 2008.

Kang, Zhuoliang, Grauman, Kristen, and Sha, Fei.
Learning with whom to share in multi-task feature
learning. In Proc. ICML, 2011.

Avron,
Haim,
Kale,
Satyen,
Kasiviswanathan,
Shiva Prasad, and Sindhwani, Vikas. Efficient and
practical stochastic subgradient descent for nuclear
norm regularization. In Proc. ICML, 2012.

LeCun, Yann, Bottou, Léon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.

Bertsimas, Dimitris and Tsitsiklis, John. Introduction to
Linear Optimization. Athena Scientific Belmont, MA,
1997.

Liu, Jun, Zhao, Zheng, Wang, Jie, and Ye, Jieping. Safe
screening with variational inequalities and its application
to lasso. In Proc. ICML, 2014.

Boyd, Stephen P., Parikh, Neal, Chu, Eric, Peleato, Borja,
and Eckstein, Jonathan. Distributed optimization and
statistical learning via the alternating direction method
of multipliers. Foundations and Trends in Machine
Learning, 3(1):1–122, 2011.

Lu, Zhaosong, Monteiro, Renato, and Yuan, Ming. Convex
optimization methods for dimension reduction and
coefficient estimation in multivariate linear regression.
Mathematical Programming, 131(1-2):163–194, 2012.

Cai, Deng, He, Xiaofei, and Han, Jiawei. Efficient kernel
discriminant analysis via spectral regression. In Proc.
ICDM, 2007.

Mazumder, Rahul, Hastie, Trevor, and Tibshirani, Robert.
Spectral regularization algorithms for learning large
incomplete matrices. Journal of Machine Learning
Research, 11:2287–2322, 2010.

Fan, Jianqing and Lv, Jinchi. Sure independence screening
for ultrahigh dimensional feature space. Journal of
the Royal Statistical Society: Series B, 70(5):849–911,
2008.

Mishra, Bamdev, Meyer, Gilles, Bach, Francis, and
Sepulchre, Rodolphe. Low-rank optimization with trace
norm penalty. SIAM Journal on Optimization, 23(4):
2124–2149, 2013.

Fan, Jianqing and Song, Rui. Sure independence screening
in generalized linear models with np-dimensionality. The
Annals of Statistics, 38(6):3567–3604, 2010.

Ogawa, Kohei, Suzuki, Yoshiki, and Takeuchi, Ichiro.
Safe screening of non-support vectors in pathwise svm
computation. In Proc. ICML, 2013.

Favaro, Paolo, Vidal, René, and Ravichandran, Avinash. A
closed form solution to robust subspace estimation and
clustering. In Proc. CVPR, 2011.

Shalev-Shwartz, Shai, Gonen, Alon, and Shamir, Ohad.
Large-scale convex minimization with a low-rank
constraint. In Proc. ICML, 2011.

Ghaoui, Laurent El, Viallon, Vivian, and Rabbani, Tarek.
Safe feature elimination in sparse supervised learning.
Pacific Journal of Optimization, 8(4):667–698, 2012.

Sim, Terence, Baker, Simon, and Bsat, Maan. The
CMU pose, illumination, and expression database.
IEEE Transactions on Pattern Analysis and Machine
Intelligence,, 25(12):1615–1618, 2003.

Güler, Osman. Foundations of Optimization. Springer,
2010.
Hsieh, Cho-Jui and Olsen, Peder A. Nuclear norm
minimization via active subspace selection. In Proc.
ICML, 2014.
Jacob, Laurent, Bach, Francis, and Vert, Jean-Philippe.
Clustered multi-task learning: A convex formulation. In
Proc. NIPS, 2008.

Tibshirani, Robert. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society. Series
B (Methodological), 58(1):267–288, 1996.
Tibshirani, Robert, Bien, Jacob, Friedman, Jerome, Hastie,
Trevor, Simon, Noah, Taylor, Jonathan, and Tibshirani,
Ryan J. Strong rules for discarding predictors in lassotype problems. Journal of the Royal Statistical Society:
Series B, 74(2):245–266, 2012.

Safe Subspace Screening for Nuclear Norm Regularized Least Squares Problems

Toh, Kim-Chuan and Yuan, Sangwoon. An accelerated
proximal gradient algorithm for nuclear norm
regularized linear least squares problems.
Pacific
Journal of Optimization, 6(15):615–640, 2010.
Wang, Jie and Ye, Jieping. Two-layer feature reduction for
sparse-group lasso via decomposition of convex sets. In
Proc. NIPS, 2014.
Wang, Jie, Zhou, Jiayu, Wonka, Peter, and Ye, Jieping.
Lasso screening rules via dual polytope projection. In
Proc. NIPS, 2013.
Wang, Jie, Wonka, Peter, and Ye, Jieping. Scaling svm
and least absolute deviations via exact data reduction. In
Proc. ICML, 2014a.
Wang, Jie, Zhou, Jiayu, Liu, Jun, Wonka, Peter, and
Ye, Jieping. A safe screening rule for sparse logistic
regression. In Proc. NIPS, 2014b.
Watson, G. Alistair. Characterization of the subdifferential
of some matrix norms.
Linear Algebra and Its
Applications, 170:33–45, 1992.
Xiang, Zhen James, Xu, Hao, and Ramadge, Peter J.
Learning sparse representations of high dimensional data
on large scale dictionaries. In Proc. NIPS, 2011.
Yuan, Ming, Ekici, Ali, Lu, Zhaosong, and Monteiro,
Renato. Dimension reduction and coefficient estimation
in multivariate linear regression. Journal of the Royal
Statistical Society: Series B, 69(3):329–346, 2007.

