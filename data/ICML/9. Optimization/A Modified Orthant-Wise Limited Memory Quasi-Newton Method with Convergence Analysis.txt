A Modified Orthant-Wise Limited Memory Quasi-Newton Method with
Convergence Analysis

Pinghua Gong
University of Michigan, Ann Arbor, MI 48109

GONGP @ UMICH . EDU

Jieping Ye
University of Michigan, Ann Arbor, MI 48109

JPYE @ UMICH . EDU

Abstract
The Orthant-Wise Limited memory QuasiNewton (OWL-QN) method has been demonstrated to be very effective in solving the ℓ1 regularized sparse learning problem. OWL-QN
extends the L-BFGS from solving unconstrained
smooth optimization problems to ℓ1 -regularized
(non-smooth) sparse learning problems. At each
iteration, OWL-QN does not involve any ℓ1 regularized quadratic optimization subproblem
and only requires matrix-vector multiplications without an explicit use of the (inverse) Hessian matrix, which enables OWL-QN to tackle large-scale problems efficiently. Although
many empirical studies have demonstrated that
OWL-QN works quite well in practice, several
recent papers point out that the existing convergence proof of OWL-QN is flawed and a rigorous convergence analysis for OWL-QN still remains to be established. In this paper, we propose a modified Orthant-Wise Limited memory Quasi-Newton (mOWL-QN) algorithm by slightly modifying the OWL-QN algorithm. As
the main technical contribution of this paper,
we establish a rigorous convergence proof for
the mOWL-QN algorithm. To the best of our
knowledge, our work fills the theoretical gap by
providing the first rigorous convergence proof
for the OWL-QN-type algorithm on solving ℓ1 regularized sparse learning problems. We also
provide empirical studies to show that mOWLQN works well and is as efficient as OWL-QN.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

1. Introduction
Sparse learning with ℓ1 -regularized optimization
problems have been extensively studied (Tibshirani,
1996; Efron et al., 2004) and have been successfully applied to many applications including
face recognition (Wright et al., 2008), gene selecsignal recovery
tion (Shevade & Keerthi, 2003),
(Figueiredo et al., 2007; Wright et al., 2009) and
image deburring (Bioucas-Dias & Figueiredo, 2007;
Beck & Teboulle, 2009). Due to the wide use of the ℓ1 regularized sparse learning problem, many algorithms have
been developed to speedup the optimization. The existing
optimization algorithms can be roughly classified into two
categories. The first family of algorithms, called first-order
algorithms (Beck & Teboulle, 2009; Wright et al., 2009),
mainly utilize the gradient or sub-gradient to solve the
problem. The first-order algorithms have low computational cost per iteration but usually achieve sub-linear
convergence rates. The second family of algorithms,
called (quasi-) Newton algorithms (Tseng & Yun, 2009;
Friedman et al., 2010), rely on the (approximated) Hessian
matrix to achieve linear or super-linear convergence rates
but commonly have high computational cost per iteration.
In general, (quasi-) Newton algorithms need to estimate
the exact or approximated (inverse) Hessian and solve
an ℓ1 -regularized quadratic optimization subproblem at
each iteration (Schmidt et al., 2009; Friedman et al., 2010;
Yuan et al., 2012). Moreover, the subproblem usually does
not admit a closed-form solution. Thus, a specific iterative
algorithm needs to be designed to solve the subproblem.
Different from the (quasi-) Newton framework presented
in Tseng & Yun (2009), the Orthant-Wise Limited memory Quasi-Newton (OWL-QN) method (Andrew & Gao,
2007) does not involve an ℓ1 -regularized quadratic optimization subproblem at each iteration. It generalizes the
limited memory quasi-Newton method (Jorge & Stephen,
1999) from solving the unconstrained smooth optimization problem to the ℓ1 -regularized (non-smooth) optimiza-

A Modified Orthant-Wise Limited Memory Quasi-Newton Method

tion problem; it only involves matrix-vector multiplications without explicitly forming the (inverse) Hessian matrix, making OWL-QN applicable for large-scale problems. Furthermore, OWL-QN is very simple and easy
to implement based on the L-BFGS (Jorge & Stephen,
1999) and is often included as a state-of-the-art method for
comparison in the ℓ1 -regularized sparse learning problem
(Schmidt et al., 2009; Yu et al., 2010; Yuan et al., 2010;
Byrd et al., 2012b;a). Yuan et al. (2010) performed an extensive empirical comparison of existing algorithms for
solving ℓ1 -regularized problems. Experimental results
show that OWL-QN is comparable to the state-of-the-art
algorithms. Moreover, Olsen et al. (2012) developed an
OWL-QN-type algorithm to solve the sparse inverse covariance estimation problem, achieving a very competitive
result, though no convergence analysis was provided.
Although OWL-QN works quite well in practice, several recent papers (Yu et al., 2010; Yuan et al., 2010;
Schmidt et al., 2011; Byrd et al., 2012b) pointed out
that the convergence proof is flawed. In particular,
Schmidt et al. (2011) presented the following comment on
the efficiency and the convergence of the OWL-QN algorithm: “Thus, while the algorithms of Andrew & Gao
(2007) and Section 1.5.1 appear to be very effective in
practice, it remains to show whether they are globally convergent in general without additional assumptions.”
Byrd et al. (2013) also presented a similar comment as follows: “Although this algorithm1 performed reliably in our
tests, its convergence has not been proved”. A key difficulty of the convergence analysis for OWL-QN is that
the objective function is not differentiable and the techniques commonly used in convergence analysis of the gradient methods (Bertsekas, 1999) do not work for OWLQN. In this paper, we first point out the problems of existing convergence analysis for OWL-QN. Then, we propose
a modified Orthant-Wise Limited memory Quasi-Newton
(mOWL-QN) algorithm by slightly modifying the OWLQN algorithm. As the main technical contribution of this
paper, we establish a rigorous convergence analysis for the
mOWL-QN algorithm. Moreover, we present empirical studies to show that mOWL-QN works well and is as efficient as OWL-QN. To our best knowledge, our work fills
the theoretical gap by providing the first rigorous convergence proof for the OWL-QN-type algorithm on solving
ℓ1 -regularized sparse learning problems.
The rest of the paper is organized as follows: We briefly review the OWL-QN algorithm and point out problems in the
existing convergence analysis in Section 2. We propose the
mOWL-QN algorithm by slightly modifying the OWL-QN
algorithm and provide detailed convergence analysis for the
mOWL-QN algorithm in Section 3. We report experimen1

This algorithm here refers to the OWL-QN algorithm.

tal results to validate the convergence analysis in Section 4
and we conclude in Section 5.

2. OWL-QN and Problems in the Existing
Convergence Analysis
OWL-QN (Andrew & Gao, 2007) is designed to solve the
following ℓ1 -regularized optimization problem:
min {f (x) = l(x) + λ∥x∥1 } ,

x∈Rn

(1)

∑n
where ∥x∥1 = i=1 is the ℓ1 -norm of x and l : Rn 7→ R
is convex, bounded from below and continuously differentiable; the gradient ∇l(x) is L-Lipschitz continuous for
some L > 0.
2.1. Basics of OWL-QN
Define a function π : Rn 7→ Rn with the i-th entry being:
{
xi , if σ(xi ) = σ(yi ),
πi (xi ; yi ) =
0, otherwise,
where y ∈ Rn (yi is the i-th entry of y) is the parameter of
the function π; σ(·) is the sign function defined as follows:
σ(xi ) = 1, if xi > 0; σ(xi ) = −1, if xi < 0 and σ(xi ) =
0, otherwise. Define the pseudo-gradient ⋄f (x) whose i-th
entry is given by:

∇i l(x) + λ, if xi > 0,




 ∇i l(x) − λ, if xi < 0,
∇i l(x) + λ, if xi = 0, ∇i l(x) + λ < 0,
⋄i f (x) =


∇i l(x) − λ, if xi = 0, ∇i l(x) − λ > 0,



0,
otherwise.
Then, the objective function in problem (1) is approximated by a quadratic function and a direction is computed by
minimizing that quadratic function as follows:
{
}
1
dk = arg min f (xk ) + ⋄f (xk )T d + dT B k d
2
d∈Rn
= −H k ⋄ f (xk ),
where B k is the (approximated) Hessian matrix at x = xk
and H k = (B k )−1 . Here, Andrew & Gao (2007) use the
L-BFGS2 (Jorge & Stephen, 1999) to approximate the inverse Hessian matrix H k and compute the matrix-vector
multiplication −H k ⋄ f (xk ), which enables OWL-QN to
tackle large-scale problems. To guarantee convergence,
Andrew & Gao (2007) align the direction as follows:
pk = π(dk ; vk ), where vk = − ⋄ f (xk ).
To restrict the next iterate in the same orthant of the previous iterate xk , Andrew & Gao (2007) propose to project
2

More details on the L-BFGS are provided in Supplement A.

A Modified Orthant-Wise Limited Memory Quasi-Newton Method

the point back onto the same orthant of the previous iterate
xk :
xk (α) = π(xk + αpk ; ξ k ),
where

{
ξik

=

σ(xki ), if xki =
̸ 0,
σ(vik ), if xki = 0,

(2)

(3)

and α is a step size chosen by the following line search
procedure: for constants β, γ ∈ (0, 1) and m = 0, 1, · · · ,
find the smallest integer m with α = β m such that
f (xk (α)) ≤ f (xk ) − γ(vk )T (xk (α) − xk ).

(4)

The pseudo code of OWL-QN is given in Algorithm 2.1.
Algorithm 1 OWL-QN: Orthant-Wise Limited memory
Quasi-Newton
1: Initialize x0 , S ← {}, Y ← {} and choose β, γ ∈
(0, 1);
2: for k = 0 to maxiter do
3:
Compute vk ← − ⋄ f (xk );
4:
Compute dk ← H k vk using L-BFGS with S, Y ;
5:
Alignment: pk ← π(dk ; vk );
6:
Initialize α ← 1;
7:
while Eq. (4) is not satisfied do
8:
α ← αβ;
9:
xk (α) ← π(xk + αpk ; ξk );
10:
end while
11:
xk+1 ← xk (α);
12:
if some stopping criterion is satisfied then
13:
stop and return xk+1 ;
14:
end if
15:
Update S with sk ← xk+1 − xk ;
16:
Update Y with yk ← ∇l(xk+1 ) − ∇l(xk );
17: end for

Algorithm 2 mOWL-QN: modified Orthant-Wise Limited
memory Quasi-Newton
1: Initialize x0 , S ← {}, Y ← {} and choose β, γ ∈
(0, 1), ϵ > 0, α0 > 0;
2: for k = 0 to maxiter do
3:
Compute vk ← − ⋄ f (xk ) and
4:
I k = {i ∈ {1, · · · , n} : 0 < |xki | ≤ ϵk , xki vik < 0},
where ϵk = min(∥vk ∥, ϵ);
5:
Initialize α ← α0 ;
6:
if I k = ∅ then
7:
(QN-step)
8:
Compute dk ← H k vk using L-BFGS with S, Y ;
9:
Alignment: pk ← π(dk ; vk );
10:
while Eq. (7) is not satisfied do
11:
α ← αβ;
12:
xk (α) ← π(xk + αpk ; ξk );
13:
end while
14:
else
15:
(GD-step)
16:
while Eq. (8) is not satisfied do
17:
α ← αβ;
{
18:
xk (α) ← arg minx ∇l(xk )T (x −}xk )
1
+ 2α
∥x − xk ∥2 + λ∥x∥1 ;
19:
end while
20:
end if
21:
xk+1 ← xk (α);
22:
if some stopping criterion is satisfied then
23:
stop and return xk+1 ;
24:
end if
25:
Update S with sk ← xk+1 − xk ;
26:
Update Y with yk ← ∇l(xk+1 ) − ∇l(xk );
27: end for
and therefore
(vk )T qkα ≥ (vk )T pk ≥ (vk )T dk .′′

2.2. Problems in the Existing Convergence Analysis of
OWL-QN
Andrew & Gao (2007) utilize the techniques used in the
convergence proof of gradient methods (Bertsekas, 1999)
to prove the convergence of OWL-QN. However, due to the
non-differentiability of the objective function, these techniques do not work for OWL-QN. In the following, we
point out the key problems in the existing convergence
proof.
First, Andrew & Gao (2007) present the following proposition and establish the convergence proof of OWL-QN
based on this proposition.
Proposition 1 “Define qkα = α1 (π(xk + αpk ; ξk ) − xk ).
Then for all α ∈ (0, ∞) and all i,
dki vik ≤ pki vik ≤ (qαk )i vik ≤ 0,

Unfortunately, Proposition 1 is NOT correct (We will correct this proposition in the next section).
Second, Andrew & Gao (2007) claim that {αk } → 0
and {∥qkαk β −1 ∥} is bounded [refer to Theorem 2 in
Andrew & Gao (2007)]. Obviously, {αk } → 0 indicates that the denominator of ∥qkαk β −1 ∥ = αk β1 −1 ∥π(xk +
αk β −1 pk ; ξ k ) − xk ∥ approaches zero. Thus, we can NOT
simply conclude that {∥qkαk β −1 ∥} is bounded.
Third, Andrew & Gao (2007) apply the mean value theorem to obtain that there exists some α̃ ∈ [0, α̂] such that
f (xk + α̂q̂k ) − f (xk )
= f ′ (xk + α̃k q̂k ; q̂k ).
α̂
This is NOT necessarily true, due to the nondifferentiability of f . In fact, we only obtain that

A Modified Orthant-Wise Limited Memory Quasi-Newton Method

there exists some α̃ ∈ [0, α̂] such that

3.1. Corrected Proposition 1
Proposition 2 (Corrected) Define qkα = α1 (π(xk +
αpk ; ξ k ) − xk ). Then for all α ∈ (0, ∞) and all i,

f (xk + α̂q̂k ) − f (xk )
∈ (q̂k )T ∂f (xk + α̃k q̂k ).
α̂

pki vik ≥ dki vik ,

Fourth, based on the following inequality:
′

k

k k

′

k

k

pki vik
k

f (x + α̃ q̂ ; q̂ ) > γf (x ; q̂ ),

(5)

where γ ∈ (0, 1), {α̃k }κ → 0, {xk }κ → x̄ and {q̂k }κ →
q̄, Andrew & Gao (2007) take limits for Eq. (5) and obtain
f ′ (x̄; q̄) > γf ′ (x̄, q̄)

(6)

and hence f ′ (x̄, q̄) > 0, since γ ∈ (0, 1). Unfortunately,
Eq. (6) is NOT correct, because f ′ (xk , q̂k ) and f ′ (xk +
α̃k q̂k , q̂k ) do NOT necessarily converge to f ′ (x̄, q̄). Actually, we only have the following upper semi-continuity
according to Proposition B.23 in Bertsekas (1999):
lim supk∈κ,k→∞ f ′ (xk ; q̂k ) ≤ f ′ (x̄; q̄),
′

′

lim supk∈κ,k→∞ f (x + α̃ q̂ ; q̂ ) ≤ f (x̄; q̄).
k

k k

k

This problem was also pointed out by Yu et al. (2010).

3. Convergence Analysis for mOWL-QN
We first propose a modified Orthant-Wise Limited memory
Quasi-Newton (mOWL-QN) algorithm as in Algorithm 2.1
by slightly modifying the OWL-QN algorithm. At each iteration, mOWL-QN adopts different strategies (QN-step or
GD-step) to generate the next iterate depending on if I k =
{i ∈ {1, · · · , n} : 0 < |xki | ≤ min(∥vk ∥, ϵ), xki vik < 0} is
an empty set. Accordingly, we use different line search criteria for each strategy: for constants α0 > 0, β, γ ∈ (0, 1)
and m = 0, 1, · · · , find the smallest integer m with α =
α0 β m such that the following inequalities hold:
QN-step : f (xk (α)) ≤ f (xk ) − γα(vk )T dk ,
(7)
γ
k
k
k
k 2
GD-step : f (x (α)) ≤ f (x ) −
∥x (α) − x ∥ . (8)
2α

Remark 1 In practice, we set ϵ > 0 as a very small value
(e.g., 10−12 ) such that I k is empty and hence QN-step is
adopted at almost all iterations. Thus, mOWL-QN not only
converges in theory but also works as efficient as OWL-QN
in practice. But without the strategy of switching between
QN-step and GD-step, it is practically impossible to prove
the convergence.
Next, we provide a rigorous convergence analysis for the
mOWL-QN algorithm. We begin the convergence analysis
by showing how to correct Proposition 1.

≥

(9)

(qαk )i vik

≥ 0,

(10)

and therefore
(vk )T pk ≥ (vk )T dk ≥ 0,

(11)

(v ) p ≥ (v )

(12)

k T

k

k T

qkα

≥ 0.

Proof Since pki = πi (dki ; vik ), Eq. (9) is obvious. By the
definition of qkα , we know that (qαk )i = pki or (qαk )i =
−xki /α. We next prove Eq. (10) by considering these two
cases separately.
(a) If (qαk )i = pki , then we have (qαk )i vik = pki vik ≥ 0 by
recalling pki = πi (dki ; vik ) and hence Eq. (10) holds.
(b) If (qαk )i = −xki /α, then we have (xki + αpki )ξik ≤
0 must hold. We next focus on the case (b) in the
following three subcases:
(1) If xki > 0, then by the definition of ξik in Eq. (3),
we have ξik = 1. Recalling (xki +αpki )ξik ≤ 0, we
have xki + αpki ≤ 0 and hence pki < 0. Recalling
pki = πi (dki ; vik ) and (qαk )i = −xki /α, we have
vik < 0 and hence (qαk )i vik > 0, (xki + αpki )vik ≥
0. Thus, we have pki vik −(qαk )i vik ≥ 0. Therefore,
Eq. (10) holds.
(2) If xki < 0, then by the definition of ξik in Eq. (3),
we have ξik = −1. Recalling (xki + αpki )ξik ≤
0, we have xki + αpki ≥ 0 and hence pki > 0.
Recalling pki = πi (dki ; vik ) and (qαk )i = −xki /α,
we have vik > 0 and hence (qαk )i vik > 0, (xki +
αpki )vik ≥ 0. Thus, we have pki vik −(qαk )i vik ≥ 0.
Therefore, Eq. (10) holds.
(3) If xki = 0, then by recalling pki = πi (dki ; vik )
and (qαk )i = −xki /α, we have (qαk )i vik = 0 and
pki vik ≥ 0. Therefore, Eq. (10) holds.
Eq. (11) and Eq. (12) readily follow from Eq. (9) and Eq. (10) by noticing that (vk )T dk = (vk )T H k vk ≥ 0 as
long as H k is positive definite (the positive definiteness of
H k is guaranteed by the L-BFGS).
3.2. Convergence Proof of mOWL-QN
We first provide an optimality condition for problem (1),
which is directly used to prove the final convergence theorem (Theorem 1).
Proposition 3 Let x̄ = limk∈K,k→∞ xk , vk = − ⋄
f (xk ) and v̄ = − ⋄ f (x̄), where K is a subsequence of

A Modified Orthant-Wise Limited Memory Quasi-Newton Method

{1, 2, · · · , k, k + 1, · · · }. If lim inf k∈K,k→∞ |vik | = 0 for
all i ∈ {1, · · · , n}, then v̄ = 0 and hence x̄ is a global
minimizer of problem (1).
Proof We firstly use contradiction to prove that if
lim inf k∈K,k→∞ |vik | = 0 for all i ∈ {1, · · · , n}, then
v̄ = 0. Assume that lim inf k∈K,k→∞ |vik | = 0 for all i ∈ {1, · · · , n} but v̄ ̸= 0. Then there exists at least
one i ∈ {1, · · · , n} such that v̄i = − ⋄i f (x̄) ̸= 0. We
consider the following two cases:
(1) If x̄i ̸= 0, then we have lim inf k∈K,k→∞ |vik | =
|v̄i | ̸= 0, leading to a contradiction with that
lim inf k∈K,k→∞ |vik | = 0 for all i ∈ {1, · · · , n}.
(2) If x̄i = 0, then v̄i = − ⋄i f (x̄) ̸= 0 implies that

(13)

By the definition of vik = − ⋄i f (xk ), we know that
−(∇i l(xk ) + λ) ≤ vik ≤ −(∇i l(xk ) − λ).

+ αpk ; ξk ) − xk ) with α > 0. Then we

(i) ∇l(xk )T (xk (α) − xk ) + λ(∥xk (α)∥1 − ∥xk ∥1 )
= − (vk )T (xk (α) − xk ),
(ii) f (xk (α)) ≤ f (xk ) − α(vk )T qkα +

(15)
2

α L k 2
∥qα ∥ .
2
(16)

Proof (i) Based on the definition of xk (α), we know that
xki (α)xki ≥ 0. We next prove for all i ∈ {1, · · · , n}, the
following equality holds by considering two cases:
∇i l(xk )(xki (α) − xki ) + λ(|xki (α)| − |xki |)
(17)

(a) If xki ̸= 0, then xki (α)xki ≥ 0 implies |xki (α)|−|xki | =
σ(xki )(xki (α) − xki ), which together with ∇i l(xk ) +
λσ(xki ) = −vik (by noticing that xki ̸= 0) implies that
Eq. (17) holds.
(b) If xki = 0, then we have xki (α) = πi (αpki ; σ(vik )) =
αpki . We next focus on the case (b) in the following
two subcases:

Taking limits of the above inequalities, we have
− (∇i l(x̄) + λ) ≤ lim inf vik ≤ −(∇i l(x̄) − λ), and
k∈K,k→∞

− (∇i l(x̄) + λ) ≤ lim sup vik ≤ −(∇i l(x̄) − λ),
k∈K,k→∞

which together with Eq. (13) imply that
lim inf |vik | ̸= 0.

k∈K,k→∞

This
leads
to
a
contradiction
with
that
lim inf k∈K,k→∞ |vik | = 0 for all i ∈ {1, · · · , n}. Therefore, if lim inf k∈K,k→∞ |vik | = 0 for all i ∈ {1, · · · , n},
then v̄ = 0.
To complete the proof, we next prove that x̄ is a global
minimizer of problem (1) if and only if v̄ = 0. According
to the definition of the pseudo gradient, it is easy to verify
that ⋄f (x̄) is the minimum norm sub-gradient at point x =
x̄, that is:
⋄f (x̄) = arg min ∥ḡ∥.

1
k
α (π(x

= − vik (xki (α) − xki ).

∇i l(x̄) + λ > ∇i l(x̄) − λ > 0
or ∇i l(x̄) − λ < ∇i l(x̄) + λ < 0.

and qkα =
have

(14)

ḡ∈∂f (x̄)

Thus, 0 ∈ ∂f (x̄) ⇔ ⋄f (x̄) = 0 ⇔ v̄ = 0 and hence x̄ is a
global minimizer of problem (1) if and only if 0 ∈ ∂f (x̄),
if and only if v̄ = 0.
We subsequently show that we have a similar Lipschitz
continuous inequality in the following proposition, which
is crucial to prove the final convergence theorem.
Proposition 4 Let ∇l(x) be L-Lipschitz continuous for
some L > 0, vk = − ⋄ f (xk ), xk (α) = π(xk + αpk ; ξk )

(1) If pki ̸= 0, then |xki (α)| = ασ(pki )pki =
σ(vik )(αpki ) = σ(vik )xki (α). Thus, |xki (α)| −
|xki | = σ(vik )(xki (α) − xki ). Noticing that pki ̸= 0
implies vik ̸= 0. According to the definition of
vik , we obtain that ∇i l(xk ) + λσ(vik ) = −vik
whenever xki = 0 and vik ̸= 0. Therefore, Eq. (17) holds.
(2) If pki = 0, then |xki (α)|−|xki | = xki (α)−xki = 0,
which implies Eq. (17) holds.
Combining (a) and (b), we obtain that Eq. (17) holds for all
i ∈ {1, · · · , n}, which implies that Eq. (15) holds.
(ii) Since ∇l(x) is L-Lipschitz continuous, we have
l(xk (α)) ≤l(xk ) + ∇l(xk )T (xk (α) − xk )
L
+ ∥xk (α) − xk ∥2 .
2
It follows that
f (xk (α)) ≤ f (xk ) + ∇l(xk )T (xk (α) − xk )
L
+ λ(∥xk (α)∥1 − ∥xk ∥1 ) + ∥xk (α) − xk ∥2 ,
2
which together with Eq. (15) and qkα =
implies that Eq. (16) holds.

1
k
α (x (α)

− xk )

We next show that both line search criteria in QN-step (Eq. (7)) and GD-step (Eq. (8)) at any iteration k is satisfied
in a finite number of trials (The detailed proof is provided
in Supplement C).

A Modified Orthant-Wise Limited Memory Quasi-Newton Method

Proposition 5 At any iteration k of the mOWL-QN algorithm, if xk is not a minimizer of problem (1), then (a) for
QN-step, there exists an α ∈ [ᾱk , α0 ] with 0 < ᾱk ≤ α0
such that the line search criterion in Eq. (7) is satisfied; (b)
for GD-step, the line search criterion in Eq. (8) is satisfied
whenever α ≥ β min(α0 , (1 − γ)/L). That is, both line
search criteria at any iteration k are satisfied in a finite
number of trials.

Then for all k ∈ K̃, k ≥ k̃, we have
{
xk+1 = arg min ∇l(xk )T (x − xk )
x
}
1
+ k ∥x − xk ∥2 + λ∥x∥1 .
2α
By the optimality condition of the above problem, we
have

We finally provide the convergence proof for the mOWLQN algorithm based on the propositions presented above.

λ∥x∥1 ≥ λ∥xk+1 ∥1
(
)T
1 k
k+1
k
+
(x
−
x
)
−
∇l(x
)
(x − xk+1 ),
αk

Theorem 1 The sequence {xk } generated by the mOWLQN algorithm has at least a limit point and every limit point
of {xk } is a global minimizer of problem (1).

∀x ∈ Rn , k ∈ K̃, k ≥ k̃.

Proof It follows from Proposition 5 that both line search
criteria in QN-step (Eq. (7)) and GD-step (Eq. (8)) at each
iteration can be satisfied in a finite number of trials. Let αk
be the accepted step size at iteration k. Then we have
f (xk ) − f (xk+1 ) ≥ γαk (vk )T dk
= γαk (vk )T H k vk (QN-step),
γ
or f (xk ) − f (xk+1 ) ≥
∥xk+1 − xk ∥2
2αk
γ
≥
∥xk+1 − xk ∥2 (GD-step).
2α0

lim

xk = x̄,

k

lim f (x ) =

k→∞

lim

k∈K,k→∞

f (x ) = f¯ = f (x̄).

lim inf |vik | > 0.

⇒

lim

k∈K̃,k→∞

xk =

lim

k∈K̃,k→∞

xk+1 = x̄.

(24)

(19)

λ∥x∥1 ≥ λ∥x̄∥1 − ∇l(x̄)T (x − x̄), ∀x ∈ Rn
⇒ −∇l(x̄) ∈ λ∂∥x̄∥1 ,
which leads to a contradiction with the assumption
that x̄ is not a global minimizer of problem (1).
(b) There exists an integer k̂ > 0 such that for all k ∈
K, k ≥ k̂, QN-step is adopted. According to Remark 5 (in Supplement B), we know that the smallest
eigenvalue of H k is uniformly bounded from below
by a positive constant, which together with Eq. (22)
implies
lim inf (vk )T H k vk > 0.

k∈K,k→∞

(21)

In the following, we prove the theorem by contradiction. Assume that x̄ is not a global minimizer of problem (1). Then by Proposition 3, there exists at least one
i ∈ {1, · · · , n} such that
k∈K,k→∞

∥xk+1 − xk ∥2 ≤ 0

lim

k∈K̃,k→∞

Taking limits with k ∈ K̃ for Eq. (23) and considering
Eq. (24) and αk ≥ β min(α0 , (1 − γ)/L) (Proposition 5), we have

(20)
k

Taking limits with k ∈ K̃ for Eq. (19) and considering
Eqs. (20), (21), we have

(18)

Recalling that H k is positive definite and γ > 0, αk > 0,
which together with Eqs.(18),(19) imply that {f (xk )} is
monotonically decreasing. Thus, {f (xk )} converges to a
finite value f¯, since f is bounded from below. Due to the
boundedness of {xk } (see Proposition 6 in Supplement B),
the sequence {xk } generated by the mOWL-QN algorithm
has at least a limit point x̄. Since f is continuous, there
exists a subsequence K of {1, 2 · · · , k, k +1, · · · } such that
k∈K,k→∞

(23)

(22)

We next consider the following two cases:
(a) There exist a subsequence K̃ of K and an integer k̃ >
0 such that for all k ∈ K̃, k ≥ k̃, GD-step is adopted.

(25)

Taking limits with k ∈ K for Eq. (18), we have
lim

k∈K,k→∞

γαk (vk )T H k vk ≤ 0,

which together with γ ∈ (0, 1), αk ∈ (0, α0 ] and Eq. (25) implies that
lim

k∈K,k→∞

αk = 0.

(26)

Eq. (22) implies that there exist an integer ǩ > 0 and
a constant ϵ̄ > 0 such that ϵk = min(∥vk ∥, ϵ) ≥ ϵ̄
for all k ∈ K, k ≥ ǩ. Notice that for all k ∈ K, k ≥
k̂, QN-step is adopted. Thus, we have I k = {i ∈

A Modified Orthant-Wise Limited Memory Quasi-Newton Method

{1, · · · , n} : 0 < |xki | ≤ ϵk , xki vik < 0} = ∅ for
all k ∈ K, k ≥ k̂. We also notice that, if |xki | ≥
ϵ̄, there exists a constant ᾱi > 0 such that xki (α) =
πi (xki + αpki ; ξik ) = xki + αpki for all α ∈ (0, ᾱi ],
as {pki } is bounded (Proposition 7 in Supplement B).
Therefore, we conclude that, for all k ∈ K, k ≥ k̄ =
max(ǩ, k̂) and for all i ∈ {1, · · · , n}, at least one of
the following three cases must happen:
xki = 0 ⇒ xki (α) = πi (xki + αpki ; ξik )
or |xki | > ϵk ≥ ϵ̄ ⇒ xki (α) = πi (xki + αpki ; ξik )
or xki vik ≥ 0 ⇒ xki pki ≥ 0 ⇒
xki (α) = πi (xki + αpki ; ξik ) = xki + αpki , ∀α > 0.
It follows that there exists a constant ᾱ > 0 such that
qkα =

(27)

Thus, considering Eq. (11) and |pki | = |πi (dki ; vik )| ≤
|dki | for all i ∈ {1, · · · , n}, we have
∥qkα ∥2 = ∥pk ∥2 ≤ ∥dk ∥2 = (vk )T (H k )2 vk ,
k T

(v )

qkα

(28)

= (v ) p ≥ (v ) d = (v ) H v ,
k T

k

k T

k

k T

∀k ∈ K, k ≥ k̄, α ∈ (0, ᾱ].

k k

(29)

According to Proposition 7 (in Supplement B), we
know that the largest eigenvalue of H k is uniformly bounded from above by some positive constant M .
Thus, we have

which together with Eqs. (28), (29) and dk = H k vk
implies
(
)
2
2
k 2
k T k
∥qα ∥ ≤
(v ) qα −
− M (vk )T dk ,
αL
αL
(30)

Considering Eqs. (16), (30), we have
(
)
αLM
k
k
f (x (α)) ≤ f (x ) − α 1 −
(vk )T dk ,
2
∀k ∈ K, k ≥ k̄, α ∈ (0, ᾱ],

Conbiming (a) and (b), we conclude that x̄ =
limk∈K,k→∞ xk is a global minimizer of problem (1),
which together with Eq. (21) implies that every limit point
of {xk } is a global minimizer of problem (1).
Remark 2 The challenge of proving the convergence of
OWL-QN without any modification is: if there exists a subsequence K such that {xki }K converges to zero, it is possible that for a large enough k ∈ K, |xki | is arbitrarily small
but xki is never equal to zero. In this case, Eq. (27) cannot
be obtained (note that for k ∈ K, k + 1 may not be in K),
while Eq. (27) is critical to the subsequent proof. To exclude the above case in the QN-step, we propose to switch
the iteration to the GD-step when |xki | is very small. We
also change (vk )T qkα [note that qkα = (xk (α) − xk )/α]
in Eq. (4) to (vk )T dk in Eq. (7) for the line search in the
QN-step. Such a modification is needed to obtain Eq. (26)
which is the contradiction we will show in the subsequent
proof. If Proposition 1 is correct, we do not need this modification, because (vk )T qkα ≥ (vk )T pk ≥ (vk )T dk can
also lead to Eq. (26).
Remark 3 Note that even for the QN-step, we use a totally
different proof framework from OWL-QN (Andrew & Gao,
2007). In particular, the Lipschitz-continuous-like inequality in Proposition 4 is new for non-smooth problems and it
is critical to the convergence proof.

2
(vk )T (H k )2 vk ≤
(vk )T H k vk
αL
(
)
2
−
− M (vk )T H k vk , ∀k,
αL

∀k ∈ K, k ≥ k̄, α ∈ (0, ᾱ].

Considering the backtracking form of the line search
in QN-step (Eq. (7)), we conclude that the line search
criterion in QN-step (Eq. (7)) is satisfied whenever

This leads to a contradiction with Eq. (26).

= xki + αpki , ∀α ∈ (0, ᾱi ],

∀k ∈ K, k ≥ k̄, α ∈ (0, ᾱ],

αLM
≥ γ , 0 < α ≤ α0 and 0 < α ≤ ᾱ,
2
∀k ∈ K, k ≥ k̄.
1−

αk ≥ β min(min(ᾱ, α0 ), 2(1 − γ)/(LM )) > 0,
∀k ∈ K, k ≥ k̄.

= xki + αpki , ∀α > 0,

1 k
(x (α) − xk ) = pk ,
α
∀k ∈ K, k ≥ k̄, α ∈ (0, ᾱ].

which together with Eq. (11) implies that the line
search criterion in QN-step (Eq. (7)) is satisfied if

4. Experiments
In this section, we validate the convergence analysis by applying the mOWL-QN algorithm to solve the following ℓ1 regularized logistic regression problem:
{
}
N
1 ∑
T
min f (x) =
log(1 + exp(−yi ai x)) + λ∥x∥1 ,
x∈Rn
N i=1
where N is the number of samples; λ > 0 is the regularized
parameter; ai ∈ Rn is the i-th sample; yi ∈ {1, −1} is the
label of the sample ai .

A Modified Orthant-Wise Limited Memory Quasi-Newton Method

Table 1. Data set statistics.

3
real-sim
72,309
20,958

kdd2010a (λ=10/N)

5. Conclusions
In this paper, we establish a detailed convergence analysis
for the mOWL-QN algorithm which is based on a slight
modification of a well-known algorithm called OWL-QN.
To the best of our knowledge, this is the first work to provide a rigorous convergence proof and fills the theoretical
gap for the OWL-QN-type algorithm.
3

The main purpose of the current experiments is to show that
the modified algorithm is almost identical to the original algorithm when ϵ is set to be very small. Meanwhile, the modified
algorithm has a rigorous convergence guarantee.

Objective Function Value (logged scale)

10 1

10 0

50

100
CPU time (seconds)

150

mOWL-QN
OWL-QN

10 0

0

200

50

100

mOWL-QN
OWL-QN

10 1

10

0

0

50

100
CPU time (seconds)

150

Objective Function Value (logged scale)

1.5
1

0.5

0.5

1
1.5
2
CPU time (seconds)

0

50

100
150
200
CPU time (seconds)

2.5

mOWL-QN
OWL-QN

0.5
0.4
0.3

0.2

0

0.5

0.6

0.4

0.2

20

40
60
CPU time (seconds)

1

1.5
2
2.5
CPU time (seconds)

3

3.5

4

rcv1 (λ=1/N)

0.8

0

300

0.6

3

mOWL-QN
OWL-QN

1

250

0.9
0.8
0.7

rcv1 (λ=10/N)
1.2

400

real-sim (λ=1/N)
mOWL-QN
OWL-QN

0

350

10 0

200

3
2

300

mOWL-QN
OWL-QN

real-sim (λ=10/N)
2.5

150
200
250
CPU time (seconds)
kdd2010b (λ=1/N)

Objective Function Value (logged scale)

Objective Function Value (logged scale)

kdd2010b (λ=10/N)

Objective Function Value (logged scale)

We report the objective function value vs. CPU time plots in Figure 1. We observe that our results agree with
empirical studies of OWL-QN algorithms in existing literature (Schmidt et al., 2009; Yu et al., 2010; Yuan et al.,
2010; Byrd et al., 2012a;b; 2013), that is, the OWL-QN
algorithm works very well in practice, though a rigorous
convergence proof has not been established so far. We also observe that mOWL-QN and OWL-QN have very similar convergence behaviors since the convergence curves of
mOWL-QN and OWL-QN almost overlap with each other.
The underlying reason is that, for a very small ϵ, mOWLQN adopts QN-step at almost all iterations3 . Moreover,
both line search criteria in Eq. (4) and Eq. (7) accept the
unit step size (i.e., the line search terminates in only one
trial).

kdd2010a (λ=1/N)
mOWL-QN
OWL-QN

0

Both algorithms are implemented in Matlab and executed
on an Intel(R) Core(TM)2 i7-3770 CPU (@3.4GHz) with
32GB memory. We choose the starting points x0 for both
algorithms using the same random vector whose entries are
independently sampled from the standard Gaussian distribution. We terminate both algorithms if the relative change
of two consecutive objective function values is less than
10−5 or the number of iterations exceeds 500. We set
γ = 10−2 , β = 0.2, α0 = 1, ϵ = 10−12 and the number
of unrolling steps (in L-BFGS) as m = 10.

4
rcv1
677,399
47,236

Objective Function Value (logged scale)

We include mOWL-QN and OWL-QN algorithms in comparison. Experiments are conducted on four large scale data sets which are summarized in Table 1. These data sets
are high-dimensional and sparse and can be downloaded
from http://www.csie.ntu.edu.tw/˜cjlin
/libsvmtools/datasets/binary.html.

2
kdd2010b
748,401
29,890,095

Objective Function Value (logged scale)

1
kdd2010a
510,302
20,216,830

Objective Function Value (logged scale)

No.
datasets
♯ samples N
dimensionality n

80

100

mOWL-QN
OWL-QN

10 -1
0

20

40
60
80
CPU time (seconds)

100

120

Figure 1. Objective function value (logged scale) vs. CPU time
(seconds) plots using OWL-QN and mOWL-QN algorithms with
different values of λ.

There are several interesting directions that we will explore
in the future. First, we plan to extend similar ideas to other
sparsity-inducing problems including non-convex regularized problems. Second, we plan to analyze the convergence
rate of the mOWL-QN algorithm in the future. Third, we
plan to develop parallel and distributed variants of the algorithm to deal with much larger data sets.

Acknowledgements
We would like to thank reviewers for their constructive
comments. This work is supported in part by research
grants from NIH (R01 LM010730, U54 EB020403) and
NSF (IIS- 0953662, IIS-1421057, IIS-1421100).

A Modified Orthant-Wise Limited Memory Quasi-Newton Method

References
Andrew, G. and Gao, J. Scalable training of ℓ1 -regularized
log-linear models. In Proceedings of the 24th International Conference on Machine Learning, pp. 33–40,
2007.
Beck, A. and Teboulle, M. A fast iterative shrinkagethresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.
Bertsekas, D.P. Nonlinear Programming. Athena Scientific, 1999.
Bioucas-Dias, J.M. and Figueiredo, M.A.T. A new TwIST:
two-step iterative shrinkage/thresholding algorithms for
image restoration. IEEE Transactions on Image Processing, 16(12):2992–3004, 2007.
Byrd, R. H., Lu, P., Nocedal, J., and Zhu, C. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5):
1190–1208, 1995.
Byrd, R. H., Chin, G. M., Nocedal, J., and Oztoprak,
F. A family of second-order methods for convex ℓ1 regularized optimization. Technical report, Industrial
Engineering and Management Sciences, Northwestern
University, Evanston, IL, 2012a.
Byrd, R. H., Chin, G. M., Nocedal, J., and Wu, Y. Sample size selection in optimization methods for machine
learning. Mathematical Programming, 134(1):127–155,
2012b.
Byrd, R.H., Nocedal, J., and Oztoprak, F. An inexact successive quadratic approximation method for convex ℓ1 regularized optimization. arXiv preprint arXiv:1309.3529, 2013.
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. Least
angle regression. Annals of Statistics, 32(2):407–451,
2004.
Figueiredo, M.A.T., Nowak, R.D., and Wright, S.J. Gradient projection for sparse reconstruction: Application to
compressed sensing and other inverse problems. IEEE
Journal on Selected Topics in Signal Processing, 1(4):
586–597, 2007.
Friedman, J., Hastie, T., and Tibshirani, R. Regularization paths for generalized linear models via coordinate
descent. Journal of Statistical Software, 33(1):1, 2010.
Jorge, N. and Stephen, J.
Springer, 1999.

Numerical Optimization.

Olsen, P., Oztoprak, F., Nocedal, J., and Rennie, S.
Newton-like methods for sparse inverse covariance estimation. In Advances in Neural Information Processing
Systems (NIPS), pp. 764–772, 2012.
Schmidt, M., Berg, E., Friedlander, M., and Murphy, K.
Optimizing costly functions with simple constraints: A
limited-memory projected quasi-newton algorithm. In
International Conference on Artificial Intelligence and
Statistics, 2009.
Schmidt, M., Kim, D., and Sra, S. Optimization on Machine Learning, chapter Projected Newton-type Methods
in Machine Learning. MIT Press, 2011.
Shevade, S.K. and Keerthi, S.S. A simple and efficient algorithm for gene selection using sparse logistic regression. Bioinformatics, 19(17):2246, 2003.
Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society. Series B
(Methodological), 58(1):267–288, 1996.
Tseng, P. and Yun, S. A coordinate gradient descent method for nonsmooth separable minimization. Mathematical Programming, 117(1-2):387–423, 2009.
Wright, J., Yang, A.Y., Ganesh, A., Sastry, S.S., and Ma, Y.
Robust face recognition via sparse representation. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 31(2):210–227, 2008.
Wright, S.J., Nowak, R., and Figueiredo, M. Sparse reconstruction by separable approximation. IEEE Transactions on Signal Processing, 57(7):2479–2493, 2009.
Yu, J., Vishwanathan, S., Günter, S., and Schraudolph,
N. A quasi-newton approach to nonsmooth convex optimization problems in machine learning. The Journal of
Machine Learning Research, 11:1145–1200, 2010.
Yuan, G., Chang, K., Hsieh, C., and Lin, C. A comparison
of optimization methods and software for large-scale ℓ1 regularized linear classification. The Journal of Machine
Learning Research, 11:3183–3234, 2010.
Yuan, G., Ho, C., and Lin, C. An improved glmnet for ℓ1 regularized logistic regression. The Journal of Machine
Learning Research, 13:1999–2030, 2012.

