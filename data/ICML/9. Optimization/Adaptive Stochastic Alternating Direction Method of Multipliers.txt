Adaptive Stochastic Alternating Direction Method of Multipliers

Peilin Zhao∗,‡
ZHAOP @ I 2 R . A - STAR . EDU . SG
Jinwei Yang†
JYANG 7@ ND . EDU
Tong Zhang‡
TZHANG @ STAT. RUTGERS . EDU
Ping Li§
PINGLI @ STAT. RUTGERS . EDU
∗
Data Analytics Department, Institute for Infocomm Research, A*STAR, Singapore
†
Department of Mathematics, Rutgers University; and Department of Mathematics, University of Notre Dame, USA
‡
Department of Statistics & Biostatistics, Rutgers University, USA; and Big Data Lab, Baidu Research, China
§
Department of Statistics & Biostatistics, Department of Computer Science, Rutgers University; and Baidu Research, USA

Abstract
The Alternating Direction Method of Multipliers
(ADMM) has been studied for years. Traditional ADMM algorithms need to compute, at each
iteration, an (empirical) expected loss function
on all training examples, resulting in a computational complexity proportional to the number
of training examples. To reduce the complexity, stochastic ADMM algorithms were proposed
to replace the expected loss function with a random loss function associated with one uniformly
drawn example plus a Bregman divergence term. The Bregman divergence, however, is derived
from a simple 2nd-order proximal function, i.e.,
the half squared norm, which could be a suboptimal choice.
In this paper, we present a new family of stochastic ADMM algorithms with optimal 2nd-order
proximal functions, which produce a new family of adaptive stochastic ADMM methods. We
theoretically prove that the regret bounds are as
good as the bounds which could be achieved by
the best proximal function that can be chosen in
hindsight. Encouraging empirical results on a variety of real-world datasets confirm the effectiveness and efficiency of the proposed algorithms.

1. Introduction
Originally introduced in (Glowinski & Marroco, 1975;
Gabay & Mercier, 1976), the offline/batch Alternating
Direction Method of Multipliers (ADMM) stemmed
from the augmented Lagrangian method, with its
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

global convergence property established in (Gabay,
1983; Glowinski & Le Tallec, 1989; Eckstein & Bertsekas,
1992). Recent studies have shown that ADMM achieves
a convergence rate of O(1/T ) (Monteiro & Svaiter, 2013;
He & Yuan, 2012) (where T is number of iterations of
ADMM), when the objective function is generally convex. Furthermore, ADMM enjoys a convergence rate
of O(αT ), for some α ∈ (0, 1), when the objective function is strongly convex and smooth (Luo, 2012;
Deng & Yin, 2012). ADMM has shown attractive performance in a wide range of real-world problems such as
compressed sensing (Yang & Zhang, 2011), image restoration (Goldstein & Osher, 2009), video processing, and matrix completion (Goldfarb et al., 2013), etc.
From the computational perspective, one drawback of ADMM is that, at every iteration, the method needs to compute
an (empirical) expected loss function on all the training examples. The computational complexity is propositional to
the number of training examples, which makes the original
ADMM unsuitable for solving large-scale learning and big
data mining problems. The online ADMM (OADMM) algorithm (Wang & Banerjee, 2012) was proposed to tackle
the computational challenge. For OADMM, the objective
function is replaced with an online function at every step,
which only depends on a single training example.√ OADMM can achieve an average regret bound of O(1/ T ) for
convex objective functions and O(log(T )/T ) for strongly convex objective functions. Interestingly, although the
optimization of the loss function is assumed to be easy in
the analysis of (Wang & Banerjee, 2012), this step is actually not necessarily easy in practice. To address this issue,
the stochastic ADMM algorithm was proposed, by linearizing the online loss function (Ouyang et al., 2013; Suzuki,
2013). In stochastic ADMM algorithms, the online loss
function is firstly uniformly drawn from all the loss functions associated with all the training examples. Then the

Adaptive Stochastic Alternating Direction Method of Multipliers

loss function is replaced with its first order expansion at the
current solution plus the Bregman divergence from the current solution. The Bregman divergence is based on a simple
proximal function, the half squared norm, so that the Bregman divergence is the half squared distance. In this way,
the optimization of the loss function enjoys a closed-form
solution. The stochastic ADMM achieves similar convergence rates as OADMM. Using the half square norm as
proximal function, however, may be a suboptimal choice.
Our paper will address this issue. We should mention
that there are other strategies recently adopted to accelerate
stochastic ADMM, including stochastic average gradient (Zhong & Kwok, 2014), dual coordinate ascent (Suzuki,
2014), which are still based on half squared distance.
Our contribution. In the previous work (Ouyang et al.,
2013; Suzuki, 2013) the Bregman divergence is derived
from a simple second order function, i.e., the half squared
norm, which could be a suboptimal choice (Duchi et al.,
2011). In this paper, we present a new family of stochastic ADMM algorithms with adaptive proximal functions,
which can accelerate stochastic ADMM by using adaptive regularization. We theoretically prove that the regret
bounds of our methods are as good as those achieved by
stochastic ADMM with the best proximal function that can
be chosen in hindsight. The effectiveness and efficiency
of the proposed algorithms are confirmed by encouraging
empirical evaluations on several real-world datasets.

2. Adaptive Stochastic Alternating Direction
Method of Multipliers
2.1. Problem Formulation
In this paper, we will study a family of convex optimization problems, where our objective functions are composite. Specially, we are interested in the following equalityconstrained optimization task:
min

w∈W,v∈V

f ((w⊤ , v⊤ )⊤ ) := Eξ ℓ(w, ξ) + φ(v),

(1)

s.t. Aw + Bv = b,
where w ∈ Rd1 , v ∈ Rd2 , A ∈ Rm×d1 , B ∈ Rm×d2 ,
b ∈ Rm , W and V are convex sets. For simplicity, the notation ℓ is used for both the instance function value ℓ(w, ξ)
and its expectation ℓ(w) = Eξ ℓ(w, ξ). It is assumed that a
sequence of identical and independent (i.i.d.) observations can be drawn from the random vector ξ, which satisfies
a fixed but unknown distribution. When ξ is deterministic, the above optimization becomes the traditional formulation of ADMM (Boyd et al., 2011). In this paper, we will
assume the functions ℓ and φ are convex but not necessarily continuously differentiable. In addition, we denote the
optimal solution of (1) as (w∗⊤ , v∗⊤ )⊤ .
Before presenting the proposed algorithm, we first introduce some notations. For a positive definite matrix G ∈

Rd1 ×d1 , we define the G-norm of a vector w as ∥w∥G :=
√
w⊤ Gw. When there is no ambiguity, we often use ∥ · ∥
to denote the Euclidean norm ∥ · ∥2 . We use ⟨·, ·⟩ to denote
the inner product in a finite dimensional Euclidean space.
Let Ht be a positive definite matrix for t ∈ N. Set the proximal function ϕt (·), as ϕt (w) = 12 ∥w∥2Ht = 21 ⟨w, Ht w⟩.
Then the corresponding Bregman divergence for ϕt (w) is
defined as
Bϕt (w, u)

ϕt (w) − ϕt (u) − ⟨∇ϕt (u), w − u⟩
1
=
∥w − u∥2Ht .
2

=

2.2. Algorithm
To solve the problem (1), a popular method is Alternating
Direction Method of Multipliers (ADMM). ADMM splits
the optimization with respect to w and v by minimizing the
augmented Lagrangian:
min Lβ (w, v, θ) := ℓ(w) + φ(v) − ⟨θ, Aw + Bv − b⟩
w,v

β
+ ∥Aw + Bv − b∥2 ,
2
where β > 0 is a pre-defined penalty. Specifically, the
ADMM algorithm minimizes Lβ as follows
wt+1

=

arg min Lβ (w, vt , θt ),

vt+1

=

arg min Lβ (wt+1 , v, θt ),

θt+1

= θt − β(Awt+1 + Bvt+1 − b).

w
v

At each step, however, ADMM requires calculating the expectation Eξ ℓ(w, ξ), which may be computationally too expensive, since we may only have an unbiased estimate of
ℓ(w) or the expectation Eξ ℓ(w, ξ) is an empirical one for
big data problems. To solve this issue, we propose to minimize the following stochastic approximation:
Lβ,t (w, v, θ) = ⟨gt , w⟩ + φ(v) − ⟨θ, Aw + Bv − b⟩
β
1
+ ∥Aw + Bv − b∥2 + Bϕt (w, wt ),
2
η
where gt = ℓ′ (wt , ξt ) and Ht for ϕt = 21 ∥w∥2Ht will
be specified later. This objective linearizes ℓ(w, ξt ) and
adopts a dynamic Bregman divergence function to keep
the new model near to the previous one. It is easy to see
that this proposed approximation includes the one proposed
by (Ouyang et al., 2013) as a special case when Ht = I. To
minimize the above function, we followed the ADMM algorithm to optimize over w, v, θ sequentially, by fixing the
others. In addition, we also need to update Ht for Bϕt at every step, which will be specified later. Finally the proposed
Adaptive Stochastic Alternating Direction Method of Multipliers (Ada-SADMM) is summarized in Algorithm 1.

Adaptive Stochastic Alternating Direction Method of Multipliers

Algorithm 1 Adaptive Stochastic Alternating Direction
Method of Multipliers (Ada-SADMM).
Initialize: w1 = 0, u1 = 0, θ1 = 0, H1 = aI, and
a > 0.
for t = 1, 2, . . . , T do
Compute gt = ℓ′ (wt , ξt );
Update Ht and compute Bϕt ;
wt+1 = arg minw∈W Lβ,t (w, vt , θt );
vt+1 = arg minv∈V Lβ,t (wt+1 , v, θt );
θt+1 = θt − β(Awt+1 + Bvt+1 − b);
end for

tions, using convexity of ℓ(w) and φ(v) and the monotonicity of operator F (·), we have for any z:
f (ūT ) − f (u) + (z̄T − z)⊤ F (z̄T )
≤

T
1 ∑
⊤
[f ((wt⊤ , vt+1
)⊤ ) − f (u) + (zt+1 − z)⊤ F (zt+1 )]
T t=1

=

1∑
[ℓ(wt )+φ(vt+1 )− ℓ(w)−φ(v)+(zt+1− z)⊤F (zt+1 )].
T t=1
T

Combining this inequality with Lemma 1 at the optimal solution (w, v) = (w∗ , v∗ ), we can derive
f (ūT ) − f (u∗ ) + (z̄T − z∗ )⊤ F (z̄T )

2.3. Analysis
This subsection is devoted to analyzing the expected convergence rate of the iterative solutions of the proposed algorithm for general Ht , t = 1, . . . , T , where the proof techniques in (Ouyang et al., 2013) and (Duchi et al., 2011) are
adopted. To accomplish this, we begin with a technical
lemma, which will facilitate the later analysis.
Lemma 1. Let ℓ(w, ξt ) and φ(w) be convex functions and
Ht be positive definite, for t ≥ 1. For Algorithm 1, we have
the following inequality
ℓ(wt ) + φ(vt+1 ) − ℓ(w) − φ(v) + (zt+1 − z)⊤ F (zt+1 )

≤

η∥gt ∥2Ht∗

1
+ [Bϕt (wt , w) − Bϕt (wt+1 , w)]
2
η
β
+ (∥Aw + Bvt − b∥2 − ∥Aw + Bvt+1 − b∥2 )
2
1
+⟨δt , w − wt ⟩ +
(∥θ − θt ∥2 − ∥θ − θt+1 ∥2 ),
2β

(wt⊤ , vt⊤ , θt⊤ )⊤ ,

⊤

⊤

⊤ ⊤

where zt =
z = (w , v , θ ) , δt =
gt − ℓ′ (wt ), and F (z) = ((−A⊤ θ)⊤ , (−B ⊤ θ)⊤ , (Aw +
Bv − b)⊤ )⊤ .
The proof is in the appendix. We now analyze the convergence behavior of Algorithm 1 and provide an upper bound
on the objective value and the feasibility violation.
Theorem 1. Let ℓ(w, ξt ) and φ(w) be convex functions
and Ht be positive definite, for t ≥ 1. For Algorithm 1, we
have the following inequality for any T ≥ 1 and ρ > 0:
E[f (ūT ) − f (u∗ ) + ρ∥Aw̄T + B v̄T − b∥] ≤
T
1 { ∑[2
E
(Bϕt (wt , w∗ ) − Bϕt (wt+1 , w∗ ))
2T
η
t=1
]
ρ2 }
+η∥gt ∥2Ht∗ + βDv2 ∗ ,B +
,
(2)
β

( ∑
T
1

∑T +1
where ūT = T t=1 wt⊤ , T1 t=2 vt⊤
∑T +1
(w∗⊤ , v∗⊤ )⊤ , and (w̄T , v̄T ) = ( T1 t=2 wt , T1
and Dv∗ ,B = ∥Bv∗ ∥.

)⊤

, u∗ =
∑T +1
t=2 vt ),

Proof. For convenience, we denote u = (w⊤ , v⊤ )⊤ , θ̄T =
∑T +1
1
⊤
⊤ ⊤ ⊤
t=2 θt , and z̄T = (w̄T , v̄T , θ̄T ) . With these notaT

≤

T
η∥gt ∥2Ht∗
1 ∑{1
[Bϕt (wt , w∗ ) − Bϕt (wt+1 , w∗ )] +
T t=1 η
2

β
(∥Aw∗ + Bvt − b∥2
2
}
1
−∥Aw∗ + Bvt+1 − b∥2 ) +
(∥θ − θt ∥2 − ∥θ − θt+1 ∥2 )
2β
T
{
∑
η∥gt ∥2Ht∗
[
1
1
≤
[Bϕt (wt , w∗ ) − Bϕt (wt+1 , w∗ )] +
T t=1 η
2
}
] β
1
+⟨δt , w∗− wt ⟩ + ∥Aw∗ + Bv1− b∥2 +
∥θ− θ1 ∥2
2
2β
T
−1
{
η∥gt ∥2Ht∗
1 ∑ [1
≤
(Bϕt (wt , w∗ ) − Bϕt (wt+1 , w∗ )) +
T t=0 η
2
}
] β
1
+⟨δt , w∗ − wt ⟩ + Dv2 ∗ ,B +
∥θ − θ1 ∥2 .
2
2β
+⟨δt , w∗ − wt ⟩ +

As the above inequality is valid for any θ, it also holds in
the ball Bρ = {θ : ∥θ∥ ≤ ρ}. Combining with the fact that
the optimal solution must also be feasible, it follows that
max {f (ūT ) − f (u∗ ) + (z̄T − z∗ )⊤ F (z̄T )}

θ∈Bρ

= max {f (ūT ) − f (u∗ ) + θ̄T⊤ (Aw∗ + Bv∗ − b)
θ∈Bρ

−θ⊤ (Aw̄T + B v̄T − b)}
= max {f (ūT ) − f (u∗ ) − θ⊤ (Aw̄T + B v̄T − b)}
θ∈Bρ

= f (ūT ) − f (u∗ ) + ρ∥Aw̄T + B v̄T − b∥.

Utilizing the above two inequalities, we obtain
E[f (ūT ) − f (u∗ ) + ρ∥Aw̄T + B v̄T − b∥]
T
1 {∑(1
[Bϕt (wt , w∗ ) − Bϕt (wt+1 , w∗ )]
≤ E
T
η
t=1
+

η∥gt ∥2Ht∗

}
) β
1
) + ⟨δt , w∗ − wt ⟩ + Dv2 ∗ ,B +
∥θ − θ1 ∥2
2
2β

2
T
1 { ∑ 2
≤
E
[ [Bϕt (wt , w∗ ) − Bϕt (wt+1 , w∗ )]
2T
η
t=1
ρ2 }
+η∥gt ∥2Ht∗ ] + βDv2 ∗ ,B +
,
β

Note Eδt = 0 in the last step. This completes the proof.

Adaptive Stochastic Alternating Direction Method of Multipliers

The above theorem allows us to derive regret bounds for
a family of algorithms that iteratively modify the proximal
functions ϕt in attempt to lower the regret bounds. Since
the rate of convergence is still dependent on Ht and η, next
we are going to choose appropriate positive definite matrix
Ht and the constant η to optimize the rate of convergence.
2.4. Diagonal Matrix Proximal Functions
In this subsection, we restrict Ht to be a diagonal matrix,
for two important reasons: (i) the diagonal matrix will provide results easier to understand than that for the general
matrix; (ii) for high dimension problem the general matrix
may result in prohibitively expensive computational cost,
which is not desirable.
Firstly, we∑notice that the upper bound in the Theorem 1
T
2
relies on
t=1 ∥gt ∥Ht∗ . If we assume all the gt ’s are
known in advance, we could minimize this term by setting
Ht = diag(s), ∀t. We shall use the following proposition.
Proposition 1. For any g1 , g2 , . . . , gT ∈ Rd1 , we have
T
∑

min

diag(s)≽0, 1⊤ s≤c

∥gt ∥2diag(s) =

t=1

Algorithm 2 Adaptive Stochastic ADMM with Diagonal
Matrix Update (Ada-SADMMdiag ).
Initialize: w1 = 0, u1 = 0 , θ1 = 0, and a > 0.
for t = 1, 2, . . . , T do
Compute gt = ℓ′ (wt , ξt );
Update Ht = aI + diag(st ), where st,i = ∥g1:t,i ∥;
wt+1 = arg minw Lβ,t (w, vt , θt );
vt+1 = arg minv∈V Lβ,t (wt+1 , v, θt );
θt+1 = θt − β(Awt+1 + Bvt+1 − b);
end for
inequality for any T ≥ 1 and ρ > 0
E[f (ūT ) − f (u∗ ) + ρ∥Aw̄T + B v̄T − b∥]
d1
∑
1 (
≤
E[2η
∥g1:T,i ∥
2T
i=1
∑
2
ρ2 )
+ max ∥wt − w∗ ∥2∞
∥g1:T,i ∥] + βDv2 ∗ ,B +
.
η t≤T
β
i=1
d1

√
If we further set η = Dw,∞ / 2 where Dw,∞ =
maxw,w′ ∥w − w′ ∥∞ , then we have

d1
)2
1(∑
∥g1:T,i ∥ ,
c i=1

E[f (ūT ) − f (u∗ ) + ρ∥Aw̄T + B v̄T − b∥]

where g1:T,i = (g1,i , . . . , gT,i )⊤ and the minimum is at∑d1
∥g1:T,j ∥.
tained at si = c∥g1:T,i ∥/ j=1
We omit proof of this proposition, since it is easy to derive.
Since we do not have all the gt ’s in advance, we receives
the stochastic (sub)gradients gt sequentially instead. As a
result, we propose to update the Ht incrementally as Ht =
aI + diag(st ), where st,i = ∥g1:t,i ∥ and a ≥ 0. For these
Ht s, we have the following inequality
T
∑

∥gt ∥2Ht∗ =

t=1

T
∑
⟨gt , (aI + diag(st ))−1 gt ⟩

d1
T
∑
∑
∥g1:T,i ∥,
⟨gt , diag(st )−1 gt ⟩ ≤ 2
t=1

The proof is in the appendix.
2.5. Full Matrix Proximal Functions
In this subsection, we derive and analyze new updates when
we estimate a full matrix Ht for the proximal function instead of a diagonal one. Although full matrix computation
may not be attractive for high dimension problems, it may
be helpful for tasks with low dimension. Furthermore, it
will provide us with a more complete insight. Similar with
the analysis for the diagonal case, we first introduce the
following proposition (Lemma 15 in (Duchi et al., 2011)).
Proposition 2. For any g1 , g2 , . . . , gT ∈ Rd1 , we have the
following equality

t=1

≤

∑
β
1 (√
ρ2 )
∥g1:T,i ∥] + Dv2 ∗ ,B +
2E[Dw,∞
.
T
2
2β
i=1
d1

≤

(3)

i=1

min
S≽0, tr(S)≤c

∑T

T
∑
t=1

∥gt ∥2S −1 =

1
tr(GT ),
c

where the last inequality used Lemma 4 in (Duchi et al.,
2011), which implies this update is a nearly optimal update method for the diagonal matrix case. Finally, the adaptive stochastic ADMM with diagonal matrix update (AdaSADMMdiag ) is summarized into the Algorithm 2.

where GT = t=1 gt gt⊤ . and the minimizer is attained at
1/2
1/2
S = cGT /tr(GT ). If GT is not of full rank, then we use
its pseudo-inverse to replace its inverse in the minimization
problem.

For the convergence rate of the proposed Algorithm 2, we
have the following specific theorem.

Because the (sub)gradients are received sequentially, we
1
2
propose
∑tot update⊤the Ht incrementally as Ht = aI + Gt ,
Gt = i=1 gi gi , t = 1, . . . , T . For these Ht s, we have
the following inequalities

Theorem 2. Let ℓ(w, ξt ) and φ(w) be convex functions
for any t > 0. Then for Algorithm 2, we have the following

Adaptive Stochastic Alternating Direction Method of Multipliers

T
∑

T
∑

t=1

t=1

∥gt ∥2Ht∗ ≤

∥gt ∥2S −1≤ 2
t

T
∑

1

∥gt ∥2S −1 =2tr(GT2 ),

t=1

(4)

T

where the last inequality used Lemma 10 in (Duchi et al.,
2011), which implies this update is a nearly optimal update method for the full matrix case. Finally, the adaptive
stochastic ADMM with full matrix update is summarized
in Algorithm 3.
Algorithm 3 Adaptive Stochastic ADMM with Full Matrix
Update (Ada-SADMMf ull ).
Initialize: w1 = 0, u1 = 0, θ1 = 0, G0 = 0, and a > 0
for t = 1, 2, . . . , T do
Compute gt = ℓ′ (wt , ξt );
Update Gt = Gt−1 + gt gt⊤ ;
1

Update Ht = aI + St , where St = Gt2 ;
wt+1 = arg minw Lβ,t (w, vt , θt );
vt+1 = arg minv∈V Lβ,t (wt+1 , v, θt );
θt+1 = θt − β(Awt+1 + Bvt+1 − b);
end for

where [z]+ = max(0, z) and the matrix F is constructed based on a graph G = {V, E}. For this graph, V =
{w1 , . . . , wd1 } is a set of variables and E = {e1 , . . . , e|E| },
where ek = {i, j} is assigned with a weight αij . And the
corresponding F is in the form: Fki = αij and Fkj =
−αij . To construct a graph for a given dataset, we adopt the sparse inverse covariance estimation (Friedman et al.,
2008) and determine the sparsity pattern of the inverse covariance matrix Σ−1 . Based on the inverse covariance matrix, we connect all index pairs (i, j) with Σ−1
ij ̸= 0 and
assign αij = 1.
3.1. Experimental Testbed and Setup
To examine the performance, we test all the algorithms on six real-world datasets from web machine learning repositories, which are listed in the Table 1. The “news20” dataset was downloaded from
www.cs.nyu.edu/˜roweis/data.html. All other
datasets were downloaded from the LIBSVM website. For
each dataset, we randomly divide it into two folds: training
set with 80% of examples and test set with the rest.
Table 1. Several real-world datasets in our experiments.

For the convergence rate of the above proposed Algorithm
3, we have the following specific theorem.
Theorem 3. Let ℓ(w, ξt ) and φ(w) be convex functions
for any t > 0. Then for Algorithm 3, we have the following
inequality for any T ≥ 1, ρ > 0,
E[f (ūT ) − f (u∗ ) + ρ ∥ Aw̄T + B v̄T − b ∥] ≤
1
1
1 {
1
E[2ηtr (GT2 ) + maxt≤T ∥w∗ − wt ∥2 tr (GT2 )]
2T
η
ρ2 }
.
+βDv2 ∗ ,B +
β

Furthermore, if we set η = Dw,2 /2, where Dw,2 =
maxw1 ,w2 ∥w1 − w2 ∥, then we have
E[f (ūT ) − f (u∗ ) + ρ∥Aw̄T + B v̄T − b∥]
1 (√
ρ2 )
β
1/2
≤
2E[Dw,2 tr (GT )] + Dv2 ∗ ,B +
.
T
2
2β

The proof is in the appendix.

3. Experiment
In this section, we evaluate the empirical performance of
the proposed adaptive stochastic ADMM algorithms for
solving Graph-Guided SVM (GGSVM) tasks, which is formulated as the following problem (Ouyang et al., 2013):
min
w,v

n
1∑
γ
∥w∥2 + ν∥v∥1 ,
[1 − yi x⊤
i w]+ +
n i=1
2

s.t. F w − v = 0,

Dataset
a9a
mushrooms
news20
splice
svmguide3
w8a

# examples
48,842
8,124
16,242
3,175
1,284
64,700

# features
123
112
100
60
21
300

To make a fair comparison, all algorithms adopt the same
experimental setup. In particular, we set the penalty parameter γ = ν = 1/n, where n is the number of training
examples, and the trade-off parameter β = 1. In addition,
we set the step size parameter ηt = 1/(γt) for SADMM
according to the theorem 2 in (Ouyang et al., 2013). Finally, the smooth parameter a is set as 1, and the step size for
adaptive stochastic ADMM algorithms are searched from
2[−5:5] using cross validation.
All the experiments were conducted with 5 different random seeds and 2 epochs (2n iterations) for each dataset.
All the result were reported by averaging over these 5 runs. We evaluated the learning performance by measuring
objective values, i.e., f (u), and test error rates on the test
datasets. In addition, we also evaluate computational efficiency of all the algorithms by their running time. All
experiments were run in Matlab over a machine of 3.4GHz
CPU.
3.2. Performance Evaluation
The figure 1 shows the performance of all the algorithms
in comparison over trials, from which we can draw several
observations. Firstly, the left column shows the objective

Adaptive Stochastic Alternating Direction Method of Multipliers

Table 2. Evaluation of stochastic ADMM algorithms on the real-world data sets.
Algorithm

a9a

mushrooms

Objective value

Test error rate

Time (s)

Objective value

Test error rate

SADMM

2.6002 ± 0.4271

0.1646 ± 0.0075

56.0914

0.7353 ± 0.2104

0.0350 ± 0.0136

7.6619

Ada-SADMMdiag

0.3550 ± 0.0001

0.1501 ± 0.0012

94.7619

0.0096 ± 0.0005

0.0006 ± 0.0000

13.0355

Ada-SADMMf ull

0.3545 ± 0.0001

0.1498 ± 0.0013

622.4459

0.0091 ± 0.0002

0.0002 ± 0.0003

67.8198

Algorithm

news20

Time (s)

splice

Objective value

Test error rate

Time (s)

Objective value

Test error rate

Time (s)

SADMM

0.5652 ± 0.0151

0.1333 ± 0.0034

13.2948

108.6823 ± 20.9655

0.2454 ± 0.0322

0.9821

Ada-SADMMdiag

0.3139 ± 0.0003

0.1280 ± 0.0015

22.4788

0.3793 ± 0.0054

0.1578 ± 0.0059

1.3674

Ada-SADMMf ull

0.3204 ± 0.0007

0.1284 ± 0.0016

148.5242

0.3710 ± 0.0014

0.1550 ± 0.0079

7.0392

Algorithm

svmguide3

w8a

Objective value

Test error rate

Time (s)

Objective value

Test error rate

Time (s)

SADMM

1.6143 ± 0.3123

0.2161 ± 0.0052

0.1288

0.3357 ± 0.0916

0.0957 ± 0.0012

191.7544

Ada-SADMMdiag

0.5163 ± 0.0046

0.2056 ± 0.0060

0.2014

0.1526 ± 0.0010

0.0931 ± 0.0005

326.1392

Ada-SADMMf ull

0.5230 ± 0.0044

0.2000 ± 0.0044

0.4602

0.1469 ± 0.0006

0.0929 ± 0.0003

4027.1963

values of the three algorithms. We can observe that the two adaptive stochastic ADMM algorithms converge much
faster than SADMM, which shows the effectiveness of exploration of adaptive regularization (Bregman Divergence)
to accelerate stochastic ADMM. Secondly, compared with
Ada-SADMMdiag , Ada-SADMMf ull achieves slightly smaller objective values on most of the datasets, which indicates full matrix is slightly more informative than the diagonal one. This should be due to the lower dimensions of
these datasets. Thirdly, the central column provides test error rates of three algorithms, where we observe that the two
adaptive algorithms achieve significantly smaller or comparable test error rates at 0.25-th epoch than SADMM at
2-th epoch. This observation indicates that we can terminate the two adaptive algorithms earlier to save time and
at the same time achieve similar performance compared
with SADMM. Finally, the right column shows the running time of three algorithms, which shows that during the
learning process, the Ada-SADMMf ull is significantly slower while the Ada-SADMMdiag is overall efficient compared with SADMM. In summary, the Ada-SADMMdiag
algorithm achieves a good trade-off between efficiency and
effectiveness.
Table 2 summarizes the performance of all the compared
algorithms over the six datasets, from which we can make
similar observations. This again verifies the effectiveness
of the proposed algorithms.

4. Conclusion
ADMM is a popular technique in machine learning. This
paper studied a method to accelerate stochastic ADMM
with adaptive regularization, by replacing the fixed proximal function with adaptive proximal function. Com-

pared with traditional stochastic ADMM, we show that the
proposed adaptive algorithms converge significantly faster
through the proposed adaptive strategies. Promising experimental results on a variety of real-world datasets further
validate the effectiveness of our techniques.

Acknowledgments
The research of Peilin Zhao and Tong Zhang is partially supported by NSF-IIS-1407939 and NSF-IIS 1250985.
The research of Jinwei Yang and Ping Li is partially supported by NSF-III-1360971, NSF-Bigdata-1419210, ONRN00014-13-1-0764, and AFOSR-FA9550-13-1-0137.

Appendix: Some Proofs
Proof for Lemma 1
Proof. Firstly, using the convexity of ℓ and the definition
of δt , we can obtain
ℓ(wt ) − ℓ(w) ≤ ⟨ℓ′ (wt ), wt − w⟩
= ⟨gt , wt+1 − w⟩ + ⟨δt , w − wt ⟩ + ⟨gt , wt − wt+1 ⟩.

Combining the above inequality with the relation between
θt and θt+1 will derive
ℓ(wt ) − ℓ(w) + ⟨wt+1 − w, −A⊤ θt+1 ⟩
≤ ⟨gt , wt+1 − w⟩ + ⟨δt , w − wt ⟩ + ⟨gt , wt − wt+1 ⟩
+⟨wt+1 − w, A⊤ [β(Awt+1 + Bvt+1 − b) − θt ]⟩
= ⟨gt + A⊤ [β(Awt+1 + Bvt − b) − θt ], wt+1 − w⟩
|
{z
}
Lt
⊤

+ ⟨w − wt+1 , βA B(vt − vt+1 )⟩ +⟨δt , w − wt ⟩
{z
}
|
Mt

+ ⟨gt , wt − wt+1 ⟩ .
|
{z
}
Nt

Adaptive Stochastic Alternating Direction Method of Multipliers

−1

10

0

0.5
1
1.5
Number of Epochs

0

Test Error Rates

Objective Values

10
10

−1

mushrooms

10

−2

0.5
1
1.5
Number of Epochs

0
0

1

news20

0.18

SADMM
Ada − diag
Ada − full

Test Error Rates

Objective Values

10

0

10

−1

10

0

0.5
1
1.5
Number of Epochs

Test Error Rates

Objective Values

10

2

10

SADMM
Ada − diag
Ada − full

1

10

0

10

−1

10

0

0.5
1
1.5
Number of Epochs

Test Error Rates

Objective Values

10

0

10

−1

10

0

svmguide3
0.5
1
1.5
Number of Epochs

2

0

10

−1

10

0

0.5
1
1.5
Number of Epochs

0.2
0.5
1
1.5
Number of Epochs

0.22

2

svmguide3

0.21

Time Costs

0
0

0.5
1
1.5
Number of Epochs

100

2

SADMM
Ada − diag
Ada − full
news20

50
0
0

0.5
1
1.5
Number of Epochs
SADMM
Ada − diag
Ada − full

6

2

splice

4
2
0
0

2

SADMM
Ada − diag
Ada − full

0.23

mushrooms
20

8

SADMM
Ada − diag
Ada − full

2

40

2

0.3

0.5
1
1.5
Number of Epochs

0.6

2

SADMM svmguide3
Ada − diag
Ada − full

0.5
0.4
0.3
0.2
0.1

0.105

SADMM
Ada − diag
Ada − full

Test Error Rates

Objective Values

w8a

0.4

0.2
0

1

10

0.5

0.24

SADMM
Ada − diag
Ada − full

1

splice

0.5
1
1.5
Number of Epochs
SADMM
Ada − diag
Ada − full

60

150

SADMM
Ada − diag
Ada − full

0.5
1
1.5
Number of Epochs

200
0
0

2

0.14

0.1
0

2

2

10

0.16

0.6

splice

3

news20

0.12
0

2

4

10

0.5
1
1.5
Number of Epochs

a9a

SADMM
600 a9aAda − diag
Ada − full
400

80

0.05 mushrooms

2

800

2

SADMM
Ada − diag
Ada − full

0.1

10

0

0.5
1
1.5
Number of Epochs

0.15

SADMM
Ada − diag
Ada − full

1

0.16
0.14
0

2

2

10

0.18

SADMM
Ada − diag
Ada − full

Time Costs

0

10

a9a

Time Costs

10

0.2

Time Costs

1

SADMM
Ada − diag
Ada − full

Time Costs

a9a

Test Error Rates

Objective Values

10

0.1

0.5
1
1.5
Number of Epochs

w8a

4000

SADMM
Ada − diag
Ada − full

0.095
0.09
0

0.5
1
1.5
Number of Epochs

0
0

2

Time Costs

2

2

3000

0.5
1
1.5
Number of Epochs

2

SADMM
Ada − diag
Ada − full

2000
w8a
1000
0
0

0.5
1
1.5
Number of Epochs

2

Figure 1. Comparison between SADMM with Ada-SADMMdiag (“Ada-diag”) and Ada-SADMMf ull (“Ada-full”) on 6 real-world
datasets. Epoch for the horizontal axis is the number of iterations divided by dataset size. Left Panels: Average objective values.
Middle Panels: Average test error rates. Right Panels: Average time costs (in seconds).

Adaptive Stochastic Alternating Direction Method of Multipliers

To provide an upper bound for the first term Lt , taking
D(u, v) = Bϕt (u, v) = 12 ∥u − v∥2Ht and applying Lemma 1 in (Ouyang et al., 2013) to the step of getting wt+1 in
Algorithm 1, we will have
⟨gt + A⊤ [β(Awt+1 + Bvt − b) − θt ], wt+1 − w⟩
1
≤ [Bϕt (wt , w) − Bϕt (wt+1 , w)− Bϕt (wt+1 , wt )].
η

To provide an upper bound for the second term Mt , we can
derive as follows
⟨w − wt+1 , βA⊤ B(vt − vt+1 )⟩
= β⟨Aw − Awt+1 , Bvt − Bvt+1 ⟩
β
= [(∥Aw + Bvt − b∥2 − ∥Aw + Bvt+1 − b∥2 )
2
+(∥Awt+1 + Bvt+1 − b∥2 − ∥Awt+1 + Bvt − b∥2 )]
β
≤ (∥Aw + Bvt − b∥2 − ∥Aw + Bvt+1 − b∥2 )
2
1
+ ∥θt+1 − θt ∥2 .
2β

To drive an upper bound for the final term Nt , we can use
Young’s inequality to get
⟨gt , wt − wt+1 ⟩

≤
=

η∥gt ∥2Ht∗
2
η∥gt ∥2Ht∗
2

+
+

∥wt −

wt+1 ∥2Ht
2η

Bϕt (wt , wt+1 )
.
η

Replacing the terms Lt , Mt and Nt with their upper
bounds, we will get
ℓ(wt ) − ℓ(w) + ⟨wt+1 − w, −A⊤ θt+1 ⟩ ≤
η∥gt ∥2Ht∗
1
[Bϕt (wt , w) − Bϕt wt+1 , w)] +
+ ⟨δt , w − wt ⟩
η
2
β
+ (∥Aw + Bvt − b∥2 − ∥Aw + Bvt+1 − b∥2 )
2
1
+ ∥θt+1 − θt ∥2 .
2β

Due to the optimality condition of the step of updating v
in Algorithm 1, i.e., ∂v Lβ,t (wt+1 , vt+1 , θt ) and the convexity of φ, we have
⊤

φ(vt+1 ) − φ(v) + ⟨vt+1 − v, −B θt+1 ⟩ ≤ 0.

Using the fact Awt+1 + Bvt+1 − b = (θt − θt+1 )/β, we
have
⟨θt+1 − θ, Awt+1 + Bvt+1 − b⟩
1
(∥θ − θt ∥2− ∥θ − θt+1 ∥2− ∥θt+1 − θt ∥2 ).
=
2β

Combining the above three inequalities and re-arranging
the terms will conclude the proof.

Proof of Theorem 2
Proof. We have the following inequality
2

T
∑
[Bϕt (wt , w∗ ) − Bϕt (wt+1 , w∗ )]
t=1

=

T
∑
(∥wt − w∗ ∥2Ht − ∥wt+1 − w∗ ∥2Ht )
t=1

T
−1
∑
≤ ∥w1 − w∗ ∥2H1+ (∥wt+1− w∗ ∥2Ht+1− ∥wt+1 − w∗ ∥2Ht )
t=1

= ∥w1 − w∗ ∥2H1 +

T
−1
∑

∥wt+1 − w∗ ∥2(Ht+1 −Ht )

t=1

≤ ∥w1 − w∗ ∥2H1 +

T
−1
∑

max(wt+1,i − w∗,i )2 ∥st+1 − st ∥1
i

t=1

= ∥w1 − w∗ ∥2H1 +

T
−1
∑

∥wt+1 − w∗ ∥2∞ (st+1 − st )⊤ 1

t=1
2 ⊤
≤ ∥w1 − w∗ ∥2H1+ max ∥wt − w∗ ∥2∞ s⊤
T 1− ∥w1− w∗ ∥∞ s1 1
t≤T

≤ max ∥wt − w∗ ∥2∞
t≤T

d1
∑

∥g1:T,i ∥,

i=1

∑d1
where the last inequality used ⟨sT , 1⟩ =
i=1 ∥g1:T,i ∥
and ∥w1 −w∗ ∥2H1 ≤ ∥w1 −w∗ ∥2∞ s⊤
1 1. Plugging the above
inequality and the inequality (4) into the inequality (2), will
conclude the first part of the theorem. Then the second part
is trivial to be derived.
Proof of Theorem 3
Proof. We consider the sum of the difference
2

T
∑
[Bϕt (wt , w∗ ) − Bϕt (wt+1 , w∗ )]
t=1

=

T
∑
(∥wt − w∗ ∥2Ht − ∥wt+1 − w∗ ∥2Ht )
t=1

≤ ∥w1 − w∗ ∥2H1 +

T
−1
∑

(∥wt+1 − w∗ ∥2Ht+1− ∥wt+1 − w∗ ∥2Ht )

t=1

= ∥w1 − w∗ ∥2H1 +

T
−1
∑

∥wt+1 − w∗ ∥2

≤ ∥w1 − w∗ ∥2H1 +

T
−1
∑

1

1

2 −G 2 )
(Gt+1
t

t=1

1

1

2
∥wt+1 − w∗ ∥2 λmax (Gt+1
− Gt2 )

t=1

= ∥w1 − w∗ ∥2H1 +

T
−1
∑

1

1

2
∥wt+1 − w∗ ∥2 tr(Gt+1
− Gt2 )

t=1

≤ ∥w1 −

w∗ ∥2H1

1

+ max ∥wt − w∗ ∥2 tr(GT2 )
t≤T −1
1

1

−∥w1 − w∗ ∥ tr(G12 ) ≤ max ∥wt − w∗ ∥2 tr(GT2 ).
2

t≤T

Plugging the above inequality and the inequality (4) into
the inequality (2), will conclude the first part of the theorem. Then the second part is trivial to be derived.

Adaptive Stochastic Alternating Direction Method of Multipliers

References
Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statistical learning via the alternating direction method of mulR in Machine Learntipliers. Foundations and Trends⃝
ing, 3(1):1–122, 2011.
Deng, Wei and Yin, Wotao. On the global and linear convergence of the generalized alternating direction method
of multipliers. Technical report, DTIC Document, 2012.
Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.
Eckstein, Jonathan and Bertsekas, Dimitri P. On the douglasrachford splitting method and the proximal point algorithm for maximal monotone operators. Mathematical
Programming, 55(1-3):293–318, 1992.
Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
Sparse inverse covariance estimation with the graphical
lasso. Biostatistics, 9(3):432–441, 2008.
Gabay, Daniel. Chapter ix applications of the method of
multipliers to variational inequalities. Studies in mathematics and its applications, 15:299–331, 1983.
Gabay, Daniel and Mercier, Bertrand. A dual algorithm for
the solution of nonlinear variational problems via finite
element approximation. Computers & Mathematics with
Applications, 2(1):17–40, 1976.
Glowinski, R. and Marroco, A. Sur lapproximation, par elements nis dordre un, et la resolution, par penalisationdualite, dune classe de problems de dirichlet non lineares.
Revue Francaise dAutomatique, Informatique, et, 1975.
Glowinski, Roland and Le Tallec, Patrick. Augmented Lagrangian and operator-splitting methods in nonlinear
mechanics, volume 9. SIAM, 1989.
Goldfarb, Donald, Ma, Shiqian, and Scheinberg, Katya.
Fast alternating linearization methods for minimizing the
sum of two convex functions. Math. Program., 141(1-2):
349–382, 2013.
Goldstein, Tom and Osher, Stanley. The split bregman
method for l1-regularized problems. SIAM Journal on
Imaging Sciences, 2(2):323–343, 2009.
He, Bingsheng and Yuan, Xiaoming. On the o(1/n) convergence rate of the douglas-rachford alternating direction
method. SIAM Journal on Numerical Analysis, 50(2):
700–709, 2012.

Luo, Zhi-Quan. On the linear convergence of the alternating direction method of multipliers. arXiv preprint
arXiv:1208.3922, 2012.
Monteiro, Renato D. C. and Svaiter, Benar Fux. Iterationcomplexity of block-decomposition algorithms and the
alternating direction method of multipliers. SIAM Journal on Optimization, 23(1):475–507, 2013.
Ouyang, Hua, He, Niao, Tran, Long, and Gray, Alexander G. Stochastic alternating direction method of multipliers. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 80–88, 2013.
Suzuki, Taiji. Dual averaging and proximal gradient descent for online alternating direction multiplier method.
In Proceedings of the 30th International Conference on
Machine Learning (ICML-13), pp. 392–400, 2013.
Suzuki, Taiji. Stochastic dual coordinate ascent with alternating direction method of multipliers. In Proceedings
of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pp.
736–744, 2014.
Wang, Huahua and Banerjee, Arindam. Online alternating
direction method. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pp.
1119–1126, 2012.
Yang, Junfeng and Zhang, Yin. Alternating direction algorithms for ℓ1 -problems in compressive sensing. SIAM
journal on scientific computing, 33(1):250–278, 2011.
Zhong, Wenliang and Kwok, James Tin-Yau. Fast stochastic alternating direction method of multipliers. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June
2014, pp. 46–54, 2014.

