Safe Screening with Variational Inequalities and Its Application to Lasso

Jun Liu, Zheng Zhao
SAS Institute Inc., Cary, NC 27513
Jie Wang, Jieping Ye
Arizona State University, Tempe, AZ 85287

Abstract
Sparse learning techniques have been routinely
used for feature selection as the resulting model
usually has a small number of non-zero entries.
Safe screening, which eliminates the features that
are guaranteed to have zero coefficients for a certain value of the regularization parameter, is a
technique for improving the computational efficiency. Safe screening is gaining increasing attention since 1) solving sparse learning formulations usually has a high computational cost especially when the number of features is large and
2) one needs to try several regularization parameters to select a suitable model. In this paper, we
propose an approach called “Sasvi” (Safe screening with variational inequalities). Sasvi makes
use of the variational inequality that provides the
sufficient and necessary optimality condition for
the dual problem. Several existing approaches
for Lasso screening can be casted as relaxed versions of the proposed Sasvi, thus Sasvi provides a
stronger safe screening rule. We further study the
monotone properties of Sasvi for Lasso, based
on which a sure removal regularization parameter can be identified for each feature. Experimental results on both synthetic and real data sets are
reported to demonstrate the effectiveness of the
proposed Sasvi for Lasso screening.

1. Introduction
Sparse learning (Candes & Wakin, 2008; Tibshirani, 1996)
is an effective technique for analyzing high dimensional
data. It has been applied successfully in various areas, such
as machine learning, signal processing, image processing,
medical imaging, and so on. In general, the ℓ1 -regularized
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

{JUN . LIU , ZHENG . ZHAO}@ SAS . COM
{JIE . WANG . USTC , JIEPING . YE}@ ASU . EDU
sparse learning can be formulated as:
min
β

loss(β) + λ∥β∥1 ,

(1)

where β ∈ Rp contains the model coefficients, loss(β) is
a loss function defined on the design matrix X ∈ Rn×p
and the response y ∈ Rn , and λ is a positive regularization parameter that balances the tradeoff between the loss
function and the ℓ1 regularization. Let xi ∈ Rp denote
the i-th sample that corresponds to the transpose of the ith row of X, and let xj ∈ Rn denote the j-th feature that
corresponds to the j-th
∑n column ofTX.i 2We use loss(β) =
1
1
2
i − β x ) in Lasso (Tibshii=1 (y
2 ∥Xβ − y∥2 = 2
∑
n
rani, 1996) and loss(β) = i=1 log(1 + exp(−yi β T xi ))
in sparse logistic regression (Koh et al., 2007).
Since the optimal λ is usually unknown in practical applications, we need to solve formulation (1) corresponding to
a series of regularization parameter λ1 > λ2 > . . . > λk ,
obtain the solutions β1∗ , β2∗ , . . . , βk∗ , and then select the solution that is optimal in terms of a pre-specified criterion,
e.g., Schwarz Bayesian information criterion (Schwarz,
1978) and cross-validation. The well-known LARS approach (Efron et al., 2004) can be modified to obtain the
full piecewise linear Lasso solution path. Other approaches
such as interior point (Koh et al., 2007), coordinate descent (Friedman et al., 2010) and accelerated gradient descent (Nesterov, 2004) usually solve formulation (1) corresponding to a series of pre-defined parameters.
The solutions βk∗ , k = 1, 2, . . . , are sparse in that many
of their coefficients are zero. Taking advantage of the nature of sparsity, the screening techniques have been proposed for accelerating the computation. Specifically, given
a solution β1∗ at the regularization parameter λ1 , if we can
identify the features that are guaranteed to have zero coefficients in β2∗ at the regularization parameter λ2 , then the
cost for computing β2∗ can be saved by excluding those
inactive features. There are two categories of screening
techniques: 1) the safe screening techniques (Ghaoui et al.,
2012; Wang et al., 2013; Ogawa et al., 2013; Zhen et al.,
2011) with which our obtained solution is exactly the same
as the one obtained by directly solving (1), and 2) the
heuristic rule such as the strong rules (Tibshirani et al.,

Safe Screening with Variational Inequalities

2012) which can eliminate more features but might mistakenly discard active features.
In this paper, we propose an approach called “Sasvi” (Safe
screening with variational inequalities) and take Lasso as
an example in the analysis. Sasvi makes use of the variational inequality which provides the sufficient and necessary optimality condition for the dual problem. Several
existing approaches such as SAFE (Ghaoui et al., 2012)
and DPP (Wang et al., 2013) can be casted as relaxed versions of the proposed Sasvi, thus Sasvi provides a stronger
screening rule. The monotone properties of Sasvi for Lasso
are studied based on which a sure removal regularization
parameter can be identified for each feature. Empirical results on both synthetic and real data sets demonstrate the effectiveness of the proposed Sasvi for Lasso screening. Extension of the proposed Sasvi to the generalized sparse linear models such as logistic regression is briefly discussed.
Notations Throughout this paper, scalars are denoted by
italic letters, and vectors by bold face letters. Let ∥ · ∥1 ,
∥ · ∥2 , ∥ · ∥∞ denote the ℓ1 norm, the Euclidean norm, and
the infinity norm, respectively. Let ⟨x, y⟩ denote the inner
product between x and y.

2. The Proposed Sasvi
Our proposed approach builds upon an analysis on the following simple problem:
min {−βb + |β|} .
β

(2)

We have the following results:
1) If |b| ≤ 1, then the minimum of (2) is 0;
2) If |b| > 1, then the minimum of (2) is −∞; and
3) If |b| < 1, then the optimal solution β ∗ = 0.
The dual problem usually can provide a good insight about
the problem to be solved. Let θ denote the dual variable of
Eq. (1). In light of Eq. (2), we can show that βj∗ , the j-th
component of the optimal solution to Eq. (1), optimizes
min {−βj ⟨xj , θ ∗ ⟩ + |βj |} ,
βj

(4)

Let λ1 and λ2 be two distinct regularization parameters that
satisfy
λmax ≥ λ1 > λ2 > 0,
(5)
This is used in deriving the last equality of Eq. (6).

The construction of a tight feasible set for θ2∗ is key to the
success of the screening technique. If the constructed feasible set is too loose, the estimated upper-bound of |⟨xj , θ2∗ ⟩|
is over 1, and thus only a few features can be discarded.
In this paper, we propose to construct the feasible set by
using the variational inequalities that provide the sufficient
and necessary optimality conditions for the dual problems
with λ = λ1 and λ2 . Then, we estimate the upper-bound
of |⟨xj , θ2∗ ⟩| in the constructed feasbile set, and discard the
j-th feature if the upper-bound is smaller than 1. For discussion convenience, we focus on Lasso in this paper, but
the underlying methodology can be extended to the general
problem in Eq. (1). Next, we elaborate the three building
blocks that are illustrated in the bottom row of Figure 1.
2.1. The Dual Problem of Lasso

Eq. (4) says that, the j-th feature can be safely eliminated
in the computation of β ∗ if |⟨xj , θ ∗ ⟩| < 1.

1

where λmax denotes the value of λ above which the solution to Eq. (1) is zero. Let β1∗ and β2∗ be the optimal primal
variables corresponding to λ1 and λ2 , respectively. Let θ1∗
and θ2∗ be the optimal dual variables corresponding to λ1
and λ2 , respectively. Figure 1 illustrates the work flow of
the proposed Sasvi. We firstly derive the dual problem of
Eq. (1). Suppose that we have obtained the primal and dual
solutions β1∗ and θ1∗ for a given regularization parameter
λ1 , and we are interested in solving Eq. (1) with λ = λ2 by
using Eq. (4) to screen the features to save computational
cost. However, the difficulty lies in that, we do not have
the dual optimal θ2∗ . To deal with this, we construct a feasible set for θ2∗ , estimate an upper-bound of |⟨xj , θ2∗ ⟩|, and
safely remove xj if this upper-bound is smaller than 1.

(3)

where xj denotes the j-th feature and θ ∗ denotes the optimal dual variable of Eq. (1). From the results to Eq. (2), we
need |⟨xj , θ ∗ ⟩| ≤ 1 to ensure that Eq. (3) does not equal to
−∞1 , and we have
|⟨xj , θ ∗ ⟩| < 1 ⇒ βj∗ = 0.

Figure 1. The work flow of the proposed Sasvi. The purpose is to
discard the features that can be safely eliminated in computing β2∗
with the information obtained at λ1 .

We follow the discussion in Section 6 of (Nesterov, 2013)
in deriving the dual problem of Lasso as follows:
[
]
1
2
min
∥Xβ − y∥2 + λ∥β∥1
β
2
[
]
1
2
= min max ⟨y − Xβ, λθ⟩ − ∥λθ∥2 + λ∥β∥1
β
θ
2
]
[
λ∥θ∥22
T
− ⟨X θ, β⟩ + ∥β∥1
= max min λ ⟨y, θ⟩ −
θ
β
2
[
  ]
1
y

2 1  y 2
2
=
max
λ − θ −  +   .
2
λ 2 2 λ 2
θ:∥X T θ∥∞ ≤1
(6)

Safe Screening with Variational Inequalities

A dual variable θ is introduced in the first equality, and the
equivalence can be verified by setting the derivative with
regard to θ to zero, which leads to the following relationship between the optimal primal variable (β ∗ ) and the optimal dual variable (θ ∗ ):
λθ ∗ = y − Xβ ∗ .

(7)

In obtaining the last equality of Eq. (6), we make use of the
results to Eq. (2).
The dual problem of Eq. (1) can be formulated as:
1
y

2
min
−  .
θ
T
λ 2
θ:∥X θ∥∞ ≤1 2

(8)

For Lasso, the λmax in Eq. (5) can be analytically computed
as λmax = ∥X T y∥∞ . In applying Sasvi, we might start
with λ1 = λmax , since the primal and dual optimals can be
y
computed analytically as: β1∗ = 0 and θ1∗ = λmax
.
2.2. Feasible Set Construction
Given λ1 , θ1∗ and λ2 , we aim at estimating the upper-bound
of |⟨xj , θ2∗ ⟩| without the actual computation of θ2∗ . To this
end, we construct a feasible set for θ2∗ , and then estimate
the upper-bound in the constructed feasible set. To construct the feasible set, we make use of the variational inequality that provides the sufficient and necessary condition of a constrained convex optimization problem.

⟨

θ2∗

y
− , θ1∗ − θ2∗
λ2

⟩
≥ 0.

(14)

With Eq. (13) and Eq. (14), we can construct the following
feasible set for θ2∗ as:
Ω(θ2∗ ) = {θ : ⟨θ1∗ −

y
y
, θ−θ1∗ ⟩ ≥ 0, ⟨θ− , θ1∗ −θ⟩ ≥ 0}.
λ1
λ2
(15)

For an illustration of the feasible set, please refer to Figure 2. Generally speaking, the closer λ2 is to λ1 , the tighter
the feasible set for θ2∗ is. In fact, when λ2 approaches to
λ1 , Ω(θ2∗ ) concentrates to a singleton set that only contains
θ2∗ . Note that one may use additional θ’s in Eq. (12) for
improving the estimation of the feasible set of θ2∗ . Next,
we discuss how to make use of the feasible set defined in
Eq. (15) for estimating an upper-bound for |⟨xj , θ2∗ ⟩|.
2.3. Upper-bound Estimation
Since θ2∗ ∈ Ω(θ2∗ ), we can estimate an upper-bound of
|⟨xj , θ2∗ ⟩| by solving
max |⟨xj , θ⟩|.

θ∈Ω(θ2∗ )

(16)

Next, we show how to solve Eq. (16). For discussion convenience, we introduce the following three variables:
y
Xβ1∗
− θ1∗ =
,
λ1
λ1
y
y
y
b=
− θ1∗ = a + ( − ),
λ2
λ2
λ1
y
r = 2θ − (θ1∗ + ),
λ2
a=

Lemma 1 (Nesterov, 2004) For the constrained convex optimization problem:
min f (x),
x∈G

(9)

with G being convex and closed and f (·) being convex and
differentiable, x∗ ∈ G is an optimal solution of Eq. (9) if
and only if
⟨f ′ (x∗ ), x − x∗ ⟩ ≥ 0, ∀x ∈ G.

(10)

Eq. (10) is the so-called variation inequality for the problem
in Eq. (9). Applying Lemma 1 to the Lasso dual problem
in Eq. (8), we can represent the optimality conditions for
θ1∗ and θ2∗ using the following two variational inequalities:
⟩
⟨
y
θ1∗ − , θ − θ1∗ ≥ 0, ∀θ : ∥X T θ∥∞ ≤ 1, (11)
λ1
⟨
⟩
y
∗
∗
θ2 − , θ − θ2 ≥ 0, ∀θ : ∥X T θ∥∞ ≤ 1. (12)
λ2
Plugging θ = θ2∗ and θ = θ1∗ into Eq. (11) and Eq. (12)
respectively, we have
⟨
⟩
y
θ1∗ − , θ2∗ − θ1∗ ≥ 0,
(13)
λ1

(17)

where a denotes the prediction based on β1∗ scaled by λ11 ,
and b is the summation of a and the change of the inputs
to the dual problem in Eq. (8) from λ1 to λ2 .
Figure 2 illustrates a and b by lines EB and EC, respectively. For the triangle EBC, the following theorem shows
that the angle between a and b is acute.
Theorem 1 Let y ̸= 0, and ∥X T y∥∞ ≥ λ1 > λ2 > 0.
We have
b ̸= 0, ⟨b, a⟩ ≥ 0,
(18)
and ⟨b, a⟩ = 0 if and only if λ1 = ∥X T y∥∞ . In addition,
if λ1 < ∥X T y∥∞ , then a ̸= 0.
The proof of Theorem 1 is given in Supplement A. With
the notations in Eq. (17), Eq. (16) can be rewritten as

⟨
⟩

1 
y
∗
max
+ ⟨xj , r⟩
xj , θ1 +

r
2
λ2
(19)
2
2
subject to ⟨a, r + b⟩ ≤ 0, ∥r∥2 ≤ ∥b∥2 .

Safe Screening with Variational Inequalities

Theorem 2 Let 0 < λ1 ≤ ∥X T y∥∞ , 0 < λ2 < λ1 , x ̸= 0
and y ̸= 0. Eq. (20) equals to −∥x∥2 ∥b∥2 , if ⟨b,a⟩
∥b∥2 ≤
√
2
⟨x,a⟩
⟨a,b⟩⟨x,a⟩
⟨b,a⟩
⊥
2
otherwise.
∥x∥2 , and −∥x ∥2 ∥b∥2 − ∥a∥2 −
∥a∥2
2

2

The proof of Theorem 2 is given in Supplement B. With
Theorem 2, we can obtain the upper-bound of |⟨xj , θ2∗ ⟩| in
the following theorem.
Theorem 3 Let y ̸= 0, and ∥X T y∥∞ ≥ λ1 > λ2 > 0.
Denote
u+
(24)
j (λ2 ) = max∗ ⟨xj , θ⟩,
θ∈Ω(θ2 )

u−
j (λ2 )

= max∗ ⟨−xj , θ⟩.

(25)

θ∈Ω(θ2 )

We have:
Figure 2. Illustration of the feasible set used in Sasvi and Theorem 3. The points in the figure are explained as follows. E: θ1∗ , B:
y
,
λ1

∗
θ1
+ y

λ2
C: λy2 , D:
. The left hand side of the dash line repre2
sents the half space {θ : ⟨θ1∗ − λy1 , θ−θ1∗ ⟩ ≥ 0}, and the ball centered at D with radius ED represents {θ : ⟨θ − λy2 , θ1∗ − θ⟩ ≥ 0}.
For Theorem 3, EX1 , EX2 , EX3 and EX4 denote ±xj in two subcases: 1) the angle between EB and EX1 (EX4 ) is larger than the
angle between EB and EC, and 2) the angle between EB and EX2
(EX3 ) is smaller than the angle between EB and EC. R2 (R3 ) is
the maximizer to Eq. (16) with EX2 (EX3 ) denoting ±xj . With
EX1 (EX4 ) denoting ±xj , the maximizer to Eq. (16) is on the intersection between the dashed line and the ball centered at D with
radius ED.

The objective function of Eq. (19) can be represented by
half of the following form:
(
)
y
y
max ⟨xj , θ1∗ + ⟩ + ⟨xj , r⟩, −⟨xj , θ1∗ + ⟩ − ⟨xj , r⟩
λ2
λ2
which indicates that Eq. (19) can be computed by maximizing ⟨xj , r⟩ and −⟨xj , r⟩ over the feasible set in the same
equation. Maximizing ⟨xj , r⟩ and −⟨xj , r⟩ can be computed by minimizing ⟨−xj , r⟩ and ⟨xj , r⟩, which can be
solved by the following minimization problem:
min
r

subject to

⟨x, r⟩
⟨a, r + b⟩ ≤ 0, ∥r∥22 ≤ ∥b∥22 .

(20)

We assume that x is a non-zero vector. Let
⊥

x =x−

a⟨x, a⟩/∥a∥22 ,

1) If a ̸= 0 and

⟨b,a⟩
∥b∥2 ∥a∥2

>
−

|⟨xj ,a⟩|
∥xj ∥2 ∥a∥2

then

[ ⊥
]
⊥
∥xj ∥2 ∥y⊥ ∥2 + ⟨x⊥
j ,y ⟩ ,
2
(26)
1
1
−
[
]
λ2
λ1
⊥
⊥
⊥
∗
u−
∥x⊥
j ∥2 ∥y ∥2 − ⟨xj , y ⟩ .
j (λ2 ) = −⟨xj , θ1 ⟩+
2
(27)
1

λ2
∗
u+
j (λ2 ) = ⟨xj , θ1 ⟩+

1
λ1

⟨xj ,a⟩
∥xj ∥2 ∥a∥2 ,

⟨b,a⟩
≤
2) If ⟨xj , a⟩ > 0 and ∥b∥
2 ∥a∥2
satisfies Eq. (26), and
∗
u−
j (λ2 ) = −⟨xj , θ1 ⟩ +

3) If ⟨xj , a⟩ < 0 and

1
[∥xj ∥2 ∥b∥2 − ⟨xj , b⟩] . (28)
2

⟨b,a⟩
∥b∥2 ∥a∥2

∗
u+
j (λ2 ) = ⟨xj , θ1 ⟩ +

then u+
j (λ2 )

≤

−⟨xj ,a⟩
∥xj ∥2 ∥a∥2 ,

then

1
[∥xj ∥2 ∥b∥2 + ⟨xj , b⟩] .
2

(29)

and u−
j (λ2 ) satisfies Eq. (27).
4) If a = 0, then Eq. (28) and Eq. (29) hold.
The proof of Theorem 3 is given in Supplement C. An illustration of Theorem 3 for different cases can be found in
Figure 2. It follows from Eq. (4) that, if u+
j (λ2 ) < 1 and
u−
(λ
)
<
1,
then
the
j-th
feature
can
be
safely
eliminated
2
j
for the computation of β2∗ . We provide the following analysis to the established upper-bound. Firstly, we have
−
∗
∗
lim u+
j (λ2 ) = ⟨xj , θ1 ⟩, lim uj (λ2 ) = −⟨xj , θ1 ⟩,

(21)

2
x⊥
j = xj − a⟨xj , a⟩/∥a∥2 ,

(22)

y⊥ = y − a⟨y, a⟩/∥a∥22 ,

(23)

which are the orthogonal projections of x, xj , and y onto
the null space of a, respectively. Our next theorem says
that Eq. (20) admits a closed form solution.

λ2 →λ1

λ2 →λ1

which attributes to the fact that limλ2 →λ1 Ω(θ2∗ ) = {θ1∗ }.
Secondly, in the extreme case that xj is orthogonal to the
Xβ ∗
scaled prediction a = λ11 which is nonzero, Theorem 3
+
−
∗
leads to x⊥
j = 0, uj (λ2 ) = ⟨xj , θ1 ⟩ and uj (λ2 ) =
∗
−⟨xj , θ1 ⟩. Thus, the j-th feature can be safely removed
for any positive λ2 that is smaller than λ1 so long as

Safe Screening with Variational Inequalities

|⟨xj , θ1∗ ⟩| < 1. Thirdly, in the case that xj has low corXβ ∗
relation with the prediction a = λ11 , Theorem 3 indicates
that the j-th feature is very likely to be safely removed for
a wide range of λ2 if |⟨xj , θ1∗ ⟩| < 1. The monotone properties of the upper-bound established in Theorem 3 is given
Section 4.

3. Comparison with Existing Approaches
Our proposed Sasvi differs from the existing screening
techniques (Ghaoui et al., 2012; Tibshirani et al., 2012;
Wang et al., 2013; Zhen et al., 2011) in the construction
of the feasible set for θ2∗ .
3.1. Comparison with the Strong Rule
The strong rule (Tibshirani et al., 2012) works on 0 < λ2 <
λ1 and makes use of the assumption
|λ2 ⟨xj , θ2∗ ⟩ − λ1 ⟨xj , θ1∗ ⟩| ≤ |λ2 − λ1 |,

(30)

from which we can obtain an estimated upper-bound for
|⟨xj , θ2∗ ⟩| as:
|λ1 ⟨xj , θ1∗ ⟩| + |λ2 ⟨xj , θ2∗ ⟩ − λ1 ⟨xj , θ1∗ ⟩|
λ2
|λ1 ⟨xj , θ1∗ ⟩| + (λ1 − λ2 )
≤
λ2
[
]
λ1
λ1
=
|⟨xj , θ1∗ ⟩| +
−1
λ2
λ2
(31)
A comparison between Eq. (31) and the upper-bound established in Theorem 3 shows that, 1) both are dependent
on ⟨xj , θ1∗ ⟩, the inner product between the j-th feature and
the dual variable θ1∗ obtained at λ1 , but note that λλ12 > 1,
2) in comparison with the data independent term λλ12 − 1
used in the strong rule, Sasvi utilizes a data dependent term
as shown in Eqs. (26)-(29). We note that, 1) when a feaXβ ∗
ture xj has low correlation with the prediction a = λ11 ,
the upper-bound for |⟨xj , θ2∗ ⟩| estimated by Sasvi might be
lower than the one by the strong rule 2 , and 2) as pointed
out in (Tibshirani et al., 2012), Eq. (30) might not always
hold, and the same applies to Eq. (31).
|⟨xj , θ2∗ ⟩| ≤

Next, we compare Sasvi with the SAFE approach (Ghaoui
et al., 2012) and the DPP approach (Wang et al., 2013), and
the differences in terms of the feasible sets are shown in
Figure 3.
2

According to the analysis given at the end of Section 2.3, this
argument is true for the extreme case that xj is orthogonal to the
Xβ ∗
nonzero prediction a = λ11 .

Figure 3. Comparison of Sasvi with existing safe screening apy
proaches. The points in the figure are as follows. A: λmax
, B: λy1 ,
y
∗
∗
C: λ2 , D: the middle point of C and E, E: θ1 , F: θ2 , and G: −θ1∗ .
The feasible set for θ2∗ used by the proposed Sasvi approach is the
intersection between the ball centered at D with radius being half
EC and the closed half space passing through E and containing the
constraint of the dual of Lasso. The feasible set for θ2∗ used by
the SAFE (Ghaoui et al., 2012) approach is the ball centered at C
with radius being the smallest distance from C to the points in the
line segment EG. The feasible set for θ2∗ used by the DPP (Wang
et al., 2013) approach is the ball centered at E with radius BC.

3.2. Comparison with the SAFE approach
Denote G(θ) = 21 ∥y||22 − 12 ∥λ2 θ − y||22 . The SAFE approach makes use of the so-called “dual” scaling, and compute the upper-bound of the G(θ) for λ2 as
1
1
∥y||22 − ∥sλ2 θ1∗ −y||22 ,
2
s:|s|≤1
s:|s|≤1 2
(32)
Note that, compared to the SAFE paper, the dual variable
θ has been scaled in the formulation in Eq. (32), but this
scaling does not influence of the following result for the
SAFE approach. Denote s∗ as the
) Solv)
( optimal
( solution.
⟨θ1∗ ,y⟩
∗
ing Eq. (32), we have s = max min λ2 ∥θ∗ ∥2 , 1 , −1
1
when θ1 ̸= 0. The SAFE approach computes the upperbound for |⟨xj , θ2∗ ⟩| as follows:

γ(λ2 ) = max G(sθ) = max

|⟨xj , θ2∗ ⟩| ≤
=

max
θ:G(θ)≥γ(λ2 )

|⟨xj , θ⟩|

max

θ:∥θ− λy ||2 ≤∥s∗ θ1∗ − λy ||2

|⟨xj , θ⟩|



 ∗ ∗
|⟨xj , y⟩|
y

=
+ ∥xj ∥2 
s
θ
−
 1 λ2  .
λ2
2
2

2

(33)

Next, we show that the feasible set for θ2∗ used in Eq. (33)
can be derived from the variational inequality in Eq. (12)
followed by relaxations.

Safe Screening with Variational Inequalities

Utilizing ∥X T θ1∗ ∥∞ ≤ 1 and |s∗ | ≤ 1, we can set θ =
s∗ θ1∗ in Eq. (12) and obtain
⟩
⟨
y
θ2∗ − , s∗ θ1∗ − θ2∗ ≥ 0,
λ2

(34)

which leads to
⟩ ⟨
⟩
⟨
y ∗
y
y ∗ ∗
y
∗
∗
θ2 − , θ2 −
− θ2 − , s θ1 −
λ2
λ2
λ2
λ2
⟨
⟩
(35)
y
y
y
∗
∗
∗ ∗
= θ2 − , θ2 −
+
− s θ1 ≤ 0.
λ2
λ2
λ2
Since
 

⟨
⟩ 
 ∗ ∗
 ∗
y
y ∗ ∗
y
y
∗



,
θ2 − , s θ1 −
≤ θ2 −  s θ1 − 
λ2
λ2
λ2 2
λ2 2
(36)
we have







 ∗
θ2 − y  ≤ s∗ θ1∗ − y  ,
(37)



λ2 2
λ2 2
which is the feasible set used in Eq. (33). Note that, the ball
defined by Eq. (37) has higher volume than the one defined
by Eq. (34) due to the relaxation used in Eq. (36), and it
can be shown that the ball defined by Eq. (34) lies within
the ball defined by Eq. (37).
3.3. Comparison with the DPP approach
The feasible set for θ2∗ used in the DPP approach is


y

 − y  ≥ ∥θ2∗ − θ1∗ ∥ ,
2
 λ2
λ 1 2

and
⟨

y
y
− , θ2∗ − θ1∗
λ2
λ1

⟩

In this subsection, we study the monotone properties of the
upper-bound established in Theorem 3 with regard to the
regularization parameter λ2 . With such study, we can identify the feature sure removal parameter—the smallest value
of λ above which a feature is guaranteed to have zero coefficient and thus can be safely removed.
Without loss of generality, we assume ⟨xj , a⟩ ≥ 0 and the
results can be easily extended to the case ⟨xj , a⟩ < 0. In
addition, we assume that if λ1 ̸= ∥X T y∥∞ then θ1∗ ̸=
y
. This is a valid assumption for real data.
∥X T y∥∞
Let y ̸= 0, and λmax = ∥X T y∥∞ ≥ λ1 ≥ λ > 03 . We
introduce the following two auxiliary functions:
f (λ) =

⟨ yλ − θ1∗ , a⟩
∥ yλ − θ1∗ ∥2

(41)

g(λ) =

⟨ yλ − θ1∗ , y⟩
∥ yλ − θ1∗ ∥2

(42)

We show in Supplement D that f (λ) is strictly increasing
with regard to λ in (0, λ1 ] and g(λ) is strictly decreasing
with regard to λ in (0, λ1 ]. Such monotone properties,
which are illustrated geometrically in the first plot of Fig⟨x ,a⟩
⟨x ,y⟩
ure 4, guarantee that f (λ) = ∥xjj ∥2 and g(λ) = ∥xjj ∥2
have unique roots with regard to λ when some conditions
are satisfied.
Our main results are summarized in the following theorem:

(38)

which can be obtained by
⟨

4. Feature Sure Removal Parameter

Theorem 4 Let y ̸= 0 and ∥X T y∥∞ ≥ λ1 > λ2 > 0.
Let ⟨xj , a⟩ ≥ 0. Assume that if λ1 ̸= ∥X T y∥∞ then θ1∗ ̸=
y
.
∥X T y∥∞
⟨x ,a⟩

≥ ⟨θ2∗ − θ1∗ , θ2∗ − θ1∗ ⟩.

(39)


⟩ 
y
y
y ∗
y
∗

− , θ2 − θ1 ≤  − 
∥θ ∗ −θ1∗ ∥2 , (40)
λ2
λ1
λ2
λ1 2 2

where Eq. (39) is a result of adding Eq. (13) and Eq. (14).
Therefore, although the authors in (Wang et al., 2013) motivates the DPP approach from the viewpoint of Euclidean
projection, the DPP approach can indeed be treated as generating the feasible set for θ2∗ using the variational inequality in Eq. (11) and Eq. (12) followed by relaxation
in Eq. (40). Note that, the ball specified by Eq. (38) has
higher volume than the one specified by Eq. (39) due to the
relaxation used in Eq. (40), and it can be shown that the ball
defined by Eq. (39) lies within the ball defined by Eq. (38).

j
Define λ2,a as follows: If ⟨y,a⟩
∥y∥2 ≥ ∥xj ∥2 , then let λ2,a =
0; otherwise, let λ2,a be the unique value in (0, λ1 ] that
⟨x ,a⟩
satisfies f (λ2,a ) = ∥xjj ∥2 .

Define λ2,y as follows: If a = 0 or if a ̸= 0 and
⟨xj ,y⟩
∥xj ∥2 ,

⟨a,y⟩
∥a∥2

≥

then let λ2,y = λ1 ; otherwise, let λ2,y be the unique

value in (0, λ1 ] that satisfies g(λ2,y ) =

⟨xj ,y⟩
∥xj ∥2 .

We have the following monotone properties:
1. u+
j (λ2 ) is monotonically decreasing with regard to λ2
in (0, λ1 ].
2. If λ2,a ≤ λ2,y , then u−
j (λ2 ) is monotonically decreasing with regard to λ2 in (0, λ1 ].
If λ1 ≥ λmax , we have β1∗ = 0 and thus we focus on λ1 ≤
λmax . In addition, for given λ1 , we are interested in the screening
for a smaller regularization parameter, i.e., λ < λ1 .
3

1

1

0.8

0.8

0.6
0.4
SAFE
DPP
Strong Rule
Sasvi

0.2
0

Rejection Ratio

3. If λ2,a > λ2,y , then u−
j (λ2 ) is monotonically decreasing with regard to λ2 in (0, λ2,y ) and (λ2,a , λ1 ),
but monotonically increasing with regard to λ2 in
[λ2,y , λ2,a ].

Rejection Ratio

Safe Screening with Variational Inequalities

0.6
0.4

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
λ/λmax

0.8

0.8

0.4
SAFE
DPP
Strong Rule
Sasvi

0.2
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
λ/λmax

(Synthetic, p̄ = 100)

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
λ/λmax

(Real: PIE)
1

Rejection Ratio

Rejection Ratio

(Real: MNIST)
1

0.6

SAFE
DPP
Strong Rule
Sasvi

0.2

0.6
0.4
SAFE
DPP
Strong Rule
Sasvi

0.2
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
λ/λmax

(Synthetic, p̄ = 1000)

1

Rejection Ratio

0.8
0.6
0.4
SAFE
DPP
Strong Rule
Sasvi

0.2

Figure 4. Illustration of the monotone properties of Sasvi for
Lasso with the assumption ⟨xj , a⟩ ≥ 0. The first plot geometrically shows the monotone properties of f (λ) and g(λ), respectively. The last three plots correspond to the three cases in Theorem 4. For illustration convenience, the x-axis denotes λ12 rather
than λ2 .

The proof of Theorem 4 is given in Supplement D. Note
that, λ2,a and λ2,y are dependent on the index j, which
is omitted for discussion convenience. Figure 4 illustrates
results presented in Theorem 4. The first two cases of Theorem 4 indicate that, if the j-th feature xj can be safely
removed for a regularization parameter λ = λ2 , then it can
also be safely discarded for any regularization parameter λ
larger than λ2 . However, the third case in Theorem 4 says
that this is not always true. This somehow coincides with
the characteristic of Lasso that, a feature that is inactive
for a regularization parameter λ = λ2 might become active for a larger regularization parameter λ > λ2 . In other
words, when following the Lasso solution path with a decreasing regularization parameter, a feature that enters into
the model might get removed.
By using Theorem 4, we can easily identify for each feature
a sure removable parameter λs that satisfies u+
j (λ) < 1
4 asand u−
(λ)
<
1,
∀λ
>
λ
.
Note
that
Theorem
s
j
sumes ⟨xj , a⟩ ≥ 0, but it can be easily extended to the
case ⟨xj , a⟩ < 0 by replacing xj with −xj .

5. Experiments
In this section, we conduct experiments to evaluate the performance of the proposed Sasvi in comparison with the sequential SAFE rule (Ghaoui et al., 2012), the sequential
strong rule (Tibshirani et al., 2012), and the sequential DPP

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
λ/λmax

(Synthetic, p̄ = 5000)
Figure 5. The rejectioin ratios—the ratios of the number features
screened out by SAFE, DPP, the strong rule and Sasvi on synthetic
and real data sets.

(Wang et al., 2013). Note that, SAFE, Sasvi and DPP methods are “safe” in the sense that the discarded features are
guaranteed to have 0 coefficients in the true solution, and
the strong rule—which is a heuristic rule—might make error and such error was corrected by a KKT condition check
as suggested in (Tibshirani et al., 2012).
Synthetic Data Set We follow (Bondell & Reich, 2008;
Zou & Hastie, 2005; Tibshirani, 1996) in simulating the
data as follows:
y = Xβ ∗ + σϵ,

ϵ ∼ N (0, 1),

(43)

where X has 250 × 10000 entries. Similar to (Bondell &
Reich, 2008; Zou & Hastie, 2005; Tibshirani, 1996), we
set the pairwise correlation between the i-th feature and
the j-th feature to 0.5|i−j| and draw X from a Gaussian
distribution. In constructing the ground truth β ∗ , we set
the number of non-zero components to p̄ and randomly assign the values from a uniform [−1, 1] distribution. We set
σ = 0.1 and generate the response vector y ∈ R250 using
Eq. (43). For the value of p̄, we try 100, 1000, and 5000.
PIE Face Image Data Set The PIE face image data set
used in this experiment 4 contains 11554 gray face images
4
http://www.cad.zju.edu.cn/home/dengcai/
Data/FaceData.html

Safe Screening with Variational Inequalities

Method
solver
SAFE
DPP
Strong
Sasvi

Synthetic with p̄
100
1000
5000
88.55 101.00 101.55
73.37 88.42
90.21
44.00 49.57
50.15
2.53
3.00
2.92
2.49
2.77
2.76

Real
MINST
PIE
2683.57 617.85
651.23 128.54
328.47
79.84
5.57
2.97
5.02
1.90

Table 1. Running time (in seconds) for solving the Lasso problems along a sequence of 100 tuning parameter values equally
spaced on the scale of λ/λmax from 0.05 to 1 by the solver (Liu
et al., 2009) without screening, and the solver combined with different screening methods.

of 68 people, taken under different poses, illumination conditions and expressions. Each of the images has 32 × 32
pixels. To use the regression model in Eq. (43), we first
randomly pick up an image as the response y ∈ R1024 ,
and then set the remaining images as the data matrix X ∈
R1024×11553 .
MNIST Handwritten Digit Data Set This data set contains grey images of scanned handwritten digits, including
60, 000 for training and 10, 000 for testing. The dimension
of each image is 28 × 28. To use the regression model
in Eq. (43), we first randomly select 5000 images for each
digit from the training set (and in total we have 50000 images) and get a data matrix X ∈ R784×50000 , and then we
randomly select an image from the testing set and treat it as
the response vector y ∈ R784 .
Experimental Settings For the Lasso solver, we make use
of the SLEP package (Liu et al., 2009). For a given generated data set (X and y), we run the solver with or without screening rules to solve the Lasso problems along a
sequence of 100 parameter values equally spaced on the
λ/λmax scale from 0.05 to 1.0. The reported results are
averaged over 100 trials of randomly drawn X and y.
Results Table 1 reports the running time by different
screening rules, and Figure 5 presents the corresponding
rejection ratios—the ratios of the number features screened
out by the screening approaches. It can be observed that the
propose Sasvi significantly outperforms the safe screening
rules such as SAFE and DPP. The reason is that, Sasvi is
able to discard more inactive features as discussed in Section 3. In addition, the rejection ratios of the strong rule
and Sasvi are comparable, and both of them are more effective in discarding inactive features than SAFE and DPP.
In terms of the speedup, Sasvi provides better performance
than the strong rule. The reason is that the strong rule is a
heuristic screening method, i.e., it may mistakenly discard
active features which have nonzero components in the solution, and thus the strong rule needs to check the KKT
conditions to make correction if necessary to ensure the
correctness of the result. In contrast, Sasvi does not need

to check the KKT conditions or make correction since the
discarded features are guaranteed to be absent from the resulting sparse representation.

6. Conclusion and Discussion
The safe screening is a technique for improving the computational efficiency by eliminating the inactive features
in sparse learning algorithms. In this paper, we propose
a novel approach called Sasvi (Safe screening with variational inequalities). The proposed Sasvi has three modules: dual problem derivation, feasible set construction,
and upper-bound estimation. The key contribution of the
proposed Sasvi is the usage of the variational inequality
which provides the sufficient and necessary optimality conditions for the dual problem. Several existing approaches
can be casted as relaxed versions of the proposed Sasvi, and
thus Sasvi provides a stronger screening rule. The monotone properties of the established upper-bound are studied
based on a sure removal regularization parameter which
can be identified for each feature.
The proposed Sasvi can be extended to solve the generalized sparse linear models, by filling in Figure 1 with the
three key modules. For example, the sparse logistic regression can be written as
min
β

n
∑

log(1 + exp(−yi β T xi )) + λ∥β∥1 .

(44)

i=1

We can derive its dual problem as
)
( yi )
n (
yi
∑
− θi
θi
+ yi log( λ
) .
min
−
log yi λ
θi
θ:∥X T θ∥∞ ≤1
λ − θi
λ
i=1
According to Lemma 1, for the dual optimal θi∗ , the optimality condition via the variational inequality is
( yi
n
∗)
∑
1
λ − θi
(θi − θi∗ ) ≤ 0, ∀θ : ∥X T θ∥∞ ≤ 1.
log
yi
∗
θ
i
i=1 λ
Then, we can construct the feasible set for θ2∗ at the regularization parameter λ2 in a similar way to the Ω(θ2∗ )
in Eq. (15). Finally, we can estimate the upper-bound of
|⟨xj , θ2∗ ⟩| by Eq. (16), and discard the j-th feature if such
upper-bound is smaller than 1. Note that, compared to the
Lasso case, Eq. (16) is much more challenging for the logistic loss case. We plan to replace the feasible set Ω(θ2∗ )
by its quadratic approximation so that Eq. (16) has an easy
solution. We also plan to apply the proposed Sasvi to solving the Lasso solution path using LARS.

Acknowledgments
This work was supported in part by NSFC (61035003),
NIH (LM010730) and NSF (IIS-0953662, CCF-1025177).

Safe Screening with Variational Inequalities

References
Bondell, H. and Reich, B. Simultaneous regression shrinkage, variable selection and clustering of predictors with
OSCAR. Biometrics, 64:115–123, 2008.
Candes, E. and Wakin, M. An introduction to compressive
sampling. IEEE Signal Processing Magazine, 25:21–30,
2008.
Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. Least
angle regression. Annals of Statistics, 32:407–499, 2004.
Friedman, J. H., Hastie, T., and Tibshirani, R. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1):1–22,
2010.
Ghaoui, L., Viallon, V., and Rabbani, T. Safe feature elimination in sparse supervised learning. Pacific Journal of
Optimization, 8:667–698, 2012.
Koh, K., Kim, S., and Boyd, S. An interior-point method
for large-scale l1-regularized logistic regression. Journal
of Machine Learning Research, 8:1519–1555, 2007.
Liu, J., Ji, S., and Ye, J. SLEP: Sparse Learning with
Efficient Projections. Arizona State University, 2009.
URL http://www.public.asu.edu/∼jye02/
Software/SLEP.
Nesterov, Y. Introductory lectures on convex optimization :
a basic course. Applied optimization. Kluwer Academic
Publ., 2004.
Nesterov, Y. Gradient methods for minimizing composite objective function. Mathematical Programming, 140:
125–161, 2013.
Ogawa, K., Suzuki, Y., and Takeuchi, I. Safe screening of
non-support vectors in pathwise SVM computation. In
International Conference on Machine Learning, 2013.
Schwarz, G. estimating the dimension of a model. Annals
of Statistics, 6:461–464, 1978.
Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society, Series B,
58:267–288, 1996.
Tibshirani, R., Bien, J., Friedman, J. H., Hastie, T., Simon,
N., Taylor, J., and Tibshirani, R. J. Strong rules for discarding predictors in lasso-type problems. Journal of the
Royal Statistical Society: Series B, 74:245–266, 2012.
Wang, J., Lin, B., Gong, P., Wonka, P., and Ye, J. Lasso
screening rules via dual polytope projection. In Advances in Neural Information Processing Systems, 2013.

Zhen, J. X., Hao, X., and Peter, J. R. Learning sparse representations of high dimensional data on large scale dictionaries. In Advances in Neural Information Processing
Systems, 2011.
Zou, H. and Hastie, T. Regularization and variable selection via the elastic net. Journal of the Royal Statistical
Society Series B, 67:301–320, 2005.

