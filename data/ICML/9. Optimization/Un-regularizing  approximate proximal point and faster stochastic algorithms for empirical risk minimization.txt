Un-regularizing: approximate proximal point and faster stochastic algorithms
for empirical risk minimization

Roy Frostig
Stanford University

RF @ CS . STANFORD . EDU

Rong Ge
Sham M. Kakade
Microsoft Research, New England

RONGGE @ MICROSOFT. COM
SKAKADE @ MICROSOFT. COM

Aaron Sidford
MIT

SIDFORD @ MIT. EDU

Abstract

This problem underlies supervised learning (e.g. the training of logistic regressors when œÜi (z) = log(1 + e‚àízbi )
and their regularized form when œÜi (z) = log(1 + e‚àízbi ) +
Œ≥
2
2n kxk2 ) and captures the widely-studied problem of linear
least-squares regression when œÜi (z) = 21 (z ‚àí bi )2 .

We develop a family of accelerated stochastic algorithms that optimize sums of convex functions.
Our algorithms improve upon the fastest running
time for empirical risk minimization (ERM),
and in particular linear least-squares regression,
across a wide range of problem settings.

Over the past five years, problems such as (1) have received
increased attention, with a recent burst of activity in the
design of fast randomized algorithms. Iterative methods
that randomly sample the œÜi have been shown to outperform standard first-order methods under mild assumptions
(Bottou & Bousquet, 2008; Johnson & Zhang, 2013; Xiao
& Zhang, 2014; Defazio et al., 2014; Shalev-Shwartz &
Zhang, 2014).

To achieve this, we establish a framework, based
on the classical proximal point algorithm, useful for accelerating recent fast stochastic algorithms in a black-box fashion. Empirically, we
demonstrate that the resulting algorithms exhibit
notions of stability that are advantageous in practice. Both in theory and in practice, the provided algorithms reap the computational benefits
of adding a large strongly convex regularization
term, without incurring a corresponding bias to
the original ERM problem.

Despite the breadth of these recent results, their use in solving the ERM problem (1) lead to sub-optimal dependence
on a natural notion of the problem‚Äôs condition number.
This dependence does, however, significantly impact the
guarantees on running time, as high-dimensional problems
encountered in practice are often poorly conditioned (due
to strong correlation structure among variables). In particular, among the recent randomized algorithms, each either:

1. Introduction
A general optimization problem central to machine learning is that of empirical risk minimization (ERM): finding
a predictor or regressor that minimizes a sum of loss functions defined by a data sample. In this paper, we focus on
empirical risk minimization of linear predictors: given a set
of n data points ai , . . . , an ‚àà Rd and convex loss functions
œÜi : R ‚Üí R for i = 1, . . . , n, solve
min F (x),

x‚ààRn

where

def

F (x) =

Pn

i=1

œÜi (aT
i x).

(1)

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

‚Ä¢ Solves the ERM problem (1), under an assumption of
strong convexity, with convergence that depends linearly on the problem‚Äôs condition number (Johnson &
Zhang, 2013; Defazio et al., 2014).
‚Ä¢ Solves only an explicitly regularized ERM problem, minx {F (x) + Œªr(x)} where the regularizer r
is a known strongly convex function and Œª must be
strictly positive, even when F is itself strongly convex. The first such result is due to Shalev-Shwartz &
Zhang (2014) and achieves acceleration, i.e. dependence only on the square root of the regularized problem‚Äôs condition number, which scales inversely with
Œª. Hence, taking small Œª to solve the ERM problem

Un-regularizing: approximate proximal point algorithms for empirical risk minimization

(where Œª = 0 in effect) is not a viable option.
In this paper we show how to bridge this gap by a blackbox reduction: solving the ERM problem (1), under an assumption of strong convexity, through repeated, approximate minimizations of the form minx {F (x) + Œªr(x)} for
fairly large Œª.
The key to our reduction is an approximate variant of
the classical proximal point algorithm (PPA) (Rockafellar,
1976; Parikh & Boyd, 2014). Both PPA and the inner minimization procedure can then be accelerated and our analysis gives precise approximation requirements for either option. We show further practical improvements when the
inner minimizer operates by a dual ascent method.
Table 1 summarizes our improvements in comparison to
existing minimization procedures. Our end result is a family of algorithms, including:
1. An approximate proximal point algorithm (APPA),
and a dual variant thereof, that matches the previous state-of-the-art running time and exhibits desirable empirical properties (Sections 2 and 5).
2. An accelerated variant of APPA that enjoys a running
time with a square root dependence on the ERM problem‚Äôs condition number (Theorem 2.6).
All analysis is performed in the common setting where each
œÜi is smooth and the sum F is strongly convex (e.g. as in
overdetermined linear regression).
Several of the algorithmic tools in this paper are similar
in principle to (and sometimes appear indirectly in) work
scattered throughout the machine learning and optimization
literature ‚Äì from classical treatments of error-tolerant PPA
(Rockafellar, 1976; GuÃàler, 1992) to the effective proximal
term used by Shalev-Shwartz & Zhang (2014) in enabling
their acceleration. By analyzing these as separate tools, and
by bookkeeping the error requirements that they impose,
we are able to assemble them into algorithms achieving an
improved runtime.
Our techniques also extend naturally to the following more
general optimization problem, which is fundamental in the
theory of convex optimization,
min

x‚ààRd

Pn

i=1

œài (x),

where

œài : Rd ‚Üí R,

(2)

and which covers ERM problems for multiclass and structured prediction. To simplify exposition and comparison to
related work we focus on (1); extensions to (2) are made
apparent in Section 2.
For the remainder of the introduction, we present the
formal setup and cover previous approaches and running
times. Section 2 provides the reduction framework and algorithms, then states the running time guarantees provided

Empirical risk minimization
Algorithm
Running time
GD
dn2 Œ∫ log(0 /)
‚àö
Accel. GD
dn3/2 Œ∫ log(0 /)
SAG, SVRG
dnŒ∫ log(0 /)
0
SDCA
dnŒ∫
‚àö log(0 /)
Acc. SDCA, APCG
dn ‚àöŒ∫0 log(0 /)
This work
dn Œ∫ log(0 /)

Problem
F
F
F
F + Œªr
F + Œªr
F

Linear least-squares regression
Algorithm
Running time
Problem
œâ‚àí1
Pseudoinverse
nd
kAx ‚àí bk22
œâ
Row sampling
(nd + d ) log(0 /) kAx ‚àí bk22
Rand. Kaczmarz
dnŒ∫
Ax = b
‚àö log(0 /)
Accel. coord.
dn‚àöŒ∫ log(0 /)
Ax = b
This work
dn Œ∫ log(0 /)
kAx ‚àí bk22
Table 1. Theoretical performance comparison on ERM and linear
regression. Running times hold in expectation for randomized
algorithms. In the ‚Äúproblem‚Äù column for ERM, F marks algorithms that can optimize the ERM objective (1), while F + Œªr
marks those that only solve the explicitly regularized problem.
For linear regression, Ax = b marks algorithms that only solve
consistent linear systems, whereas kAx ‚àí bk22 marks those that
more generally minimize the squared loss. The constant œâ is the
exponent of the matrix multiplication running time (currently below 2.373 (Williams, 2012)).

by the framework. Section 3 provides analysis for the simplest among the algorithms (the other two are analyzed in
the appendix). Finally, Section 4 discusses implementation
concerns, and Section 5 concludes with an empirical analysis. Most proofs are deferred to the appendix.
1.1. Formal setup
Consider ERM (1) in the following common setting:
Assumption 1.1 (Regularity). Each loss function œÜi is Lsmooth, i.e. for all x, y ‚àà R,
œÜ(y) ‚â§ œÜ(x) + œÜ0 (x)(y ‚àí x) +

L
2 (y

‚àí x)2 ,

and the sum F is ¬µ-strongly convex, i.e. for all x, y ‚àà Rd ,
F (x) ‚â• F (x) + ‚àáF (x)T (y ‚àí x) + ¬µ2 ky ‚àí xk22 .
def

Let R = maxi kai k2 , A ‚àà Rn√ód be the matrix whose i‚Äôth
2
row is aT
i , and Œ∫ = dLR /¬µe denote the condition number.
Although many algorithms are designed for special cases
of the ERM objective F where there is some known, exploitable structure to the problem, our aim is to study the
most general case subject to Assumption 1.1. To standardize the comparison among algorithms, we consider the following generic model of interaction with F :

Un-regularizing: approximate proximal point algorithms for empirical risk minimization

Assumption 1.2 (Computational model). For any i ‚àà [n]
and x ‚àà Rd , two possible primitive operations are:
‚Ä¢ For b ‚àà R, compute the gradient of x 7‚Üí œÜi (aT
i x ‚àí b).
‚Ä¢ For b ‚àà R, c ‚àà Rd , minimize œÜi (aT
x)
+
bkx
‚àí
ck22 .
i
We refer to these operations, as well as to the evaluation
of œÜi (aT
i x), as single accesses to œÜi , and assume that these
operations can be computed in O(d) time.
def

Notation Denote [n] = {1, . . . , n}. Denote the optimal
value of a convex function by f opt = minx f (x), and, when
f is clear from context, let xopt denote a minimizer. A point
x0 is an -approximate minimizer of f if f (x0 ) ‚àí f opt ‚â§ .
The Fenchel dual of a convex function f : Rk ‚Üí R is f ‚àó :
Rk ‚Üí R defined by f ‚àó (y) = supx‚ààRk {hy, xi ‚àí f (x)}.
1.2. Running times and related work
Table 1 compares our results with the running time of both
classical and recent algorithms for solving the ERM problem (1) and linear least-squares regression.
In the more general context of the ERM problem, GD refers
to canonical gradient descent on F , Accel. GD is Nesterov‚Äôs accelerated gradient decent (Nesterov, 1983; 2004),
SVRG is the stochastic variance-reduced gradient of Johnson & Zhang (2013), SAG is the stochastic average gradient of Roux et al. (2012) and Defazio et al. (2014), SDCA
is the stochastic dual coordinate ascent of Shalev-Shwartz
& Zhang (2013), Acc. SDCA is the accelerated proximal
SDCA of Shalev-Shwartz & Zhang (2014) and APCG is
the accelerated coordinate algorithm of Lin et al. (2014).
The latter three algorithms are more restrictive in that they
only solve the explicitly regularized problem F + Œªr, even
if F is itself strongly convex (such algorithms run in time
inversely proportional to Œª).

and algorithms based on subspace embedding or row sampling (Nelson & Nguyen, 2013; Li et al., 2013; Cohen et al.,
2015). Some Kaczmarz-based methods can only solve the
more restrictive problem of consistent systems (finding x
satisfying Ax = b) rather than minimize the squared loss
kAx ‚àí bk22 . The running times depend on the same four parameters n, d, Œ∫, 0 / as before, except for computing the
closed-form pseudoinverse, which for simplicity we consider ‚Äúexact,‚Äù independent of initial and target errors 0 /.
Condition numbers in machine learning The condition
number of the ERM problem (1) captures notions of data
complexity such as variable correlation. Machine learning problems are typically high-dimensional with highly
correlated variables, which implies large Œ∫. On the other
hand these problems often need not be optimized to a precision far below the statistical noise level of O(1/n), so
log(0 /) = O(log n). Hence, in the statistically interesting regime, condition number dependence often comprises
the main concern in runtime bounds.

2. Main results
This section describes our framework for iteratively applying and accelerating certain minimization algorithms.
When instantiated with recent fast algorithms we obtain,
under Assumptions 1.1 and 1.2, algorithms
guaranteed to
‚àö
solve the ERM problem in time OÃÉ(nd Œ∫ log(1/)).
Our framework stems from a critical insight of the classical proximal point algorithm (PPA) or proximal iteration:
to minimize F (or more generally, any convex function) it
suffices to iteratively minimize
fs,Œª (x) = F (x) + Œª2 kx ‚àí sk22

(3)

The running time of the algorithms are presented based
on the setting considered in this paper, i.e. under Assumptions 1.1 and 1.2. Many of the algorithms can be applied
in more general settings (e.g. even if the function F is not
strongly convex) and have different convergence guarantees in those cases. The running times are characterized by
four parameters: d is the data dimension, n is the number
of samples, Œ∫ = dLR2 /¬µe is the condition number (for
F + Œªr the condition number Œ∫0 = dLR2 /Œªe is used) and
0 / is the ratio between the initial and desired accuracy.
Running times are stated per OÃÉ-notation; factors that depend poly-logarithmically on n, Œ∫, and Œ∫0 are ignored.

for Œª > 0 and proper choice of center s ‚àà Rd . PPA iteratively applies the update rule x(t+1) ‚Üê argminx fx(t) ,Œª (x)
and converges to the minimizer of F . The minimization
in the update is known as the proximal operator (Parikh &
Boyd, 2014), and we refer to it in the sequel as the inner
minimization problem.

For the linear least-squares regression problem, there is
greater variety in the algorithms that apply. For comparison we include the use of a Moore-Penrose pseudoinverse
to compute a solution in closed form via the standard normal equations, algorithms based on the randomized Kaczmarz method (Strohmer & Vershynin, 2009; Needell et al.,
2014) and their accelerated variant (Lee & Sidford, 2013),

‚Ä¢ Section 2.1 provides APPA: an algorithm most similar
to PPA, but requiring inner minimization by only a
fixed multiplicative constant in each iteration.

We establish three distinct types of approximate proximal
point algorithms, i.e. algorithms that do not require full inner minimization. Each enables the use of a different existing fast algorithm as its inner minimizer, in turn yielding
several ways to obtain our improved ERM running time:

‚Ä¢ Section 2.2 provides Accelerated APPA: an accelerated version of APPA. Its instantiation with SVRG
(Johnson & Zhang, 2013) as an inner minimizer

Un-regularizing: approximate proximal point algorithms for empirical risk minimization

Algorithm 1 Approximate PPA (APPA)
(0)

d

input x ‚àà R , Œª > 0
, Œª)-oracle P
input primal ( 2(Œª+¬µ)
¬µ
for t = 1, . . . , T do
x(t) ‚Üê P(x(t‚àí1) )
end for
output x(T )

achieves the accelerated runtime for the general ERM
problem (2).
‚Ä¢ Section 2.3 provides Dual APPA: an algorithm whose
approximate inner minimizers operate on the dual
fs,Œª , with warm starts between iterations. Dual APPA
enables several inner minimizers that are otherwise incompatible with APPA. Its instantiation with APCG
(Lin et al., 2014) or with Accelerated Proximal SDCA
(Shalev-Shwartz & Zhang, 2014) as an inner minimizer achieves the accelerated runtime for the ERM
problem (1).
2.1. An approximate proximal point algorithm
To design APPA, we quantify the error that can be tolerated
of an inner minimizer, while accounting for the computational cost of ensuring such error. The abstraction we use
is the following notion of inner approximation:
Definition 2.1. An algorithm P is a primal (c, Œª)-oracle if,
opt
given x ‚àà Rd , it outputs P(x) that is a ([fx,Œª (x)‚àífx,Œª
]/c)1
approximate minimizer of fx,Œª in time TP .
In other words, a primal oracle is an algorithm initialized
at x that reduces the error of fx,Œª by a 1/c fraction, in time
that depends on Œª, and c, and regularity properties of F .
Typical iterative first-order algorithms, such as those in Table 1, yield primal (c, ‚àö
Œª)-oracles with runtimes TP that
scale inversely in Œª or Œª, and logarithmically in c. For
instance:
Theorem 2.2 (SVRG (Johnson & Zhang, 2013)). SVRG
is a primal (c, Œª)-oracle with runtime complexity TP =
LR2
O(ndd ¬µ+Œª
e log c).
Note, moreover, that SVRG is a primal oracle even for the
general ERM problem (2).
APPA (Algorithm 1) takes any primal oracle and queries it
repeatedly. We prove the following lemma to guarantee a
geometric convergence rate for the iterates produced in this
manner (proved in Section 3).
Lemma 2.3 (Contraction in APPA). Fix any c0 ‚àà (0, 1),
0
x ‚àà Rd , and primal ( Œª+¬µ
c0 ¬µ , Œª)-oracle P. If x = P(x),
1
When the oracle is a randomized algorithm, we require that
its outputs is, in expectation, -approximate in the same way.

Algorithm 2 Accelerated APPA
input x(0) ‚àà Rd , ¬µ > 0, Œª > 2¬µ
input primal (4œÅ3/2 , Œª)-oracle P, where œÅ =
Define Œ∂ = ¬µ2 + Œª1
v (0) ‚Üê x(0)
for t = 0, . . . , T ‚àí 1 do
œÅ‚àí1/2
(t)
y (t) ‚Üê 1+œÅ1‚àí1/2 x(t) + 1+œÅ
‚àí1/2 v

¬µ+2Œª
¬µ

x(t+1) ‚Üê P(x(t) )
g (t) ‚Üê Œª(y (t) ‚àí x(t+1) )


v (t+1) ‚Üê (1 ‚àí œÅ‚àí1/2 )v (t) + œÅ‚àí1/2 y (t) ‚àí Œ∂g (t)
end for
output x(T )

then2
F (x0 ) ‚àí F opt ‚â§

Œª + c0 ¬µ
(F (x) ‚àí F opt ) .
Œª+¬µ

(4)

Lemma 2.3 in turn implies the following runtime bound for
APPA.
Theorem 2.4 (Un-regularizing in APPA). Given a primal
( 2(¬µ+Œª)
, Œª)-oracle P, Algorithm 1 minimizes the general
¬µ
ERM problem (2) to within accuracy  in time


0
¬µ+Œª
O TP
log
¬µ

Corollary 2.5. Instantiating Theorem 2.4 with SVRG
(Johnson & Zhang, 2013) as the primal oracle and
taking Œª = ¬µ yields the running time bound
O(ndŒ∫ log( 2(¬µ+Œª)
) log(0 /)) for the general ERM prob¬µ
lem (2).
2.2. Accelerated APPA
We show how to generically accelerate APPA by developing Algorithm 2, Accelerated APPA. In a sense, it uses inner minimizers more efficiently, but requires a more precise constant minimization factor. We prove a contraction
lemma, analogous to Lemma 2.3, as part of a complete
analysis of Accelerated APPA in Appendix D. As before, it
implies the following runtime bound.
Theorem 2.6 
(Un-regularizing in Accelerated APPA).
3/2
Given a primal 4( 2Œª+¬µ
, Œª -oracle P for Œª ‚â• 2¬µ, Al¬µ )
gorithm 2 minimizes the general ERM problem (2) to within
accuracy  in time
s
!
¬µ+Œª
0
O TP
log
.
¬µ

2
When the oracle is a randomized algorithm, the guarantee (4)
holds in expectation over x0 , conditioned on x.

Un-regularizing: approximate proximal point algorithms for empirical risk minimization

Algorithm 3 Dual APPA
input x(0) ‚àà Rd , Œª > 0
input dual (œÉ, Œª)-oracle D
y (0) ‚Üê yÃÇ(x(0) )
for t = 1, . . . , T do
y (t) ‚Üê D(x(t‚àí1) , y (t‚àí1) )
x(t) ‚Üê xÃÇx(t‚àí1) ,Œª (y (t) )
end for
output x(T )

(see Theorem 2.10 for œÉ)

Definition 2.8. An algorithm D is a dual (c, Œª)-oracle if,
given s ‚àà Rd and y ‚àà Rn , it outputs D(s, y) that is a
opt
([gs,Œª (y) ‚àí gs,Œª
]/c)-approximate minimizer of gs,Œª in time
1
TD .
For instance, it is immediate from its analysis that SDCA
is a valid dual oracle:

Corollary 2.7. Instantiating Theorem 2.6 with SVRG
(Johnson & Zhang, 2013) as the primal oracle and taking Œª‚àö = 2¬µ + LR2 yields the running time bound
OÃÉ(nd Œ∫ log(0 /)) for the general ERM problem (2).
2.3. Dual APPA
Several oracles that are well-suited inner minimizers ‚Äì for
F as the ERM objective (1) in particular ‚Äì operate in the
regularized ERM dual. That is, fs,Œª is minimized by instead decreasing the negative dual objective gs,Œª : Rn ‚Üí R
given by
gs,Œª (y) = G(y) +

provides a significant speedup in practice (Section 5). Altogether, we formalize this procedure as Dual APPA (Algorithm 3), which uses a dual oracle:

1
kAT yk22 ‚àí sT AT y,
2Œª

(5)

Pn

‚àó
where G(y) =
i=1 œÜi (yi ). To make corresponding
progress in the primal, dual-based algorithms make use of
the dual-to-primal mapping, given by

xÃÇs,Œª (y) = s ‚àí Œª1 AT y,

(6)

and the primal-to-dual mapping, given entrywise by


‚àÇœÜi (z) 
[yÃÇ(x)]i =
 T
‚àÇz
z=a x

(7)

i

for i = 1, . . . , n (Appendix A elaborates on duality.) Such
dual algorithms include SDCA and Accelerated Proximal
SDCA (Shalev-Shwartz & Zhang, 2013; 2014).
The theoretical guarantees of dual-based algorithms can
usually be augmented so that they fit Definition 2.1 of a
primal oracle P(x), under the scheme where x determines
yÃÇ(x) as the initial dual point. However, such an initialization scheme is not typically considered in these algorithms‚Äô
analyses, so some further custom-tailored proof is needed
to ensure the oracle assumption is valid in this case.
Far more importantly, such an initialization scheme has an
O(nd) runtime overhead per APPA iteration. A more natural scheme is to operate contiguously in the dual, or in other
words to ‚Äúwarm start,‚Äù initializing by the dual point y from
the end of the previous oracle invocation. This scheme incurs only O(d) overhead (see Section 4) and we find that it

Theorem 2.9 (SDCA (Shalev-Shwartz & Zhang, 2013)).
SDCA is a dual (c, Œª)-oracle with runtime complexity
2
TD = OÃÉ(ndd LR
Œª e log c).
By repeatedly querying a dual oracle ‚Äì producing along the
way primal iterates via the dual-to-primal mapping (6) ‚Äì
we obtain Dual APPA, with the following runtime bound:
Theorem 2.10 (Un-regularizing in Dual APPA). Given
a dual (œÉ, Œª)-oracle D, Algorithm 3 minimizes the ERM
problem (1) to within accuracy  in time


0
1
log
,
OÃÉ TD
1‚àír

where œÉ = O(poly(n, Œ∫)) and r < 1 is a positive scalar
depending on Œª/(Œª + ¬µ).
The proof of Theorem 2.10 is given in Appendix C, including the precise definition of the numerical constants r and
œÉ in (19) and (20).
An acceleration scheme alternative to that of Section 2.2,
which obtains the same overall running time as Accelerated APPA (Algorithm 2), is to run Dual APPA using an
accelerated procedure for its inner dual oracle:
Corollary 2.11. Instantiating Theorem 2.10 with
(Lin et al., 2014) or with Accelerated Proximal
(Shalev-Shwartz & Zhang, 2014) as the dual
and taking
Œª = LR2 yields the running time
‚àö
OÃÉ(nd Œ∫ log(0 /)).

APCG
SDCA
oracle
bound

2.4. Discussion
Notions of error-tolerance in (primal) PPA ‚Äì for both its
plain and accelerated variants ‚Äì have been defined and studied in prior work (Rockafellar, 1976; GuÃàler, 1992). These
mainly consider the cumulative absolute error of a given
sequence of minimizers of fx(t) ,Œª , assuming that such a
sequence is somehow provided. Such a view falls short
of providing a complete and instantiable study of runtime
complexity: how, and in what time, can we produce such
minimizers? Most any procedure of interest begins at some
initial point, and has runtime that depends on the relative error ratio between its start and end. Definitions 2.1

Un-regularizing: approximate proximal point algorithms for empirical risk minimization

and 2.8 capture such procedures succinctly and hence Algorithms 1, 3, and 2 fully specify families of concrete
algorithms. Analytically, the main challenge for proving runtime guarantees lies in proving contractions, as in
Lemma 2.3 and Theorems 2.6 and 2.10, namely in showing
that a constant reduction of relative error in inner problems
suffices.
As a separate note, we believe that the presentation of Accelerated APPA simplifies, and clarifies in terms of broader
convex optimization theory, the ‚Äúouter loop‚Äù steps used
in Accelerated Proximal SDCA to enable its acceleration
(Shalev-Shwartz & Zhang, 2014).

3. Convergence analysis of APPA
In this section we present a proof of Lemma 2.3, which in
turn implies the geometric convergence of APPA iterates
and hence the overall runtime in Theorem 2.4. The analogous proofs for Dual APPA and Accelerated APPA are
significantly more complex and deferred to the appendix.
However, the simple analysis carried out in this section
demonstrates the conceptual core of many of the tools involved, and in particular explains the fixed relative error
reduction needed in APPA, as discussed in Section 2.
Note that, as is always the case with APPA (but not with
Dual APPA), the function F in this section can represent
any ¬µ-strongly convex function. In particular, it need not
represent the ERM objective as it does in other sections.
First, consider exact inner minimizers. The following relates the minimum of the sub-problem fs,Œª to F opt .
Lemma 3.1 (Relationship between minima). For s ‚àà Rd
and Œª ‚â• 0
opt
fs,Œª
‚àí F opt ‚â§

Œª
(F (s) ‚àí F opt ) .
¬µ+Œª

Proof. Let xopt = argminx F (x) and for all Œ± ‚àà [0, 1]
let xŒ± = (1 ‚àí Œ±)s + Œ±xopt . The ¬µ-strong convexity of F
implies that, for all Œ± ‚àà [0, 1],

This immediately implies contraction for the exact PPA,
as it implies that in every iteration of PPA the error in F
decreases by a multiplicative Œª/(Œª + ¬µ). Here we show
that it also implies contraction for approximate minimizers. Namely, we use the lemma above to relate approximate
error reduction in fs,Œª to contraction in F :
Lemma 3.2 (Error reduction implies contraction). For any
x, s ‚àà Rd , Œª > 0, and c > 0, if
opt
fs,Œª (x) ‚àí fs,Œª
‚â§


1
opt
fs,Œª (s) ‚àí fs,Œª
,
c

(8)

then
F (x) ‚àí F

opt


‚â§

1
Œª
+
c ¬µ+Œª



(F (s) ‚àí F opt )

Proof. Note that fs,Œª (x) ‚â• F (x) and by the same reasonopt
ing fs,Œª
‚â• F opt . By definition, fs,Œª (s) = F (s), so rearranging (8) and subtracting F opt from both sides implies
F (x) ‚àí F opt ‚â§


  opt
1
F (s) ‚àí F opt + fs,Œª
‚àí F opt .
c

Invoking Lemma 3.1 then yields the result.
This inequality now essentially proves Lemma 2.3:
Proof of Lemma 2.3. Suppose we have c0 ‚àà (0, 1) and x ‚àà
Rd , and that x0 = P(x) where P is a primal ( Œª+¬µ
c0 ¬µ , Œª)oracle. By definition
opt
fx,Œª (x0 ) ‚àí fx,Œª
‚â§


c0 ¬µ 
opt
fx,Œª (x) ‚àí fx,Œª
.
Œª+¬µ

Applying Lemma 3.2 proves the claim.
Remark 3.3. The proofs in this section did not require that
F be smooth, or even differentiable.

4. Practical concerns

While theoretical convergence rates lay out a broad-view
comparison of the algorithms in the literature, we briefly
remark on some of the finer-grained differences between
Œ±(1 ‚àí Œ±)¬µ
ks ‚àí xopt k22 . algorithms, which inform their implementation or empiriF (xŒ± ) ‚â§ (1 ‚àí Œ±)F (s) + Œ±F (xopt ) ‚àí
2
cal behavior. To match the terminology used for SVRG in
opt
Johnson & Zhang (2013), we refer to a ‚Äústage‚Äù as a single
Consequently, by the definition of fs,Œª
,
step of APPA, i.e. the time spent executing the inner minimization of fx(t) ,Œª or gx(t) ,Œª (as in (3) and (5)).
Œª
opt
fs,Œª
‚â§ F (xŒ± ) + kxŒ± ‚àí sk22
2
Re-centering overhead of Dual APPA vs. SVRG
At
‚â§ (1 ‚àí Œ±)F (s) + Œ±F (xopt )
the end of every one of its stages, SVRG pauses to compute
an exact gradient by a complete pass over the dataset (costŒ±(1 ‚àí Œ±)¬µ
ŒªŒ±2
‚àí
ks ‚àí xopt k22 +
ks ‚àí xopt k22
ing Œò(nd) time during which n gradients are computed).
2
2
Although an amortized runtime analysis hides this cost, this
¬µ
Choosing Œ± = ¬µ+Œª
yields the result.
operation cannot be carried out in-step with the iterative

Un-regularizing: approximate proximal point algorithms for empirical risk minimization

updates of the previous stage, since the exact gradient is
computed at a point that is only selected at the stage‚Äôs end.
Meanwhile, if each stage in Dual APPA is initialized with
a valid primal-dual pair for the inner problem, Dual APPA
can update the current primal point together with every dual
coordinate update, in time O(d), i.e. with negligible increase in the overhead of the update. When doing so, the
corresponding data row remains fresh in cache and, unlike
SVRG, no additional gradient need be computed.

sdca(0.1)

svrg(1.0)

appa(1.0)

1e-1
5e-2
2e-2
1e-2
5e-3

Stable update steps
Dual coordinate-wise ascent provides a convenient framework in which to derive parameter updates with data-dependent step sizes, or sometimes
closed-form updates altogether (i.e. optimal solutions to
each single-coordinate maximization sub-problem), which
reduces the number of parameters needing tuning and can
thereby improve stability overall.

5. Empirical analysis
We experiment with Dual APPA in comparison with
SDCA, SVRG, and SGD on several binary classification
tasks.
Beyond general benchmarking, the experiments also
demonstrate the advantages of the unordinary ‚Äúbiasvariance tradeoff‚Äù presented by approximate proximal iteration: the vanishing proximal term empirically provides
advantages of regularization (added strong convexity, lower
variance) at a bias cost that is less severe than with typical
`2 regularization. Even if some amount of `2 shrinkage is
desired, Dual APPA can place yet higher weight on its `2
term, enjoy improved speed and stability, and after a few

ls(0.1)

ls(1.0)
0.22

5e-2

2e-3
1e-3
5e-4
0

5

(a)

10

2e-2
0
20

15

0.02
5

10

15

20

# gradients / n
# gradients / n
MNIST. Left: excess train loss F (x) ‚àí F opt . Right: test error rate.

sdca(0.1)

svrg(1.0)

appa(1.0)

sgd(1.0)

ls(0)

ls(0.1)

0.06

ls(1.0)
0.28

2e-2

2e-1

1e-2
5e-3
0.11
0

5

(b)

10

15

20

0

5

10

15

20

# gradients / n
# gradients / n
CIFAR. Left: excess train loss F (x) ‚àí F opt . Right: test error rate.

sdca(0.1)

svrg(0.01)

appa(0.1)

2e-3

Decreasing Œª
APPA and Dual APPA enjoy the nice
property that, as long as the inner problems are solved
with enough accuracy, the algorithm does not diverge even
for large choice of Œª. In practice this allows us to start
with a large Œª and make faster inner minimizations. If we
heuristically observe that the function error is not decreasing rapidly enough, we can switch to a smaller Œª. Figure 3
(Section 5) demonstrates this empirically. On the contrary,
algorithms like SGD or SVRG are more sensitive to the
choice of Œª and can suddenly diverge when it is taken too
large. This makes them less amenable to (mid-run) dynamic parameter tuning.

ls(0)

1e-1

5e-2

Moreover, initializing each stage with a valid such primaldual pair can be done in only O(d) time. At the end of
a stage where s was the center point, Dual APPA holds
a primal-dual pair (x, y) where x = xÃÇs (y). The next
stage is centered at x and the dual variables initialized at
y, so it remains to set up a corresponding primal point
x0 = xÃÇx (y) = x ‚àí Œª1 AT y. This can be done by computing
x0 ‚Üê 2x ‚àí s, since we know that x ‚àí s = ‚àí Œª1 AT y.

sgd(1.0)
2e-1

0.1

sgd(1.0)

ls(0)

ls(0.1)

0.002

ls(0.1)
0.009

1e-3
5e-4
2e-4
1e-4
5e-5
0

(c)

5

10

15

5e-3
20
0

5

10

15

0.005
20

# gradients / n
# gradients / n
Protein. Left: excess train loss F (x) ‚àí F opt . Right: test error rate.

Figure 1. Sub-optimality curves when optimizing under squared
1
loss œÜi (z) = 2n
(z ‚àí bi )2 .

stages achieve roughly the desired bias.
Datasets In this section we show results for three binary
classification tasks, derived from MNIST,3 CIFAR-10,4
and Protein:5 in MNIST we classify the digits {1, 2, 4, 5, 7}
vs. the rest, and in CIFAR we classify the animal categories
vs. the automotive ones. MNIST and CIFAR are taken
under non-linear feature transformations that increase the
problem scale significantly: we normalize the rows by scaling the data matrix by the inverse average `2 row norm. We
then take take n/5 random Fourier features per the randomized scheme of Rahimi & Recht (2007). This yields
12K features for MNIST (60K training examples, 10K test)
and 10K for CIFAR (50K training examples, 10K test).
Meanwhile, Protein is a standard pre-featurized benchmark
(75 features, ‚àº117K training examples, ‚àº30K test) that
we preprocess minimally by row normalization and an appended affine feature, and whose train/test split we obtain
by randomly holding out 20% of the original labeled data.
Algorithms Each algorithm is parameterized by a scalar
value Œª analogous to the Œª used in proximal iteration: Œª
3

http://yann.lecun.com/exdb/mnist/
http://www.cs.toronto.edu/‚àºkriz/cifar.html
5
http://osmot.cs.cornell.edu/kddcup/datasets.html
4

Un-regularizing: approximate proximal point algorithms for empirical risk minimization

0.15

appa(0.1)

sdca

sgd(1000.0)

5e-2

0.05

final value

1e-1
5e-2

2e-2
1e-2

0.01
0

5

10

15

2e-2
0
20

10

15

2e-1
1e-1
5e-2

svrg(10.0)

appa(0.1)

sdca

sgd(1000.0)

final value
2e-1
0.17
5

10

15

0

5

10

10‚àí210‚àí1100 101 102 103 104 105 106 107 108

15

svrg

appa

20

# gradients / n
# gradients / n
(b) CIFAR. Left: train loss F (x). Right: test error rate.

Figure 2. Objective curves when optimizing under logistic loss
œÜi (z) = n1 log(1 + e‚àízbi ).

is the step size for SVRG, Œªt‚àí1/2 is the decaying step
size for SGD, and Œª2 kxk22 is the ridge penalty for SDCA.
(See Johnson & Zhang (2013) for a comparison of SVRG
to a more thoroughly tuned SGD under different decay
schemes.) We use Dual APPA (Algorithm 3) with SDCA
as the inner minimizer. For the algorithms with a notion
of a stage ‚Äì i.e. Dual APPA‚Äôs time spent invoking the inner
minimizer, SVRG‚Äôs period between computing exact gradients ‚Äì we set the stage size equal to the dataset size for
simplicity.6 SVRG is given an advantage in that we choose
not to count its gradient computations when it computes
the exact gradient between stages. All algorithms are initialized at x = 0. Each algorithm was run under Œª = 10i
for i = ‚àí8, ‚àí7, . . . , 8, and plots report the trial that best
minimized the original ERM objective.
Convergence and bias
The proximal term in APPA
introduces a vanishing bias for the problem (towards the
initial point of x = 0) that provides a speedup by adding
strong convexity to the problem. We investigate a natural
baseline: for the purpose of minimizing the original ERM
problem, how does APPA compare to solving one instance
of a regularized ERM problem (using a single run of its
inner optimizer)? In other words, to what extent does recentering the regularizer over time help in solving the unregularized problem? Intuitively, even if SDCA is run to
convergence, some of the minimization is of the regularization term rather than the ERM term, hence one cannot
weigh the regularization too heavily. Meanwhile, APPA
can enjoy more ample strong convexity by placing a larger
weight on its `2 term. This advantage is evident for MNIST
6
Such a choice is justified by the observation that doubling the
stage size does not have noticeable effect on the results discussed.

sgd

5

2
1
5e-1
2e-1
1e-1
5e-2
2e-2

0.12
20

2e-1

5e-2

5

0.22
2e-1

0

1
5e-1

parameter setting Œª
parameter setting Œª
(a) Squared loss. Left: MNIST. Right: CIFAR.

0.58
5e-1

sgd

1e-1

10‚àí210‚àí1100 101 102 103 104 105 106 107 108

20

# gradients / n
# gradients / n
(a) MNIST. Left: train loss F (x). Right: test error rate.
sdca(0.01)

appa
2

2e-2

0.02
5

svrg

2
1
5e-1

final value

svrg(10.0)

final value

sdca(1e ‚àí 08)

10‚àí210‚àí1100 101 102 103 104 105 106 107 108

2
1
5e-1
2e-1
10‚àí210‚àí1100 101 102 103 104 105 106 107 108

parameter setting Œª
parameter setting Œª
(b) Logistic loss. Left: MNIST. Right: CIFAR.

Figure 3. Sensitivity to Œª: the final objective values attained by
each algorithm, after 20 stages (or the equivalent), with Œª chosen
at different orders of magnitude. SGD and SVRG exhibit a sharp
threshold past which they easily diverge, whereas SDCA degrades
more gracefully, and Dual APPA yet more so.

and CIFAR in Figures 1 and 2: recalling that Œª is the same
strong convexity added both by APPA and by SDCA, we
see that APPA takes Œª at least an order of magnitude larger
than SDCA does, to achieve faster and more stable convergence towards an ultimately lower final value.
Figure 1 also shows dashed lines corresponding to the ERM
performance of the least-squares fit and of fully-optimized
ridge regression, using Œª as that of the best APPA and
SDCA runs. These appear in the legend as ‚Äúls(Œª).‚Äù They
indicate lower bounds on the ERM value attainable by
any algorithm that minimizes the corresponding regularized ERM objective. Lastly, test set classification accuracy
demonstrates the extent to which a shrinkage bias is statistically desirable. In the MNIST and CIFAR holdout, we
want only the small bias taken explicitly by SDCA (and
effectively achieved by APPA). In the Protein holdout, we
want no bias at all (again effectively achieved by APPA).
Parameter sensitivity By solving only regularized ERM
inner problems, SDCA and APPA enjoy a stable response
to poor specification of the biasing parameter Œª. Figure 3
plots the algorithms‚Äô final value after 20 stages, against different choices of Œª. Overestimating the step size in SGD or
SVRG incurs a sharp transition into a regime of divergence.
Meanwhile, APPA and SDCA always converge, with solution quality degrading more smoothly. APPA then exhibits
an even better degradation as it overcomes an overaggressive biasing by the 20th stage.

Un-regularizing: approximate proximal point algorithms for empirical risk minimization

Acknowledgments
Part of this work took place while RF and AS were at Microsoft Research, New England, and another part while AS
was visiting the Simons Institute for the Theory of Computing, UC Berkeley. This work was partially supported
by NSF awards 0843915 and 1111109, NSF Graduate Research Fellowship (grant no. 1122374).

References
Bottou, L. and Bousquet, O. The tradeoffs of large scale
learning. In Advances in Neural Information Processing
Systems (NIPS), 2008.
Cohen, M. B., Lee, Y. T., Musco, C., Musco, C., Peng, R.,
and Sidford, A. Uniform sampling for matrix approximation. In Innovations in Theoretical Computer Science
(ITCS), 2015.
Defazio, A., Bach, F., and Lacoste-Julien, S. Saga: A
fast incremental gradient method with support for nonstrongly convex composite objectives. In Advances in
Neural Information Processing Systems (NIPS), 2014.
GuÃàler, O. New proximal point algorithms for convex minimization. SIAM Journal on Optimization, 2(4):649‚Äì664,
1992.
Johnson, R. and Zhang, T. Accelerating stochastic gradient
descent using predictive variance reduction. In Advances
in Neural Information Processing Systems (NIPS), 2013.
Lee, Y. T. and Sidford, A. Efficient accelerated coordinate
descent methods and faster algorithms for solving linear
systems. In Foundations of Computer Science (FOCS),
2013.
Li, M., Miller, G. L., and Peng, R. Iterative row sampling.
In Foundations of Computer Science (FOCS), 2013.
Lin, Q., Lu, Z., and Xiao, L. An accelerated proximal coordinate gradient method. In Advances in Neural Information Processing Systems (NIPS), 2014.
Needell, D., Srebro, N., and Ward, R. Stochastic gradient
descent, weighted sampling, and the randomized kaczmarz algorithm. In Advances in Neural Information Processing Systems (NIPS), 2014.
Nelson, J. and Nguyen, H. L. Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings.
In Foundations of Computer Science (FOCS), 2013.
Nesterov, Y. A method of solving a convex programming
problem with convergence rate o(1/k 2 ). Soviet Mathematics Doklady, 27(2):372‚Äì376, 1983.

Nesterov, Y. Introductory Lectures on Convex Optimization: A Basic Course. Springer, 2004.
Parikh, N. and Boyd, S. Proximal algorithms. Foundations
and Trends in Optimization, 1(3):123‚Äì231, 2014.
Rahimi, A. and Recht, B. Random features for large-scale
kernel machines. In Advances in Neural Information
Processing Systems (NIPS), 2007.
Rockafellar, R. T. Monotone operators and the proximal
point algorithm. SIAM Journal on Control and Optimization, 14(5):877‚Äì898, 1976.
Roux, N. L., Schmidt, M., and Bach, F. A stochastic gradient method with an exponential convergence rate for
finite training sets. In Advances in Neural Information
Processing Systems (NIPS), 2012.
Shalev-Shwartz, S. and Zhang, T. Stochastic dual coordinate ascent methods for regularized loss minimization.
Journal of Machine Learning Research (JMLR), 14:567‚Äì
599, 2013.
Shalev-Shwartz, S. and Zhang, T. Accelerated proximal
stochastic dual coordinate ascent for regularized loss
minimization. Mathematical Programming, pp. 1‚Äì41,
2014.
Strohmer, T. and Vershynin, R. A randomized kaczmarz
algorithm with exponential convergence. Journal of
Fourier Analysis and Applications, 15:262‚Äì278, 2009.
Williams, V. V.
Multiplying matrices faster than
Coppersmith-Winograd. In Symposium on Theory of
Computing (STOC), 2012.
Xiao, L. and Zhang, T. A proximal stochastic gradient
method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057‚Äì2075, 2014.

