Riemannian Pursuit for Big Matrix Recovery

Mingkui Tan 1
TANMINGKUI @ GMAIL . COM
Ivor W. Tsang 2
IVOR . TSANG @ GMAIL . COM
Li Wang 3
LIW 022@ UCSD . EDU
Bart Vandereycken 4
BARTV @ PRINCETON . EDU
Sinno Jialin Pan 5
JSPAN @ I 2 R . A - STAR . EDU . SG
1
School of Computer Science, The University of Adelaide, Ingkarni Wardli North Terrace Campus 5005, Australia
2
Center for Quantum Computation & Intelligent Systems, University of Technology Sydney, Australia
3
Department of Mathematics, University of California, San Diego, USA
4
Department of Mathematics, Princeton University, Fine Hall, Washington Road, Princeton NJ 08544-1000, USA
5
Institute for Infocomm Research, 1 Fusionopolis Way, #21-01 Connexis (South) 138632, Singapore

Abstract
Low rank matrix recovery is a fundamental task
in many real-world applications. The performance of existing methods, however, deteriorates
significantly when applied to ill-conditioned or
large-scale matrices. In this paper, we therefore
propose an efficient method, called Riemannian
Pursuit (RP), that aims to address these two problems simultaneously. Our method consists of
a sequence of fixed-rank optimization problems.
Each subproblem, solved by a nonlinear Riemannian conjugate gradient method, aims to correct
the solution in the most important subspace of
increasing size. Theoretically, RP converges linearly under mild conditions and experimental results show that it substantially outperforms existing methods when applied to large-scale and
ill-conditioned matrices.

1. Introduction
Matrix recovery (MR) has attracted a lot of attention from
various research communities, such as statistical machine
learning, collaborative filtering, image and signal processing (CandeÃÄs & Recht, 2009; Negahban & Wainwright,
2012). With the fast development of Web 2.0 in the last
decade, big MR problems have been widely involved in
many practical applications, leading to great challenges in
computation. For instance, in collaborative filtering tasks,
the Netflix Prize problem involves 108 ratings of 480,189
users on 17,770 movies (KDDCup, 2007). The MR problem is defined as follows:
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

Definition 1. Given a linear operator A : Rm√ón ‚Üí Rl , let
b + e be l linear measurements of an unknown
b = A(X)
b ‚àà Rm√ón , where e denotes noise. Then
rank-b
r matrix X
b by solving
the task of MR is to recover X
min f (X),
X

s.t. rank(X) ‚â§ r,

(1)

where l  mn, r ‚â• rb, and f (X) = 12 kb ‚àí A(X)k22 .
The definition of A depends on the application context,
such as matrix completion, quantum state tomography, matrix factorizations; see, e.g., (Recht et al., 2010; CandeÃÄs &
Plan, 2010a; Recht, 2011; Keshavan et al., 2010b; Laue,
2012). Although our derivation is valid for any A that allows for MR, in the numerical experiments, we will focus
on matrix completion (MC) as a specific application. In this
case, A(X) is defined as the element-wise restriction of X
on Œû, a subset of the complete set of entries of X.
Problem (1) is known to be NP-hard. To address it, many
researchers proposed to solve the nuclear-norm convex relaxation (Fazel, 2002; Hazan, 2008; Recht et al., 2010):
minX kXk‚àó , s.t. A(X) = b, where k ¬∑ k‚àó denotes the
matrix nuclear norm. A number of algorithms have been
proposed to solve this relaxation, such as the singular value
thresholding (SVT) (Cai et al., 2010), the augmented Lagrangian method (ALM) (Lin et al., 2010; 2011; Yang &
Yuan, 2013). In practice, the following matrix lasso problem is also often studied (Toh & Yun, 2010): minX 21 kb ‚àí
A(X)k22 + ŒªkXk‚àó , where Œª is a regularization parameter.
Regarding this problem, the accelerated proximal gradient (APG) method has been proven to be effective (Toh &
Yun, 2010; Mishra et al., 2013). While nuclear-norm based
methods have shown some success in practice, their applicability to large-scale problems is rather limited because
of their necessity to compute high-dimensional SVDs. Recently, Mishra et al. (2013) proposed a method to solve the

Riemannian Pursuit for Big Matrix Recovery

matrix lasso problem by avoiding high-dimensional SVDs.
However, empirically, this method still performs similarly
to APG.
To improve the scalability of MR, a different solution is
to relax problem (1) to a fixed-rank optimization problem (Mitra et al., 2010; Keshavan et al., 2010a):
min f (X),
X

s.t. rank(X) = r,

(2)

where r is an estimated rank. This problem is nonconvex, but it can be solved efficiently by local-search
methods. Particularly, since the fixed-rank matrices belong
to a smooth matrix manifold, many efficient methods based
on manifold optimization have been proposed (Meyer et al.,
2011; Boumal & Absil, 2012; Vandereycken, 2013).

hA, Bi = tr(ABT ) be the element-wise product and inner product of A and B, respectively. Denote
Pqthe SVD of
T
X ‚àà Rm√ón as X = U(diag(œÉ))VT =
i=1 œÉi ui vi ,
where q = min{m, n} and œÉi is arranged in descending order.PThe nuclear norm of X is defined as kXk‚àó =
kœÉk1 = i |œÉi | and the Frobenius norm of X is defined as
kXkF = kœÉk2 . The condition number Œ∫r (X) of X w.r.t. a
given number r is defined as Œ∫r (X) = œÉ1 /œÉr .
2.1. Matrix RIP Condition
To discuss convergence, we introduce the matrix restricted
isometry property (RIP) condition (Recht et al., 2010).
Specifically, the matrix RIP condition describes a property
of a linear operator A as the smallest number Œ≥r such that

By exploiting the smooth geometry of fixed-rank matrices,
fixed-rank based methods have shown superior scalability
compared with the nuclear-norm based methods (Boumal
& Absil, 2012; Mishra et al., 2012; Vandereycken, 2013).
However, there are two deficiencies for the existing fixedrank methods. Firstly, since the ground-truth rank of the
matrix to be recovered is usually unknown, it is nontrivial
to set the value of r in (2). Secondly, as observed in (Ngo
& Saad, 2012; Boumal & Absil, 2012), when solving (2)
b existing fixed-rank based methods
with ill-conditioned X,
may converge slowly.

holds for all matrices of rank at most r. Observe that
the RIP condition does not hold for MC. To study the exact recovery condition of MC, the incoherence of matrices is introduced (CandeÃÄs & Recht, 2009; CandeÃÄs & Plan,
2010b). Specifically, a matrix X of rank r with SVD
X = Udiag(œÉ)VT is ¬µ-incoherent (¬µ ‚â• 1) if ‚àÄi ‚àà [r]

To address the above issues, we develop an efficient and
scalable algorithm for MR that iteratively increases the
rank of the matrix to be recovered by a fixed integer œÅ. The
main contributions of this paper are as follows:

Under these conditions, the RIP conditions of MR has been
extended to MC (Meka et al., 2009a).

1) We propose the Riemannian Pursuit (RP) method, which
essentially solves a sequence of fixed-rank minimization
problems using a nonlinear Riemannian Conjugate Gradient (NRCG) method. The convergence of NRCG is guaranteed by the application of a strong Wolfe line search.
2) We prove that RP converges linearly under mild conditions. Compared with other fixed-rank based methods,
the proposed optimization scheme can effectively address
the convergence issues that occur with ill-conditioned and
large rank problems.
3) RP automatically estimates the rank under proper stopping conditions, which avoids the difficulty of the rank estimation in most existing fixed-rank based methods.

2. Notations and Preliminaries
Throughout the paper, we denote by the superscript T the
transpose of a vector/matrix, 0 a vector/matrix with all zeros, diag(v) a diagonal matrix with a vector of diagonal
entries equal to v, and kvkp the `p -norm of a vector v. We
denote by [n] the list {1, ..., n}. Given a linear operator
A, its adjoint operator is denoted by A‚àó . Let A  B and

(1 ‚àí Œ≥r )||X||2F ‚â§ ||A(X)||2F ‚â§ (1 + Œ≥r )||X||2F

||ui ||‚àû ‚â§

p

¬µ/m and ||vi ||‚àû ‚â§

p
¬µ/n.

(3)

(4)

Proposition 1. In MC, suppose the observed entry set Œû is
sampled according to the Bernoulli model with each entry
(i, j) ‚àà Œû being independently drawn from a probability p.
There exists a constant C > 0, for all Œ≥r ‚àà (0, 1), ¬µ ‚â• 1,
n ‚â• m ‚â• 3, if p ‚â• C¬µ2 r2 log(n)/(Œ≥r2 m), the following
RIP condition holds
(1 ‚àí Œ≥r )p||X||2F ‚â§ ||A(X)||2F ‚â§ (1 + Œ≥r )p||X||2F ,

(5)

for any ¬µ-incoherent matrix X ‚àà Rm√ón of rank at most r
with probability at least 1 ‚àí exp(‚àín log n).
2.2. Differential Geometry of Fixed-Rank Matrices
Given a positive integer r, consider the smooth submanifold of fixed rank-r matrices,
Mr

=

{X ‚àà Rm√ón : rank(X) = r}

=

n
{Udiag(œÉ)VT : U ‚àà Stm
r , V ‚àà Str , ||œÉ||0 = r}

m√ór
with Stm
: UT U = I} the Stiefel manifold
r = {U ‚àà R
of m √ó r real and orthonormal matrices. The tangent space
TX Mr of Mr at X = Udiag(œÉ)VT ‚àà Rm√ón is given by
TX Mr = {UMVT+Up VT+UVpT : M ‚àà Rr√ór , Up ‚àà
n√ór
Rm√ór , UT
, VpT V = 0}.
p U = 0, Vp ‚àà R

Riemannian Pursuit for Big Matrix Recovery

Define a metric gX (A, B) = hA, Bi, where X ‚àà Mr
and A, B ‚àà TX Mr , then Mr is a Riemannian manifold
by restricting hA, Bi to the tangent bundle. Here the tangent bundle is defined
S as the disjoint union of all tangent
spaces T Mr = X‚ààMr {X} √ó TX Mr = {(X, E) ‚àà
Rm√ón √ó Rm√ón : X ‚àà Mr , E ‚àà TX Mr }. By restricting f (X) = 21 kb ‚àí A(X)k22 to Mr we obtain a smooth
function on Mr . Its Riemannian gradient is given as the
orthogonal projection onto the tangent space of the gradient of f . Define PU = UUT and PU‚ä• = I ‚àí UUT for any
m√ón
U ‚àà Stm
r . The orthogonal projection of any Z ‚àà R
T
onto the tangent space at X = Udiag(œÉ)V is defined as
PTX Mr (Z) : Z 7‚Üí PU ZPV + PU‚ä• ZPV + PU ZPV‚ä• .

(6)

‚àó

Let G = A (A(X) ‚àí b). Then, the Riemannian gradient
of f (X) on Mr can be calculated as
gradf (X) = PTX Mr (G).

(7)

For convenience, we define PT0 Mr (Z) = 0 when X = 0.
Moreover, the Retraction mapping on Mr is to go back
from an element in the tangent space to the manifold, which
can be computed in a closed form as
RX (E) = PMr (X + E) =

r
X

œÉi pi qTi ,

(8)

with two main steps: 1) identifying the most-active subspace through an active-subspace search in Step 2; and 2)
solving a master problem optimization regarding a fixedrank minimization problem in Step 3. In Algorithm 1 and
in the rest of the paper, we use the notation
Pt := PTXt MtœÅ .
The parameter out is a tolerance on the stopping condition
of Algorithm 1 and will be detailed in Section 3.1.
Algorithm 1 RP: Riemannian Pursuit for MR.
Require: Rank increase œÅ. Inner and outer iteration tolerance in and out .
1: Initialize X0 = 0, Œæ 0 = b, G = A‚àó (Œæ 0 ), and t = 1.
2: Perform an active-subspace search as follows.
2a: Compute Q = G ‚àí Pt‚àí1 (G).
2b: Compute a best rank œÅ approximation of Q:
Ht‚àí1
= UœÅ diag(œÉ œÅ )VœÅT
2
3: Let Ht‚àí1
= Pt‚àí1 (G) and Ht‚àí1 = Ht‚àí1
+ Ht‚àí1
1
1
2 .
3a: Choose a proper step size œÑt from (10) and set
Xintial = RXt‚àí1 (‚àíœÑt Ht‚àí1 ).
3b: Using X

i=1

Pr
where i=1 œÉi pi qT
i denotes a best rank-r approximation
to X + E. The norm of a tangent vector
p Œ∂ X ‚àà TX Mr
hŒ∂ X , Œ∂ X i. We
evaluated at X is defined as ||Œ∂ X || =
refer to (Vandereycken, 2013) and references therein for
more details on the geometry of Mr .

3. Riemannian Pursuit
Solving problem (1) is hard because there is little knowledge about the rank of the matrix to be recovered; otherwise (1) is reduced to a fixed-rank minimization problem.
Considering that fixed-rank methods have gained great success in solving big MR problems with explicit knowledge
of the rank, we propose to iteratively increase the rank by
a fixed integer œÅ and then solve a series of fixed-rank minimization subproblems until a proper stopping condition is
achieved. Once this procedure is finished, the final rank
returned is our rank estimation and we can perform a final, more accurate fixed-rank optimization step. Based on
this motivation, the proposed method is presented in Algorithm 1. Since Riemannian optimization is a core element
of the algorithm, we refer to it as the Riemannian Pursuit
(RP) in the sequel. The parameter œÅ in RP is crucial for
both rank estimation and convergence. For ease of presentation, we leave the detailed setting of choosing œÅ for
Section 3.2.
As shown in Algorithm 1, Œæ t = b ‚àí A(Xt ) represents the
residual at iteration t. Starting with X0 = 0, RP iterates

initial

(Warm Start)

as initial guess, call

Xt = NRCG(Xinitial , in ).
4: Update Œæ t = b ‚àí A(Xt ) and G = A‚àó (Œæ t ).
5: Quit if stopping condition on out is achieved; other-

wise, let t := t + 1 and go to Step 2.
In Step 2, the active-subspace search determines the mostactive subspace that is orthogonal to Xt‚àí1 from G =
A‚àó (Œæ t‚àí1 ). Such a subspace is obtained by computing the
top œÅ singular values and vectors of G ‚àí Pt‚àí1 (G), which
is orthogonal to TXt‚àí1 M(t‚àí1)œÅ and thus also to Xt‚àí1 . Remark that, due to computational reasons, the master problem in step 3b is not solved exactly, thus Pt‚àí1 (G) is not
necessarily zero (or negligible).
After the active-subspace search, we have rank(Xt ) = tœÅ,
namely Xt ‚àà MtœÅ . The master problem optimization in
Step 3 is to solve a fixed-rank problem
min f (X), s.t. rank(X) = tœÅ,
X

(9)

where f (X) = 12 ||A(X) ‚àí b||22 is a smooth function. In
particular, we solve it using LRGeomCG from (Vandereycken, 2013), which is a nonlinear Riemannian Conjugate
Gradient (NRCG) method.
As shown in Algorithm 1, NRCG involves two parameters,
namely the initial point Xintial and its stopping tolerance
in . Here, Xintial of rank tœÅ in Step (3a) is used as a warmstart in NRCG, which is important for improving the overall efficiency of the algorithm. To be more specific, we use

Riemannian Pursuit for Big Matrix Recovery

RXt‚àí1 (‚àíœÑt Ht‚àí1 ) as an initial point for NRCG, where œÑt is
a step size that is obtained by a line search on the condition
f (RXt‚àí1 (‚àíœÑt Ht‚àí1 )) ‚â§ f (Xt‚àí1 ) ‚àí

œÑt
hHt‚àí1 , Ht‚àí1 i.
2

(10)

Main Theoretical Results: Before presenting the details
of NRCG, we summarize two major theoretical results regarding the convergence of Algorithm 1.
Firstly, let {Xt } be the sequence generated by RP, then
f (Xt ) decreases monotonically w.r.t. t.
Lemma 1. Let {Xt } be the sequence generated by RP, then
f (Xt ) ‚â§ f (Xt‚àí1 ) ‚àí

œÑt
||Ht‚àí1
||2F ,
2
2

3.2. Parameter Selection on œÅ
As shown in Theorem 1, RP with a larger œÅ converges
faster. However, a small œÅ is required in order to make
an accurate estimation to the rank. Particularly, when dealing with problems of ill-conditioning, œÅ should be small
enough. We present a simple and effective method to
set an appropriate œÅ. Let œÉ be the singular vector of
A‚àó (b), where œÉi is arranged in descending order. Motivated by the thresholding strategy in StOMP for sparse recovery (Donoho et al., 2012), we choose œÅ such that for
0<Œ∑‚â§1

(11)

œÉi ‚â• Œ∑ œÉ1 , ‚àÄi ‚â§ œÅ.

(13)

where œÑt satisfies condition (10).
b be the ground-truth low-rank matrix and e
Secondly, let X
be the additive noise, the following theorem indicates that
b = 1 ||e||2 .
f (Xt ) decreases linearly when f (Xt ) > f (X)
2
Theorem 1. Let {Xt } be the sequence generated by RP
and Œ∂ = min{œÑ1 , ¬∑ ¬∑ ¬∑ , œÑŒπ }. As long as f (Xt ) ‚â• C2 ||e||2
(where C > 1) and if there exists an integer Œπ > 0 such
that Œ≥(br+2ŒπœÅ) < 12 , then RP decreases linearly in objective
values when t < Œπ, namely f (Xt+1 ) ‚â§ ŒΩf (Xt ), where
ŒΩ =1‚àí

œÅŒ∂
2b
r

2

C(1 ‚àí 2Œ≥(br+2ŒπœÅ) )
‚àö
( C + 1)2 (1 ‚àí Œ≥(br+2ŒπœÅ) )

!

1
1‚àí ‚àö
C

2
.

This theorem illustrates the convergence speed of RP under
the RIP condition Œ≥(br+2ŒπœÅ) < 21 . To apply it to the matrix
completion problem, we need to adapt a variant of the RIP
condition in (5) and assume that the Xt are incoherent, uniformly in t.

In other words, œÅ denotes the number of sufficiently large
singular values of A‚àó (b). In general, a smaller Œ∑ leads to
a larger œÅ. When setting Œ∑ = 1, we trivially have œÅ =
1, which is not efficient when the exact rank rb is large.1
We refer to the Supplementary Materials for an efficient
computational strategy to compute œÅ given Œ∑.
3.3. Nonlinear Riemannian Conjugate Gradient
In this section, we detail the NRCG method for solving the
fixed-rank problem (9) in Step 3b of RP. To differentiate
from the outer iteration variable Xt of RP, we use Xk for
the inner iteration index k of NRCG. In Euclidean space,
the search direction Œ∂ k of nonlinear CG is calculated as
Œ∂ k = ‚àígradf (Xk ) + Œ≤k Œ∂ k‚àí1 ,
where Œ≤k is calculated from, for example, the Fletcher‚Äì
Reeves (FR) rule:
Œ≤t =

3.1. Stopping Conditions for RP
According to Lemma 1, RP monotonically decreases in
objective value. Without proper stopping conditions, RP
may increase the rank until tœÅ ‚â• h = min(m, n), where
Œæ t = b ‚àí A(Xt ) = 0, and the solution will likely be overfitted. To avoid this issue, one can terminate on a small
relative residual,
kŒæt kF /||b||F ‚â§ ŒªF .

(12)

In real-world problems, the matrix to be recovered is not
exactly low-rank and then (12) may not be adequate. Since
RP decreases the objective values monotonically, we propose to use a difference in function values as the stopping
condition,
2(f (Xt‚àí1 ) ‚àí f (Xt ))/(œÅkbk2F ) ‚â§ out ,
where out is a predefined tolerance value. This condition
is based on the assumption that over-fitting will happen if
increasing the rank does not significantly decrease the objective value. In practice, out = 10‚àí5 is usually a good
choice.

hgradf (Xk ), gradf (Xk )i
.
hgradf (Xk‚àí1 ), gradf (Xk‚àí1 )i

(14)

Different from Euclidean space, the search direction on a
manifold are adapted to follow a path on the manifold (Absil et al., 2008). Particularly, since gradf (Xk ) ‚àà TXk Mr ,
gradf (Xk‚àí1 ) ‚àà TXk‚àí1 Mr , and Œ∂ k‚àí1 are in different tangent spaces of the manifold, the above two equations are
not applicable on Riemannian manifolds. To extend nonlinear CG of Euclidean space to Riemannian manifolds, two
additional operators, namely retraction and vector transport are necessary. With the previously defined retraction
mapping in (8), one can move points towards the direction
of a tangent vector and make them stay on the manifold. A
vector transport T on a manifold Mr is a smooth map that
transports tangent vectors from one tangent space to another (Absil et al., 2008). Denoting such a vector transport
by TX‚ÜíY : TX Mr ‚Üí TY Mr , the conjugate direction can
be calculated as
Œ∂ k = ‚àíEk + Œ≤t TXk‚àí1 ‚ÜíXk (Œ∂ k‚àí1 ),
1

In practice, we suggest setting Œ∑ ‚â• 0.60.

(15)

Riemannian Pursuit for Big Matrix Recovery

where Ek = gradf (Xk ) and Œ≤k is determined from (14).
The NRCG method is presented in Algorithm 2, which includes two major steps: 1) calculating the conjugate search
direction in Step 3, and 2) updating Xk+1 by retraction,
namely Xk+1 = RXk (Œ∏k Œ∂ k ), where Œ∏k denotes the step
size satisfying the strong Wolfe conditions. Specifically,
given a descent direction Œ∂ k ‚àà TXk Mr , Œ∏k is determined
such that
f (RXk (Œ∏k Œ∂ k )) ‚â§ f (Xk ) + c1 Œ∏k hgradf (Xk ), Œ∂ k i,

(16)

|hgradf (RXk (Œ∏k Œ∂ k )), TXk‚àí1 ‚ÜíXk (Œ∂ k )i|
‚â§ c2 |hgradf (Xk ), Œ∂ k i|,

(17)

Assumption 1. Let {Xk } and {Œ∂ k } be the sequence of iterates and search directions generated by the NRCG method
in Step (3b). We assume that Œ∏ 7‚Üí f (RXk (Œ∏Œ∂ k )) is Lipschitz continuously differentiable with a uniform Lipschitz
constant L > 0.
Theorem 2. Let {Xk } be the sequence generated by the
NRCG method in Step (3b) of Algorithm 1 with the strong
Wolfe line search, where 0 < c1 < c2 < 1/2, then we have
limk‚Üí‚àû inf gradf (Xk ) = 0.
Proof : Combining Lemma 2 and Assumption 1 as in (Sato
& Iwai, 2013).

where 0 < c1 < c2 < 1/2.
Two choices for vector transport are orthogonal projection
TX‚ÜíY (Œ∂) = PTY Mr (Œ∂)

(18)

and the scaled differentiated retraction
TX‚ÜíY (Œ∂) = Œ± ¬∑

d
RX (ŒΩ + tŒ∂)|t=0 ,
dt

(19)

‚àí1
where ŒΩ = RX
(Y) and Œ± is such that kTX‚ÜíY (Œ∂)kF =
kŒ∂kF ; see (Sato & Iwai, 2013).

The standard choice (18) from (Vandereycken, 2013) is
cheaper to evaluate, but (19) is required for proving the
convergence of NRCG.
Proposition 2 (Sato & Iwai (2013)). Given the retraction
(8) and vector transport (19) on Mr , there exists a step
size Œ∏k that satisfies the strong Wolfe conditions (16) and
(17).
Lemma 2 (Sato & Iwai (2013)). The search direction Œ∂ k
generated by NRCG using the vector transport (19) and
the strong Wolfe conditions (16) and (17) satisfies
‚àí

hgradf (Xk ), Œ∂ k i
1
2c2 ‚àí 1
‚â§
‚â§
. (20)
1 ‚àí c2
hgradf (Xk‚àí1 ), gradf (Xk‚àí1 )i
1 ‚àí c2

Using (Dieci & Eirola, 1999), (19) can be implemented efficiently even though it will be more expensive than (18).
One can show that the difference between (18) and (19) is
O(kŒ∂k2 ), hence they have the same behavior near the optimizer where kŒ∂k ‚Üí 0. For convenience, we therefore use
(18) in the numerical experiments.
Global convergence of NCRG is obtained if the functions f (RXk (Œ∏Œ∂ k )) are Lipschitz continuously differentiable in Œ∏. Since the manifold MtœÅ is open at points where
rank(X) < tœÅ, such a condition cannot hold uniformly
on MtœÅ . In (Vandereycken, 2013), the additional term
1 2
‚Ä† 2
2
2 ¬µ (||X ||F + ||X||F ) was added to the objective function
to penalize rank drops. For simplicity, we assume that f
satisfies the necessary Lipschitz conditions throughout the
iteration.

Algorithm 2 NRCG(Xintial ,r, in ) for solving (2).
1: Initialize X1 = Xintial and Œ∂ 0 = 0. Let k = 1.
2: Compute the gradient Ek = gradf (Xk ) by (7).
3: Compute a conjugate direction Œ∂ k according to (15).
4: Choose a step size Œ∏k satisfying the strong Wolfe conditions (16) and (17), and set Xk+1 = RXk (Œ∏k Œ∂ k ).
5: Terminate and output Xk+1 if the stopping conditions
are achieved; otherwise, let k = k + 1 and go to step 1.

3.4. Computational Advantages of RP
The proposed RP algorithm has several computational advantages. First of all, it is useful for rank detection. Specifically, under the stopping conditions in Section 3.1, RP will
automatically estimate the rank as r = tœÅ.
Secondly, RP converges well on ill-conditioned problems.
Even when the condition number Œ∫r (X) is very large,
Œ∫tœÅ (X) will be small for t small. Thus NCRG can converge well and consequently, the total convergence of RP
is improved significantly.
Thirdly, RP has good scaling characteristics for solving
large-scale problems. For example, the only SVD calculations required are the best rank tœÅ approximation of Xt + Œæ
for the retraction and the best rank œÅ of the matrix Q in
Step 2b. In all cases, the matrices involved are highly structured. Specifically, Xk + Œ∏k Œ∂ k is a rank 2tœÅ matrix, and
thus the full SVD can be computed very efficiently; see
(Vandereycken, 2013). For Q = G ‚àí Pt (G), we can use
PROPACK (Larsen, 2004) or randomized low-rank approximation (Halko et al., 2011) at a cost of O((|G| + tœÅn)œÅ)
where |G| is the cost of one matrix-vector product with G.
For example, |G| = O(b
rn log2 n) is highly sparse for MC.
Finally, thanks to the warm-start, each application of
NRCG requires only a few iterations to achieve a sufficiently accurate solution.

Riemannian Pursuit for Big Matrix Recovery

4. Related Studies

Shwartz et al. (2011) proposed a Greedy Efficient Component Optimization (GECO) algorithm to solve a convex
relaxation of the rank-constrained problem by iteratively
increasing the rank by 1. Jaggi & Sulovsky (2010) proposed a new approximation algorithm based on a sparse
approximate SDP model (Hazan, 2008). Laue (2012) proposed a hybrid strategy to solve the MR problem by iteratively increases the rank by 1. Different from these methods, RP essentially solves a sequence of fixed-rank methods and incrementally increases the rank by œÅ ‚â• 1. In
practice, it is crucial to use a large œÅ to accelerate the convergence speed on big matrices of large ranks. Moreover,
unlike RP, GECO solves much more expensive regression
subproblems; Laue‚Äôs method solves the nonlinear master
problems with a BFGS method, which is memory inefficient on large-scale problems. Finally, the importance of
stopping conditions for rank estimation is absent in these
methods.

5. Numerical Experiments
5.1. Baseline Methods and Performance Matric
Following state-of-the-art methods are adopted as baseline methods: SVP (Meka et al., 2009b), APG (Toh &
Yun, 2010) (which uses a homotopy strategy to solve
the matrix lasso problem), GECO (Shwartz et al., 2011),
LMaFit (Wen et al., 2012), LMaFit-A (Wen et al., 2012)
(which extends LMaFit by automatically estimating the
rank). ScGrassMC (Ngo & Saad, 2012), a fixed-rank
method which is claimed to alleviate the convergence issue
over ill-conditioning problems. LRGeomCG (Vandereycken, 2013), a fixed-rank method that adopts the nonlinear
Riemannian Conjugate Descent method with Armijo line
search. qGeomMC, a fixed-rank method based on mani-

10
GECO
ScGrassMC
LRGeomCG
LMaFit
qGeomMC
RP(Œ∑ = 1.00)
RP(Œ∑ = 0.75)
RP(Œ∑ = 0.65)
RP(Œ∑ = 0.55)

‚àí2

10

‚àí4

10

‚àí6

10

Relative Objective Value

Relative Objective Value

Fixed-rank methods such as the low-rank geometric conjugate gradient method (LRGeomCG) (Vandereycken, 2013),
the quotient geometric matrix completion method (qGeomMC) (Mishra et al., 2012), and the method of scaled
gradients on Grassmann manifolds for matrix completion
(ScGrassMC) (Ngo & Saad, 2012), have shown promising performance. Gradient methods and stochastic gradient methods have been developed to address the fixedrank problems (Jaggi & Sulovsky, 2010; Wen et al., 2012).
All these methods require the estimate of r. Greedylike algorithms have also been proposed to solve the
fixed-rank problem, such as the Singular Value Projection (SVP) (Meka et al., 2009b), Atomic Decomposition
for Minimum Rank Approximation (ADMiRA) (Lee &
Bresler, 2010), SpaRCS (Waters et al., 2011), and so on.
For these methods, they are guaranteed to converge under
restricted conditions.

0

0

10

ScGrassMC
LRGeomCG
LMaFit
qGeomMC
RP(Œ∑ = 1.00)
RP(Œ∑ = 0.75)
RP(Œ∑ = 0.65)
RP(Œ∑ = 0.55)

‚àí2

10

‚àí4

10

‚àí6

10

‚àí8

10
‚àí8

10

0

50

100

150

200

0

Time (in seconds)

50

100
150
Time (in seconds)

200

(a) Gaussian singular values sg (b) œá2 singular values s2œá where
where œÅ w.r.t. different Œ∑‚Äôs is 1, œÅ w.r.t. different Œ∑‚Äôs is 1, 4, 6,
8, 14, 18, resp.
12, respectively.
Figure 1. Convergence of comparison methods on sg and sœá2 ,
where Œ∂os = 3 and rb = 50. GECO cannot converge within 1
hour on sg , and we omit its results on sœá2 . ScGrassMC gets numerical problems after 50 iterations on sœá2 .

fold optimization. 2 Note that some related methods, such
as the IALM and OptSpace are not considered as baselines
in this paper since the adopted baseline methods above have
been shown to be state-of-the-art in MR (Wen et al., 2012;
Vandereycken, 2013). For RP, we adopt the stopping criteria discussed in Section 3.1.
The relative testing error (RTE) is adopted as the
comparison metric in synthetic experiments: RTE =
b ‚àí X‚àó )kF /kX‚àó kF . The testing root-mean-square
kPŒû (X
Œû
error (RMSE) is used as the comparison metric p
in realb ‚àí X‚àó )kF / (|Œû|).
world applications: RMSE = kPŒû (X
b denotes the observed matrix (with missing entries
Here X
filled with ‚Äò0‚Äô), X‚àó denotes the recovered matrix, and Œû
denotes the index set of observed entries.
All the experiments (except for GECO) are conducted in
Matlab on a PC installed a 64-bit operating system with
an Intel(R) Core(TM) i7 CPU (2.80GHz with single-thread
mode) and 24GB memory.
5.2. Synthetic Problem Generation
In synthetic experiments, we focus on matrices with
large condition numbers.
Following (Ngo & Saad,
b =
2012), we generate ground-truth low-rank matrices X
T
b
b
b
Udiag(b
œÉ )V + e, where œÉ is a rb-sparse vector, U ‚àà Stm
r ,
b
and V ‚àà Stnr . Two types of singular values are studied: 1)
Gaussian sparse singular value sg with each nonzero entry
2
LMaFit, ScGrassMC, qGeomMC and LRGeomCG
are
from:
http://www.montefiore.ulg.ac.
be/Àúmishra/fixedrank/fixedrank.html;
APG, SVP and GECO are from:
http://www.
math.nus.edu.sg/Àúmattohkc/NNLS.html,
http://www.cs.utexas.edu/Àúpjain/svp/,
www.cs.huji.ac.il/Àúshais/code/index.html,
respectively.

Riemannian Pursuit for Big Matrix Recovery

sampled from Gaussian distribution N (0, 1000), and 2) œá2
sparse singular values s2œá , where each entry is the square
of sg . The two types of singular values are fast decaying,
b are very large. Once X
b
and their condition numbers Œ∫rb(X)
is generated, we sample l = r(m + n ‚àí r) √ó Œ∂os entries
b uniformly to produce b, where Œ∂os is the oversamfrom X
pling factor (Lin et al., 2010). In the noisy cases, each sampled entry is disturbed by a Gaussian noise with strength
‚àÜkbk2 /knk2 , where ‚àÜ is a strength factor and n ‚àà Rl is
generated by n = randn(l, 1) in Matlab.

Table 1. Averaged training time (seconds) on big matrices.
Data Type
r
LRGeomCG
qGeomMC
RP
*

sg
50
316.8
216.1
57.5(48)

sœá2

100
992.2
1091.5
205.6(102)

50
564.3
415.1
75.4(48)

100
1018.8
455.3
150.8(85)

The number in bracket is the rank estimated by
RP. Note that LRGeomCG and qGeomMC use the
ground-truth rank (r = rb).

target rank correctly under various oversampling factors;
while APG can only correctly detect the rank with enough
observations.

5.3. Performance Comparison in Noiseless Cases
We compare the convergence of RP on sg and sœá2 with
several baseline methods. To show the impact of Œ∑ from
(13) (and thus œÅ) on the convergence of RP, we test
Œ∑ = 1.00, 0.75, 0.65, 0.55, respectively. For simplicity, we
set the rank parameter for the fixed-rank methods as the
ground-truth rank, namely, r = rb = 50, which is the best
choice for r in the noiseless case. The relative objective
value w.r.t. the computational time are shown in Fig. 1.
As can be seen from Fig. 1, RP with different Œ∑ generally
converges faster than other methods on both types of matrices. The reason that the other methods converge slowly
is due to the large condition numbers of sg and sœá2 . All the
above observations justify that RP can converge well on
ill-conditioned problems thanks to the reasons mentioned
in Section 3.4.
5.4. Performance Comparison in Noisy Cases
In this section, we study the performance of the comparison
methods on noisy problems of medium size (i.e., m = n =
5, 000). We report the training time and RTE of various
methods for comparison.
For medium-sized problems, we generate matrices with
the ground-truth rank rb = 50, and produce the observations with noise strength factor ‚àÜ = 0.01 under oversampling rates Œ∂os ‚àà {2, 2.3, 2.5, 2.8, 3.0, 3.3, 3.5, 3.8, 4.0}.
We compare RP with SVP, APG, LMaFit, LMaFit-A, qGeomMC and LRGeomCG. Note that LMaFit-A can automatically adjust the rank. We set ŒªF = 0.01, out = 10‚àí5 for
the stopping conditions and Œ∑ = 0.65 for RP. For APG, we
set the trade-off parameter Œª = 10‚àí3 œÉmax , where œÉmax is
the largest singular value of A‚àó (b). For all the fixed-rank
methods, we set r = rb = 50. We use default settings for
the other parameters of the baselines. For each oversampling rate, we run 10 independent experiments. The Averaged computational time, RTE and the estimated ranks
are shown in Fig. 2(a), Fig. 2(b) and Fig. 2(c), respectively.
From Fig. 2(a) and Fig. 2(b), under various oversampling
factors, RP generally shows the least relative testing error
and the best optimization efficiency among all methods.
More importantly, from Fig. 2(c), only RP can estimate the

5.5. Performance Comparison on Big Matrices
In the experiments on large-scale matrix completion problems, we vary the values of rb in {50, 100}, and set m = n =
20, 000, Œ∂os = 4 and ‚àÜ = 0.05. Here, we only compare the
scalability of RP with that of LRGeomCG and qGeomMC
since there two methods have shown better efficiency than
other baselines. We set ŒªF = 0.05, out = 10‚àí5 and
Œ∑ = 0.65 for RP. To demonstrate the superiority of RP over
the baselines in terms of computational speed, we terminate
the baselines once they achieve the same objective value
of RP or a maximum of 400 iterations are achieved. In addition, we set r = rb for these two fixed-rank methods. The
other parameters are the same as those in the experiments
on the medium-sized problems.
We run 10 independent experiments and record the averaged results for comparison. The averaged computational
time and RTE are listed in Table 1 and Table 2, respectively.
As can be found from Table 1, in general, the computational time of RP is 4 times faster than that of LRGeomCG,
and 3 times faster than that of qGeomMC, respectively. In
addition, as can be seen from Table 2, with the same training error, RP is also slightly better than the other two baseline methods in terms of the relative testing error. Notice
that here we choose r = rb for LRGeomCG and qGeomMC,
which is the optimal choice for rank estimation.
Table 2. Averaged relative testing error on big matrices.
Data Type
r
LRGeomCG
qGeomMC
RP

sg
50
0.0374
0.0351
0.0316

sœá2
100
0.0388
0.0316
0.0306

50
0.074
0.120
0.031

100
0.0695
0.0347
0.0275

5.6. Real-World Experiments
In this section, we compare RP with the baseline methods on two real-world large-scale collaborative filtering datasets: MovieLens with 10M ratings (denoted by
Movie-10M) (Herlocker et al., 1999) and Netflix Prize
dataset (KDDCup, 2007). Movie-10M contains 10M ratings given by 71,567 users on 10,681 movies while Netflix Prize contains 100,480,507 ratings given by 480,189
users on 17,770 movies. The baseline methods include

Riemannian Pursuit for Big Matrix Recovery
LRGeomCG

LMaFit

LMaFit‚àíA

qGeomMC

SVP

RP

200

10

APG
LMaFit‚àíA
RP
Ground Truth

‚àí2

10

2

2.5

3

2

2.2

2.4

2.6

160

Estimated Rank

‚àí1

10

Time (seconds in log scale)

180

Time (seconds in log scale)

Relative Testing Error (in log scale)

APG
0

2

10

2.8

3

3.2

Oversampling factor Œ∂os

3.4

3.6

3.8

140
120
100
80

460
40

1

3.5

Oversampling factor Œ∂os

4

10

2

2.5

3

Oversampling factor Œ∂os

3.5

4

2

2.5

3

Oversampling factor Œ∂

3.5

4

os

(a) Relative testing error (in log scale) (b) Training time (seconds in log scale)

(c) Estimated rank

Figure 2. Comparison on medium-sized problems of rank rb = 50. The results are obtained by averaging over 10 independent trials.

APG, LRGeomCG, qGeomMC, Lmafit, Lmafit-A, GECO,
Jaggi‚Äôs method and Laue‚Äôs method.
In general, collaborative filtering data are very noisy. As a
result, the singular values of the matrices tend to be longtail. Therefore, we need to set larger stopping tolerances to
alleviate over-fitting. For this experiment, we set ŒªF = 0.2
and out = 10‚àí4 . We set Œ∑ = 0.65, and the detected ranks
by RP are used as the rank estimations for the fixed-rank
methods, namely LRGeomCG, qGeomMC and LMaFIT.
Finally, we constrain the maximum rank for all methods to
100. For comparison, we report the testing RMSE of different methods over 10 random 80/20 train/test partitions
as explained in (Laue, 2012).
Comparison results are shown in Table 3. From the table,
we can observe that RP performs the best among all the
methods in terms of RMSE and computational efficiency.
It is worth mentioning that we use the rank detected by
RP as the rank estimation for LRGeomCG and qGeomMC.
Therefore, RP can be much faster than these two methods
if the cost of the model selection is considered.
Table 3. Experimental results on real-world datasets.
Dataset
APG
LRGeomCG
QgeomMC
Lmafit
Lmafit-A
RP
*

RMSE
1.096
0.824
0.850
0.837
0.969
0.817

Movie-10M
Time (seconds)
1048¬±17
338¬±11
189¬±7
307¬±1
421¬±16
81¬±1

RMSE
0.867
0.880
0.875
0.962
0.859

Netflix
Time (seconds)
3128¬±35
3965 ¬± 74
3798¬±50
5286¬±165
1332¬±27

Result of APG on Netflix is absent due to out-of-memory issue. The standard variations of RMSE are not reported since they are not significant.
The average ranks estimated by APG, Lmafit-A and RP on Movie are 100,
77 and 10, respectively. The average ranks estimated by Lmafit-A and RP
on Netflix are 81 and 12, respectively.

Due to the absence of source codes, we record the
published results of GECO (Shwartz et al., 2011),
Jaggi‚Äôs method (Jaggi & Sulovsky, 2010) and Laue‚Äôs
method (Laue, 2012), on the Movie-10M dataset. The experimental settings of these methods reported in the liter-

atures are similar to ours, thus the comparison is fair. In
addition, we list the training time, the times of speedup,
RMSE and the CPU details in Table 4 for reference.
Table 4. Performance comparison on Movie-10M dataset.
Methods
GECO
Laue
Jaggi
RP

Time (in seconds)
784,941
2,663
3,120
81

SpeedUp
9,000x
30x
38x
‚Äì

RMSE
0.821
0.815
0.862
0.817

CPU(GHz)
2.5
2.5
2.4
2.8

From Table 4, we observe that on the Movie-10M dataset,
RP obtains comparable or better performance to the baseline methods in terms of RMSE, but can achieve great
speedup with similar CPUs. Particularly, RP is orders of
magnitude faster than all the other methods. With these
comparisons, we can conclude that RP can achieve much
faster training speed over the comparison methods.

6. Conclusion
We propose a Riemannian Pursuit (RP) method for tackling
big MR problems. In contrast to nuclear-norm based methods, RP only needs to compute rank-œÅ truncated SVD with
œÅ very small per iteration, as opposed to APG which may
take hundreds of high-dimensional SVDs. By exploiting
the Riemannian geometry of the fixed-rank manifold, RP
uses a more efficient master solver. Moreover, RP increases
the rank of the matrix by œÅ > 1 per iteration, thus it exhibits
good scalability for big MR problems with large ranks. Finally, RP automatically detects the rank with appropriate
stopping conditions, and performs well on ill-conditioned
problems. Extensive experimental results show that RP
achieves superb scalability and maintain similar or better
MR performance compared with state-of-the-art methods.

Acknowledgments
This research was in part supported by the Singapore NTU
A*SERC under Grant 112 172 0013, and the Australian
Research Council Future Fellowship FT130100746.

Riemannian Pursuit for Big Matrix Recovery

References
Absil, P.-A. and Mahony, R. and Sepulchre, R. Optimization
Algorithms on Matrix Manifolds. Princeton University Press,
2008.
Boumal, N. and Absil, P.-A. Rtrmc: A Riemannian trust-region
method for low-rank matrix completion. In NIPS, 2012.
Cai, J., CandeÃÄs, J., E., and Shen, Z. A singular value thresholding
algorithm for matrix completion. SIAM J. on Optim., 20(4):
1956‚Äì1982, 2010.
CandeÃÄs, E. J. and Plan, Y. Tight oracle bounds for low-rank matrix
recovery from a minimal number of random measurements.
IEEE Trans. on Inform. Theory, 57(4):2342‚Äì2359, 2010a.
CandeÃÄs, E. J. and Plan, Y. Matrix completion with noise. Proceedings of the IEEE, 98(6):925‚Äì936, 2010b.
CandeÃÄs, E. J. and Recht, B. Exact matrix completion via convex
optimization. Found. Comput. Math., 9:717‚Äì772, 2009.
Dieci, L. and Eirola, T. On smooth decompositions of matrices.
SIAM J. Numer. Anal., 20(3):800-819, 1999.
Donoho, D. L., Tsaig, Y., Drori, I., and Starck, J. L. Sparse solution of underdetermined systems of linear equations by stagewise orthogonal matching pursuit. IEEE Trans. Info. Theory,
58(2):1094‚Äì1121, 2012.
Fazel, M. Matrix rank minimization with applications. 2002. PhD
thesis, Stanford University.
Golub, G. H. and Van Loan, C. F. Matrix Computations. JHU
Press, 3rd edition, 1996.
Halko, N. and Martinsson, P. G. and Tropp, J. A. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions SIAM Review, 53(2):
217‚Äì288, 2011
Hazan, E. Sparse approximate solutions to semidefinite programs.
LATIN, pp. 306‚Äì316, 2008.
Herlocker, J. L., Konstan, J. A., Borchers, A., and Riedl, J. An
algorithmic framework for performing collaborative filtering.
In SIGIR, 1999.
Jaggi, M. and Sulovsky, M. A simple algorithm for nuclear norm
regularized problems. In ICML, 2010.
KDDCup. ACM SIGKDD and Netflix. In Proceedings of KDD
Cup and Workshop, 2007.
Keshavan, R. H., Montanari, A., and Oh, S. Matrix completion
from a few entries. IEEE Trans. on Info. Theory, 56:2980‚Äì
2998, 2010a.
Keshavan, R. H, Montanari, A., and Oh, S. Matrix completion
from noisy entries. JMLR, 99:2057‚Äì2078, 2010b.
Laue, S. A hybrid algorithm for convex semidefinite optimization.
In ICML, 2012.
Lee, K. and Bresler, Y. ADMiRA: Atomic decomposition for
minimum rank approximation. IEEE Trans. on Inform. Theory,
56(9):4402‚Äì4416, 2010.
Lin, Z., Chen, M., and Ma, Y. The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices. Technical report, UIUC, 2010.
Lin, Z., Liu, R., and Su, Z. Linearized alternating direction method with adaptive penalty for low-rank representation.
arXiv preprint arXiv:1109.0367, 2011.
Meka, R., Jain, P., and Dhillon, I. S. Guaranteed rank minimization via singular value projection. Technical report, 2009a.
Meka, R., Jain, P., and I.S.Dhillon. Guaranteed rank minimization
via singular value projection. In NIPS, 2009b.

Meyer, G., Bonnabel, S., and Sepulchre, R. Linear regression under fixed-rank constraints: A Riemannian approach. In ICML,
2011.
Mishra, B., Apuroop, K. A., and Sepulchre, R. A Riemannian
geometry for low-rank matrix completion. Technical report,
2012.
Mishra, B., Meyer, G., Bach, F., and Sepulchre, R. Low-rank
optimization with trace norm penalty. SIAM J. Optim., 23(4):
2124‚Äì2149, 2013.
Mitra, K., Sheorey, S., and Chellappa, R. Large-scale matrix factorization with missing data under additional constraints. In
NIPS, 2010.
Negahban, S. and Wainwright, M. J. Restricted strong convexity
and weighted matrix completion: Optimal bounds with noise.
JMLR, 13:1665‚Äì1697, 2012.
Ngo, T. T. and Saad, Y. Scaled gradients on Grassmann manifolds
for matrix completion. In NIPS, 2012.
Larsen, R. M. PROPACK‚ÄîSoftware for large and sparse SVD
calculations
http://soi.stanford.edu/Àúrmunk/
PROPACK, 2004
Recht, B. A simpler approach to matrix completion. JMLR, pp.
3413‚Äì3430, 2011.
Recht, B., Fazel, M., and Parrilo, P. A. Guaranteed minimum
rank solutions to linear matrix equations via nuclear norm minimization. SIAM Rev., 52(3), 2010.
Ring, W. and Wirth, B. Optimization methods on Riemannian
manifolds and their application to shape space. SIAM J. Optim.,
22(2):596‚Äì627, 2012.
Sato, H. and Iwai, T. A new, globally convergent Riemannian conjugate gradient method. Optimization: A Journal of Mathematical Programming and Operations Research, (ahead-of-print):
1‚Äì21, 2013.
Selvan, S. E., Amato, U., Gallivan, K. A., Qi, Ch., Carfora, M. F.,
Larobina, M., and Alfano, B. Descent algorithms on oblique
manifold for source-adaptive ica contrast. IEEE Trans. Neural
Netw. Learning Syst., 23(12):1930‚Äì1947, 2012.
Shalit, U., Weinshall, D., and Chechik, G. Online learning in the
embedded manifold of low-rank matrices. JMLR, 13:429‚Äì458,
2012.
Shwartz, S. S., Gonen, A., and Shamir, O. Large-scale convex
minimization with a low-rank constraint. In ICML, 2011.
Toh, K.-C. and Yun, S. An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems. Pac.
J. Optim., 6:615‚Äì640, 2010.
Vandereycken, B. Low-rank matrix completion by Riemannian
optimization. SIAM J. Optim., 23(2):1214‚Äì1236, 2013.
Waters, A. E., Sankaranarayanan, A. C., and Baraniuk, Richard G.
Sparcs: Recovering low-rank and sparse matrices from compressive measurements. In NIPS, 2011.
Wen, Z., Yin, W., and Zhang, Y. Solving a low-rank factorization
model for matrix completion by a non-linear successive overrelaxation algorithm. Math. Program. Comput., 4(4):333‚Äì361,
2012.
Yang, J. and Yuan, X. Linearized augmented Lagrangian and
alternating direction methods for nuclear norm minimization.
Mathematics of Computation, 82(281):301‚Äì329, 2013.

