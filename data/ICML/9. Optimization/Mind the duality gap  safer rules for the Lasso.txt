Mind the duality gap: safer rules for the Lasso

Olivier Fercoq
Alexandre Gramfort
Joseph Salmon
Institut Mines-TeÌleÌcom, TeÌleÌcom ParisTech, CNRS LTCI
46 rue Barrault, 75013, Paris, France

OLIVIER . FERCOQ @ TELECOM - PARISTECH . FR
ALEXANDRE . GRAMFORT @ TELECOM - PARISTECH . FR
JOSEPH . SALMON @ TELECOM - PARISTECH . FR

Abstract

one can mention dictionary learning (Mairal, 2010), biostatistics (Haury et al., 2012) and medical imaging (Lustig
et al., 2007; Gramfort et al., 2012) to name a few.

Screening rules allow to early discard irrelevant
variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In
this paper, we propose new versions of the socalled safe rules for the Lasso. Based on duality
gap considerations, our new rules create safe test
regions whose diameters converge to zero, provided that one relies on a converging solver. This
property helps screening out more variables, for
a wider range of regularization parameter values.
In addition to faster convergence, we prove that
we correctly identify the active sets (supports)
of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine
learning use cases. Significant computing time
reductions are obtained with respect to previous
safe rules.

1. Introduction
Since the mid 1990â€™s, high dimensional statistics has attracted considerable attention, especially in the context of
linear regression with more explanatory variables than observations: the so-called p Ä… n case. In such a context, the
least squares with `1 regularization, referred to as the Lasso
(Tibshirani, 1996) in statistics, or Basis Pursuit (Chen et al.,
1998) in signal processing, has been one of the most popular tools. It enjoys theoretical guarantees (Bickel et al.,
2009), as well as practical benefits: it provides sparse solutions and fast convex solvers are available. This has made
the Lasso a popular method in modern data-science toolkits. Among successful fields where it has been applied,
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

Many algorithms exist to approximate Lasso solutions, but
it is still a burning issue to accelerate solvers in high dimensions. Indeed, although some other variable selection
and prediction methods exist (Fan & Lv, 2008), the best
performing methods usually rely on the Lasso. For stability selection methods (Meinshausen & BuÌˆhlmann, 2010;
Bach, 2008; Varoquaux et al., 2012), hundreds of Lasso
problems need to be solved. For non-convex approaches
such as SCAD (Fan & Li, 2001) or MCP (Zhang, 2010),
solving the Lasso is often a required preliminary step (Zou,
2006; Zhang & Zhang, 2012; CandeÌ€s et al., 2008).
Among possible algorithmic candidates for solving the
Lasso, one can mention homotopy methods (Osborne et al.,
2000), LARS (Efron et al., 2004), and approximate homotopy (Mairal & Yu, 2012), that provide solutions for the full
Lasso path, i.e., for all possible choices of tuning parameter Î». More recently, particularly for p Ä… n, coordinate
descent approaches (Friedman et al., 2007) have proved to
be among the best methods to tackle large scale problems.
Following the seminal work by El Ghaoui et al. (2012),
screening techniques have emerged as a way to exploit the
known sparsity of the solution by discarding features prior
to starting a Lasso solver. Such techniques are coined safe
rules when they screen out coefficients guaranteed to be
zero in the targeted optimal solution. Zeroing those coefficients allows to focus more precisely on the non-zero ones
(likely to represent signal) and helps reducing the computational burden. We refer to (Xiang et al., 2014) for a concise
introduction on safe rules. Other alternatives have tried to
screen the Lasso relaxing the â€œsafetyâ€. Potentially, some
variables are wrongly disregarded and post-processing is
needed to recover them. This is for instance the strategy
adopted for the strong rules (Tibshirani et al., 2012).
The original basic safe rules operate as follows: one

Mind the duality gap: safer rules for the Lasso

chooses a fixed tuning parameter Î», and before launching
any solver, tests whether a coordinate can be zeroed or not
(equivalently if the corresponding variable can be disregarded or not). We will refer to such safe rules as static
safe rules. Note that the test is performed according to a
safe region, i.e., a region containing a dual optimal solution of the Lasso problem. In the static case, the screening
is performed only once, prior any optimization iteration.
Two directions have emerged to improve on static strategies.
â€¢ The first direction is oriented towards the resolution
of the Lasso for a large number of tuning parameters.
Indeed, practitioners commonly compute the Lasso
over a grid of parameters and select the best one in a
data-driven manner, e.g., by cross-validation. As two
consecutive Î»1 s in the grid lead to similar solutions,
knowing the first solution may help improve screening for the second one. We call sequential safe rules
such strategies, also referred to as recursive safe rules
in (El Ghaoui et al., 2012). This road has been pursued in (Wang et al., 2013; Xu & Ramadge, 2013; Xiang et al., 2014), and can be thought of as a â€œwarm
startâ€ of the screening (in addition to the warm start of
the solution itself). When performing sequential safe
rules, one should keep in mind that generally, only an
approximation of the previous dual solution is computed. Though, the safety of the rule is guaranteed
only if one uses the exact solution. Neglecting this issue, leads to â€œunsafeâ€ rules: relevant variables might
be wrongly disregarded.
â€¢ The second direction aims at improving the screening by interlacing it throughout the optimization algorithm itself: although screening might be useless at the
beginning of the algorithm, it might become (more)
efficient as the algorithm proceeds towards the optimal solution. We call these strategies dynamic safe
rules following (Bonnefoy et al., 2014a;b).
Based on convex optimization arguments, we leverage duality gap computations to propose a simple strategy unifying both sequential and dynamic safe rules. We coined
GAP SAFE rules such safe rules.

finite time the active variables of the optimal solution (or
equivalently the inactive variables), and the tests become
more and more precise as the optimization algorithm proceeds. We also show that our new GAP SAFE rules, built
on dual gap computations, are converging safe rules since
their associated safe regions have a diameter converging to
zero. We also explain how our GAP SAFE tests are sequential by nature. Application of our GAP SAFE rules
with a coordinate descent solver for the Lasso problem is
proposed in Section 4. Using standard data-sets, we report
the time improvement compared to prior safe rules.
1.1. Model and notation
We denote by rds the set t1, . . . , du for any integer d P N.
Our observation vector is y P Rn and the design matrix
X â€œ rx1 , Â¨ Â¨ Â¨ , xp s P RnË†p has p explanatory variables (or
features) column-wise. We aim at approximating y as a
linear combination of few variables xj â€™s, hence expressing
y as XÎ² where Î² P Rp is a sparse vector. The standard
Euclidean norm is written } Â¨ }, the `1 norm } Â¨ }1 , the `8
norm } Â¨ }8 , and the matrix transposition of a matrix Q is
denoted by QJ . We denote ptq` â€œ maxp0, tq.
For such a task, the Lasso is often considered (see
BuÌˆhlmann & van de Geer (2011) for an introduction). For a
tuning parameter Î» Ä… 0, controlling the trade-off between
data fidelity and sparsity of the solutions, a Lasso estimator
Î²Ì‚ pÎ»q is any solution of the primal optimization problem
Î²Ì‚ pÎ»q P arg min
Î²PRp

1
2
kXÎ² Â´ yk2 ` Î» kÎ²k1 .
2
looooooooooooomooooooooooooon

(1)

â€œPÎ» pÎ²q



(

Denoting âˆ†X â€œ Î¸ P Rn : xJ
j Î¸ Ä 1, @j P rps the
dual feasible set, a dual formulation of the Lasso reads (see
for instance Kim et al. (2007) or Xiang et al. (2014)):
1
Î»2 
y

2
2
Î¸Ì‚pÎ»q â€œ arg max kyk2 Â´
(2)
Î¸ Â´  .
2
2
Î» 2
Î¸Pâˆ†X Ä‚Rn looooooooooooomooooooooooooon
â€œDÎ» pÎ¸q

We can reinterpret Eq. (2) as Î¸Ì‚pÎ»q â€œ Î âˆ†X py{Î»q, where
Î C refers to the projection onto a closed convex set C. In
particular, this ensures that the dual solution Î¸Ì‚pÎ»q is always
unique, contrarily to the primal Î²Ì‚ pÎ»q .

The main contributions of this paper are 1) the introduction
of new safe rules which demonstrate a clear practical improvement compared to prior strategies 2) the definition of
a theoretical framework for comparing safe rules by looking at the convergence of their associated safe regions.

1.2. A KKT detour

In Section 2, we present the framework and the basic concepts which guarantee the soundness of static and dynamic
screening rules. Then, in Section 3, we introduce the new
concept of converging safe rules. Such rules identify in

The Karush-Khun-Tucker (KKT) conditions state:
#
pÎ»q
pÎ»q
tsignpÎ²Ì‚j qu if Î²Ì‚j â€° 0,
J pÎ»q
@j P rps, xj Î¸Ì‚ P
(4)
pÎ»q
rÂ´1, 1s
if Î²Ì‚j â€œ 0.

For the Lasso problem, a primal solution Î²Ì‚ pÎ»q P Rp and the
dual solution Î¸Ì‚pÎ»q P Rn are linked through the relation:
y â€œ X Î²Ì‚ pÎ»q ` Î»Î¸Ì‚pÎ»q .

(3)

Mind the duality gap: safer rules for the Lasso
Rule

Center

Static Safe (El Ghaoui et al., 2012)

y{Î»

Dynamic ST3 (Xiang et al., 2011)

y{Î» Â´ Î´xj â€¹
y{Î»

Dynamic Safe (Bonnefoy et al., 2014a)
Î¸Ì‚

Sequential (Wang et al., 2013)

pÎ»tÂ´1 q

Î¸k

GAP SAFE sphere (proposed)

Radius
y
q
q
RÎ» p Î»max

Ingredients
1
2

qÎ» pÎ¸k q2 Â´ Î´ 2 q
pR
qÎ» pÎ¸k q
R
Ë‡
Ë‡
Ë‡
Ë‡ 1
Ë‡ Î»tÂ´1 Â´ Î»1t Ë‡ }y}
a
rÎ»t pÎ²k , Î¸k q â€œ Î»1t 2GÎ»t pÎ²k , Î¸k q

Î»max â€œ }X J y}8 â€œ |xJ
j â€¹ y|
`Î»
Ë˜
max
Î´â€œ
Â´ 1 {}xj â€¹ }
Î»
Î¸k P âˆ†X (e.g., as in (11) )
exact Î¸Ì‚pÎ»tÂ´1 q required
dual gap for Î²k , Î¸k

Table 1. Review of some common safe sphere tests.

See for instance (Xiang et al., 2014) for more details. The
KKT conditions lead to the fact that for Î» Ä› Î»max â€œ
}X J y}8 , 0 P Rp is a primal solution. It can be considered as the mother of all safe screening rules. So from now
on, we assume that Î» Ä Î»max for all the considered Î»â€™s.

2. Safe rules
Safe rules exploit the KKT condition (4). This equation impÎ»q
pÎ»q
plies that Î²Ì‚j â€œ 0 as soon as |xJ
| Äƒ 1. The main chalj Î¸Ì‚
lenge is that the dual optimal solution is unknown. Hence, a
safe rule aims at constructing a set C Ä‚ Rn containing Î¸Ì‚pÎ»q .
We call such a set C a safe region. Safe regions are all the
more helpful that for many jâ€™s, ÂµC pxj q :â€œ supÎ¸PC |xJ
j Î¸| Äƒ
pÎ»q

1, hence for many jâ€™s, Î²Ì‚j

â€œ 0.

Practical benefits are obtained if one can construct a region
C for which it is easy to compute its support function, denoted by ÏƒC and defined for any x P Rn by:
ÏƒC pxq â€œ max xJ Î¸ .

(5)

Î¸PC

Cast differently, for any safe region C, any j P rps, and any
primal optimal solution Î²Ì‚ pÎ»q , the following holds true:
pÎ»q

If ÂµC pxj q â€œ maxpÏƒC pxj q, ÏƒC pÂ´xj qq Äƒ 1 then Î²Ì‚j â€œ 0.
(6)
We call safe test or safe rule, a test associated to C and
screening out explanatory variables thanks to Eq. (6).
Remark 1. Reminding that the support function of a set is
the same as the support function of its closed convex hull
(Hiriart-Urruty & LemareÌchal, 1993)[Proposition V.2.2.1],
we restrict our search to closed convex safe regions.
Based on a safe region C one can partition the explanatory
variables into a safe active set AÎ» pCq and a safe zero set
Z Î» pCq where:
ApÎ»q pCq â€œ tj P rps : ÂµC pxj q Ä› 1u,

(7)

Z pÎ»q pCq â€œ tj P rps : ÂµC pxj q Äƒ 1u.

(8)

Remark 2. If C â€œ tÎ¸Ì‚pÎ»q u, the safe active set is the equicorpÎ»q
relation set ApÎ»q pCq â€œ EÎ» :â€œ tj P rps : |xJ
| â€œ 1u
j Î¸Ì‚
(in most cases (Tibshirani, 2013) it is exactly the active set
of Î²Ì‚ pÎ»q ). If the Lasso has a unique solution, its support
is exactly the equicorrelation set. If it is not unique, the
equicorrelation set contains all the solutionsâ€™ supports and
there exists a Lasso solution whose support is exactly this
set (Tibshirani, 2013)[Lemma 12]. The other extreme case
is when C â€œ âˆ†X , and ApÎ»q pCq â€œ rps. Here, no variable is
screened out: Z pÎ»q pCq â€œ H and the screening is useless.
We now consider common safe regions whose support
functions are easy to obtain in closed form. For simplicity
we focus only on balls and domes, though more complicated regions could be investigated (Xiang et al., 2014).
2.1. Sphere tests
Following previous work on safe rules, we call sphere tests,
tests relying on balls as safe regions. For a sphere test, one
chooses a ball containing Î¸Ì‚pÎ»q with center c and radius r,
i.e., C â€œ Bpc, rq. Due to their simplicity, safe spheres have
been the most commonly investigated safe regions (see for
instance Table 1 for a brief review). The corresponding test
is defined as follows:
pÎ»q

If ÂµBpc,rq pxj q â€œ |xJ
j c| ` r}xj } Äƒ 1, then Î²Ì‚j

â€œ 0. (9)

Note that for a fixed center, the smaller the radius, the better
the safe screening strategy.
Example 1. The first introduced sphere test (El Ghaoui
et al., 2012) consists in using the center c â€œ y{Î» and radius
r â€œ |1{Î» Â´ 1{Î»max |}y}. Given that Î¸Ì‚pÎ»q â€œ Î âˆ†X py{Î»q,
this is a safe region since y{Î»max P âˆ†X and }y{Î»max Â´
Î âˆ†X py{Î»q} Ä }y}|1{Î» Â´ 1{Î»max |. However, one can
check that this static safe rule is useless as soon as
Ëœ
Â¸
1 ` |xJ
Î»
j y|{p}xj }}y}q
Ä min
.
(10)
Î»max
1 ` Î»max {p}xj }}y}q
jPrps

pÎ»q

Note that for nested safe regions C1 Ä‚ C2 then A pC1 q Ä‚
ApÎ»q pC2 q. Consequently, a natural goal is to find safe regions as small as possible: narrowing safe regions can only
increase the number of screened out variables.

2.2. Dome tests
Other popular safe regions are domes, the intersection between a ball and a half-space. This kind of safe region has

Mind the duality gap: safer rules for the Lasso

of the primal) then with the previous display and (3), we
can show that limkÃ‘`8 Î¸k â€œ Î¸Ì‚pÎ»q . Moreover, the convergence of the primal is unaltered by safe rules: screening
out unnecessary coefficients of Î²k , can only decrease the
distance between Î²k and Î²Ì‚ pÎ»q .

Figure 1. Representation of the dome Dpc, r, Î±, wq (dark blue).
In our case, note that Î± is positive.

been considered for instance in (El Ghaoui et al., 2012; Xiang & Ramadge, 2012; Xiang et al., 2014; Bonnefoy et al.,
2014b). We denote Dpc, r, Î±, wq the dome with ball center
c, ball radius r, oriented hyperplane with unit normal vector w and parameter Î± such that c Â´ Î±rw is the projection
of c on the hyperplane (see Figure 1 for an illustration in
the interesting case Î± Ä… 0).
Remark 3. The dome is non-trivial whenever Î± P rÂ´1, 1s.
When Î± â€œ 0, one gets simply a hemisphere.
For the dome test one needs to compute the support function for C â€œ Dpc, r, Î±, wq. Interestingly, as for balls, it can
be obtained in a closed form. Due to its length though, the
formula is deferred to the Appendix (see also (Xiang et al.,
2014)[Lemma 3] for more details).
2.3. Dynamic safe rules
For approximating a solution Î²Ì‚ pÎ»q of the Lasso primal
problem PÎ» , iterative algorithms are commonly used. We
denote Î²k P Rp the current estimate after k iterations of
any iterative algorithm (see Section 4 for a specific study
on coordinate descent). Dynamic safe rules aim at discovering safe regions that become narrower as k increases. To
do so, one first needs dual feasible points: Î¸k P âˆ†X . Following El Ghaoui et al. (2012) (see also (Bonnefoy et al.,
2014a)), this can be achieved by a simple transformation of
the current residuals Ïk â€œ y Â´ XÎ²k , defining Î¸k as
#
Î¸k â€œ Î±k Ïkâ€,
Â´ J
Â¯
Ä±
y Ïk
Â´1
1
,
Î±k â€œ min max Î»kÏ
2 , kX J Ï k
J
kX Ïk k8 .
k 8
kk
(11)
Such dual feasible Î¸k is proportional to Ïk , and is the closest point (for the norm } Â¨ }) to y{Î» in âˆ†X with such a property, i.e., Î¸k â€œ Î âˆ†X XSpanpÏk q py{Î»q. A reason for choosing
this dual point is that the dual optimal solution Î¸Ì‚pÎ»q is the
projection of y{Î» on the dual feasible set âˆ†X , and the optimal Î¸Ì‚pÎ»q is proportional to y Â´ X Î²Ì‚ pÎ»q , cf. Equation (3).
Remark 4. Note that if limkÃ‘`8 Î²k â€œ Î²Ì‚ pÎ»q (convergence

Example 2. Note that any dual feasible point Î¸ P âˆ†X immediately provides a ball that contains Î¸Ì‚pÎ»q since


 

y
 1 y 

 pÎ»q y 
qÎ» pÎ¸q.
Â´
Â´
Ä
Î¸
Î¸

 :â€œ R
Î¸Ì‚ Â´  â€œ min
Î¸ 1 Pâˆ†X
Î»
Î»
Î»
(12)
`
Ë˜
qÎ» pÎ¸k q corresponds to the simplest safe
The ball B y{Î», R
region introduced in (Bonnefoy et al., 2014a;b) (cf. Figure 2
for more insights). When the algorithm proceeds, one expects that Î¸k gets closer to Î¸Ì‚pÎ»q , so }Î¸k Â´ y{Î»} should get
closer to }Î¸Ì‚pÎ»q Â´ y{Î»}. Similarly to Example 1, this dynamic rule becomes useless once Î» is too small. More precisely, this occurs as soon as
Â¸
Ëœ
1 ` |xJ
Î»
j y|{p}xj }}y}q
.
Ä min
Î»max
jPrps
Î»max }Î¸Ì‚pÎ»q }{}y} ` Î»max {p}xj }}y}q
(13)
Noticing that }Î¸Ì‚pÎ»q } Ä }y{Î»} (since Î âˆ†X is a contraction
and 0 P âˆ†X ) and proceeding as for (10), one can show that
this dynamic safe rule is inefficient when:
Â¸
Ëœ
|xJ
Î»
j y|
Ä min
.
(14)
Î»max
Î»max
jPrps
This is a critical threshold, yet the screening might stop
even at a larger Î» thanks to Eq. (13). In practice the bound
in Eq. (13) cannot be evaluated a priori due to the term
}Î¸Ì‚pÎ»q }). Note also that the bound in Eq. (14) is close to the
one in Eq. (10), explaining the similar behavior observed
in our experiments (see Figure 3 for instance).

3. New contributions on safe rules
3.1. Support discovery in finite time
Let us first introduce the notions of converging safe regions
and converging safe tests.
Definition 1. Let pCk qkPN be a sequence of closed convex
sets in Rn containing Î¸Ì‚pÎ»q . It is a converging sequence of
safe regions for the Lasso with parameter Î» if the diameters
of the sets converge to zero. The associated safe screening
rules are referred to as converging safe tests.
Not only converging safe regions are crucial to speed up
computation, but they are also helpful to reach exact active
set identification in a finite number of steps. More precisely, we prove that one recovers the equicorrelation set of
the Lasso (cf. Remark 2) in finite time with any converging strategy: after a finite number of steps, the equicorrelation set EÎ» is exactly identified. Such a property is

Mind the duality gap: safer rules for the Lasso

(a) Location of the dual optimal (b) Refined location of the dual (c) Proposed GAP SAFE sphere (d) Proposed GAP SAFE dome
(orange).
(orange).
optimal Î¸Ì‚pÎ»q (dark blue).
Î¸Ì‚pÎ»q in the annulus.
Figure 2. Our new GAP SAFE sphere and dome (in orange). The dual optimal solution Î¸Ì‚pÎ»q must lie in the dark blue region; Î² is any
point in Rp , and Î¸ is any point in the dual feasible set âˆ†X . Remark that the GAP SAFE dome is included in the GAP SAFE sphere, and
that it is the convex hull of the dark blue region.

sometimes referred to as finite identification of the support
(Liang et al., 2014). This is summarized in the following.
Theorem 1. Let pCk qkPN be a sequence of converging
safe regions. The estimated support provided by Ck ,
ApÎ»q pCk q â€œ tj P rps : maxÎ¸PCk |Î¸J xj | Ä› 1u, satisfies
limkÃ‘8 ApÎ»q pCk q â€œ EÎ» , and there exists k0 P N such that
@k Ä› k0 one gets ApÎ»q pCk q â€œ EÎ» .
Proof. The main idea of the proof is to use that
limkÃ‘8 Ck â€œ tÎ¸Ì‚pÎ»q u, limkÃ‘8 ÂµCk pxq â€œ ÂµtÎ¸Ì‚pÎ»q u pxq â€œ
|xJ Î¸Ì‚pÎ»q | and that the set ApÎ»q pCk q is discrete. Details are
delayed to the Appendix.
Remark 5. A more general result is proved for a specific algorithm (Forward-Backward) in Liang et al. (2014).
Interestingly, our scheme is independent of the algorithm
considered (e.g., Forward-Backward (Beck & Teboulle,
2009), Primal Dual (Chambolle & Pock, 2011), coordinatedescent (Tseng, 2001; Friedman et al., 2007)) and relies
only on the convergence of a sequence of safe regions.
3.2. GAP SAFE regions: leveraging the duality gap
In this section, we provide new dynamic safe rules built on
converging safe regions.
Theorem 2. Let us take any pÎ², Î¸q P Rp Ë† âˆ†X . Denote
`
Ë˜
pÎ» pÎ²q :â€œ 1 kyk2 Â´kXÎ² Â´ yk2 Â´2Î» kÎ²k 1{2 , R
qÎ» pÎ¸q :â€œ
R
1 `
Î»
pÎ»q
kÎ¸ Â´ y{Î»k , Î¸Ì‚b
the dual optimal Lasso solution and
q
pÎ» pÎ²q2 , then
rÌƒÎ» pÎ², Î¸q :â€œ RÎ» pÎ¸q2 Â´ R

1998) for a reminder on weak and strong duality). Fix
Î¸ P âˆ†X and Î² P Rp , then it holds that
1
Î»2 
y
1

2
2
2
kyk Â´
Î¸ Â´  Ä kXÎ² Â´ yk ` Î» kÎ²k1 .
2
2
Î»
2
Hence,

y


Î¸ Â´  Ä›
Î»

cÂ´
Â¯
2
2
kyk Â´ kXÎ² Â´ yk Â´ 2Î» kÎ²k1

`

Î»

. (16)

pÎ» pÎ²q. ComIn particular, this provides }Î¸Ì‚pÎ»q Â´ y{Î»} Ä› R
pÎ»q
bining (12) and (16), asserts that Î¸Ì‚
belongs to the anqÎ» pÎ¸q, R
pÎ» pÎ²qq :â€œ tz P Rn : R
pÎ» pÎ²q Ä
nulus Apy{Î», R
q
}z Â´ y{Î»} Ä RÎ» pÎ¸qu (the light blue zone in Figure 2).
Remind that the dual feasible set âˆ†X is convex, hence
qÎ» pÎ¸qq is also convex. Thanks to (16),
âˆ†X X Bpy{Î», R
qÎ» pÎ¸qq â€œ âˆ†X XApy{Î», R
qÎ» pÎ¸q, R
pÎ» pÎ²qq, and
âˆ†X XBpy{Î», R
q
p
then âˆ†X X Apy{Î», RÎ» pÎ¸q, RÎ» pÎ²qq is convex too. Hence,
qÎ» pÎ¸q, R
pÎ» pÎ²qq and so
Î¸Ì‚pÎ»q is inside the annulus Apy{Î», R
pÎ»q
qÎ» pÎ¸q, R
pÎ» pÎ²qq by convexity (see
is rÎ¸, Î¸Ì‚ s Ä Apy{Î», R
Figure 2,(a) and Figure 2,(b)). Moreover, Î¸Ì‚pÎ»q is the
point of rÎ¸, Î¸Ì‚pÎ»q s which is closest to y{Î». The farthest
where Î¸Ì‚pÎ»q can be according to this information would be
pÎ» pÎ²qq
if rÎ¸, Î¸Ì‚pÎ»q s were tangent to the inner ball Bpy{Î», R
pÎ»q
p
and }Î¸Ì‚
Â´ y{Î»} â€œ RÎ» pÎ²q. Let us denote Î¸int such a
pÎ» pÎ²q
point. The tangency property reads kÎ¸int Â´ y{Î»k â€œ R
and pÎ¸ Â´ Î¸int qJ py{Î» Â´ Î¸int q â€œ 0. Hence, with the later
qÎ» pÎ¸q, kÎ¸ Â´ y{Î»k2 â€œ kÎ¸ Â´ Î¸int k2 `
and the definition of R
2
2
qÎ» pÎ¸q2 Â´ R
pÎ» pÎ²q2 .
kÎ¸int Â´ y{Î»k and kÎ¸ Â´ Î¸int k â€œ R

(15)

Since by construction Î¸Ì‚pÎ»q cannot be further away from Î¸
than Î¸int (again, insights
from Figure
` can be gleaned
Ë˜ 2), we
qÎ» pÎ¸q2 Â´ R
pÎ» pÎ²q2 q1{2 .
conclude that Î¸Ì‚pÎ»q P B Î¸, pR

Proof. The construction of the ball BpÎ¸, rÌƒÎ» pÎ², Î¸qq is based
on the weak duality theorem (cf. (Rockafellar & Wets,

Remark 6. Choosing Î² â€œ 0 and Î¸ â€œ y{Î»max , then one
recovers the static safe rule given in Example 1.

Â´
Â¯
Î¸Ì‚pÎ»q P B Î¸, rÌƒÎ» pÎ², Î¸q .

Mind the duality gap: safer rules for the Lasso

With the definition of the primal (resp. dual) objective for
PÎ» pÎ²q, (resp. DÎ» pÎ¸q), the duality gap reads as GÎ» pÎ², Î¸q â€œ
PÎ» pÎ²qÂ´DÎ» pÎ¸q. Remind that if GÎ» pÎ², Î¸q Ä , then one has
PÎ» pÎ²q Â´ PÎ» pÎ²Ì‚ pÎ»q q Ä , which is a standard stopping criterion for Lasso solvers. The next proposition establishes a
connection between the radius rÎ» pÎ², Î¸q and the duality gap
GÎ» pÎ², Î¸q.
Proposition 1. For any pÎ², Î¸q P Rp Ë† âˆ†X , the following
holds
rÌƒÎ» pÎ², Î¸q2 Ä rÎ» pÎ², Î¸q2 :â€œ

2
GÎ» pÎ², Î¸q.
Î»2

(17)

qÎ» pÎ¸q2 â€œ kÎ¸ Â´ y{Î»k2 and
Proof. Use the fact that R
2
pÎ» pÎ²q2 Ä› pkyk Â´ kXÎ² Â´ yk2 Â´ 2Î» kÎ²k q{Î»2 .
R
1
If we could choose the â€œoracleâ€ Î¸ â€œ Î¸Ì‚pÎ»q and Î² â€œ Î²Ì‚ pÎ»q
in (15) then we would obtain a zero radius. Since those
quantities are unknown, we rather pick dynamically the
current available estimates given by an optimization algorithm: Î² â€œ Î²k and Î¸ â€œ Î¸k as in Eq. (11). Introducing GAP
SAFE spheres and domes as below, Proposition 2 ensures
that they are converging safe regions.
GAP SAFE sphere:
Ck â€œ B pÎ¸k , rÎ» pÎ², Î¸qq .

(18)

GAP SAFE dome:
Â¨
Ë›
Ëœ
Â¸2
y
y
q
p
Î¸
Â´
`
Î¸
R
pÎ¸
q
R
pÎ²
q
k
k
Î» k
Î» k
Î» â€š
Ck â€œ DË Î»
.
,
,2
Â´ 1,
qÎ» pÎ¸k q
2
2
}Î¸k Â´ Î»y }
R
(19)
Proposition 2. For any converging primal sequence
pÎ²k qkPN , and dual sequence pÎ¸k qkPN defined as in Eq. (11),
then the GAP SAFE sphere and the GAP SAFE dome are
converging safe regions.
Proof. For the GAP SAFE sphere the result follows
from strong duality, Remark 4 and Proposition 1 yield
limkÃ‘8 rÎ» pÎ²k , Î¸k q â€œ 0, since limkÃ‘8 Î¸k â€œ Î¸Ì‚pÎ»q and
limkÃ‘8 Î²k â€œ Î²Ì‚ pÎ»q . For the GAP SAFE dome, one can
check that it is included in the GAP SAFE sphere, therefore
inherits the convergence (see also Figure 2,(c) and (d)).

3.3. GAP SAFE rules : sequential for free
As a byproduct, our dynamic screening tests provide a
warm start strategy for the safe regions, making our GAP
SAFE rules inherently sequential. The next proposition
shows their efficiency when attacking a new tuning parameter, after having solved the Lasso for a previous Î», even
only approximately. Handling approximate solutions is a
critical issue to produce safe sequential strategies: without
taking into account the approximation error, the screening
might disregard relevant variables, especially the one near
the safe regions boundaries. Except for Î»max , it is unrealistic to assume that one can dispose of exact solutions.
Consider Î»0 â€œ Î»max and a non-increasing sequence of
T Â´ 1 tuning parameters pÎ»t qtPrT Â´1s in p0, Î»max q. In practice, we choose the common grid (BuÌˆhlmann & van de
Geer, 2011)[2.12.1]): Î»t â€œ Î»0 10Â´Î´t{pT Â´1q (for instance
in Figure 3, we considered Î´ â€œ 3). The next result controls
how the duality gap, or equivalently, the diameter of our
GAP SAFE regions, evolves from Î»tÂ´1 to Î»t .
Proposition 3. Suppose that t Ä› 1 and pÎ², Î¸q P Rp Ë† âˆ†X .
Reminding rÎ»2 t pÎ², Î¸q â€œ 2GÎ»t pÎ², Î¸q{Î»2t , the following holds
Ë™
Ë†
Î»tÂ´1
2
rÎ»2 tÂ´1 pÎ², Î¸q
(20)
rÎ»t pÎ², Î¸q â€œ
Î»t

2
Î»t 
XÎ² Â´ y 
 Â´ p Î»tÂ´1 Â´ 1q kÎ¸k2 .
` p1 Â´
q
Î»tÂ´1  Î»t 
Î»t
Proof. Details are given in the Appendix.
This proposition motivates to screen sequentially as follows: having pÎ², Î¸q P Rp Ë† âˆ†X such that GÎ»tÂ´1 pÎ², Î¸q Ä ,
then, we can screen using the GAP SAFE sphere with center Î¸ and radius rÎ» pÎ², Î¸q. The adaptation to the GAP SAFE
dome is straightforward and consists in replacing Î¸k , Î²k , Î»
by Î¸, Î², Î»t in the GAP SAFE dome definition.
Remark 8. The basic sphere test of (Wang et al., 2013) requires the exact dual solution Î¸ â€œ Î¸Ì‚pÎ»tÂ´1 q for center, and
has radius |1{Î»t Â´ 1{Î»tÂ´1 | kyk, which is strictly larger than
ours. Indeed, if one has access to dual and primal optimal solutions at Î»tÂ´1 , i.e., pÎ¸, Î²q â€œ pÎ¸Ì‚pÎ»tÂ´1 q , Î²Ì‚ pÎ»tÂ´1 q q, then
rÎ»2 tÂ´1 pÎ², Î¸q â€œ 0, Î¸ â€œ py Â´ XÎ²q{Î»tÂ´1 and
Ë™
Î»2tÂ´1
Î»t
Î»tÂ´1
2
p1
Â´
q
Â´
p
Â´
1q
kÎ¸k ,
Î»2t
Î»tÂ´1
Î»t
Ë™2
Ë†
1
1
2
Ä
Â´
kyk ,
Î»t
Î»tÂ´1
Ë†

Remark 7. The radius rÎ» pÎ²k , Î¸k q can be compared
with the radius considered for the Dynamic Safe rule
and Dynamic ST3 (Bonnefoy et al., 2014a) respectively:
qÎ» pÎ¸k q â€œ }Î¸k Â´ y{Î»}2 and pR
qÎ» pÎ¸k q2 Â´ Î´ 2 q1{2 , where
R
Î´ â€œ pÎ»max {Î» Â´ 1q{ kxj â€¹ k. We have proved that
limkÃ‘8 rÎ» pÎ²k , Î¸k q â€œ 0, but a weaker property is satisfied
qÎ» pÎ¸k q â€œ R
qÎ» pÎ¸Ì‚pÎ»q q â€œ
by the two other radius: limkÃ‘8 R
pÎ»q
2
2
qÎ» pÎ¸Ì‚ q Â´ y{Î»} and limkÃ‘8 pR
qÎ» pÎ¸k q Â´ Î´ 2 q1{2 â€œ
}R
qÎ» pÎ¸Ì‚pÎ»q q2 Â´ Î´ 2 q1{2 Ä… 0.
pR

rÎ»2 t pÎ², Î¸q â€œ

since }Î¸} Ä }y}{Î»tÂ´1 for Î¸ â€œ Î¸Ì‚pÎ»tÂ´1 q .
Note that contrarily to former sequential rules (Wang et al.,
2013), our introduced GAP SAFE rules still work when one
has only access to approximations of Î¸Ì‚pÎ»tÂ´1 q .

Mind the duality gap: safer rules for the Lasso

4. Experiments
4.1. Coordinate Descent
Screening procedures can be used with any optimization
algorithm. We chose coordinate descent because it is well
suited for machine learning tasks, especially with sparse
and/or unstructured design matrix X. Coordinate descent
requires to extract efficiently columns of X which is typically not easy in signal processing applications where X is
commonly an implicit operator (e.g. Fourier or wavelets).
Algorithm 1 Coordinate descent with GAP SAFE rules
input X, y, , K, f, pÎ»t qtPrT Â´1s
Initialization:
Î»0 â€œ Î»max
Î² Î»0 â€œ 0
for t P rT Â´ 1s do
Î² Ã Î² Î»tÂ´1 (previous -solution)
for k P rKs do
if k mod f â€œ 1 then
Compute Î¸ and C thanks to (11) and (18) or (19)
Get AÎ»t pCq â€œ tj P rps : ÂµC pxj q Ä› 1u as in (7)
if GÎ»t pÎ², Î¸q Ä  then
Î² Î»t Ã Î²
break
for j P AÎ»t pCq do
`
xJ pXÎ²Â´yq Ë˜
Î²j Ã ST kxÎ»tk2 , Î²j Â´ j kx k2
j
j
# STpu, xq â€œ signpxq p|x| Â´ uq` (softthreshold)
output pÎ² Î»t qtPrT Â´1s
We implemented the screening rules of Table 1 based on the
coordinate descent in Scikit-learn (Pedregosa et al., 2011).
This code is written in Python and Cython to generate low
level C code, offering high performance. A low level language is necessary for this algorithm to scale. Two implementations were written to work efficiently with both
dense data stored as Fortran ordered arrays and sparse data
stored in the compressed sparse column (CSC) format. Our
pseudo-code is presented in Algorithm 1. In practice, we
perform the dynamic screening tests every f â€œ 10 passes
through the entire (active) variables. Iterations are stopped
when the duality gap is smaller than the target accuracy.
The naive computation
of Î¸k in (11) involves the compu
tation of X J Ïk 8 (Ïk being the current residual), which
costs Opnpq operations. This can be avoided as one knows
when using a safe rule that the index achieving the maximum for this norm is in AÎ»t pCq. Indeed, by construcJ
tion arg maxjPAÎ»t pCq |xJ
j Î¸k | â€œ arg maxjPrps |xj Î¸k | â€œ
J
arg maxjPrps |xj Ïk |. In practice the evaluation of the dual
gap is therefore not a Opnpq but Opnqq where q is the size
of AÎ»t pCq. In other words, using screening also speeds up

Figure 3. Proportion of active variables as a function of Î» and the
number of iterations K on the Leukemia dataset. Better strategies
have longer range of Î» with (red) small active sets.

the evaluation of the stopping criterion.
We did not compare our method against the strong rules
of Tibshirani et al. (2012) because they are not safe and
therefore need complex post-processing with parameters to
tune. Also we did not compare against the sequential rule
of Wang et al. (2013) (e.g., EDDP) because it requires the
exact dual optimal solution of the previous Lasso problem,
which is not available in practice and can prevent the solver
from actually converging: this is a phenomenon we always
observed on our experiments.
4.2. Number of screened variables
Figure 3 presents the proportion of variables screened by
several safe rules on the standard Leukemia dataset. The
screening proportion is presented as a function of the number of iterations K. As the SAFE screening rule of El
Ghaoui et al. (2012) is sequential but not dynamic, for a
given Î» the proportion of screened variables does not depend on K. The rules of Bonnefoy et al. (2014a) are more
efficient on this dataset but they do not benefit much from
the dynamic framework. Our proposed GAP SAFE tests
screen much more variables, especially when the tuning parameter Î» gets small, which is particularly relevant in practice. Moreover, even for very small Î»â€™s (notice the logarithmic scale) where no variable is screened at the beginning
of the optimization procedure, the GAP SAFE rules manage to screen more variables, especially when K increases.
Finally, the figure demonstrates that the GAP SAFE dome
test only brings marginal improvement over the sphere.

Mind the duality gap: safer rules for the Lasso
5

Time (s)

3
2

-log10(duality gap)

-log10(duality gap)

Figure 5. Time to reach convergence using various screening rules
on bag of words from the 20newsgroup dataset (sparse data: with
n â€œ 961, p â€œ 10094).

In all cases, Lasso paths are computed as required to estimate optimal regularization parameters in practice (when
using cross-validation one path is computed for each fold).
For each Lasso path, solutions are obtained for T â€œ 100
values of Î»â€™s, as detailed in Section 3.3. Remark that the
grid used is the default one in both Scikit-Learn and the
glmnet R package. With our proposed GAP SAFE screening we obtain on all datasets substantial gains in computational time. We can already get an up to 3x speedup when
we require a duality gap smaller than 10Â´4 . The interest of the screening is even clearer for higher accuracies:
GAP SAFE sphere is 11x faster than its competitors on the
Leukemia dataset, at accuracy 10Â´8 . One can observe that
with the parameter grid used here, the larger is p compared
to n, the higher is the gain in computation time.
In our experiments, the other safe screening rules did not
show much speed-up. As one can see on Figure 3, those
screening rules keep all the active variables for a wide range
of Î»â€™s. The algorithm is thus faster for large Î»â€™s but slower
afterwards, since we still compute the screening tests. Even
if one can avoid some of these useless computations thanks
to formulas like (14) or (10), the corresponding speed-up

10

8

No screening
SAFE (El Ghaoui et al.)
ST3 (Bonnefoy et al.)
SAFE (Bonnefoy et al.)
GAP SAFE (sphere)
GAP SAFE (dome)

6

180
160
140
120
100
80
60
40
20
0

4

The main interest of variable screening is to reduce computation costs. Indeed, the time to compute the screening
itself should not be larger than the gains it provides. Hence,
we compared the time needed to compute Lasso paths to
prescribed accuracy for different safe rules. Figures 4, 5
and 6 illustrate results on three datasets. Figure 4 presents
results on the dense, small scale, Leukemia dataset. Figure 5 presents results on a medium scale sparse dataset
obtained with bag of words features extracted from the
20newsgroup dataset (comp.graphics vs. talk.religion.misc
with TF-IDF removing English stop words and words occurring only once or more than 95% of the time). Text
feature extraction was done using Scikit-Learn. Figure 6
focuses on the large scale sparse RCV1 (Reuters Corpus
Volume 1) dataset, cf. (Schmidt et al., 2013).

Time (s)

4.3. Gains in the computation of Lasso paths

2

Figure 4. Time to reach convergence using various screening rules
on the Leukemia dataset (dense data: n â€œ 72, p â€œ 7129).

10

8

4

0

6

1
2

0

5
4

8

1

No screening
SAFE (El Ghaoui et al.)
ST3 (Bonnefoy et al.)
SAFE (Bonnefoy et al.)
GAP SAFE (sphere)
GAP SAFE (dome)

6

6

2

4

3

2

Time (s)

4

7

No screening
SAFE (El Ghaoui et al.)
ST3 (Bonnefoy et al.)
SAFE (Bonnefoy et al.)
GAP SAFE (sphere)
GAP SAFE (dome)

-log10(duality gap)

Figure 6. Computation time to reach convergence using different
screening strategies on the RCV1 (Reuters Corpus Volume 1)
dataset (sparse data with n â€œ 20242 and p â€œ 47236).

would not be significant.

5. Conclusion
We have presented new results on safe rules for accelerating algorithms solving the Lasso problem (see Appendix
for extension to the Elastic Net). First, we have introduced
the framework of converging safe rules, a key concept independent of the implementation chosen. Our second contribution was to leverage duality gap computations to create
two safer rules satisfying the aforementioned convergence
properties. Finally, we demonstrated the important practical benefits of those new rules by applying them to standard
dense and sparse datasets using a coordinate descent solver.
Future works will extend our framework to generalized linear model and group-Lasso.

Acknowledgment
We acknowledge the support from Chair Machine Learning for Big Data at TeÌleÌcom ParisTech and from the Orange/TeÌleÌcom ParisTech think tank phi-TAB. This work
benefited from the support of the â€FMJH Program Gaspard
Monge in optimization and operation researchâ€, and from
the support to this program from EDF.

Mind the duality gap: safer rules for the Lasso

References
Bach, F. Bolasso: model consistent Lasso estimation
through the bootstrap. In ICML, 2008.
Beck, A. and Teboulle, M. A fast iterative shrinkagethresholding algorithm for linear inverse problems.
SIAM J. Imaging Sci., 2(1):183â€“202, 2009.
Bickel, P. J., Ritov, Y., and Tsybakov, A. B. Simultaneous
analysis of Lasso and Dantzig selector. Ann. Statist., 37
(4):1705â€“1732, 2009.
Bonnefoy, A., Emiya, V., Ralaivola, L., and Gribonval, R.
A dynamic screening principle for the lasso. In EUSIPCO, 2014a.
Bonnefoy, A., Emiya, V., Ralaivola, L., and Gribonval,
R. Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and Group-Lasso. ArXiv e-prints,
2014b.
BuÌˆhlmann, P. and van de Geer, S. Statistics for highdimensional data. Springer Series in Statistics. Springer,
Heidelberg, 2011. Methods, theory and applications.
CandeÌ€s, E. J., Wakin, M. B., and Boyd, S. P. Enhancing
sparsity by reweighted l1 minimization. J. Fourier Anal.
Applicat., 14(5-6):877â€“905, 2008.
Chambolle, A. and Pock, T. A first-order primal-dual algorithm for convex problems with applications to imaging.
J. Math. Imaging Vis., 40(1):120â€“145, 2011.
Chen, S. S., Donoho, D. L., and Saunders, M. A. Atomic
decomposition by basis pursuit. SIAM J. Sci. Comput.,
20(1):33â€“61 (electronic), 1998.

Gramfort, A., Kowalski, M., and HaÌˆmaÌˆlaÌˆinen, M. Mixednorm estimates for the M/EEG inverse problem using
accelerated gradient methods. Physics in Medicine and
Biology, 57(7):1937â€“1961, 2012.
Haury, A.-C., Mordelet, F., Vera-Licona, P., and Vert, J.P. TIGRESS: Trustful Inference of Gene REgulation using Stability Selection. BMC systems biology, 6(1):145,
2012.
Hiriart-Urruty, J.-B. and LemareÌchal, C. Convex analysis
and minimization algorithms. I, volume 305. SpringerVerlag, Berlin, 1993.
Kim, S.-J., Koh, K., Lustig, M., Boyd, S., and Gorinevsky,
D.
An interior-point method for large-scale l1 regularized least squares. IEEE J. Sel. Topics Signal Process., 1(4):606â€“617, 2007.
Liang, J., Fadili, J., and PeyreÌ, G. Local linear convergence of forwardâ€“backward under partial smoothness. In
NIPS, pp. 1970â€“1978, 2014.
Lustig, M., Donoho, D. L., and Pauly, J. M. Sparse MRI:
The application of compressed sensing for rapid MR
imaging. Magnetic Resonance in Medicine, 58(6):1182â€“
1195, 2007.
Mairal, J. Sparse coding for machine learning, image processing and computer vision. PhD thesis, EÌcole normale
supeÌrieure de Cachan, 2010.
Mairal, J. and Yu, B. Complexity analysis of the lasso regularization path. In ICML, 2012.
Meinshausen, N. and BuÌˆhlmann, P. Stability selection. J.
Roy. Statist. Soc. Ser. B, 72(4):417â€“473, 2010.

Efron, B., Hastie, T., Johnstone, I. M., and Tibshirani, R.
Least angle regression. Ann. Statist., 32(2):407â€“499,
2004. With discussion, and a rejoinder by the authors.

Osborne, M. R., Presnell, B., and Turlach, B. A. A new
approach to variable selection in least squares problems.
IMA J. Numer. Anal., 20(3):389â€“403, 2000.

El Ghaoui, L., Viallon, V., and Rabbani, T. Safe feature
elimination in sparse supervised learning. J. Pacific Optim., 8(4):667â€“698, 2012.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay,
E. Scikit-learn: Machine learning in Python. J. Mach.
Learn. Res., 12:2825â€“2830, 2011.

Fan, J. and Li, R. Variable selection via nonconcave penalized likelihood and its oracle properties. J. Amer. Statist.
Assoc., 96(456):1348â€“1360, 2001.
Fan, J. and Lv, J. Sure independence screening for ultrahigh
dimensional feature space. J. Roy. Statist. Soc. Ser. B, 70
(5):849â€“911, 2008.

Rockafellar, R. T. and Wets, R. J.-B. Variational analysis,
volume 317 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical
Sciences]. Springer-Verlag, Berlin, 1998.

Friedman, J., Hastie, T., HoÌˆfling, H., and Tibshirani, R.
Pathwise coordinate optimization. Ann. Appl. Stat., 1
(2):302â€“332, 2007.

Schmidt, M., Le Roux, N., and Bach, F. Minimizing finite
sums with the stochastic average gradient. arXiv preprint
arXiv:1309.2388, 2013.

Mind the duality gap: safer rules for the Lasso

Tibshirani, R. Regression shrinkage and selection via the
lasso. J. Roy. Statist. Soc. Ser. B, 58(1):267â€“288, 1996.
Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon,
N., Taylor, J., and Tibshirani, R. J. Strong rules for discarding predictors in lasso-type problems. J. Roy. Statist.
Soc. Ser. B, 74(2):245â€“266, 2012.
Tibshirani, R. J. The lasso problem and uniqueness. Electron. J. Stat., 7:1456â€“1490, 2013.
Tseng, P. Convergence of a block coordinate descent
method for nondifferentiable minimization. J. Optim.
Theory Appl., 109(3):475â€“494, 2001.
Varoquaux, G., Gramfort, A., and Thirion, B. Smallsample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering. In
ICML, 2012.
Wang, J., Zhou, J., Wonka, P., and Ye, J. Lasso screening
rules via dual polytope projection. In NIPS, pp. 1070â€“
1078, 2013.
Xiang, Z. J. and Ramadge, P. J. Fast lasso screening tests
based on correlations. In ICASSP, pp. 2137â€“2140, 2012.
Xiang, Z. J., Xu, H., and Ramadge, P. J. Learning sparse
representations of high dimensional data on large scale
dictionaries. In NIPS, pp. 900â€“908, 2011.
Xiang, Z. J., Wang, Y., and Ramadge, P. J. Screening tests
for lasso problems. arXiv preprint arXiv:1405.4897,
2014.
Xu, P. and Ramadge, P. J. Three structural results on the
lasso problem. In ICASSP, pp. 3392â€“3396, 2013.
Zhang, C.-H. Nearly unbiased variable selection under
minimax concave penalty. Ann. Statist., 38(2):894â€“942,
2010.
Zhang, C.-H. and Zhang, T. A general theory of concave regularization for high-dimensional sparse estimation problems. Statistical Science, 27(4):576â€“593, 2012.
Zou, H. The adaptive lasso and its oracle properties. J. Am.
Statist. Assoc., 101(476):1418â€“1429, 2006.

