Margins, Kernels and Non-linear Smoothed Perceptrons

Aaditya Ramdas
Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213 USA

ARAMDAS @ CS . CMU . EDU

Javier PenÌƒa
Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213 USA

JFP @ ANDREW. CMU . EDU

Abstract
We focus on the problem of finding a non-linear
classification function that lies in a Reproducing Kernel Hilbert Space (RKHS) both from
the primal point of view (finding a perfect separator when one exists) and the dual point of
view (giving a certificate of non-existence), with
special focus on generalizations of two classical schemes - the Perceptron (primal) and VonNeumann (dual) algorithms.
We cast our problem as one of maximizing
the regularized normalized hard-margin (Ï) in
an RKHS and rephrase it in terms of a Mahalanobis dot-product/semi-norm associated with
the kernelâ€™s (normalized and signed) Gram matrix. We derive an accelerated smoothed
algoâˆš
n
rithm with a convergence rate of log
given
Ï
n separable points, which is strikingly similar
to the classical kernelized Perceptron algorithm
whose rate is Ï12 . When no such classifier exists,
we prove a version of Gordanâ€™s separation theorem for RKHSs, and give a reinterpretation of
negative margins. This allows us to give guaranteesâˆšfor âˆša primal-dual algorithm that halts in
min{ |Ï|n , n } iterations with a perfect separator
in the RKHS if the primal is feasible or a dual
-certificate of near-infeasibility.

1. Introduction
We are interested in the problem of finding a non-linear
separator for a given set of n points x1 , ..., xn âˆˆ Rd with
labels y1 , ..., yn âˆˆ {Â±1}. Finding a linear separator can be
stated as the problem of finding a unit vector w âˆˆ Rd (if
st

Proceedings of the 31 International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

one exists) such that for all i
yi (w> xi ) â‰¥ 0

i.e.

sign(w> xi ) = yi .

(1)

This is called the primal problem. In the more interesting
non-linear setting, we will be searching for functions f in a
Reproducing Kernel Hilbert Space (RKHS) FK associated
with kernel K (to be defined later) such that for all i
yi f (xi ) â‰¥ 0.

(2)

We say that problems (1), (2) have an unnormalized margin
Ï > 0, if there exists a unit vector w, such that for all i,
yi (w> xi ) â‰¥ Ï or yi f (xi ) â‰¥ Ï.
True to the paperâ€™s title, margins of non-linear separators in
an RKHS will be a central concept, and we will derive interesting smoothed accelerated variants of the Perceptron
algorithm that have convergence rates (for the aforementioned primal and a dual problem introduced later) that are
inversely proportional to the RKHS-margin as opposed to
inverse squared margin for the Perceptron.
The linear setting is well known by the name of linear feasibility problems - we are asking if there exists any vector
w which makes an acute angle with all the vectors yi xi , i.e.
(XY )> w > 0n ,

(3)

where Y := diag(y), X := [x1 , ..., xn ]. This can be seen
as finding a vector w inside the dual cone of cone{yi xi }.
When normalized, as we will see in the next section, the
margin is a well-studied notion of conditioning for these
problems. It can be thought of as the width of the feasibility cone as in (Freund & Vera, 1999), a radius of wellposedness as in (Cheung & Cucker, 2001), and its inverse
can be seen as a special case of a condition number defined
by (Renegar, 1995) for these systems.
1.1. Related Work
In this paper we focus on the famous Perceptron algorithm
(Rosenblatt, 1958) and the less-famous Von-Neumann algorithm (Dantzig, 1992) that we introduce in later sections.

Margins, Kernels and Non-linear Smoothed Perceptron

Our work builds on (Soheili & PenÌƒa, 2012; 2013a) from the
field of optimization - we generalize the setting to learning functions in RKHSs, extend the algorithms, simplify
proofs, and simultaneously bring new perspectives to it.
There is extensive literature around the Perceptron algorithm in the learning community; we restrict ourselves to
discussing only a few directly related papers, in order to
point out the several differences from existing work.
We provide a general unified proof in the Appendix which
borrows ideas from accelerated smoothing methods developed by Nesterov (Nesterov, 2005) - while this algorithm
and others by (Nemirovski, 2004), (Saha et al., 2011) can
achieve similar rates for the same problem, those algorithms do not possess the simplicity of the Perceptron or
Von-Neumann algorithms and our variants, and also donâ€™t
look at the infeasible setting or primal-dual algorithms.
Accelerated smoothing techniques have also been seen in
the learning literature like in (Tseng, 2008) and many others. However, most of these deal with convex-concave
problems where both sets involved are the probability simplex (as in game theory, boosting, etc), while we deal with
hard margins where one of the sets is a unit `2 ball. Hence,
their algorithms/results are not extendable to ours trivially.
This work is also connected to the idea of -coresets (Clarkson, 2010), though we will not explore that angle.
A related algorithm is called the Winnow (Littlestone,
1991) - this works on the `1 margin and is a saddle point
problem over two simplices. One can ask whether such
accelerated smoothed versions exist for the Winnow. The
answer is in the affirmative - however such algorithms look
completely different from the Winnow, while in our setting
the new algorithms retain the simplicity of the Perceptron.

2. Linear Feasibility Problems
2.1. Perceptron
The classical perceptron algorithm can be stated in many
ways, one is in the following form
Algorithm 1 Perceptron
Initialize w0 = 0
for k = 0, 1, 2, 3, ... do
if sign(wk> xi ) 6= yi for some i then
wk+1 := wk + yi xi
else
Halt: Return wk as solution
end if
end for
It comes with the following classic guarantee as proved by
(Block, 1962) and (Novikoff, 1962): If there exists a unit
vector u âˆˆ Rd such that Y X > u â‰¥ Ï > 0, then a perfect
maxi kxi k22
separator will be found in
iterations/mistakes.
Ï2
The algorithm works when updated with any arbitrary point
(xi , yi ) that is misclassified; it has the same guarantees
when w is updated with the point that is misclassified by
the largest amount, arg mini yi w> xi . Alternately, one can
define the probability distribution over examples
p(w) = arg min hY X > w, pi,
pâˆˆâˆ†n

(4)

where âˆ†n is the n-dimensional probability simplex.
Intuitively, p picks the examples that have the lowest margin when classified by w. One can also normalize the updates so that we can maintain a probability distribution over
examples used for updates from the start, as seen below:

1.2. Paper Outline
Sec.2 will introduce the Perceptron and Normalized Perceptron algorithm and their convergence guarantees for linear separability, with specific emphasis on the unnormalized and normalized margins. Sec.3 will then introduce
RKHSs and the Normalized Kernel Perceptron algorithm,
which we interpret as a subgradient algorithm for a regularized normalized hard-margin loss function.
Sec.4 describes the Smoothed Normalized Kernel Perceptron algorithm that works with a smooth approximation to
the original loss function, and outlines the argument for its
faster convergence rate. Sec.5 discusses the non-separable
case and the Von-Neumann algorithm, and we prove a version of Gordanâ€™s theorem in RKHSs.
We finally give an algorithm in Sec.6 which terminates with
a separator if one exists, and with a dual certificate of nearinfeasibility otherwise, in time inversely proportional to the
margin. Sec.7 has a discussion and some open problems.

Algorithm 2 Normalized Perceptron
Initialize w0 = 0, p0 = 0
for k = 0, 1, 2, 3, ... do
if Y X > wk > 0 then
Exit, with wk as solution
else
1
Î¸k := k+1
wk+1 := (1 âˆ’ Î¸k )wk + Î¸k XY p(wk )
end if
end for
Remark. Normalized Perceptron has the same guarantees as perceptron - the Perceptron can perform its update online on any misclassified point, while the Normalized Perceptron performs updates on the most misclassified
point(s), and yet there does not seem to be any change in
performance. However, we will soon see that the ability to
see all the examples at once gives us much more power.

Margins, Kernels and Non-linear Smoothed Perceptron

2.2. Normalized Margins

3. Kernels and RKHSs

If we normalize the data points by the `2 norm, the resulting
mistake bound of the perceptron algorithm is slightly different. Let X2 represent the matrix with columns xi /kxi k2 .
Define the unnormalized and normalized margins as

The theory of Reproducing Kernel Hilbert Spaces (RKHSs)
has a rich history, and for a detailed introduction, refer to
(SchoÌˆlkopf & Smola, 2002). Let K : Rd Ã— Rd â†’ R be a
symmetric positive definite kernel, giving rise to a Reproducing Kernel Hilbert Space FK with an associated feature
mapping at each point x âˆˆ Rd called Ï†x : Rd â†’ FK where
Ï†x (.) = K(x, .) i.e. Ï†x (y) = K(x, y). FK has an associated inner product hÏ†u , Ï†v iK = K(u, v). For any f âˆˆ FK ,
we have f (x) = hf, Ï†x iK .

Ï :=
Ï2

:=

sup

inf hY X > w, pi,

sup

inf hY X2> w, pi.

kwk2 =1 pâˆˆâˆ†n
kwk2 =1 pâˆˆâˆ†n

Remark. Note that we have supkwk2 =1 in the definition,
this is equivalent to supkwk2 â‰¤1 iff Ï2 > 0.
Normalized Perceptron has the following guarantee on X2 :
If Ï2 > 0, then it finds a perfect separator in Ï12 iterations.
2

Remark. Consider the max-margin separator uâˆ— for X
(which is also a valid perfect separator for X2 ). Then




âˆ—
âˆ—
yi x>
yi x>
Ï
i u
i u
=
min
â‰¤
min
maxi kxi k2
i
i
maxi kxi k2
kxi k2


>
yi xi u
â‰¤
sup min
= Ï2 .
kxi k2
kuk2 =1 i
Hence, it is always better to normalize the data as pointed
out in (Graepel et al., 2001). This idea extends to RKHSs,
motivating the normalized Gram matrix considered later.
Example Consider a simple example in R2+ . Assume that
+ points are located along the line 6x2 = 8x1 , and the
âˆ’ points along 8x2 = 6x1 , for 1/r â‰¤ kxk2 â‰¤ r, where
r > 1. The max-margin linear separator will be x1 = x2 .
If all the data were normalized to have unit Euclidean norm,
then all the + points would all be at (0.6, 0.8) and all the
âˆ’ points at (0.8, 0.6), giving us a normalized margin of
Ï2 â‰ˆ 0.14. Unnormalized, the margin is Ï â‰ˆ 0.14/r and
maxi kxi k2 = r. Hence, in terms of bounds, we get a
discrepancy of r4 , which can be arbitrarily large.
Winnow The question arises as to which norm we should
normalize by. There is a now classic algorithm in machine
learning, called Winnow (Littlestone, 1991) or Multiplicate
Weights. It works on a slight transformation of the problem
where we only need to search for u âˆˆ Rd+ . It comes with
some very well-known guarantees - If there exists a u âˆˆ Rd+
such that Y X > u â‰¥ Ï > 0, then feasibility is guaranteed
in kuk21 maxi kai k2âˆ log n/Ï2 iterations. The appropriate
notion of normalized margin here is
>
Ï1 := max min hY Xâˆ
w, pi,
wâˆˆâˆ†d pâˆˆâˆ†n

where Xâˆ is a matrix with columns xi /kxi kâˆ . Then, the
appropriate iteration bound is log n/Ï21 . We will return to
this `1 -margin in the discussion section. In the next section,
we will normalize by using the kernel appropriately.

Define the normalized feature map
Ï†x
âˆˆ FK and Ï†ÌƒX := [Ï†Ìƒxi ]n1 .
Ï†Ìƒx = p
K(x, x)
For any function f âˆˆ FK , we use the following notation
in
h
Y fËœ(X) := hf, Y Ï†ÌƒX iK = [yi hf, Ï†Ìƒxi iK ]n1 = âˆšyi f (xi )
.
K(xi ,xi ) 1

We analogously define the normalized margin here to be
D
E
(5)
ÏK :=
sup inf Y fËœ(X), p .
kf kK =1 pâˆˆâˆ†n

Consider the following regularized empirical loss function

D
E
L(f ) = sup âˆ’Y fËœ(X), p
+ 12 kf k2K .
(6)
pâˆˆâˆ†n



Denoting t := kf kK > 0 and writing f = t kffkK = tfÂ¯,
let us calculate the minimum value of this function
2
inf L(f ) = inf inf sup hâˆ’htfÂ¯, Y Ï†ÌƒX iK , pi + t
2

t>0 kfÂ¯kK =1 pâˆˆâˆ†n

f âˆˆFK

=
=


	
inf âˆ’tÏK + 21 t2

t>0
âˆ’ 12 Ï2K

when t = ÏK > 0.
(7)
D
E
Since maxpâˆˆâˆ†n âˆ’Y fËœ(X), p is some empirical loss
function on the data and 12 kf k2K is an increasing function
of kf kK , the Representer Theorem (SchoÌˆlkopf et al., 2001)
implies that the minimizer of the above function lies in the
span of Ï†xi s (also the span of the yi Ï†Ìƒxi s). Explicitly,
arg min L(f ) =
f âˆˆFK

n
X

Î±i yi Ï†Ìƒxi = hY Ï†ÌƒX , Î±i.

(8)

i=1

Substituting this back into Eq.(6), we can define


L(Î±) :=
sup hâˆ’Î±, piG + 21 kÎ±k2G ,

(9)

pâˆˆâˆ†n

where G is a normalized signed Gram matrix with Gii = 1,
Gji = Gij := âˆš

yi yj K(xi ,xj )
K(xi ,xi )K(xj ,xj )

= hyi Ï†Ìƒxi , yj Ï†Ìƒxj iK ,

âˆš
and hp, Î±iG := p> GÎ±, kÎ±kG := Î±> GÎ±. One can verify
that G is a PSD matrix and the G-norm k.kG is a seminorm, whose properties are of great importance to us.

Margins, Kernels and Non-linear Smoothed Perceptron

3.1. Some Interesting and Useful Lemmas

4. Smoothed Normalized Kernel Perceptron

The first lemma justifies our algorithmsâ€™ exit condition.
Lemma 1. L(Î±) < 0 implies GÎ± > 0 and there exists a
perfect classifier iff GÎ± > 0.
Proof. L(Î±) < 0 â‡’ suppâˆˆâˆ†n hâˆ’GÎ±, pi < 0 â‡” GÎ± > 0.
GÎ± > 0 â‡’ fÎ± := hÎ±, Y Ï†ÌƒX i is perfect since

Define the distribution over the worst-classified points
D
E
p(f ) := arg min Y fËœ(X), p

y f (x )
pj Î± j
K(xj , xj )

=

n
X
i=1

Î±i p

yi yj K(xi , xj )
K(xi , xi )K(xj , xj )

= Gj Î± > 0.
If a perfect classifier exists, then ÏK > 0 by definition and
L(f âˆ— ) = L(Î±âˆ— ) = âˆ’ 12 Ï2K < 0

â‡’

GÎ± > 0,

where f âˆ— , Î±âˆ— are the optimizers of L(f ), L(Î±).
The second lemma bounds the G-norm of vectors.
âˆš
Lemma 2. For any Î± âˆˆ Rn , kÎ±kG â‰¤ kÎ±k1 â‰¤ nkÎ±k2 .
Proof. Using the triangle inequality of norms, we get
rD
E
âˆš
Î±> GÎ± =
hÎ±, Y Ï†ÌƒX i, hÎ±, Y Ï†ÌƒX i
K
X
X
= k
Î±i yi Ï†Ìƒxi kK â‰¤
kÎ±i yi Ï†Ìƒxi kK
i

â‰¤

X
i

i




Ï†x i


|Î±i | yi p


K(xi , xi ) 

K

=

X

|Î±i |,

i

where we used hÏ†xi , Ï†xi iK = K(xi , xi ).
The third lemma gives a new perspective on the margin.
Lemma 3. When ÏK > 0, f maximizes the margin iff ÏK f
optimizes L(f ). Hence, the margin is equivalently
ÏK = sup

inf hÎ±, piG â‰¤ kpkG

for all p âˆˆ âˆ†n .

kÎ±kG =1 pâˆˆâˆ†n

Proof. Let fÏ be any function with kfÏ kK = 1 that
achieves the max-margin ÏK > 0. Then, it is easy to plug
ÏK fÏ into Eq. (6) and verify that L(ÏK fÏ ) = âˆ’ 12 Ï2K and
hence ÏK fÏ minimizes L(f ).
Similarly, let fL be any function that minimizes L(f ),
i.e. achieves the value L(fL ) = âˆ’ 12 Ï2K . Defining t := kfL kK , and examining Eq. (7), we see that
L(fL ) cannotDachieve the value
âˆ’ 12 Ï2K unless t = ÏK
E
and sup
âˆ’Y fËœL (X), p = âˆ’Ï2 which means that
pâˆˆâˆ†n

=

or p(Î±)

inf

sup hÎ±, piG â‰¤ sup hÎ±, piG

pâˆˆâˆ†n kÎ±kG =1

kÎ±kG =1

â‰¤ kpkG by applying Cauchy-Schwartz
(can also be seen by going back to function space).

:=

arg min hÎ±, piG .
pâˆˆâˆ†n

(10)

Algorithm 3 Normalized Kernel Perceptron (NKP)
Set Î±0 := 0
for k = 0, 1, 2, 3, ... do
if GÎ±k > 0n then
Exit, with Î±k as solution
else
1
Î¸k := k+1
Î±k+1 := (1 âˆ’ Î¸k )Î±k + Î¸k p(Î±k )
end if
end for
Implicitly fk+1

(1 âˆ’ Î¸k )fk + Î¸k hY Ï†ÌƒX , p(fk )i


= fk âˆ’ Î¸k fk âˆ’ hY Ï†ÌƒX , p(fk )i

=

= fk âˆ’ Î¸k âˆ‚L(fk )
and hence the Normalized Kernel Perceptron (NKP) is a
subgradient algorithm to minimize L(f ) from Eq. (6).
Remark. Lemma 3 yields deep insights. Since NKP can
get arbitrarily close to the minimizer of strongly convex
L(f ), it also gets arbitrarily close to a margin maximizer. It
is known that it finds a perfect classifier in 1/Ï2K iterations
- we now additionally infer that it will continue to improve
to find an approximate max-margin classifier. While both
classical and normalized Perceptrons find perfect classifiers
in the same time, the latter is guaranteed to improve.
Remark. Î±k+1 is always a probability distribution. Curiously, a guarantee that the solution will lie in âˆ†n is not
made by the Representer Theorem in Eq. (8) - any Î± âˆˆ Rn
could satisfy Lemma 1. However, since NKP is a subgradient method for minimizing Eq. (6), we know that we will
approach the optimum while only choosing Î± âˆˆ âˆ†n .
Define the smooth minimizer analogous to Eq. (10) as
n
o
pÂµ (Î±) := arg min hÎ±, piG + Âµd(p) (11)
pâˆˆâˆ†n

K

fL /ÏK must achieve the max-margin.
P
Hence considering only f = i Î±i yi Ï†Ìƒxi is acceptable for
both. Plugging this into Eq. (5) gives the equality and
ÏK

pâˆˆâˆ†n

where d(p)

eâˆ’GÎ±/Âµ
=
,
keâˆ’GÎ±/Âµ k1
X
:=
pi log pi + log n

(12)

i

is 1-strongly convex with respect to the `1 -norm (Nesterov,
2005). Define a smoothened loss function as in Eq. (9)


LÂµ (Î±) = sup âˆ’ hÎ±, piG âˆ’ Âµd(p) + 12 kÎ±k2G .
pâˆˆâˆ†n

Note that the maximizer above is precisely pÂµ (Î±).

Margins, Kernels and Non-linear Smoothed Perceptron

Algorithm 4 Smoothed Normalized Kernel Perceptron
Set Î±0 = 1n /n, Âµ0 := 2, p0 := pÂµ0 (Î±0 )
for k = 0, 1, 2, 3, ... do
if GÎ±k > 0n then
Halt: Î±k is solution to Eq. (8)
else
2
Î¸k := k+3
Î±k+1 := (1 âˆ’ Î¸k )(Î±k + Î¸k pk ) + Î¸k2 pÂµk (Î±k )
Âµk+1 = (1 âˆ’ Î¸k )Âµk
pk+1 := (1 âˆ’ Î¸k )pk + Î¸k pÂµk+1 (Î±k+1 )
end if
end for

5. Infeasible Problems
What happens when the points are not separable by any
function f âˆˆ FK ? We would like an algorithm that terminates with a solution when there is one, and terminates with
a certificate of non-separability if there isnâ€™t one. The idea
is based on theorems of the alternative like Farkasâ€™ Lemma,
specifically a version of Gordanâ€™s theorem (Chvatal, 1983):
Lemma 6 (Gordanâ€™s Thm). Exactly one of the following
two statements can be true
1. Either there exists a w âˆˆ Rd such that for all i,
yi (w> xi ) > 0,

Lemma 4 (Lower Bound). At any step k, we have
LÂµk (Î±k ) â‰¥ L(Î±k ) âˆ’ Âµk log n.
Proof. First note that suppâˆˆâˆ†n d(p) = log n. Also,

	
sup âˆ’ hÎ±, piG âˆ’ Âµd(p)
pâˆˆâˆ†n

â‰¥

sup
pâˆˆâˆ†n



âˆ’ hÎ±, piG

	


	
âˆ’ sup Âµd(p) .
pâˆˆâˆ†n

Combining these two facts gives us the result.
Lemma 5 (Upper Bound). In any round k, SNKP satisfies
LÂµk (Î±k ) â‰¤ âˆ’ 12 kpk k2G .
Proof. We provide a concise, self-contained and unified
proof by induction in the Appendix for Lemma 5 and
Lemma 8, borrowing ideas from Nesterovâ€™s excessive gap
technique (Nesterov, 2005) for smooth minimization of
structured non-smooth functions.
Finally, we combine the above lemmas to get the following
theorem about the performance of SNKP.
Theorem 1. The SNKP algorithm
 âˆš finds
 a perfect classifier
log n
f âˆˆ FK when one exists in O
iterations.
ÏK
Proof. Lemma 4 gives us for any round k,
LÂµk (Î±k ) â‰¥ L(Î±k ) âˆ’ Âµk log n.
From Lemmas 3, 5 we get
1 2
LÂµk (Î±k ) â‰¤ âˆ’ 12 p>
k Gpk â‰¤ âˆ’ 2 ÏK .

Combining the two equations, we get that
L(Î±k ) â‰¤ Âµk log n âˆ’ 12 Ï2K .
4
4
Noting that Âµk = (k+1)(k+2)
< (k+1)
2 , we see that
L(Î±k ) < 0 (and hence
we
solve
the
problem
by Lemma 1)
âˆš
after at most k = 2 2 log n/ÏK steps.

2. Or, there exists a p âˆˆ âˆ†n such that
kXY pk2 = 0,
or equivalently

P

i

(13)

pi yi xi = 0.

As mentioned in the introduction, the primal problem can
be interpreted as finding a vector in the interior of the dual
cone of cone{yi xi }, which is infeasible the dual cone is
flat i.e. if cone{yi xi } is not pointed, which happens when
the origin is in the convex combination of yi xi s.
We will generalize the following algorithm for linear feasibility problems, that can be dated back to Von-Neumann,
who mentioned it in a private communication with Dantzig,
who later studied it himself (Dantzig, 1992).
Algorithm 5 Normalized Von-Neumann (NVN)
Initialize p0 = 1n /n, w0 = XY p0
for k = 0, 1, 2, 3, ... do
if kXY pk k2 â‰¤  then
Exit and return pk as an -solution to (13)
else
j := arg mini yi x>
i wk
Î¸k := arg minÎ»âˆˆ[0,1] k(1 âˆ’ Î»)wk + Î»yj xj k2
pk+1 := (1 âˆ’ Î¸k )pk + Î¸k ej
wk+1 := XY pk+1 = (1 âˆ’ Î¸k )wk + Î¸k yj xj
end if
end for
This algorithm comes with a guarantee: If the problem (3)
is infeasible, then the above algorithm will terminate with
an -approximate solution to (13) in 1/2 iterations.
(Epelman & Freund, 2000) proved an incomparable bound
- Normalized Von-Neumann
(NVN)
can compute an 

1
1
solution to (13) in O Ï2 log  and can also find a solu2
 
tion to the primal (using wk ) in O Ï12 when it is feasible.
2

We derive a smoothed variant of NVN in the next section,
after we prove some crucial lemmas in RKHSs.

Margins, Kernels and Non-linear Smoothed Perceptron

5.1. A Separation Theorem for RKHSs

5.2. The infeasible margin ÏK

While finite dimensional Euclidean spaces come with
strong separation guarantees that come under various
names like the separating hyperplane theorem, Gordanâ€™s
theorem, Farkasâ€™ lemma, etc, the story isnâ€™t always the
same for infinite dimensional function spaces which can
often be tricky to deal with. We will prove an appropriate
version of such a theorem that will be useful in our setting.

Note that constraining kf kK = 1 (or kÎ±kG = 1) in Eq. (5)
and Lemma 3 allows ÏK to be negative in the infeasible
case. If it was â‰¤, then ÏK would have been non-negative
because f = 0 (ie Î± = 0) is always allowed.

What follows is an interesting version of the Hahn-Banach
separation theorem, which looks a lot like Gordanâ€™s theorem in finite dimensional spaces. The conditions to note
here are that either GÎ± > 0 or kpkG = 0.
Theorem 2. Exactly one of the following has a solution:
1. Either âˆƒf âˆˆ FK such that for all i,
y f (xi )
p i
= hf, yi Ï†Ìƒxi iK > 0 i.e. GÎ± > 0,
K(xi , xi )
2. Or âˆƒp âˆˆ âˆ†n such that
X
pi yi Ï†Ìƒxi = 0 âˆˆ FK i.e. kpkG = 0.

(14)

i

i

be the convex hull of the yi Ï†Ìƒxi s.
Theorem 3. When the primal is infeasible, the margin1 is
n 
o
|ÏK | = Î´max := sup Î´  kf kK â‰¤ Î´ â‡’ f âˆˆ conv(Y Ï†ÌƒX )
Proof. (1) For inequality â‰¥. Choose any Î´ such that f âˆˆ
conv(Y Ï†ÌƒX ) for any kf kK â‰¤ Î´. Given an arbitrary f 0 âˆˆ
FK with kf 0 kK = 1, put fËœ := âˆ’Î´f 0 .
By our assumption on Î´, we have fËœ âˆˆ conv(Y Ï†ÌƒX ) implying there exists a pÌƒ âˆˆ âˆ†n such that fËœ = hY Ï†ÌƒX , pÌƒi . Also
D
E
f 0 , hY Ï†ÌƒX , pÌƒi
= hf 0 , fËœiK
K

Proof. Consider the following set
(
!
)
X
X
Q =
(f, t) =
pi yi Ï†Ìƒxi ,
pi : p âˆˆ âˆ†n
i

=

So what is ÏK when the problem is infeasible? Let
nX
o
conv(Y Ï†ÌƒX ) :=
pi yi Ï†Ìƒxi |p âˆˆ âˆ†n âŠ‚ FK

i

= âˆ’Î´kf 0 k2K = âˆ’Î´.
Since this holds for a particular pÌƒ, we can infer
D
E
inf
f 0 , hY Ï†ÌƒX , pÌƒi
â‰¤ âˆ’Î´.
pâˆˆâˆ†n

h

conv (y1 Ï†Ìƒx1 , 1), ..., (yn Ï†Ìƒxn , 1)

i

âŠ† FK Ã— R.

K

Since this holds for any f 0 with kf 0 kG = 1, we have
D
E
â‰¤ âˆ’Î´ i.e. |ÏK | â‰¥ Î´.
sup inf f 0 , hY Ï†ÌƒX , pÌƒi
kf kK =1 pâˆˆâˆ†n

If (2) does not hold, then it implies that (0, 1) âˆˆ
/ Q. Since Q
is closed and convex, we can find a separating hyperplane
between Q and (0, 1), or in other words there exists (f, t) âˆˆ
FK Ã— R such that
D
E
(f, t), (g, s)
â‰¥ 0 âˆ€(g, s) âˆˆ Q
D
E
and (f, t), (0, 1)
< 0.
The second condition immediately yields t < 0. The first
condition, when applied to (g, s) = (yi Ï†Ìƒxi , 1) âˆˆ Q yields
hf, yi Ï†Ìƒxi iK + t â‰¥
yi f (xi )
>
â‡” p
K(xi , xi )

0

(2) For inequality â‰¤. It suffices to show kf kK â‰¤ |ÏK | â‡’
f âˆˆ conv(Y Ï†ÌƒX ). We will prove the contrapositive f âˆˆ
/
conv(Y Ï†ÌƒX ) â‡’ kf kK > |ÏK |.
Since âˆ†n is compact and convex, conv(Y Ï†ÌƒX ) âŠ‚ FK is
closed and convex. Therefore if f âˆˆ
/ conv(Y Ï†ÌƒX ), then
there exists g âˆˆ FK with kgkK = 1 that separates f and
conv(Y Ï†ÌƒX ), i.e. for all p âˆˆ âˆ†n ,
hg, f iK
i.e. hg, f iK

< 0 and hg, hY Ï†ÌƒX , piiK â‰¥ 0
<
â‰¤

0

Since ÏK < 0

since t < 0, which shows that (1) holds.

inf hg, hY Ï†ÌƒX , piiK

pâˆˆâˆ†n

sup

inf hf, hY Ï†ÌƒX , piiK = ÏK .

kf kK =1 pâˆˆâˆ†n

|ÏK | < |hf, giK |
â‰¤

It is also immediate that if (2) holds, then (1) cannot.
Note that G is positive semi-definite - infeasibility requires
both that it is not positive definite, and also that the witness
to p> Gp = 0 must be a probability vector. Similarly, while
it suffices that GÎ± > 0 for some Î± âˆˆ Rn , but coincidentally
in our case Î± will also lie in the probability simplex.

K

1

kf kK kgkK = kf kK .

We thank a reviewer for pointing out that by this definition,
ÏK might always be 0 for infinite dimensional RKHSs because
there are always directions perpendicular to the finite-dimensional
hull - we conjecture the definition can be altered to restrict attention to the relative interior of the hull, making it non-zero.

Margins, Kernels and Non-linear Smoothed Perceptron

6. Kernelized Primal-Dual Algorithms

As a consequence, kpkG = 0 iff p âˆˆ W .

Algorithm 6 Smoothed Normalized Kernel PerceptronVonNeumann (SN KP V N (q, Î´))
input q âˆˆ âˆ†n , accuracy Î´ > 0
Set Î±0 = q, Âµ0 := 2n, p0 := pqÂµ0 (Î±0 )
for k = 0, 1, 2, 3, ... do
if GÎ±k > 0n then
Halt: Î±k is solution to Eq. (8)
else if kpk kG < Î´ then
Return pk
else
2
Î¸k := k+3
Î±k+1 := (1 âˆ’ Î¸k )(Î±k + Î¸k pk ) + Î¸k2 pqÂµk (Î±k )
Âµk+1 = (1 âˆ’ Î¸k )Âµk
pk+1 := (1 âˆ’ Î¸k )pk + Î¸k pqÂµk+1 (Î±k+1 )
end if
end for

Proof. This is trivial for p âˆˆ W . For arbitrary p âˆˆ âˆ†n \W ,
K |p
let pÌƒ := âˆ’ |Ï
kpkG so that khY Ï†ÌƒX , pÌƒikK = kpÌƒkG â‰¤ |ÏK |.

When the primal is feasible, SNKPVN is similar to SNKP.
Lemma 8 (When ÏK > 0 and Î´ < ÏK ). For any q âˆˆ âˆ†n ,

Hence by Theorem 3, there exists Î± âˆˆ âˆ†n such that

âˆ’ 21 kpk k2G â‰¥ LqÂµk (Î±k ) â‰¥ L(Î±k ) âˆ’ Âµk .
âˆš 
Hence SNKPVN finds a separator f in O ÏKn iterations.

The preceding theorems allow us to write a variant of the
Normalized VonNeumann algorithm from the previous section that is smoothed and works for RKHSs. Define
X

o
n
o n


W := p âˆˆ âˆ†n 
pi yi Ï†Ìƒxi = 0 = p âˆˆ âˆ†n kpkG = 0
i

as the set of witnesses to the infeasibility of the primal.
The following lemma bounds the distance of any point in
the simplex from the witness set by its k.kG norm.
Lemma 7. For all q âˆˆ âˆ†n , the distance to the witness set
)
(
âˆš
âˆš
2kqkG
2,
.
dist(q, W ) := min kq âˆ’ wk2 â‰¤ min
wâˆˆW
|ÏK |

hY Ï†ÌƒX , Î±i = hY Ï†ÌƒX , pÌƒi.
G
Let Î² = Î»Î± + (1 âˆ’ Î»)p where Î» = kpkkpk
. Then
G +|ÏK |
D
E
1
hY Ï†ÌƒX , Î²i =
Y Ï†ÌƒX , kpkG Î± + |ÏK |p
kpkG + |Ï|K
1
=
hY Ï†ÌƒX , kpkG pÌƒ + |ÏK |pi
kpkG + |Ï|K
= 0,

so Î² âˆˆ W (by definition of what it means to be in W ) and
(
)
âˆš
âˆš
âˆš
2kqkG
2,
.
kp âˆ’ Î²k2 = Î»kp âˆ’ Î±k2 â‰¤ Î» 2 â‰¤ min
|ÏK |
âˆš
We take min with 2 because ÏK might be 0.
Hence for the primal or dual problem, points with small Gnorm are revealing - either Lemma 3 shows that the margin
ÏK â‰¤ kpkG will be small, or if it is infeasible then the
above lemma shows that it is close to the witness set.

Proof. We give a unified proof for the first inequality and
Lemma 5 in the Appendix. The second inequality mimics
Lemma 4. The final statement mimics Theorem 1.
The following lemma captures the near-infeasible case.
Lemma 9 (When ÏK < 0 or Î´ > ÏK ). For any q âˆˆ âˆ†n ,
âˆ’ 12 kpk k2G â‰¥ LqÂµk (Î±k ) â‰¥ âˆ’ 12 Âµk dist(q, W )2 .
Hence
finds

nSNKPVN
o a Î´-solution
âˆš
âˆš
nkqkG
O min Î´n , Î´|Ï
iterations.
K|

which can easily be found by sorting the entries of q âˆ’ GÎ±
Âµ .

most

pâˆˆâˆ†n


â‰¥


âˆ’ hÎ±, piG âˆ’ Âµk dq (p)

sup
pâˆˆW


=

sup
pâˆˆW

dq (p) = 12 kp âˆ’ qk22

pâˆˆâˆ†n

at

Proof. The first inequality is the same as in the above
Lemma 8, and is proved in the Appendix.


q
LÂµk (Î±k ) = sup âˆ’ hÎ±, piG âˆ’ Âµk dq (p) + 12 kÎ±k2G

We need a small alteration to the smoothing entropy proxfunction that we used earlier. We will now use

for some given q âˆˆ âˆ†n , which is strongly convex with
respect to the `2 norm. This allows us to define
Âµ
pqÂµ (Î±) = arg min hGÎ±, pi + kp âˆ’ qk22 ,
pâˆˆâˆ†n
2


q
LÂµ (Î±) = sup âˆ’ hÎ±, piG âˆ’ Âµdq (p) + 12 kÎ±k2G ,

in

âˆ’ 12 Âµk kp âˆ’ qk22

= âˆ’ 12 Âµk dist(q, W )2
n
o
kqk2
â‰¥ âˆ’Âµk min 2, |ÏK G
2
|



using Lemma 7.

4n
â‰¤ (k+1)
2 we get


âˆš
âˆš kqkG
2 n
kpk kG â‰¤
min
2,
.
(k + 1)
ÏK
nâˆš
o
âˆš
G
Hence kpkG â‰¤ Î´ after 2 Î´ n min
2, kqk
steps.
ÏK

Since Âµk =

4n
(k+1)(k+2)

Margins, Kernels and Non-linear Smoothed Perceptron

Using SNKPVN as a subroutine gives our final algorithm.

7. Discussion

Algorithm 7 Iterated Smoothed Normalized Kernel
Perceptron-VonNeumann (ISN KP V N (Î³, ))
input Constant Î³ > 1, accuracy  > 0
Set q0 := 1n /n
for t = 0, 1, 2, 3, ... do
Î´t := kqt kG /Î³
qt+1 := SN KP V N (qt , Î´t )
if Î´t <  then
Halt; qt+1 is a solution to Eq. (14)
end if
end for

The SNK-Perceptron algorithm presented
in this paâˆš
log n
and
the Itper has a convergence rate of
ÏK
erated
SNK-Perceptron-Von-Neumann
algorithm
has a
nâˆš âˆš o
n
n
min  , |ÏK | dependence on the number of points.
Note that both of these are independent of the underlying
dimensionality of the problem. We conjecture
that it is posâˆš
sible to reduce this dependence to log n for the primaldual algorithm also, without paying a price in terms of the
dependence on margin 1/Ï (or the dependence on ).

Theorem 4. Algorithm ISNKPVN satisfies
1. If the primal (2) is feasible and âˆš< ÏK , then each call
to SNKPVN halts in at most 2 ÏK2n iterations. AlgoK)
rithm ISNKPVN finds a solution in at most log(1/Ï
log(Î³)
outer loops, bounding the total iterations by
âˆš


n
1
O
log
.
ÏK
ÏK

2. If the dual (14) is feasible or > ÏnKâˆš, thenâˆšeach
ocall to
steps.
SNKPVN halts in at most O min n , |ÏKn|
Algorithm ISNKPVN finds an -solution in at most
log(1/)
log(Î³) outer loops, bounding the total iterations by
 

âˆš âˆš 
n
n
1
,
log
.
O min
 |ÏK |

Proof. First note that if ISNKPVN has not halted, then we
know that after t outer iterations, qt+1 has small G-norm:
kqt+1 kG â‰¤ Î´t â‰¤

kq0 kG
.
Î³ t+1

(15)

The first inequality holds because of the inner loop return
condition, the second because of the update for Î´t .
1. Lemma 3 shows that for all p we have ÏK â‰¤ kpkG , so
the inner loop will halt with a solution to the primal
as soon as Î´t â‰¤ ÏK (so that kpkG < Î´t â‰¤ ÏK cannot
be satisfied for the inner loop to return). From Eq.
0 kG
(15), this will definitely happen when kq
Î³ t+1 â‰¤ ÏK ,
ie within T =

log(kqo kG /ÏK )
log(Î³)

iterations. By Lemma 8,

each iteration runs for at most

âˆš
2 2n
ÏK

steps.

2. We halt with an -solution when Î´t < , which
0 kG
definitely happens when kq
< , ie within
Î³ t+1
o kG /)
T
= log(kq
iterations. Since kqÎ´ttkG =
log(Î³)
Î³, by Lemma
each iteration runs for at most
n âˆš âˆš9, o
n
n
O min  , |ÏK |
steps.

It is possible that tighter dependence on n is possible if we
try other smoothing functions instead of the `2 norm used
in the last section. Specifically, it might be tempting to
smooth with the k.kG semi-norm and define:
Âµ
pqÂµ (Î±) = arg min hÎ±, piG + kp âˆ’ qk2G
pâˆˆâˆ†n
2
One can actually see that the proofs in the Appendix go
through with no dimension dependence on n at all! However, it is not possible to solve this in closed form - taking
Î± = q and Âµ = 1 reduces the problem to asking
pq (q) = arg min

1
kpk2G
pâˆˆâˆ†n 2

which is an oracle for our problem as seen by equation (14)
- the solutionâ€™s G-norm is 0 iff the problem is infeasible.
In the bigger picture, there are several interesting open
questions. The ellipsoid algorithm for solving linear feasibility problems has a logarithmic dependence on 1/, and
a polynomial dependence on dimension. Recent algorithms
involving repeated rescaling of the space like (Dunagan &
Vempala, 2008) have logarithmic dependence on 1/Ï and
polynomial in dimension. While both these algorithms are
poly-time under the real number model of computation of
(Blum et al., 1998), it is unknown whether there is any algorithm that can achieve a polylogarithmic dependence on
the margin/accuracy, and a polylogarithmic dependence on
dimension. This is strongly related to the open question of
whether it is possible to learn a decision list polynomially
in its binary description length.
One can nevertheless ask whether rescaled smoothed perceptron methods like (Dunagan & Vempala, 2008) can be
lifted to RKHSs, and whether using an iterated smoothed
kernel perceptron would yield faster rates. The recent work
(Soheili & PenÌƒa, 2013b) is a challenge to generalize - the
proofs relying on geometry involve arguing about volumes
of balls of functions in an RKHS - we conjecture that it is
possible to do, but we leave it for a later work.
Acknowledgements
We thank Negar Soheili, Avrim Blum for discussions and
the excellent reviewers for references and Footnote 1.

Margins, Kernels and Non-linear Smoothed Perceptron

References
Block, HD. The perceptron: A model for brain functioning.
i. Reviews of Modern Physics, 34(1):123, 1962.
Blum, Lenore, Cucker, Felipe, Shub, Michael, and Smale,
Steve. Complexity and real computation. Springer, 1998.
Cheung, Dennis and Cucker, Felipe. A new condition number for linear programming. Mathematical programming, 91(1):163â€“174, 2001.
Chvatal, Vasek. Linear programming. Macmillan, 1983.
Clarkson, Kenneth L. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. ACM Transactions
on Algorithms (TALG), 6(4):63, 2010.
Dantzig, George B. An -precise feasible solution to a
linear program with a convexity constraint in 1/2 iterations independent of problem size. Technical report,
Technical Report, Stanford University, 1992.
Dunagan, John and Vempala, Santosh.
A simple
polynomial-time rescaling algorithm for solving linear
programs. Mathematical Programming, 114(1):101â€“
114, 2008.
Epelman, Marina and Freund, Robert M. Condition number complexity of an elementary algorithm for computing a reliable solution of a conic linear system. Mathematical Programming, 88(3):451â€“485, 2000.
Freund, Robert M and Vera, Jorge R. Condition-based
complexity of convex optimization in conic linear form
via the ellipsoid algorithm. SIAM Journal on Optimization, 10(1):155â€“176, 1999.
Graepel, Thore, Herbrich, Ralf, and Williamson, Robert C.
From margin to sparsity. Advances in neural information
processing systems, pp. 210â€“216, 2001.
Littlestone, Nicholas. Redundant noisy attributes, attribute
errors, and linear-threshold learning using winnow. In
Proceedings of the fourth annual workshop on Computational learning theory, pp. 147â€“156. Morgan Kaufmann
Publishers Inc., 1991.
Nemirovski, Arkadi. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz
continuous monotone operators and smooth convexconcave saddle point problems. SIAM Journal on Optimization, 15(1):229â€“251, 2004.
Nesterov, Yu. Excessive gap technique in nonsmooth convex minimization. SIAM Journal on Optimization, 16
(1):235â€“249, 2005.
Novikoff, Albert BJ. On convergence proofs for perceptrons. Technical report, 1962.

Renegar, James. Incorporating condition measures into the
complexity theory of linear programming. SIAM Journal
on Optimization, 5(3):506â€“524, 1995.
Rosenblatt, Frank. The perceptron: a probabilistic model
for information storage and organization in the brain.
Psychological review, 65(6):386, 1958.
Saha, Ankan, Vishwanathan, SVN, and Zhang, Xinhua.
New approximation algorithms for minimum enclosing
convex shapes. In Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, pp.
1146â€“1160. SIAM, 2011.
SchoÌˆlkopf, Bernhard and Smola, Alexander J. Learning
with kernels. The MIT Press, 2002.
SchoÌˆlkopf, Bernhard, Herbrich, Ralf, and Smola, Alex J.
A generalized representer theorem. In Computational
learning theory, pp. 416â€“426. Springer, 2001.
Soheili, Negar and PenÌƒa, Javier. A smooth perceptron algorithm. SIAM Journal on Optimization, 22(2):728â€“737,
2012.
Soheili, Negar and PenÌƒa, Javier. A primalâ€“dual smooth
perceptronâ€“von Neumann algorithm. In Discrete Geometry and Optimization, pp. 303â€“320. Springer, 2013a.
Soheili, Negar and PenÌƒa, Javier. A deterministic rescaled
perceptron algorithm. 2013b.
Tseng, Paul. On accelerated proximal gradient methods
for convex-concave optimization. SIAM Journal on Optimization, 2008.

