Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets
Dan Garber
Technion - Israel Institute of Technology
Elad Hazan
Princeton University

DANGAR @ TX . TECHNION . AC . IL

EHAZAN @ CS . PRINCETON . EDU

Abstract
The Frank-Wolfe method (a.k.a. conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine
learning. A key advantage of the method is that
it avoids projections - the computational bottleneck in many applications - replacing it by a linear optimization step. Despite this advantage, the
known convergence rates of the FW method fall
behind standard first order methods for most settings of interest. It is an active line of research
to derive faster linear optimization-based algorithms for various settings of convex optimization.
In this paper we consider the special case of optimization over strongly convex sets, for which
we prove that the vanila FW method converges
at a rate of t12 . This gives a quadratic improvement in convergence rate compared to the general case, in which convergence is of the order 1t ,
and known to be tight. We show that various balls
induced by `p norms, Schatten norms and group
norms are strongly convex on one hand and on
the other hand, linear optimization over these sets
is straightforward and admits a closed-form solution. We further show how several previous fastrate results for the FW method follow easily from
our analysis.

1. Introduction
The Frank-Wolfe method, originally introduced by Frank
and Wolfe in the 1950’s (Frank & Wolfe, 1956), is a first
order method for the minimization of a smooth convex
function over a convex set. Its main advantage in largend

Proceedings of the 32
International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

scale problems is that it is a first-order and projection-free
method - i.e. the algorithm proceeds by iteratively solving a
linear optimization problem and remaining inside the feasible domain. For matrix completion problems, metric learning, sparse PCA, structural SVM and other large-scale machine learning problems, this feature enabled the derivation
of algorithms that are practical on one hand and come with
provable convergence rates on the other (Jaggi & Sulovský,
2010; Lacoste-Julien et al., 2013; Dudı́k et al., 2012; Harchaoui et al., 2012; Hazan & Kale, 2012; Shalev-Shwartz
et al., 2011; Laue, 2012).
Despite its empirical success, the main drawback of the
method is its relatively slow convergence rate in comparison to optimal first order methods. The convergence rate
of the method is on the order of 1/t where t is the number of iterations, and this is known to be tight. In contrast, Nesterov’s accelerated gradient descent method gives
a rate of 1/t2 for general convex smooth problems and a
rate e ⇥(t) is known for smooth and strongly convex problems. The following question arises: are there projectionfree methods with convergence rates matching that of projected gradient-descent and its extensions?
Motivated by this question, in this work we advance the line
of research for faster convergence rates of projection free
methods. We prove that in case both the objective function
and the feasible set are strongly convex (in fact a slightly
weaker assumption than strong convexity of the objective is
required), the vanilla Frank-Wolfe method converges at an
accelerated rate of 1/t2 . The improved convergence rate is
independent of the dimension. This is also the first convergence result for the FW that we are aware of that achieves
a rate that is between the standard 1/t rate and a linear rate.
We further show how the analysis used to prove the latter result enables to easily derive previous fast convergence
rates for the FW method.
We motivate the study of optimization over strongly convex
sets by demonstrating that various norms that serve as popular regularizes in machine learning problems, including
`p norms, matrix Schatten norms and matrix group norms,

Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets

give rise to strongly convex sets. We further show that indeed linear optimization over these sets is straightforward
to implement and admits a closed-form solution. Hence the
FW method is appealing for solving optimization problems
with such constraints, such as regularized linear regression.
1.1. Related Work
The Frank-Wolfe method dates back to the original work of
Frank and Wolfe (Frank & Wolfe, 1956) which presented
an algorithm for minimizing a quadratic function over a
polytope using only linear optimization steps over the feasible set. Recent results by Clarkson (Clarkson, 2008),
Hazan (Hazan, 2008) and Jaggi (Jaggi, 2013) extend the
method to smooth convex optimization over the simplex,
spectrahedron and arbitrary convex and compact sets respectively.
It was shown in numerous works that the convergence rate
of the method is on the order of 1/t and that it could not
be improved in general, even if the objective function is
strongly convex for instance, as shown in (Clarkson, 2008;
Hazan, 2008; Jaggi, 2013), even though it is known that in
this case, the projected gradient method achieves an exponentially fast convergence rate.
Over the past years, several results tried to improve the
convergence rate of the Frank-Wolfe method under various assumptions. GuéLat and Marcotte (GuéLat & Marcotte, 1986) showed that in case the objective function is
strongly convex and the feasible set is a polytope, then in
case the optimal solution is located in the interior of the set,
the FW method converges exponentially fast. A similar result was presented in the work of Beck and Teboulle (Beck
& Teboulle, 2004) who considered a specific problem they
refer to a the convex feasibility problem over an arbitrary
convex set. They also obtained a linear convergence rate
under the assumption that an optimal solution that is far
enough from the boundary of the set exists.
Recently, Garber and Hazan (Garber & Hazan, 2013a) gave
the first natural linearly-converging FW variant without any
restricting assumptions on the location of the optimum.
They showed that a variant of the Frank Wolfe method
with the addition of away steps converges exponentially
fast in case the objective function is strongly convex and
the feasible set is a polytope. In follow-up work, Jaggi
and Lacoste-Julien (Lacoste-Julien & Jaggi, 2013) gave a
refined analysis of an algorithm presented in (GuéLat &
Marcotte, 1986) which also uses away steps and showed
that it also converges exponentially fast in the same setting
as the Garber-Hazan result. Also relevant in this context
is the work of Ahipasaoglu, Sun and Todd (Ahipasaoglu
et al., 2008) who showed that in the specific case of minimizing a smooth and strongly convex function over the unit
simplex, a variant of the Frank-Wolfe method that also uses

away steps converges with a linear rate.
In a different line of work, Migdalas and recently Lan
(Migdalas, 1994; Lan, 2013) considered the Frank-Wolfe
algorithm with a stronger optimization oracle that is able to
solve quadratic problems over the feasible domain. They
show that in case the objective function is strongly convex
then exponentially fast convergence is attainable. However,
in most settings of interest, an implementation of such a
non-linear oracle is computationally much more expensive
than the linear oracle, and the key benefit of the FrankWolfe method is lost.
In the specific case that the feasible set is strongly convex,
an assumption also made in this paper, Levitin and Polyak
showed in their classical work (Levitin & Polyak, 1966)
that under the restrictive assumption that the norm of the
gradient of the objective function is lower bounded by a
constant everywhere in the feasible set, the FW method
converges with an exponential rate. The same result appeared in following works by Demyanov and Rubinov (Demyanov & Rubinov, 1970) and Dunn (Dunn, 1979), both
also requiring that the magnitude of the gradients is lower
bounded by a constant everywhere in the feasible set. As
we later show, the lower bound requirement on the gradients is in a sense much stronger than requiring that the objective function is strongly convex. Under our assumption
however, which is slightly weaker than strong convexity of
the objective, the gradient may become arbitrarily small on
the feasible set.
We summarize previous convergence rate results for the
standard FW method in Table 1.1.

2. Preliminaries
2.1. Smoothness and Strong Convexity
For the following definitions let E be a finite vector space
and k · k, k · k⇤ be a pair of dual norms over E.
Definition 1 (smooth function). We say that a function f :
E ! R is smooth over a convex set K ⇢ E with respect
to k · k if for all x, y 2 K it holds that
f (y)  f (x) + rf (x) · (y

x) +

2

kx

yk2 .

Definition 2 (strongly convex function). We say that a
function f : E ! R is ↵-strongly convex over a convex
set K ⇢ E with respect to k · k if it satisfies the following
two equivalent conditions
1. 8x, y 2 K :
f (y)

f (x) + rf (x) · (y

x) +

↵
kx
2

yk2 .

Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets

Reference
(Jaggi, 2013)
(GuéLat & Marcotte, 1986)
(Beck & Teboulle, 2004)
(Levitin & Polyak, 1966)
(Demyanov & Rubinov, 1970)
(Dunn, 1979)
this paper

Feasible set K
convex
polytope
convex
strongly convex

Objective function f
convex
strongly convex
f (x) = kAx bk22
krf (x)k

strongly convex

c>0

8x 2 K

strongly convex

Location of x⇤
unrestricted
interior
interior

Conv. rate
1/t
exp( ⇥(t))
exp( ⇥(t))

unrestricted

exp( ⇥(t))

unrestricted

1/t2

Table 1. Comparison of convergence rates for the Frank-Wolfe method under different assumptions. We denote the optimal solution by
x⇤ . Note that since all results assume smoothness of the function we omit it from column 3.

2. 8x, y 2 K,

2 [0, 1] :

f ( x + (1

)y)



f (x) + (1
)f (y)
↵
(1
)kx yk2 .
2

The above definition (part 1) combined with first order optimality conditions imply that for a ↵-strongly convex function f , if x⇤ = arg minx2K f (x), then for any x 2 K
↵
f (x) f (x⇤ )
kx x⇤ k2 .
(1)
2
Eq. (1) further implies that the magnitude of the gradient
of f at point x, krf (x)k⇤ is at least of the order of the
square-root of the approximation error at x, f (x) f (x⇤ ).
This follows since
r
2
(f (x) f (x⇤ )) · krf (x)k⇤ kx x⇤ k · krf (x)k⇤
↵
(x x⇤ ) · rf (x)
f (x)

f (x⇤ ),

where the first inequality follows from (1), the second from
Holder’s inequality and the third from convexity of f . Thus
we have that at any point x 2 K it holds that
r
↵ p
krf (x)k⇤
· f (x) f (x⇤ ).
(2)
2

We will show that this property, that is in fact weaker than
strong convexity, combined with an additional property of
the convex set that we define next, allows to obtain the
faster rates 1 .
Definition 3 (strongly convex set). We say that a convex
set K ⇢ E is ↵-strongly convex with respect to k · k if for
any x, y 2 K, any 2 [0, 1] and any vector z 2 E such
that kzk = 1, it holds that
↵
x + (1
)y + (1
) kx yk2 z 2 K.
2
1

In this work we assume that the convex set K is fulldimensional. In case this assumption does not hold, e.g. if the
convex set is the unit simplex, then Eq. (2) holds even if we replace rf (x) with PS(K) [rf (x)] where PS(K) denotes the projection operator onto the smallest subspace that contains K.

That is, K contains a ball of of radius (1
) ↵2 kx yk2
induced by the norm k · k centered at x + (1
)y.
2.2. The Frank-Wolfe Algorithm
The Frank-Wolfe algorithm, also known as the conditional
gradient algorithm, is an algorithm for the minimization
of a convex function f : E ! R which is assumed to
be f -smooth with respect to a norm k · k, over a convex and compact set K ⇢ E. The algorithm implicitly
assumes that the convex set K is given in terms of a linear optimization oracle OK : E ! K which given a linear
objective c 2 E returns a point x = OK (c) 2 K such
that x 2 arg miny2K y · c. The algorithm is given below.
The algorithm proceeds in iterations, taking on each iteration t the new iterate xt+1 to be a convex combination between the previous feasible iterate xt and a feasible point
that minimizes the dot product with the gradient direction
at xt , which is generated by invoking the oracle OK with
the input vector rf (xt ). There are various ways to set the
parameter that controls the convex combination ⌘t in order
to guarantee convergence of the method. The option that
we choose here is the optimization of ⌘t via a simple line
search rule, which is straightforward and computationally
cheap to implement.
Algorithm 1 Frank-Wolfe Algorithm
1: Let x0 be an arbitrary point in K.
2: for t = 0, 1, ... do
3:
pt
OK (rf (xt )).
4:
⌘t
arg min⌘2[0,1] ⌘(pt
2 f
⌘ 2 kpt xt k2 .
5:
xt+1
xt + ⌘t (pt xt ).
6: end for

xt ) · rf (xt ) +

The following theorem states the well-known convergence
rate of the Frank-Wolfe algorithm for smooth convex minimization over a compact and convex set, without any further assumptions. A proof is given in the appendix for
completeness though similar proofs could also be found in
(Levitin & Polyak, 1966; Jaggi, 2013).

Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets

Theorem 1. Let x⇤ 2 arg minx2K f (x) and denote DK =
maxx,y2K kx yk (the diameter of the set with respect to
k · k). For every t 1 the iterate xt of Algorithm 1 satisfies
✓ ◆
2
8 f DK
1
⇤
f (xt ) f (x ) 
=O
.
t
t
2.3. Our Results
In this work, we consider the case in which the function
to optimize f is not only f -smooth with respect to k · k
but also ↵f -strongly convex with respect to k · k (we relax
this assumption a bit in subsection 4.3). We further assume
that the feasible set K is ↵K -strongly convex with respect
to k · k. Under these two additional assumptions alone we
prove the following theorem.
Theorem
2. Let x⇤ = arg minx2K f (x) and let M =
p
↵f ↵K
p
. Denote DK = maxx,y2K kx yk. For every
8 2 f
t 1 the iterate xt of Algorithm 1 satisfies
✓ ◆
2
max{ 92 f DK
, 18M 2 }
1
f (xt ) f (x⇤ ) 
=
O
.
(t + 2)2
t2

3. Proof of Theorem 2
We denote the approximation error of the iterate xt produced by the algorithm by ht . That is ht = f (xt ) f (x⇤ )
where x⇤ = arg minx2K f (x).
To better illustrate our results, we first shortly revisit the
proof technique of Theorem 1. The main observation to be
made is the following:
ht+1 = f (xt + ⌘t (pt

xt ))

xt ) · rf (xt ) +

rate will be attained. However in general, under the linear oracle assumption, we have no way to solve the linear
optimization problem over the intersection of K and a ball
without greatly increasing the number of calls to the linear
oracle, which is the most expensive step in many settings.
In case the feasible set K is strongly convex, then the main
observation to be made is that while the quantity kpt xt k
may still be much larger than kx⇤ xt k (the distance to the
optimum), in this case, the duality gap must also be large,
which results in faster convergence. This observation is
illustrated in Figure 1 and given formally in Lemma 1.
Lemma 1. On any iteration t of Algorithm 1 it holds that
1
ht+1  ht · max{ , 1
2

f (x⇤ ) 

⌘t2 f
kpt
2
2
⌘ f
ht + ⌘t (x⇤ xt ) · rf (xt ) + t kpt
2
⌘t2 f
2
(1 ⌘t )ht +
kpt xt k ,
2
ht + ⌘t (pt

Figure 1. For strongly convex sets, as in the left picture, the duality gap (denoted dg) increases with kpt xt k2 , which accelerates
the convergence of the Frank-Wolfe method. As shown in the picture on the right, this property clearly does not hold for arbitrary
convex sets.

xt k2 
xt k2 

Proof. By the optimality of the point pt we have that
(pt

(3)

where the the first inequality follows from the smoothness
of f , the second from the optimality of pt and the third
from convexity of f . Choosing ⌘t to be roughly 1/t yields
the convergence rate of 1/t stated in Theorem 1. This rate
cannot be improved in general since while the so-called duality gap (xt pt ) · rf (xt ) could be arbitrarily small (as
small as (xt x⇤ ) · rf (xt )), the quantity kpt xt k may
remain as large as the diameter of the set. Note that in case
f is strongly-convex, then according to Eq. (1) it holds that
xt converges to x⇤ and thus according to Eq. (3) it suffices
to solve the inner linear optimization problem in Algorithm
1 on the intersection of K and a small ball centered at xt .
As a result the quantity kpt xt k2 will be proportional to
the approximation error at time t, and a linear convergence

↵K krf (xt )k⇤
}.
8 f

xt ) · rf (xt )




(x⇤
⇤

f (x )

xt ) · rf (xt )
f (xt ) =

ht , (4)

where the second inequality follows from convexity of f .
Denote ct = 12 (xt + pt ) and wt 2 arg minw2E,kwk1 w ·
rf (xt ). Note that from Holder’s inequality we have that
wt ·rf (xt ) = krf (xt )k⇤ . Using the strong convexity of
the set K we have that the point p̃t = ct + ↵8K kxt pt k2 wt
is in K. Again using the optimality of pt we have that
(pt

xt ) · rf (xt )  (p̃t

xt ) · rf (xt )

1
↵K kxt pt k2
(pt xt ) · rf (xt ) +
wt · rf (xt )
2
8
2
1
↵K kxt pt k

ht
krf (xt )k⇤ ,
(5)
2
8
=

where the last inequality follows from Eq. (4).

Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets

We now analyze the decrease in the approximation error
ht+1 . By smoothness of f we have


f (xt+1 )

xt ) · rf (xt )

f (xt ) + ⌘t (pt
f

+

2

⌘t2 kpt

For the base case t = 1 we need to prove that h1 = f (x1 )
f (x⇤ )  C/4. By f smoothness of f we have
f (x1 )

f (x⇤ ) = f (x0 + ⌘0 (p0

xt k2 .

 h0 + ⌘0 (p0

Subtracting f (x⇤ ) from both sides we have
ht+1 

xt ) · rf (xt ) +

ht + ⌘t (pt

f

2

 h0 (1
⌘t2 kpt

xt k2 .
(6)

Plugging Eq. (5) we have
ht+1


+
=
+

⇣

⌘t ⌘
2

ht 1
f

↵K kxt pt k2
⌘t
krf (xt )k⇤
8

⌘t2 kpt

2

xt k
⌘
⌘t
ht 1
2 ✓
kxt pt k2
⌘t2
2
2⇣

f

⌘t

↵K krf (xt )k⇤
4

◆

ht+1

Otherwise, we can set ⌘t =
ht+1

↵K krf (xt )k⇤
4 f

✓
ht 1



↵K krf (xt )k⇤
8 f

◆

max{ 92
for all t

=

2
f DK , 18M

1, ht 

2

}.

and

2
DK

2
DK
,

1, that is

ht
C
C
(t + 3)2

=
·
2
2(t + 2)2
(t + 3)2 2(t + 2)2
C
.
(8)
(t + 3)2
1.

We now turn to the case in which the result of the max
operation in Eq. (7) is the second argument. We consider
two cases.
C
2(t+2)2

then as in Eq. (8) it holds for any t

1 that

C
C

,
2(t + 2)2
(t + 3)2

C
Otherwise, ht > 2(t+2)
2 . By Eq. (7) and the induction
assumption we have

⇣
ht+1  ht 1

C
<
(t + 2)2

p
↵ ↵K
pf
8 2 f

2

where the first inequality follows from Eq. (7).

We can now prove Theorem 2.
M



ht+1  ht 

Note that Lemma 1 only relies on the strong convexity of
the set K and did not assume anything regrading f beyond
convexity and smoothness. In particular it does not require
f to be strongly convex.

Proof. Let

2

where the last inequality holds for any t

If ht 

.

2
f ⌘0

2
f ⌘0

If the result of the max operation in Eq. (7) is the first
argument, that is 1/2, we have that



and get

x0 ) · rf (x0 ) +

Assume now that the induction holds for time t
C
ht  (t+2)
2.

.

ht
 .
2

f (x⇤ )

where the last inequality follows from convexity of f . By
the optimal choice of ⌘0 we can in particular set ⌘0 = 1
2
which gives h1  2f DK
 C/9.

ht+1

In case ↵K krf4 (xt )k⇤
f , by the optimal choice of ⌘t in
Algorithm 1, we can set ⌘t = 1 and get

⌘0 ) +

x0 ))

C

=

We prove by induction that

C
(t+2)2 .

Since we assume that the objective function f satisfies Eq.
(2), we have from Lemma 1 that on any iteration t,
p
↵K ↵f p
1
p
ht+1  ht · max{ , 1
ht }
2
8 2 f
1
1/2
= ht · max{ , 1 M ht }.
(7)
2

1/2

M ht
1

⌘

M

r

C 1
2 t+2

!

!
C 1
1 M
2 t+2
!
r
(t + 2)2 + 2t + 5
C
C 1
=
·
1 M
(t + 3)2
(t + 2)2
2 t+2
!
r
✓
◆
C
3(t + 2)
C 1
<
1+
1 M
(t + 3)2
(t + 2)2
2 t+2
!
r
✓
◆
C
3
C 1
=
1+
1 M
.
(t + 3)2
t+2
2 t+2
C
(t + 3)2
=
·
(t + 3)2 (t + 2)2

r

Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets

Thus for C
ht+1

we have that
✓
◆✓
C
3

1+
1
(t + 3)2
t+2
C
<
.
(t + 3)2

point p̃t as in the proof of Lemma 1 we define it to be
p̃t = x⇤ + rwt . Because of our assumption on the location of x⇤ , it holds that p̃t 2 K. We thus have that

18
M2

3
t+2

◆

(p̃t

xt ) · rf (xt ) = (x⇤t


xt ) · rf (xt ) + rwt · rf (xt )

rkrf (xt )k⇤ .

Plugging this into Eq. (6) we have

4. Derivation of Previous Fast Rates Results
and Extensions
4.1. Deriving the Linear Rate of Polayk & Levitin
Polyak & Levitin considered in (Levitin & Polyak, 1966)
the case in which the feasible set is strongly convex, the
objective function is smooth and there exists a constant g >
0 such that
8x 2 K :

krf (x)k⇤

g.

(9)

They showed that under the lower-bounded gradient assumption, Algorithm 1 converges with a linear rate, that
is e ⇥(t) . Clearly by plugging Eq. (9) into Lemma 1 we
have that on each iteration t
1
ht+1  ht · max{ , 1
2

↵k g
}.
8 f

which results in the same exponentially fast convergence
rate as in (Levitin & Polyak, 1966) and following works
such as (Demyanov & Rubinov, 1970; Dunn, 1979).
4.2. Deriving a Linear Rate for Arbitrary Convex Sets
in case x⇤ is in the Interior of the Set
Assume now that the feasible set K is convex but not necessarily strongly convex. We assume that the objective function f is smooth, convex, satisfies Eq. (2) with some constant ↵f and admits a minimizer (not necessarily unique)
x⇤ that lies in the interior of K, i.e. there exists a parameter r > 0 such that the ball of radius r with respect to
norm k · k centered at x⇤ is fully contained in K 2 . GuéLat
and Marcotte (GuéLat & Marcotte, 1986) showed the under
the above conditions, the Frank-Wolfe algorithm converges
with a linear rate. We now show how a slight modification
in the proof of Lemma 1 yields this linear convergence result.
Let wt be as in the proof of Lemma 1, that is wt 2
arg minw2E,kwk1 w · rf (xt ). Instead of defining the
2

We assume here that K is full-dimensional. In any other case,
we can assume instead that the intersection of the ball centered at
x⇤ with the smallest subspace containing K is fully contained in
K. In this case we also need to replace the gradient rf (x) with
its projection onto this subspace, see also footnote 1.

ht+1




ht
ht

⌘t rkrf (xt )k⇤ +
r
↵f p
⌘t r
ht +
2

2 2
f ⌘t D K

2

2 2
f ⌘t D K

2

.

where DK denotes the diameter of K with respect to norm
k · k and the second inequality follows from p
Eq. (2). By the
optimal choice of ⌘t , we can set ⌘t =
ht+1  ht

p
r ↵f ht
p
2
2 f DK

and get

r 2 ↵f
2 ht ,
4 f DK

which results in a linear convergence result.
4.3. Relaxing the Strong Convexity of f
So far we have considered the case in which the objective
function f is strongly convex. Notice however that our
main instrument for proving the accelerated convergence
rate, i.e. Lemma 1, did not rely directly on strong convexity of f , but on the magnitude of the gradient, krf (xt )k⇤ .
We have seen in Eq. (2) that indeed if f is strongly
convex
p
than the gradient is at least of the order of ht . We now
show that there exists functions which are not strongly convex but still satisfy Eq. (2) and hence our results apply also
for them.
Consider the function
f (x) =

1
kAx
2

bk22 .

where x 2 Rn , A 2 Rm⇥n , b 2 Rm . Assume that m < n
and all rows of A are linearly independent. In this case
the optimization problem minx2K f (x) is the problem of
finding a point in K that best satisfies an under-determined
linear system in terms of the mean square error. An application of the Frank-Wolfe method to this problem was
studied in (Beck & Teboulle, 2004). Under these assumptions, the function f is smooth and convex but not strongly
convex since the Hessian matrix given by A> A is not positive definite (note however that the matrix AA> is positive
definite).
The gradient of f is given by
rf (x) = A> (Ax

b).

Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets

5.2. `p Balls for p 2 (1, 2]

Thus we have that
krf (x)k22 = [A> (Ax

b)]> [A> (Ax

b)]

>

)kAx bk22
⇣1
2 min (AA> ) kAx bk22
2
⌘
1
⇤
2
kAx
bk2 ,
2
min (AA

Given a parameter p

1, consider the `p ball of radius r,

Bp (r) = {x 2 Rn | kxkp  r}.
The following lemma is proved in (Shwartz, 2007).
Lemma 4. Fix p 2 (1, 2]. The function 12 kxk2p is (p
strongly convex w.r.t. the norm k·kp .

1)-

where min (AA> ) denotes the smallest eigenvalue of
AA> . Since AA> is positive definite, min (AA> ) > 0
and it follows that f satisfies Eq. (2).

The following corollary is a consequence of combining
Lemma 4 and Lemma 3. The proof is given in the appendix

Combining the result of this subsection with the previous
one yields the linear convergence rate of the Frank-Wolfe
method applied to the convex feasibility problem studied in
(Beck & Teboulle, 2004).

convex with respect to the norm k · kp and (p
strongly convex with respect to the norm k · k2 .

5. Examples of Strongly Convex Sets
In this section we explore convex sets for which Theorem
2 is applicable. That is, convex sets which on one hand are
strongly convex, and on the other, admit a simple and efficient implementation of a linear optimization oracle. We
show that various norms that give rise to natural regularization functions in machine learning, induce convex sets
that fit both of the above requirements. A summary of our
findings is given in Table 5. We note that in all cases in
which the norm parameter p is smaller than 2 (or one of the
parameters s, p in case of group norms), we are not aware
of a practical algorithm for computing the projection.
5.1. Partial Characterization of Strongly Convex Sets
The following lemma is taken from (Journée et al., 2010)
(Theorem 12).

Corollary 1. Fix p 2 (1, 2]. The set Bp (r) is

p 1
r -strongly
1

1)n 2
r

1
p

The following lemma establishes that linear optimization
over Bp (r) admits a simple closed-form solution that can
be computed in time that is linear in the number of nonzeros in the linear objective. The proof is given in the appendix.
Lemma 5. Fix p 2 (1, 2], r > 0 and a linear objective
c 2 Rn . Let x 2 Rn such that xi = kckrq 1 sgn(ci )|ci |q 1
q

where q satisfies: 1/q + 1/p = 1, and sgn(·) is the sign
function. Then x = arg miny2Bp (r) y · c
5.3. Schatten `p Balls for p 2 (1, 2]

Given a matrix X 2 Rm⇥n we denote by (X) the vector of singular values of X in descending order, that is
(X)1
(X)2
... (X)min(m,n) . The Schatten `p
norm is given by
0

kXkS(p) = k (X)kp = @

min(m,n)

X
i=1

11/p

(X)pi A

.

Lemma 2. Let E be a finite vector space and let f : E ! R
be non-negative, ↵-strongly convex and -smooth. Then
the set K = {x | f (x)  r} is p2↵ r -strongly convex.

Consider the Schatten `p ball of radius r,

This lemma for instance shows that the Euclidean ball of radius r is 1/r-strongly convex (by applying the lemma with
f = kxk22 ).

The following lemma is taken from (Kakade et al., 2012).

The following lemma will be useful to prove that convex
sets that are induced by certain norms, which do not correspond to a smooth function as in the previous lemma, are
strongly convex. The proof is given in the appendix.

Lemma 3. Let E be a finite vector space, let k·k be a norm
over E and assume that the function f (x) = kxk2 is ↵strongly convex over E with respect to the norm k·k. Then
↵
for any r > 0, the set Bk·k (r) = {x 2 E | kxk  r} is 2r
strongly convex with respect to k · k.

-

BS(p) (r) = {X 2 Rm⇥n | kXkS(p)  r}.
Lemma 6. Fix p 2 (1, 2]. The function 12 kXk2S(p) is (p
1)-strongly convex w.r.t. the norm k·kS(p) .
The proof of the following corollary follows the exact same
lines as the proof of Corollary 1 by using Lemma 6 instead
of Lemma 4.
Corollary 2. Fix p 2 (1, 2]. The set BS(p) (r) is
p 1
r -strongly convex with respect to the norm k · kS(p)
1
2

1
p

and (p 1) min(m,n)
-strongly convex with respect to the
r
frobenius norm k · kF .

Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets

E
Rn
Rm⇥n
Rm⇥n

Domain name
`p ball, p 2 (1, 2]
Schatten `p ball, p 2 (1, 2]
Group `s,p ball, s, p 2 (1, 2]

Domain expression
{x 2 Rn | kxkp  r}
{X 2 Rm⇥n | k (X)kp  r}
{X 2 Rm⇥n | kXks,p  r}

S.C. parameter
p 1
r
p 1
r
(s 1)(p 1)
(s+p 2)r

Complexity of lin. opt.
O(nnz)
O(n3 ) (SVD)
O(nnz)

Table 2. Examples of strongly convex sets with corresponding strong convexity parameter and complexity of a linear optimization oracle
implementation . nnz denotes the number of non-zero entries in the linear objective and (X) denotes the vector of singular values.

The following lemma establishes that linear optimization
over BS(p) (r) admits a simple closed-form solution given
the singular value decomposition of the linear objective.
The proof is given in the appendix.
Lemma 7. Fix p 2 (1, 2], r > 0 and a linear objective C 2 Rm⇥n . Let C = U ⌃V > be the singular value
decomposition of C. Let be a vector such that i =
r
(C)qi 1 where q satisfies: 1/q + 1/p = 1. Fik (C)kq 1
q

nally, let X = U Diag( )V > where Diag( ) is an m ⇥ n
diagonal matrix with the vector as the main diagonal.
Then X = arg minY 2BS(p) (r) Y • C, where • denotes the
standard inner product for matrices.
5.4. Group `s,p Balls for s, p 2 (1, 2]

n

The `s,p norm of X is given by,
kXks,p = k(kX1 ks , kX2 ks , ..., kXm ks )kp .
We define the `s,p ball as follows:
Bs,p (r) = {X 2 Rm⇥n | kXks,p  r}.
The proof of the following lemma is given in the appendix.
1)(p 1)
Lemma 8. Let s, p 2 (1, 2]. The set Bs,p (r) is (s(s+p
2)r strongly convex with respect to the norm k · ks,p and
1
1
1
1
1)(p 1)
n s 2 m p 2 (s(s+p
2)r -strongly convex with respect to the
frobenius norm k · kF .

The following lemma establishes that linear optimization
over Bs,p (r) admits a simple closed-form solution that can
be computed in time that is linear in the number of nonzeros in the linear objective. The proof is given in the appendix.
Lemma 9. Fix s, p 2 (1, 2], r > 0 and a linear objective C 2 Rm⇥n . Let X 2 Rm⇥n be such that
Xi,j = kCkq 1rkC kz q sgn(Ci,j )|Ci,j |z 1 where z satisz,q

i z

In this paper we proved that the Frank-Wolfe algorithm
converges at an accelerated rate of O(1/t2 ) for smooth and
strongly-convex optimization over strongly-convex sets,
beating the known tight convergence rate of the method for
general smooth and convex optimization. This is one of the
very few known results that achieve such an improvement
in convergence rate under natural and standard assumptions
(i.e. strong convexity etc.). We have further demonstrated
that various regularization functions in machine learning
give rise to strongly convex sets. We have also demonstrated how previous fast convergence rate results follow
easily from our analysis.
The following questions naturally arise.

Given a matrix X 2 R
denote by Xi 2 R the ith row
of X. That is X = (X1 , X2 , ..., Xm )> .
m⇥n

6. Conclusions and Open Problems

fies: 1/s + 1/z = 1, q satisfies: 1/p + 1/q = 1 and Ci denotes the ith row of C. Then X = arg minY 2Bs,p (r) Y • C,
where • denotes the standard inner product for matrices.

It is known that in case the objective function is both
smooth and strongly convex, projection/prox-based methods achieve a convergence rate of O(log(1/✏)). Is it possible to achieve such a rate for the FW method in case the
convex set is strongly convex?
We have shown that it is possible to obtain faster rates for
optimization over balls induced by norms that give rise to
strongly convex functions. Is it possible to obtain faster
rates for balls induced by norms that do not give rise to
strongly convex functions (but rather to smooth functions)?
e.g. is it possible to obtain faster rates for `p balls for p > 2.
Finally, the most intriguing question is to give a linear optimization oracle-based method that enjoys the same convergence rate, at least in terms of the approximation error, as optimal projection/prox-based gradient methods, in
any regime (including non-smooth problems). A progress
in this direction was made recently by Garber and Hazan
(Garber & Hazan, 2013b) who showed that in case the feasible set is a polytope, a variant of the FW-method obtains the same rates as the projected (sub)gradient descent
method.

Acknowledgments
The research leading to these results has received funding
from the European Unions Seventh Framework Programme
(FP7/2007-2013) under grant agreement n 336078 – ERCSUBLRN.

Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets

References
Ahipasaoglu, S. Damla, Sun, Peng, and Todd, Michael J.
Linear convergence of a modified frank-wolfe algorithm
for computing minimum-volume enclosing ellipsoids.
Optimization Methods and Software, 23(1):5–19, 2008.
Beck, Amir and Teboulle, Marc. A conditional gradient
method with linear rate of convergence for solving convex linear systems. Math. Meth. of OR, 59(2):235–247,
2004.
Clarkson, Kenneth L. Coresets, sparse greedy approximation, and the frank-wolfe algorithm. In Proceedings of
the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA, 2008.
Demyanov, Vladimir F. and Rubinov, Aleksandr M. Approximate methods in optimization problems. Elsevier
Publishing Company, 1970.
Dudı́k, Miroslav, Harchaoui, Zaı̈d, and Malick, Jérôme.
Lifted coordinate descent for learning with trace-norm
regularization. Journal of Machine Learning Research Proceedings Track, 22:327–336, 2012.

Jaggi, Martin. Revisiting frank-wolfe: Projection-free
sparse convex optimization. In Proceedings of the 30th
International Conference on Machine Learning, ICML,
2013.
Jaggi, Martin and Sulovský, Marek. A simple algorithm for
nuclear norm regularized problems. In Proceedings of
the 27th International Conference on Machine Learning,
ICML, 2010.
Journée, Michel, Nesterov, Yurii, Richtárik, Peter, and
Sepulchre, Rodolphe. Generalized power method for
sparse principal component analysis. Journal of Machine Learning Research, 11:517–553, 2010.
Kakade, Sham M., Shalev-Shwartz, Shai, and Tewari, Ambuj. Regularization techniques for learning with matrices. Journal of Machine Learning Research, 13:1865–
1890, 2012.
Lacoste-Julien, Simon and Jaggi, Martin. An affine invariant linear convergence analysis for frank-wolfe algorithms. CoRR, abs/1312.7864, 2013.

Dunn, Joseph C. Rates of Convergence for Conditional
Gradient Algorithms Near Singular and Nonsingular Extremals. SIAM Journal on Control and Optimization, 17
(2), 1979.

Lacoste-Julien, Simon, Jaggi, Martin, Schmidt, Mark W.,
and Pletscher, Patrick. Block-coordinate frank-wolfe optimization for structural svms. In Proceedings of the 30th
International Conference on Machine Learning, ICML,
2013.

Frank, M. and Wolfe, P. An algorithm for quadratic programming. Naval Research Logistics Quarterly, 3:149–
154, 1956.

Lan, Guanghui. The complexity of large-scale convex programming under a linear optimization oracle. CoRR,
abs/1309.5550, 2013.

Garber, Dan and Hazan, Elad. Playing non-linear games
with linear oracles. In 54th Annual IEEE Symposium on
Foundations of Computer Science, FOCS, 2013a.

Laue, Sören. A hybrid algorithm for convex semidefinite
optimization. In Proceedings of the 29th International
Conference on Machine Learning, ICML, 2012.

Garber, Dan and Hazan, Elad. A linearly convergent conditional gradient algorithm with applications to online and
stochastic optimization. CoRR, abs/1301.4666, 2013b.

Levitin, Evgeny S and Polyak, Boris T. Constrained minimization methods. USSR Computational mathematics
and mathematical physics, 6:1–50, 1966.

GuéLat, Jacques and Marcotte, Patrice. Some comments
on Wolfe’s ‘away step’. Mathematical Programming, 35
(1), 1986.

Migdalas, Athanasios. A regularization of the frankwolfe
method and unification of certain nonlinear programming methods. Mathematical Programming, 65:331–
345, 1994.

Harchaoui, Zaı̈d, Douze, Matthijs, Paulin, Mattis, Dudı́k,
Miroslav, and Malick, Jérôme. Large-scale image classification with trace-norm regularization. In IEEE Conference on Computer Vision and Pattern Recognition,
CVPR, 2012.
Hazan, Elad. Sparse approximate solutions to semidefinite
programs. In 8th Latin American Theoretical Informatics Symposium, LATIN, 2008.
Hazan, Elad and Kale, Satyen. Projection-free online learning. In Proceedings of the 29th International Conference
on Machine Learning, ICML, 2012.

Shalev-Shwartz, Shai, Gonen, Alon, and Shamir, Ohad.
Large-scale convex minimization with a low-rank constraint. In Proceedings of the 28th International Conference on Machine Learning, ICML, 2011.
Shwartz, Shay Sahlev. Phd thesis. 2007.

