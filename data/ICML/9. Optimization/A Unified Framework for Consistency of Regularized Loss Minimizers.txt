A Unified Framework for Consistency of Regularized Loss Minimizers
Jean Honorio
Tommi Jaakkola
CSAIL, MIT, Cambridge, MA 02139, USA

Abstract
We characterize a family of regularized loss minimization problems that satisfy three properties:
scaled uniform convergence, super-norm regularization, and norm-loss monotonicity. We show
several theoretical guarantees within this framework, including loss consistency, norm consistency, sparsistency (i.e. support recovery) as well
as sign consistency. A number of regularization
problems can be shown to fall within our framework and we provide several examples. Our results can be seen as a concise summary of existing guarantees but we also extend them to new
settings. Our formulation enables us to assume
very little about the hypothesis class, data distribution, the loss, or the regularization. In particular, many of our results do not require a bounded
hypothesis class, or identically distributed samples. Similarly, we do not assume boundedness,
convexity or smoothness of the loss nor the regularizer. We only assume approximate optimality
of the empirical minimizer. In terms of recovery, in contrast to existing results, our sparsistency and sign consistency results do not require
knowledge of the sub-differential of the objective
function.

1. Introduction
Several problems in machine learning can be modeled as
the minimization of an empirical loss, which is computed
from some available training data. Assuming that data samples come from some unknown arbitrary distribution, we
can define the expected loss as the expected value of the
empirical loss. The minimizers of the empirical and expected loss are called the empirical minimizer and the true
hypothesis, respectively.
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

JHONORIO @ CSAIL . MIT. EDU
TOMMI @ CSAIL . MIT. EDU

One of the goals in machine learning is to infer properties
of the true hypothesis by having access to a limited amount
of training data. One largely used property in the context of
classification and regression is loss consistency which measures the generalization ability of the learning algorithm.
Loss consistency guarantees are usually stated as an upper
bound on the difference between the expected loss of the
empirical minimizer and that of the true hypothesis. Another set of properties relates to the ability to recover the
true hypothesis. Norm consistency measures the distance
between the empirical minimizer and the true hypothesis.
Sparsistency refers to the recovery of the sparsity pattern
(i.e. support recovery) of the true hypothesis, while sign
consistency refers to the recovery of the signs of the true
hypothesis. We expect these guarantees to become stronger
as we have access to more data samples.
In many settings, these guarantees are made possible by
the use of a regularizer in the learning process. Consistency guarantees are now available for several specific regularized loss minimization problems. We can hardly do
justice to the body of prior work, and we provide a few
references here. The work on linear regression includes
the analysis of: the sparsity promoting `1 -norm (Wainwright, 2009b), the multitask `1,2 and `1,1 -norms (Negahban & Wainwright, 2011; Obozinski et al., 2011), the
multitask `1,2 -norm for overlapping groups (Jacob et al.,
2009), the dirty multitask regularizer (Jalali et al., 2010),
the Tikhonov regularizer (Hsu et al., 2012), and the trace
norm (Bach, 2008). The analysis of `1 -regularization has
also been performed for: the estimation of exponential family distributions (Kakade et al., 2010; Ravikumar et al.,
2008; Wainwright et al., 2006), generalized linear models (Kakade et al., 2010; van de Geer, 2008; Yang et al.,
2013), and SVMs and logistic regression (Rocha et al.,
2009; van de Geer, 2008). These works have focused on
norm consistency and sparsistency, with the exception of
(Jalali et al., 2010; Obozinski et al., 2011; Ravikumar et al.,
2008; Rocha et al., 2009; Wainwright, 2009b; Wainwright
et al., 2006) which also analyzed sign consistency, and
(Hsu et al., 2012; Kakade et al., 2010) which also analyzed
loss consistency. We refer the interested reader to the article of (Negahban et al., 2012) for additional references.

A Unified Framework for Consistency of Regularized Loss Minimizers

There has been some notable contributions which characterize general frameworks with theoretical guarantees.
Loss consistency for bounded losses and different notions of stability of the learning algorithm was analyzed
in (Bousquet & Elisseeff, 2002; Mukherjee et al., 2006;
Rakhlin et al., 2005; Shalev-Shwartz et al., 2010). Stability follows from the use of regularization for many different problems (Bousquet & Elisseeff, 2002). A twolevel framework for loss consistency of bounded losses
was provided by (van de Geer, 2005): a regularized outerminimization is performed with respect to a set of model
classes, while an unregularized inner-minimization is done
with respect to functions on each class. Norm consistency
for restricted strongly convex (i.e. strongly convex with
respect to a subset of directions) losses and certain type
of regularizers was analyzed in (Lee et al., 2013; Loh &
Wainwright, 2013; Negahban et al., 2009; 2012; Yang &
Ravikumar, 2013). In (Lee et al., 2013; Negahban et al.,
2009; 2012; Yang & Ravikumar, 2013) the loss is differentiable and convex, and the regularizer is a mixture of decomposable norms; while in (Loh & Wainwright, 2013)
the loss is differentiable and nonconvex, and the regularizer is coordinate-separable, symmetric and nondecreasing,
among other technical requirements. The work of (Bousquet & Elisseeff, 2002; Mukherjee et al., 2006; Rakhlin
et al., 2005; Shalev-Shwartz et al., 2010; van de Geer,
2005) focus on loss consistency and requires an everywhere
bounded loss. On the other hand, the work of (Lee et al.,
2013; Loh & Wainwright, 2013; Negahban et al., 2009;
2012; Yang & Ravikumar, 2013) focus on norm consistency and requires a differentiable loss. The framework
of (van de Geer, 2005) requires a measure of complexity
for each model class as well as over all classes (which are
infinity for the problems that we analyze here). Finally, the
availability of independent and identically distributed samples is a requirement for all these previous works.
In this paper, we characterize a family of regularized loss
minimization problems that fulfill three properties: scaled
uniform convergence, super-scale regularization and normloss monotonicity. We show loss consistency, norm consistency, sparsistency and sign consistency. We show that
several problems in the literature fall in our framework,
such as the estimation of exponential family distributions,
generalized linear models, matrix factorization problems,
nonparametric models and PAC-Bayes learning. Similarly,
several regularizers fulfill our assumptions, such as sparsity
promoting priors, multitask priors, low-rank regularizers,
elastic net, total variation, dirty models, quasiconvex regularizers, among others. Note that our theoretical results
imply that loss consistency, norm consistency, sparsistency
and sign consistency hold for any combination of losses
and regularizers that we discuss here. Many of these combinations have not been previously explored.

2. Preliminaries
We first characterize a general regularized loss minimization problem. To this end, we define a problem as a tuple
⇧ = (H, D, Lbn , R) for a hypothesis class H, a data distribution D, an empirical loss Lbn and a regularizer R. For
clarity of presentation, we assume that H is a normed vector space.
Let ✓ be a hypothesis belonging to a possibly unbounded
hypothesis class H. Let Lbn (✓) be the empirical loss of n
samples drawn from a distribution D. We do not assume either independence or identical distribution of the samples.
Let R(✓) be a regularizer and n > 0 be a penalty parameter. The empirical minimizer is given by:
b⇤ = arg min Lbn (✓) +
✓
n
✓2H

(1)

n R(✓)

We relax this optimality assumption by defining an ⇠bn with the following
approximate empirical minimizer ✓
property for ⇠ 0:
bn ) +
Lbn (✓

b

n R(✓ n )

 ⇠ + min Lbn (✓) +
✓2H

n R(✓)

(2)

Let L(✓) = ED [Lbn (✓)] be the expected loss. The true
hypothesis is given by:
✓ ⇤ = arg min L(✓)

(3)

✓2H

In this paper, we do not assume boundedness, convexity or
smoothness of Lbn , R or L, although convexity is a very
useful property from an optimization viewpoint.

In order to illustrate the previous setting, consider for instance a function `(x|✓) to be the loss of a data sample
P
x given ✓. We can define Lbn (✓) = n1 i `(x(i) |✓) to be
the empirical loss of n i.i.d. samples x(1) , . . . , x(n) drawn
from a distribution D. Then, L(✓) = Ex⇠D [`(x|✓)] is
the expected loss of x drawn from a distribution D. Our
framework is far more general than this specific example.
We do not require identically distributed samples.
In order to gain some theoretical understanding of the
general problem defined above, we make some reasonable assumptions. Therefore, we extend the problem tuple with additional terms related to these assumptions. That is, we define a problem as a tuple ⇧0 =
(H, D, Lbn , R, "n, , c, r, b) for a hypothesis class H, a data
distribution D, an empirical loss Lbn , a regularizer R, a uniform convergence rate "n, and scale function c, a regularizer lower bound r and norm-loss function b. In what follows, we explain in detail the use of these additional terms.

A Unified Framework for Consistency of Regularized Loss Minimizers

2.1. Scaled Uniform Convergence

2.3. Norm-Loss Monotonicity

First, we present our definition of scaled uniform convergence, which differs from regular uniform convergence.
In both uniform convergence schemes, the goal is to find
a bound on the difference between the empirical and expected loss for all ✓. In regular uniform convergence such
bound is the same for all ✓, while in scaled uniform convergence the bound depends on the “scale” of ✓. For finite
and infinite dimensional vector spaces as well as for function spaces, we can choose the scale to be the norm of ✓.
For the space of probability distributions, we can choose
the scale to be the Kullback-Leibler divergence from the
distribution ✓ to a prior ✓ (0) . Next, we formally state our
definition.

Finally, we state our last assumption of norm-loss monotonicity. For clarity of presentation, we use the `1 -norm
in the following assumption. (The use of other norm that
upper-bounds the `1 -norm, modifies the norm consistency
result in Theorem 2 with respect to the new norm, and
leaves the sparsistency and sign consistency result in Theorem 3 unchanged.)

Assumption A (Scaled uniform convergence). Let c :
H ! [0; +1) be the scale function. The empirical loss
Lbn is close to its expected value L, such that their absolute difference is proportional to the scale of the hypothesis
✓. That is, with probability at least 1
over draws of n
samples:
(8✓ 2 H) |Lbn (✓)

L(✓)|  "n, c(✓)

(4)

where the rate "n, is nonincreasing with respect to n and
. Furthermore, assume limn!+1 "n, = 0 for 2 (0; 1).
In settings with a bounded complexity measure (e.g. empirical risk minimization with finite hypothesis class, VC
dimension, Rademacher complexity), the regular uniform
convergence statement is as follows (8✓ 2 H) |Lbn (✓)
L(✓)|  "n, . This condition is sufficient for loss consistency and regularization is unnecessary. This also occurs
with a bounded hypothesis class H, since in that case we
can relax Assumption A to (8✓ 2 H) |Lbn (✓) L(✓)| 
"n, max✓2H c(✓).
2.2. Super-Scale Regularizers
Next, we define super-scale regularizers. That is, regularizers that are lower-bounded by a scale function.
Assumption B (Super-scale regularization). Let c : H !
[0; +1) be the scale function. Let r : [0; +1) ! [0; +1)
be a function such that:
(8z

0) z  r(z)

(5)

(8✓ 2 H) b(k✓

✓ ⇤ k1 )  L(✓)

L(✓ ⇤ )

(7)

Furthermore, we define the inverse of function b as:
b† (z) = max
z0
0
b(z )=z

(8)

As we discuss later, nonsmooth strongly convex functions,
“minimax bounded” functions (which includes some convex functions and all strictly convex functions) and some
family of nonconvex functions fulfill this assumption.
Note that Assumption C is with respect to the expected loss
and therefore it holds in high dimensional spaces (e.g. for
domains with nonzero Lebesgue measure). On the other
hand, it is trivial to provide instances where strong convexity with respect to the empirical loss does not hold in high
dimensions, as shown by (Negahban et al., 2009; 2012;
Bickel et al., 2009) in the context of linear regression.

3. Theoretical Results
3.1. Loss Consistency
First, we provide a worst-case guarantee of the difference
between the expected loss of the ⇠-approximate empirical
bn and that of the true hypothesis ✓ ⇤ .
minimizer ✓
Theorem 1 (Loss consistency). Under Assumptions A and
B, regularized loss minimization is loss-consistent. That is,
for ↵ 1 and n = ↵"n, , with probability at least 1
:
bn )
L(✓

L(✓ ⇤ )  "n, (↵R(✓ ⇤ ) + c(✓ ⇤ )) + ⇠

(9)

(See Appendix A for detailed proofs.)

The regularizer R is bounded as follows:
(8✓ 2 H) r(c(✓))  R(✓) < +1

Assumption C (Norm-Loss monotonicity). Let b :
[0; +1) ! [0; +1) be a nondecreasing function such that
b(0) = 0. The expected loss L around the true hypothesis
✓ ⇤ is lower-bounded as follows:

(6)

Note that the above assumption implies that c(✓)  R(✓).
We opted to introduce the r function for clarity of presentation.

Given the above result, one would be tempted to make
R(✓) = c(✓) in order to minimize the upper bound. In
practice, the function c(·) is chosen in order to get a good
rate "n, in Assumption A. The regularizer is chosen in order to obtain some desired structure in the empirical minibn .
mizer ✓

A Unified Framework for Consistency of Regularized Loss Minimizers

3.2. Norm Consistency
Here, we provide a worst-case guarantee of the distance
bn and the
between the ⇠-approximate empirical minimizer ✓
⇤
true hypothesis ✓ .
Theorem 2 (Norm consistency). Under Assumptions A, B
and C, regularized loss minimization is norm-consistent.
That is, for ↵
1 and n = ↵"n, , with probability at
least 1
:
bn
k✓

✓ ⇤ k1  b† ("n, (↵R(✓ ⇤ ) + c(✓ ⇤ )) + ⇠)

(10)

As mentioned before, we use the `1 -norm for clarity of
presentation. (The use of other norm that upper-bounds
the `1 -norm in Assumption C, modifies Theorem 2 with
respect to the new norm.)

⇤
e
S(✓)=S(✓
) , (8i 2 S(✓ ⇤ )) sgn(✓ei ) = sgn(✓i⇤ )

(13)

Our result also holds for the “approximately sparse” setting
by constructing a thresholded version of the true hypothesis. That is, we guarantee correct sign recovery for all i
such that |✓i⇤ | > 2⌧ .

4. Examples
4.1. Losses with Scaled Uniform Convergence

3.3. Sparsistency and Sign Consistency
Next, we analyze the exact recovery of the sparsity pattern
(i.e. support recovery or sparsistency) as well as the signs
(i.e. sign consistency) of the true hypothesis ✓ ⇤ , by using
bn in order to infer
the ⇠-approximate empirical minimizer ✓
these properties. A related problem is the estimation of
the sparsity level of the empirical minimizer as analyzed
in (Bickel et al., 2009; Kakade et al., 2010). Here, we are
interested in the stronger guarantee of perfect recovery of
the support and signs.
Our approach is to perform thresholding of the empirical minimizer. In the context of `1 -regularized linear regression, thresholding has been previously used for obtaining sparsistency and sign consistency (Meinshausen & Yu,
2009; Zhou, 2009). (See Appendix B for additional discussion.)
Next, we formally define the support of a hypothesis and a
thresholding operator. The support S of a hypothesis ✓ is
the set of its nonzero elements, i.e.:
S(✓) = {i | ✓i 6= 0}

followed by hard-thresholding is sparsistent and signconsistent. More formally, for ↵ 1, n = ↵"n, and ⌧ =
e = h(✓
bn , ⌧ )
b† ("n, (↵R(✓ ⇤ ) + c(✓ ⇤ )) + ⇠), the solution ✓
has the same support and signs as the true hypothesis ✓ ⇤
provided that mini2S(✓⇤ ) |✓i⇤ | > 2⌧ . That is, with probability at least 1
:

(11)

A hard-thresholding operator h : H ⇥ R ! H converts
to zero the elements of the hypothesis ✓ that have absolute
value smaller than a threshold ⌧ . That is, for each i we
have:
hi (✓, ⌧ ) = ✓i 1[|✓i | > ⌧ ]
(12)
In what follows, we state our sparsistency and sign consistency guarantees. The minimum absolute value of the entries in the support has been previously used in (Ravikumar
et al., 2008; Tibshirani, 2011; Wainwright, 2009a;b; Zhou,
2009).
Theorem 3 (Sparsistency and sign consistency). Under
Assumptions A, B and C, regularized loss minimization

First, we show that several problems in the literature have
losses that fulfill Assumption A.
Maximum Likelihood Estimation for Exponential Family Distributions. First, we focus on the problem of
learning exponential family distributions (Kakade et al.,
2010). This includes for instance, the problem of learning the parameters (and possibly structure) of Gaussian and
discrete MRFs. While the results in (Kakade et al., 2010;
Ravikumar et al., 2008) concentrate on `1 -norm regularization, here we analyze arbitrary norms.
Claim i (MLE for exponential family).
Let t(x) be the sufR
ficient statistics and Z(✓) = x eht(x),✓i be the partition
b n = 1 P t(x(i) )
function. Given n i.i.d. samples, let T
i
n
and T = Ex⇠D [t(x)] be the empirical and expected sufb n , ✓i +
ficient statistics, respectively. Let Lbn (✓) = hT
log Z(✓) and L(✓) = hT, ✓i + log Z(✓) be the empirical and expected negative log-likelihood, respectively. Assumption A holds with probability at least 1 , scale function c(✓) = k✓k and rate "n, , provided that the dual norm
b n Tk⇤  "n, .
fulfills kT
Forpsub-Gaussian t(x), we can obtain a rate "n, 2
O( 1/n log 1/ ) for n independent samples.pWhile for finite variance, we can obtain a rate "n, 2 O( 1/(n )). (See
Appendix D.)

Generalized Linear Models. We focus on generalized
linear models, which generalizes linear regression when
Gaussian noise is assumed. This also includes for instance, logistic regression and compressed sensing with
exponential-family noise (Rish & Grabarnik, 2009). For
simplicity, we chose to analyze the fixed design model.
That is, we analyze the case in which y is a random variable
and x is a constant.

A Unified Framework for Consistency of Regularized Loss Minimizers

Claim ii (GLM with fixed design).
Let t(y) be the sufR
ficient statistics and Z(⌫) = y et(y)⌫ be the partition
function.
Given n independent samples, let Lbn (✓) =
P
1
(i)
(i)
(i)
i t(y )hx , ✓i + log Z(hx , ✓i) be the empirical
n
(i)
negative log-likelihood of y given their linear predictors hx(i) , ✓i. Let L(✓) = E(8i) y(i) ⇠Di [Lbn (✓)]. Assumption A holds with probability at least 1
, scale function
c(✓) = k✓k
and
rate
"
,
provided
that
the dual norm
n,
P
fulfills k n1 i (t(y (i) ) Ey⇠Di [t(y)])x(i) k⇤  "n, .
Forpsub-Gaussian t(y), we can obtain a rate "n, 2
O( 1/n log 1/ ) for n independent samples. While
p for finite variance, we can obtain a rate "n, 2 O( 1/(n )).
Both cases hold for bounded kxk⇤ . (See Appendix D.)

Matrix Factorization. We focus on two problems:
exponential-family PCA and max-margin matrix factorization. We assume that the hypothesis ✓ is a matrix. That is,
✓ 2 H = Rn1 ⇥n2 . We assume that each entry in the random matrix X 2 Rn1 ⇥n2 is independent, and might follow
a different distribution. Additionally, we assume that the
matrix size grows with n. That is, we let n = n1 n2 .
First, we analyze exponential-family PCA, which was introduced by (Collins et al., 2001) as a generalization of the
more common Gaussian PCA.
Claim iii (Exponential-family PCA).
R Let t(y) be the sufficient statistics and Z(⌫) = y et(y)⌫ be the partition function. Assume the entries of the random matrix
X 2 Rn1 ⇥n2Pare independent. Let n = n1 n2 and let
Lbn (✓) = n1 ij t(xij )✓ij + log Z(✓ij ) be the empirical negative log-likelihood of xij given ✓ij . Let L(✓) =
E(8ij) xij ⇠Dij [Lbn (✓)]. Assumption A holds with probability at least 1
, scale function c(✓) = k✓k and
rate "n, , provided that the dual norm fulfills k n1 (t(x11 )
Ex⇠D11 [t(x)], . . . , t(xn1 n2 ) Ex⇠Dn1 n2 [t(x)])k⇤  "n, .
For sub-Gaussian
t(xij ), we can obtain a rate "n, 2
p
p
1
log
n
O(
/n log / ) for n independent entries. While
for
p
finite variance, we can obtain a rate "n, 2 O( 1/(n )).
(See Appendix D.)
Next, we focus on max-margin matrix factorization. This
problem was introduced by (Srebro et al., 2004) which used
a hinge loss. We analyze the more general case of Lipschitz
continuous functions, which also includes for instance, the
logistic loss. Note however that the following claim also
applies to nonconvex Lipschitz losses.
Claim iv (Max-margin factorization with Lipschitz loss).
Let f : R ! R be a Lipschitz continuous loss function. Assume the entries of the random matrix X 2
{ 1, +1}n1 ⇥n2 are independent. Let n = n1 n2 and let
P
Lbn (✓) = n1 ij f (xij ✓ij ) be the empirical risk of predicting the binary values xij 2 { 1, +1} by using sgn(✓ij ).

Let L(✓) = E(8ij) xij ⇠Dij [Lbn (✓)]. Assumption A holds
with probability 1 (i.e. = 0), scale function c(✓) = k✓k1
and rate "n,0 2 O(1/n).
By using norm inequalities, Assumption A holds with
probability 1 for other matrix norms besides `1 .
Nonparametric Generalized Regression. Next, we analyze nonparametric regression with exponential-family
noise. The goal is to learn a function, thus the hypothesis
class H is a function space. Each function is represented in
an infinite dimensional orthonormal basis. One instance of
this problem is the Gaussian case, with orthonormal basis
functions that depend on single coordinates, and a `1 -norm
prior as in (Ravikumar et al., 2005). In our nonparametric
model, we allow for the number of basis functions to grow
with more samples. For simplicity, we chose to analyze the
fixed design model. That is, we analyze the case in which
y is a random variable and x is a constant.
Claim v (Nonparametric regression). Let X be the domain of x. Let ✓ : X ! R be a predictor.
Let
R
t(y) be the sufficient statistics and Z(⌫) = y et(y)⌫ be
the partition function.
Given n independent samples, let
P
Lbn (✓) = n1 i t(y (i) )✓(x(i) ) + log Z(✓(x(i) )) be the
empirical negative log-likelihood of y (i) given their predictors ✓(x(i) ). Let L(✓) = E(8i) y(i) ⇠Di [Lbn (✓)]. Let
1 , . . . , 1 : X ! R be an infinitely dimensional orthonormal basis, and let (x) = ( 1 (x), . . . , 1 (x)).
Assumption A holds with probability at least 1
, scale
function c(✓) = k✓k
and
rate
"
,
provided
that
the
dual
n,
P
norm fulfills k n1 i (t(y (i) ) Ey⇠Di [t(y)]) (x(i) )k⇤ 
"n, .
Let 2 (0; 1/2). For sub-Gaussian
t(y), we can obtain a
p
rate "n, 2 O((1/n1/2 ) log 1/ ) for n independent sam2
ples and O(en ) basis functions. While for
p finite variance,
we can obtain a rate "n, 2 O((1/n1/2 ) 1/ ) for O(n2 )
basis functions. Both cases hold for bounded k (x)k⇤ .
(See Appendix D.)
Nonparametric Clustering with Exponential Families.
We consider a version of the clustering problem, where the
number of clusters is not fixed (and possibly infinite), and
where the goal is to estimate the exponential-family parameters of each cluster. Thus, the hypothesis class H is
an infinite dimensional vector space. An analysis for the
Gaussian case, with fixed covariances and number of clusters (i.e. k-means) was given by (Sun et al., 2012). In our
nonparametric model, we allow for the number of clusters
to grow with more samples. For simplicity, we chose to analyze the case of “balanced” clusters. That is, each cluster
contains the same number of training samples.
Claim vi (Nonparametric clustering). Let ✓ (j) be the pa-

A Unified Framework for Consistency of Regularized Loss Minimizers

rameters of cluster j. Let ✓ = (✓ (1) , . . . , ✓ (1) ) be the
concatenation of an infinite set of clusters.
Let t(x) be
R
the sufficient statistics and Z(⌫) = x eht(x),⌫i be the
partition function. Given n i.i.d. samples, let Lbn (✓) =
P
(j)
1
(i)
i + log Z(✓ (j) ) be the empirii minj ht(x ), ✓
n
cal negative log-likelihood of x(i) on its assigned cluster. Let L(✓) = Ex⇠D [Lbn (✓)]. Let X be the domain of x. Assumption A holds with
at least
P1probability
(j)
1
, scale function c(✓) =
k✓
k
and
rate
j=1
(1)
(1)
"n, , provided that for all partitions
X
,
.
.
.
,
X
of
X,
P
the dual norm fulfills (8j) k n1 i 1[x(i) 2 X (j) ]t(x(i) )
Ex⇠D [1[x 2 X (j) ]t(x)]k⇤  "n, .
Forpsub-Gaussian t(x), we can obtain a rate "n, 2
p
O( log n/n log 1/ ) for n independent samples and O( n)
clusters. For finite variance, we were not able to obtain a
decreasing rate "n, with respect to n. (See Appendix D.)
PAC-Bayes Learning. In the PAC-Bayes framework, ✓
is a probability distribution of predictors f in a hypothesis class F. Thus, the hypothesis class H is the space of
probability distributions of support F. After observing a
training set, the task is to choose a posterior distribution
✓bn . PAC-Bayes guarantees are then given with respect to
a prior distribution ✓(0) . Next, we show a connection between PAC-Bayes learning and Kullback-Leibler regularization (Bousquet & Elisseeff, 2002; Germain et al., 2009).
The following theorem applies to tasks such as classification as well as structured prediction.
Claim vii (PAC-Bayes learning). Let X and Y be the domain of x and y respectively. Let f : X ! Y be a predictor and d : Y ⇥ Y ! [0, 1] be a distortion function. Let
✓ be a probability distribution
P of predictors. Given n i.i.d.
samples, let Lbn (✓) = n1 i Ef ⇠✓ [d(y (i) , f (x(i) ))] be the
empirical risk of predicting y (i) by using the Gibbs predictor f (x(i) ). Let L(✓) = E(y,x)⇠D [Lbn (✓)]. Let ✓(0) be a
prior distribution. Assumption A holds with probability at
least 1 p
, scale function c(✓) = KL(✓||✓(0) ) + 1 and rate
"n, 2 O( log n/n log 1/ ).
4.2. Super-Scale Regularizers

In what follows, we show that several regularizers commonly used in the literature fulfill Assumption B. We also
provide yet unexplored priors with guarantees.
Norms. Norms regularizers (i.e. R(✓) = k✓k) fulfill Assumption B for c(✓) = k✓k and r(z) = z. These regularizers include: the sparsity promoting regularizers, such as
the `1 -norm (e.g. Ravikumar et al. 2008) and the k-support
norm (Argyriou et al., 2012), the multitask `1,2 and `1,1 norms for overlapping groups (Jacob et al., 2009; Mairal
et al., 2010) as well as for non-overlapping groups (Negah-

ban & Wainwright, 2011; Obozinski et al., 2011), and the
trace norm for low-rank regularization (Bach, 2008; Srebro
et al., 2004).
Functions of Norms. The Tikhonov regularizer (i.e.
R(✓) = k✓k22 + 1/4) fulfills Assumption B for c(✓) = k✓k2
and r(z) = z 2 + 1/4. We can define some instances
that have not been explored yet, but that have theoretical
guarantees. Consider, for instance a polynomial bound
1/(
/(
1)
1)
r(z) = z
+
for
> 1, a insensitive bound r(z) = max(0, z
) + , a logistic
bound r(z) = log (1 + ez ), an exponential bound r(z) =
ez 1, as well as an entropy bound r(z) = z log z + 1.
Mixture of Norms. The sparse and low-rank prior
(Richard et al., 2012) of the form R(✓) = k✓k1 + k✓ktr ,
fulfills Assumption B by making either c(✓) = k✓k1 or
c(✓) = k✓ktr , and r(z) = z. The elastic net (Zou &
Hastie, 2005) of the form R(✓) = k✓k1 + k✓k22 + 1/4, fulfills Assumption B by making c(✓) = k✓k1 and r(z) = z;
or c(✓) = k✓k2 , r(z) = z 2 + 1/4.
Dirty Models. The dirty multitask prior (Jalali et al.,
2010) of the form R(✓) = k✓ (1) k1 + k✓ (2) k1,1 where
✓ = ✓ (1) +✓ (2) , fulfills Assumption B with c(✓) = k✓k1,1
and r(z) = z. (See Appendix C.)
Other Priors. The Kullback-Leibler regularizer (Bousquet & Elisseeff, 2002; Germain et al., 2009) fulfills Assumption B for c(✓) = KL(✓||✓(0) ) and r(z) = z, where
✓(0) is a prior distribution. Any regularizer of the form
R(✓) = k✓k + f (✓) where f (✓)
0, fulfills Assumption B with c(✓) = k✓k and r(z) = z. This includes
the mixture of norms, and the total variation prior (Kolar
et al., 2010; 2009; Zhang & Wang, 2010). Since f is not
required to be convex, quasiconvex regularizers of the form
R(✓) = k✓k1 + k✓kp for p < 1, fulfill Assumption B.
4.3. Norm-Loss Monotonicity
Here, we show some specific conditions on the expected
loss in order fulfill Assumption C. We also provide yet
unexplored cases with theoretical guarantees.
Strong Convexity. First, we show that strongly convex
expected losses are a special case in our framework. We
consider strongly convex functions that are not necessarily
smooth.
Several authors have shown different flavors of strong convexity for specific problems. Almost strong convexity with
respect to a small neighborhood around the true minimizer
✓ ⇤ was shown in (Kakade et al., 2010) for maximum likelihood estimation of exponential family distributions. Strong
convexity of SVMs and logistic regression for Gaussian

A Unified Framework for Consistency of Regularized Loss Minimizers

p
MLE for exponential family: sub-Gaussian ( log 1/ )
p
Finite variance ( 1/ )
p
GLM with fixed design: sub-Gaussian ( log 1/ )
p
Finite variance ( 1/ )
p
Exponential-family PCA: sub-Gaussian ( log 1/ )
p
Finite variance ( 1/ )
Max-margin factorization with Lipschitz loss ( =0)
p
Nonparametric regression: sub-Gaussian ( log 1/ )
p
Finite variance ( 1/ )
p
Nonparametric clustering: sub-Gaussian ( log 1/ )
p
PAC-Bayes learning ( log 1/ )

log p
n

pp
q n

log p
n

pp

n
p
log n
n
p1
n
1
n
p
log p
n1/2
p
p
n1/2
q
log np
n

predictors was proved in (Rocha et al., 2009). Restricted
strong convexity (i.e. strong convexity with respect to a
subset of directions) was shown in (Negahban et al., 2009;
2012) for generalized linear models under sparsity, groupsparsity and low-rank promoting regularizers. For simplicity, we focus on the regular form of strong convexity.
Claim viii (Strong convexity). Assumption C holds for
b(z) = ⌫2 z 2 provided that the expected loss L is strongly
convex with parameter ⌫. Moreover, if L is twice continuously differentiable, Assumption C holds if the Hessian
of L is positive definite, i.e. if there is ⌫ > 0 such that
2
(8✓ 2 H) @@✓L2 (✓) ⌫ ⌫I.
Note that the function b(z) = ⌫2 z 2 is strictly increasing, and
p
its inverse function is b† (z) = 2z/⌫ . Furthermore, since
b† (0) = 0, Theorem 2 guarantees exact recovery of the true
hypothesis in the asymptotic case with exact minimization.
That is, since we require limn!+1 "n, = 0 in Assumpbn ✓ ⇤ k1 = 0.
tion A and for ⇠ = 0, we have limn!+1 k✓

The constant ⌫ has a problem-specific meaning. In linear
regression, ⌫ is the minimum eigenvalue of the expected
covariance matrix of the predictors (Wainwright, 2009b).
In the estimation of Gaussian MRFs, ⌫ is the squared minimum eigenvalue of the true covariance matrix (Ravikumar
et al., 2008).

k log p
qn

q

kp
n

k log p
n

q

kp
n

NA
NA
NA

q

p log p
n

p
p1/4p log p
n
3/4

pp
n
p log p
n

pp
q n

log n
n

NG

p1
n
p
p
k log p
p log p
1/2
1/2
n
n
p
kp
p3/2
1/2
n1/2
qn
p
k log np p p
log np
n
n

pp
n

p

1/4 p

p log p
n

3/4
pp
n
p
log n
n3/4
1
n1/4
1
n3/4
p
p log p
n1/2
p
qn1/2
p log np
n

q

g log p
n

p gp
q n

g log p
n

p gp
n

p
g p
log p
n
p
g p
p
n
p
g p
log p
n
p
g p
p
n

NA

NA

NA
NA

NA
NA

p
p
g log p
g log p
n1/2
n1/2
p
p
gp
g p
n1/2
qn1/2
p
g log np g p
log np
n
n
q

Kullback-Leibler regularization

log n
n

q

Low-rank

Overlap multitask (`1,1 )
g is maximum group size

Overlap multitask (`1,2 )
g is maximum group size

q

Multitask (`1,2 )

q

Tikhonov
Multitask (`1,1 )
Dirty multitask

q

Sparsity
(k-support norm)

Rates "n, for n samples, with probability at least 1
.
We show dependence with respect to dimension, i.e. ✓ 2
H = Rp . For exponential-family PCA and max-margin matrix factorization: ✓ 2 H = Rn1 ⇥n2 where n = n1 n2 .
Rates were not optimized. All rates follow from the `1 -norm
regularizer and norm inequalities.
NA: not applicable, NG: no guarantees, 2 (0; 1/2).

Sparsity (`1 )
Elastic net
Total variation
Sparsity and low-rank
Quasiconvex (`1 +`p , p<1)

Table 1. Rates for different losses and regularizers (See Appendix D for further details.)

p log p
n
pp
n

NA
NA
q

log n
n

NG
p1
n

NA
q

NA

p log np
n

Minimax Boundedness. Next, we provide an approach
for creating a “minimax” lower bound for an arbitrary expected loss. Note that we consider functions that are not
necessarily smooth or convex. On the other hand, any
strictly convex function is a “minimax bounded” function.
Our constructed lower bound resembles a cone without
apex. First, we create a flat disk of a prescribed radius centered at the true hypothesis ✓ ⇤ . Then, we create a linear
lower bound (i.e. linear in k✓ ✓ ⇤ k2 ) with minimum slope
across the maximum over all possible directions. This linear function is the lower bound of the expected loss outside
the flat disk region.
Claim ix (Minimax boundedness). Let ⌫1 > 0 be a
fixed radius around the true hypothesis ✓ ⇤ . Let M(✓) =
sup 0 { | L(✓) L(✓ ⇤ )
(k✓ ✓ ⇤ k2 ⌫1 )} be
the maximum slope for a linear lower bound of the expected loss L in the direction of ✓
✓ ⇤ . Let ⌫2 =
inf ✓2H,k✓ ✓⇤ k2 >⌫1 M(✓) be the “minimax” slope across
all possible directions. Assumption C holds for b(z) =
⌫2 max(0, z ⌫1 ) provided that ⌫2 > 0.
Note that the function b(z) = ⌫2 max(0, z ⌫1 ) is not
strictly increasing for z 2 (0; ⌫1 ), and its inverse function
is b† (z) = ⌫z2 + ⌫1 . Furthermore, since b† (0) = ⌫1 , Theorem 2 only guarantees recovery of the true hypothesis up
to a small region in the asymptotic case, even with exact
optimization. That is, for limn!+1 "n, = 0 and ⇠ = 0,
b n ✓ ⇤ k1 = ⌫ 1 .
we have limn!+1 k✓

A Unified Framework for Consistency of Regularized Loss Minimizers

Other Types of Nonconvexity. By using our previous results, one can devise some yet unexplored settings
for which our framework provides theoreticalpguarantees.
Functions such as the square root (i.e. b(z) = z) and the
logarithm (i.e. b(z) = log(1 + z)) can be used for nonconvex problems.
We construct a lower bound of the expected loss as follows.
e
First, we define a “transformed” expected loss L(✓)
=
b† (L(✓)
L(✓ ⇤ )). Then, we invoke strong convexity
(Claim viii) or minimax boundedness (Claim ix) for the
e
“transformed” expected loss
p L. Thus, by using the square
root function (i.e. b(z) = z and b† (z) = z 2 ), we define
the family of “squared strongly convex” and “squared minimax bounded” functions. By using the logarithmic function (i.e. b(z) = log(1 + z) and b† (z) = ez 1), we define
the “exponential strongly convex” and “exponential minimax bounded” functions. As expected, in order to obtain
theoretical guarantees, scaled uniform convergence has to
be shown with respect to the “transformed” expected loss
e
L.
4.4. Rates and Novel Results

Table 1 shows the rates for the different losses and regularizers. We have not optimized these rates. All the rates
follow mainly from the `1 -norm regularizer and norm inequalities. Thus, while we match some rates in the literature, some are not better. Note that our framework is more
general and uses less assumptions than previous analyses
for specific problems.
New results include the four types of consistency of MLE
of exponential family distributions, and GLMs for other
priors besides `1 . We prove the four types of consistency
of regularizers that are a norm plus a nonnegative function
(elastic net, total variation, dirty models, sparsity and lowrank), of relatively new regularizers (k-support norm, multitask priors with overlapping groups), and of a proposed
quasiconvex regularizer. The analysis of matrix factorization problems is novel and without the i.i.d assumption.
Our analysis of max-margin matrix factorization does not
assume convexity. We provide a new analysis of nonparametric models, such as generalized regression and clustering. All the problems above have unbounded hypothesis
class and unbounded loss. Finally, we show a connection
between PAC-Bayes learning and Kullback-Leibler regularization.

5. Concluding Remarks
There are several ways of extending this research. While
we focused on the exact recovery of the entire sparsity pattern, approximate sparsistency should also be studied. The
use of a surrogate loss and theoretical guarantees with re-

spect to the original loss is a challenging open problem.
Most consistency results, including ours, do not provide a
data-dependent mechanism for setting n . Extending the
results on cross-validation for `1 -penalized linear regression (Homrighausen & McDonald, 2013) is one of our future goals. We provided examples for the i.i.d. and the
independent sampling settings. We plan to analyze examples for the non-i.i.d. setting as in (London et al., 2013;
Mohri & Rostamizadeh, 2010).

References
Argyriou, A., Foygel, R., and Srebro, N. Sparse prediction
with the k-support norm. NIPS, 2012.
Bach, F. Consistency of trace norm minimization. JMLR,
2008.
Bickel, P., Ritov, Y., and Tsybakov, A. Simultaneous analysis of lasso and dantzig selector. Annals of Statistics,
2009.
Bousquet, O. and Elisseeff, A. Stability and generalization.
JMLR, 2002.
Collins, M., Dasgupta, S., and Schapire, R. A generalization of principal component analysis to the exponential
family. NIPS, 2001.
Germain, P., Lacasse, A., Laviolette, F., Marchand, M., and
Shanian, S. From PAC-Bayes bounds to KL regularization. NIPS, 2009.
Homrighausen, D. and McDonald, D. The lasso, persistence, and cross-validation. ICML, 2013.
Hsu, D., Kakade, S., and Zhang, T. Random design analysis of ridge regression. COLT, 2012.
Jacob, L., Obozinski, G., and Vert, J. Group lasso with
overlap and graph lasso. NIPS, 2009.
Jalali, A., Ravikumar, P., Sanghavi, S., and Ruan, C. A
dirty model for multi-task learning. NIPS, 2010.
Kakade, S., Shamir, O., Sridharan, K., and Tewari,
A. Learning exponential families in high-dimensions:
Strong convexity and sparsity. AISTATS, 2010.
Kolar, M., Song, L., and Xing, E. Sparsistent learning
of varying-coefficient models with structural changes.
NIPS, 2009.
Kolar, M., Song, L., Ahmed, A., and Xing, E. Estimating time-varying networks. Annals of Applied Statistics,
2010.
Lee, J., Sun, Y., and Taylor, J. On model selection consistency of M-estimators with geometrically decomposable
penalties. NIPS, 2013.

A Unified Framework for Consistency of Regularized Loss Minimizers

Loh, P. and Wainwright, M. Regularized M-estimators with
nonconvexity: Statistical and algorithmic theory for local optima. NIPS, 2013.

Rish, I. and Grabarnik, G. Sparse signal recovery with
exponential-family noise. Allerton, 2009.

London, B., Huang, B., Taskar, B., and Getoor, L. Collective stability in structured prediction: Generalization
from one example. ICML, 2013.

Rocha, G., Xing, W., and Yu, B. Asymptotic distribution and sparsistency for `1 -penalized parametric Mestimators with applications to linear SVM and logistic
regression. TR0903, Indiana University, 2009.

Mairal, J., Jenatton, R., Obozinski, G., and Bach, F. Network flow algorithms for structured sparsity. NIPS,
2010.

Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K. Learnability, stability and uniform convergence.
JMLR, 2010.

Maurer, A. A note on the PAC-Bayesian theorem. ArXiv,
2004.

Srebro, N., Rennie, J., and Jaakkola, T. Maximum-margin
matrix factorization. NIPS, 2004.

Meinshausen, N. and Yu, B. Lasso-type recovery of sparse
representations for high dimensional data. Annals of
Statistics, 2009.

Sun, W., Wang, J., and Fang, Y. Regularized k-means clustering of high-dimensional data and its asymptotic consistency. Electronic Journal of Statistics, 2012.

Mohri, M. and Rostamizadeh, A. Stability bounds for stationary '-mixing and -mixing processes. JMLR, 2010.

Tibshirani, R. Regression shrinkage and selection via the
lasso: a retrospective (comments from Bühlmann). J.
Royal Statistical Society, 2011.

Mukherjee, S., Niyogi, P., Poggio, T., and Rifkin, R. Learning theory: stability is sufficient for generalization and
necessary and sufficient for consistency of empirical risk
minimization. Advances in Computational Mathematics,
2006.
Negahban, S. and Wainwright, M. Simultaneous support
recovery in high dimensions: Benefits and perils of block
`1 /`1 -regularization. IEEE Transactions on Information Theory, 2011.
Negahban, S., Ravikumar, P., Wainwright, M., and Yu,
B. A unified framework for high-dimensional analysis
of M-estimators with decomposable regularizers. NIPS,
2009.
Negahban, S., Ravikumar, P., Wainwright, M., and Yu, B.
A unified framework for high-dimensional analysis of
M-estimators with decomposable regularizers. Statistical Science, 2012.
Obozinski, G., Wainwright, M., and Jordan, M. Support
union recovery in high-dimensional multivariate regression. Annals of Statistics, 2011.
Rakhlin, S., Mukherjee, S., and Poggio, T. Stability results
in learning theory. Analysis and Applications, 2005.
Ravikumar, P., Liu, H., Lafferty, J., and Wasserman, L.
Spam: Sparse additive models. NIPS, 2005.
Ravikumar, P., Raskutti, G., Wainwright, M., and Yu, B.
Model selection in Gaussian graphical models: Highdimensional consistency of `1 -regularized MLE. NIPS,
2008.
Richard, E., Savalle, P., and Vayatis, N. Estimation of simultaneously sparse and low rank matrices. ICML, 2012.

van de Geer, S. A survey on empirical risk minimization.
Oberwolfach Reports, 2005.
van de Geer, S. High-dimensional generalized linear models and the lasso. Annals of Statistics, 2008.
Wainwright, M. Information-theoretic limits on sparsity recovery in the high-dimensional and noisy setting. IEEE
Transactions on Information Theory, 2009a.
Wainwright, M. Sharp thresholds for high-dimensional and
noisy sparsity recovery using constrained quadratic programming (lasso). IEEE Transactions on Information
Theory, 2009b.
Wainwright, M., Ravikumar, P., and Lafferty, J.
High dimensional graphical model selection using `1 regularized logistic regression. NIPS, 2006.
Yang, E. and Ravikumar, P. Dirty statistical models. NIPS,
2013.
Yang, E., Tewari, A., and Ravikumar, P. On robust estimation of high dimensional generalized linear models.
IJCAI, 2013.
Zhang, B. and Wang, Y. Learning structural changes of
Gaussian graphical models in controlled experiments.
UAI, 2010.
Zhao, P. and Yu, B. On model selection consistency of
lasso. JMLR, 2006.
Zhou, S. Thresholding procedures for high dimensional
variable selection and statistical estimation. NIPS, 2009.
Zou, H. and Hastie, T. Regularization and variable selection via the elastic net. J. Royal Statistical Society, 2005.

