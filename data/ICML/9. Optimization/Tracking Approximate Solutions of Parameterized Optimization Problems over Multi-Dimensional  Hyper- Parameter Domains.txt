Tracking Approximate Solutions of Parameterized Optimization Problems
over Multi-Dimensional (Hyper-)Parameter Domains

Katharina Blechschmidt
Joachim Giesen
SoÃàren Laue
Friedrich-Schiller-UniversitaÃàt Jena, Germany

Abstract
Many machine learning methods are given as parameterized optimization problems. Important
examples of such parameters are regularizationand kernel hyperparameters. These parameters
have to be tuned carefully since the choice of
their values can have a significant impact on
the statistical performance of the learning methods. In most cases the parameter space does not
carry much structure and parameter tuning essentially boils down to exploring the whole parameter space. The case when there is only one
parameter received quite some attention over the
years. First, algorithms for tracking an optimal
solution for several machine learning optimization problems over regularization- and hyperparameter intervals had been developed, but since
these algorithms can suffer from numerical problems more robust and efficient approximate path
tracking algorithms have been devised and analyzed recently. By now approximate path tracking algorithms are known for regularization- and
kernel hyperparameter paths with optimal path
complexities that depend only on the prescribed
approximation error. Here we extend the work
on approximate path tracking algorithms with approximation guarantees to multi-dimensional parameter domains. We show a lower bound on the
complexity of approximately exploring a multidimensional parameter domain that is the product
of the corresponding path complexities. We also
show a matching upper bound that can be turned
into a theoretically and practically efficient algorithm. Experimental results for kernelized support vector machines and the elastic net confirm
the theoretical complexity analysis.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

KATHI . JENA @ WEB . DE
JOACHIM . GIESEN @ UNI - JENA . DE
SOEREN . LAUE @ UNI - JENA . DE

1. Introduction
We consider parameterized optimization problems of the
form
(1)
min ft (x),
x‚ààFt

where t ‚àà ‚Ñ¶ ‚äÜ Rp is a parameter vector, ‚Ñ¶ is the parameter domain whose dimension is p, ft : Rd ‚Üí R is some
function depending on t, and Ft ‚äÜ Rd is the feasible region
of the optimization problem at parameter value t ‚àà ‚Ñ¶.
The parameter vector t is typically tuned by minimizing
some measure of generalization error on test data while an
optimal solution to Problem (1) at a given parameter vector t is computed from training data. Other criteria like the
sparsity of the solution can also be relevant for the choice
of t. In any case, for optimizing t it is necessary to track an
optimal or approximately optimal solution of Problem (1)
over the whole parameter domain ‚Ñ¶.
The one-dimensional case. The case p = 1, i.e., onedimensional parameter domains, has been extensively studied mostly in the context of regularization paths, i.e., parameterized optimization problems of the form
ft (x) = r(x) + t ¬∑ l(x),
where l : Rd ‚Üí R is a loss function and r : Rd ‚Üí R is
some regularizer, e.g., r(x) = kxk22 that enables the kernel
trick, or r(x) = kxk1 that promotes sparse solutions.
The work on regularization paths started with the work
by (Efron et al., 2004) who observed that the regularization
path of the LASSO is piecewise linear. In (Rosset & Zhu,
2007) a fairly general theory of piecewise linear regularization paths has been developed and exact path following
algorithms have been devised. Important special cases
are support vector machines whose regularization paths
have been studied in (Zhu et al., 2003; Hastie et al., 2004),
support vector regression, where also the loss-sensitivity
parameter can be tracked (Wang et al., 2006b), and the
generalized LASSO (Tibshirani & Taylor, 2011). From
the beginning it was known, see for example (Allgower &

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

Georg, 1993; Hastie et al., 2004; Bach et al., 2004), that
exact regularization path following algorithms suffer from
numerical instabilities as they repeatedly need to invert a
matrix whose condition number can be poor, especially
when using kernels. It also turned out (GaÃàrtner et al., 2012;
Mairal & Yu, 2012) that the combinatorial- and thus also
computational complexity of exact regularization paths can
be exponential in the number of data points. This triggered
the interest in approximate path algorithms (Rosset, 2004;
Friedman et al., 2007). By now numerically robust,
approximate regularization path tracking algorithms
are known for many problems including support vector
machines (Giesen et al., 2012b;c), the LASSO (Mairal
& Yu, 2012), and regularized matrix factorization- and
completion problems (Giesen et al., 2012a;c). These
algorithms ‚àöcompute
a piecewise constant approximation

with O 1/ Œµ segments, where Œµ > 0 is the guaranteed
approximation error. Notably, the complexity is independent of the number of data points and even matching lower
bounds are known (Giesen et al., 2012c).
Another important example that involves a onedimensional parameter domain is when ft is given as
a function f : Rd ‚Üí R that is parameterized by a positive
kernel function kt : X √ó X ‚Üí R that itself is parameterized by t ‚àà R. This leads to the kernel hyperparameter
path tracking problem that has been first studied by (Wang
et al., 2007b) for kernelized support vector machines,
by (Wang et al., 2007a) for the kernelized LASSO, and
by (Wang et al., 2006a; 2012) for Laplacian-regularized
semi-supervised classification. All this work addresses
the exact path tracking problem which is also prone to
numerical problems. A numerically robust and efficient
algorithm for approximate kernel path tracking has been
designed and analyzed by (Giesen et al., 2014). The path
complexity of this algorithm is in O(1/Œµ), where Œµ > 0
is again the guaranteed approximation error. A matching
lower bound shows that this is optimal.
The multi-dimensional case. In contrast to the onedimensional case, most methods for the multi-dimensional
case are heuristics that do not come with guarantees.
Still the most commonly used method for multi-parameter
tuning is a grid or manual search over the parameter
domain. As (Bergstra & Bengio, 2012) have shown, a
simple random search can yield better results than grid
search, when the different parameters are not independent
or not equally important since this can lower the effective
dimension of the parameter domain.
Recently global optimization techniques, especially
Bayesian optimization, have been used successfully for
parameter tuning over large continuous, discrete and mixed
parameter domains for various machine learning problems,
see for example (Hutter et al., 2011; Bergstra et al., 2011;
Snoek et al., 2012) and the references therein.

Contributions. Here we address the multi-dimensional
case for continuous parameter domains. The complexity of
the parameter domain exploration task can be measured in
the number of near optimal solutions that need to be computed for different parameter vectors such that the gamut
of these solutions is sufficient to provide an approximate
solution with prescribed error bound on the whole parameter domain. We show matching upper and lower bounds
on this complexity for multi-parameter domains. We also
turn the upper bound construction into a numerically stable and practically efficient algorithm for low dimensional
problems.

2. Definitions and problem set-up
Our results apply to a fairly general class of parameterized convex optimization problems, namely problems of
the form
min ft (x) s.t. ct (x) ‚â§ 0,
(2)
x‚ààRd

where ft : Rd ‚Üí R is convex and ct : Rd ‚Üí Rn is convex
in every component cit : Rd ‚Üí R, i = 1, . . . , n, for all
parameter vectors t ‚àà ‚Ñ¶ ‚äÜ Rp . We assume that ft (x) and
ct (x) are Lipschitz continuous in t at any feasible point
x, but we do not require convexity (or concavity) of these
functions in t. The feasible region at t is given as
Ft =



	
x ‚àà Rd | ct (x) ‚â§ 0 ,

with componentwise inequalities.
Lagrangian duality. The Lagrangian of the parameterized convex optimization problem (2) is the function
`t : Rd √ó Rn‚â•0 ‚Üí R, (x, Œ±) 7‚Üí ft (x) + Œ±T ct (x),
from which we derive a dual optimization problem as
max min `t (x, Œ±) s.t. Œ± ‚â• 0.

Œ±‚ààRn x‚ààRd

We call
œïÃÇt : Rn ‚Üí R, Œ± 7‚Üí min `t (x, Œ±).
x‚ààRd

the dual objective function. From the Lagrangian we can
also derive an alternative expression for the primal objective function, namely
œït : Rd ‚Üí R, x 7‚Üí max `t (x, Œ±)
Œ±‚â•0

Note that ft (x) = œït (x) for all x ‚àà Ft since Œ±T ct (x) ‚â§
0 and thus maxŒ±‚â•0 Œ±T ct (x) = 0 (which can always be
obtained by setting Œ± = 0) for all x ‚àà Ft .

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

Weak and strong duality. At a fixed parameter vector t
we have the following well known weak duality property
œïÃÇt (Œ±) ‚â§ œït (x)
for any x ‚àà Rd and any Œ± ‚àà Rn‚â•0 . In particular, we have
œïÃÇt (Œ±t‚àó ) ‚â§ œït (x‚àót ), where
Œ±t‚àó = argmaxŒ±‚â•0 œïÃÇt (Œ±) and x‚àót = argminx‚ààFt œït (x)
are the dual and primal optimal solutions, respectively. We
say that strong duality holds if œïÃÇt (Œ±t‚àó ) = œït (x‚àót ) for all t ‚àà
‚Ñ¶. In the following we assume that strong duality holds.
Duality gap and approximate solution.
vector t we call

At parameter

gt (x, Œ±) = œït (x) ‚àí œïÃÇt (Œ±)
the duality gap at (x, Œ±) ‚àà Ft √ó Rn‚â•0 . For Œµ > 0, we
call x ‚àà Ft an Œµ-approximate solution of the parameterized
optimization problem (2) at parameter vector t, if
ft (x) ‚àí ft (x‚àót ) ‚â§ Œµ.
Assume that gt (x, Œ±) ‚â§ Œµ, then we have
ft (x) ‚àí ft (x‚àót ) = œït (x) ‚àí œït (x‚àót )
= œït (x) ‚àí œïÃÇt (Œ±) + œïÃÇt (Œ±) ‚àí œït (x‚àót )

= gt (x, Œ±) ‚àí œït (x‚àót ) ‚àí œïÃÇt (Œ±)
‚â§ gt (x, Œ±) ‚â§ Œµ.
Approximate solution gamut. Let
Q :=

p
Y

[t(i,min) , t(i,max) ] ‚äÇ Rp

i=1

be a compact parameter cuboid and Œµ > 0. We call a function
x : Q ‚Üí Rd , t 7‚Üí x(t)
an Œµ-approximate solution gamut of the parameterized optimization problem (2), if for all t ‚àà Q
1.

x(t) ‚àà Ft

and

2.

ft (x(t)) ‚àí ft (x‚àó (t)) ‚â§ Œµ.

We say that the function x : Q ‚Üí Rd has a combinatorial
complexity k ‚àà N, if x can be computed from k primal-dual
pairs (x(ti ), Œ±(ti )) with ti ‚àà Q, i = 1, . . . , k.
The goal of this paper is to give upper and lower bounds
on the complexity of Œµ-approximate solution gamuts, and
to devise efficient algorithms for computing them.

3. Complexity of solution gamuts
We show matching upper and lower bounds on the combinatorial complexity of approximate solution gamuts by
providing lower and upper bounds, respectively, on the size
of the regions where near optimal primal-dual pairs remain
good approximate solutions. The latter bounds are derived
from the corresponding complexity analysis for the onedimensional case, i.e., the complexity of solution paths. It
turns out that the bounds for the multi-dimensional case are
the product of the corresponding path complexities, i.e., the
complexity along the paths where all but one parameter are
fixed.
Upper bound on the gamut complexity. The known
algorithms for computing approximate solution paths
for one-dimensional parameterized optimization problems (Giesen et al., 2014; Mairal & Yu, 2012; Giesen et al.,
2012c) essentially make use of two problem dependent
families of functions (shift functions):
xt : [tmin , tmax ] ‚Üí Rd , œÑ 7‚Üí xt (œÑ )
Œ±t : [tmin , tmax ] ‚Üí Rn‚â•0 , œÑ 7‚Üí Œ±t (œÑ )
for t ‚àà [tmin , tmax ]. The functions xt are such that a
primal feasible solution x ‚àà Rd at parameter value t is
mapped to a feasible solution xt (œÑ ) at parameter value
œÑ , and xt (t) = x. Analogously, the Œ±t are such that a
dual feasible solution Œ± ‚àà Rn‚â•0 at parameter value t is
mapped to a feasible solution Œ±t (œÑ ) at parameter value œÑ ,
and Œ±t (t) = Œ±. For the approximate path algorithms to be
efficient, the functions xt and Œ±t need to satisfy some continuity conditions and need to be efficiently computable.
The crucial property that allows an efficient computation
of approximate solution paths for one-dimensional parameterized optimization problems is that the duality gap for
the primal-dual pair (xt (œÑ ), Œ±t (œÑ )) at parameter value œÑ ‚àà
[t, t + ‚àÜt] can be bounded by the duality gap at parameter
value œÑ = t as
gœÑ (xt (œÑ ), Œ±t (œÑ )) ‚â§ gt (xt (t), Œ±t (t)) + e(‚àÜt),
where e : [0, tmax ‚àí tmin ] ‚Üí R is some (error) function that depends on the specific optimization problem
and the shift functions, but not on t. For a large class
of regularization path problems it was shown in (Mairal
& Yu, 2012; Giesen et al., 2012c) that there exist shift
functions such that e(‚àÜt) = L2 (‚àÜt)2 , where L is some
problem dependent constant that can be computed explicitly for many problems and the appropriate shift functions.
Thus any given primal-dual pair (xt (œÑ ), Œ±t (œÑ )) that is an
/Œ≥-approximate solution for Œ≥ > 1 at parameter value
œÑ = t, i.e., gt (xt (t), Œ±t (t)) ‚â§ Œµ/Œ≥, is still at least an Œµapproximation
on the whole interval [t, t + ‚àÜt] for ‚àÜt ‚â§
‚àö
Œµ/L. For several kernel-hyperparameter path problems

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

it was shown in (Giesen et al., 2014) that there exist shift
functions such that e(‚àÜt) = L‚àÜt, where L is again some
problem dependent constant. Thus any given primal-dual
pair (xt (œÑ ), Œ±t (œÑ )) that is an /Œ≥-approximate solution at
parameter value œÑ = t is still at least an Œµ-approximation
on the whole interval [t, t + ‚àÜt] for ‚àÜt ‚â§ Œµ/L.
The approach from above can be generalized to p parameters if we already have shift functions for the corresponding
one-dimensional problems, i.e., keeping all but one parameter fixed. Similarly as in the one-dimensional case, we
assume that there exist error functions ei such that at any
parameter vector t = (t1 , . . . , tp ),

gœÑi xt (œÑi ), Œ±t (œÑi )
‚â§ gt (xt (t), Œ±t (t)) + ei (‚àÜti )

(i = 1, . . . , p)

for all œÑi = (t1 , . . . , œÑÃÇi , . . . , tp ) with œÑÃÇi ‚àà [ti , ti + ‚àÜti ].
Here the p-dimensional shift function xt (¬∑) is defined such
that xt (œÑi ) is the i-th one-dimensional shift function applied to œÑi for fixed tj , j 6= i, and similarly for Œ±t (¬∑). Combining these inequalities for the duality gap iteratively gives
gœÑi (xt (œÑi ), Œ±t (œÑi )) ‚â§ gt (xt (t), Œ±t (t)) +

i
X

ei (‚àÜtj )

j=1

Qi
for all œÑi ‚àà
Let
j=1 [tj , tj + ‚àÜtj ], i = 1, . . . , p.
p‚àíi+1
‚àÜti be such that if (xs (s), Œ±s (s)) is an Œµ/Œ≥
approximation at some parameter vector s = (s1 , . . . , sp ),
then (xs (œÑi ), Œ±s (œÑi )) is at least an Œµ/Œ≥ p‚àíi -approximation

for all œÑi in the interval s, (s1 , . . . , si + ‚àÜti , . . . , sp ) . It
follows inductively that any primal-dual pair (xt (t), Œ±t (t))
that is an /Œ≥ p -approximate solution for Œ≥ > 1 at parameter vector t = (t1 , . .Q
. , tp ) is at least an Œµ-approximation
p
on the whole cuboid i=1 [ti , ti + ‚àÜti ]. This results in a
Œµ-approximate
Qpsolution gamut complexity for a parameter
cuboid Q = i=1 [t(i,min) , t(i,max) ] of at most
p
Y
t(i,max) ‚àí t(i,min)
,
‚àÜti
i=1

i.e., the solution gamut complexity can be upper bounded
by the product of the corresponding‚àöpath complexities. For
instance, if all the ‚àÜti are in ‚Ñ¶( Œµ), i.e., as for regularization paths, then the solution gamut complexity is in
O Œµ‚àíp/2 .
Lower bound on the gamut complexity. In the onedimensional case matching lower bounds for path complexities are known. These lower bounds result from upper bounds on‚àö‚àÜt. For regularization paths it was shown
that ‚àÜt ‚àà O( Œµ) and for kernel-hyperparameter paths it
was shown‚àöthat ‚àÜt ‚àà O(Œµ). Hence, the path complexity
is in ‚Ñ¶(1/ Œµ) for regularization paths and in ‚Ñ¶(1/Œµ) for
kernel-hyperparameter paths.

For constructing a matching lower bound example in the
multi-parameter case we consider p problems of the form
min fti (x), with fti (x) ‚â• 0 for all x ‚àà Rd , that are each
parameterized by a single parameter ti , i = 1, . . . , p. Assume that the Œµ-approximate path complexity of the i-th
problem is in ‚Ñ¶(œâi (Œµ)). Then the problem
min

x‚ààRpd

p
X

fti (x[i] ),

(3)

i=1


where x[i] = x(i‚àí1)d+1 , . . . ,xid , has a solution gamut
Qp
‚àó
‚àó
complexity in ‚Ñ¶
i=1 œâi (Œµ) . To see this, let (xt , Œ±t )
be an optimal primal-dual pair at some parameter vector
t. The region where this pair remains an Œµ-approximation
must be contained in a cuboid Q with side lengths 2‚àÜti ‚àà
O(1/œâi (Œµ)) since all the terms fti (x[i] ) need to be optimized independently. The volume of the cuboid Q is
!
p
p
p
Y
‚àí1
Y
Y
p
2
‚àÜti ‚àà
O(1/œâi (Œµ)) = O
œâi (Œµ)
.
i=1

i=1

i=1

Q

p



Thus we need at least ‚Ñ¶
i=1 œâi (Œµ) such cuboids to
cover the whole parameter domain whose volume is independent of Œµ. Hence, the solution gamut complexity for
Problem (3) can be lower bounded by the product of the
corresponding path complexities.

4. Computing solution gamuts adaptively
Here we turn the upper bound construction from the previous section into an algorithm for computing an approximate solution gamut that inherits the theoretical complexity guarantee and is also practically efficient. The algorithm is based on two simple observations. First, the lower
bound on ‚àÜt in the upper bound construction can be too
pessimistic locally, and second, it is computationally much
cheaper to evaluate the duality gap for a given primal-dual
pair than to compute such a pair.
The upper bound construction from the previous section
shows, that a lower bound œÉi (Œµ, Œ≥) on ‚àÜti such that an
Œµ/Œ≥ p‚àíi+1 -approximation at some parameter vector t =
(t1 , . . . , tp ) remains at least an Œµ/Œ≥ p‚àíi -approximation
on

the whole interval t, (t1 , . . . , ti + ‚àÜti , . . . , tp ) guarantees that a grid search, i.e., computing /Œ≥ p -approximate
solutions at the vertices of the grid, on a grid with spacing
œÉi (Œµ, Œ≥) in the i-th parameter direction (that only depends
on the often explicitly known error function ei ) provides an
Œµ-approximate solution gamut.
The idea now is to keep the grid, but trade the computation
of primal-dual pairs for the evaluation of duality gaps at
grid vertices. The adaptive algorithm works iteratively and
stores at every grid vertex the primal-dual pair that has the

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

smallest duality gap so far. Once the duality gap at a grid
vertex is smaller than the prescribed error bound Œµ/Œ≥ p > 0
the grid vertex does not have to be considered anymore.
More formally, the algorithm comprises the following initialization and iteration phases:
Compute an optimal primal-dual pair

(x‚àó , Œ±‚àó ) = xtmin (tmin ), Œ±tmin (tmin )

at the grid vertex tmin = t1,min , . . . , tp,min and
 compute
the duality gap of the pairs xtmin (t), Œ±tmin (t) at all grid
vertices t. Here xtmin (¬∑) and Œ±tmin (¬∑) are shift functions as
defined in the previous section.
Initialization.

Iteration. While there is a grid vertex at which the stored
duality gap is still larger than Œµ/Œ≥ p : compute an optimal
primal-dual pair at the grid vertex tmax at which the stored
duality gap is maximal, and update the duality gap at all the
grid vertices t, where the stored duality gap is larger than

Œµ/Œ≥ p , using the primal-dual pairs xtmax (t), Œ±tmax (t) , if
the resulting duality gap is smaller than the stored gap.

5. Experiments
We consider two examples with a two-dimensional parameter domain each, namely kernelized support vector machines, that are parameterized by a regularization- and a
kernel hyperparameter, and elastic net regularization, a regression method that has two regularization parameters.

Shift functions. A dual solution Œ± that is feasible for
some parameter pair (c, Œ≥) is also feasible for any parameter pair (cÃÇ, Œ≥ÃÇ) as long as cÃÇ ‚â• c. Whenever cÃÇ < c, we
can obtain a dual feasible solution by scaling Œ± appropriately. Hence, an easily computable shift function Œ±(c,Œ≥) (¬∑)
is given as


Œ±
: cÃÇ ‚â• c
Œ±(c,Œ≥) cÃÇ, Œ≥ÃÇ =
Œ± ¬∑ (cÃÇ/c) : cÃÇ < c.
The corresponding one-dimensional shift functions are the
identity function for the parameter Œ≥ and the shift function
Œ± 7‚Üí Œ± ¬∑ max{1, cÃÇ/c} for the parameter c.
For primal solutions (w, b, Œæ) we do not need explicit shift
functions, because feasible primal solutions can be computed from feasible dual solutions. A primal solution w
can be computed as w = y  Œ± from a solution Œ± to the
dual problem. If Œ± is an optimal dual solution, then the
bias b can be computed as b = yi ‚àí KŒ≥ (i, :)w for a support vector index i, i.e., where 0 < Œ±i < c holds true.
Here KŒ≥ (i, :) is the i-th row of KŒ≥ . In the case that Œ± is
not an optimal dual solution, the bias is chosen such that
the primal objective function value becomes minimal. This
can be accomplished by a linear scan over the sorted vector
y(KŒ≥ w). Once w and b are given also Œæ can be computed.
Computing the duality gap. From Œ±(c,Œ≥) (cÃÇ, Œ≥ÃÇ) and the
corresponding feasible primal solutions (w, b, Œæ) at (cÃÇ, Œ≥ÃÇ)
we can directly compute the duality gap of the resulting
primal-dual pair at any grid vertex (cÃÇ, Œ≥ÃÇ).

5.1. Kernelized support vector machines (SVMs)
Primal and dual problem. We consider the standard
hinge loss SVM with a kernel. The primal SVM optimization problem reads as

min

w‚ààRn ,b‚ààR,Œæ‚ààRn

s.t.

1 T
w KŒ≥ w + c ¬∑ kŒæk1
2
y  (KŒ≥ w + b) ‚â• 1 ‚àí Œæ and Œæ ‚â• 0,

where c > 0 is a regularization parameter, y ‚àà Rn is a
label vector with entries in {‚àí1, +1},  is the elementwise multiplication, and KŒ≥ is some kernel matrix that is
parameterized by Œ≥ > 0. In our experiments we use the
Gaussian kernel with bandwidth parameter Œ≥, i.e.,


KŒ≥ = kŒ≥ (x, x0 ) = exp(‚àíŒ≥kx ‚àí x0 k22 ) .
Hence, the two parameters to consider for kernelized SVMs
are the regularization parameter c and the kernel hyperparameter Œ≥.
The dual SVM problem is given as
maxn

Œ±‚ààR

s.t.

1
‚àí (y  Œ±)T KŒ≥ (y  Œ±) + kŒ±k1
2
y T Œ± = 0 and 0 ‚â§ Œ± ‚â§ c.

Experiments. In our implementation of the adaptive algorithm from Section 4 we used the LIBSVM package,
see (Fan et al., 2005), to compute a near optimal dual solution at a given grid vertex, i.e., parameter pair (c, Œ≥).
We considered the two-dimensional parameter space (c, Œ≥)
with c ‚àà [2‚àí10 , 210 ] and Œ≥ ‚àà [2‚àí10 , 210 ], and a uniform
grid with vertices at (2i , 2j ), where i and j were incremented in steps of 0.05, i.e., the grid had 400 √ó 400 =
160, 000 vertices.
The data sets that have been used in our experiments were
obtained from the LIBSVM website, see (Lin) for a description.
Discussion. From the upper bound analysis in Section 3
we know that there exists an Œµ-approximate solution gamut
for the kernelized SVM problem whose complexity is at
most the product‚àöof the regularization path complexity,
which is in O 1/ Œµ , and the kernel hyperparameter path
complexity, which is in O(1/Œµ). That is, there exists a solution gamut with complexity in O Œµ‚àí3/2 . Such a solution
gamut is indeed computed by our adaptive algorithm as can
be seen from Table 1 and Figure 1. Notably, also the lower
bound holds experimentally,
 i.e., the computed gamut has
a complexity in Œò Œµ‚àí3/2 .

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

215

10

2

28

10

gamma

gamma

2
0

2

‚àí2

2

20

0.7

‚àí5

0.6

2

‚àí6

2

‚àí8

2

‚àí6

2

‚àí4

2

‚àí2

2

0

2
c

2

2

4

2

6

2

8

2

0.5

‚àí15
‚àí15

2

10

2

150

2

0

2

‚àí2

100

2

‚àí4

2

‚àí6

‚àí10

2

2‚àí8

200

4

2

2

2

‚àí4

2

0.8

2

2

‚àí10
‚àí10

26

5

gamma

4

2

250

28

2

26

2

10

2
0.9

2

‚àí10

2

‚àí5

2

0

2
c

5

2

10

2

50

2

2‚àí8
‚àí10
‚àí10 ‚àí8

2

15

2

2

2

‚àí6 ‚àí4

2

2

‚àí2

2

0

2
c

2

2

4

2

6

2

8

10

2 2

Figure 2. I ONOSPHERE data set. Left: connected parameter regions that are covered by the same primal-dual pair by the adaptive
algorithm are shown in the same color. Middle: 10-fold cross-validation values over the parameter domain. Right: optimal values for
the primal kernelized SVM objective function over the parameter domain (remark: here the objective function value was scaled by 1/c).
Table 1. Kernelized SVM: Œµ-solution gamut complexity for various data sets.
DATA SET
A1A
A2A
A3A
A4A
DIABETES
HEART
IONOSPHERE

Œµ = 22

21

20

2‚àí1

2‚àí2

2‚àí3

29
21
23
21
1222
602
909

62
45
45
42
2389
1743
1842

155
118
114
93
3710
3273
3021

659
444
434
329
5030
4918
5105

2027
1463
1770
1332
6136
6868
7420

3643
2752
3333
2706
7420
9239
9958

racy does not change much only very few (or even a single)
primal-dual pairs are sufficient to cover the region, while
in regions where the cross-validation accuracy changes a
lot many primal-dual pairs are necessary. That is, the most
primal-dual pairs are computed in statistically interesting
regions. These regions cannot be determined by looking
just at the optimal primal objective function values over the
parameter domain that are shown in Figure 2(right). That
is, the adaptive algorithm indeed adapts to the statistically
interesting regions but not to regions with similar optimal
objective function values.

214
a3a
a4a
diabetes
heart
ionosphere
1/Œµ3/2

number of primal-dual pairs

212
210
28
26
24
22
20

2‚àí2

20

22

24

26

28

Œµ

Figure 1. Œµ-solution gamut complexity for various data sets (loglog plot).

The adaptivity and practical efficiency of our algorithm
can be seen in Figure 2. In Figure 2(left) grid regions
are shown for the I ONOSPHERE data set that are covered
by one primal-dual pair. Note that many primal-dual pairs
are sufficiently good solutions for wide ranges of parameter
values which renders our adaptive algorithm much more efficient than a simple grid search. In Figure 2(middle) a 10fold cross-validation plot is shown for the same data set. It
can be seen that in regions where the cross-validation accu-

5.2. Elastic Net regularization
Primal and dual problem. Elastic net regularization
combines `2 - and `1 -regularization for linear regression. It
is given as the following unconstrained optimization problem, see (Zou & Hastie, 2005),


1
1‚àíŒª
min
kAx ‚àí yk22 + c
kxk22 + Œªkxk1 ,
2
x‚ààRd 2n
where A ‚àà Rn√ód is the data matrix for n data points in Rd
and y ‚àà Rn are the corresponding responses. The problem
is parameterized by c ‚â• 0 and 0 ‚â§ Œª ‚â§ 1. Special cases
of the elastic net are ridge regression (for Œª = 0) and the
Lasso (for Œª = 1), see (Tibshirani, 1996).
A standard calculation shows that the dual of the elastic net
is the following constrained optimization problem
1 T
1
(u + 2AT y)T Q(u + 2AT y) +
y y
8n
2n
s. t. 0 ‚â§ u ‚â§ 2ncŒª,

max

u‚ààRd

‚àí

where Q ‚àà Rd√ód is the pseudoinverse of AT A+nc(1‚àíŒª)I
and I ‚àà Rd√ód is the identity matrix.
Shift functions. We do not need a shift function for the
primal elastic net since it is an unconstrained problem, i.e.,

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains
Table 2. Elastic net: Œµ-solution gamut complexity for various data sets.
DATA SET
ABALONE
BODYFAT
CPUSMALL
PYRIM
SYNTHETIC
SYNTHETIC
SYNTHETIC
SYNTHETIC

Œµ = 2‚àí1

2‚àí2

2‚àí3

2‚àí4

2‚àí5

2‚àí6

2‚àí7

2‚àí8

2‚àí9

2‚àí10

9
2
6
2
3
2
2
3

14
3
16
2
3
3
3
6

17
5
26
3
6
5
4
8

32
7
35
5
10
7
6
13

75
12
56
7
17
10
10
24

136
22
85
14
37
22
17
48

251
38
145
29
62
52
35
105

438
69
215
52
152
106
71
249

750
137
376
99
365
220
132
517

1274
280
670
202
771
408
262
929

(n = 50, d = 40)
(n = 500, d = 100)
(n = 5000, d = 100)
(n = 5000, d = 1000)

any x ‚àà Rd is feasible for all admissible parameter pairs
(c, Œª). Hence, we only need shift functions for the dual
problem. Note first, that an optimal solution u for the dual
problem can be computed from an optimal solution x for
the primal problem as
u = 2(AT A + nc(1 ‚àí Œª)I)x ‚àí 2AT y,

duces to evaluating the expression
(Œ¥u + 2AT y)T Q(Œ¥u + 2AT y)
= (Œ¥u + 2AT y)T . . .
. . . U (S + ncÃÇ(1 ‚àí ŒªÃÇ)I)‚àí1 U T (Œ¥u + 2AT y)
= (Œ¥U T u + 2U T AT y)T . . .
. . . (S + ncÃÇ(1 ‚àí ŒªÃÇ)I)‚àí1 (Œ¥U T u + 2U T AT y)

which follows from duality theory and some straightforward calculations. An optimal dual solution u at some parameter pair (c, Œª) is a feasible solution for the dual problem at some other parameter pair (cÃÇ, ŒªÃÇ) whenever kuk‚àû ‚â§
2ncÃÇŒªÃÇ. Otherwise, we can scale u such that it becomes feasible. Thus, an easily computable shift function is given
as
(


u(c,Œª) cÃÇ, ŒªÃÇ =

u
u¬∑

cÃÇŒªÃÇ
cŒª

: kuk‚àû ‚â§ 2ncÃÇŒªÃÇ
: kuk‚àû > 2ncÃÇŒªÃÇ.

The corresponding one-dimensional shift function for the
parameter c is u 7‚Üí u¬∑cÃÇ/c if kuk‚àû > 2ncÃÇŒª, and the identity
function otherwise. Analogously, the shift function for the
parameter Œª is u 7‚Üí u ¬∑ ŒªÃÇ/Œª if kuk‚àû > 2ncŒªÃÇ, and the
identity function otherwise.
Computing the duality gap. Given a primal solution x,
the value of the primal objective function can be computed
in constant time at any parameter pair (cÃÇ, ŒªÃÇ) from the value
at (c, Œª) since the computation boils down to evaluating a
linear function in the product cŒª. For computing the value
of the dual objective function note that the matrix Q can be
computed efficiently for varying values of c and Œª from the
singular value decomposition of AT A. Let U SU T be the
singular value decomposition of AT A. We then have

= (Œ¥U T u + 2U T AT y)T . . .


. . . (Œ¥U T u + 2U T AT y)  (s + ncÃÇ(1 ‚àí ŒªÃÇ)1) ,
where  is the elementwise vector division, s = diag(S),
and 1 is the all-ones vector. The last equality follows since
S is a diagonal matrix. The values U T u and 2U T AT y can
be precomputed for any optimal solution u. Hence, the
dual objective function value for varying parameter pairs
(c, Œª) can be computed in time O(d). Note that this is much
faster than computing a primal-dual pair which amounts to
a running time in Œò(d7/2 ).
Experiments. In our implementation of the adaptive algorithm from Section 4 we used GLMNET, see (Friedman
et al., 2010), for solving the primal optimization problem
at given parameter values for c and Œª. Note that GLMNET
allows to compute the exact regularization path for c. We
considered parameter values Œª ‚àà [0, 1] and c ‚àà [2‚àí10 , 25 ].
For the experiments we used standard data sets from the
LIBSVM website and also generated synthetic data similarly as in (Friedman et al., 2010), i.e., the synthetic outcome values were generated as
Y =

k
X

Xi Œ≤i + Œ± ¬∑ Z

i=1

Q = U (S + nc(1 ‚àí Œª)I)‚àí1 U T

where the Xi are Gaussian variables with d observations,
the coefficients Œ≤i are linearly decreasing, Z ‚àº N (0, 1),
and Œ± is chosen such that the signal-to-noise ratio is 3.

in case that c(1 ‚àí Œª) > 0, and otherwise Q is simply the
cÃÇŒªÃÇ
pseudoinverse of AT A. Let Œ¥ = cŒª
, computing the dual
objective function value at some dual solution u now re-

Discussion. From the upper bound analysis in Section 3
we know that there exists an Œµ-approximate solution gamut
for the elastic net problem whose complexity is the product

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

sparsity
1

1

0.8

1.8

0.8

1

1

1.9

1

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.2

0.2

1.6
1.5

0.4

0.8
lambda

0.4

0.6

lambda

0.6

lambda

lambda

1.7

0.6
0.4

1.4
0.2

1.3

0.2

0.4
0.2
0.2

1.2
0
2‚àí10

‚àí5

0

2

0
2‚àí10

5

2

2

c

‚àí5

0

2

5

2

2

0
2‚àí10

‚àí5

0

2

2
c

c

5

2

0

0
2‚àí10

‚àí5

0

2

2

5

2

c

Figure 3. S YNTHETIC data set. Left: connected parameter regions that are covered by the same primal-dual pair by the adaptive algorithm are shown in the same color. Middle/left: 10-fold cross-validation RMSE values over the parameter domain. Middle/right: sparsity
of the computed solution over the parameter domain. Right: optimal values for the primal elastic net function over the parameter domain.

only at the optimal primal objective function values over
the parameter domain that are shown in Figure 3(right).
This holds also true for the sparsity of the solution that
is shown in Figure 3(middle/right). Note that the sparsity
of a solution is not necessarily a monotone function in c.
A comparison of Figures 3(middle/left) and (middle/right)
also shows that only exploring the whole parameter domain
allows to make an informed trade-off between the two objectives of low RMSE and sparsity.

212
abalone
bodyfat
cpusmall
pyrim
synthetic
1/Œµ

10

number of primal-dual pairs

2

28
26
24

6. Conclusions

2

2

20‚àí10
2

2‚àí8

2‚àí6

2‚àí4
Œµ

2‚àí2

20

22

Figure 2. Œµ-solution gamut complexity for various data sets (loglog plot).

‚àö 
of two regularization path complexities each in O 1/ Œµ .
Hence, there exists a solution gamut with complexity in
O(1/Œµ). Again, such a solution gamut is computed by our
adaptive algorithm as can be seen from Table 2 and Figure 2. Experimentally, the computed gamut also
 obeys the
theoretical lower complexity bound in ‚Ñ¶ 1/Œµ .
In Figure 3(left) grid regions are shown for the S YNTHETIC
(n = 50, d = 40) data set that are covered by one primaldual pair. As for the kernelized SVM many primal-dual
pairs are again sufficiently good solutions for a wide range
of parameter values not only for c but also for Œª. This information is lost if one considers only the one-dimensional
regularization path in c as it has been done previously.
In Figure 3(middle/left) a 10-fold cross-validation RMSE
plot is shown for the same data set. Also here it can be
seen that in regions where the cross-validation accuracy
does not change much only very few primal-dual pairs
are sufficient to cover the region, while in regions where
the cross-validation accuracy changes rapidly many primaldual pairs are necessary. Also for the elastic net these statistically interesting regions cannot be determined by looking

We addressed the problem of exploring multi-dimensional
parameter domains of parameterized optimization problems that are frequently encountered in machine learning.
We showed matching upper- and lower bounds on the complexity of this task in terms of a prescribed approximation
error that are the product of the associated path complexities, i.e., the parameter tracking problems where all but
one parameter are fixed. The path complexities for a fairly
large class of problems
had previously been shown to be
‚àö 
in at least ‚Ñ¶ 1/ Œµ for a prescribed approximation error
Œµ > 0. Under the assumption of this lower bound on the
path complexities our lower bound construction shows that
the complexity of the parameter space exploration problem grows exponentially with the number of parameters.
Hence, parameter domain exploration with guarantees will
only be practically feasible for low dimensional problems,
if the domain does not possess an additional (dependence)
structure. Identifying such structures could be an interesting direction of future research.
We have also turned the upper bound construction into an
efficient and numerically robust algorithm for exploring
low-dimensional parameter domains that adapts to the true
problem complexity. Remarkably, the seemingly loose theoretical lower complexity bound is attained in both example problems that we have analyzed with an implementation of this algorithm.
Acknowledgments. This work has been supported by the
DFG grant (GI-711/3-2).

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

References
Allgower, Eugene and Georg, Kurt. Continuation and path
following. Acta Numerica, 2:1‚Äì64, 1993.

Giesen, Joachim, Laue, SoÃàren, and Wieschollek, Patrick.
Robust and efficient kernel hyperparameter paths with
guarantees. In International Conference on Machine
Learning (ICML), pp. 1296‚Äì1304, 2014.

Bach, Francis R., Thibaux, Romain, and Jordan, Michael I.
Computing regularization paths for learning multiple
kernels. In Advances in Neural Information Processing
Systems (NIPS), 2004.

Hastie, Trevor, Rosset, Saharon, Tibshirani, Robert, and
Zhu, Ji. The Entire Regularization Path for the Support Vector Machine. In Advances in Neural Information
Processing Systems (NIPS), 2004.

Bergstra, James and Bengio, Yoshua. Random Search
for Hyper-Parameter Optimization. Journal of Machine
Learning Research, 13:281‚Äì305, 2012.

Hutter, Frank, Hoos, Holger H., and Leyton-Brown, Kevin.
Sequential Model-Based Optimization for General Algorithm Configuration. In Learning and Intelligent Optimization (LION), pp. 507‚Äì523, 2011.

Bergstra, James, Bardenet, ReÃÅmi, Bengio, Yoshua, and
KeÃÅgl, BalaÃÅzs. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems (NIPS), pp. 2546‚Äì2554, 2011.
Efron, Bradley, Hastie, Trevor, Johnstone, Iain, and Tibshirani, Robert. Least angle regression. The Annals of
Statistics, 32(2):407‚Äì499, 2004.
Fan, Rong-En, Chen, Pai-Hsuen, and Lin, Chih-Jen. Working Set Selection Using Second Order Information for
Training Support Vector Machines. Journal of Machine
Learning Research, 6:1889‚Äì1918, 2005.
Friedman, Jerome, Hastie, Trevor, HoÃàfling, Holger, and
Tibshirani, Robert. Pathwise Coordinate Optimization.
The Annals of Applied Statistics, 1(2):302‚Äì332, 2007.
Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
Regularized Paths for Generalized Linear Models via
Coordinate Descent. Journal of Statistical Software, 33
(1), 2010.
GaÃàrtner, Bernd, Jaggi, Martin, and Maria, CleÃÅment. An
Exponential Lower Bound on the Complexity of Regularization Paths. Journal of Computational Geometry
(JoCG), 3(1):168‚Äì195, 2012.

Lin, Chih-Jen.
LIBSVM Tools.
Data sets available
at
www.csie.ntu.edu.tw/Àúcjlin/
libsvmtools/datasets/.
Mairal, Julien and Yu, Bin. Complexity analysis of the
lasso regularization path. In International Conference
on Machine Learning (ICML), 2012.
Rosset, Saharon. Following curved regularized optimization solution paths. In Advances in Neural Information
Processing Systems (NIPS), 2004.
Rosset, Saharon and Zhu, Ji. Piecewise linear regularized
solution paths. The Annals of Statistics, 35(3):1012‚Äì
1030, 2007.
Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P.
Practical Bayesian Optimization of Machine Learning
Algorithms. In Advances in Neural Information Processing Systems (NIPS), pp. 2960‚Äì2968, 2012.
Tibshirani, Robert. Regression Shrinkage and Selection
Via the Lasso. Journal of the Royal Statistical Society,
Series B, 58:267‚Äì288, 1996.
Tibshirani, Ryan and Taylor, Jonathan. The solution path
of the generalized lasso. The Annals of Statistics, 39(3):
1335‚Äì1371, 2011.

Giesen, Joachim, Jaggi, Martin, and Laue, SoÃàren. Regularization Paths with Guarantees for Convex Semidefinite Optimization. In International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 432‚Äì439,
2012a.

Wang, Gang, Chen, Tao, Yeung, Dit-Yan, and Lochovsky,
Frederick H. Solution path for semi-supervised classification with manifold regularization. In IEEE International Conference on Data Mining (ICDM), pp. 1124‚Äì
1129, 2006a.

Giesen, Joachim, Jaggi, Martin, and Laue, SoÃàren. Approximating parameterized convex optimization problems.
ACM Transactions on Algorithms, 9(1):10, 2012b.

Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H.
Two-dimensional solution path for support vector regression. In International Conference on Machine Learning
(ICML), pp. 993‚Äì1000, 2006b.

Giesen, Joachim, MuÃàller, Jens K., Laue, SoÃàren, and
Swiercy, Sascha. Approximating Concavely Parameterized Optimization Problems. In Advances in Neural Information Processing Systems (NIPS), pp. 2114‚Äì2122,
2012c.

Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H.
The Kernel Path in Kernelized LASSO. In International
Conference on Artificial Intelligence and Statistics (AISTATS), pp. 580‚Äì587, 2007a.

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H.
A kernel path algorithm for support vector machines. In
International Conference on Machine Learning (ICML),
pp. 951‚Äì958, 2007b.
Wang, Gang, Wang, Fei, Chen, Tao, Yeung, Dit-Yan, and
Lochovsky, Frederick H. Solution Path for Manifold
Regularized Semisupervised Classification. IEEE Transactions on Systems, Man, and Cybernetics, Part B, 42(2):
308‚Äì319, 2012.
Zhu, Ji, Rosset, Saharon, Hastie, Trevor, and Tibshirani,
Robert. 1-norm Support Vector Machines. In Advances
in Neural Information Processing Systems (NIPS), 2003.
Zou, Hui and Hastie, Trevor. Regularization and Variable
Selection via the Elastic Net. Journal of the Royal Statistical Society, Series B, pp. 301‚Äì320, 2005.

