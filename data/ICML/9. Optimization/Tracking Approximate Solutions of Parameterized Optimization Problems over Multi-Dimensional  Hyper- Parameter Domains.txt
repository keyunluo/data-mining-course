Tracking Approximate Solutions of Parameterized Optimization Problems
over Multi-Dimensional (Hyper-)Parameter Domains

Katharina Blechschmidt
Joachim Giesen
Sören Laue
Friedrich-Schiller-Universität Jena, Germany

Abstract
Many machine learning methods are given as parameterized optimization problems. Important
examples of such parameters are regularizationand kernel hyperparameters. These parameters
have to be tuned carefully since the choice of
their values can have a significant impact on
the statistical performance of the learning methods. In most cases the parameter space does not
carry much structure and parameter tuning essentially boils down to exploring the whole parameter space. The case when there is only one
parameter received quite some attention over the
years. First, algorithms for tracking an optimal
solution for several machine learning optimization problems over regularization- and hyperparameter intervals had been developed, but since
these algorithms can suffer from numerical problems more robust and efficient approximate path
tracking algorithms have been devised and analyzed recently. By now approximate path tracking algorithms are known for regularization- and
kernel hyperparameter paths with optimal path
complexities that depend only on the prescribed
approximation error. Here we extend the work
on approximate path tracking algorithms with approximation guarantees to multi-dimensional parameter domains. We show a lower bound on the
complexity of approximately exploring a multidimensional parameter domain that is the product
of the corresponding path complexities. We also
show a matching upper bound that can be turned
into a theoretically and practically efficient algorithm. Experimental results for kernelized support vector machines and the elastic net confirm
the theoretical complexity analysis.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

KATHI . JENA @ WEB . DE
JOACHIM . GIESEN @ UNI - JENA . DE
SOEREN . LAUE @ UNI - JENA . DE

1. Introduction
We consider parameterized optimization problems of the
form
(1)
min ft (x),
x∈Ft

where t ∈ Ω ⊆ Rp is a parameter vector, Ω is the parameter domain whose dimension is p, ft : Rd → R is some
function depending on t, and Ft ⊆ Rd is the feasible region
of the optimization problem at parameter value t ∈ Ω.
The parameter vector t is typically tuned by minimizing
some measure of generalization error on test data while an
optimal solution to Problem (1) at a given parameter vector t is computed from training data. Other criteria like the
sparsity of the solution can also be relevant for the choice
of t. In any case, for optimizing t it is necessary to track an
optimal or approximately optimal solution of Problem (1)
over the whole parameter domain Ω.
The one-dimensional case. The case p = 1, i.e., onedimensional parameter domains, has been extensively studied mostly in the context of regularization paths, i.e., parameterized optimization problems of the form
ft (x) = r(x) + t · l(x),
where l : Rd → R is a loss function and r : Rd → R is
some regularizer, e.g., r(x) = kxk22 that enables the kernel
trick, or r(x) = kxk1 that promotes sparse solutions.
The work on regularization paths started with the work
by (Efron et al., 2004) who observed that the regularization
path of the LASSO is piecewise linear. In (Rosset & Zhu,
2007) a fairly general theory of piecewise linear regularization paths has been developed and exact path following
algorithms have been devised. Important special cases
are support vector machines whose regularization paths
have been studied in (Zhu et al., 2003; Hastie et al., 2004),
support vector regression, where also the loss-sensitivity
parameter can be tracked (Wang et al., 2006b), and the
generalized LASSO (Tibshirani & Taylor, 2011). From
the beginning it was known, see for example (Allgower &

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

Georg, 1993; Hastie et al., 2004; Bach et al., 2004), that
exact regularization path following algorithms suffer from
numerical instabilities as they repeatedly need to invert a
matrix whose condition number can be poor, especially
when using kernels. It also turned out (Gärtner et al., 2012;
Mairal & Yu, 2012) that the combinatorial- and thus also
computational complexity of exact regularization paths can
be exponential in the number of data points. This triggered
the interest in approximate path algorithms (Rosset, 2004;
Friedman et al., 2007). By now numerically robust,
approximate regularization path tracking algorithms
are known for many problems including support vector
machines (Giesen et al., 2012b;c), the LASSO (Mairal
& Yu, 2012), and regularized matrix factorization- and
completion problems (Giesen et al., 2012a;c). These
algorithms √compute
a piecewise constant approximation

with O 1/ ε segments, where ε > 0 is the guaranteed
approximation error. Notably, the complexity is independent of the number of data points and even matching lower
bounds are known (Giesen et al., 2012c).
Another important example that involves a onedimensional parameter domain is when ft is given as
a function f : Rd → R that is parameterized by a positive
kernel function kt : X × X → R that itself is parameterized by t ∈ R. This leads to the kernel hyperparameter
path tracking problem that has been first studied by (Wang
et al., 2007b) for kernelized support vector machines,
by (Wang et al., 2007a) for the kernelized LASSO, and
by (Wang et al., 2006a; 2012) for Laplacian-regularized
semi-supervised classification. All this work addresses
the exact path tracking problem which is also prone to
numerical problems. A numerically robust and efficient
algorithm for approximate kernel path tracking has been
designed and analyzed by (Giesen et al., 2014). The path
complexity of this algorithm is in O(1/ε), where ε > 0
is again the guaranteed approximation error. A matching
lower bound shows that this is optimal.
The multi-dimensional case. In contrast to the onedimensional case, most methods for the multi-dimensional
case are heuristics that do not come with guarantees.
Still the most commonly used method for multi-parameter
tuning is a grid or manual search over the parameter
domain. As (Bergstra & Bengio, 2012) have shown, a
simple random search can yield better results than grid
search, when the different parameters are not independent
or not equally important since this can lower the effective
dimension of the parameter domain.
Recently global optimization techniques, especially
Bayesian optimization, have been used successfully for
parameter tuning over large continuous, discrete and mixed
parameter domains for various machine learning problems,
see for example (Hutter et al., 2011; Bergstra et al., 2011;
Snoek et al., 2012) and the references therein.

Contributions. Here we address the multi-dimensional
case for continuous parameter domains. The complexity of
the parameter domain exploration task can be measured in
the number of near optimal solutions that need to be computed for different parameter vectors such that the gamut
of these solutions is sufficient to provide an approximate
solution with prescribed error bound on the whole parameter domain. We show matching upper and lower bounds
on this complexity for multi-parameter domains. We also
turn the upper bound construction into a numerically stable and practically efficient algorithm for low dimensional
problems.

2. Definitions and problem set-up
Our results apply to a fairly general class of parameterized convex optimization problems, namely problems of
the form
min ft (x) s.t. ct (x) ≤ 0,
(2)
x∈Rd

where ft : Rd → R is convex and ct : Rd → Rn is convex
in every component cit : Rd → R, i = 1, . . . , n, for all
parameter vectors t ∈ Ω ⊆ Rp . We assume that ft (x) and
ct (x) are Lipschitz continuous in t at any feasible point
x, but we do not require convexity (or concavity) of these
functions in t. The feasible region at t is given as
Ft =



	
x ∈ Rd | ct (x) ≤ 0 ,

with componentwise inequalities.
Lagrangian duality. The Lagrangian of the parameterized convex optimization problem (2) is the function
`t : Rd × Rn≥0 → R, (x, α) 7→ ft (x) + αT ct (x),
from which we derive a dual optimization problem as
max min `t (x, α) s.t. α ≥ 0.

α∈Rn x∈Rd

We call
ϕ̂t : Rn → R, α 7→ min `t (x, α).
x∈Rd

the dual objective function. From the Lagrangian we can
also derive an alternative expression for the primal objective function, namely
ϕt : Rd → R, x 7→ max `t (x, α)
α≥0

Note that ft (x) = ϕt (x) for all x ∈ Ft since αT ct (x) ≤
0 and thus maxα≥0 αT ct (x) = 0 (which can always be
obtained by setting α = 0) for all x ∈ Ft .

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

Weak and strong duality. At a fixed parameter vector t
we have the following well known weak duality property
ϕ̂t (α) ≤ ϕt (x)
for any x ∈ Rd and any α ∈ Rn≥0 . In particular, we have
ϕ̂t (αt∗ ) ≤ ϕt (x∗t ), where
αt∗ = argmaxα≥0 ϕ̂t (α) and x∗t = argminx∈Ft ϕt (x)
are the dual and primal optimal solutions, respectively. We
say that strong duality holds if ϕ̂t (αt∗ ) = ϕt (x∗t ) for all t ∈
Ω. In the following we assume that strong duality holds.
Duality gap and approximate solution.
vector t we call

At parameter

gt (x, α) = ϕt (x) − ϕ̂t (α)
the duality gap at (x, α) ∈ Ft × Rn≥0 . For ε > 0, we
call x ∈ Ft an ε-approximate solution of the parameterized
optimization problem (2) at parameter vector t, if
ft (x) − ft (x∗t ) ≤ ε.
Assume that gt (x, α) ≤ ε, then we have
ft (x) − ft (x∗t ) = ϕt (x) − ϕt (x∗t )
= ϕt (x) − ϕ̂t (α) + ϕ̂t (α) − ϕt (x∗t )

= gt (x, α) − ϕt (x∗t ) − ϕ̂t (α)
≤ gt (x, α) ≤ ε.
Approximate solution gamut. Let
Q :=

p
Y

[t(i,min) , t(i,max) ] ⊂ Rp

i=1

be a compact parameter cuboid and ε > 0. We call a function
x : Q → Rd , t 7→ x(t)
an ε-approximate solution gamut of the parameterized optimization problem (2), if for all t ∈ Q
1.

x(t) ∈ Ft

and

2.

ft (x(t)) − ft (x∗ (t)) ≤ ε.

We say that the function x : Q → Rd has a combinatorial
complexity k ∈ N, if x can be computed from k primal-dual
pairs (x(ti ), α(ti )) with ti ∈ Q, i = 1, . . . , k.
The goal of this paper is to give upper and lower bounds
on the complexity of ε-approximate solution gamuts, and
to devise efficient algorithms for computing them.

3. Complexity of solution gamuts
We show matching upper and lower bounds on the combinatorial complexity of approximate solution gamuts by
providing lower and upper bounds, respectively, on the size
of the regions where near optimal primal-dual pairs remain
good approximate solutions. The latter bounds are derived
from the corresponding complexity analysis for the onedimensional case, i.e., the complexity of solution paths. It
turns out that the bounds for the multi-dimensional case are
the product of the corresponding path complexities, i.e., the
complexity along the paths where all but one parameter are
fixed.
Upper bound on the gamut complexity. The known
algorithms for computing approximate solution paths
for one-dimensional parameterized optimization problems (Giesen et al., 2014; Mairal & Yu, 2012; Giesen et al.,
2012c) essentially make use of two problem dependent
families of functions (shift functions):
xt : [tmin , tmax ] → Rd , τ 7→ xt (τ )
αt : [tmin , tmax ] → Rn≥0 , τ 7→ αt (τ )
for t ∈ [tmin , tmax ]. The functions xt are such that a
primal feasible solution x ∈ Rd at parameter value t is
mapped to a feasible solution xt (τ ) at parameter value
τ , and xt (t) = x. Analogously, the αt are such that a
dual feasible solution α ∈ Rn≥0 at parameter value t is
mapped to a feasible solution αt (τ ) at parameter value τ ,
and αt (t) = α. For the approximate path algorithms to be
efficient, the functions xt and αt need to satisfy some continuity conditions and need to be efficiently computable.
The crucial property that allows an efficient computation
of approximate solution paths for one-dimensional parameterized optimization problems is that the duality gap for
the primal-dual pair (xt (τ ), αt (τ )) at parameter value τ ∈
[t, t + ∆t] can be bounded by the duality gap at parameter
value τ = t as
gτ (xt (τ ), αt (τ )) ≤ gt (xt (t), αt (t)) + e(∆t),
where e : [0, tmax − tmin ] → R is some (error) function that depends on the specific optimization problem
and the shift functions, but not on t. For a large class
of regularization path problems it was shown in (Mairal
& Yu, 2012; Giesen et al., 2012c) that there exist shift
functions such that e(∆t) = L2 (∆t)2 , where L is some
problem dependent constant that can be computed explicitly for many problems and the appropriate shift functions.
Thus any given primal-dual pair (xt (τ ), αt (τ )) that is an
/γ-approximate solution for γ > 1 at parameter value
τ = t, i.e., gt (xt (t), αt (t)) ≤ ε/γ, is still at least an εapproximation
on the whole interval [t, t + ∆t] for ∆t ≤
√
ε/L. For several kernel-hyperparameter path problems

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

it was shown in (Giesen et al., 2014) that there exist shift
functions such that e(∆t) = L∆t, where L is again some
problem dependent constant. Thus any given primal-dual
pair (xt (τ ), αt (τ )) that is an /γ-approximate solution at
parameter value τ = t is still at least an ε-approximation
on the whole interval [t, t + ∆t] for ∆t ≤ ε/L.
The approach from above can be generalized to p parameters if we already have shift functions for the corresponding
one-dimensional problems, i.e., keeping all but one parameter fixed. Similarly as in the one-dimensional case, we
assume that there exist error functions ei such that at any
parameter vector t = (t1 , . . . , tp ),

gτi xt (τi ), αt (τi )
≤ gt (xt (t), αt (t)) + ei (∆ti )

(i = 1, . . . , p)

for all τi = (t1 , . . . , τ̂i , . . . , tp ) with τ̂i ∈ [ti , ti + ∆ti ].
Here the p-dimensional shift function xt (·) is defined such
that xt (τi ) is the i-th one-dimensional shift function applied to τi for fixed tj , j 6= i, and similarly for αt (·). Combining these inequalities for the duality gap iteratively gives
gτi (xt (τi ), αt (τi )) ≤ gt (xt (t), αt (t)) +

i
X

ei (∆tj )

j=1

Qi
for all τi ∈
Let
j=1 [tj , tj + ∆tj ], i = 1, . . . , p.
p−i+1
∆ti be such that if (xs (s), αs (s)) is an ε/γ
approximation at some parameter vector s = (s1 , . . . , sp ),
then (xs (τi ), αs (τi )) is at least an ε/γ p−i -approximation

for all τi in the interval s, (s1 , . . . , si + ∆ti , . . . , sp ) . It
follows inductively that any primal-dual pair (xt (t), αt (t))
that is an /γ p -approximate solution for γ > 1 at parameter vector t = (t1 , . .Q
. , tp ) is at least an ε-approximation
p
on the whole cuboid i=1 [ti , ti + ∆ti ]. This results in a
ε-approximate
Qpsolution gamut complexity for a parameter
cuboid Q = i=1 [t(i,min) , t(i,max) ] of at most
p
Y
t(i,max) − t(i,min)
,
∆ti
i=1

i.e., the solution gamut complexity can be upper bounded
by the product of the corresponding√path complexities. For
instance, if all the ∆ti are in Ω( ε), i.e., as for regularization paths, then the solution gamut complexity is in
O ε−p/2 .
Lower bound on the gamut complexity. In the onedimensional case matching lower bounds for path complexities are known. These lower bounds result from upper bounds on√∆t. For regularization paths it was shown
that ∆t ∈ O( ε) and for kernel-hyperparameter paths it
was shown√that ∆t ∈ O(ε). Hence, the path complexity
is in Ω(1/ ε) for regularization paths and in Ω(1/ε) for
kernel-hyperparameter paths.

For constructing a matching lower bound example in the
multi-parameter case we consider p problems of the form
min fti (x), with fti (x) ≥ 0 for all x ∈ Rd , that are each
parameterized by a single parameter ti , i = 1, . . . , p. Assume that the ε-approximate path complexity of the i-th
problem is in Ω(ωi (ε)). Then the problem
min

x∈Rpd

p
X

fti (x[i] ),

(3)

i=1


where x[i] = x(i−1)d+1 , . . . ,xid , has a solution gamut
Qp
∗
∗
complexity in Ω
i=1 ωi (ε) . To see this, let (xt , αt )
be an optimal primal-dual pair at some parameter vector
t. The region where this pair remains an ε-approximation
must be contained in a cuboid Q with side lengths 2∆ti ∈
O(1/ωi (ε)) since all the terms fti (x[i] ) need to be optimized independently. The volume of the cuboid Q is
!
p
p
p
Y
−1
Y
Y
p
2
∆ti ∈
O(1/ωi (ε)) = O
ωi (ε)
.
i=1

i=1

i=1

Q

p



Thus we need at least Ω
i=1 ωi (ε) such cuboids to
cover the whole parameter domain whose volume is independent of ε. Hence, the solution gamut complexity for
Problem (3) can be lower bounded by the product of the
corresponding path complexities.

4. Computing solution gamuts adaptively
Here we turn the upper bound construction from the previous section into an algorithm for computing an approximate solution gamut that inherits the theoretical complexity guarantee and is also practically efficient. The algorithm is based on two simple observations. First, the lower
bound on ∆t in the upper bound construction can be too
pessimistic locally, and second, it is computationally much
cheaper to evaluate the duality gap for a given primal-dual
pair than to compute such a pair.
The upper bound construction from the previous section
shows, that a lower bound σi (ε, γ) on ∆ti such that an
ε/γ p−i+1 -approximation at some parameter vector t =
(t1 , . . . , tp ) remains at least an ε/γ p−i -approximation
on

the whole interval t, (t1 , . . . , ti + ∆ti , . . . , tp ) guarantees that a grid search, i.e., computing /γ p -approximate
solutions at the vertices of the grid, on a grid with spacing
σi (ε, γ) in the i-th parameter direction (that only depends
on the often explicitly known error function ei ) provides an
ε-approximate solution gamut.
The idea now is to keep the grid, but trade the computation
of primal-dual pairs for the evaluation of duality gaps at
grid vertices. The adaptive algorithm works iteratively and
stores at every grid vertex the primal-dual pair that has the

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

smallest duality gap so far. Once the duality gap at a grid
vertex is smaller than the prescribed error bound ε/γ p > 0
the grid vertex does not have to be considered anymore.
More formally, the algorithm comprises the following initialization and iteration phases:
Compute an optimal primal-dual pair

(x∗ , α∗ ) = xtmin (tmin ), αtmin (tmin )

at the grid vertex tmin = t1,min , . . . , tp,min and
 compute
the duality gap of the pairs xtmin (t), αtmin (t) at all grid
vertices t. Here xtmin (·) and αtmin (·) are shift functions as
defined in the previous section.
Initialization.

Iteration. While there is a grid vertex at which the stored
duality gap is still larger than ε/γ p : compute an optimal
primal-dual pair at the grid vertex tmax at which the stored
duality gap is maximal, and update the duality gap at all the
grid vertices t, where the stored duality gap is larger than

ε/γ p , using the primal-dual pairs xtmax (t), αtmax (t) , if
the resulting duality gap is smaller than the stored gap.

5. Experiments
We consider two examples with a two-dimensional parameter domain each, namely kernelized support vector machines, that are parameterized by a regularization- and a
kernel hyperparameter, and elastic net regularization, a regression method that has two regularization parameters.

Shift functions. A dual solution α that is feasible for
some parameter pair (c, γ) is also feasible for any parameter pair (ĉ, γ̂) as long as ĉ ≥ c. Whenever ĉ < c, we
can obtain a dual feasible solution by scaling α appropriately. Hence, an easily computable shift function α(c,γ) (·)
is given as


α
: ĉ ≥ c
α(c,γ) ĉ, γ̂ =
α · (ĉ/c) : ĉ < c.
The corresponding one-dimensional shift functions are the
identity function for the parameter γ and the shift function
α 7→ α · max{1, ĉ/c} for the parameter c.
For primal solutions (w, b, ξ) we do not need explicit shift
functions, because feasible primal solutions can be computed from feasible dual solutions. A primal solution w
can be computed as w = y  α from a solution α to the
dual problem. If α is an optimal dual solution, then the
bias b can be computed as b = yi − Kγ (i, :)w for a support vector index i, i.e., where 0 < αi < c holds true.
Here Kγ (i, :) is the i-th row of Kγ . In the case that α is
not an optimal dual solution, the bias is chosen such that
the primal objective function value becomes minimal. This
can be accomplished by a linear scan over the sorted vector
y(Kγ w). Once w and b are given also ξ can be computed.
Computing the duality gap. From α(c,γ) (ĉ, γ̂) and the
corresponding feasible primal solutions (w, b, ξ) at (ĉ, γ̂)
we can directly compute the duality gap of the resulting
primal-dual pair at any grid vertex (ĉ, γ̂).

5.1. Kernelized support vector machines (SVMs)
Primal and dual problem. We consider the standard
hinge loss SVM with a kernel. The primal SVM optimization problem reads as

min

w∈Rn ,b∈R,ξ∈Rn

s.t.

1 T
w Kγ w + c · kξk1
2
y  (Kγ w + b) ≥ 1 − ξ and ξ ≥ 0,

where c > 0 is a regularization parameter, y ∈ Rn is a
label vector with entries in {−1, +1},  is the elementwise multiplication, and Kγ is some kernel matrix that is
parameterized by γ > 0. In our experiments we use the
Gaussian kernel with bandwidth parameter γ, i.e.,


Kγ = kγ (x, x0 ) = exp(−γkx − x0 k22 ) .
Hence, the two parameters to consider for kernelized SVMs
are the regularization parameter c and the kernel hyperparameter γ.
The dual SVM problem is given as
maxn

α∈R

s.t.

1
− (y  α)T Kγ (y  α) + kαk1
2
y T α = 0 and 0 ≤ α ≤ c.

Experiments. In our implementation of the adaptive algorithm from Section 4 we used the LIBSVM package,
see (Fan et al., 2005), to compute a near optimal dual solution at a given grid vertex, i.e., parameter pair (c, γ).
We considered the two-dimensional parameter space (c, γ)
with c ∈ [2−10 , 210 ] and γ ∈ [2−10 , 210 ], and a uniform
grid with vertices at (2i , 2j ), where i and j were incremented in steps of 0.05, i.e., the grid had 400 × 400 =
160, 000 vertices.
The data sets that have been used in our experiments were
obtained from the LIBSVM website, see (Lin) for a description.
Discussion. From the upper bound analysis in Section 3
we know that there exists an ε-approximate solution gamut
for the kernelized SVM problem whose complexity is at
most the product√of the regularization path complexity,
which is in O 1/ ε , and the kernel hyperparameter path
complexity, which is in O(1/ε). That is, there exists a solution gamut with complexity in O ε−3/2 . Such a solution
gamut is indeed computed by our adaptive algorithm as can
be seen from Table 1 and Figure 1. Notably, also the lower
bound holds experimentally,
 i.e., the computed gamut has
a complexity in Θ ε−3/2 .

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

215

10

2

28

10

gamma

gamma

2
0

2

−2

2

20

0.7

−5

0.6

2

−6

2

−8

2

−6

2

−4

2

−2

2

0

2
c

2

2

4

2

6

2

8

2

0.5

−15
−15

2

10

2

150

2

0

2

−2

100

2

−4

2

−6

−10

2

2−8

200

4

2

2

2

−4

2

0.8

2

2

−10
−10

26

5

gamma

4

2

250

28

2

26

2

10

2
0.9

2

−10

2

−5

2

0

2
c

5

2

10

2

50

2

2−8
−10
−10 −8

2

15

2

2

2

−6 −4

2

2

−2

2

0

2
c

2

2

4

2

6

2

8

10

2 2

Figure 2. I ONOSPHERE data set. Left: connected parameter regions that are covered by the same primal-dual pair by the adaptive
algorithm are shown in the same color. Middle: 10-fold cross-validation values over the parameter domain. Right: optimal values for
the primal kernelized SVM objective function over the parameter domain (remark: here the objective function value was scaled by 1/c).
Table 1. Kernelized SVM: ε-solution gamut complexity for various data sets.
DATA SET
A1A
A2A
A3A
A4A
DIABETES
HEART
IONOSPHERE

ε = 22

21

20

2−1

2−2

2−3

29
21
23
21
1222
602
909

62
45
45
42
2389
1743
1842

155
118
114
93
3710
3273
3021

659
444
434
329
5030
4918
5105

2027
1463
1770
1332
6136
6868
7420

3643
2752
3333
2706
7420
9239
9958

racy does not change much only very few (or even a single)
primal-dual pairs are sufficient to cover the region, while
in regions where the cross-validation accuracy changes a
lot many primal-dual pairs are necessary. That is, the most
primal-dual pairs are computed in statistically interesting
regions. These regions cannot be determined by looking
just at the optimal primal objective function values over the
parameter domain that are shown in Figure 2(right). That
is, the adaptive algorithm indeed adapts to the statistically
interesting regions but not to regions with similar optimal
objective function values.

214
a3a
a4a
diabetes
heart
ionosphere
1/ε3/2

number of primal-dual pairs

212
210
28
26
24
22
20

2−2

20

22

24

26

28

ε

Figure 1. ε-solution gamut complexity for various data sets (loglog plot).

The adaptivity and practical efficiency of our algorithm
can be seen in Figure 2. In Figure 2(left) grid regions
are shown for the I ONOSPHERE data set that are covered
by one primal-dual pair. Note that many primal-dual pairs
are sufficiently good solutions for wide ranges of parameter
values which renders our adaptive algorithm much more efficient than a simple grid search. In Figure 2(middle) a 10fold cross-validation plot is shown for the same data set. It
can be seen that in regions where the cross-validation accu-

5.2. Elastic Net regularization
Primal and dual problem. Elastic net regularization
combines `2 - and `1 -regularization for linear regression. It
is given as the following unconstrained optimization problem, see (Zou & Hastie, 2005),


1
1−λ
min
kAx − yk22 + c
kxk22 + λkxk1 ,
2
x∈Rd 2n
where A ∈ Rn×d is the data matrix for n data points in Rd
and y ∈ Rn are the corresponding responses. The problem
is parameterized by c ≥ 0 and 0 ≤ λ ≤ 1. Special cases
of the elastic net are ridge regression (for λ = 0) and the
Lasso (for λ = 1), see (Tibshirani, 1996).
A standard calculation shows that the dual of the elastic net
is the following constrained optimization problem
1 T
1
(u + 2AT y)T Q(u + 2AT y) +
y y
8n
2n
s. t. 0 ≤ u ≤ 2ncλ,

max

u∈Rd

−

where Q ∈ Rd×d is the pseudoinverse of AT A+nc(1−λ)I
and I ∈ Rd×d is the identity matrix.
Shift functions. We do not need a shift function for the
primal elastic net since it is an unconstrained problem, i.e.,

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains
Table 2. Elastic net: ε-solution gamut complexity for various data sets.
DATA SET
ABALONE
BODYFAT
CPUSMALL
PYRIM
SYNTHETIC
SYNTHETIC
SYNTHETIC
SYNTHETIC

ε = 2−1

2−2

2−3

2−4

2−5

2−6

2−7

2−8

2−9

2−10

9
2
6
2
3
2
2
3

14
3
16
2
3
3
3
6

17
5
26
3
6
5
4
8

32
7
35
5
10
7
6
13

75
12
56
7
17
10
10
24

136
22
85
14
37
22
17
48

251
38
145
29
62
52
35
105

438
69
215
52
152
106
71
249

750
137
376
99
365
220
132
517

1274
280
670
202
771
408
262
929

(n = 50, d = 40)
(n = 500, d = 100)
(n = 5000, d = 100)
(n = 5000, d = 1000)

any x ∈ Rd is feasible for all admissible parameter pairs
(c, λ). Hence, we only need shift functions for the dual
problem. Note first, that an optimal solution u for the dual
problem can be computed from an optimal solution x for
the primal problem as
u = 2(AT A + nc(1 − λ)I)x − 2AT y,

duces to evaluating the expression
(δu + 2AT y)T Q(δu + 2AT y)
= (δu + 2AT y)T . . .
. . . U (S + nĉ(1 − λ̂)I)−1 U T (δu + 2AT y)
= (δU T u + 2U T AT y)T . . .
. . . (S + nĉ(1 − λ̂)I)−1 (δU T u + 2U T AT y)

which follows from duality theory and some straightforward calculations. An optimal dual solution u at some parameter pair (c, λ) is a feasible solution for the dual problem at some other parameter pair (ĉ, λ̂) whenever kuk∞ ≤
2nĉλ̂. Otherwise, we can scale u such that it becomes feasible. Thus, an easily computable shift function is given
as
(


u(c,λ) ĉ, λ̂ =

u
u·

ĉλ̂
cλ

: kuk∞ ≤ 2nĉλ̂
: kuk∞ > 2nĉλ̂.

The corresponding one-dimensional shift function for the
parameter c is u 7→ u·ĉ/c if kuk∞ > 2nĉλ, and the identity
function otherwise. Analogously, the shift function for the
parameter λ is u 7→ u · λ̂/λ if kuk∞ > 2ncλ̂, and the
identity function otherwise.
Computing the duality gap. Given a primal solution x,
the value of the primal objective function can be computed
in constant time at any parameter pair (ĉ, λ̂) from the value
at (c, λ) since the computation boils down to evaluating a
linear function in the product cλ. For computing the value
of the dual objective function note that the matrix Q can be
computed efficiently for varying values of c and λ from the
singular value decomposition of AT A. Let U SU T be the
singular value decomposition of AT A. We then have

= (δU T u + 2U T AT y)T . . .


. . . (δU T u + 2U T AT y)  (s + nĉ(1 − λ̂)1) ,
where  is the elementwise vector division, s = diag(S),
and 1 is the all-ones vector. The last equality follows since
S is a diagonal matrix. The values U T u and 2U T AT y can
be precomputed for any optimal solution u. Hence, the
dual objective function value for varying parameter pairs
(c, λ) can be computed in time O(d). Note that this is much
faster than computing a primal-dual pair which amounts to
a running time in Θ(d7/2 ).
Experiments. In our implementation of the adaptive algorithm from Section 4 we used GLMNET, see (Friedman
et al., 2010), for solving the primal optimization problem
at given parameter values for c and λ. Note that GLMNET
allows to compute the exact regularization path for c. We
considered parameter values λ ∈ [0, 1] and c ∈ [2−10 , 25 ].
For the experiments we used standard data sets from the
LIBSVM website and also generated synthetic data similarly as in (Friedman et al., 2010), i.e., the synthetic outcome values were generated as
Y =

k
X

Xi βi + α · Z

i=1

Q = U (S + nc(1 − λ)I)−1 U T

where the Xi are Gaussian variables with d observations,
the coefficients βi are linearly decreasing, Z ∼ N (0, 1),
and α is chosen such that the signal-to-noise ratio is 3.

in case that c(1 − λ) > 0, and otherwise Q is simply the
ĉλ̂
pseudoinverse of AT A. Let δ = cλ
, computing the dual
objective function value at some dual solution u now re-

Discussion. From the upper bound analysis in Section 3
we know that there exists an ε-approximate solution gamut
for the elastic net problem whose complexity is the product

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

sparsity
1

1

0.8

1.8

0.8

1

1

1.9

1

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.2

0.2

1.6
1.5

0.4

0.8
lambda

0.4

0.6

lambda

0.6

lambda

lambda

1.7

0.6
0.4

1.4
0.2

1.3

0.2

0.4
0.2
0.2

1.2
0
2−10

−5

0

2

0
2−10

5

2

2

c

−5

0

2

5

2

2

0
2−10

−5

0

2

2
c

c

5

2

0

0
2−10

−5

0

2

2

5

2

c

Figure 3. S YNTHETIC data set. Left: connected parameter regions that are covered by the same primal-dual pair by the adaptive algorithm are shown in the same color. Middle/left: 10-fold cross-validation RMSE values over the parameter domain. Middle/right: sparsity
of the computed solution over the parameter domain. Right: optimal values for the primal elastic net function over the parameter domain.

only at the optimal primal objective function values over
the parameter domain that are shown in Figure 3(right).
This holds also true for the sparsity of the solution that
is shown in Figure 3(middle/right). Note that the sparsity
of a solution is not necessarily a monotone function in c.
A comparison of Figures 3(middle/left) and (middle/right)
also shows that only exploring the whole parameter domain
allows to make an informed trade-off between the two objectives of low RMSE and sparsity.

212
abalone
bodyfat
cpusmall
pyrim
synthetic
1/ε

10

number of primal-dual pairs

2

28
26
24

6. Conclusions

2

2

20−10
2

2−8

2−6

2−4
ε

2−2

20

22

Figure 2. ε-solution gamut complexity for various data sets (loglog plot).

√ 
of two regularization path complexities each in O 1/ ε .
Hence, there exists a solution gamut with complexity in
O(1/ε). Again, such a solution gamut is computed by our
adaptive algorithm as can be seen from Table 2 and Figure 2. Experimentally, the computed gamut also
 obeys the
theoretical lower complexity bound in Ω 1/ε .
In Figure 3(left) grid regions are shown for the S YNTHETIC
(n = 50, d = 40) data set that are covered by one primaldual pair. As for the kernelized SVM many primal-dual
pairs are again sufficiently good solutions for a wide range
of parameter values not only for c but also for λ. This information is lost if one considers only the one-dimensional
regularization path in c as it has been done previously.
In Figure 3(middle/left) a 10-fold cross-validation RMSE
plot is shown for the same data set. Also here it can be
seen that in regions where the cross-validation accuracy
does not change much only very few primal-dual pairs
are sufficient to cover the region, while in regions where
the cross-validation accuracy changes rapidly many primaldual pairs are necessary. Also for the elastic net these statistically interesting regions cannot be determined by looking

We addressed the problem of exploring multi-dimensional
parameter domains of parameterized optimization problems that are frequently encountered in machine learning.
We showed matching upper- and lower bounds on the complexity of this task in terms of a prescribed approximation
error that are the product of the associated path complexities, i.e., the parameter tracking problems where all but
one parameter are fixed. The path complexities for a fairly
large class of problems
had previously been shown to be
√ 
in at least Ω 1/ ε for a prescribed approximation error
ε > 0. Under the assumption of this lower bound on the
path complexities our lower bound construction shows that
the complexity of the parameter space exploration problem grows exponentially with the number of parameters.
Hence, parameter domain exploration with guarantees will
only be practically feasible for low dimensional problems,
if the domain does not possess an additional (dependence)
structure. Identifying such structures could be an interesting direction of future research.
We have also turned the upper bound construction into an
efficient and numerically robust algorithm for exploring
low-dimensional parameter domains that adapts to the true
problem complexity. Remarkably, the seemingly loose theoretical lower complexity bound is attained in both example problems that we have analyzed with an implementation of this algorithm.
Acknowledgments. This work has been supported by the
DFG grant (GI-711/3-2).

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

References
Allgower, Eugene and Georg, Kurt. Continuation and path
following. Acta Numerica, 2:1–64, 1993.

Giesen, Joachim, Laue, Sören, and Wieschollek, Patrick.
Robust and efficient kernel hyperparameter paths with
guarantees. In International Conference on Machine
Learning (ICML), pp. 1296–1304, 2014.

Bach, Francis R., Thibaux, Romain, and Jordan, Michael I.
Computing regularization paths for learning multiple
kernels. In Advances in Neural Information Processing
Systems (NIPS), 2004.

Hastie, Trevor, Rosset, Saharon, Tibshirani, Robert, and
Zhu, Ji. The Entire Regularization Path for the Support Vector Machine. In Advances in Neural Information
Processing Systems (NIPS), 2004.

Bergstra, James and Bengio, Yoshua. Random Search
for Hyper-Parameter Optimization. Journal of Machine
Learning Research, 13:281–305, 2012.

Hutter, Frank, Hoos, Holger H., and Leyton-Brown, Kevin.
Sequential Model-Based Optimization for General Algorithm Configuration. In Learning and Intelligent Optimization (LION), pp. 507–523, 2011.

Bergstra, James, Bardenet, Rémi, Bengio, Yoshua, and
Kégl, Balázs. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems (NIPS), pp. 2546–2554, 2011.
Efron, Bradley, Hastie, Trevor, Johnstone, Iain, and Tibshirani, Robert. Least angle regression. The Annals of
Statistics, 32(2):407–499, 2004.
Fan, Rong-En, Chen, Pai-Hsuen, and Lin, Chih-Jen. Working Set Selection Using Second Order Information for
Training Support Vector Machines. Journal of Machine
Learning Research, 6:1889–1918, 2005.
Friedman, Jerome, Hastie, Trevor, Höfling, Holger, and
Tibshirani, Robert. Pathwise Coordinate Optimization.
The Annals of Applied Statistics, 1(2):302–332, 2007.
Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
Regularized Paths for Generalized Linear Models via
Coordinate Descent. Journal of Statistical Software, 33
(1), 2010.
Gärtner, Bernd, Jaggi, Martin, and Maria, Clément. An
Exponential Lower Bound on the Complexity of Regularization Paths. Journal of Computational Geometry
(JoCG), 3(1):168–195, 2012.

Lin, Chih-Jen.
LIBSVM Tools.
Data sets available
at
www.csie.ntu.edu.tw/˜cjlin/
libsvmtools/datasets/.
Mairal, Julien and Yu, Bin. Complexity analysis of the
lasso regularization path. In International Conference
on Machine Learning (ICML), 2012.
Rosset, Saharon. Following curved regularized optimization solution paths. In Advances in Neural Information
Processing Systems (NIPS), 2004.
Rosset, Saharon and Zhu, Ji. Piecewise linear regularized
solution paths. The Annals of Statistics, 35(3):1012–
1030, 2007.
Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P.
Practical Bayesian Optimization of Machine Learning
Algorithms. In Advances in Neural Information Processing Systems (NIPS), pp. 2960–2968, 2012.
Tibshirani, Robert. Regression Shrinkage and Selection
Via the Lasso. Journal of the Royal Statistical Society,
Series B, 58:267–288, 1996.
Tibshirani, Ryan and Taylor, Jonathan. The solution path
of the generalized lasso. The Annals of Statistics, 39(3):
1335–1371, 2011.

Giesen, Joachim, Jaggi, Martin, and Laue, Sören. Regularization Paths with Guarantees for Convex Semidefinite Optimization. In International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 432–439,
2012a.

Wang, Gang, Chen, Tao, Yeung, Dit-Yan, and Lochovsky,
Frederick H. Solution path for semi-supervised classification with manifold regularization. In IEEE International Conference on Data Mining (ICDM), pp. 1124–
1129, 2006a.

Giesen, Joachim, Jaggi, Martin, and Laue, Sören. Approximating parameterized convex optimization problems.
ACM Transactions on Algorithms, 9(1):10, 2012b.

Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H.
Two-dimensional solution path for support vector regression. In International Conference on Machine Learning
(ICML), pp. 993–1000, 2006b.

Giesen, Joachim, Müller, Jens K., Laue, Sören, and
Swiercy, Sascha. Approximating Concavely Parameterized Optimization Problems. In Advances in Neural Information Processing Systems (NIPS), pp. 2114–2122,
2012c.

Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H.
The Kernel Path in Kernelized LASSO. In International
Conference on Artificial Intelligence and Statistics (AISTATS), pp. 580–587, 2007a.

Tracking Approximate Solutions over Multi-Dimensional (Hyper-)Parameter Domains

Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H.
A kernel path algorithm for support vector machines. In
International Conference on Machine Learning (ICML),
pp. 951–958, 2007b.
Wang, Gang, Wang, Fei, Chen, Tao, Yeung, Dit-Yan, and
Lochovsky, Frederick H. Solution Path for Manifold
Regularized Semisupervised Classification. IEEE Transactions on Systems, Man, and Cybernetics, Part B, 42(2):
308–319, 2012.
Zhu, Ji, Rosset, Saharon, Hastie, Trevor, and Tibshirani,
Robert. 1-norm Support Vector Machines. In Advances
in Neural Information Processing Systems (NIPS), 2003.
Zou, Hui and Hastie, Trevor. Regularization and Variable
Selection via the Elastic Net. Journal of the Royal Statistical Society, Series B, pp. 301–320, 2005.

