Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing

Rongda Zhu
RZHU 4@ ILLINOIS . EDU
Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA
Quanquan Gu
QG 5 W @ VIRGINIA . EDU
Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, USA

Abstract
In this paper, we propose a novel algorithm based
on nonconvex sparsity-inducing penalty for onebit compressed sensing. We prove that our algorithm has a sample complexity of O(s/2 ) for
strong signals, and O(s log d/2 ) for weak signals, where s is the number of nonzero entries in
the signal vector, d is the signal dimension and
 is the recovery error. For general signals, the
sample complexity of our algorithm lies between
O(s/2 ) and O(s log d/2 ). This is a remarkable improvement over the existing best sample complexity O(s log d/2 ). Furthermore, we
show that our algorithm achieves exact support
recovery with high probability for strong signals.
Our theory is verified by extensive numerical experiments, which clearly illustrate the superiority
of our algorithm for both approximate signal and
support recovery in the noisy setting.

1. Introduction
Compressed sensing (Donoho, 2006; Candes & Tao, 2006)
is a technique to design measurement matrices and recovery algorithms to estimate a sparse signal vector using a
few linear measurements. Recently, one-bit compressed
sensing (Boufounos & Baraniuk, 2008), which is a variant of conventional compressed sensing, has received increasing attention for its low computational cost and robustness to noise and non-linearity (Boufounos, 2010). In
contrast to conventional compressed sensing, which uses
real-valued measurement, one-bit compressed sensing uses
one-bit measurement to recover the unknown signals. For
example, suppose xâˆ— is the unknown signal vector, and
{ui }ni=1 is a set of measurement vectors. The sign of realProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

valued measurement is observed as follows:
yi = sign(hui , xâˆ— i), i = 1, 2, . . . , n,
where yi is the binary one-bit measurement. Since signs
of real-valued measurements are used, scaling xâˆ— will not
make changes on the measurements.
 In other
	n words, we
cannot recover the norm of xâˆ— from (yi , ui ) i=1 . For this
reason, when studying approximate vector recovery in onebit compressed sensing, we always assume that the original
signal xâˆ— is a unit vector, i.e., kxâˆ— k2 = 1.
In general, there are two major tasks in one-bit compressed
sensing: (1) support recovery (Gupta et al., 2010; Haupt &
Baraniuk, 2011; Gopi et al., 2013), which recovers the support of the unknown signal vector xâˆ— ; and (2) approximate
signal vector recovery (Gopi et al., 2013; Jacques et al.,
2013; Zhang et al., 2014), which aims at finding a unit vecb such that kb
tor x
x âˆ’ xâˆ— k2 is small.
In this paper, we aim at presenting an algorithm, which
is able to achieve both approximate signal recovery and
support recovery with strong theoretical guarantees. At
the core of our method is the nonconvex sparsity-inducing
penalty. While nonconvex sparsity-inducing penalties have
achieved great success in the statistics community (Fan &
Li, 2001; Zhang, 2010; Wang et al., 2014; Gu et al., 2014),
it is unclear whether nonconvex sparsity-inducing penalties are advantageous for one-bit compressed sensing. In
our study, we show that the answer is in the affirmative.
More specifically, the main contributions of this work are
summarized as follows:
â€¢ We propose to incorporate sparsity-inducing penalty
functions into one-bit compressed sensing, and derive an
algorithm to efficiently solve the resulting problem. To
the best of our knowledge, this is the first work on onebit compressed sensing that utilizes nonconvex penalty
functions.
â€¢ We prove that our proposed method improves sample complexity from previous best results O(s log d/2 )
to O(s/2 ) for strong signals. And for general signals, our algorithm attains a sample complexity between

Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing

O(s log d/2 ) and O(s/2 ).
â€¢ We prove that our proposed method can exactly recover
the support of the signal under mild magnitude assumptions on the signal.
â€¢ We verify the effectiveness of our method by thorough
numerical experiments.
The remainder of this paper is organized as follows. We
will review related work in Section 2, and then describe
our method in Section 3. We present the main theoretical
results in Section 4 and experiment results in Section 5. We
conclude the paper in Section 6.

levels. Zeng & Figueiredo (2014) studied one-bit compressed sensing on piece-wise smoothing signals.
On the other hand, most of the existing studies only target
one of the tasks in approximate vector recovery and support recovery. In this paper, we propose a method that can
improve the previous best results for approximate vector recovery and achieve exact support recovery simultaneously.

3. The Proposed Method
In this section we will describe our method. Theoretical
analysis of our method can be found in the next section.

2. Related Work

3.1. Background

One-bit compressed sensing was first introduced
in (Boufounos & Baraniuk, 2008), with only the noiseless
one-bit measurement considered. In particular, Boufounos
& Baraniuk (2008) proposed an estimator by minimizing
the `1 norm of a unit vector consistent with the measurements. Since the minimization is on the unit square, the
problem is non-convex. To address this problem, Plan
& Vershynin (2013b) proposed a convex formulation
by putting the constraint on the `1 norm of real-valued
measurement vector instead of the `2 norm of the signal.
The sample complexity of this work is O(s log2 d/5 ).
Another convex estimator was proposed in (Plan & Vershynin, 2013a), which maximizes the dot product of the
one-bit measurements of the real signal and the real-valued
measurements of the recovered signal. This framework can
recover both exactly and approximately sparse signals with
noise, with sample complexity O(s log d/4 ). Currently,
the best sample complexity result for vector recovery
is achieved by (Zhang et al., 2014), where the authors
proposed an efficient algorithm with close-from solution
based on adding `1 regularization. In noisy and noiseless
cases, the sample complexity of their work is O(s log d/2 )
for exactly sparse signals.

We will first briefly review the general framework of onebit compressed sensing. As described in (Plan & Vershynin, 2013a), we assume yi can be viewed as drawn independently with the expectation

E(yi |ui ) = Î¸ hui , xâˆ— i , i = 1, 2, . . . , n,

Another branch of one-bit compressed sensing is support
recovery. Current best result in terms of sample complexity
is O(s log d) in (Haupt & Baraniuk, 2011). However, their
work depends on various specially designed measurement
matrices, thus not universal. A universal support recovery
method is proposed in (Gopi et al., 2013), where all signals
can be recovered using a single measurement matrix. The
sample complexity of their work is O(s2 log d).
Most of the methods mentioned above use Gaussian measurements, and recently it is also extended to non-Gaussian
measurements in (Ai et al., 2014), where the authors use
sub-Gaussian measurements to recover both exactly and
approximately sparse signals. There are also other extensions. For example, Movahed et al. (2014) considered recovery of signals with unknown and time-variant sparsity

where value domain of the function Î¸(z) is [âˆ’1, 1]. We
define
E[Î¸(g)g] =: Î³ > 0,

(3.1)

where g is a standard Gaussian random variable, and Î³
measures the correlation between yi and hui , xâˆ— i.
When these two are well correlated, Î³ will get a larger
value. When yi is equalpto sign(hui , xâˆ— i) with no noise, Î³
will get maximal value 2/Ï€. Since scaling of xâˆ— does not
influence the one-bit measurements and we cannot restore
the scale, we assume xâˆ— is a unit vector, i.e., kxâˆ— k2 = 1.
3.2. Nonconvex Penalty Functions
We now introduce the decomposable nonconvex penalty
functions we consider in our work, i.e., GÎ»,b (x) =
Pd
i=1 gÎ»,b (xi ).
Typical nonconvex penalties include the smoothly clipped
absolute deviation (SCAD) penalty (Fan & Li, 2001) and
minimax concave penalty (MCP) (Zhang, 2010). For example, MCP is given by
ï£±
t2
ï£´
ï£´
ï£² Î»|t| âˆ’ , if |t| â‰¤ bÎ»,
2b
gÎ»,b (t) =
(3.2)
2
ï£´
bÎ»
ï£´
ï£³
,
if |t| > bÎ»,
2
with fixed regularization parameters b > 0, Î» > 0. A wellused property of the nonconvex penalties gÎ»,b (t) is that they
can be formulated as the sum of a `1 penalty part and a
concave part hÎ»,b (t) : gÎ»,b (t) = Î»|t| + hÎ»,b (t).

Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing

Note that we do not require specific forms of gÎ»,b (t), such
as MCP or SCAD. Generally, our work only depends on
the following conditions on gÎ»,b (t) and hÎ»,b (t):
0
C1. gÎ»,b
(t) = 0, for |t| â‰¥ Î½ â‰¥ 0.
C2. h0Î»,b (t) is monotone, and for t0 > t, there is a constant
Î¶âˆ’ â‰¥ 0 such that

where S(y, Î») is the soft-thresholding operator (Donoho
et al., 1993) defined for Î» â‰¥ 0 by
ï£±
ï£´
ï£² y âˆ’ Î», if y > Î»,
if |y| â‰¤ Î»,
S(y, Î») = 0,
ï£´
ï£³
y + Î», if y < âˆ’Î».
Proof. For b > 1, please see (Breheny & Huang, 2011).
For b â‰¤ 1, please see the longer version of this paper.

âˆ’Î¶âˆ’ (t0 âˆ’ t) â‰¤ h0Î»,b (t0 ) âˆ’ h0Î»,b (t).
C3. hÎ»,b (0) = h0Î»,b (0) = 0.
C4. |h0Î»,b (t)| â‰¤ Î» for any t.

We can quickly come up with a similar version of
Lemma 3.1 with Ï„ > 0.

There are a lot of nonconvex penalty functions that hold the
above properties. We can prove that MCP and SCAD are
valid choices. For MCP, Î½ = bÎ» and Î¶âˆ’ = 1/b. Since we
use MCP as our nonconvex penalty, we will use g, G and h,
H to specifically denote MCP in (3.2) and its concave part
for the rest of this paper.

Lemma 3.2. The solution to the following optimization
problem
1
Ï„
x
b = argmin (x âˆ’ y)2 + gÎ»,b (|x|) + x2
2
2
x
is given by
â€¢ if b(1 + Ï„ ) > 1

3.3. One-bit Compressed Sensing with Nonconvex
Penalty
Our estimator is any local optimal solution to the following
optimization problem
n

bÏ„ = argmin âˆ’
x
kxk2 â‰¤1

Ï„
1X
yi hui , xi + GÎ»,b (x) + kxk22 ,
n i=1
2
(3.3)

where u1 , u2 , . . . , un âˆˆ Rd are the rows of known measurement matrix U âˆˆ RnÃ—d .
Now we describe our algorithm to efficiently compute our
estimator by deriving the local minima of (3.3). We denote
v = U> y/n âˆˆ Rd . We begin with the following lemma.
Lemma 3.1. The solution to the following optimization
problem

is given by
â€¢ if b > 1

(3.4)

x
b=

0,

if |y| â‰¤

y, if |y| >

âˆš
âˆš

bÎ»,
bÎ»,

(3.7)

For the omitted proof of lemmas and theorems in the rest
of this work, please see the longer version of this paper.
Now we are ready to solve (3.3). For the sake of simplicity,
we first consider the case where Ï„ = 0 to illustrate our
method. We will show that the case where Ï„ > 0 can be
solved in a similar way.

f (Âµ) = min âˆ’x> v + GÎ»,b (x) + Âµ(kxk22 âˆ’ 1)
x
!
1
v 2 GÎ»,b (x)
kvk22
= min 2Âµ
kx âˆ’
k2 +
âˆ’
âˆ’Âµ
x
2
2Âµ
2Âµ
4Âµ
!
X
1
vi  2
= 2Âµ
min
xi âˆ’
+ gÎ»/(2Âµ),2Âµb (|xi |)
xi 2
2Âµ
i
âˆ’

â€¢ if b â‰¤ 1
(

â€¢ if b(1 + Ï„ ) â‰¤ 1
ï£±
p
ï£² 0,
if |y| â‰¤ b(1 + Ï„ )Î»,
p
x
b=
y
ï£³
, if |y| > b(1 + Ï„ )Î».
1+Ï„

(3.6)

We consider the Lagrange function f (Âµ) of (3.3) given by

1
x
b = argmin (x âˆ’ y)2 + gÎ»,b (|x|)
2
x

ï£±
ï£² S(y, Î») , if |y| â‰¤ bÎ»,
x
b = 1 âˆ’ 1/b
ï£³
y,
if |y| > bÎ»,

S(y, Î»)
, if |y| â‰¤ bÎ»(1 + Ï„ ),
1
+
Ï„ âˆ’ 1/b
x
b=
ï£´
ï£³ y ,
if |y| > bÎ»(1 + Ï„ ).
1+Ï„
ï£±
ï£´
ï£²

(3.5)

kvk22
âˆ’ Âµ.
4Âµ

(3.8)

Note that for the last step, given (3.2), we would easily
1
get that
gÎ»,b (|xi |) = gÎ»/2Âµ,2Âµb (|xi |). We will use Âµâˆ— to
2Âµ
denote the dual optimal solution that maximizes f (Âµ).

Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing

According to Lemma 3.1, we need to consider two cases:
(1) 2Âµb â‰¤ 1 and (2) 2Âµb > 1. For the first case, i.e.,
0 < Âµ â‰¤ 1/2b, we have summarized our method in Algorithm 1. For Âµ > 1/2b, our method is summarized in Algorithm 2. We will just sketch the outline here, and derivation
and technical details of Algorithm 1 and 2 can be found in
the longer version of this paper.
â€¢ 2Âµb â‰¤ 1: In this case, the solution to (3.8) comes
from (3.5). Therefore,
we need to compare the value
p
of |vi /2Âµ| and Î» b/2Âµ, which is equivalent to comparing Âµ and vi2 /2bÎ»2 , to decide the value of each term in
the summation in (3.8). After sorting |vi | and dividing
the feasible region into intervals, we will compute f (Âµ)
and find Âµâˆ— within each interval, which has a close form
solution as in Line 5 to 9 of Algorithm 1. Finally, among
optimal solutions in each interval, we find Âµâˆ—1 that maximizes f (Âµ).
Algorithm 1 Find maximizer of f (Âµ) when Âµ â‰¤ 1/2b
1: Input: Î», b, v
2: Output: Âµâˆ—1
3: Initialize f = f (1/2b), Âµâˆ—1 = 1/2b
4: v(1) , v(2) , ..., v(d) = Sort(|v1 |, |v2 |, ..., |vd |)
5: v(0) = 0, v(d+1) = âˆž
6: l = Find(v(l) â‰¤ 1/2b < v(l+1) )
7: for i:=0
qP... l do
n
2
2 2
2
2
8:
if
j=i v(j) /2 âˆˆ (v(i) /2bÎ» , v(i+1) /2bÎ» ] then
qP
d
2
9:
Âµ=
j=i v(j) /2
10:
else
2
11:
Âµ = v(i+1)
/2bÎ»2
12:
end if
13:
if f (Âµ) > f and Âµ < 1/2b then
14:
f = f (Âµ), Âµâˆ—1 = Âµ
15:
end if
16: end for
â€¢ 2Âµb > 1: In this case, the solution to (3.8) comes
from (3.4). We do similar sorting and dividing operation, yet within each interval, we need to solve a simple
optimization as in Line 8, Algorithm 2. Then we will
find the final Âµâˆ—2 based on values from each interval.
After finding the optimal values of Âµ from the above two
cases, we compare the objective function values of outputs
of Algorithm 1 and 2 to get the final Âµâˆ— :
Âµâˆ— = argmax f (Âµ).
âˆ—
Âµâˆˆ{Âµâˆ—
1 ,Âµ2 }

The optimal primal solution is
v
GÎ»,b (x)
1
b = argmin kx âˆ’ âˆ— k22 +
.
x
2
2Âµ
2Âµâˆ—
x

(3.9)

Algorithm 2 Find maximizer of f (Âµ) when Âµ > 1/2b
1: Input: Î», b, v
2: Output: Âµâˆ—2
3: Initialize f = f (1/2b), Âµâˆ—2 = 1/2b
4: v(1) , v(2) , ..., v(d) = Sort(|v1 |, |v2 |, ..., |vd |)
5: v(0) = 0, v(d+1) = âˆž
6: l = Find(v(l) â‰¤ 1/2b < v(l+1) )
7: for i:=l ...
Pnn do 2
8:
S1 = j=i+1 v(j)
Pi
9:
S2 = j=l (|v(j) | âˆ’ Î»)2
S1
S2
10:
J(Âµ) = 4Âµ
+ 2(2Âµâˆ’1/b)
+Âµ
11:
if Âµi = argminÂµ J(Âµ) âˆˆ (|v(i) |/2bÎ», |v(i+1) |/2bÎ»]
then
12:
Âµ = Âµi
13:
else
14:
Âµ = |v(i+1) |/2bÎ»
15:
end if
16:
if f (Âµ) > f and Âµ > 1/2b then
17:
f = f (Âµ), Âµâˆ—2 = Âµ
18:
end if
19: end for
By Lemma 3.1, we would finally get our estimator as follows:
â€¢ if 2Âµâˆ— b > 1
ï£±
S(vi , Î»)
ï£´
ï£´
, if |vi | â‰¤ 2Âµâˆ— Î»b,
ï£² âˆ—
2Âµ âˆ’ 1/b
x
bi =
vi
ï£´
ï£´
if |vi | > 2Âµâˆ— Î»b.
ï£³ âˆ—,
2Âµ
â€¢ if 2Âµâˆ— b â‰¤ 1
ï£±
p
ï£² 0,
if |vi | â‰¤ 2Âµâˆ— bÎ»,
p
x
bi =
v
ï£³ iâˆ— , if |vi | > 2Âµâˆ— bÎ».
2Âµ
For the case Ï„ > 0, we have a similar Lagrange function
f (Âµ0 ) with Âµ0 = Âµ + Ï„ /2. The optimization of f (Âµ0 ) is in
a similar manner, and we omit it here.
Note that although our algorithm is fairly involved, it only
involves sorting and analytic form calculation. So it is still
very efficient.
Remark 3.3. When Ï„ > Î¶âˆ’ , the estimator in (3.3) is actually strongly convex. The output of our algorithm is the
global optimal solution for (3.3). When Ï„ = 0, the estimator in (3.3) is nonconvex and our algorithm will output a
local minimum in the primal.

4. Main Results
We will prove that under a reasonable assumption on the
elements of the true signal xâˆ— , our estimator will have ora-

Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing

cle property, i.e., identical to the oracle estimator, with high
probability. This indicates exact support recovery. We will
also show the advantage of our method in terms of sample
complexity.
4.1. Oracle Property and Sample Complexity of Our
Estimator for Strong Signals
In this section, we will introduce the advantage of our estimator for strong signals, which consists of two parts, the oracle property and improved sample complexity of O(s/2 ).
bO is given by
The definition of the oracle estimator x
bO =
x

argmin

LO (x),

(4.1)

supp(x)âŠ‚S,kxk2 â‰¤1

Theorem 4.3.
Pd Assume that we have the nonconvex penalty
GÎ» (x) =
i=1 gÎ»,b (xi ) that satisfies conditions C1 and
C2. If the true signal xâˆ— satisfies the magnitude condition
bÏ„ with
minjâˆˆS |xâˆ—j | â‰¥ Î½ + kb
xO âˆ’ xâˆ— k2 , for our estimator x
p
regularization parameter Î» = C log d/n + |Î³ âˆ’ Ï„ | and
Î¶âˆ’ < Ï„ â‰¤ kvS k2 as in Lemma 4.1, we have
bÏ„ = x
bO ;
(1) x
(2) With a probability of at least 1 âˆ’ 1/d,
r
C s
âˆ—
,
kb
xÏ„ âˆ’ x k2 â‰¤
Î³ n
where C is a universal constant.

Pn

where LO (x) = âˆ’1/n i=1 yi hui , xi. Note that the oracle estimator does not have `2 regularizer.
In the rest of this paper, we define the following shorthand
notations
HÎ»,b (x) =

d
X

hÎ»,b (xi ) = GÎ»,b (x) âˆ’ Î»kxk1 ,

i=1

L(x) = LO (x) +

1
Ï„
Ï„
kxk22 = âˆ’ y> Ux + kxk22 ,
2
n
2

and
1
Ï„
LeÎ» (x) = L(x) + HÎ»,b (x) = âˆ’ y> Ux + kxk22
n
2
+ HÎ»,b (x).
Before we lay out the main result, we first present two lemmas, which are central to prove the main result. First, we
have the following important property for the oracle estimator.
Pn
Lemma 4.1. If Ï„ â‰¤ kvS k2 where v = âˆ’1/n i=1 yi ui
and S is the support of xâˆ— . The following optimization
problem
n

b=
x

argmin
supp(x)âŠ‚S,kxk2 â‰¤1

âˆ’

Ï„
1X
yi hui , xi + kxk22 , (4.2)
n i=1
2

has the same solution as the oracle estimator in (4.1).
Second, the following lemma reveals the relation between
the real signal and the measurements.
Lemma 4.2. With a probability at least 1 âˆ’ 1/d, we have
r
 >


 US y
s
âˆ—

(4.3)
 n âˆ’ Î³xS  â‰¤ C n ,
2
where C is a universal constant and S is the support of xâˆ— .
Equipped with Lemma 4.1 and Lemma 4.2, we have the
following theorem establishing the oracle property and
sample complexity of our estimator for strong signals.

In Theorem 4.3, the dependence on s is suboptimal, because we only obtain the `2 norm based estimator error
bound. In order to get rid of s, we need `âˆž norm based estimator error bound, which requires a much stronger condition namely irrepresentable condition (Wainwright, 2009).
Theorem 4.3 indicates that our estimator will be identical
to oracle estimator under a magnitude assumption, while
requiring no oracle information a priori. This will lead
to exact support recovery directly. It is worth noting that
the `2 regularizer in (3.3) is essential to achieve the oracle
property, the estimator in (3.3) with Ï„ > Î¶âˆ’ is identical to
the oracle estimator in (4.1). In particular, the `2 regularizer makes the objective function in (3.3) strongly convex,
based on which we can prove the estimator with `2 regularizer in (3.3) is identical to the oracle estimator in (4.1).
Note that the oracle estimator in (4.1) does not have `2 regularizer. Therefore, the l2 regularizer in (3.3) does not introduce any extra approximation error. Note also that for
Theorem 4.5, we do not need the `2 regularizer (Ï„ = 0).
Now we analyze the error bound of oracle estimator, which
is also the error bound of our estimator. We will also show
that the magnitude assumption is actually a weak assumption.
Moreover, we can see that thep
recovery error of our method
for strong signals is just O( s/n), indicating a sample
complexity of O(s/2 ), which is a significant improvement
from previous best result O(s log d/2 ).
p
In addition, we have kb
xO âˆ’ xâˆ— k2 â‰¤ C/Î³ s/n with high
probability, which does not depend on the magnitude assumption. Therefore, we will only need
p
min |xâˆ—j | â‰¥ Î½ + C/Î³ s/n
(4.4)
jâˆˆS

bÏ„ = x
bO with probability at least 1 âˆ’ 1/d. This
to get x
is a weak assumption, since one-bit measurements can be
acquired at very high rates. When n is very large, the righthand side of (4.4) will converge to a constant Î½. Note again

Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing

that for the oracle
p estimator, the error bound is always of
the order of O( s/n), which does not depend on the magnitude assumption.
4.2. Sample Complexity of Our Estimator for General
Signals
We now turn to the case of general signals, where the magnitude assumption does not hold necessarily. In this case,
bÏ„ =0 .
we consider our estimator in (3.3) with Ï„ = 0, i.e., x
Note that when Ï„ = 0, the problem in (3.3) is not convex.
However, as we will see later, our theory applies to any
local optimal solution to (3.3). In other words, it is sufficient to get a local optimal solution to (3.3) as our estimator.
We start with the following lemma, which characterizes the
curvature of the loss function in the ball kxk2 â‰¤ 1.
Lemma 4.4. For any x where kxk2 â‰¤ 1, we have
1
hE[U> y]/n, xâˆ— âˆ’ xi
â‰¥ kxâˆ— âˆ’ xk22 .
Î³
2

hE[U> y]/n, xâˆ— âˆ’ xi = hÎ³xâˆ— , xâˆ— âˆ’ xi
= hÎ³xâˆ— âˆ’ Î³x + Î³x, xâˆ— âˆ’ xi
= Î³kxâˆ— âˆ’ xk22 + Î³hx, xâˆ— âˆ’ xi.
(4.5)
On the other hand, we have
1 1
âˆ’ kxk22
2 2

1
1
= x> xâˆ— âˆ’ kxâˆ— k22 âˆ’ kxk22
2
2
1
âˆ— 2
= âˆ’ kx âˆ’ x k2 .
2
Substituting (4.6) into (4.5), we obtain
Î³
hE[U> y]/n, xâˆ— âˆ’ xi â‰¥ kx âˆ’ xâˆ— k22 ,
2
which completes the proof.

where C is a universal constant.

S2 :0<|xâˆ—
i |<Î½

5. Experiments

â€¢ Passive: the passive algorithm proposed in (Zhang et al.,
2014), the best previous result on sample complexity.
â€¢ Convex: the convex programming approach proposed
in (Plan & Vershynin, 2013a).
â€¢ BIHT and BIHT-`2 proposed in (Jacques et al., 2013)
5.1. Approximate Vector Recovery for General Signals

(4.6)

We are now ready to present the following theorem, which
bounds the error of our estimator for general signals.
Theorem 4.5. Suppose the nonconvex penalty GÎ»,b (x) =
Pd
i=1 gÎ»,b (xi ) satisfies conditions C2, C3 and C4. For
bÏ„ =0 to (3.3) with Ï„ = 0,
any local
r optimal solution x
log d
Î³
Î» = C
and Î¶âˆ’ < , we have with probability
n
2
at least 1 âˆ’ 1/d that
r
âˆš r
6C s2 log d
2C
s1
âˆ—
kb
xÏ„ =0 âˆ’ x k2 â‰¤
+
,
Î³ âˆ’ 2Î¶âˆ’ n
Î³ âˆ’ 2Î¶âˆ’
n
|
{z
} |
{z
}
S1 :|xâˆ—
i |â‰¥Î½

When Ï„ = 0, our algorithm will output a local minimum in
the primal. Note the proof of Theorem 4.5 relies only on the
first order necessary condition for local minima. Therefore,
any local minima to the optimization problem (3.3) with
Ï„ = 0 enjoys the rate in Theorem 4.5. Since the output of
our algorithm in this case (Ï„ = 0) is a local minimum in the
primal, it enjoys the theoretical guarantee in Theorem 4.5.

In this section, we will verify our theoretical results on synthetic datasets. For each recovery task, we will tune C and
b by cross validation and select Î» and Ï„ according to Theorem 4.3 for strong signals and Theorem 4.5 for general
signals. For each parameter setting, we present the average
results of 100 trials of our method and four other methods:

Proof. We have

hx, xâˆ— âˆ’ xi = x> xâˆ— âˆ’ kxk22 â‰¥ x> xâˆ— âˆ’

From Theorem 4.5, we can see that for strong signals, we
have |xâˆ—i | â‰¥ Î½ for all
pi âˆˆ S, thus s2 = 0. Then our recovery error is just O( s/n), which is equivalent to a sample complexity of O(s/2 ). In the worst case, |xâˆ—i | < Î½
forp
all i âˆˆ S, thus s2 = s, and our recovery error is
O( s log d/n). This yields the worst sample complexity
of O(s log d/2 ).

In this subsection, we will show our experimental results on
general signals, i.e., no magnitude assumption guaranteed.
We will randomly select their support and draw the values
of nonzero elements from a standard normal distribution.
The elements in the matrix U are also drawn from standard
normal distribution. We choose the noisy setting in (Plan
& Vershynin, 2013a) by flipping the signs of measurements
with a probability of 0.1.
Figure 1(a) shows the recovery error against the dimensionality of signals d. We can see that our proposed method outperforms all the other algorithms with a remarkable margin.
As the dimensionality of signal d goes up, the recovery error grows slowly, because the dependency on d is logarithmic by Theorem 4.5. We can also see that in this noisy setting, the more vulnerable BIHT and BIHT-`2 consistently
perform worse than the other methods.
Figure 1(b) shows the recovery error against the number
of measurements n. Our method consistently achieves the
best performance. The passive algorithm also performs reasonably well, but our method outperforms it in a wide range
of n.

Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing

kb
x âˆ’ xâˆ— k 2

0.8
0.7
0.6
0.5

1

BIHT
BIHT-`2
Convex
Passive
Ours

0.8

0.4

0.6

0.4

BIHT
BIHT-`2
Convex
Passive
Ours

0.9
0.8

kb
x âˆ’ xâˆ— k 2

BIHT
BIHT-`2
Convex
Passive
Ours

kb
x âˆ’ xâˆ— k 2

1
0.9

0.7
0.6
0.5
0.4
0.3

0.2

0.3

0.2
2000

4000

6000

8000

0
2000

10000

d

(a) s = 10, n = 1000.

4000

6000

n

8000

10000

(b) s = 10, d = 1000.

0.1

20

40

s

60

80

100

(c) d = 1000, n = 3000.

Figure 1. Recovery error for general signals

Figure 1(c) shows the recovery error against the sparsity
of signals s. We can see that for all the algorithms except
BIHT, the error goes up quickly when s becomes larger.
Our algorithm is still consistently the best among all. Note
that the dependency on s is not logarithmic, therefore, the
error grows much faster than the case of varying d. We
choose number of measurements n = 3000 here, which is
larger than the signal dimension d. This is practical in onebit compressed sensing, because the one-bit measurements
can be generated at very high rates. To sum up, our method
can improve recovery accuracy in different parameter settings even with noise.
5.2. Approximate Vector Recovery for Strong Signals

covery error stays on the same level, while the errors of all
the other algorithms go up with increasing d. Note that the
error of BIHT is much higher than the other algorithms.
For better illustration and scaling the behavior of the other
methods, we omit it in the figure here.
5.3. Support Recovery
We are now going to investigate the problem of support
recovery. According to Theorem 4.3, our estimator enjoys
oracle property for strong signals. We generate the signals
in the same way as section 5.2 and present the F1 score of
support recovery in different d and n settings. F1 score is
defined as the harmonic mean of precision and recall.
TP
TP
, Recall =
TP + FP
TP + FN
2 Â· Precision Â· Recall
F1 =
,
Precision + Recall

In this subsection, we present results of our recovery algorithm for strong signals. We will first generate unit sparse
signals
âˆš with random support, and set all nonzero entries to
1/ s. Noise is added in the same way with section 5.1.

Precision =

where
BIHT-`2
Convex
Passive
Ours

TP =

1(b
xi 6= 0, xâˆ—i 6= 0), FP =

i=1

0.3

kb
x âˆ’ xâˆ— k2

d
X

FN =

0.25

d
X

d
X

1(b
xi 6= 0, xâˆ—i = 0)

i=1

1(b
xi = 0, xâˆ—i 6= 0).

i=1

Figure 2. Recovery error of strong signals against d when s = 10,
n = 1000.

Note that our method is different from best previous work
on support recovery. We do not need to construct specific
measurement matrix as (Gopi et al., 2013; Haupt & Baraniuk, 2011), nor do we depend on dynamic range or adaption of the measurement process as (Gupta et al., 2010).
Therefore, their methods are not directly comparable with
ours.

Figure 2 shows the recovery error of strong signals. According to Theorem 4.3, our error rate does not depend on
dimensionality d, which is verified by the results. Our re-

Figure 3(a) shows the F1 score against signal dimension
d. We can see that as the assumption in Theorem 4.3 is
satisfied, our algorithm can achieve exact support recovery
with very high probability. Our method and BIHT-`2 out-

0.2

0.15

2000

4000

6000

8000

10000

d

Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing

BIHT
BIHT-`2
Convex
Passive
Ours

1

F1 Score

0.8
0.6

5.4. Oracle Property

0.4
0.2
0

our algorithm can achieve very good performance, almost
recover the support with probability 1. BIHT-`2 has a similar behavior as our algorithm with enough measurements,
but our method requires fewer measurements.

2000

4000

6000

8000

10000

d
(a) s = 10, n = 1000.

We will further study the oracle property of our estimator.
We plot the difference between proposed estimator and the
oracle estimator in (4.1). By Theorem 4.3, the two should
be the same with high probability. In Figure 4, we can see
BIHT
BIHT-`2
Convex
Passive
Ours

1

1

b O k2
kb
xâˆ’x

0.8

F1 Score

0.8
0.6

0.6
0.4

0.4

BIHT
BIHT-`2
Convex
Passive
Ours

0.2
0

2000

4000

6000

8000

n
(b) s = 10, d = 1000.

0.2
0

2000

4000

n

6000

8000

10000

10000

Figure 4. Difference between estimators and oracle estimators
against n when s = 10, d = 1000.

Figure 3. F1 score for support reovery

perform the other algorithms with notable margins. In addition, Theorem 4.3 indicates that the support recovery of
our method does not depend on d, which is also validated
by the experiments. While for the other algorithms, the performance of the passive algorithm drops significantly as d
goes up; BIHT is not effective either, nor can it achieve a
stable performance. Note that for the convex optimization
method, there is no `0 constraints on the signal. Therefore,
most of the entries in the estimator are nonzero, resulting
in very low precision. This explains the observation that
convex optimization method always have a F1-score close
to zero.
In Figure 3(b), we plot the F1 score against number of measurements n. For the same reason, the convex optimization
method still suffers very low F1 score close to 0. For the
other four methods, when there are not enough measurements, they perform poorly on support recovery. As the
number of measurements goes up, the passive algorithm is
the fastest to boost the performance. However, the F1 score
will stop increasing around 0.7 in spite of the increase of
measurements. For BIHT, the performance is less stable,
but F1 score will still converge around 0.7 with increasing
measurements. Compared with the passive algorithm, our
algorithm needs a bit more measurements to converge in
terms of F1 score. Moreover, when n is larger than 500,

that when the number of measurements goes up, the difference between our estimator and oracle estimator converges
to zero very quickly. For BIHT and BIHT-`2 , the differences are large; for the passive algorithm, the difference is
still discernible, and the support recovery is not satisfying;
for the convex optimization algorithm, although the norm
of the difference is converging, it cannot recover the support. Therefore, our estimator is the only one that enjoys
oracle property.

6. Conclusions
In this paper, we proposed a novel and effective method
based on nonconvex penalty functions for one-bit compressed sensing. Compared with existing methods, our
method improves the sample complexity significantly, and
achieves excellent performance on support recovery. We
also showed that our method is robust to noise by numerical experiments.

Acknowledgments
We would like to thank the anonymous reviewers for their
helpful comments. Research was sponsored by Quanquan
Guâ€™s startup funding at Department of Systems and Information Engineering, University of Virginia.

Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing

References
Ai, Albert, Lapanowski, Alex, Plan, Yaniv, and Vershynin,
Roman. One-bit compressed sensing with non-gaussian
measurements. Linear Algebra and its Applications, 441
(0):222 â€“ 239, 2014. ISSN 0024-3795. Special Issue on
Sparse Approximate Solution of Linear Systems.

Jacques, L., Laska, J. N., Boufounos, P. T., and Baraniuk,
R. G. Robust 1-bit compressive sensing via binary stable
embeddings of sparse vectors. IEEE Trans. Info. Theory,
59(4), April 2013.

Boufounos, P. T. and Baraniuk, R. G. 1-bit compressive
sensing. In Proc. Conf. Inform. Science and Systems
(CISS), Princeton, NJ, March 19-21 2008.

Movahed, A., Panahi, A., and Reed, M.C. Recovering signals with variable sparsity levels from the noisy 1-bit
compressive measurements. In Acoustics, Speech and
Signal Processing (ICASSP), 2014 IEEE International
Conference on, pp. 6454â€“6458, May 2014.

Boufounos, P.T. Reconstruction of sparse signals from distorted randomized measurements. In Acoustics Speech
and Signal Processing (ICASSP), 2010 IEEE International Conference on, pp. 3998â€“4001, March 2010.

Plan, Y. and Vershynin, R. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. Information Theory, IEEE Transactions
on, 59(1):482â€“494, Jan 2013a. ISSN 0018-9448.

Breheny, Patrick and Huang, Jian. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. The Annals of
Applied Statistics, 5(1):232â€“253, 03 2011.

Plan, Yaniv and Vershynin, Roman.
One-bit compressed sensing by linear programming. Communications on Pure and Applied Mathematics, 66(8):1275â€“
1297, 2013b. ISSN 1097-0312.

Candes, E.J. and Tao, T. Near-optimal signal recovery from
random projections: Universal encoding strategies? Information Theory, IEEE Transactions on, 52(12):5406â€“
5425, Dec 2006. ISSN 0018-9448.

Wainwright, Martin J.
Sharp thresholds for highdimensional and noisy sparsity recovery usingconstrained quadratic programming (lasso). Information
Theory, IEEE Transactions on, 55(5):2183â€“2202, 2009.

Donoho, D., Johnstone, I., and Johnstone, Iain M. Ideal
spatial adaptation by wavelet shrinkage. Biometrika, 81:
425â€“455, 1993.

Wang, Zhaoran, Liu, Han, and Zhang, Tong. Optimal computational and statistical rates of convergence for sparse
nonconvex learning problems. The Annals of Statistics,
42(6):2164â€“2201, 12 2014.

Donoho, D.L. Compressed sensing. Information Theory,
IEEE Transactions on, 52(4):1289â€“1306, April 2006.
ISSN 0018-9448.
Fan, J. and Li, R. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal
of the American Statistical Association, 96(456):1348â€“
1360, 2001.
Gopi, Sivakant, Netrapalli, Praneeth, Jain, Prateek, and
Nori, Aditya V. One-bit compressed sensing: Provable
support and vector recovery. In Proceedings of the 30th
International Conference on Machine Learning, ICML
2013, Atlanta, GA, USA, 16-21 June 2013, pp. 154â€“162,
2013.
Gu, Quanquan, Wang, Zhaoran, and Liu, Han. Sparse pca
with oracle property. In Advances in Neural Information
Processing Systems, pp. 1529â€“1537, 2014.
Gupta, A., Nowak, R., and Recht, B. Sample complexity
for 1-bit compressed sensing and sparse classification.
In Information Theory Proceedings (ISIT), 2010 IEEE
International Symposium on, pp. 1553â€“1557, June 2010.
Haupt, J. and Baraniuk, R. Robust support recovery using
sparse compressive sensing matrices. In Information Sciences and Systems (CISS), 2011 45th Annual Conference
on, pp. 1â€“6, March 2011.

Zeng, Xiangrong and Figueiredo, M.A.T. Robust binary
fused compressive sensing using adaptive outlier pursuit.
In Acoustics, Speech and Signal Processing (ICASSP),
2014 IEEE International Conference on, pp. 7674â€“7678,
May 2014.
Zhang, Cun-Hui. Nearly unbiased variable selection under
minimax concave penalty. Ann. Statist., 38(2):894â€“942,
2010.
Zhang, Lijun, Yi, Jinfeng, and Jin, Rong. Efficient algorithms for robust one-bit compressive sensing. In Jebara,
Tony and Xing, Eric P. (eds.), Proceedings of the 31st
International Conference on Machine Learning (ICML14), pp. 820â€“828. JMLR Workshop and Conference Proceedings, 2014.

