Efficient Algorithms for Robust One-bit Compressive Sensing

Lijun Zhang
ZHANGLJ @ LAMDA . NJU . EDU . CN
National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China
Jinfeng Yi
IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598, USA

JINFENGY @ US . IBM . COM

Rong Jin
RONGJIN @ CSE . MSU . EDU
Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824, USA

Abstract
While the conventional compressive sensing assumes measurements of infinite precision, onebit compressive sensing considers an extreme
setting where each measurement is quantized to
just a single bit. In this paper, we study the vector
recovery problem from noisy one-bit measurements, and develop two novel algorithms with
formal theoretical guarantees. First, we propose
a passive algorithm, which is very efficient in
the sense it only needs to solve a convex optimization problem that has a closed-form solution. Despite the apparent simplicity, our theoretical analysis reveals that the proposed algorithm
can recover both the exactly sparse and the approximately sparse vectors. In particular, for a
sparse vector with s nonzero elements, the sample complexity is O(s log n/ǫ2 ), where n is the
dimensionality and ǫ is the recovery error. This
result improves significantly over the previously best known sample complexity in the noisy
setting, which is O(s log n/ǫ4 ). Second, in the
case that the noise model is known, we develop an adaptive algorithm based on the principle
of active learning. The key idea is to solicit the
sign information only when it cannot be inferred
from the current estimator. Compared with the
passive algorithm, the adaptive one has a lower
sample complexity if a high-precision solution is
desired.

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

1. Introduction
Compressive sensing is designed to recover a sparse signal from a small number of linear measurements (Donoho,
2006; Candes & Tao, 2006).
A variant of compressive sensing, named one-bit compressive sensing,
has attracted considerable interests over the past few
years (Boufounos & Baraniuk, 2008). Unlike the conventional compressive sensing which relies on real-valued
measurements, in one-bit compressive sensing, each measurement is quantized to a single bit (e.g., the sign of a
measurement). This new setup is appealing because (i) the
hardware implementation of one-bit quantizer is low-cost
and efficient, (ii) one-bit measurement is robust to nonlinear distortions (Boufounos, 2010), and (iii) in certain situations, for example, when the signal-to-noise ratio is low,
one-bit compressive sensing performs even better than the
conventional one (Laska & Baraniuk, 2012).
In this paper, we focus on the sample complexity of vector
recovery in one-bit compressive sensing,1 i.e., the number
of one-bit measurements that are needed to recover the direction of the target vector x∗ with at most ǫ error. Sample complexity of one-bit compressive sensing has been studied extensively in the noiseless case (Plan & Vershynin,
2013a; Jacques et al., 2013; Gopi et al., 2013). When coming to the noisy case (i.e., the output bit is randomly perturbed from the sign of the real-valued measurement), only
limited results are available (Plan & Vershynin, 2013b) .
We address this issue by developing two novel algorithms
for robust one-bit compressive sensing that are computationally efficient and demonstrate significantly lower sample complexities than the existing studies. More specifically, the main contributions of this paper are:
1
Strictly speaking, there are two types of sample complexities
in compressive sensing: one holds for a fixed vector and the other
holds for all possible vectors (Gopi et al., 2013). In this study, we
focus on the sample complexity for a fixed vector.

Efficient Algorithms for Robust One-bit Compressive Sensing

• Unlike previous studies of one-bit compressive
sensing that require solving optimization problems (Plan & Vershynin, 2013b), the proposed algorithm
has a closed-form solution, making it computationally
attractive.
• Our analysis shows that in the case of noisy one-bit
measure, the proposed algorithm improves the sample complexity from O(s log n/ǫ4 ) to O(s log n/ǫ2 )
when the target signal is an exactly s-sparse ndimensional vector.
• We develop a novel adaptive algorithm to further reduce the number of one-bit measurements.
When the noisy model is known, the
proposed adaptive algorithm improves√the sample
complexity to O(min(s log n/ǫ2 , s n log n/ǫ))
if the target vector
√ is exactly s-sparse and to
O(min(s log n/ǫ4 , s n log n/ǫ3 )) if the target √
vector
is approximately s-sparse (i.e., kx∗ k1 /kxk2 ≤ s).

2. Related Work
One-bit compressive sensing was first introduced
in (Boufounos & Baraniuk, 2008),
where only
the noiseless one-bit measure is considered.
Let
U = [u1 , . . . , um ] ∈ Rn×m be a known measurement
matrix, and y = [y1 , . . . , ym ]⊤ be the m-dimensional
The
one-bit measurement, where yi = sign(x⊤
∗ ui ).
authors propose to recover the direction of target signal x∗
by solving the following optimization problem
⊤

min kxk1 s. t. y ◦ (U x) ≥ 0, kxk2 = 1

(1)

x

where ◦ stands for the element-wise product between two
vectors. One problem with (1) is that it requires solving
a non-convex optimization problem. A provable optimization algorithm was proposed in (Laska et al., 2011) to find
a stationary point of (1). However, none of these two works
provide a formal guarantee on the sample complexity.
In (Jacques et al., 2013), the authors study a similar formulation by replacing kxk1 in (1) with kxk0 , and show a sample complexity of O(s log n/ǫ) for recovering the direction
of a s-sparse vector. However, it remains unsolved as how
to efficiently solve the corresponding non-convex optimization problem is unclear. Gopi et al. (2013) developed an efficient two-stage algorithm for one-bit compressive sensing
that achieves a sample complexity of O(s log n/ǫ).
The first convex formulation for one-bit compressive sensing was proposed in (Plan & Vershynin, 2013a). It solves
the following linear programming problem
min kxk1 s. t. y ◦ (U ⊤ x) ≥ 0, kU ⊤ xk1 = m

(2)

x

An important property of the formulation in (2) is that it can
recover not only the exactly sparse vector but also the ap-

√
proximately sparse vector (i.e., kx∗ k1 /kxk2 ≤ s). However, a major drawback of this study is the sample complexity, which is O(s log2 n/ǫ5 ), exhibits a very high dependence on 1/ǫ.
So far, all the related work discussed above assume the
one-bit measure to be perfect (i.e., yi = sign(x⊤
∗ ui )).
Although several heuristic algorithms (Yan et al., 2012;
Movahed et al., 2012; Jacques et al., 2013) were proposed to handle noise in one-bit measure, none of them
has theoretical guarantees. The only provable recovery algorithm for robust compressive sensing is given
in (Plan & Vershynin, 2013b), where the sparse vector is
recovered by solving the following convex optimization
problem
max x⊤ U y s. t. kxk2 ≤ 1, kxk1 ≤

√
s

(3)

x

According to (Plan & Vershynin, 2013b), the above formulation can recover both the exactly sparse and approximately sparse vectors with a sample complexity O(s log n/ǫ4 ).
Table 1 summarizes the sample complexities of existing results, as well as the two algorithms proposed in this paper.
Besides the vector recovery problem, several algorithms
have been developed for recovering the support set of the
target vector (Gupta et al., 2010; Haupt & Baraniuk, 2011;
Gopi et al., 2013), which is not closely related to our work.
Finally, the idea of adaptive sensing, which uses information gathered from previous measurements to guide
the design and selection of the next measurement, has been applied to both conventional compressive
sensing (Haupt et al., 2009a;b) and one-bit compressive (Gupta et al., 2010). The general conclusion is that
adaptive sensing is helpful in recovering sparse signals
when signal-to-noise ratio is low (Malloy & Nowak, 2012).

3. Efficient Algorithms for One-bit
Compressive Sensing (CS)
We first introduce notations and assumptions of one-bit
compressive sensing. We then present both passive and
adaptive algorithms for one-bit compressive sensing, followed by their theoretical guarantees.
3.1. Preliminary
Let x∗ ∈ Rn be a sparse or compressible vector to be
recovered. Let U = [u1 , . . . , um ] ∈ Rn×m be the random Gaussian matrix used to obtain the binary measurements for x∗ , and elements of U are i.i.d. standard Gaussian random variables. For each vector ui , we receive a
1-bit measurement yi ∈ {−1, 1} from an Oracle. Following (Plan & Vershynin, 2013b), we assume that yi is drawn

Efficient Algorithms for Robust One-bit Compressive Sensing

Table 1. Sample Complexities of existing algorithms for one-bit compressive sensing.
x∗

N OISELESS

x∗

S AMPLE C OMPLEXITY

R EFERENCE


n
O s log
ǫ


2
n
O s log
5
ǫ

(JACQUES ET AL ., 2013)
(G OPI ET AL ., 2013)

O

i = 1, . . . , m

(4)

where θ(z) : R 7→ [−1, +1] is some nonlinear function that
can be unknown. In order to capture the relation between
ui and yi , following (Plan & Vershynin, 2013b), we define
λ for θ(z) as follows,
λ := Eg∼N (0,1) [θ(g)g]

(5)

(P LAN & V ERSHYNIN , 2013 A )

Since we only receive the sign information about the random measurements, it is impossible to recover the scale of
x∗ . As a result, we will only consider the recovery of the
direction of x∗ , and therefore assume kx∗ k2 = 1.
3.2. Passive Algorithm for 1-bit CS
The proposed algorithm is inspired by the convex formula√
tion in (3). Instead of having a constraint kxk1 ≤ s to
ensure a sparse solution, we introduce a ℓ1 regularizer in
the objective function, leading to the following optimization problem
1 ⊤
x U y + γkxk1
m

O


min



s log n
ǫ4



(P LAN & V ERSHYNIN , 2013 B )

s log n
,
ǫ4

(O UR PASSIVE A LGORITHM )


√
s n log n
(O UR
3
ǫ

A DAPTIVE A LGORITHM )

Pγ ([α1 , . . . , αm ]⊤ ) = [Pγ (α1 ), . . . , Pγ (αm )]⊤ .
b be the optimal solution of (6). Then, we
Lemma 1. Let x
have
1

(
0,
if  m
U y∞ ≤ γ;

1
b=
x
P 1 U y , otherwise.
kPγ ( m1 U y)k2 γ m
The proof can be found in the supplementary material.

where λ measures how well yi is correlated with x⊤
∗ ui . We
assume λ > 0, implying that a positive correlation between
the real-valued measurement and the binary output from
θ(·).

min −



s log2 n
ǫ5

We extend the operator Pγ (·) to vectors as

independently at random satisfying

kxk2 ≤1



R EFERENCE

(P LAN & V ERSHYNIN , 2013 A )



E[yi |ui ] = θ(x⊤
∗ ui ),

IS APPROXIMATELY SPARSE

S AMPLE C OMPLEXITY

s log n
(P LAN & V ERSHYNIN , 2013 B )
ǫ4

s log n
O
(O UR PASSIVE A LGORITHM )
ǫ2



√
n s n log n
,
(O UR A DAPTIVE A LGORITHM )O
O min s log
ǫ
ǫ2

O

N OISY

IS EXACTLY SPARSE

(6)

where γ > 0 is a regularization parameter, whose value
will be discussed later. As shown below, the problem in (6)
has a closed-form solution.
Define the soft-thresholding operator (Donoho, 1995;
Duchi & Singer, 2009) as

0,
if |α| ≤ γ;
Pγ (α) =
(7)
sign(α)(|α| − γ), otherwise.

The following theorem provides the recovery rate for the
optimal solution to (6).
Theorem 1. Assume
r
t + log n
γ = 2c
(8)
m
for some constant c. If x∗ is exactly sparse, i.e., kx∗ k0 ≤ s,
with a probability at least 1 − e1−t , we have
!
r
s log n
3γ p
.
kx∗ k0 = O
kb
x − x∗ k2 ≤
λ
m
√
If x∗ is approximately sparse, i.e., kxk1 ≤ s, with a probability at least 1 − e1−t , we have
!
r
r
3γ
4 s log n
kb
x − x∗ k2 ≤
kx∗ k1 = O
.
λ
m
Remark Compared to the result in (Plan & Vershynin,
2013b), the proposed algorithm improves the sample complexity from O(s log n/ǫ4 ) to O(s log n/ǫ2 ) when recovering an exactly s-sparse vector from noisy one-bit measurements. In addition, the sample complexity of the proposed algorithm for one-bit compressive sensing matches the minimax rate of conventional compressive sensing (Raskutti et al., 2011) for both exactly sparse and approximately sparse vectors. We however emphasize that

Efficient Algorithms for Robust One-bit Compressive Sensing

Algorithm 1 An adaptive algorithm for One-bit Compressive Sensing
1: Input: the number of stages K, the initial sample
size m1 , the initial regularizer γ1 , the step size η ∈
{21/2 , 21/4 }
2: Let x1 be any unit vector, δ1 = 1
3: for k = 1 to K do
4:
Randomly sample mk Gaussian random vectors
Gk = {uk1 , . . . , ukmk }.
5:
Divide Gk into Ak and Bk according to (11)
6:
For uki ∈ Ak , generate the one-bit measurement yik
k
from sign(x⊤
k ui )
k
7:
For ui ∈ Bk , query the Oracle to obtain the one-bit
measurement yik
8:

xk+1

mk
1 X
= argmin −
y k x⊤ uki + γk kxk1
mk i=1 i
kxk2 ≤1

√

9:
mk+1 = 2mk , γk+1 = γk / 2, δk+1 = δk /η
10: end for
11: Output: the final solution xK+1

One possible noise model is
y = ξsign(x⊤
∗ u),

(9)

where ξ is a independent {−1, 1} valued random variable
with Pr(ξ = −1) = p, representing random bit flips (Plan & Vershynin, 2013b). It is straightforward to generate the one-bit measurement y if both sign(x⊤
∗ u) and p
are provided.
The complete procedure is provided in Algorithm 1. Our
algorithm is closely related to the epoch gradient algorithm developed for stochastic optimization (Hazan & Kale,
2011). It divides the recovery process into K stages. At
each stage k > 1, we assume that an approximate solution
xk is obtained from the previous stage with
kxk k2 = 1, and kxk − x∗ k2 ≤ δk .

(10)

{uk1 , . . . , ukmk }

Let Gk =
be a set of mk vectors that are
independently sampled from Gaussian distribution. We divide the set Gk into two subsets:




k 

k  ⊤ ui
 > δk ,
A k = u i : x k
kuki k2 
(11)





uki 
Bk = uki : x⊤
≤
δ
.
k
k
kuki k2 

the guarantee for conventional compressive sensing algorithm does not directly apply to one-bit compressive sensing because E[yi ] is not proportional to x⊤
∗ ui . We also note
that this sample complexity is better than O(n/ǫ2 ), which
is the optimal rate for binary classification in the noisy setting (Anthony & Bartlett, 1999, Theorem 5.2).

where Ak includes random vectors whose directions are
close to xk or −xk while Bk includes those that are far
away from xk and −xk . The following Lemma reveals an
important property of Ak
Lemma 2. Under the condition in (10), we have

3.3. An Adaptive Algorithm for 1-bit CS

Since for any u ∈ Ak , sign(x⊤
∗ u) can be inferred from
sign(xk⊤ u), we can skip one-bit measurement for any u ∈
Ak and reduce the number of one-bit measurements.

The proposed algorithm aims to explore the idea of active
learning (Dasgupta, 2011) to reduce the number of onebit measurements. The key observation is that after observing certain number of one-bit measurements, we can
b that is reasonably close
obtain an intermediate solution x
to the direction of the target vector. As a result, for the
sequentially sampled random vector u, we would expect sign(b
x⊤ u) = sign(x⊤
∗ u) if the direction of u is close
b (or −b
to that of x
x) and therefore do not need to ask for
an one-bit measurement for u. However, it is problematic
to directly replace y, the one-bit measurement for u, with
sign(b
x⊤ u) since y is perturbed by random noise. A similar
issue was also raised in (Yang & Hanneke, 2013), where
the authors propose to re-noise the data to ensure all the
measurements follow the same distribution. In this paper,
for the sake of simplicity, we make the following assumption:
A1: We assume that for a vector u, if the value of
sign(x⊤
∗ u) is provided, we can generate the one-bit measurement y without querying the Oracle.

sign(x∗⊤ u) = sign(x⊤
k u), ∀u ∈ Ak .

We now discuss the recover property of Algorithm 1. For
the case that x∗ is exactly sparse, we have the following
theorem for the adaptive algorithm.
Theorem 2. Suppose x∗ is exactly sparse, i.e., kx∗ k0 ≤ s,
and assumption A1 holds. Let
m1 =

λ
72c2 s(t + log n)
, γ1 = √ , η = 21/2
λ2
3 2s

where c is the constant in Theorem 1. Then, with a probability at least 1 − Ke1−t , we have
bk2 ≤ δK+1 =
kxK+1 − x

1

2K/2

.

Furthermore, with a probability at least 1 − (e + 1)(K −
PK
1)e−t , the number of calls to the Oracle
k=1 |Bk | is
bounded by


√
min 2(K − 1)t + (5 n2K/2 + 1)m1 , m1 2K .

Efficient Algorithms for Robust One-bit Compressive Sensing

The above theorem immediately implies the following
corollary.
Corollary 1. Under the condition in Theorem 2, the recovery rate of the adaptive algorithm is
!!
r
√
s log n s n log n
,
,
O min
m
m
PK

where m =
k=1 |Bk | is the total number of measurements. And thus the sample complexity is



√
s log n s n log n
O min
,
.
ǫ2
ǫ
Remark As a result, the sample complexity of the adaptive algorithm is smaller
than that of the passive algorithm,
√
when ǫ ≤ O(1/ n). Thus, if we want to find a highprecision solution, the adaptive algorithm is preferred.

the sample complexity with respect to the recovery error,
while the existing methods improve the sample complexity respect to the signal-to-noise level (Malloy & Nowak,
2012) or the dynamic range (Gupta et al., 2010).

4. Analysis
We here present the proofs of main theorems. The omitted
proofs are provided in the supplementary material.
4.1. Proof of Theorem 1
The analysis is fundamentally built upon the following ob1
servation between m
U y and λx∗ .
Lemma 3. With a probability at least 1 − e1−t , we have


1

 U y − λx∗ 
m


∞

≤c

r

t + log n (8) 1
= γ
m
2

A similar result can be obtained when x∗ is approximately
sparse.

for some constant c > 0.

Theorem√ 3. Suppose x∗ is approximately sparse, i.e.,
kxk1 ≤ s, and assumption A1 holds. Let

b is the optimal solution, we have
Since x

72c2 s(t + log n)
λ
m1 =
, γ1 = √ , η = 21/4
λ2
3 2s
where c is the constant in Theorem 1. Then, with a probability at least 1 − Ke1−t , we have
bk2 ≤ δK+1 =
kxK+1 − x

1
2K/4

.

Furthermore, with a probability at least 1 − (e + 1)(K −
PK
1)e−t , the number of calls to the Oracle
k=1 |Bk | is
bounded by


√
min 2(K − 1)t + (3 n23K/4 + 1)m1 , m1 2K .
Corollary 2. Under the condition in Theorem 3, the recovery rate of the adaptive algorithm is
!!
r √
r
n log n
3 s
4 s log n
,
,
O min
m
m
PK

where m =
k=1 |Bk | is the total number of measurements. And thus the sample complexity is



√
s log n s n log n
O min
,
.
ǫ4
ǫ3
Again, this sample complexity is√better than that of the passive algorithm, when ǫ ≤ O(1/ n).
Remark It is interesting to compare our adaptive algorithm
with the previous adaptive algorithms in compressive sensing. The main difference is that our algorithm improves

−

Thus,

1 ⊤
1
b U y + γkb
x
xk1 ≤ − x⊤
U y + γkx∗ k1 .
m
m ∗

γkx∗ k1


Uy
b,
+ γkb
xk1
≥ x∗ − x
m


Uy
b, λx∗ i + x∗ − x
b,
− λx∗ + γkb
=hx∗ − x
xk1
m


Uy


b⊤ x∗ ) − kx∗ − x
bk1 
xk1 .
≥λ(1 − x
 m − λx∗  + γkb
∞

Based on Lemma 3, we have

b⊤ x∗ ) + γkb
λ(1 − x
xk1 ≤ γkx∗ k1 +

γ
bk1 . (12)
kx∗ − x
2

First, we consider the case that x∗ is exactly sparse, i.e.,
kx∗ k0 ≤ s. Let S be the support set of x∗ and S = [n] \ S
be the complement set. We denote by PS (x) the sub-vector
of x indexed by the set S, that is
PS (x) = [xi : i ∈ S]⊤ .
From (12), we have
b⊤ x∗ ) + γkPS (b
λ(1 − x
x)k1 + γkPS (b
x)k1
γ
γ
b)k1 + kPS (b
x)k1 .
≤γkx∗ k1 + kPS (x∗ − x
2
2

Efficient Algorithms for Robust One-bit Compressive Sensing

Thus,
γ
kP (b
x)k1
2 S
γ
b)k1
≤γkx∗ k1 − γkPS (b
x)k1 + kPS (x∗ − x
2
3γ p
3γ
b)k1 ≤
b)k2 .
≤ kPS (x∗ − x
kx∗ k0 kPS (x∗ − x
2
2
b ⊤ x∗ ) +
λ(1 − x

Then, we have

bk22 ≤ 2(1 − x
b ⊤ x∗ ) ≤
kx∗ − x

which implies

bk2 ≤
kx∗ − x

3γ p
bk2
kx∗ k0 kx∗ − x
λ

3γ p
kx∗ k0 .
λ

Next, we consider
√ the case that x∗ is approximately sparse,
i.e., kx∗ k1 ≤ s. From (12), we have
b ⊤ x∗ )
λ(1 − x

γ
3γ
γ
xk1 ≤
kx∗ k1 .
≤γkx∗ k1 − γkb
xk1 + kx∗ k1 + kb
2
2
2
Thus,
bk22 ≤ 2(1 − x
b ⊤ x∗ ) ≤
kx∗ − x

3γ
kx∗ k1 .
λ

4.2. Proof of Theorem 2

From the updating rule in our algorithm, it is easy to check
that
r
t + log n
1
, ∀k.
δk = (k−1)/2 , γk = 2c
mk
2
So, the condition (8) in Theorem 1 is satisfied at each state.
We first consider the first stage. Since kx1 k = 1 and
δ1 = 1, the definitions in (11) ensures B1 = G1 . And
thus we will query the Oracle to obtain the one-bit measurements for all the elements in G1 . As a result, we can apply
Theorem 1 to bound the recovery error of x2 . Specifically,
with a probability at least 1 − e1−t , we have
1
3γ1 √
bk2 ≤
s = √ = δ2 .
kx2 − x
λ
2

Thus, the condition in (10) is true for k = 2. Based on
Lemma 2, we can apply Theorem 1 again and get
bk2 ≤
kx3 − x

3γ1 √ 1
δ2
3γ2 √
s=
s √ = √ = δ3 .
λ
λ
2
2

Repeating the above argument for all the stages, we obtain
the first part of the theorem.

Now, we consider bounding the size of Bk . Since B1 = G1 ,
we have
|B1 | = m1 .

For k = 2, we have with a probability at least 1−e1−t , (10)
holds. We condition on the event that (10) is true, and proceed by analyzing the distribution of x2⊤ u2i /ku2i k2 appears
in the definition of B2 . Since ui2 is a Gaussian random vector, it is known that u2i /ku2i k2 is uniformly distributed on
the n − 1-sphere (Muller, 1959). Additionally, the distribu2
2
tion of x⊤
2 ui /kui k2 is characterized by (Cho, 2009)
(
Γ( n
2)
2 n−3
2 ,
for − 1 < z < 1,
n−1 √ (1 − z )
Γ( 2 ) π
f (z) =
0,
otherwise.
where Γ(·) is the Gamma function. Based on the bound for
the ratio of two gamma functions (Luo & Qi, 2012, Equation 2.18), we have
r
r

Γ n2
n 1
n 1
≤
− ≤
− .
2
4
2
2
Γ n−1
2

As a result, we have
r



 ⊤ u2i 
√
n 1


√ ≤ nδ2 .
≤ δ2 ≤ 2δ2 f (0) ≤ 2δ2
Pr x2
ku2i k2 
2 π

Thus, for each vector √
u2i , the probability that it belongs
to Bk is smaller than nδ2 . According to the Chernoff bound (Angluin & Valiant, 1979) provided in the supplementary material, we have with a probability at least
1 − e−t ,
√
|B2 | ≤ 2E[|B2 ] + 2t ≤ 2 nδ2 m2 + 2t.

Factoring in the conditioned event, which happens with a
probability
at least 1 − e1−t , overall, we get that |B2 | <
√
2 nδ2 + 2t with a probability at least 1 − (e + 1)e−t .
Repeating the above argument for all the stages, we have
with a probability at least 1 − (e + 1)(K − 1)e−t ,
√
|Bk | ≤ 2 nδk mk + 2t, ∀k = 2, . . . , K.
Thus, the total number of calls to the Oracle is upper
bounded by
K
√ X
m1 + 2(K − 1)t + 2 n
δ k mk
k=2

K
X
√
2(k−1)/2
=m1 + 2(K − 1)t + 2 nm1
k=2

√
≤2(K − 1)t + (5 n2K/2 + 1)m1 .

On the other hand, we know that the size of Bk must be
smaller than mk , and thus we also have
K
X

k=1

|Bk | ≤

K
X

k=1

m k = m1

K
X

k=1

2k−1 ≤ m1 2K .

Efficient Algorithms for Robust One-bit Compressive Sensing

4.3. Proof of Lemma 3
We need the following lemma on the expectation of ui yi .
Lemma 4.

Table 2. Running time of each algorithm, when s=10, n = 1000,
and m = 1000. For BIHT and BIHT-ℓ2 , there is no formal stoping criterion, and we report the running time after 100 iterations.

E [ui yi ] = λx∗ , i = 1, . . . n.
1
U y − λx∗ , that is,
Consider the j-th element of m


m
1
1 X j
u yi − λxj∗ ,
U y − λx∗ =
m
m i=1 i
j

where uji and xj∗ are the j-th element of ui and x∗ , respectively.
Lemma 4 implies E[uij yi ] = λxj∗ . From (Vershynin, 2012,
Remark 5.18), we have




 j 

 j
(13)
ui yi − λxj∗  ≤ 2 ui yi 
ψ2

ψ2

where

kXkψ2 = sup p−1/2 (E|X|p )1/p

T IME ( S )

PASSIVE

BIHT

BIHT-ℓ2

C ONVEX

1.1e−3

1.7

1.7

0.72

Experimental Setup We generate the target vector x∗ ∈
Rn by drawing its nonzero elements from the standard
Gaussian distribution, and then normalize it to have unit
length. The locations of the s nonzero elements of x∗
are randomly selected. The elements in the matrix matrix
U ∈ Rn×m are also drawn from the standard Gaussian distribution. To generate noisy measurements, we choose the
observation model in (9), where the sign of u⊤
i x∗ is flipped
with probability p = 0.1. For each setting of m, n, and s,
we repeat the recovery experiment for 100 trials, and report
the average recovery error.

p≥1

is the sub-gaussian norm of random variable X (Vershynin,
2012, Definition 5.7). Since yi ∈ {±1}, we have
 


 j
 j 
(14)
ui yi  = ui  ≤ c
ψ2

ψ2

where c > 0 is an absolute constant, and the last inequality
follows from uij ∼ N (0, 1) and (Vershynin, 2012, Example
5.8).

We will use the Hoeffding-type inequality for sub-gaussian
random variables given below.
Lemma 5. (Vershynin, 2012, Proposition 5.10) Let
X1 , . . . , XN be independent centered sub-gaussian random variables, and let K = maxi kXi kψ2 . Then, for any
α = [α1 , . . . , αN ]⊤ ∈ RN and every t ≥ 0, we have


!


N

X
ct2


αi Xi  ≥ t ≤ exp 1 − 2 2
Pr 


K |α|2
i=1
where c > 0 is an absolute constant.

Combining Lemma 5 with (13) and (14), we have with a
probability at least 1 − e1−t ,


r
m

1 X
t

j
j
ui yi − λx∗  ≤ c


m
m
i=1

for some constant c > 0. We complete the proof by taking
the union bound over j = 1, . . . , n.

5. Experiments
In this section, we perform the recovery experiment to verify our theoretical claims. Due to space limitations, we only
provide results for the exactly sparse vectors.

The Passive Algorithm To apply our passive algorithm,
we need to determine the regularization
q parameter γ. From

n
(8), we observe that γ can be set as C log
m for some constant C. Fig. 1 shows how the recovery error of the passive
algorithm varies with respect to the value of C. From the
result, we observe
qthat the best value of C is around 1, and

thus we set γ =

log n
m

in the following experiments.

We compare our passive algorithm (Passive) with the following three algorithms.
• Convex: the provable recovery algorithm proposed in
(Plan & Vershynin, 2013b), which solves the convex
optimization problem in (3);2
• BIHT and BIHT-ℓ2 : two heuristic algorithms developed in (Jacques et al., 2013).3

Fig. 2 plots the recovery error versus the number of measurements m for each algorithm, when s = 10 and n =
1000. Note that in the conventional compressive sensing, it is not interesting to acquire more measurements that
the dimensionality. But in one-bit compressive sensing, it
becomes very practical to set m > n, since the one-bit
measurements can be taken at extremely high rates. From
Fig. 2, we observe that Passive and Convex outperform the
other two algorithms significantly. The performance of BIHT is very bad, that is because it is very sensitive to noise
2

We use the CVX package to solve this optimization problem (Boyd & Vandenberghe, 2004; Grant & Boyd, 2008; 2013).
3
A matlab implementation can be downloaded from
http://perso.uclouvain.be/laurent.jacques/
index.php/Main/BIHTDemo.

Efficient Algorithms for Robust One-bit Compressive Sensing
1.2

Recovery Error

Recovery Error

0.8
0.6
0.4

0.8
0.6
0.4
0.2

0.2
0.5

1

1.5

2
C

2.5

3

3.5

4

log n
,
m

2000

4000

6000

8000

10000

m

Figure 1. Theq
recovery error of the passive algorithm versus to C,

when γ = C

BIHT
BIHT-ℓ2
Convex
Passive

1

n=1000,m=1000
n=1000, m=10000
n=10000, m=1000
n=10000, m=10000

1

and s = 10.

Figure 2. The recovery error of each algorithm versus the number
of measurements m, when s = 10 and n = 1000.
7

BIHT
BIHT-ℓ2
Convex
Passive

1

x 10

Adaptive
Passive

8

0.8
6
0.6

m

Recovery Error

10

4
0.4
2
0.2

0
2000

4000

6000

8000

10000

n

−3

10

−2

10
Recovery Error

−1

10

Figure 3. The recovery error of each algorithm versus the dimen- Figure 4. The number of measurements required by the passive and
sionality n, when s = 10 and m = 1000.
adaptive algorithms versus the recovery error, when s = 10 and
n = 1000.

in the one-bit measurements (Jacques et al., 2013).

6. Conclusion and Future Work

We also examine the relation between the recovery error
and the dimensionality n in Fig. 3, where s = 10 and
m = 1000. We observe that the recovery error increases
very slowly with respect to n, which is consistent with the
conclusion that the recovery error only has a logarithmic
dependence on n. Finally, we would like to emphasize that
although the recovery error of Convex is similar to Passive,
its computational cost is significantly higher, and the running time of those algorithms can be found in Table 2.

In this paper, we develop two efficient algorithms for onebit compressive sensing. Compared with the existing methods, the proposed algorithms have several important advantages: they can recover both the exactly sparse and approximately sparse vectors; they are robust to the noisy measurements; they are computationally efficient, and they have
lower sample complexities in certain scenarios.

The Adaptive Algorithm In Fig. 4, we show the number of measurements required by our passive and adaptive
algorithms versus the recovery error, when s = 10 and
n = 1000. As can be seen, when the recovery error is
small, the adaptive algorithm is able to reduce the number
of measurements dramatically, which validates the claim in
Theorem 2.

Currently, the adaptive algorithm relies on a strong assumption that allows us to generate the one-bit measurement
based on the sign of x⊤ u. In the future, we will investigate how to alleviate this assumption for other observation
models by exploring more advanced techniques in active
learning.

Acknowledgments
This work is partially supported
(N000141210431) and NSF (IIS-1251031).

by

ONR

Efficient Algorithms for Robust One-bit Compressive Sensing

References
Angluin, D. and Valiant, L.G. Fast probabilistic algorithms for
hamiltonian circuits and matchings. Journal of Computer and
System Sciences, 18(2):155–193, 1979.
Anthony, Martin and Bartlett, Peter L. Neural Network Learning:
Theoretical Foundations. Cambridge University Press, 1999.
Boufounos, Petros T. Reconstruction of sparse signals from distorted randomized measurements. In Proceedings of the IEEE
International Conference on Acoustics Speech and Signal Processing, pp. 3998–4001, 2010.
Boufounos, Petros T. and Baraniuk, Richard G. 1-bit compressive
sensing. In Proceedings of the 42nd Annual Conference on
Information Sciences and Systems, pp. 16–21, 2008.
Boyd, Stephen and Vandenberghe, Lieven. Convex Optimization.
Cambridge University Press, 2004.
Candes, Emmanuel J. and Tao, Terence. Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE Transactions on Information Theory, 52(12):5406–
5425, 2006.
Cho, Eungchun. Inner product of random vectors. International Journal of Pure and Applied Mathematics, 56(2):217–221,
2009.
Dasgupta, Sanjoy. Active learning theory. Encyclopedia of Machine Learning, pp. 14–19, 2011.
Donoho, David L. De-noising by soft-thresholding. IEEE Transactions on Information Theory, 41(3):613–627, 1995.
Donoho, David L. Compressed sensing. IEEE Transactions on
Information Theory, 52(4):1289–1306, 2006.
Duchi, John and Singer, Yoram. Efficient online and batch learning using forward backward splitting. Journal of Machine
Learning Research, 10:2899–2934, 2009.
Gopi, Sivakant, Netrapalli, Praneeth, Jain, Prateek, and Nori, Aditya. One-bit compressed sensing: Provable support and vector recovery. In Proceedings of the 30th International Conference on Machine Learning, pp. 154–162, 2013.
Grant, Michael and Boyd, Stephen. Graph implementations for
nonsmooth convex programs. In Recent Advances in Learning
and Control, pp. 95–110, 2008.
Grant, Michael and Boyd, Stephen. CVX: Matlab software for
disciplined convex programming, version 2.0 beta. http://
cvxr.com/cvx, 2013.
Gupta, Ankit, Nowak, Robert, and Recht, Benjamin. Sample
complexity for 1-bit compressed sensing and sparse classification. In Proceedings of the IEEE International Symposium
on Information Theory, pp. 1553–1557, 2010.
Haupt, Jarvis and Baraniuk, Richard. Robust support recovery using sparse compressive sensing matrices. In Proceedings of the
45th Annual Conference on Information Sciences and Systems,
pp. 1–6, 2011.
Haupt, Jarvis, Nowak, Robert, and Castro, Rui. Adaptive sensing
for sparse signal recovery. In Proceedings of the IEEE 13th
Digital Signal Processing Workshop and 5th IEEE Signal Processing Education Workshop, pp. 702–707, 2009a.

Haupt, Jarvis D., Baraniuk, Richard G., Castro, Rui M., and
Nowak, Robert D. Compressive distilled sensing: Sparse recovery using adaptivity in compressive measurements. In Proceedings of the 43rd Asilomar Conference on Signals, Systems
and Computers, pp. 1551–1555, 2009b.
Hazan, Elad and Kale, Satyen. Beyond the regret minimization
barrier: an optimal algorithm for stochastic strongly-convex
optimization. In Proceedings of the 24th Annual Conference
on Learning Theory, pp. 421–436, 2011.
Jacques, Laurent, Laska, Jason N., Boufounos, Petros T., and
Baraniuk, Richard G. Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors. IEEE Transactions
on Information Theory, 59(4):2082–2102, 2013.
Laska, Jason N. and Baraniuk, Richard G. Regime change: Bitdepth versus measurement-rate in compressive sensing. IEEE
Transactions on Signal Processing, 60(7):3496–3505, 2012.
Laska, Jason N., Wen, Zaiwen, Yin, Wotao, and Baraniuk,
Richard G. Trust, but verify: Fast and accurate signal recovery
from 1-bit compressive measurements. IEEE Transactions on
Signal Processing, 59(11):5289–5301, 2011.
Luo, Qiu-Ming and Qi, Feng. Bounds for the ratio of two gamma
functions—from wendel’s and related inequalities to logarithmically completely monotonic functions. Banach Journal of
Mathematical Analysis, 6(2):132–158, 2012.
Malloy, Matthew L. and Nowak, Robert D. Near-optimal adaptive
compressed sensing. In Proceedings of the 46th Asilomar Conference on Signals, Systems and Computers, pp. 1935–1939,
2012.
Movahed, Amin, Panahi, Ashkan, and Durisi, Giuseppe. A robust
rfpi-based 1-bit compressive sensing reconstruction algorithm.
In Proceedings of the IEEE Information Theory Workshop, pp.
567–571, 2012.
Muller, Mervin E. A note on a method for generating points uniformly on n-dimensional spheres. Communications of the
ACM, 2(4):19–20, 1959.
Plan, Yaniv and Vershynin, Roman. One-bit compressed sensing
by linear programming. Communications on Pure and Applied
Mathematics, 66(8):1275–1297, 2013a.
Plan, Yaniv and Vershynin, Roman. Robust 1-bit compressed
sensing and sparse logistic regression: A convex programming
approach. IEEE Transactions on Information Theory, 59(1):
482–494, 2013b.
Raskutti, Garvesh, Wainwright, Martin J., and Yu, Bin. Minimax
rates of estimation for high-dimensional linear regression over
ℓq -balls. IEEE Transactions on Information Theory, 57(10):
6976–6994, 2011.
Vershynin, Roman. Introduction to the non-asymptotic analysis of
random matrices. In Compressed Sensing, Theory and Applications, chapter 5, pp. 210–268. Cambridge University Press,
2012.
Yan, Ming, Yang, Yi, and Osher, Stanley. Robust 1-bit compressive sensing using adaptive outlier pursuit. IEEE Transactions
on Signal Processing, 60(7):3868–3875, 2012.
Yang, Liu and Hanneke, Steve. Activized learning with uniform
classification noise. In Proceedings of the 30th International
Conference on Machine Learning (ICML), 2013.

