Submodularity in Data Subset Selection and Active Learning

Kai Wei
Rishabh Iyer
Jeff Bilmes
University of Washington, Seattle, WA 98195, USA

Abstract
We study the problem of selecting a subset of big
data to train a classifier while incurring minimal
performance loss. We show the connection of
submodularity to the data likelihood functions for
Naı̈ve Bayes (NB) and Nearest Neighbor (NN)
classifiers, and formulate the data subset selection
problems for these classifiers as constrained
submodular maximization. Furthermore, we
apply this framework to active learning and
propose a novel scheme called filtered active
submodular selection (FASS), where we combine the uncertainty sampling method with a
submodular data subset selection framework. We
extensively evaluate the proposed framework on
text categorization and handwritten digit recognition tasks with four different classifiers, including
deep neural network (DNN) based classifiers.
Empirical results indicate that the proposed
framework yields significant improvement over
the state-of-the-art algorithms on all classifiers.

1 Introduction
A relatively recent turn of events is that data sets for training machine learning systems are getting extremely large —
the unprecedented amount of data at our disposal on which
we can train our systems is one of the pillars of machine
learning’s recent successes. Big data has its own problems,
however, namely that it is computationally demanding, resource hungry, and often redundant. One solution to this
latter problem is to carefully choose a subset of the data
so as to minimize any significant loss in performance. In
the context of machine learning training data subset selection, we call this problem supervised data subset selection
(when training data labels are available), or unsupervised
data subset selection (when not).
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

KAIWEI @ U . WASHINGTON . EDU
RKIYER @ U . WASHINGTON . EDU
BILMES @ U . WASHINGTON . EDU

One recent class of methods treats the problem as submodular maximization, where appropriate submodular functions
are chosen as the surrogate model to measure the utility
of each subset for an underlying task (Shinohara, 2014;
Zheng et al., 2014; Hoi et al., 2006; Shamaiah et al., 2010;
Prasad et al., 2014; Krause et al., 2008; Das & Kempe,
2011; Wei et al., 2014b; Kempe et al., 2003; Gabillon et al.,
2013; Chen & Krause, 2013; Reed & Ghahramani, 2013;
Singla et al., 2014; Liu et al., 2013; Iyer et al., 2013). While
existing work typically shows very good performance,
in some cases a mathematical connection can be made
between the true objective and a submodular model, a core
goal we wish to further advance in the present paper.
Submodular functions, traditionally studied in mathematics,
economics, and operations research, are defined as follows:
f : 2V → R, returning a real value for any subset S ⊆ V ,
is submodular if it satisfies f (A) + f (B) ≥ f (A ∩ B) +
f (A∪B), ∀A, B ⊆ V . An equivalent definition is diminishing returns: f (j|S) ≥ f (j|T ), ∀S ⊆ T, j ∈ V \ T , where
f (j|S) , f (j ∪ S) − f (S) is the marginal gain of adding an
item j to a set S. A function f is monotone non-decreasing
if f (j|S) ≥ 0, ∀S ⊆ V, j ∈ V \ S. We say that f is normalized if f (∅) = 0. Submodular functions naturally model
notions of information, diversity, and coverage in many applications. Moreover, they can be optimized efficiently by
extremely simple algorithms. For example, a greedy algorithm (Nemhauser et al., 1978) ensures that the cardinality
constrained submodular maximization problem can be approximated up to a factor of 1 − 1/e. Submodularity can be
further exploited to accelerate the greedy implementation
leading to an algorithm often called lazy greedy (Minoux,
1978) with almost linear time complexity.
Our contributions: In this paper, we study submodular
functions in connection to data subset selection. We first
propose an approach to supervised data subset selection by
connecting submodularity to likelihood functions of classifiers. Specifically, we express the utility set function for two
simple classes of classifiers, the Naı̈ve Bayes (NB) classifier
and the Nearest Neighbor (NN) classifier utilizing submodularity. We identify two classes of submodular functions,

Submodularity in Data Subset Selection and Active Learning

Naı̈ve Bayes submodular and Nearest Neighbor submodular, that naturally model maximum likelihood estimates
over data subsets for both NB and NN classifiers. Data subset selection is then performed as submodular maximization.
The Naı̈ve Bayes submodular function is a special case of
the “feature based” submodular functions that have been
recently defined and used in the literature (Wei et al., 2014b;
Kirchhoff & Bilmes, 2014), while the Nearest Neighbor
submodular function generalizes the well-known class of
facility location function (Mirchandani & Francis, 1990)
that have also been previously successfully used for subset selection problems (Iyer & Bilmes, 2013; Zheng et al.,
2014).
Supervised data subset selection for the NN classifier, in
particular, has great practical importance, since the NN classifier is non-parametric — i.e., the classifier must essentially
memorize, and allocate storage for, the entire training set.
The complexity of classifying one sample is dependent on
the training set size, which can be expensive for large-scale
applications. Our supervised data subset selection strategies
reduce the training set size, and when the submodular function used to perform the subset selection is matched with the
NN classifier, there is little performance loss even though
the NN classifier has significantly less data to memorize.
We then extend the data subset selection problem to an
active learning setting. We make a distinction between
two extreme forms of active learning (Guillory & Bilmes,
2011): 1) batch active learning, where there is one round of
data selection and the data points are chosen to be labeled
without any knowledge of the resulting labels that will
be returned, and 2) adaptive active learning, where there
are many rounds of data selection, each of which selects
one data point whose label may be used to select the data
point at future rounds. A hybrid multistage scheme we call
mini-batch adaptive active learning is where in each round
a mini-batch of data points are selected to be labeled, and
that may inform the determination of future mini-batches.
We propose a novel multi-stage scheme we call filtered active submodular selection (FASS) for the mini-batch adaptive active learning. At every round, as more labeled data
becomes available, FASS uses an improved approximation to supervised data subset selection. We show how our
framework naturally combines the notions of sample informativeness and, via submodularity, representativeness. We
also show how our method is scalable relative to existing
active learning techniques that attempt to combine these two
notions. We then empirically demonstrate that our methods
(both the purely supervised selection methods, and FASS)
outperform existing baselines, including in our results a
deep neural network (DNN) case.
Related work: There are three fundamental problems
related to data subset selection. The first is supervised data

subset selection (Wei et al., 2014b; 2013; Shinohara, 2014;
Tsang et al., 2005), where the selection algorithm has access
to the labels of the training data set. The second problem
is unsupervised subset selection, where the algorithm does
not use the labels for selecting data (Wei et al., 2014c;
Har-Peled & Mazumdar, 2004). The third is active learning,
where label queries are made on subsets of data (Settles,
2010; Lewis & Gale, 1994).
A number of authors have studied these versions of data subset selection, and have observed that submodular functions
nicely fit this problem. In particular, (Wei et al., 2014c;
Shinohara, 2014) study the subset selection of speech data
for training speech recognition systems in both the supervised and unsupervised setting, and model the utility of a
speech training data as the coverage of a set of speech units
through submodular functions. Similarly, (Shamaiah et al.,
2010) model the utility a subset of sensors as the reduction
on the estimation error covariance matrix and show that this
utility function is monotone submodular. (Das & Kempe,
2011) analyze the objective of feature subset selection for
linear regression and show that it is often approximately
submodular. (Hoi et al., 2006) investigates the batch active
learning problem for logistic regression, and connect this
to submodular optimization. (Guillory & Bilmes, 2011)
study the role of submodular functions in subset selection
for very general simultaneously active and semi-supervised
learning algorithms. Another thread of work (Guillory &
Bilmes, 2010; Golovin & Krause, 2010) provides a link
between submodularity and active learning through notions
of interactive and adaptive submodularity. In much of
this related work, the submodular function is heuristically
chosen to model the classifier accuracy. This paper, on the
other hand, attempts to make a formal connection between
submodular functions, and accuracy functions for data
subset selection and active learning. The closest work to
this paradigm is the work of (Guillory & Bilmes, 2010;
Golovin & Krause, 2010; Cuong et al., 2010) which models
version space reduction via adaptive submodular functions.
While this focuses on the fully adaptive setting, (Chen &
Krause, 2013) extend this, to what we call the mini-batch
adaptive setting. In this paper, we show that the data
likelihood functions in the supervised setting are closely
related to submodularity. Moreover, we show how we can
extend this to a mini-batch adaptive active learning setting.
A number of papers on the other hand, do not use submodularity for data selection. For example, another common
approach for training data summarization in the supervised
and unsupervised setting is to use the concept of a coreset (Agarwal et al., 2005). The paradigm of coresets aims to
efficiently approximate various geometric extent measures
over a large set of data instances via a small subset. Many
machine learning problems including SVMs, k-means clustering, k-median clustering, and Gaussian mixtures (Tsang

Submodularity in Data Subset Selection and Active Learning

et al., 2005; Har-Peled & Mazumdar, 2004; Feldman et al.,
2011) can be approximately solved on a coreset.
Other algorithms having an active learning flavor include selecting the most informative set of items at every round (Settles, 2010), where the informativeness refers to utility of
the items from the classifiers’ point of view. We measure
the informativeness, by the classifier’s uncertainty (in the
case of uncertainty sampling) (Lewis & Gale, 1994), or the
variance (in the case of Query by Committee) (Seung et al.,
1992). It is interesting to note that some of these methods,
e.g., uncertainty sampling, are special cases of adaptive
submodular maximization (Cuong et al., 2010). These techniques do not ideally capture the representativeness of the
samples, a problem further aggravated in the mini-batch
adaptive setting. By representativeness, we mean how well
a set of items covers the entire training set. This aspect is
naturally modeled by density based methods (Nguyen &
Smeulders, 2004). In order to obtain the benefits of both
classes of algorithms, several papers have combined both
notions (informativeness and representativeness) in a single
objective (Xu et al., 2003; Huang et al., 2010). In this paper, we show that we can naturally incorporate both these
notions in our multistage active learning algorithm FASS.

2 Naı̈ve Bayes Classifier
We first consider the class of Naı̈ve Bayes classification
problems. Let V = {(xi , y i )}m
i=1 be a set of training samples, where xi ∈ X d is a d-dimensional feature vector, and
each feature takes a value from the finite set X ; each sample’s label y i ∈ Y takes a value from the finite set Y of
classes. The ground set V may be partitioned as V = V 1 ∪
V 2 ∪· · ·∪V |Y| , where V y is the set of all samples in V with
class label y. We write the jth dimension of a feature vector
x as xj ∈ X . We denote the maximum likelihood (ML) estimate of the parameters θ of a Naı̈ve Bayes model, given a set
of training samples S ⊆ V , as θ(S), and also use θxj |y =
p(xj |y) and θy = p(y). For simplicity, we first assume no
smoothing occurs during estimation but this will be considered later. The ML parameter function θ(S) can be given
mx

,y (S)

m (S)

y
as follows: θxj |y (S) = mjy (S) , θy (S) = |S|
. where
P
i
i
mxj ,y (S) = i∈S 1{xj = xj ∧ y = y} and my (S) =
P
i
i∈S 1{y = y}. By definition, mxj ,y (S) counts the number of samples in S whose class label is y ∈ Y and whose
jth dimension feature takes value xj ∈ X . Similarly, my (S)
counts the number of samples in S whose class label is
y ∈ Y. Both mxj ,y (S) and my (S) are modular
set funcP
tions, i.e., for P
any S ⊆ V , mxj ,y (S) =
s∈S mxj ,y (s)
and my (S) = s∈S my (s). Given the parameter function
θ(S), we introduce the notion of data log-likelihood set
function `NB : 2V → R that maps from each set S ⊆ V of
training samples to a log P
likelihood evaluated on the whole
i i
data set V : `NB (S) =
i∈V log p(x , y ; θ(S)), where

`NB (S) =

d X X
X

mxj ,y (V ) log(mxj ,y (S))

j=1 xj ∈X y∈Y

{z

|

term 1: fNB (S)

− (d − 1)

X
y∈Y

|

}

my (V ) log(my (S)) − |V | log |S| .
| {z }
term 3
{z
}
term 2

Figure 1. The NB likelihood as a function of S.

p(xi , y i ; θ(S)) is the likelihood of the sample i parameterized by θ(S). `NB (S) acts as a utility set function for
training a NB classifier. Under the Naı̈ve Bayes assumption, we can express `NB (S) as shown in Figure 1. Since
mxj ,y (V ), my (V ), and |V | are all independent of S, they
can be treated as constants in `NB (S). Then the first, second,
and third terms of `NB (S) are in the form of a sum of concave over modular functions, hence they are all monotone
submodular (Stobbe & Krause, 2010; Lin & Bilmes, 2011).
As a result, `NB (S) is in the form of difference of submodular functions, and the underlying optimization problem
becomes a difference of submodular (DS) optimization:
max `NB (S).
(1)
|S|=k

While there are scalable heuristics that work well in
practice to minimize a difference of submodular functions (Narasimhan & Bilmes, 2005; Iyer & Bilmes,
2012; Iyer et al., 2014), these techniques lack worst-case
guarantees since the underlying problem is hard to optimize.
Fortunately, the second and the third term of `NB (S)
become constants when we enforce a set of inconsequential
constraints. We call the first term (the only active term
in a transformed optimization problem) the Naı̈ve Bayes
submodular function:
d X X
X
mxj ,y (V ) log mxj ,y (S). (2)
fNB (S) =
j=1 xj ∈X y∈Y

Given the equality constraint |S| = k, the third term in
`NB (S) becomes a constant in Problem 1. Furthermore, we
make an assumption that the selected set should be balanced,
which makes the second term also a constant. In particular,
we say that a set S is balanced if S maintains the same
distribution over the class labels as the whole data set. A
set S of size k is balanced if the proportion of each class
label in the set yS is the same as the whole data set V , i.e.,
|S∩V y | = k |V|V || for any y ∈ Y for some k. We assume that
y

k is chosen such that k |V|V || is an integer for all y ∈ Y, and if
y

not then we round k |V|V || to the closest integer. With balance
enforced and |S| = k, we have that my (S) = |S ∩ V y | =
y
k |V|V || for all y ∈ Y. Therefore, term 2 of `NB (S) also becomes a constant. Let M(V, I) be a partition matroid using
the partition {V y }y∈Y , where the set of bases B(M) is dey
fined as B(M) = {S ⊆ V : |S ∩ V y | = k |V|V || , ∀y ∈ Y}.
Thus, a set S of size k being balanced is equivalent to S

Submodularity in Data Subset Selection and Active Learning

being a base of the matroid M, i.e., S ∈ B(M). Since the
second and the third term of `NB (S) are constants given the
constraint S ∈ B(M), the above is equivalent to:
max fNB (S).
(3)
S∈B(M)

We may efficiently, scalably, and approximately solve this
problem using the lazy greedy algorithm (Fisher et al.,
1978). This formulation asks for a small training data subset
S such that the likelihood of the ML parameters θ(S) is
large on the entire data set V , and the following Theorem
offers perspective in terms of the KL-divergence.
Theorem 1. Let DKL (p(x, y; θ(V ))||p(x, y; θ(S))) ,
P
P
p(x,y;θ(V ))
x∈X d
y∈Y p(x, y; θ(V )) log p(x,y;θ(S)) be the KLdivergence between p(x, y; θ(V )) and p(x, y; θ(S)), where
p(x, y; θ(S)) is the maximum likelihood estimate of the joint
distribution given a data set S. Under the Naı̈ve Bayes assumption, Problem 1 is equivalent to
min DKL (p(x, y; θ(V ))||p(x, y; θ(S))).
(4)
|S|=k

The proofs of results in this paper are in (Wei et al., 2015).
We next point out connections between the Naı̈ve Bayes
submodular function fNB and the class of feature-based
submodular functions ffea defined in (Wei et al., 2014b)
and that have effectively been applied in various subset
selection tasks (Shinohara, 2014; Tschiatschek et al., 2014;
Wei et al., 2014c;b; Kirchhoff
P & Bilmes, 2014). These
are defined as ffea (S) =
u∈U wu g(cu (S)), where g is
a concave function, U is a set of “features”, {wu }u∈U
is a set of non-negative
weights for each feature u ∈ U,
P
and cu (S) =
s∈S cu (s) is a non-negative modular
score for feature u ∈ U in S, with cu (s) measuring the
degree to which item s “possesses” feature u. Defining U as the set of all possible input-label pairs, i.e.,
U = {(xj , y)|xj ∈ X , y ∈ Y, j = 1, . . . , d}, the weight
for each feature as wu = mu (V ), the concave function as
g(x) = log x, and the modular score as cu (S) = mu (S),
fNB is then an instance of a feature-based function.
Maximizing fNB chooses data that has diverse coverage in
the set of features U where the desired coverage for each
feature u ∈ U is controlled by its weight wu = mu (V ).
Laplace smoothing: Without any smoothing, it may be
that fNB is undefined at S = ∅. Smoothing not only fixes
this issue, it is naturally incorporated into the submodular
framework. Given a Laplace smoothing parameter α > 0,
for a set S ⊆ V , the Laplace smoothed ML estimated parameters become θxαj |y (S) =
my (S)+α
|S|+α|Y| .

mxj ,y (S)+α
my (S)+α|X |

and θyα (S) =

We may formulate this similar to Problem 3,
where the objective is slightly modified from before. First,
define a slightly expanded ground set V 0 = V ∪ {v 0 } where
v 0 is a pseudo-sample that has the property mxj ,y (v 0 ) =
α, ∀xj ∈ X , y ∈ Y, j = 1, . . . , d. We next define a func0
0
: 2V → R+ as:
tion fNB(α)

0
fNB(α)
(S) =

d P P
P

mxj ,y (V ) log(mxj ,y (S))

(5)

j=1xj ∈X y∈Y

0
For any S ⊆ V , fNB(α)
(S ∪ {v 0 }) represents the Laplacesmoothed ML objective. From this, we obtain a normalized
monotone submodular function fNB(α) : 2V → R+ where
0
0
fNB(α) (S) = fNB(α)
(S ∪ {v 0 }) − fNB(α)
({v 0 }) whose score
is the Laplace-smoothed ML estimate minus a constant.

3 Nearest Neighbor Classification
In this section, we consider (non-parametric) Nearest Neighbor (NN) classifiers and formulate supervised data subset
selection problem as constrained submodular maximization.
Given a set of training samples V = {(xi , y i )}m
i=1 ⊆ X ×Y
and a similarity function w : V × V → R+ which measures
the similarity between any pair of data instances in feature
space, the NN classifier simply classifies x ∈ X based on
its closest training sample. The similarity between training
sample pairs i and j is given as w(i, j) = d − kxi − xj k22 ≥
0
0, where d = maxv∈V,v0 ∈V kxv − xv k22 is the maximum
pairwise distance. Though extremely simple, the NN classifier has been applied on a number of machine learning tasks,
including hand-written digit recognition, text categorization,
object recognition, etc (Bhatia et al., 2010; Boiman et al.,
2008). For a NN classifier, no model needs to be “learnt” as
nearly all the computation takes place at the classification
stage. The complexity of classifying a sample can be expensive and is dependent on the number of training samples.
A way to alleviate this problem is to reduce the training
set size ideally without losing performance, a problem well
suited to supervised data subset selection.
Similar to the NB classifier analysis, we consider a data
log-likelihood set function `NN : 2V → R that maps each
subset S ⊆ V P
to a log likelihood score on the whole set
V : `NN (S) = i∈V (log p(xi |y i ; θ(S))+log p(y i ; θ(S))),
where p(xi |y i ; θ(S)) and p(y i ; θ(S)) are the generative likelihood and the prior likelihood of the sample i ∈ V given
by θ(S). The idea of the data subset selection is to select a
small sized set S so that `NN (S) is maximized. As in the NB
scenario, we express the prior as p(y i ; θ(S)) = myi (S)/|S|.
The key question regarding the function `NN is how to appropriately characterize the generative likelihood function
p(xi |y i ; θ(S)) so that it is of a simple form leading `NN
to be submodular and also maps well to the NN classifier. To this end, we assume that p(xi |y i ; θ(S)) is determined only by the sample j ∈ S that is with label y i
and is closest to i, i.e., j ∈ argmaxs∈S∩V yi w(i, s), and
that the generative likelihood is parameterized with a Gausi
j 2
sian kernel: p(xi |y i ; θ(S)) = ce−kx −x k2 = cew(i,j)−d =
c0 ew(i,j) = c0 exp(maxs∈S∩V yi w(i, s)), where c and c0
are constants. We express the generative log-likelihood as
log p(xi |y i ; θ(S)) = log c0 + maxs∈S∩V yi w(i, s) yielding:

Submodularity in Data Subset Selection and Active Learning

X X

X

0
imating ffac with a surrogate function ffac
that is defined
on a K-Nearest Neighbor sparse sub-graph of Gfac . In this
y∈Y i∈V
y∈Y
|
{z
} |
{z
} work, we use the same idea and utilize a surrogate function
term 2
term 1:fNN
0
fNN
for fNN . Instead of being instantiated on GNN , we deC .
− |V | log |S| + |{z}
0
fine the surrogate function fNN
on its K-Nearest Neighbor
| {z }
constant
K
term 3
sparse sub-graph ĜNN , where each vertex v ∈ V of label
The first term is the Nearest Neighbor submodular function:
y is connected only to its K most similar neighbors in the
X X
block V y . As a result, significant reduction on the memory
fNN (S) =
max y w(i, s).
(6)
s∈S∩V
and time complexity of evaluating fNN is achieved. Furthery∈Y i∈V y
0
more, we show that fNN
performs just as well as fNN even
Similar to the NB case, `NN (S) is a difference of submodwhen the sparsity of ĜK
NN is K = O(log |V |) (additional
ular functions. In a manner similar to the NB classifier, we
details are given in (Wei et al., 2015)).
assume that the selected set is balanced and of fixed size k.
Extension to k-Nearest Neighbor classifiers: In the exThe second and the third term of `NN are treated as constants.
tended version (Wei et al., 2015), we show how to extend the
Hence, the problem maxS:|S|=k `NN (S) is equivalently
analysis to k-Nearest Neighbor classifiers, for k > 1. In this
expressed as constrained submodular maximization:
case, we use the weighted matroid rank functions (Shioura,
max fNN (S).
(7)
S∈B(M)
2009) to model the utility of data subset selection. We defer
the details to (Wei et al., 2015).
Connection to facility location function:
Next we
show fNN ’s connections to the facility location func4 Active Learning
tion
P (Mirchandani & Francis, 1990), defined as ffac (S) =
max
w(i,
j).
Facility
location
function
is
often
j∈S
i∈V
We next extend the results of supervised data subset selecapplied to identify representative instances from a big coltion to the active learning setting, where we incrementally
lection of items (Zheng et al., 2014; Wei et al., 2013; Lin
obtain labels. We define a multistage mini-batch adaptive
& Bilmes, 2011; Gomes & Krause, 2010; Iyer & Bilmes,
active learning framework, where selection adapts to the
2013). Sharing very similar definitions, the facility location
labels previously obtained. Moreover, we show how we
function ffac is in fact a special case of fNN , when all items
can naturally combine the notions of representativeness
in V take the same class labels, or equivalently, |Y| = 1.
and information by filtering. In this section, we focus on
Given its resemblance to fNN , the facility location function
uncertainty sampling to represent information, but our
should naturally model the utility of data sets for training
methods can extend to other strategies, such as Query by
NN classifiers, although it was originally designed to meaCommittee, as well.
sure the representativeness of each set S about the whole
set V . Also, fNN can be writtenP
as a sum of facility locaIn the mini-batch active learning setting, the algorithm ity
tion functions since fNN (S) = y∈Y ffac
(S ∩ V y ) with
eratively selects a set of B unlabeled instances to label at
P
y
every round, and this is done for T rounds. Later rounds get
ffac
(S) = i∈V y maxj∈S w(i, j) a facility location with
to use the labels previously selected, so this is an adaptive
ground set V y . As far as we know, this is the first work to 1)
strategy, but within each mini-batch all labels are selected
connect the facility location function to the utility of training
simultaneously without mutual knowledge of or interaction
NN classifiers, and 2) to show that the utility of a set for
with each other. In the end, we obtain k = BT labeled
training NN classifiers is measured by its representativeness
instances. We denote the set of unlabeled instances as U,
about the data partition for each class.
and the goal is to select a labeled set L such that |L| = k.
Scalability of fNN : To instantiate fNN , one needs to comA common strategy is uncertainty sampling, where the B
pute the similarity for every pair of data instances in each
most uncertain examples (from the current classifiers perblock V y . Equivalently, we can define fNN via a similarspective) are chosen for labeling (Lewis & Gale, 1994) at
ity graph GNN (V, E), where each block V y constitutes a
every round. Given a round t and a set of labeled items L, let
complete graph and every pair of vertices v, v 0 ∈ V y is
δut ≥ 0 be the uncertainty score for an example u ∈ U \ L.
connected with edge weight w(v, v 0 ). Similarly, the facility
The uncertainty sampling
Papproach, then, simply selects
location function ffac is defined via a complete similarity
S ∈ maxS 0 ⊆U \L;|S 0 |=B u∈S 0 δut , and adds these to the
graph Gfac (V, E). Therefore, we call both functions graphlabeled set L. The drawback of this approach is that it fails
based submodular functions. The time and memory comto model the interactions between samples, i.e., labeling
plexity for constructing and storing the similarity graph GNN
one sample could often affect the utility of labeling another.
2
and Gfac is, in the worst case, O(|V | ). The applicability of
Simply choosing the most uncertain samples might lead to a
fNN and ffac is thus significantly limited when |V | is large.
selected set with high redundancy. A better strategy would
(Wei et al., 2014a) addresses this problem for ffac by approxchoose a diverse set of samples from amongst those that the

`NN (S) =

y

max w(i, s) +

s∈S∩V y

my (V ) log my (S)

Submodularity in Data Subset Selection and Active Learning

currently trained model is most uncertain about.
We propose a multi-stage batch active learning scheme
called filtered active submodular selection (FASS). This
algorithm (see Alg. 1) attempts to solve the original data
subset selection problem of maximizing a submodular function f (i.e., either the Naı̈ve Bayes submodular function fNB ,
or the NN submodular function fNN ) in an iterative manner.
At every round t, we first filter out data samples that the
current model is certain about, and preserve a candidate set
of βt (βt ≥ B) most uncertain samples. Specifically,
we
P
find a solution to U t ∈ argmaxU 0 ⊆U \L;|U 0 |=βt u∈U 0 δut .
Since we do not know the labels of the items in U t , we
use the most probable prediction (based on the current classifier) ŷu for each item u ∈ U t as its hypothesized label.
We then instantiate an appropriate submodular objective
t
fˆt : 2U → R+ , which has essentially the same form as f ,
except that it is defined on the ground set U t , and uses the
hypothesized labels ŷu , ∀u ∈ U t . We then solve the optimization problem : max|S|=B;S⊆U t fˆt (S). The scheme of
FASS is fully characterized by the choice of the monotone
submodular objective f , the scaling parameters {βt }Tt=1
and the classifier. Given a classifier, better performance of
FASS is expected when the submodular objective f matches
the utility function for training the classifier. Hence in the
case of the NB classifier, we use fNB as the submodular
function, while in the case of NN classifier, we can use fNN
or ffac . In general however, we can use any submodular
function f in our framework. In Section 5, we show that
FASS with fNB yields superior performance in the case of
NB classifiers, while FASS with ffac or fNN performs better
on NN classifiers.
Algorithm 1 Filtered Active Submodular Selection
1: Input: U, T , B, {βt }T
t=1 , Starting set of labels L
2: for t = 1, · · · , T do
3:
Train the classifier using the labeled set L, and derive
the uncertainty scores δ t ; P
4:
U t ∈ argmaxU ⊆U \L;|U |=βt u∈U δut ;
5:
Obtain the most probable labels as the hypothesized
labels {ŷu }u∈U t .
t
6:
Instantiate fˆt : 2U → R+ on {ŷu }u∈U t and U t ;
7:
Find Lt ∈ argmax|S|=B;S⊆U t fˆt (S).
8:
L = L ∪ Lt .
9: end for
Next, we discuss the scaling parameters {βt }Tt=1 for FASS.
For any round t, βt is the size of the candidate set U t and
controls the trade-off between the criteria of uncertainty
and the submodular objective f . If βt = B, ∀t, the selected
set is chosen only accounting for the uncertainty scores,
and FASS is reduced to the uncertainty sampling approach.
When βt = |U\L|, the selected set does not account for the
uncertainty scores, and is solely chosen by the submodular

objective f . In our experiments, we set the scaling
parameters at each round as constant β, i.e., βt = β, ∀t.
Choice of β affects the time and memory complexity of an
instance of FASS scheme. It becomes significant when f
is a graph-based submodular function, e.g., fNN and ffac .
The time and memory complexity for constructing the
similarity graph grows quadratically with β. Fortunately,
we empirically observe that FASS often performs rather
well for small values of β (β  |U |). As a result, FASS
can easily scale to extremely large data sets. Thanks to our
uncertainty sampling based filtering and data selection via
submodular maximization, we naturally incorporate notions
both of information and representativeness. Moreover,
thanks to the greedy algorithm for maximization, as well as
the prefiltering we perform, our approach can easily scale
to large real-world machine learning problems.
We also point out that FASS subsumes the submodular
active learning framework in (Hoi et al., 2006) as it is a
special case of FASS with βt = |U \ L| and f being chosen
as the Fisher information submodular function ffs defined
P
P
π (1−πi )
Pi
πi (1 − πi ) −
, where
as ffs (S) = 1c
c+
πi πj (xT xj )2
i∈U

i∈S
/

j∈S

i

c > 0, πi is the posterior probability of a sample i, and xi is
the feature representation for i. It is shown (Hoi et al., 2006)
that ffs is normalized monotone submodular and approximates the utility of a given set S by how much it reduces
the Fisher information for logistic regression classifier. We
compare our method with ffs in the experiments.

5 Experimental results
We empirically evaluate the proposed framework on the
supervised data subset selection problem and the active
learning problem. In the set of experiments, we wish
to address the following: 1) How Eqn 3 and 7 perform
on the supervised data subset selection for NB and NN
classifier, respectively; 2) How FASS performs on active
learning under various choices of f and β; and 3) How
well the proposed framework extend to other classifiers,
including Logistic Regression (LR) and Deep Neural
Networks (DNN). In our experiments, we evaluate on two
separate tasks: 1) text categorization, where we tested
three classifiers: NB, NN, and LR; and 2) handwritten digit
recognition, where we evaluated NN and DNN classifiers.
5.1

Text Categorization Experiments

Experimental setup: We evaluate text categorization on
the 20 Newsgroups data set 1 , which consists of 18774
articles divided almost evenly among 20 different UseNet
discussion groups (Lang, 1995). The goal is to classify
an article into one newsgroup (of twenty) to which it was
1

Data is obtained at http://qwone.com/∼jason/20Newsgroups/

Submodularity in Data Subset Selection and Active Learning
Naive Bayes Classifier (20 Newsgroup)

70
65
60
55
50
45

75
70

Logistic Regression Classifier (20 Newsgroup)

80

RS
US
FASS+RS(- = 4000)
FASS+ff s (- = 4000)
FASS+fN B (- = 4000)
FASS+fN N (- = 4000)
FASS+ff ac (- = 4000)
SS+fN N

80

US
FASS+RS (- = 4000)
RS
FASS+fN B (- = 4000)
FASS+ff s (- = 4000)
FASS+fN N (- = 4000)
FASS+ff ac (- = 4000)
SS+fN B
SS+fN N

70

Error Rate (%)

75

Error Rate (%)

Nearest Neighbor Classifier (20 Newsgroup)

US
FASS+RS(- = 4000)
RS
FASS+fN N (- = 4000)
FASS+ff s (- = 4000)
FASS+ff ac (- = 4000)
FASS+fN B (- = 4000)
SS+fN B

Error Rate (%)

80

65
60
55

60

50

40

40
50
35

30
45

30
25

40
200

300

400

500

600

700

800

900

20
100

1000

200

Number of Data Points

300

400

Figure 2.

700

800

900

1000

25

20

10

RS
FASS+ff s (- = 300)
SS+fN N
US
US+RS
FASS+ff ac (- = 300)
FASS+fN N (- = 300)

20
18
16
14
12
10

600

700

800

900

1000

700

800

900

1000

US
RS
FASS
FASS
FASS
FASS
FASS
FASS
FASS
FASS
FASS
FASS
FASS
FASS
FASS
FASS
FASS

75
70
65
60
55
50
45

+
+
+
+
+
+
+
+
+
+
+
+
+
+
+

RS (- = 2000)
RS (- = 4000)
RS (- = 6000)
fNB (- = 2000)
fNB (- = 4000)
fNB (- = 6000)
ffs(- = 2000)
ffs(- = 4000)
ffs(- = 6000)
ffac(- = 2000)
ffac(- = 4000)
ffac(- = 6000)
fNN (- = 2000)
fNN (- = 4000)
fNN (- = 6000)

35
30

2
500

600

40

25

5

Number of Data Points

500

Comparison of - on FASS (NB classi-er on 20 Newsgroup)

4

400

400

80

6

300

300

Number of Data Points

8

15

200

200

Figure 4.

22

Error Rate (%)

30

100

100

Deep Neural Network classifier (MNIST)

24

RS
FASS+ff s (- = 4000)
US
FASS+RS(- = 4000)
FASS+fN N (- = 4000)
FASS+ff ac (- = 4000)
SS+fN N

35

Error Rate (%)

600

Figure 3.

Nearest Neighbor classifier (MNIST)

40

500

Number of Data Points

Error Rate (%)

100

100

200

300

400

500

600

700

Number of Data Points

800

900

1000

100

200

300

400

500

600

700

800

900

1000

Number of Data Points

Figure 5.
Figure 6.
Figure 7.
Figure 8. Comparison of different subset selection strategies: uncertainty sampling (US), random sampling (RS) (error bars indicate
standard deviation over multiple random draws), FASS+ffs , ffac , fNB , fNN , supervised data subset selection (SS) with fNB or fNN . Text
categorization experiments are evaluated on NB (Fig 2), NN (Fig 3), and LR (Fig 4) classifiers. The error rates for the whole set are
11.1%, 19.1%, and 11.7%. Handwritten digit recognition experiments are evaluated on NN (Fig 5) and DNN (Fig 6) classifiers. The error
rates for the whole set are 3.1% and 1.0%. Comparison of FASS among different β on text categorization for NB classifiers (Fig 7).

posted. For each instance of the experiment, we randomly
split 23 and 13 of the whole data set as the training and
test samples. Each subset selection strategy is applied to
sub-select the training samples and then train a classifier.
We report its classification error rate on the test set averaged
over 20 instances of random data splits as the performance
for each subset selection strategy. For mini-batch active
learning experiments, we first randomly label B = 100
samples, on which we train a classifier as the initial model.
In each iteration, additional B unlabeled examples are
selected for labeling to update the model. We evaluate for
T = 10 iterations ending with a total of k = 1000 labeled
examples. Under the least confident criterion, in each
iteration t, we compute the uncertainty score δut of a sample
u as δut = 1 − p(ŷu |xu ), where ŷu is the most probable prediction of the sample u given by the currently trained model,
i.e., ŷu ∈ argmaxy∈Y p(y|xu ). We evaluate the proposed
supervised data subset selection framework also on the same
sequence of subset sizes so that it can be compared with the
mini-batch active learning results. We construct a random
sampling baseline for comparison, where we randomly
sample the data set at appropriate sizes for labeling. The
similarity between any pair of documents is defined as the
cosine similarity between their TF-IDF representations. For
FASS, we fix βt = β = 4000, ∀t and test four different

submodular objectives: fNB , fNN , ffac , and ffs (c = 0.1). In
addition, we construct another baseline (FASS+RS), which
is implemented the same as FASS, except that in each
iteration, the submodular optimization procedure in Line
7 is replaced with a random sub-sampling strategy.
Naı̈ve Bayes Classifier: We first explore the NB classifier
under the bag-of-words model. We apply a Laplace
smoothing parameter of 0.02 for training all NB models in
the experiments. We evaluate the supervised data subset
selection framework (SS+fNB ) as Problem 3 with the
objective fNB(α=0.02) . As shown in Figure 2, SS+fNB
and FASS+f for any choice of f perform consistently
superior to random sampling (RS), which outperforms
the uncertainty sampling (US) at all sizes. FASS+RS is
significantly outperformed by FASS+f for any f . Drastic
improvement at small sizes is achieved by SS+fNB , which
is outperformed by FASS+fNB at larger sizes. Comparing
different f in FASS+f , fNB performs the best.
Nearest Neighbor Classifier: Next, we focus on how the
proposed approaches perform on training NN classifiers.
We evaluate the supervised data subset selection framework
(SS+fNN ) formulated as Eqn 7 with fNN . Unlike NB classifier, NN is not a probabilistic model. We model the posterior
probability of a sample u given by a labeled training set

Submodularity in Data Subset Selection and Active Learning

S as p(y|xu ; θ(S)) =

P

exp (maxj∈S∩V y w(u,j))
exp (maxj∈S∩V y0 w(u,j)) .

y 0 ∈Y

As

shown in Figure 3, SS+fNN yields superior performance
across the board over all other subset selection methods.
The performances of different FASS schemes are ordered
as ffac > fNN > fNB > ff s . Superior results are achieved
with ffac or fNN , either of which matches well with the
Nearest Neighbor classifier. Between fNN and ffac , ffac
always performs better, which may be due to the fact that
the effectiveness of fNN is very sensitive to the accuracy
of the hypothesized class labels on which it is defined.
Logistic Regression Classifier: Lastly, we extend to select
data for training an LR classifier, which is formulated and
solved by the LIBLINEAR tools (Fan et al., 2008). The
results are shown in Figure 4. Although fNN and fNB are
not derived based on the LR model, superior results are still
observed with them in the supervised setting. Between fNN
and fNB , fNN performs better, which indicates that fNN may
fit better with the properties of the LR classifiers. Similar
to the results in the NN classifier, FASS with fNN and ffac
perform better than other objectives, and yield performance
competitive with SS+{fNN , fNB } at large subset sizes.
5.2 Handwritten Digit Recognition Experiments:
Experimental Setup: We evaluate the handwritten digit
recognition task on the MNIST database 2 , which consists of
60,000 training and 10,000 test samples. Each data sample
is an image of a handwritten digit. The training and test data
are both almost evenly divided among 10 different classes.
The goal is to classify each image as a digit. Different from
the setup for the text categorization experiments, we only
run one instance of each subset selection strategy except
for the random sampling baseline, since the training and
test data are fixed. We run 10 instances of random draw
for the random sampling baseline, and report the averaged
classification error as its performance. For mini-batch
active learning, we also experiment with B = 100, T = 10
and k = 1000. We bootstrap the mini-batch active learning
with a different strategy: instead of randomly selecting B
examples, we label a set of B representative data instances
by solving max|S|=B;S⊆U ffac (S) (ffac does not assume
any labels). On this task, we examine how various subset
selection strategies perform on NN and DNN classifiers.
The NB classifier is not included since it does not fit
with this task. We didn’t evaluate SS+fNB or FASS+fNB
for comparison either, since the proposed Naı̈ve Bayes
submodular function fNB is defined on discrete features,
i.e., a set features that take categorical values.
Nearest Neighbor Classifier: First, we evaluate the proposed framework on NN classifiers. The similarity between any pair of data instances i and j is measured as
2

The data set is downloaded from yann.lecun.com/exdb/mnist

d − kxi − xj k22 , where d = maxu,u0 ∈V kxu − xu0 k22 . We
represent the feature xi of each image i as the vector of its
pixel values. We compare different instances of FASS with
β = 4, 000. The results in Figure 5 show similar trends to
the 20 Newsgroup experiments under the NN classifier.
Deep Neural Network Classifier: Lastly we test on DNN
based classifiers. A DNN model, which consists of two
convolution layers followed by two fully connected layers, is trained using Caffe (Jia et al., 2014) on the set of
labeled images selected by each approach. We report the
results in Figure 6. SS+fNN performs well at small sizes and
then matches the random baseline. This indicates that fNN ,
though performing well on NN and LR classifiers for the supervised setting, does not fit with the properties of the DNN
model. Interestingly, drastic improvements are achieved
by the uncertainty sampling strategy, which suggests that
manually labeling the samples that are uncertain to the current system is very valuable for updating the DNN model.
Different from other classifiers, FASS+f tends to perform
well when β is small and in the range [300, 1000]. Here we
show results for β = 300. Significant improvements are
achieved by FASS+fNN or ffac . Though formal analysis
for the DNN model is not available, the empirical results
suggest that it is beneficial to select a set of uncertain data
instances that are representative about the whole set as well.
Choice of β for FASS: Lastly, we compare different
choices of β for FASS. Due to space constraint we only
show the results for NB classifier on 20Newsgroup dataset
in Figure 7. We observe that FASS, in this case, is not sensitive to the choice of β, when β ∈ [2000, 6000]. A complete
set of comparison results, including other classifiers and the
MNIST dataset, are given in (Wei et al., 2015).

6 Discussion
As an extension to our work, we would like to look at
the likelihood, or more generally risk, of a large family
of classifiers as a function of subsets of training data and
from the perspective of submodularity. Given the enormous
empirical success of deep neural networks (DNNs), it would
also be useful to complement our currently empirical-only
results of Figure 6 that, while showing good performance,
could be significantly improved with a submodular function
that better matches the properties of the DNN.
Acknowledgments: We thank Rahul Kidambi, Shengjie
Wang, Chandrashekhar Lavania, and other MELODI-lab
members for useful discussions. This material is based upon
work supported by the National Science Foundation under
Grant No. IIS-1162606, the National Institutes of Health
under award R01GM103544, and by a Google, a Microsoft,
and an Intel research award. R. Iyer acknowledges support
from the Microsoft Research Ph.D Fellowship.

Submodularity in Data Subset Selection and Active Learning

References
Agarwal, Pankaj K, Har-Peled, Sariel, and Varadarajan,
Kasturi R. Geometric approximation via coresets. Combinatorial and computational geometry, 52:1–30, 2005.
Bhatia, Nitin et al. Survey of nearest neighbor techniques.
arXiv preprint arXiv:1007.0085, 2010.
Boiman, Oren, Shechtman, Eli, and Irani, Michal. In defense of nearest-neighbor based image classification. In
CVPR, pp. 1–8. IEEE, 2008.
Chen, Yuxin and Krause, Andreas. Near-optimal batch
mode active learning and adaptive submodular optimization. In ICML, pp. 160–168, 2013.
Cuong, Nguyen Viet, Lee, Wee Sun, and Ye, Nan. Nearoptimal adaptive pool-based active learning with general
loss. UAI, 2010.
Das, A. and Kempe, D. Submodular meets spectral: Greedy
algorithms for subset selection, sparse approximation and
dictionary selection. arXiv preprint arXiv:1102.3975,
2011.

Hoi, Steven CH, Jin, Rong, Zhu, Jianke, and Lyu, Michael R.
Batch mode active learning and its application to medical
image classification. In ICML, 2006.
Huang, Sheng-Jun, Jin, Rong, and Zhou, Zhi-Hua. Active learning by querying informative and representative
examples. In NIPS, 2010.
Iyer, R. and Bilmes, J. Algorithms for approximate minimization of the difference between submodular functions,
with applications. In UAI, 2012.
Iyer, R. and Bilmes, J. Submodular optimization with submodular cover and submodular knapsack constraints. In
NIPS, 2013.
Iyer, R., Jegelka, S., and Bilmes, J. Fast semidifferential
based submodular function optimization. In ICML, 2013.
Iyer, Rishabh, Jegelka, Stefanie, and Bilmes, Jeff. Monotone
closure of relaxed constraints in submodular optimization:
Connections between minimization and maximization.
2014.

Fan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang,
Xiang-Rui, and Lin, Chih-Jen. LIBLINEAR: A library
for large linear classification. JMLR, 9:1871–1874, 2008.

Jia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev,
Sergey, Long, Jonathan, Girshick, Ross, Guadarrama,
Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.

Feldman, Dan, Faulkner, Matthew, and Krause, Andreas.
Scalable training of mixture models via coresets. In NIPS,
2011.

Kempe, David, Kleinberg, Jon, and Tardos, Éva. Maximizing the spread of influence through a social network. In
Proc. SIGKDD, pp. 137–146. ACM, 2003.

Fisher, M.L., Nemhauser, G.L., and Wolsey, L.A. An analysis of approximations for maximizing submodular set
functions—ii. Polyhedral combinatorics, 1978.

Kirchhoff, Katrin and Bilmes, Jeff. Submodularity for data
selection in machine translation. In EMNLP, October
2014.

Gabillon, Victor, Kveton, Branislav, Wen, Zheng, Eriksson, Brian, and Muthukrishnan, S. Adaptive submodular
maximization in bandit setting. In NIPS, 2013.

Krause, A., Singh, A., and Guestrin, C. Near-optimal sensor
placements in Gaussian processes: Theory, efficient algorithms and empirical studies. JMLR, 9:235–284, 2008.

Golovin, Daniel and Krause, Andreas. Adaptive submodularity: A new approach to active learning and stochastic
optimization. In COLT, 2010.

Lang, Ken. Newsweeder: Learning to filter netnews. In
ICML, 1995.

Gomes, Ryan and Krause, Andreas. Budgeted nonparametric learning from data streams. In ICML, 2010.

Lewis, David D and Gale, William A. A sequential algorithm for training text classifiers. In Proc. SIGIR, pp.
3–12. Springer-Verlag New York, Inc., 1994.

Guillory, Andrew and Bilmes, Jeff. Interactive submodular
set cover. ICML, 2010.

Lin, Hui and Bilmes, Jeff. A class of submodular functions
for document summarization. In ALC/HLT, 2011.

Guillory, Andrew and Bilmes, Jeff. Active semi-supervised
learning using submodular functions. In UAI, 2011.

Liu, Yuzong, Wei, Kai, Kirchhoff, Katrin, Song, Yisong,
and Bilmes, Jeff. Submodular feature selection for highdimensional acoustic score spaces. In ICASSP, 2013.

Har-Peled, Sariel and Mazumdar, Soham. On coresets for
k-means and k-median clustering. In Proceedings of
the thirty-sixth annual ACM symposium on Theory of
computing, pp. 291–300. ACM, 2004.

Minoux, M. Accelerated greedy algorithms for maximizing submodular set functions. Optimization Techniques,
1978.

Submodularity in Data Subset Selection and Active Learning

Mirchandani, Pitu B and Francis, Richard L. Discrete Location Theory. Wiley, 1990.
Narasimhan, Mukund and Bilmes, Jeff. A submodularsupermodular procedure with applications to discriminative structure learning. In UAI, 2005.
Nemhauser, George L, Wolsey, Laurence A, and Fisher, Marshall L. An analysis of approximations for maximizing
submodular set functionsi. Mathematical Programming,
14(1):265–294, 1978.

Wei, K., Iyer, R., and Bilmes, J. Submodularity in data
subset selection and active learning: Extended version.
2015.
Wei, Kai, Liu, Yuzong, Kirchhoff, Katrin, and Bilmes, Jeff.
Using document summarization techniques for speech
data subset selection. In NAACL/HLT, 2013.
Wei, Kai, Iyer, Rishabh, and Bilmes, Jeff. Fast multi-stage
submodular maximization. In ICML, 2014a.

Nguyen, Hieu T and Smeulders, Arnold. Active learning
using pre-clustering. In ICML, pp. 79. ACM, 2004.

Wei, Kai, Liu, Yuzong, Kirchhoff, Katrin, Bartels, Chris,
and Bilmes, Jeff. Submodular subset selection for largescale speech training data. In ICASSP, 2014b.

Prasad, Adarsh, Jegelka, Stefanie, and Batra, Dhruv. Submodular meets structured: Finding diverse subsets in
exponentially-large structured item sets. In NIPS, 2014.

Wei, Kai, Liu, Yuzong, Kirchhoff, Katrin, and Bilmes, Jeff.
Unsupervised submodular subset selection for speech
data. In ICASSP, 2014c.

Reed, Colorado and Ghahramani, Zoubin. Scaling the indian buffet process via submodular maximization. arXiv
preprint arXiv:1304.3285, 2013.

Xu, Zhao, Yu, Kai, Tresp, Volker, Xu, Xiaowei, and Wang,
Jizhi. Representative sampling for text classification using
support vector machines. Springer, 2003.

Settles, Burr. Active learning literature survey. University
of Wisconsin, Madison, 52(55-66):11, 2010.

Zheng, Jingjing, Jiang, Zhuolin, Chellappa, Rama, and
Phillips, Jonathon P. Submodular attribute selection for
action recognition in video. In NIPS, 2014.

Seung, H Sebastian, Opper, Manfred, and Sompolinsky,
Haim. Query by committee. In Proceedings of the fifth
annual workshop on Computational learning theory, pp.
287–294. ACM, 1992.
Shamaiah, Manohar, Banerjee, Siddhartha, and Vikalo,
Haris. Greedy sensor selection: Leveraging submodularity. In Proc. CDC, pp. 2572–2577. IEEE, 2010.
Shinohara, Yusuke. A submodular optimization approach to
sentence set selection. In ICASSP, pp. 4112–4115. IEEE,
2014.
Shioura, Akiyoshi. On the pipage rounding algorithm for
submodular function maximizationa view from discrete
convex analysis. Discrete Mathematics, Algorithms and
Applications, 1(01):1–23, 2009.
Singla, Adish, Bogunovic, Ilija, Bartók, Gábor, Karbasi,
Amin, and Krause, Andreas. Near-optimally teaching the
crowd to classify. arXiv preprint arXiv:1402.2092, 2014.
Stobbe, P. and Krause, A. Efficient minimization of decomposable submodular functions. In NIPS, 2010.
Tsang, Ivor W, Kwok, James T, and Cheung, Pak-Ming.
Core vector machines: Fast svm training on very large
data sets. In JMLR, 2005.
Tschiatschek, Sebastian, Iyer, Rishabh K, Wei, Haochen,
and Bilmes, Jeff A. Learning mixtures of submodular
functions for image collection summarization. In NIPS,
2014.

