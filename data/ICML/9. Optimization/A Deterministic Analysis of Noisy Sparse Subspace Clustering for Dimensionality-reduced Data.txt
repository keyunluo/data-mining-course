A Deterministic Analysis of Noisy Sparse Subspace Clustering for
Dimensionality-reduced Data
Yining Wang
Yu-Xiang Wang
Aarti Singh
Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA

Abstract
Subspace clustering groups data into several lowrank subspaces. In this paper, we propose
a theoretical framework to analyze a popular
optimization-based algorithm, Sparse Subspace
Clustering (SSC), when the data dimension is
compressed via some random projection algorithms. We show SSC provably succeeds if
the random projection is a subspace embedding,
which includes random Gaussian projection, uniform row sampling, FJLT, sketching, etc. Our
analysis applies to the most general deterministic
setting and is able to handle both adversarial and
stochastic noise. It also results in the first algorithm for privacy-preserved subspace clustering.

1. Introduction
Subspace clustering groups a collection of data points into
k clusters so that data points within a single cluster lie
near some low rank subspace. It has found a wide range
of applications as many high dimensional data can be approximated by a union of low rank subspaces. Some examples include motion trajectories (Costeira & Kanade,
1998), face images (Basri & Jacobs, 2003), network hop
counts (Eriksson et al., 2012), movie ratings (Zhang et al.,
2012) and social graphs (Jalali et al., 2011).
A large body of research has been devoted to subspace clustering in the last decade. Recently a class of convex optimization based algorithms, in particular Low Rank Representation (LRR, (Liu et al., 2013)) and Sparse Subspace
Clustering (SSC, (Elhamifar & Vidal, 2013)), have drawn
much interest from the literature. It is known that SSC enjoys superb performance in practice (Elhamifar & Vidal,
2009) and have theoretical guarantee under fairly general
conditions (Soltanolkotabi et al., 2012; Wang & Xu, 2013;
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

YININGWA @ CS . CMU . EDU
YUXIANGW @ CS . CMU . EDU
AARTI @ CS . CMU . EDU

Soltanolkotabi et al., 2014).
Let X âˆˆ RdÃ—N denote the data matrix, where d is the ambient dimension and N is the number of data points. For
noiseless data (i.e., data points lie exactly on low-rank subspaces), the exact SSC algorithm solves the optimization
problem in Eq. (1.1) for each data point xi to obtain self
regression solutions ci âˆˆ RN .
min kci k1 ,

ci âˆˆRN

s.t. xi = Xci , cii = 0.

(1.1)

For noisy data, the following Lasso version of SSC is often
used in practice:
min kxi âˆ’ Xci k22 + 2Î»kci k1 ,

ci âˆˆRN

s.t. cii = 0.

(1.2)

Although success conditions for both exact SSC and Lasso
SSC have been extensively analyzed in previous literature,
in practice it is inefficient or even infeasible to operate on
data with high dimension. Some types of dimension reduction is usually required. In this paper, we propose a theoretical framework that analyzes SSC under many popular
dimension reduction settings, including
â€¢ Compressive measurement: For compressive measurement dimensionality-reduced data are obtained by
multiplying the original data typically with a random
Gaussian matrix. We show that SSC provably succeeds when the projected dimension is at the order of
the maximum intrinsic rank of each subspace.
â€¢ Efficient computation: By using fast JohnsonLindenstrauss transform (Ailon & Chazelle, 2009)
or sketching (Charikar et al., 2004; Clarkson &
Woodruff, 2013) one can computationally efficiently
reduce the data dimension while still preserving important structures in the underlying data. We prove
similar results for both FJLT and sketching.
â€¢ Handling missing data: In many applications the
data matrix may be incomplete due to measurement
and sensing limits. It is shown in this paper that when
data meet some incoherent criteria uniform feature
sampling suffices for SSC.

A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data

â€¢ Data privacy: Privacy is an important concern in
modern machine learning applications. It was shown
that random Johnson-Lindenstrauss transform with
added Gaussian noise preserves both informationtheoretic (Zhou et al., 2009) and differential privacy
(Kenthapadi et al., 2013). We provide a utility analysis which shows that SSC can achieve exact subspace
detection despite stringent privacy constraints.
A key observation is that all projections for the aforementioned settings are subspace embeddings, which means
they uniformly preserve the two norm of any vector belonging to a low-rank subspace. Our analysis applies to the
fully deterministic setting under which both subspaces and
data points within each subspace are placed deterministically. It can also handle data corrupted by deterministic or
stochastic noise. This generalizes previous work (Heckel
et al., 2014) which only applies to semi-random models
with noiseless data 1 . The fully deterministic setting poses
more challenges because the perturbation of dual directions
introduced in (Soltanolkotabi et al., 2012) cannot be easily
bounded if exact SSC is used. As a result, even for noiseless data, we employ a Lasso SSC formulation to obtain
strong convexity in the dual problem.

2. Problem setup
Notations The uncorrupted data matrix is denoted as
Y âˆˆ RdÃ—N , where d is the ambient dimension and N is the
total number of data points. Y is normalized so that each
column has unit two norm. Each column in Y belongs to
a union of k subspaces U (1) âˆª Â· Â· Â· U (k) . For each subspace
(`)
(`)
U (`) we write Y(`) = (y 1 , Â· Â· Â· , y N` ) for all columns belonging to U (`) , where N` is the number of data points in
Pk
U (`) and `=1 N` = N . We assume the rank of the `th
subspace U (`) is r` and define r = max` r` . In addition,
we use U(`) âˆˆ RdÃ—r` to represent an orthonormal basis
of U (`) . The observed matrix is denoted by X âˆˆ RdÃ—N .
Under the noiseless setting we have X = Y; for the noisy
setting we have X = Y + Z where Z âˆˆ RdÃ—N is a noise
matrix which can be either deterministic or stochastic.
We use â€œâˆ’iâ€ to denote all except the ith column in a data matrix.
For example, Yâˆ’i =
(`)
(y 1 , Â· Â· Â· , y iâˆ’1 , y i+1 , Â· Â· Â· , y N )
and
Yâˆ’i
=
(`)

(`)

(`)

(`)

(y 1 , Â· Â· Â· , y iâˆ’1 , y i+1 , Â· Â· Â· , y N` ).
For any matrix A,
let Q(A) = conv(Â±a1 , Â· Â· Â· , Â±aN ) denote the symmetric
convex hull spanned by all columns in A. For any subspace U and vector v, denote PU v = argminuâˆˆU ku âˆ’ vk
as the projection of v onto U.

Methods The first step is to perform dimensionality reduction on the observation matrix X. More specifically,
for a target projection dimension p < d, the projected obe 0 âˆˆ RpÃ—N is obtained by first computing
servation matrix X
e
X = Î¨X for some random projection matrix Î¨ âˆˆ RpÃ—d
e 0 has unit
and then normalizing it so that each column in X
two norm. Afterwards, Lasso self-regression as formulated
e 0 to obtain
in Eq. (1.2) is performed for each column in X
N
N Ã—N
the similarity matrix C = {ci }i=1 âˆˆ R
. Spectral
clustering is then be applied to C to obtain an explicit clustering of X. In this paper we use the normalized-cut algorithm (Shi & Malik, 2000) for spectral clustering.
Evaluation measures To evaluate the quality of obtained
similarity matrix C, we consider the Lasso subspace detection property defined in (Wang & Xu, 2013). More specifically, C satisfies Subspace Detection Property (SDP) if
for each i âˆˆ {1, Â· Â· Â· , N } the following holds: 1) ci is a
non-trivial solution. That is, ci is not a zero vector; 2) if
cij 6= 0 then data points xi and xj belong to the same subspace cluster. The second condition alone is referred to as
â€œSelf-Expressiveness Propertyâ€ (SEP) in (Elhamifar & Vidal, 2013). Note that we do not require cij 6= 0 for every
pair of xi , xj belonging to the same cluster. We also remark that in general SEP is not necessary for spectral clustering to succeed, cf. (Wang & Xu, 2013) 2 .

3. Dimension reduction methods
In this section we review several popular dimensionality
reduction methods and show that they are subspace embeddings. A linear projection Î¨ âˆˆ RpÃ—d is said to be
a subspace embedding if for some r-dimesional subspace
L âŠ† Rd the following holds:
Pr [kÎ¨xk âˆˆ (1 Â± )kxk, âˆ€x âˆˆ L] â‰¥ 1 âˆ’ Î´.
Î¨

(3.1)

The following proposition is a simple property of subspace
embeddings, which we prove in Appendix A.1.
Proposition 1. Fix , Î´ > 0. Suppose Î¨ is a subspace
0
embedding with respect to B = {span(U (`) âˆª U (` ) ); `, `0 âˆˆ
[k]} âˆª {xi , z i ; i âˆˆ [N ]} with parameters r0 = 2r, 0 = /3
and Î´ 0 = 2 log((k + N )/Î´). Then with probability â‰¥ 1 âˆ’ Î´
0
for all x, y âˆˆ U (`) âˆª U (` ) we have


2
2


hx, yi âˆ’ hÎ¨x, Î¨yi â‰¤  kxk + kyk ;
(3.2)
2
furthermore, for all x âˆˆ {x1 , z 1 , Â· Â· Â· , xN , z N } the following holds:
(1 âˆ’ )kxk22 â‰¤ kÎ¨xk22 â‰¤ (1 + )kxk22 .

(3.3)

1

In semi/fully random models the underlying subspaces
and/or data points are distributed uniformly at random. Detailed
definitions can be found in (Soltanolkotabi et al., 2012).

2
It is almost sufficient for perfect clustering both in practice (Elhamifar & Vidal, 2013) and in theory (Wang et al., 2015).

A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data

3.1. Random Gaussian projection

3.3. FJLT and sketching

In a random Gaussian projection matrix Î¨ each entry Î¨ij
âˆš
is generated from i.i.d. Gaussian distributions N (0, 1/ p),
where p is the target dimension after projection. Using
standard Gaussian tail bounds and Johnson-Lindenstrauss
argument we have the following proposition, which is
proved in Appendix A.1.

The Fast Johnson-Lindenstrauss Transform (FJLT, (Ailon
& Chazelle, 2009)) computes a compressed version of a
data matrix X âˆˆ RdÃ—N using O(d log d + p) operations
per column with high probability. The projection matix Î¦
can be written as Î¦ = PHD, where P âˆˆ RpÃ—d is a sparse
JL matrix, H âˆˆ RdÃ—d is a deterministic Walsh-Hadamard
matrix and D âˆˆ RdÃ—d is a random diagonal matrix. Details
of FJLT can be found in (Ailon & Chazelle, 2009).

Proposition 2. Gaussian random matrices Î¨ âˆˆ RpÃ—d is a
subspace embedding with respect to B if
p
Sketching (Charikar et al., 2004; Clarkson & Woodruff,
p â‰¥ 2âˆ’2 (r+log(2k 2 /Î´)+ 4r log(2k 2 /Î´)+12 log(4N/Î´)).
2013) is another powerful tool for dimensionality reduction
(3.4)
on sparse inputs. The sketching operator S : Rd â†’ Rp is
constructed as S = Î Î£, where Î  is a permutation matrix
3.2. Uniform row sampling
and Î£ is a random sign diagonal matrix. The projected vecFor uniform row sampling each row in the observed data
tor Sx can be computed in O(nnz(x)) time, where nnz(x)
matrix X is sampled independently at random so that the
is the number of nonzero entries in x.
resulting matrix has p non-zero rows. Formally speaking,
The following two propositions show that both FJLT and
each row of the projection
matrix
h
iâ„¦ is sampled i.i.d. from
q
sketching are subspace embeddings. In fact, they are oblivthe distribution Pr â„¦iÂ· = dp ej = d1 , where i âˆˆ [p],
ious in the sense that they work for any low-dimensional
j âˆˆ [d] and ej is a d-dimensional indicator vector with
subspace L.
only the jth entry not zero.
Proposition 6. (Clarkson & Woodruff, 2013) The FJLT
operator Î¦ is an oblivious subspace embedding if p =
For uniform row sampling to work, both the observation
â„¦(r/2 ), with Î´ considered as a constant.
matrix X and the column space of the uncorrupted data
matrix Y should satisfy certain incoherence conditions. In
Proposition 7. (Avron et al., 2014) The sketching operator
this paper, we apply the following two types of incoherS is an oblivious subspace embedding if p = â„¦(r2 /(2 Î´)).
ence/spikiness definitions, which are widely used in the low
rank matrix completion literature (Recht, 2011; Balzano
4. Main results
et al., 2010; Krishnamurthy & Singh, 2014).
Definition 3 (Column space incoherence). Suppose U is
the column space of some matrix and rank(U) = r. Let
U âˆˆ RdÃ—r be an orthonormal basis of U. The incoherence
of U is defined as
Âµ(U) :=

d
max kU(i) k22 ,
r i=1,Â·Â·Â· ,d

(3.5)

where U(i) indicates the ith row of U.
Definition 4 (Column spikiness). For a vector x âˆˆ Rd , the
spikiness of x is defined as
Âµ(x) := dkxk2âˆ /kxk22 ,

(3.6)

where kxkâˆ = maxi |xi | denotes the vector infinite norm.
We have the following proposition for the uniform row
sampling operator â„¦, which we prove in Appendix A.1.
Proposition 5. Suppose maxk`=1 Âµ(U (`) ) â‰¤ Âµ0 and
maxN
i=1 max(Âµ(xi ), Âµ(z i )) â‰¤ Âµ0 for some constant Âµ0 >
0. The uniform sampling operator â„¦ is a subspace embedding with respect to B if
p â‰¥ 8âˆ’2 Âµ0 (r log(4rk 2 /Î´) + log(8N/Î´)).

(3.7)

We present general geometric separation conditions for
Lasso sparse subspace clustering (Eq. (1.2)) to succeed
for dimensionality-reduced data in the fully deterministic setting; that is, both subspaces and data points within
subspaces are deterministically distributed. In addition,
our analysis reveals that SSC is able to robustly detect
the correct subspaces with substantially compressed data
even when the data points are adversarially perturbed,
stochastically contaminated, or subject to formal privacy
constraints. These contributions significantly expand the
previous provable results on the same subject that works
only with noiseless data generated from the â€œsemi-randomâ€
model (Heckel et al., 2014).
We begin our analysis with two key concepts introduced in the seminal work of Soltanolkotabi and Candes
(Soltanolkotabi et al., 2012): subspace incoherence and inradius. Subspace incoherence characeterizes how well the
subspaces associated with different clusters are separated.
It is based on the dual direction of the optimization problem in Eq. (1.1) and (1.2), which is defined as follows:
Definition 8 (Dual direction, (Soltanolkotabi et al., 2012;
Wang & Xu, 2013)). Fix a column x of X belonging to
subspace U (`) . Its dual direction Î½(x) is defined as the

A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data

solution to the following dual optimization problem:
max hx, Î½i âˆ’

Î½âˆˆRd

Î» >
Î½ Î½,
2

s.t. kX> Î½kâˆ â‰¤ 1.

3

(4.1)

Note that Eq. (4.1) has unique solution when Î» > 0.

max

`=1,Â·Â·Â· ,k

The subspace incoherence for U (`) , Âµ` , is defined in
Eq. (4.2). Note that it is not related to the column subspace incoherence defined in Eq. (3.5). The smaller Âµ` is
the further U (`) is separated from the other subspaces.

Definition 9 (Subspace incoherence, (Soltanolkotabi et al.,
2012; Wang & Xu, 2013)). Subspace incoherence Âµ` for
subspace U (`) is defined as
Âµ` :=

max

xâˆˆX\X(`)

kV(`)> xkâˆ ,

(`)

(4.2)

(`)

where V(`) = (v(x1 ), Â· Â· Â· , v(xN` )) and v(x) =
PU Î½(x)/kPU Î½(x)k2 . Î½(x) is the dual direction of x defined in Eq. (4.1).
The concept of inradius characterizes how well data points
are distributed within a single subspace. More specifically,
we have the following definition:
Definition 10 (Inradius, (Soltanolkotabi et al., 2012; Wang
& Xu, 2013)). For subspace U (`) , its inradius Ï` is defined
as
(`)
Ï` := min r(Q(Yâˆ’i )),
(4.3)
i=1,Â·Â·Â· ,N`

where r(Â·) denotes the radius of the largest ball inscribed
in a convex body.
The larger Ï` is the more uniformly data points are distributed in the `th subspace. Note that unlike subspace incoherence, the inradius is defined in terms of the uncorrupted data Y. We also remark that both Âµ` and Ï` are
between 0 and 1 because of normalization.
Success condition for exact SSC was proved in
(Soltanolkotabi et al., 2012) and was generalized to
the noisy case in (Wang & Xu, 2013). Below we cite
Theorem 6 and Theorem 8 in (Wang & Xu, 2013) for a
success condtition of Lasso SSC. In general, Lasso SSC
succeeds when there is a sufficiently large gap between
subspace incoherence and inradius. Results are restated
below, with minor simplification in our notation.
Theorem 11 ((Wang & Xu, 2013), Theorem 6 and 8). Suppose X = Y + Z where Y is the uncorrupted data matrix
and Z = (z 1 , Â· Â· Â· , z N ) is a determinisitc noise matrix that
satisfies maxN
i=1 kz i k2 â‰¤ Î·. Define Ï := min` Ï` . If
Î·â‰¤
3

min

`=1,Â·Â·Â· ,k

Ï(Ï` âˆ’ Âµ` )
,
7Ï` + 2

For exact SSC simply set Î» = 0.

then subspace detection property holds for the Lasso SSC
algorithm in Eq. (1.2) if the regularization coefficient Î» is
in the range

(4.4)

Î·(1 + Î·)(2 + Ï` )
< Î» < Ï âˆ’ 2Î· âˆ’ Î· 2 .
Ï` âˆ’ Âµ` âˆ’ 2Î·

(4.5)

2
In addition, if Zij âˆ¼ N (0, Ïƒij
/d) are independent Gaus2
2
satisfying
sian noise with variance Ïƒ := maxi,j Ïƒij

r

n
o
log N
Ïƒ(1 + Ïƒ) < C min
Ï, râˆ’1/2 , Ï` âˆ’ Âµ`
`=1,Â·Â·Â· ,k
d
(4.6)
for sufficiently small constant C â‰¥ 1/80, then with probability at least 1 âˆ’ 10
N the subspace detection property holds
if Î» is in the range
r
r
log N
C1 Ïƒ(1 + Ïƒ) log N
< Î» < Ï âˆ’ C2 Ïƒ(1 + Ïƒ)
.
Ï` âˆ’ Âµ`
d
d
(4.7)
Here C1 â‰¤ 80 and C2 â‰¤ 20 are absolute constants.
In the remainder of this section we prove general success
conditions for Lasso SSC on dimensionality-reduced data.
We will first describe the result for the noiseless case and
then the results are extended to handle a small amount
of adversarial perturbation or a much larger amount of
stochastic noise. A performance guarantee under differential privacy can then be stated as a simple corollary of
the noisy recovery result. The basic idea common in all
of the upcoming results is to show that the subspace incoherence and inradius (therefore the geometric gap) are
approximately preserved under dimension reduction.
4.1. The noiseless case
We first bound the perturbation of dual directions when the
data are noiseless.
Lemma 12 (Perturbation of dual directions, the noiseless
case). Assume Î» < 1/4. Fix a column x in X with dual
direction Î½ = Î½(x) and v = v(x) defined in Eq. (4.1)
e denote the projected data matrix Î¨X
and (4.2). Let X
0
e
e Suppose Î½ âˆ—
and X denote the normalized version of X.
âˆ—
and v are computed using the normalized projected data
e 0 . If Î¨ satisfies Eq. (3.2, 3.3) with parameter 
matrix X
and  < 1/ max(1, kÎ½k) then with probability â‰¥ 1 âˆ’ Î´ the
following holds for all w âˆˆ X\X(`) :
p


hv, wi âˆ’ hv âˆ— , wÌƒ0 i â‰¤ 32 /Î» + 2.

(4.8)

As a simple corollary, perturbation of subspace incoherence can then be bounded as in Corollary 13.
Corollary 13 (Perturbation of subsapce incoherence, the
noiseless case). Assume the same notations in Lemma 12.

A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data

Let Âµ` and ÂµÌƒ` be the subspace incoherence of the `th subspace before and after dimension reduction. Then with
probability 1 âˆ’ N Î´ the following holds:
p
(4.9)
ÂµÌƒ` â‰¤ Âµ` + 32 /Î» + 2, âˆ€` = 1, Â· Â· Â· , k.

The following lemma bounds the perturbation of inradius
for each subspace.
Lemma 14 (Perturbation of inradius). Fix ` âˆˆ {1, Â· Â· Â· , k}
and Î´,  > 0. Let Y = Y(`) = (y 1 , Â· Â· Â· , y N` ) âŠ† U (`)
be the noiseless d Ã— N` matrix with all columns belonging
e = Î¨Y âˆˆ RpÃ—N`
to U (`) with unit two norm. Suppose Y
0
e
e
is the projected matrix and Y scales every column in Y
so that they have unit norm. Let Ï` and ÏËœ` be the inradius of subspace U (`) before and after dimensionality ree 0 respectively. If Î¨ satisfies
duction, defined on Y and Y
Eq. (3.2,3.3) with parameter  then with probability â‰¥ 1âˆ’Î´
the following holds:
ÏÌƒ` â‰¥ Ï` /(1 + ).

(4.10)

With perturbation bounds on both subspace incoherence
and inradius we can easily prove the following main theorem, which gives sufficient success condition for Lasso
SSC on dimensionality-reduced noiseless data.
Theorem 15. Suppose X âˆˆ RdÃ—N is a noiseless input matrix with subspace incoherence {Âµ` }k`=1 and inradii
e0
{Ï` }k`=1 . Assume Âµ` < Ï` for all ` âˆˆ {1, Â· Â· Â· , k}. Let X
be the normalized data matrix after compression. Assume
Î» < 1/4 and Î» < Ï/2. If Î¨ satisfies Eq. (3.2,3.3) with
parameter  then Lasso SSC satisfies subspace detection
property with probability â‰¥ 1 âˆ’ Î´, if  is upper bounded by


âˆ†
1
2
,
, c1 Î»âˆ† ,
(4.11)
 â‰¤ min
2 2(2 + Ï)
where c1 > 0 is some absolute constant and âˆ† =
min` (Ï` âˆ’ Âµ` ) is the minimum gap between subspace incoherence and inradius for each subspace.
We make several remarks on Theorem 15. First, an upper bound on  implies a lower bound on projection dimension p, and exact p values vary for different data compression schemes. In addition, even for noiseless data the
regularization coefficient Î» cannot be too small if projection error  is present (recall that Î» â†’ 0 corresponds to
the exact SSC formulation). This is because when Î» goes
to zero the strong convexity of the dual optimization problem decreases. As a result, small perturbation on X could
result in drastic changes of the dual direction and Lemma
12 fails subsequently. On the other hand, as Î» increases
the similarity graph connectivity decreases because the optimal solution to Eq. (1.2) becomes sparser. To guarantee
the obtained solution is nontrivial (i.e., at least one nonzero
entries in ci ), Î» must not exceed Ï/2.

4.2. The noisy case
When the input matrix is corrupted with noise, Lemma 14
remains unchanged because the inradius is defined in terms
of the noiseless data matrix Y. Therefore, we only need to
prove a noisy version of Lemma 12 that bounds the perturbation of dual directions.
Lemma 16 (Perturbation of dual directions, the noisy
case). Suppose X = Y + Z where Y is the uncorrupted data matrix and Z is the noise matrix with
maxi=1,Â·Â·Â· ,n kz i k2 â‰¤ Î·. Assume Î» < 1/4. Fix a column x
with dual direction Î½ and v defined in Eq. (4.1) and (4.2).
e = Î¨Y is the projected noiseless data matrix
Suppose Y
0
e
e Let X
e0 = Y
e0 + Z
e
and Y is the normalized version of Y.
e
be the noisy observation after projection, where Z = Î¨Z
is the projected noise. If Î¨ satisfies Eq. (3.2,3.3) with parameter  and  < 1/ max(1, kÎ½k) then with probability
â‰¥ 1 âˆ’ Î´ the following holds for all w âˆˆ X\X(`) :
s
2


hv, wiâˆ’hv âˆ— , wÌƒ0 i â‰¤ 16 5Î· + 8( + 3Î·) +2. (4.12)
Ï`
Î»
With Lemma 16 the following corollary on subspace incoherence perturbation immediately follows.
Corollary 17 (Perturbation of subsapce incoherence, the
noisy case). Assume the conditions as in Lemma 16. Let
Âµ` and ÂµÌƒ` be the subspace incoherence before and after
dimension reduction. Then with probability â‰¥ 1 âˆ’ N Î´,
s
5Î· 2
8( + 3Î·)
ÂµÌƒ` â‰¤ Âµ` +16
+2, âˆ€` âˆˆ [k]. (4.13)
+
Ï`
Î»
Finally, we have Theorem 18 and Theorem 19 as simple
consequences of Corollary 17 and Lemma 14.
Theorem 18 (Compressed-SSC under Deterministic
noise). Suppose X = Y + Z is a noisy input matrix with
subspace incoherence {Âµ` }k`=1 and inradii {Ï` }k`=1 . Assume maxi kz i k2 â‰¤ Î· and Âµ` < Ï` for all ` âˆˆ {1, Â· Â· Â· , k}.
e0 = Y
e0 + Z
e where Y
e 0 is the normalized uncorSuppose X
e = Î¨Z is the
rupted data matrix after compression and Z
projected noise matrix. Assume Î· satisfies
Î·â‰¤

min

`=1,Â·Â·Â· ,k

Ï(Ï` âˆ’ Âµ` )
.
96

(4.14)

If Î¨ satisfies Eq. (3.2,3.3) with parameter  and Î» = Ï/4,
then Lasso SSC satisfies the subspace detection property
with probability â‰¥ 1 âˆ’ Î´. Here  is upper bounded by




1
âˆ†
Î»
5Î· 2
2
 â‰¤ min
,
,
c2 âˆ† âˆ’
âˆ’ 3Î· ,
3 4(2 + Ï) 8
Ï
(4.15)
where c2 > 0 is some absolute constant and âˆ† =
min` (Ï` âˆ’ Âµ` ) is the minimum gap between subspace incoherence and inradius.

A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data

Theorem 19 (Compressed-SSC under Gaussian noise).
Define the same positive quantities {Âµ` }k`=1 , {Ï` }k`=1 , Ï, âˆ†
and projection matrix Î¨ as in Theorem 18. Assume each
2
column of Z is sampled from N (0, Ïƒd I). Suppose Î¨ is a
linear transform that satisfies Eq. (3.2,3.3) with parameter
âˆš
, and moreover its spectral norm satisfies kÎ¨k â‰¤ Î¾ dp
(For Gaussian JL projection Î¾ â‰¤ 3 with high probability).
In addition, assume the noise parameter Ïƒ satisfies
s
n
o
log N
C
Ïƒ(1 + Ïƒ) â‰¤ 2 min Ï, râˆ’1/2 , Ï` âˆ’ Âµ`
p
4Î¾ `=1,...,k
(4.16)
with the same constant C as in Eq. (4.6). Then Lasso SSC
with Î» = Ï/4 satisfies the subspace detection property with
probability â‰¥ 1 âˆ’ 8/N âˆ’ Î´, if  is upper bounded by
 â‰¤ min



1
âˆ†
Î»
,
,
3 4(2 + Ï) 8



c2 âˆ†2 âˆ’

45Ïƒ 2
Ï




âˆ’ 9Ïƒ .

(4.17)
Here âˆ† = min` (Ï` âˆ’ Âµ` ) is the minimum gap between
subspace incoherence and inradius.
These results put forward an interesting view of the subspace clustering problem in terms of resource allocation.
The critical geometric gap âˆ† (called â€œMargin of Errorâ€ in
Wang & Xu (2013)) can be viewed as the amount of resource that we have for a problem while preserving the subspace detection property. It can be used to tolerate noise,
compress the data matrix, or alleviate the graph connectivity problem of SSC (Wang et al., 2013). For example, if the
noise level is high then it will use more of âˆ† and as a result
we can only compress the data less aggressively, as shown
in Eq. (4.15) and (4.17).
4.3. Subspace clustering under privacy constraints
Another common motivation to compress the data before
data analysis is to protect data privacy. It has been formally shown that random projections (at least with Gaussian random matrices) protect information privacy (Zhou
et al., 2009). Stronger privacy protection can be enforced
by injecting additional noise to the dimension reduced data
(Kenthapadi et al., 2013). Algorithmically, this basically
involves adding iid Gaussian noise to the data after we apply a Johnson-Lindenstrauss transform Î¨ of choice to X
and normalize every column. This procedure guarantees
differential privacy (Dwork et al., 2006; Dwork, 2006) at
the attribute level, which prevents any single entry of the
data matrix from being identified â€œfor sureâ€ given the privatized data and arbitrary side information. The amount
of noise to add is calibrated according to how â€œunsureâ€ we
need and how â€œspikyâ€ (Definition 4) each data point can be.
Due to space constraints, we will describe the detailed
definition and our technical results on differential privacy

preserved subspace clustering in the supplementary document. We show that Lasso-SSC can still achieve exact
subspace detection despite differential privacy constraints.
To the best of our knowledge, this is the first result of its
kind for subspace clustering and it is not possible without dimensionality reduction. So the knife cuts in both
sides: dimension-reduction helps in both computational efficiency and privacy protection.
On the other hand, we are not able to generalize the result to an even stronger form of differential privacy that
protects each full column in the data matrix. Such privacy
requirements make more sense if we consider each column
corresponding to an individual. In the supplementary document we present an argument showing that it is impossible to protect differential privacy of this kind if subspace
detection property holds with high probability. This calls
for a more realistic measure of utility for subspace clustering, for example, percentage of correctly clustered points
or closeness of recovered subspaces to the ground truth.

5. Proofs
In this section we give proof sketches for the key lemmas.
Complete proofs are deferred to Appendix A.
Proof sketch of Lemma 12. Let f : Rd â†’ R, Î½ âˆˆ Rd and
fËœ : Rp â†’ R, Î½ âˆ— âˆˆ Rp denote the objective functions and
optimal solutions of the dual problem in Eq. (4.1) on the
original data and projected data, respectively. Note that for
noiseless data, Î½(x) lies exactly on the subspace to which
x belongs and the same holds after linear projection. Suppose Î½Ìƒ 0 âˆˆ Rp is a properly shrinked version of Î½ after random projection so that Î½Ìƒ 0 is feasible to the projected dual
optimization problem. Since random projection preserves
inner products, one can show that with high probability
fËœ(Î½Ìƒ 0 ) is close to f (Î½). On the other hand, fËœ(Î½ âˆ— ) is close to
f (Î½Ì„ 0 ) where Î½Ì„ 0 âˆˆ Rd is some feasible solution to the dual
problem on original data, obtained by inversely projecting
Î½ âˆ— onto the original subspace and properly shrink it so that
it is feasible. 4 In general, we have the following:
fËœ(Î½Ìƒ 0 ) â‰ˆ f (Î½) < f (Î½Ì„ 0 ) â‰ˆ fËœ(Î½ âˆ— ) < fËœ(Î½Ìƒ 0 ).

(5.1)

The difference |fËœ(Î½ âˆ— )âˆ’ fËœ(Î½Ìƒ 0 )| can then be upper bounded
by applying Eq. (5.1). Consequently, one can bound the
dual direction perturbation kÎ½ âˆ— âˆ’ Î½Ìƒ 0 k by noting that the
dual problem in Eq. (4.1) is strongly convex for both the
original data and the projected data. With the upper bound
on kÎ½ âˆ— âˆ’ Î½Ìƒ 0 k we can easily bound the inner product perturbation |hv, wi âˆ’ hv âˆ— , wÌƒ0 i| because hÎ½Ìƒ 0 , wÌƒ0 i â‰ˆ hÎ½, wi and
v is nothing but a normalized version of Î½.
4

This requires uniform inner product preservation between
two low-rank subspaces. Also, there might be multiple Î½Ì„ that
correspond to Î½ âˆ— . Any of them can be taken.

A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data
Neg. Relative SEP Violation

2

350
âˆ’0.1
300
250

âˆ’0.2

200
âˆ’0.3
150
100

âˆ’0.4

Relative SEP Violation

400

Reduced data dimension (p)

Proof sketch of Lemma 14. For notational simpicity ree (âˆ’i) for some fixed data point
define Y = Y(âˆ’i) and Y
(`)
e 0 ) denote the convex hull of the
xi . Let Q(Y) and Q(Y
original and (normalized) projected data. Suppose C, C 0 are
e 0 ). Let cÌƒ be
the largest balls inscribed in Q(Y) and Q(Y
e By
the point that lies at the intersecion of âˆ‚C and âˆ‚Q(Y).
e
definition, kcÌƒk = r(Q(Y)). Suppose c lies in the original data space and it corresponds to cÌƒ after projection (i.e.,
cÌƒ = Î¨c). It is easy to prove that c does not lie at the interior of Q(Y) and hence kck is lower bounded by r(Q(Y)).
Subsequently, a lower bound on kck yields a lower bound
on kcÌƒk because a subspace embedding preserves vector
norms uniformly on a low-rank subspace.

kPU (`)âŠ¥ Î½k2 â‰¤ Î»Î· (1/Ï` + 1) â‰¤ 2Î»Î·/Ï` .

(5.2)

6. Related work
Heckel et al. analyzed both SSC and Threshold-based Subspace Clustering (TSC) on projected data (Heckel et al.,
2014). The key difference is that the analysis in (Heckel
et al., 2014) only applies to noiseless data and is limited
to the semi-random model introduced in (Soltanolkotabi
et al., 2012), which is arguably less practical. In contrast,
our analysis generalizes to fully deterministic settings. It
also applies to a broader class of dimensionality reduction
methods and can handle data corrupted by noise.
Arpit et al. proposed a novel dimensionality reduction algorithm to preserve independent subspace structures (Arpit
et al., 2014). They showed that by using p = 2k one
can preserve the independence structure among subspaces.
However, their analysis only applies to noiseless and independent subspaces. Furthermore, in our analysis the target dimension p required depends on the intrinsic subspace
rank r instead of k. Usually r is quite small in practice
(Elhamifar & Vidal, 2013; Basri & Jacobs, 2003).
Another relevant line of research is high-rank matrix completion. In (Eriksson et al., 2012) the authors proposed a
neighborhood selection based algorithm to solve multiple
matrix completion problems. Although their method does
recover points lying on the same subspace, the completion
problem is quite different from subspace clustering as we
discuss in Section 8. Furthermore, though their sampling
scheme is more practical than ours (does not need sampling

1

0.5

50
âˆ’1.6

âˆ’1.4

âˆ’1.2

âˆ’1

âˆ’0.5

âˆ’0.8

0
0

Regularization coefficient (log10 Î»)

100

200

300

400

Dimension after projection (p)

Figure 1. Relative SEP violation on extended Yale Face B dataset.
Left: Lasso SSC (varying Î», p); rightmost two columns indicate
trivial solutions. White indicates good recovery and black indicates poor recovery. Right: Lasso SSC and TSC (varying q, p).

0.7

0.8

0.6
0.7
0.5

Clustering Error

Clustering Error

Proof sketch of Lemma 16. The proof is essentially similar
to the one for Lemma 12. The major difference is that under
the noisy setting a dual direction Î½ no longer falls exactly
onto an underlying subspace U (`) and one needs to upper bound the norm of the orthogonal component PU (`)âŠ¥ Î½.
This can be done using, for example, Eq. (5.16) in (Wang
& Xu, 2013), which states that

TSC, q=5
TSC, q=10
TSC, q=15
SSC, q=5
SSC, q=10
SSC, q=15

1.5

0.4
0.3
0.2
0.1
0

TSC, q=5
TSC, q=10
SSC, q=5
SSC, q=10
âˆ’2
LRR, Î»=10
LRR, Î»=10âˆ’3
10
20

0.6
TSC, q=5
TSC, q=10
SSC, q=5
SSC, q=10
LRR, Î»=10âˆ’2

0.5

0.4

âˆ’3

LRR, Î»=10
30

40

Dimension after projection (p)

50

10

20

30

40

50

Dimension after projection (p)

Figure 2. Clustering error on the Extended Yale B dataset with
five individuals (left) and ten individuals (right).

entire rows), an exponential number of data points are required. In contrast, in our analysis N only needs to scale
polynomially with r if a stochastic model is imposed.

7. Numerical results
In this section we present numerical results that validate
our theoretical findings and compare Lasso SSC with TSC
(Heckel & Bolcskei, 2013) and LRR (Liu et al., 2013).
The Lasso SSC algorithm is implemented using augmented
Lagrangian method (ALM) when the regularization coefficient Î» is fixed and known. We also implement Lasso SSC
using a solution path algorithm (Tibshirani & Taylor, 2011)
to tune Î» separately for each data point. The LRR implementation is obtained from (Liu, 2013). Random Gaussian
projection is used for all experiments. All algorithms are
implemented in Matlab.
We evaluate clustering results by both clustering error and
the relative violation of SEP. Clustering error is defined as
the percentage of mis-clustered data points up to permutation. The relative violation of SEP characterizes how
much the obtained similarity matrix C violates the selfexpressiveness property. It was introduced in (Wang & Xu,
2013) and defined as
P
|C|ij
(i,j)âˆˆM
/
RelViolation(C, M) = P
,
(7.1)
(i,j)âˆˆM |C|ij
where (i, j) âˆˆ M means xi and xj belong to the same
cluster and vice versa.

A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data
Clustering error
âˆ’0.04
40

âˆ’0.05
âˆ’0.06

30

âˆ’0.07
20

âˆ’0.08
âˆ’0.09

10
âˆ’5

âˆ’4

âˆ’3

âˆ’2

âˆ’1

âˆ’0.1

Reduced data dimension (p)

Reduced data dimension (p)

Neg. Relative SEP Violation
50

50

0.16
0.14

40

0.12
30

0.1
0.08

20

0.06
0.04

10
âˆ’5

Regularization coefficient (log10 Î»)

âˆ’4

âˆ’3

âˆ’2

âˆ’1

Regularization coefficient (log10 Î»)

Figure 3. Relative SEP violation (left) and clustering error for
Lasso SSC on the Hopkins-155 dataset. The rightmost two
columns in the left figure indicate trivial solutions. White indicates good similarity graph or clustering and black indicates poor
similarity graph or clustering.
0.35

0.7

Relative SEP Violation

Clustering Error

0.3
TSC, q=5
TSC, q=10
âˆ’2
SSC, Î»=10

0.25
0.2

SSC, Î»=10âˆ’3
âˆ’2

LRR, Î»=10

0.15

LRR, Î»=10âˆ’3

0.1
0.05
0

0.6

TSC, q=5
TSC, q=10
SSC, Î»=10âˆ’2

0.5

SSC, Î»=10

âˆ’3

LRR, Î»=10âˆ’2
0.4

âˆ’3

LRR, Î»=10

0.3
0.2
0.1

10

20

30

40

Dimension after projection (p)

50

dimension p is small under which LRR performance guarantee fails because subspaces are no longer independent.

0

10

20

30

40

50

Dimension after projection (p)

Figure 4. Comparison of clustering error (left) and relative SEP
violation (right) for Lasso SSC, TSC and LRR on the Hopkins155 dataset.

7.1. Face Clustering
We start by evaluating the performance of Lasso SSC with
random Gaussian projection on the extended Yale B face
recognition dataset (Lee et al., 2005). We also compare
with TSC, which is known to be robust to random projection (Heckel et al., 2014), and LRR. We preprocess the
dataset by projecting face images for each individual onto
a 9D affine subsapce via PCA. Such preprocess steps were
justified in (Basri & Jacobs, 2003) and also adopted in
(Wang & Xu, 2013).
In Figure 1 we report the relative SEP violation for both
Lasso SSC and TSC. Results are averaged for 10 random
projections. We use q to denote the number of solutionpath steps taken for each self-regression solution ci . Figure
1 shows that as Î» decreases the relative SEP violation for
Lasso SSC increases, which is predicted by our theoretical
analysis. In addition, Figure 1 shows that the relative SEP
violation for TSC is rather high compared to Lasso SSC.
This is because the analysis for TSC heavily relies upon the
semi-random model assumption, which rarely holds true in
real-world applications.
Figure 2 shows the clustering accuracy of Lasso SSC, TSC
and LRR. For this experiment we randomly selected 5 and
10 individuals from the dataset and report the average clustering error. The total data dimension is 5 Ã— 9 = 45 for
5 individuals and 10 Ã— 9 = 90 for 10 individuals. We can
see that Lasso SSC significantly outperforms TSC under all
p and q settings. It outperforms LRR when the projection

7.2. Motion segmentation
We evaluate the performance of Lasso SSC with random projection for motion trajectory segmentation on the
Hopkins-155 dataset (Tron & Vidal, 2007). Figure 3 shows
the mean relative SEP violation and clustering error for
SSC across all 158 video sequences in the dataset. The
ambient data dimension ranges from 112 to 240. We can
see that the relative SEP violation goes up when Î» or the
projection dimension p decreases. The clustering accuracy
acts accordingly, with the exception of very large Î» values
under which we get very sparse self-regression vectors and
hence connectivity of the similarity graph is affected.
In Figure 4 we report the clustering error and relative SEP
violation for Lasso SSC, TSC and LRR on Hopkins-155.
Both clustering error and relative SEP violation are averaged across all 158 sequences. Unlike the face recognition
task, we set specific Î» values instead of solution-path steps
(q) for Lasso SSC because the former works better on the
Hopkins-155 dataset. Figure 4 shows that Lasso SSC outperforms TSC and LRR under various regularization and
projection dimension settings, which is consistent with previous experimental results (Elhamifar & Vidal, 2013).

8. Discussion
We discuss on the relationship between subspace clustering
and high-rank matrix completion. In general, if one can
complete a high-rank matrix then exact subspace clustering
algorithms can be applied to obtain subspace clusters. On
the other hand, once the perfect subspace clustering result
is availble we can run separate low-rank matrix completion
for each cluster to complete the entire matrix.
However, we remark that under the missing data setting
subspace clustering is easier than matrix completion in two
ways. First, most matrix completion algorithms require
both row and column spaces of a matrix to be incoherent
(Recht, 2011), while for subspace clustering we only assume incoherence on the column space. Furthermore, the
uniform sampling scheme proposed in Section 3 is a passive sampling scheme because the probability of observing a particular matrix entry is fixed a priori. Although it
suffices for the purpose of subspace clustering, it is shown
in (Krishnamurthy & Singh, 2014) that any passive sampling scheme fails to complete a column space coherent
matrix unless it observes a constant fraction of matrix entries. Adaptive sampling is required to complete a lowrank matrix with coherent column space (Krishnamurthy
& Singh, 2014; Chen et al., 2013).

A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data

Acknowledgement
This research is supported in part by grants NSF CAREER
IIS-1252412 and AFOSR YIP FA9550-14-1-0285. YuXiang Wang was supported by NSF Award BCS-0941518
to CMU Statistics and Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.

Elhamifar, Ehsan and Vidal, ReneÌ. Sparse subspace clustering. In Computer Vision and Pattern Recognition,
2009. CVPR 2009. IEEE Conference on, pp. 2790â€“2797.
IEEE, 2009.
Elhamifar, Ehsen and Vidal, Rene. Sparse subspace clustering: Algorithm, theory and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35
(11):2765â€“2781, 2013.

References

Eriksson, Brian, Balzano, Laura, and Nowak, Robert. High
rank matrix completion. In AISTATS, 2012.

Ailon, Nir and Chazelle, Bernard. The fast johnsonlindenstrauss transform and approximate nearest neighbors. SIAM Journal of Computing, 39(1):302â€“322, 2009.

Heckel, Reinhard and Bolcskei, Helmut. Robust subspace
clustering via thresholding. arXiv:1307.4891, 2013.

Arpit, Devansh, Nwogu, Ifeoma, and Govindaraju,
Venu. Dimensionality reduction with subspace structure
preservation. In NIPS, 2014.
Avron, Haim, Nguyen, Huy, and Woodruff, David. Subspace embeddings for the polynomial kernel. In NIPS,
2014.
Balzano, Laura, Recht, Benjamin, and Nowak, Robert.
High-dimensional matched subspace detection when
data are missing. In ISIT, 2010.
Basri, Ronen and Jacobs, David. Lambertian reflectance
and linear subspaces.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(2):218â€“233,
2003.
Charikar, Moses, Chen, Kevin, and Farach-Colton, Martin. Finding frequent items in data streams. Theoretical
Computer Science, 312(1):3â€“15, 2004.
Chen, Yudong, Bhojanapalli, Srinadh, Sanghavi, Sujay,
and Ward, Rachel. Completing any low-rank matrix,
provably. arXiv:1306.2979, 2013.
Clarkson, Kenneth and Woodruff, David. Low rank approximation and regression in input sparsity time. In
STOC, 2013.
Costeira, Joao and Kanade, Takeo. A multibody factorization method for independently moving objects. International Journal of Computer Vision, 29(3):159â€“179,
1998.
Dwork, Cynthia. Differential privacy. In Automata, languages and programming, pp. 1â€“12. Springer, 2006.
Dwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and
Smith, Adam. Calibrating noise to sensitivity in private
data analysis. In Theory of cryptography, pp. 265â€“284.
Springer, 2006.

Heckel, Reinhard, Tschannen, Michael, and Bolcskei, Helmut. Subspace clustering of dimensionality-reduced
data. In ISIT, 2014.
Jalali, Ali, Chen, Yudong, Sanghavi, Sujay, and Xu, Huan.
Clustering partially observed graphs via convex optimization. In ICML, 2011.
Kenthapadi, Krishnaram, Korolova, Aleksandra, Mironov,
Ilya, and Mishra, Nina. Privacy via the johnsonlindenstrauss transform. Journal of Privacy and Confidentiality, 5(1):39â€“71, 2013.
Krishnamurthy, Akshay and Singh, Aarti. On the power
of adaptivity in matrix completion and approximation.
Arxiv:1407.3619, 2014.
Lee, Kuang-Chih, Ho, Jeffrey, and Kriegman, David. Acquiring linear subspaces for face recognition under variable lighting. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 27(5):684â€“698, 2005.
Liu, Guangcan. Solving the low-rank representation (LRR)
problems. Available online, August 2013. URL https:
//sites.google.com/site/guangcanliu/.
Liu, Guangcan, Lin, Zhouchen, Shuicheng, Yan, Sun, Ju,
Ma, Yi, and Yu, Yong. Robust recovery of subspace
structures by low-rank representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35
(1):171â€“184, 2013.
Recht, Benjamin. A simpler approach to matrix completion. The Journal of Machine Learning Research, 12:
3413â€“3430, 2011.
Shi, Jianbo and Malik, Jitendra. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888â€“905, 2000.
Soltanolkotabi, Mahdi, Candes, Emmanuel J, et al. A geometric analysis of subspace clustering with outliers. The
Annals of Statistics, 40(4):2195â€“2238, 2012.

A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data

Soltanolkotabi, Mahdi, Elhamifar, Ehsan, Candes, Emmanuel J, et al. Robust subspace clustering. The Annals
of Statistics, 42(2):669â€“699, 2014.
Tibshirani, Ryan and Taylor, Jonathan. The solution path of
the generalized lasso. Annals of Statistics, 39(3):1335â€“
1371, 2011.
Tron, Roberto and Vidal, Rene. A benchmark for the
comparison of 3-D motion segmentation algorithms. In
CVPR, 2007.
Wang, Yining, Wang, Yu-Xiang, and Singh, Aarti.
Clustering consistent sparse subspace clustering.
arXiv:1504.01046, 2015.
Wang, Yu-Xiang and Xu, Huan. Noisy sparse subspace
clustering. arXiv:1309.1233, 2013.
Wang, Yu-Xiang, Xu, Huan, and Leng, Chenlei. Provable
subspace clustering: When LRR meets SSC. In NIPS,
2013.
Zhang, Amy, Fawaz, Nadia, Ioannidis, Stratis, and Montanari, Andrea. Guess who rated this movie: Identifying
users through subspace clustering. In UAI, 2012.
Zhou, Shuheng, Lafferty, John, and Wasserman, Larry.
Compressed and privacy-sensitive sparse regression.
IEEE Transactions on Information Theory, 55(2):846â€“
866, 2009.

