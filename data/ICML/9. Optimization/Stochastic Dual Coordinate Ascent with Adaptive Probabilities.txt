Stochastic Dual Coordinate Ascent with Adaptive Probabilities

Dominik Csiba
University of Edinburgh

CDOMINIK @ GMAIL . COM

Zheng Qu
University of Edinburgh

ZHENG . QU @ ED . AC . UK

Peter Richtárik
University of Edinburgh

PETER . RICHTARIK @ ED . AC . UK

Abstract
This paper introduces AdaSDCA: an adaptive
variant of stochastic dual coordinate ascent
(SDCA) for solving the regularized empirical
risk minimization problems. Our modification
consists in allowing the method to adaptively
change the probability distribution over the dual
variables throughout the iterative process. AdaSDCA achieves provably better complexity bound
than SDCA with the best fixed probability distribution, known as importance sampling. However, it is of a theoretical character as it is expensive to implement. We also propose AdaSDCA+:
a practical variant which in our experiments outperforms existing non-adaptive methods.

1. Introduction
Empirical Loss Minimization. In this paper we consider
the regularized empirical risk minimization problem:
"
#
n
X
def 1
>
min P (w) =
φi (Ai w) + λg(w) .
(1)
n i=1
w∈Rd
In the context of supervised learning, w is a linear predictor,
A1 , . . . , An ∈ Rd are samples, φ1 , . . . , φn : R → R are
loss functions, g : Rd → R is a regularizer and λ > 0 a
regularization parameter. Hence, we are seeking to identify
the predictor which minimizes the average (empirical) loss
P (w).
We assume throughout that the loss functions are 1/γsmooth for some γ > 0. That is, we assume they are
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

differentiable and have Lipschitz derivative with Lipschitz
constant 1/γ:
|φ0 (a) − φ0 (b)| ≤

1
|a − b|
γ

for all a, b ∈ R. Moreover, we assume that g is 1-strongly
convex with respect to the L2 norm:
g(w) ≤ αg(w1 ) + (1 − α)g(w2 ) −

α(1 − α)
kw1 − w2 k2
2

for all w1 , w2 ∈ dom g, 0 ≤ α ≤ 1 and w = αw1 + (1 −
α)w2 .
The ERM problem (1) has received considerable attention
in recent years due to its widespread usage in supervised
statistical learning (Shalev-Shwartz & Zhang, 2013b). Often, the number of samples n is very large and it is important to design algorithms that would be efficient in this
regime.
Modern stochastic algorithms for ERM. Several highly
efficient methods for solving the ERM problem were proposed and analyzed recently. These include primal methods such as SAG (Schmidt et al., 2013), SVRG (Johnson &
Zhang, 2013), S2GD (Konečný & Richtárik, 2014), SAGA
(Defazio et al., 2014), mS2GD (Konečný et al., 2014a)
and MISO (Mairal, 2015). Importance sampling was considered in ProxSVRG (Xiao & Zhang, 2014) and S2CD
(Konečný et al., 2014b).
Stochastic Dual Coordinate Ascent. One of the most successful methods in this category is stochastic dual coordinate ascent (SDCA), which operates on the dual of the
ERM problem (1):
max
α=(α1 ,...,αn )∈R

h
i
def
D(α)
=
−f
(α)
−
ψ(α)
,
n

(2)

Stochastic Dual Coordinate Ascent with Adaptive Probabilities

where functions f and ψ are defined by
n

def

f (α) = λg ∗

1 X
Ai α i
λn i=1

!
,

(3)

n

def

ψ(α) =

1X ∗
φ (−αi ),
n i=1 i

(4)

and g ∗ and φ∗i are the convex conjugates1 of g and φi , respectively. Note that in dual problem, there are as many
variables as there are samples in the primal: α ∈ Rn .
SDCA in each iteration randomly selects a dual variable
αi , and performs its update, usually via closed-form formula – this strategy is know as randomized coordinate
descent. Methods based on updating randomly selected
dual variables enjoy, in our setting, a linear convergence
rate (Shalev-Shwartz & Zhang, 2013b; 2012; Takáč et al.,
2013; Shalev-Shwartz & Zhang, 2013a; Zhao & Zhang,
2015; Qu et al., 2014). These methods have attracted
considerable attention in the past few years, and include
SCD (Shalev-Shwartz & Tewari, 2011), RCDM (Nesterov,
2012), UCDC (Richtárik & Takáč, 2014), ICD (Tappenden
et al., 2013), PCDM (Richtárik & Takáč, 2015), SPCDM
(Fercoq & Richtárik, 2013), SPDC (Zhang & Xiao, 2015),
APCG (Lin et al., 2014), RCD (Necoara & Patrascu, 2014),
APPROX (Fercoq & Richtárik, 2013), QUARTZ (Qu et al.,
2014) and ALPHA (Qu & Richtárik, 2014). Recent advances on mini-batch and distributed variants can be found
in (Liu & Wright, 2014), (Zhao et al., 2014b), (Richtárik &
Takáč, 2013a), (Fercoq et al., 2014), (Trofimov & Genkin,
2014), (Jaggi et al., 2014), (Mareček et al., 2014) and (Mahajan et al., 2014). Other related work includes (Nemirovski et al., 2009; Duchi et al., 2011; Agarwal & Bottou, 2015; Zhao et al., 2014a; Fountoulakis & Tappenden,
2014; Tappenden et al., 2014). We also point to (Wright,
2014) for a review on coordinate descent algorithms.
Selection Probabilities. Naturally, both the theoretical
convergence rate and practical performance of randomized coordinate descent methods depends on the probability distribution governing the choice of individual coordinates. While most existing work assumes uniform distribution, it was shown by Richtárik & Takáč (2014); Necoara
et al. (2012); Zhao & Zhang (2015) that coordinate descent works for an arbitrary fixed probability distribution
over individual coordinates and even subsets of coordinates (Richtárik & Takáč, 2013b; Qu et al., 2014; Qu &
Richtárik, 2014; Qu & Richtárik, 2014). In all of these
works the theory allows the computation of a fixed probability distribution, known as importance sampling, which
optimizes the complexity bounds. However, such a distri1
By the convex (Fenchel) conjugate of a function h : Rk →
R we mean the function h∗ : Rk → R defined by h∗ (u) =
sups {s> u − h(s)}.

bution often depends on unknown quantities, such as the
distances of the individual variables from their optimal values (Richtárik & Takáč, 2014; Qu & Richtárik, 2014). In
some cases, such as for smooth strongly convex functions
or in the primal-dual setup we consider here, the probabilities forming an importance sampling can be explicitly computed (Richtárik & Takáč, 2013b; Zhao & Zhang, 2015; Qu
et al., 2014; Qu & Richtárik, 2014; Qu & Richtárik, 2014).
Typically, the theoretical influence of using the importance
sampling is in the replacement of the maximum of certain
data-dependent quantities in the complexity bound by the
average.
Adaptivity. Despite the striking developments in the field,
there is virtually no literature on methods using an adaptive choice of the probabilities. We are aware of a few
pieces of work; but all resort to heuristics unsupported by
theory (Glasmachers & Dogan, 2013; Lukasewitz, 2013;
Schaul et al., 2013; Banks-Watson, 2012; Loshchilov et al.,
2011), which unfortunately also means that the methods
are sometimes effective, and sometimes not. We observe
that in the primal-dual framework we consider, each
dual variable can be equipped with a natural measure
of progress which we call “dual residue”. We propose
that the selection probabilities be constructed based on
these quantities.
Outline: In Section 2 we summarize the contributions of
our work. In Section 3 we describe our first, theoretical
methods (Algorithm 1: AdaSDCA) and describe the intuition behind it. In Section 4 we provide convergence analysis. In Section 5 we introduce AdaSDCA+ (Algorithm 2)
: an variant of AdaSDCA (Algorithm 1) containing heuristic elements which make it efficiently implementable. We
conclude with numerical experiments in Section 6. Technical proofs and additional numerical experiments can be
found in the supplementary materials.

2. Contributions
We now briefly highlight the main contributions of this
work.
Two algorithms with adaptive probabilities. We propose
two new stochastic dual ascent algorithms: AdaSDCA (Algorithm 1) and AdaSDCA+ (Algorithm 2) for solving (1)
and its dual problem (2). The novelty of our algorithms is
in adaptive choice of the probability distribution over the
dual coordinates.
Complexity analysis. We provide a convergence rate analysis for the first method, showing that AdaSDCA enjoys
better rate than the best known rate for SDCA with a
fixed sampling (Zhao & Zhang, 2015; Qu et al., 2014).
The probabilities are proportional to a certain measure of
dual suboptimality associated with each variable.

Stochastic Dual Coordinate Ascent with Adaptive Probabilities

Practical method. AdaSDCA requires the same computational effort per iteration as the batch gradient algorithm.
To solve this issue, we propose AdaSDCA+ (Algorithm 2):
an efficient heuristic variant of the AdaSDCA. The computational effort of the heuristic method in a single iteration is low, which makes it very competitive with methods
based on importance sampling, such as IProx-SDCA (Zhao
& Zhang, 2015). We support this with computational experiments in Section 6.

3. The Algorithm: AdaSDCA
It is well known that the optimal primal-dual pair
(w∗ , α∗ ) ∈ Rd × Rn satisfies the following optimality conditions:


1
Aα∗
(5)
w∗ = ∇g ∗
λn
def

∗
αi∗ = −∇φi (A>
i w ), ∀i ∈ [n] = {1, . . . , n},

(6)

where A is the d-by-n matrix with columns A1 , . . . , An .
Definition 1 (Dual residue). The dual residue, κ =
(κ1 , . . . , κn ) ∈ Rn , associated with (w, α) is given by:
def

κi = αi + ∇φi (A>
i w).

(7)

Algorithm 1 AdaSDCA
1
0
n
0
0
Init: vi = A>
i Ai for i ∈ [n]; α ∈ R ; ᾱ = λn Aα
for t ≥ 0 do
Primal update: wt = ∇g ∗ (ᾱt )
Set: αt+1 = αt
t
Compute residue κt : κti = αit +∇φi (A>
i w ), ∀i ∈ [n]
t
Compute probability distribution p coherent with κt
Generate random it ∈ [n] according to pt
Compute:

∆αitt = arg max −φ∗it (−(αitt + ∆))
∆∈R
	
vit
t
2
−A>
it w ∆ − 2λn |∆|
Dual update: αit+1
= αitt + ∆αitt
t
Average update: ᾱt = ᾱt +
end for
Output: wt , αt

∆αtit
λn

Ai t

Lemma 3. Consider the AdaSDCA algorithm during iteration t ≥ 0 and assume that pt is coherent with κt . Then



Et D(αt+1 ) − D(αt ) − θ P (wt ) − D(αt )


θ X θ(vi + nλγ)
≥−
−
nλγ
|κti |2 ,
(8)
2λn2
pti
i∈It

for arbitrary
Note, that κti = 0 if and only if αi satisfies (5). This motivates the design of AdaSDCA (Algorithm 1) as follows:
whenever |κti | is large, the ith dual coordinate αi is suboptimal and hence should be updated more often.
Definition 2 (Coherence). We say that probability vector
pt ∈ Rn is coherent with the dual residue κt if for all i ∈
[n] we have
κti 6= 0 ⇒ pti > 0.
Alternatively, pt is coherent with k t if for
def

It = {i ∈ [n] : κti 6= 0} ⊆ [n].
we have mini∈It pti > 0.
AdaSDCA is a stochastic dual coordinate ascent method,
with an adaptive probability vector pt , which could potentially change at every iteration t. The primal and
dual update rules are exactly the same as in standard
SDCA (Shalev-Shwartz & Zhang, 2013b), which instead
uses uniform sampling probability at every iteration and
does not require the computation of the dual residue κ.
Our first result highlights a key technical tool which ultimately leads to the development of good adaptive sampling distributions pt in AdaSDCA. For simplicity we denote by Et the expectation with respect to the random index
it ∈ [n] generated at iteration t.

0 ≤ θ ≤ min pti .
i∈It

(9)

Proof. Lemma 3 is proved similarly to Lemma 2 in (Zhao
& Zhang, 2015), but in a slightly more general setting. For
completeness, we provide the proof in the supplementary
materials.
Lemma 3 plays a key role in the analysis of stochastic
dual coordinate methods (Shalev-Shwartz & Zhang, 2013b;
Zhao & Zhang, 2015; Shalev-Shwartz & Zhang, 2013a).
Indeed, if the right-hand side of (8) is positive, then the primal dual error P (wt ) − D(αt ) can be bounded by the expected dual ascent Et [D(αt+1 ) − D(αt )] times 1/θ, which
yields the contraction of the dual error at the rate of 1 − θ
(see Theorem 7). In order to make the right-hand side of (8)
positive we can take any θ smaller than θ(κt , pt ) where the
function θ(·, ·) : Rn+ × Rn+ → R is defined by:
2
i:κi 6=0 |κi |
.
−1
2
i:κi 6=0 pi |κi | (vi + nλγ)

θ(κ, p) ≡ P

nλγ

P

(10)

We also need to make sure that 0 ≤ θ ≤ mini∈It pti in
order to apply Lemma 3. A “good” adaptive probability pt
should then be the solution of the following optimization
problem:

Stochastic Dual Coordinate Ascent with Adaptive Probabilities

4. Convergence results
max

p∈Rn
+

s.t.

θ(κt , p)
n
X

(11)

pi = 1

i=1

pi
θ(κt , p) ≤ min
t

In this section we present our theoretical complexity results
for AdaSDCA. The main results are formulated in Theorem 7, covering the general case, and in Theorem 11 in the
special case when {φi }ni=1 are all quadratic.
4.1. General loss functions

i:κi 6=0

We derive the convergence result from Lemma 3.
A feasible solution to (11) is the importance sampling (also
known as optimal serial sampling) p̄ defined by:
def

vi + nλγ
, ∀i ∈ [n],
j=1 (vj + nλγ)

p̄i = Pn

(12)

which was proposed in (Zhao & Zhang, 2015) to obtain
proximal stochastic dual coordinate ascent method with
importance sampling (IProx-SDCA). The same optimal
probability vector was also deduced, via different means
and in a more general setting in (Qu et al., 2014). Note that
in this special case, since pt is independent of the residue
κt , the computation of κt is unnecessary and hence the
complexity of each iteration does not scale up with n.
It seems difficult to identify other feasible solutions to
program (11) apart from p∗ , not to mention solve it exactly. However, by relaxing the constraint θ(κt , p) ≤
mini:κti 6=0 pi , we obtain an explicit optimal solution.

Proposition 6. Let t ≥ 0. If mini∈It pti > 0 and
θ(κt , pt ) ≤ mini∈It pti , then



Et D(αt+1 ) − D(αt ) ≥ θ(κt , pt ) P (wt ) − D(αt ) .
Proof. This follows directly from Lemma 3 and the fact
that the right-hand side of (8) equals 0 when θ = θ(κt , pt ).
Theorem 7. Consider AdaSDCA. If at each iteration t ≥
0, mini∈It pti > 0 and θ(κt , pt ) ≤ mini∈It pti , then
E[P (wt ) − D(αt )] ≤

for all t ≥ 0 where
def

θ̃t =

Lemma 4. The optimal solution p∗ (κt ) of
max

p∈Rn
+

s.t.

θ(κt , p)
n
X

(13)

t

1 Y
(1 − θ̃k ) D(α∗ ) − D(α0 ) ,
θ̃t k=0
(15)

E[θ(κt , pt )(P (wt ) − D(αt ))]
.
E[P (wt ) − D(αt )]

Proof. By Proposition 6, we know that
E[D(αt+1 ) − D(αt )] ≥ E[θ(κt , pt )(P (wt ) − D(αt ))]

pi = 1

(16)

i=1

= θ̃t E[P (wt ) − D(αt )]

is:

∗

(p∗ (κt ))i =

(17)

t

≥ θ̃t E[D(α ) − D(α )],

√

|κt | vi + nλγ
,
Pn i t p
j=1 |κj | vj + nλγ

(16)

∀i ∈ [n].

(14)

Proof. The proof is deferred to the supplementary materials.
The suggestion made by (14) is clear: we should update
more often those dual coordinates αi which have large absolute dual residue |κti | and/or large Lipschitz constant vi .
If we let pt = p∗ (κt ) and θ = θ(κt , pt ), the constraint (9)
may not be satisfied, in which case (8) does not necessarily hold. However, as shown by the next lemma, the
constraint (9) is not required for obtaining (8) when all the
functions {φi }i are quadratic.
Lemma 5. Suppose that all {φi }i are quadratic. Let t ≥ 0.
If mini∈It pti > 0, then (8) holds for any θ ∈ [0, +∞).
The proof is deferred to the supplementary materials.

whence
E[D(α∗ ) − D(αt+1 )] ≤ (1 − θ̃t ) E[D(α∗ ) − D(αt )].
Therefore,
E[D(α∗ ) − D(αt )] ≤

t
Y


(1 − θ̃k ) D(α∗ ) − D(α0 ) .

k=0

By plugging the last bound into (17) we get the bound on
the primal dual error:
1
E[D(αt+1 ) − D(αt )]
θ̃t
1
≤
E[D(α∗ ) − D(αt )]
θ̃t
t

1 Y
≤
(1 − θ̃k ) D(α∗ ) − D(α0 ) .
θ̃t k=0

E[P (wt ) − D(αt )] ≤

Stochastic Dual Coordinate Ascent with Adaptive Probabilities

As mentioned in Section 3, by letting every sampling probability pt be the importance sampling (optimal serial sampling) p∗ defined in (12), AdaSDCA reduces to IProxSDCA proposed in (Zhao & Zhang, 2015). The convergence theory established for IProx-SDCA in (Zhao &
Zhang, 2015), which can also be derived as a direct corollary of our Theorem 7, is stated as follows.
Theorem 8 ((Zhao & Zhang, 2015)). Consider AdaSDCA
with pt = p̄ defined in (12) for all t ≥ 0. Then
E[P (wt ) − D(αt )] ≤
where


1
(1 − θ̄)t D(α∗ ) − D(α0 ) ,
θ̄
nλγ
.
(v
i=1 i + λγn)

θ̄ = Pn

The next corollary suggests that a better convergence rate
than IProx-SDCA can be achieved by using properly
chosen adaptive sampling probability.
Corollary 9. Consider AdaSDCA. If at each iteration t ≥
0, pt is the optimal solution of (11), then (15) holds and
θ̃t ≥ θ̄ for all t ≥ 0.
However, solving (11) requires large computational effort,
because of the dimension n and the non-convex structure of
the program. We show in the next section that when all the
loss functions {φi }i are quadratic, then we can get better
convergence rate in theory than IProx-SDCA by using the
optimal solution of (13).

solution of (13), which has a closed form (14), then (15)
holds for


P
nλγ i |κti |2
t
t
E P t√
2 (P (w ) − D(α ))
[ i |κj | vj +nλγ ]
θ̃t =
≥ θ̄.
E[P (wt ) − D(αt )]
Remark 13. The latter inequality follows easily from
Cauchy-Schwarz inequality. The gap between θ̃t of our
method and θ̄ of IProx-SDCA depends on the proportion
of the dual residue κt . We leave a quantitative comparison
between θ̃t and θ̄ for future work.

5. Efficient heuristic variant
Corollary 9 and 12 suggest how to choose adaptive sampling probability in AdaSDCA which yields a theoretical
convergence rate at least as good as IProx-SDCA (Zhao &
Zhang, 2015). However, there are two main implementation issues of AdaSDCA:
1. The update of the dual residue κt at each iteration
costs O(nnz(A)) where nnz(A) is the number of
nonzero elements of the matrix A;
2. We do not know how to compute the optimal solution
of (11).
In this section, we propose a heuristic variant of AdaSDCA,
which avoids the above two issues while staying close to
the ’good’ adaptive sampling distribution.

4.2. Quadratic loss functions
The main difficulty of solving (11) comes from the inequality constraint, which originates from (9). In this section we
mainly show that the constraint (9) can be released if all
{φi }i are quadratic.
Proposition 10. Suppose that all {φi }i are quadratic. Let
t ≥ 0. If mini∈It pti > 0, then



Et D(αt+1 ) − D(αt ) ≥ θ(κt , pt ) P (wt ) − D(αt ) .
Proof. This is a direct consequence of Lemma 5 and the
fact that the right-hand side of (8) equals 0 when θ =
θ(κt , pt ).
Theorem 11. Suppose that all {φi }i are quadratic. Consider AdaSDCA. If at each iteration t ≥ 0, mini∈It pti > 0,
then (15) holds for all t ≥ 0.
Proof. We only need to apply Proposition 10. The rest of
the proof is the same as in Theorem 7.
Corollary 12. Suppose that all {φi }i are quadratic. Consider AdaSDCA. If at each iteration t ≥ 0, pt is the optimal

5.1. Description of Algorithm
AdaSDCA+ has the same structure as AdaSDCA with a
few important differences.
Epochs AdaSDCA+ is divided into epochs of length n. At
the beginning of every epoch, sampling probabilities are
computed according to one of two options. During each
epoch the probabilities are cheaply updated at the end of
every iteration to approximate the adaptive model. The intuition behind is as follows. After i is sampled and the
dual coordinate αi is updated, the residue κi naturally decreases. We then decrease also the probability that i is
chosen in the next iteration, by setting pt+1 to be proportional to (pt1 , . . . pti−1 , pti /m, pti+1 , . . . , ptn ). By doing this
we avoid the computation of κ at each iteration (issue 1)
which costs as much as the full gradient algorithm, while
following closely the changes of the dual residue κ. We reset the adaptive sampling probability after every epoch of
length n.
Parameter m The setting of parameter m in AdaSDCA+
directly affects the performance of the algorithm. If m is
too large, the probability of sampling the same coordinate

Stochastic Dual Coordinate Ascent with Adaptive Probabilities

Algorithm 2 AdaSDCA+
Parameter a number m > 1
1
Initialization Choose α0 ∈ Rn , set ᾱ0 = λn
Aα0
for t ≥ 0 do
Primal update: wt = ∇g ∗ (ᾱt )
Set: αt+1 = αt
if mod (t, n) == 0 then
Option I: Adaptive sampling:
t
> t
Compute: κti =
√ αi + ∇φi (Ai w ), ∀i ∈ [n]
t
t
Set: pi ∼ |κi | vi + nλγ, ∀i ∈ [n]
Option II: Optimal importance sampling:
Set: pti ∼ (vi + nλγ), ∀i ∈ [n]
end if
Generate random it ∈ [n] according to pt
Compute:

∆αitt = arg max −φ∗it (−(αitt + ∆))
∆∈R
	
vit
t
2
−A>
it w ∆ − 2λn |∆|
= αitt + ∆αitt
Dual update: αit+1
t
∆α

Average update: ᾱt = ᾱt + λnit Ait
Probability update:
pt+1
∼ ptit /m, pt+1
∼ ptj , ∀j 6= it
it
j
end for
Output: wt , αt

twice during an epoch will be very small. This will result in
a random permutation through all coordinates every epoch.
On the other hand, for m too small the coordinates having larger probabilities at the beginning of an epoch could
be sampled more often than it should, even after their corresponding dual residues become sufficiently small. We
don’t have a definitive rule on the choice of m and we leave
this to future work. Experiments with different choices of
m can be found in Section 6.
Option I & Option II At the beginning of each epoch, one
can choose between two options for resetting the sampling
probability. Option I corresponds to the optimal solution
of (13), given by the closed form (14). Option II is the
optimal serial sampling probability (12), the same as the
one used in IProx-SDCA (Zhao & Zhang, 2015). However,
AdaSDCA+ differs significantly with IProx-SDCA since
we also update iteratively the sampling probability, which
as we show through numerical experiments yields a faster
convergence than IProx-SDCA.
5.2. Computational cost
Sampling and probability update During the algorithm
we sample i ∈ [n] from non-uniform probability distribution pt , which changes at each iteration. This process
can be done efficiently using the Random Counters algorithm introduced in Section 6.2 of (Nesterov, 2012), which
takes O(n log(n)) operations to create the probability tree

Table 1. One epoch computational cost of different algorithms
A LGORITHM

COST OF AN EPOCH

O(nnz)

SDCA& QUARTZ( UNIFORM )

O(nnz +n log(n))

IP ROX -SDCA

O(n · nnz)

A DA SDCA

O(nnz +n log(n))

A DA SDCA+

and O(log(n)) operations to sample from the distribution
or change one of the probabilities.
Total computational cost We can compute the computational cost of one epoch. At the beginning of an epoch,
we need O(nnz) operations to calculate the dual residue
κ. Then we create a probability tree using O(n log(n))
operations. At each iteration we need O(log(n)) operations to sample a coordinate, O(nnz /n) operations to calculate the update to α and a further O(log(n)) operations
to update the probability tree. As a result an epoch needs
O(nnz +n log(n)) operations. For comparison purpose we
list in Table 1 the one epoch computational cost of comparable algorithms.

6. Numerical Experiments
In this section we present results of numerical experiments.
Additional experiments can be found in the supplementary
materials.
6.1. Loss functions
We test AdaSDCA and AdaSDCA+, SDCA, and IProxSDCA for two different types of loss functions {φi }ni=1 :
quadratic loss and smoothed Hinge loss. Let y ∈ Rn be the
vector of labels. The quadratic loss is given by
φi (x) =

1
(x − yi )2
2γ

and the smoothed Hinge loss is:


0
φi (x) = 1 − yi x − γ/2

 (1−yi x)2
2γ

yi x ≥ 1
yi x ≤ 1 − γ
otherwise,

In both cases we use L2 -regularizer, i.e.,
g(w) =

1
kwk2 .
2

Quadratic loss functions appear usually in regression problems, and smoothed Hinge loss can be found in linear support vector machine (SVM) problems (Shalev-Shwartz &
Zhang, 2013a).

Stochastic Dual Coordinate Ascent with Adaptive Probabilities
Table 2. Dimensions and nonzeros of the datasets
d

n

nnz /(nd)

300

49, 749

3.9%

100, 000

800

0.9%

112

8, 124

18.8%

COV 1

54

581, 012

22%

IJCNN 1

22

49, 990

41%

DATASET
W8A
DOROTHEA
MUSHROOMS

Acknowledgements
The authors thank Jakub Konečný for his valuable suggestions.

6.2. Numerical results
We used 5 different datasets: w8a, dorothea, mushrooms,
cov1 and ijcnn1 (see Table 2).
In all our experiments we used γ = 1 and λ = 1/n.
AdaSDCA The results of the theory developed in Section 4
can be observed on Figure 1. AdaSDCA needs the least
amount of iterations to converge, confirming the theoretical
result.

Figure 1. w8a dataset d = 300, n = 49749, Quadratic loss with
L2 regularizer, comparing number of iterations with known algorithms

AdaSDCA+ V.S. others We can observe on Figure 6 and
Figure 7, that both options of AdaSDCA+ outperforms
SDCA and IProx-SDCA, in terms of number of iterations,
for quadratic loss functions and for smoothed Hinge loss
functions. One can observe similar results in terms of time
through Figure 2 to Figure 5.
Option I V.S. Option II Despite the fact that Option I is
not theoretically supported for smoothed hinge loss, it still
converges faster than Option II on every dataset and for every loss function. The biggest difference can be observed
on Figure 5, where Option I converges to the machine precision in just 15 seconds.

Figure 2. dorothea dataset d = 100000, n = 800, Quadratic loss
with L2 regularizer, comparing real time with known algorithms

Different choices of m To show the impact of different
choices of m on the performance of AdaSDCA+, in Figure 8 and 9 we compare the results of the two options of
AdaSDCA+ using different m equal to 2, 10 and 50. It is
hard to draw a clear conclusion here because clearly the optimal m shall depend on the dataset and the problem type.
Remark 14. Recently there has been several work on applying Nesterov’s acceleration technique to SDCA so that
1
the iteration complexity can be improved from Õ(n+ λγ
) to
q
n
Õ(n + λγ
), see (Shalev-Shwartz & Zhang, 2013a; 2015;
Zhang & Xiao, 2015; Lin et al., 2014). In all our numerical experiments, the condition number 1/λγ equals n, in
which case SDCA and accelerated SDCA are comparable.
It is important to notice that the adaptive sampling technique developed in this paper is an independent speedup
trick to the acceleration technique used in the above cited
papers. Whether the two approaches can be combined together is left for future work.

Figure 3. ijcnn1 dataset d = 22, n = 49990, Quadratic loss with
L2 regularizer, comparing real time with known algorithms

Stochastic Dual Coordinate Ascent with Adaptive Probabilities

Figure 4. mushrooms dataset d = 112, n = 8124, Smooth Hinge
loss with L2 regularizer, comparing real time with known algorithms

Figure 7. w8a dataset d = 300, n = 49749, Smooth Hinge loss
with L2 regularizer, comparing number of iterations with known
algorithms

Figure 5. cov1 dataset d = 54, n = 581012, Smooth Hinge loss
with L2 regularizer, comparing real time with known algorithms

Figure 8. cov1 dataset d = 54, n = 581012, Quadratic loss with
L2 regularizer, comparison of different choices of the constant m

Figure 6. ijcnn1 dataset d = 22, n = 49990, Quadratic loss with
L2 regularizer, comparing number of iterations with known algorithms

Figure 9. mushrooms dataset d = 112, n = 8124, Smooth Hinge
loss with L2 regularizer, comparison of different choices of the
constant m

Stochastic Dual Coordinate Ascent with Adaptive Probabilities

References
Agarwal, Alekh and Bottou, Leon. A lower bound for the
optimization of finite sums. ICML, 2015.
Banks-Watson, Alexander. New classes of coordinate descent methods. Master’s thesis, University of Edinburgh,
2012.
Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon. SAGA: A fast incremental gradient method with
support for non-strongly convex composite objectives.
Advances in Neural Information Processing Systems 27
(NIPS 2014), 2014.
Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research,
12(1):2121–2159, 2011.
Fercoq, Olivier and Richtárik, Peter. Accelerated, parallel
and proximal coordinate descent. SIAM Journal on Optimization (after minor revision), arXiv:1312.5799, 2013.

Konečný, Jakub, Qu, Zheng, and Richtárik, Peter. Semistochastic coordinate descent. arXiv:1412.6293, 2014b.
Lin, Qihang, Lu, Zhaosong, and Xiao, Lin. An accelerated
proximal coordinate gradient method and its application
to regularized empirical risk minimization. Technical
Report MSR-TR-2014-94, July 2014.
Liu, Ji and Wright, Stephen J. Asynchronous stochastic
coordinate descent: Parallelism and convergence properties. arXiv:1403.3862, 2014.
Loshchilov, I., Schoenauer, M., and Sebag, M. Adaptive Coordinate Descent. In et al., N. Krasnogor
(ed.), Genetic and Evolutionary Computation Conference (GECCO), pp. 885–892. ACM Press, July 2011.
Lukasewitz, Isabella. Block-coordinate frank-wolfe optimization. A study on randomized sampling methods,
2013.

Fercoq, Olivier and Richtárik, Peter. Smooth minimization
of nonsmooth functions by parallel coordinate descent.
arXiv:1309.5885, 2013.

Mahajan, Dhruv, Keerthi, S. Sathiya, and Sundararajan, S.
A distributed block coordinate descent method for training l1 regularized linear classifiers. arXiv:1405.4544,
2014.

Fercoq, Olivier, Qu, Zheng, Richtárik, Peter, and Takáč,
Martin. Fast distributed coordinate descent for minimizing non-strongly convex losses. IEEE International
Workshop on Machine Learning for Signal Processing,
2014.

Mairal, Julien. Incremental majorization-minimization optimization with application to large-scale machine learning. SIAM Journal on Optimization, 25(2):829–855,
2015. doi: 10.1137/140957639. URL http://dx.
doi.org/10.1137/140957639.

Fountoulakis, Kimon and Tappenden, Rachael. Robust
block coordinate descent. arXiv:1407.7573, 2014.
Glasmachers, Tobias and Dogan, Urun. Accelerated coordinate descent with adaptive coordinate frequencies.
In Asian Conference on Machine Learning, pp. 72–86,
2013.
Jaggi, Martin, Smith, Virginia, Takáč, Martin, Terhorst,
Jonathan, Krishnan, Sanjay, Hofmann, Thomas, and Jordan, Michael I. Communication-efficient distributed
dual coordinate ascent. In Advances in Neural Information Processing Systems 27, pp. 3068–3076. Curran
Associates, Inc., 2014.
Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
NIPS, 2013.

Mareček, Jakub, Richtárik, Peter, and Takáč, Martin. Distributed block coordinate descent for minimizing partially separable functions. arXiv:1406.0328, 2014.
Necoara, Ion and Patrascu, Andrei. A random coordinate
descent algorithm for optimization problems with composite objective function and linear coupled constraints.
Computational Optimization and Applications, 57:307–
337, 2014.
Necoara, Ion, Nesterov, Yurii, and Glineur, Francois. Efficiency of randomized coordinate descent methods on
optimization problems with linearly coupled constraints.
Technical report, Politehnica University of Bucharest,
2012.

Konečný, Jakub and Richtárik, Peter. S2GD: Semistochastic gradient descent methods. arXiv:1312.1666,
2014.

Nemirovski, Arkadi, Juditsky, Anatoli, Lan, Guanghui, and
Shapiro, Alexander. Robust stochastic approximation
approach to stochastic programming. SIAM Journal on
Optimization, 19(4):1574–1609, 2009.

Konečný, Jakub, Lu, Jie, Richtárik, Peter, and Takáč, Martin. mS2GD: Mini-batch semi-stochastic gradient descent in the proximal setting. arXiv:1410.4744, 2014a.

Nesterov, Yurii. Efficiency of coordinate descent methods
on huge-scale optimization problems. SIAM Journal on
Optimization, 22(2):341–362, 2012.

Stochastic Dual Coordinate Ascent with Adaptive Probabilities

Qu, Zheng and Richtárik, Peter. Coordinate descent methods with arbitrary sampling I: Algorithms and complexity. arXiv:1412.8060, 2014.

Takáč, Martin, Bijral, Avleen Singh, Richtárik, Peter, and
Srebro, Nathan. Mini-batch primal and dual methods for
svms. CoRR, abs/1303.2314, 2013.

Qu, Zheng and Richtárik, Peter. Coordinate Descent with
Arbitrary Sampling II: Expected Separable Overapproximation. ArXiv e-prints, 2014.

Tappenden, Rachael, Richtárik, Peter, and Gondzio, Jacek.
Inexact block coordinate descent method: complexity
and preconditioning. arXiv:1304.5530, 2013.

Qu, Zheng, Richtárik, Peter, and Zhang, Tong. Randomized Dual Coordinate Ascent with Arbitrary Sampling.
arXiv:1411.5873, 2014.

Tappenden, Rachael, Richtárik, Peter, and Büke, Burak.
Separable approximations and decomposition methods
for the augmented lagrangian. Optimization Methods
and Software, 2014.

Richtárik, Peter and Takáč, Martin. Distributed coordinate descent method for learning with big data.
arXiv:1310.2059, 2013a.
Richtárik, Peter and Takáč, Martin. On optimal probabilities in stochastic coordinate descent methods.
arXiv:1310.3438, 2013b.
Richtárik, Peter and Takáč, Martin. Iteration complexity of
randomized block-coordinate descent methods for minimizing a composite function. Mathematical Programming, 144(2):1–38, 2014.
Richtárik, Peter and Takáč, Martin. Parallel coordinate
descent methods for big data optimization. Mathematical Programming, pp. 1–52, 2015. ISSN 0025-5610.
doi: 10.1007/s10107-015-0901-6. URL http://dx.
doi.org/10.1007/s10107-015-0901-6.
Schaul, Tom, Zhang, Sixin, and LeCun, Yann. No more
pesky learning rates. Journal of Machine Learning Research, 3(28):343–351, 2013.

Trofimov, Ilya and Genkin, Alexander. Distributed coordinate descent for l1-regularized logistic regression.
arXiv:1411.6520, 2014.
Wright, Stephen J.
Coordinate descent algorithms.
Technical report, 2014.
URL http:
//www.optimization-online.org/DB_
FILE/2014/12/4679.pdf.
Xiao, Lin and Zhang, Tong. A proximal stochastic gradient method with progressive variance reduction. SIAM
Journal on Optimization, 24(4):2057–2075, 2014. doi:
10.1137/140961791. URL http://dx.doi.org/
10.1137/140961791.
Zhang, Yuchen and Xiao, Lin. Stochastic primal-dual coordinate method for regularized empirical risk minimization. ICML, 2015.
Zhao, Peilin and Zhang, Tong. Stochastic optimization
with importance sampling. ICML, 2015.

Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Minimizing finite sums with the stochastic average gradient.
arXiv:1309.2388, 2013.

Zhao, Tuo, Liu, Han, and Zhang, Tong. A general theory
of pathwise coordinate optimization. arXiv:1412.7477,
2014a.

Shalev-Shwartz, Shai and Tewari, Ambuj. Stochastic methods for `1 -regularized loss minimization. Journal of Machine Learning Research, 12:1865–1892, 2011.

Zhao, Tuo, Yu, Mo, Wang, Yiming, Arora, Raman, and
Liu, Han. Accelerated mini-batch randomized block coordinate descent method. In Ghahramani, Z., Welling,
M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q.
(eds.), Advances in Neural Information Processing Systems 27, pp. 3329–3337. Curran Associates, Inc., 2014b.

Shalev-Shwartz, Shai and Zhang, Tong. Proximal stochastic dual coordinate ascent. arXiv:1211.2717, 2012.
Shalev-Shwartz, Shai and Zhang, Tong. Accelerated minibatch stochastic dual coordinate ascent. In Advances
in Neural Information Processing Systems 26, pp. 378–
385. 2013a.
Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual
coordinate ascent methods for regularized loss. Journal
of Machine Learning Research, 14(1):567–599, 2013b.
Shalev-Shwartz, Shai and Zhang, Tong. Accelerated proximal stochastic dual coordinate ascent for regularized loss
minimization. to appear in Mathematical Programming,
2015.

