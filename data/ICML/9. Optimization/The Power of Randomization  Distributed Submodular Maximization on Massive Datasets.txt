The Power of Randomization: Distributed Submodular Maximization on
Massive Datasets
Rafael Barbosa1
Department of Computer Science and DIMAP, University of Warwick

RAFAEL @ DCS . WARWICK . AC . UK

Alina Ene
Department of Computer Science and DIMAP, University of Warwick

A.E NE @ DCS . WARWICK . AC . UK

Huy Le Nguyen
Simons Institute, University of California, Berkeley

HLNGUYEN @ CS . PRINCETON . EDU

Justin Ward
Department of Computer Science and DIMAP, University of Warwick

Abstract
A wide variety of problems in machine learning,
including exemplar clustering, document summarization, and sensor placement, can be cast
as constrained submodular maximization problems. Unfortunately, the resulting submodular
optimization problems are often too large to be
solved on a single machine. We consider a distributed, greedy algorithm that combines previous approaches with randomization. The result
is an algorithm that is embarrassingly parallel
and achieves provable, constant factor, worstcase approximation guarantees. In our experiments, we demonstrate its efficiency in large
problems with different kinds of constraints with
objective values always close to what is achievable in the centralized setting.

1. Introduction
A set function f : 2V ! R 0 on a ground set V is submodular if f (A) + f (B) f (A \ B) + f (A [ B) for any
two sets A, B ✓ V . Several problems of interest can be
modeled as maximizing a submodular objective function
subject to certain constraints:
max f (A) subject to A 2 C,
where C ✓ 2V is the family of feasible solutions. Indeed, the general meta-problem of optimizing a constrained
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

J.D.WARD @ DCS . WARWICK . AC . UK

submodular function captures a wide variety of problems
in machine learning applications, including exemplar clustering, document summarization, sensor placement, image
segmentation, maximum entropy sampling, and feature selection.
At the same time, in many of these applications, the amount
of data that is collected is quite large and it is growing at
a very fast pace. For example, the wide deployment of
sensors has led to the collection of large amounts of measurements of the physical world. Similarly, medical data
and human activity data are being captured and stored at an
ever increasing rate and level of detail. This data is often
high-dimensional and complex, and it needs to be stored
and processed in a distributed fashion.
In these settings, it is apparent that the classical algorithmic approaches are no longer suitable and new algorithmic
insights are needed in order to cope with these challenges.
The algorithmic challenges stem from the following competing demands imposed by huge datasets: the computations need to process the data that is distributed across several machines using a minimal amount of communication
and synchronization across the machines, and at the same
time deliver solutions that are competitive with the centralized solution on the entire dataset.
The main question driving the current work is whether
these competing goals can be reconciled. More precisely,
can we deliver very good approximate solutions with minimal communication overhead? Perhaps surprisingly, the
answer is yes; there is a very simple distributed greedy
algorithm that is embarrassingly parallel and it achieves
1

The authors are listed alphabetically.

The Power of Randomization: Distributed Submodular Maximization on Massive Datasets

provable, constant factor, worst-case approximation guarantees. Our algorithm can be easily implemented in a parallel model of computation such as MapReduce (Dean &
Ghemawat, 2004).
1.1. Background and Related Work
In the MapReduce model, there are m independent machines. Each of the machines has a limited amount of memory available. In our setting, we assume that the data is
much larger than any single machine’s memory and so must
be distributed across all of the machines. At a high level, a
MapReduce computation proceeds in several rounds. In a
given round, the data is shuffled among the machines. After
the data is distributed, each of the machines performs some
computation on the data that is available to it. The output
of these computations is either returned as the final result
or becomes the input to the next MapReduce round. We
emphasize that the machines can only communicate and
exchange data during the shuffle phase.
In order to put our contributions in context, we briefly discuss two distributed greedy algorithms that achieve complementary trade-offs in terms of approximation guarantees
and communication overhead.
Mirzasoleiman et al. (2013) give a distributed algorithm,
called G REE D I, for maximizing a monotone submodular function subject to a cardinality constraint. The
G REE D I algorithm partitions the data arbitrarily on the
machines and on each machine it then runs the classical G REEDY algorithm to select a feasible subset of the
items assigned to that machine. The G REEDY solutions
on these machines are then placed on a single machine
and the G REEDY algorithm is used once more to select
the final solution from amongst the resulting set of items.
The G REE D I algorithm is very simple and embarrassingly
2
parallel, but
p its worst-case approximation guarantee is
1/⇥(min{ k, m}), where m is the number of machines
and k is the cardinality constraint. Mirzasoleiman et al.
show that the G REE D I algorithm achieves very good approximations for datasets with geometric structure, and
performs well in practice for a wide variety of experiments.
Kumar et al. (2013) give distributed algorithms for maximizing a monotone submodular function subject to a cardinality or more generally, a matroid constraint. Their algorithm combines the Threshold Greedy algorithm of (Gupta
et al., 2010) with a sample and prune strategy. In each
round, the algorithm samples a small subset of the elements
2
Mirzasoleiman et al. (2013) give a family of instances where
the approximation achieved is only 1/ min {k, m} if the solution
picked on each of the machines is the optimal solution for the set
of items on the machine. These instances are not hard for the
G REE D I algorithm. We show in p
the supplement that the G REE D I
algorithm achieves a 1/⇥(min{ k, m}) approximation.

that fit on a single machine and runs the Threshold Greedy
algorithm on the sample in order to obtain a feasible solution. This solution is then used to prune some of the
elements in the dataset and reduce the size of the ground
set. The S AMPLE &P RUNE algorithms achieve constant
factor approximation guarantees but they incur a higher
communication overhead. For a cardinality constraint, the
number of rounds is a constant but for more general constraints such as a matroid constraint, the number of rounds
is ⇥(log ), where is the maximum increase in the objective due to a single element. The maximum increase
can be much larger than even the number of elements in
the entire dataset, which makes the approach infeasible for
massive datasets.
On the negative side, Indyk et al. (2014) studied coreset
approaches to develop distributed algorithms for finding
representative and yet diverse subsets in large collections.
While succeeding in several measures, they also showed
that their approach provably does not work for k-coverage,
which is a special case of submodular maximization with a
cardinality constraint.
1.2. Our Contribution
In this paper, we analyze a variant of the distributed
G REE D I algorithm of (Mirzasoleiman et al., 2013), and
show that one can achieve both the communication efficiency of the G REE D I algorithm and a provable, constant
factor approximation guarantee. Our analysis relies crucially on the following modification: instead of partitioning
the dataset arbitrarily onto the machines, we perform this
initial partitioning randomly. Our analysis thus provides
some theoretical justification for the very good empirical
performance of the G REE D I algorithm that was established
previously in the extensive experiments of (Mirzasoleiman
et al., 2013). Moreover, we show that this approach delivers provably good performance in much wider settings than
originally envisioned.
The G REE D I algorithm was originally studied in the special case of monotone submodular maximization under
a cardinality constraint. In contrast, our analysis holds
for any hereditary constraint. Specifically, we show that
the randomized variant of the G REE D I algorithm achieves
a constant factor approximation for any hereditary, constrained problem for which the classical (centralized)
G REEDY algorithm achieves a constant factor approximation. This is the case not only for cardinality constraints,
but also for matroid constraints, knapsack constraints, and
p-system constraints (Jenkyns, 1976), which generalize the
intersection of p matroid constraints. Table 1 gives the approximation ratio ↵ obtained by the G REEDY algorithm on
a variety of problems, and the corresponding constant factor obtained by the randomized G REE D I algorithm.

The Power of Randomization: Distributed Submodular Maximization on Massive Datasets
Table 1. New approximation bounds for randomized G REE D I for constrained monotone and non-monotone submodular maximization

Constraint

Centralized G REEDY

cardinality

1
e

1

matroid

1
2

knapsack

⇡ 0.35

(Fisher et al., 1978)

1
p+1

p-system

(Nemhauser et al., 1978)
(Wolsey, 1982)3
(Fisher et al., 1978)

Additionally, we show that if the greedy algorithm satisfies
a slightly stronger technical condition, then our approach
gives a constant factor approximation for constrained nonmonotone submodular maximization. The resulting approximation ratios for non-monotone maximization problems are given in the last column of Table 1.

MapReduce Model. In a MapReduce computation, the
data is represented as hkey, valuei pairs and it is distributed
across m machines. The computation proceeds in rounds.
In a given round, the data is processed in parallel on each of
the machines by map tasks that output hkey, valuei pairs.
These pairs are then shuffled by reduce tasks; each reduce
task processes all the hkey, valuei pairs with a given key.
The output of the reduce tasks either becomes the final output of the MapReduce computation or it serves as the input
of the next MapReduce round.
Submodularity. As noted in the introduction, a set function f : 2V ! R 0 is submodular if, for all sets A, B ✓ V ,
f (A) + f (B)

f (A [ B) + f (A \ B).

A useful alternative characterization of submodularity can
be formulated in terms of diminishing marginal gains.
Specifically, f is submodular if and only if:
f (A)

f (B [ {e})

for all A ✓ B ✓ V and e 2
/ B.
The Lovász extension f
function f is given by:
f (x) =

1
2 (1

G REE D I Non-Monotone

1
e)

1
10

1
4

1
10

⇡ 0.17

1
14

1
2(p+1)

1
2+4(p+1)

Lemma 1. Let S be a random set, and suppose that
E[1S ] = c · p (for c 2 [0, 1]). Then, E[f (S)] c · f (p).
Proof. We have:
E[f (S)] = E[f (1S )]
f (E[1S ]) = f (c · p)

1.3. Preliminaries

f (A [ {e})

G REE D I Monotone

: [0, 1]V ! R

E

✓2U (0,1)

[f ({i : xi

0

f (B)

of a submodular
✓})].

For any submodular function f , the Lovász extension f
satisfies the following properties: (1) f (1S ) = f (S) for
all S ✓ V , (2) f is convex, and (3) f (c · x) c · f (x)
for any c 2 [0, 1]. These three properties immediately give
the following simple lemma:
3
Wolsey’s algorithm satisfies all technical conditions required
for our analysis (in particular, those for Lemma 2).

c · f (p),

where the first equality follows from property (1), the first
inequality from property (2), and the final inequality from
property (3).
Hereditary Constraints. Our results hold quite generally
for any problem which can be formulated in terms of a
hereditary constraint. Formally, we consider the problem
max{f (S) : S ✓ V, S 2 I},

(1)

where f : 2V ! R 0 is a submodular function and
I ✓ 2V is a family of feasible subsets of V . We require
that I be hereditary in the sense that if some set is in I,
then so are all of its subsets. Examples of common hereditary families include cardinality constraints (I = {A ✓
V : |A|  k}), matroid constraints (I corresponds to the
collection independent sets
P of the matroid), knapsack constraints (I = {A ✓ V : i2A wi  b}), as well as combinations of such constraints. Given some constraint I ✓ 2V ,
we shall also consider restricted instances in which we are
presented only with a subset V 0 ✓ V , and must find a set
S ✓ V 0 with S 2 I that maximizes f . We say that an algorithm is an ↵-approximation for maximizing a submodular
function subject to a hereditary constraint I if, for any submodular function f : 2V ! R 0 and any subset V 0 ✓ V
the algorithm produces a solution S ✓ V 0 with S 2 I,
satisfying f (S)
↵ · f (OPT), where OPT 2 I is any
feasible subset of V 0 .

2. The Standard Greedy Algorithm
Before describing our general algorithm, let us recall the
standard greedy algorithm, G REEDY, shown in Algorithm
1. The algorithm takes as input hV, I, f i, where V is a
set of elements, I ✓ 2V is a hereditary constraint, represented as a membership oracle, and f : 2V ! R 0 is a

The Power of Randomization: Distributed Submodular Maximization on Massive Datasets

Algorithm 1 The standard greedy algorithm G REEDY
S
;
loop
Let C = {e 2 V \ S : S [ {e} 2 I}
Let e = arg maxe2C {f (S [ {e}) f (S)}
if C = ; or f (S [ {e}) f (S) < 0 then
return S
end if
end loop
non-negative submodular function, represented as a value
oracle. Given hV, I, f i, G REEDY iteratively constructs a
solution S 2 I by choosing at each step the element maximizing the marginal increase of f . For some A ✓ V , we let
G REEDY(A) denote the set S 2 I produced by the greedy
algorithm that considers only elements from A.
The greedy algorithm satisfies the following property:
Lemma 2. Let A ✓ V and B ✓ V be two disjoint subsets of V . Suppose that, for each element e 2 B, we have
G REEDY(A [ {e}) = G REEDY(A). Then G REEDY(A [
B) = G REEDY(A).
Proof. Suppose for contradiction that G REEDY(A [ B) 6=
G REEDY(A). We first note that, if G REEDY(A [ B) ✓ A,
then G REEDY(A [ B) = G REEDY(A); this follows from
the fact that each iteration of the Greedy algorithm chooses
the element with the highest marginal value whose addition
to the current solution maintains feasibility for I. Therefore, if G REEDY(A [ B) 6= G REEDY(A), the former solution contains an element of B. Let e be the first element
of B that is selected by Greedy on the input A [ B. Then
Greedy will also select e on the input A[{e}, which contradicts the fact that G REEDY(A [ {e}) = G REEDY(A).

3. A Randomized, Distributed Greedy
Algorithm for Monotone Submodular
Maximization
Algorithm. We now describe the specific variant of the
G REE D I algorithm of Mirzasoleiman et al. that we consider. The algorithm, shown in Algorithm 2, proceeds exactly as G REE D I, except we perform the initial partitioning of V randomly.4 Specifically, we suppose that each
e 2 V is assigned to a machine chosen independently
and uniformly at random. On each machine i, we execute
G REEDY(Vi ) to select a feasible subset Si of the elements
on that machine. In the second round, we place all of these
4

Indeed, this was the case in several of the experiments performed by (Mirzasoleiman et al., 2013), and so our results provide
some explanation for the gap between their worst-case bounds and
experimental performance.

Algorithm 2 The distributed algorithm R AND G REE D I
for e 2 V do
Assign e to a machine i chosen uniformly at random.
end for
Let Vi be the elements assigned to machine i
Run G REEDY
S (Vi ) on each machine i to obtain Si
Place S = i Si on machine 1
Run A LG(S) on machine 1 to obtain T
Let S 0 = arg maxi {f (Si )}
return arg max{f (T ), f (S 0 )}
selected subsets on a single machine, and run some algorithm A LG on this machine in order to select a final solution T . Finally, we return whichever is better: the final
solution T or the best solution amongst all the Si from the
first phase. We call the resulting algorithm R AND G REE D I,
to emphasize our assumption that the initial partitioning is
performed randomly.
Analysis. We devote the rest of this section to the analysis of the R AND G REE D I algorithm. Fix hV, I, f i, where
I ✓ 2V is a hereditary constraint, and f : 2V ! R 0
is any non-negative, monotone submodular function. Suppose that G REEDY is an ↵-approximation and A LG is a
-approximation for the associated constrained monotone
submodular maximization problem of the form (1). Let
n = |V | and suppose that OPT = arg maxA2I f (A) is
a feasible set maximizing f .
Let V(1/m) denote the distribution over random subsets
of V where each element is included independently with
probability 1/m. Let p 2 [0, 1]n be the following vector.
For each element e 2 V , we have
8
< Pr [e 2 G REEDY(A [ {e})] if e 2 OPT
pe = A⇠V(1/m)
:0
otherwise
Our main theorem follows from the next two lemmas,
which characterize the quality of the best solution from the
first round and that of the solution from the second round,
respectively. Recall that f is the Lovász extension of f .
Lemma 3. For each machine i, E[f (Si )]
↵ ·
f (1OPT p) .

Proof. Consider machine i. Let Vi denote the set of elements assigned to machine i in the first round. Let Oi =
{e 2 OPT : e 2
/ G REEDY(Vi [ {e})}. We make the following key observations.
We apply Lemma 2 with A = Vi and B = Oi \ Vi to
obtain that G REEDY(Vi ) = G REEDY(Vi [ Oi ) = Si . Since
OPT 2 I and I is hereditary, we must have Oi 2 I as
well. Since G REEDY is an ↵-approximation, it follows that
f (Si )

↵ · f (Oi ).

The Power of Randomization: Distributed Submodular Maximization on Massive Datasets

Since the distribution of Vi is the same as V(1/m), for each
element e 2 OPT, we have
Pr[e 2 Oi ] = 1

Pr[e 2
/ Oi ] = 1

E[1Oi ] = 1OPT

pe

↵ · E[f (Oi )]

Lemma 4. E[f (A LG(S))]

↵ · f (1OPT

p) .

· f (p).

S

Proof. Recall that S = i G REEDY(Vi ). Since OPT 2 I
and I is hereditary, S \ OPT 2 I. Since A LG is a approximation, we have
f (A LG(S))

(2)

· f (S \ OPT).

Consider an element e 2 OPT. For each machine i, we
have
Pr[e 2 S | e is assigned to machine i]

= Pr[e 2 G REEDY(Vi ) | e 2 Vi ]
=
=

Pr

[e 2 G REEDY(A) | e 2 A]

Pr

[e 2 G REEDY(B [ {e})]

A⇠V(1/m)

B⇠V(1/m)

= pe .
The first equality follows from the fact that e is included in
S if and only if it is included in G REEDY(Vi ). The second
equality follows from the fact that the distribution of Vi is
identical to V(1/m). The third equality follows from the
fact that the distribution of A ⇠ V(1/m) conditioned on
e 2 A is identical to the distribution of B [ {e} where
B ⇠ V(1/m). Therefore, Pr[e 2 S \ OPT] = pe and so
E[1S\OPT ] = p. Lemma 1 thus implies that
E[f (A LG(S))]

· E[f (S \ OPT)]

· f (p).

Combining Lemma 4 and Lemma 3 gives us our main theorem.
Theorem 5. Suppose that G REEDY is an ↵-approximation
algorithm and A LG is a -approximation algorithm for
maximizing a monotone submodular function subject to a
hereditary constraint I. Then R AND G REE D I is (in ex↵
pectation) an ↵+
-approximation algorithm for the same
problem.
S
Proof. Let Si = G REEDY(Vi ), S = i Si be the set of
elements on the last machine, and T = A LG(S) be the
solution produced on the last machine. Then, the output
D of R AND G REE D I satisfies f (D)
maxi {f (Si )} and
f (D) f (T ). Thus, from Lemmas 3 and 4 we have:
E[f (D)]

↵ · f (1OPT

p)

(3)

(4)

· f (p).

By combining (3) and (4), we obtain
( + ↵) E[f (D)]

p.

By combining these observations with Lemma 1, we obtain
E[f (Si )]

E[f (D)]

↵

f (p) + f (1OPT

p)

↵ · f (1OPT ) = ↵ · f (OPT).
In the second inequality, we have used the fact that f
convex and f (c· x) cf (x) for any c 2 [0, 1].

is

If we use the standard G REEDY algorithm for A LG, we obtain the following simplified corollary of Theorem 5.
Corollary 6. Suppose that G REEDY is an ↵-approximation
algorithm for maximizing a monotone submodular function, and use G REEDY as the algorithm A LG in R AND G REE D I. Then, the resulting algorithm is (in expectation)
an ↵2 -approximation algorithm for the same problem.

4. Non-Monotone Submodular Functions
We consider the problem of maximizing a non-monotone
submodular function subject to a hereditary constraint. Our
approach is a slight modification of the randomized, distributed greedy algorithm described in Section 3, and it
builds on the work of (Gupta et al., 2010). Again, we show
how to combine the standard G REEDY algorithm, together
with any algorithm A LG for the non-monotone case in order to obtain a randomized, distributed algorithm for nonmonotone submodular maximization.
Algorithm. Our modified algorithm, NMR AND G REE D I,
works as follows. As in the monotone case, in the first
round we distribute the elements of V uniformly at random amongst the m machines. Then, we run the standard greedy algorithm twice to obtain two disjoint solutions
Si1 and Si2 on each machine. Specifically, each machine
first runs G REEDY on Vi to obtain a solution Si1 , then runs
G REEDY on Vi \ Si1 to obtain a disjoint solution Si2 . In the
second round, both of these solutions
S are sent to a single
machine, which runs A LG on S = i (Si1 [ Si2 ) to produce
a solution T . The best solution amongst T and all of the
solutions Si1 and Si2 is then returned.
Analysis. We devote the rest of this section to the analysis of the algorithm. In the following, we assume that
we are working with an instance hV, I, f i of non-negative,
non-monotone submodular maximization for which the
G REEDY algorithm satisfies the following property (for
some ):
For all S 2 I: f (G REEDY(V ))

· f (G REEDY(V ) [ S)
(GP)
The standard analyses of the G REEDY algorithm show that
(GP) is satisfied with = 12 for cardinality and matroid
1
constraints, = 13 for knapsack constraints, and = p+1
for p-system constraints.

The Power of Randomization: Distributed Submodular Maximization on Massive Datasets

The analysis is similar to the approach from the previous
section. We define V(1/m) as before, but modify the definition of the vector p as follows: for each e 2 V \ OPT
we let pe = 0 and for each e 2 OPT, we let pe be:
⇥
Pr
e 2 G REEDY(A [ {e}) or
A⇠V(1/m)
⇤
e 2 G REEDY((A [ {e})\G REEDY(A [ {e})) .
We now give analogues of Lemmas 3 and 4. The proof of
the Lemma 8 is similar to that of Lemma 4, and is deferred
to the supplement.
Lemma 7. Suppose
that G REEDY satisfies
(GP). For each
⇥
⇤
machine i, E max{f (Si1 ), f (Si2 )}
·
f
(1OPT p).
2

Proof. Consider machine i and let Vi be the set of elements
assigned to machine i in the first round. Let
Oi = {e 2OPT : e 2
/ G REEDY(Vi [ {e}) and

e2
/ G REEDY((Vi [ {e}) \ G REEDY(Vi [ {e}))}

Note that, since OPT 2 I and I is hereditary, we have
Oi 2 I. It follows from Lemma 2 that
Si1 = G REEDY(Vi ) = G REEDY(Vi [ Oi ),

Si2 = G REEDY(Vi \ Si1 ) = G REEDY((Vi \ Si1 ) [ Oi ).
By combining the equations above with the greedy property
(GP), we obtain
f (Si1 )

· f (Si1 [ Oi ),

f (Si2 )

· f (Si2 [ Oi ).

f (Oi ).

We experimentally evaluate and compare the following distributed algorithms for maximizing a monotone submodular function subject to a cardinality constraint: the randomized variant of the G REE D I algorithm described in Sections 3 and 4, a deterministic variant of the G REE D I algorithm that assigns elements to machines in consecutive
blocks of size |V |/m, and the S AMPLE &P RUNE algorithm
of (Kumar et al., 2013). We run these algorithms in several
scenarios and we evaluate their performance relative to the
centralized G REEDY solution on the entire dataset.

(7)

Since the distribution of Vi is the same as V(1/m), for each
element e 2 OPT, we have
Pr[e 2 Oi ] = 1

Pr[e 2
/ Oi ] = 1

E[1Oi ] = 1OPT

pe ,

p.

(9)

By combining (8), (9), and Lemma 1, we obtain
E[f (Si1 ) + f (Si2 )]

· E[f (Oi )]

· f (1OPT

which immediately implies the desired inequality.
Lemma 8. E[f (A LG(S))]

· f (p).

Corollary 10. Consider the problem of maximizing a submodular function subject to some hereditary constraint I
and suppose that G REEDY satisfies (GP) for this problem. Let A LG be the algorithm described above. Then
NMR AND G REE D I achieves (in expectation) an 4+2 approximation for the same problem.

(6)

(8)

· f (Oi ).

We remark that one can use the following approach on
the last machine (Gupta et al., 2010). As in the first
round, we run G REEDY twice to obtain two solutions T1 =
G REEDY(S) and T2 = G REEDY(S \ T1 ). Additionally,
we select a subset T3 ✓ T1 using an unconstrained submodular maximization algorithm on T1 , such as the Double Greedy algorithm of (Buchbinder et al., 2012), which
is a 12 -approximation. The final solution T is the best solution among T1 , T2 , T3 . If G REEDY satisfies property (GP),
then it follows from the analysis of (Gupta et al., 2010) that
the resulting solution T satisfies f (T ) 2(1+ ) · f (OPT).
This gives us the following corollary of Theorem 9.

5. Experiments

By combining (5), (6), and (7), we obtain
f (Si1 ) + f (Si2 )

Theorem 9. Consider the problem of maximizing a submodular function under some hereditary constraint I, and
suppose that G REEDY satisfies (GP) and A LG is a approximation algorithm for this problem. Then NMR AND G REE D I is (in expectation) an +2 -approximation
algorithm for the same problem.

(5)

Now we observe that the submodularity and non-negativity
of f , together with Si1 \ Si2 = ;, imply
f (Si1 [ Oi ) + f (Si2 [ Oi )

Lemmas 7 and 8 imply our main result for non-monotone
submodular maximization (the proof is similar to that of
Theorem 5).

p),

Exemplar based clustering. Our experimental setup is
similar to that of (Mirzasoleiman et al., 2013). Our goal
is to find a representative set of objects from a dataset
by solving a k-medoid problem (Kaufman & Rousseeuw,
2009) that aims to minimize the sum of pairwise dissimilarities between the chosen objects and the entire dataset.
Let V denote the set of objects in the dataset and let
d : V ⇥ V ! R be a dissimilarity function; we assume
that d is symmetric, that is, d(i, j) = d(j, i) for each
pair i, j. LetPL : 2V ! R be the function such that
L(A) = |V1 | v2V mina2A d(a, v) for each set A ✓ V .
We can turn the problem of minimizing L into the prob-

The Power of Randomization: Distributed Submodular Maximization on Massive Datasets

1.01

0.9998
0.9996
0.9994
0.9992
0.9990
0.9988

1.00
0.99

1.0000

Centralized Greedy
BlockGreeDi
RandGreeDi

Performance/Centralized

Centralized Greedy
BlockGreeDi
RandGreeDi

Performance/Optimal

Performance/Optimal

1.0000

0.98
0.97
0.96
0.95

0.9986

0.94

0.9984
0

0.93
0

0.9998
0.9996
0.9994
0.9992
0.9990
0.9988

BlockGreeDi
RandGreeDi

0.9986
50

100

k=m

150

200

(a) Kosarak dataset

50

100

150

k=m

0

200

(b) accidents dataset

1.00
0.95
0.90
0.85
0.80

0.95
0.90
0.85
0.80
0.75

0.75
0

50

100

k=m

150

200

0

50

(d) Kosarak dataset

1.00

25000

0.95

20000

150

0.90

0.85

200

Sample&Prune
RandGreeDi

0

50

100

150

k=m

200

(f) 10K tiny images
0.38

BlockGreeDi
RandGreeDi

0.37

0.90

Utility f

0.36

Utility f

Performance/Centralized

100

k=m

30000
NMRandGreeDi

200

0.95

(e) accidents dataset

1.05

150

1.00

Sample&Prune
RandGreeDi

Performance/Centralized

Performance/Optimal

Performance/Optimal

1.00

Sample&Prune
RandGreeDi

100

k=m

(c) 10K tiny images

1.10
1.05

50

15000

0.85

10000

0.80

5000

0.35
0.34
0.33
BlockGreeDi (added images)
BlockGreeDi
RandGreeDi (added images)
RandGreeDi

0.32

0.75
0

20

40

60

k=m

80

100

(g) synthetic diverse-yet-relevant instance
(n = 10000, = n/k)

0
0

0.31
5

10

15

20

25

30

0.30
0

20

40

60

80

100

120

k

(h) synthetic hard instance for G REE D I

(i) 1M tiny images

Figure 1. Experiment Results (I)

lem of maximizing a monotone submodular function f
by introducing an auxiliary element v0 and by defining
f (S) = L({v0 }) L(S [ {v0 }) for each set S ✓ V .

Tiny Images experiments: In our experiments, we used a
subset of the Tiny Images dataset consisting of 32 ⇥ 32
RGB images (Torralba et al., 2008), each represented as
3, 072 dimensional vector. We subtracted from each vector the mean value and normalized the result, to obtain a
collection of 3, 072-dimensional vectors of unit norm. We
considered the distance function d(x, y) = kx yk2 for
every pair x, y of vectors. We used the zero vector as the
auxiliary element v0 in the definition of f .

In our smaller experiments, we used 10,000 tiny images,
and compared the utility of each algorithm to that of the
centralized G REEDY. The results are summarized in Figures 1(c) and 1(f).
In our large scale experiments, we used one million tiny
images, and m = 100 machines. In the first round of
the distributed algorithm, each machine ran the G REEDY
algorithm to maximize a restricted objective function f ,
which is based on the average dissimilarity L taken over
only those images assigned to that machine. Similarly, in
the second round, the final machine maximized an objective function f based on the total dissimilarity of all those

The Power of Randomization: Distributed Submodular Maximization on Massive Datasets

Maximum Coverage experiments. We ran several experiments using instances of the Maximum Coverage problem.
In the Maximum Coverage problem, we are given a collection C ✓ 2V of subsets of a ground set V and an integer k,
and the goal is to select k of the subsets in C that cover as
many elements as possible.
Kosarak and accidents datasets: We evaluated and compared the algorithms on the datasets used in (Kumar et al.,
2013). In both cases, we computed the optimal centralized solution using CPLEX, and calculated the actual performance ratio attained by the algorithms. The results are
summarized in Figures 1(a), 1(d), 1(b), 1(e).
Synthetic hard instances: We generated a synthetic dataset
with hard instances for the deterministic G REE D I. We describe the instances in the supplement. We ran the G REE D I
algorithm with a worst-case partition of the data. The results are summarized in Figure 1(h).
Finding diverse yet relevant items. We evaluated the randomized algorithm NMR AND G REE D I described in Section 4 on the following instance of non-monotone submodular maximization subject to a cardinality constraint.
We usedP
the objective
function
P
Pof (Lin & Bilmes, 2009):
f (A) = i2V j2A sij
is a rei,j2A sij , where
dundancy parameter and {sij }ij is a similarity matrix. We
generated an n ⇥ n similarity matrix with random entries
sij 2 U(0, 100) and we set = n/k. The results are summarized in Figure 1(g).
Matroid constraints. In order to evaluate our algorithm
on a matroid constraint, we considered the following variant of maximum coverage: we are given a space containing
several demand points and n facilities (e.g. wireless access
points or sensors). Each facility can operate in one of r
modes, each with a distinct coverage profile. The goal is to
find a subset of at most k facilities to activate, along with
a single mode for each activated facility, so that the total
number of demand points covered is maximized. In our ex-

Performance/Centralized

Remark on the function evaluation. In decomposable cases
such as exemplar clustering, the function is a sum of distances over all points in the dataset. By concentration results such as Chernoff bounds, the sum can be approximated additively with high probability by sampling a few
points and using the (scaled) empirical sum. The random
subset each machine receives can readily serve as the samples for the above approximation. Thus the random partition is useful for evaluating the function in a distributed
fashion, in addition to its algorithmic benefits.

1.01

1.00

BlockGreeDi
RoundRobinGreeDi
RandGreeDi

0.99

0.98

0.97
0

50

100

150

200

250

k=m

(a) matroid coverage (n = 900, r = 5)
1.05

Performance/Centralized

images it received . We also considered a variant similar
to that described by (Mirzasoleiman et al., 2013), in which
10,000 additional random images from the original dataset
were added to the final machine. The results are summarized in Figure 1(i).

1.00

0.95

0.90

0.85

0.80
0

BlockGreeDi
RoundRobinGreeDi
RandGreeDi
20

40

60

80

100

k=m

(b) matroid coverage (n = 100, r = 100)
Figure 2. Experiment Results (II)

periment, we placed 250,000 demand points in a grid in the
unit square, together with a grid of n facilities. We modeled coverage profiles as ellipses centered at each facility
with major axes of length 0.1`, minor axes of length 0.1/`
rotated by ⇢ where ` 2 N (3, 13 ) and ⇢ 2 U(0, 2⇡) are chosen randomly for each ellipse. We performed two series
of experiments. In the first, there were n = 900 facilities, each with r = 5 coverage profiles, while in the second
there were n = 100 facilities, each with r = 100 coverage
profiles.
The resulting problem instances were represented as
ground set comprising a list of ellipses, each with a designated facility, together with a partition matroid constraint
ensuring that at most one ellipse per facility was chosen.
Here, we compared the randomized G REE D I algorithm to
two deterministic variants that assigned elements to machines in consecutive blocks and in round robin order, respectively. The results are summarized in Figures 2(a) and
2(b).

Acknowledgements. We thank Moran Feldman for

suggesting a modification to our original analysis that led
to the simpler and stronger analysis included in this version
of the paper. This work was supported by EPSRC grant
EP/J021814/1.

The Power of Randomization: Distributed Submodular Maximization on Massive Datasets

References
Buchbinder, Niv, Feldman, Moran, Naor, Joseph, and
Schwartz, Roy. A tight linear time (1/2)-approximation
for unconstrained submodular maximization. In Foundations of Computer Science (FOCS), pp. 649–658. IEEE,
2012.
Dean, Jeffrey and Ghemawat, Sanjay. Mapreduce: Simplified data processing on large clusters. In Symposium on
Operating System Design and Implementation (OSDI),
pp. 137–150. USENIX Association, 2004.
Fisher, Marshall L, Nemhauser, George L, and Wolsey,
Laurence A. An analysis of approximations for maximizing submodular set functions—II. Mathematical
Programming Studies, 8:73–87, 1978.
Gupta, Anupam, Roth, Aaron, Schoenebeck, Grant, and
Talwar, Kunal. Constrained non-monotone submodular
maximization: Offline and secretary algorithms. In Internet and Network Economics, pp. 246–257. Springer,
2010.
Indyk, Piotr, Mahabadi, Sepideh, Mahdian, Mohammad,
and Mirrokni, Vahab S. Composable core-sets for diversity and coverage maximization. In ACM Symposium on
Principles of Database Systems (PODS), pp. 100–108.
ACM, 2014.
Jenkyns, Thomas A. The efficacy of the “greedy” algorithm. In Southeastern Conference on Combinatorics,
Graph Theory, and Computing, pp. 341–350. Utilitas
Mathematica, 1976.
Kaufman, Leonard and Rousseeuw, Peter J. Finding groups
in data: An introduction to cluster analysis, volume 344.
John Wiley & Sons, 2009.
Kumar, Ravi, Moseley, Benjamin, Vassilvitskii, Sergei, and
Vattani, Andrea. Fast greedy algorithms in mapreduce
and streaming. In ACM Symposium on Parallelism in
Algorithms and Architectures (SPAA), pp. 1–10. ACM,
2013.
Lin, Hui and Bilmes, Jeff A. How to select a good trainingdata subset for transcription: Submodular active selection for sequences. In Annual Conference of the International Speech Communication Association (INTERSPEECH), Brighton, UK, September 2009.
Mirzasoleiman, Baharan, Karbasi, Amin, Sarkar, Rik, and
Krause, Andreas. Distributed submodular maximization:
Identifying representative elements in massive data. In
Advances in Neural Information Processing Systems
(NIPS), pp. 2049–2057, 2013.

Nemhauser, George L, Wolsey, Laurence A, and Fisher,
Marshall L. An analysis of approximations for maximizing submodular set functions—I. Mathematical Programming, 14(1):265–294, 1978.
Torralba, Antonio, Fergus, Robert, and Freeman,
William T. 80 million tiny images: A large data
set for nonparametric object and scene recognition.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 30(11):1958–1970, 2008.
Wolsey, Laurence A. Maximising real-valued submodular functions: Primal and dual heuristics for location
problems. Mathematics of Operations Research, 7(3):
pp. 410–425, 1982.

