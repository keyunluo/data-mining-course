Probabilistic Matrix Factorization with Non-random Missing Data

José Miguel Hernández-Lobato
Neil Houlsby
Zoubin Ghahramani
University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK

Abstract
We propose a probabilistic matrix factorization
model for collaborative filtering that learns from
data that is missing not at random (MNAR). Matrix factorization models exhibit state-of-the-art
predictive performance in collaborative filtering.
However, these models usually assume that the
data is missing at random (MAR), and this is
rarely the case. For example, the data is not
MAR if users rate items they like more than ones
they dislike. When the MAR assumption is incorrect, inferences are biased and predictive performance can suffer. Therefore, we model both
the generative process for the data and the missing data mechanism. By learning these two models jointly we obtain improved performance over
state-of-the-art methods when predicting the ratings and when modeling the data observation
process. We present the first viable MF model
for MNAR data. Our results are promising and
we expect that further research on NMAR models will yield large gains in collaborative filtering.

1. Introduction
Collaborative filtering (CF) data consists of ratings by users
on a set of items. CF systems learn patterns in this data to
make accurate predictions, for example, in order to recommend new items to users. Typically, most users rate only a
small fraction of the available items, so most of the CF data
is missing. Probabilistic matrix factorization (MF) models
have become popular for CF because i) they can be robust to overfitting and come with automatic estimates of
uncertainty (Mnih & Salakhutdinov, 2007); ii) they can be
adapted to different data types, such as continuous, binary
or ordinal data (Stern et al., 2009); and iii) they can theoretically handle missing data in a formal manner. A common
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

JMH 233@ CAM . AC . UK
NMTH 2@ CAM . AC . UK
ZOUBIN @ ENG . CAM . AC . UK

assumption that MF models make is that CF data is missing
at random (MAR) (Little & Rubin, 1987). That is, the process that selects the observed data (the observation process)
is independent of the value of this unobserved data.
When the data is MAR, the observation process can be ignored and standard inference methods can be used without introducing bias. However, there is evidence that CF
data is missing not at random (MNAR) (Marlin & Zemel,
2007). Consider the following scenarios: i) users only
watch movies that they like, and only rate movies that they
watch; ii) a user only provides extreme ratings, that is, they
only provide feedback particularly bad or good items; iii)
certain basic items (e.g. stationary) are simply expected to
function correctly and these items are only rated when they
are defective. The first scenario is an example of global
censoring where low-valued ratings are usually missing.
The second two are examples of local censoring where
missing ratings for specific users or items take higher or
lower values on average than the observed ones. In these
scenarios dependencies between the missing data values
and the observation process exist, and consequently the
data is not MAR. When the MAR assumption is incorrect,
inferences can be biased and prediction accuracy suffers.
The lack of robustness of the MAR assumption has been
widely addressed in the statistics literature. However, almost all probabilistic models for CF ignore this issue. We
fill this gap by extending state-of-the-art MF models for CF
to the MNAR scenario. The general approach for dealing
with MNAR data is to learn jointly a complete data model
(CDM), that explains how the data is generated, and a missing data model (MDM), that explains the observation process for the data (Little & Rubin, 1987). Our CDM is a new
MF model for ordinal rating data with state-of-the-art predictive performance. This model uses hierarchical priors
to increase robustness to the selection of hyper-parameters
and is heteroskedastic in the sense that the ratings of different users and items can exhibit different levels of noise
(Lakshminarayanan et al., 2011). Our MDM is also a MF
model that can capture complex dependencies between the
missing data and the observation process, such as those de-

Probabilistic Matrix Factorization with Non-random Missing Data

scribed in the previous paragraph. We perform efficient
inference in the resulting MF model for MNAR data (MFMNAR) using expectation propagation (Minka, 2001) and
stochastic variational inference (Hoffman et al., 2013).

L(Θ|RO ) =

2. Probabilistic Treatment of Missing Data
The theory of missing data has been widely studied. We
review here the main principles developed in (Little & Rubin, 1987) in the context of rating data. Our data is formed
by ratings ri,j given by user i on item j, where i = 1, . . . ,
n and j = 1, . . . , d. We collect these into an n × d rating
matrix R = {RO , R¬O }, where RO and R¬O denote the
the sets of observed and missing entries in R, respectively.
For each ri,j , we define a Bernoulli random variable xi,j
that indicates whether ri,j is observed (xi,j = 1) or not
(xi,j = 0), and collect all the xi,j in the n × d binary matrix X. We assume that R is generated by a complete data
model (CDM) with parameters Θ, and X is generated by
a missing data model (MDM) with parameters Ω. Both
models may also share a set of latent variables Z. The joint
distribution for R, X, and Z given Θ and Ω is
p(X, R, Z|Θ, Ω) = p(X|R, Ω, Z)p(R, Z|Θ) .

(1)

Most machine learning focuses on the estimation of the
CDM given by p(R, Z|Θ). In (1), p(X|R, Θ, Z) is the
MDM, which is normally ignored in CF systems.
The mechanisms for missing data are usually divided into
three classes (Little & Rubin, 1987): completely missing
at random (CMAR), missing at random (MAR) and missing not at random (MNAR). CMAR is the most restrictive
assumption, where the probability of observing a rating is
independent of the value of any rating or latent variable
generated by the CDM, that is, p(X|R, Z, Ω) = p(X|Ω).
With MAR data, the observation probability depends only
upon the value of the observed data and the MDM parameters, that is p(X|R, Ω, Z) = p(X|RO , Ω). This assumption is popular in machine learning because it means that
the MDM can be ignored without introducing any biases
during inference. Under the MAR assumption, the likelihood for Θ given RO is

p(X|R, Z, Ω)p(R, Z|Θ)dZ

(2)

R¬O
MAR

Experimentally, the combination of the MDM and the
CDM in MF-MNAR produces gains in both the modeling of the ratings and the modeling of the data observation process. We also find that MF-MNAR outperforms
the MAR version of this method, other state-of-the-art MF
MAR alternatives (Paquet et al., 2012) as well as an alternative model for MNAR rating data based on mixtures of
multinomials (Marlin & Zemel, 2009). In summary, we
present the first attempt to model MNAR data using probabilistic matrix factorization. Our results are promising and
we expect that additional research in this domain will yield
significant further improvements in CF systems.

XZ

= p(X|RO , Ω)

XZ

p(R, Z|Θ)dZ

(3)

R¬O

= p(X|RO , Ω)p(RO |Θ) ∝ p(RO |Θ) .

Since p(X|RO , Ω) is constant with respect to Θ, we can
ignore the MDM when learning Θ.
However, in the general case of MNAR data, X will not be
independent of R or Z. In this case, the step from (2) to (3)
does not hold and, when integrating over Z and summing
over R¬O , one must weight the CDM likelihood p(R,
Z|Θ) by the observation probabilities given by the MDM
p(X|R, Z, Ω). This is because the binary matrix X has
information about the possible values that R¬O or Z may
have taken. Maximum likelihood estimates and Bayesian
inference will be biased if the MAR assumption is used but
the data is MNAR. For example, consider users who mainly
rate items that they like and seldom rate items that they dislike. Under the MAR assumption, the estimated CDM will
over-estimate the value of the missing ratings that have not
yet been provided by such users.
We can correct this observational bias by jointly learning
the CDM and the MDM. We now describe how to do this
with an ordinal matrix factorization model for MNAR data.

3. An Ordinal Matrix Factorization Model
with Data not Missing at Random
We are given a dataset D = {ri,j : 1 ≤ i ≤ n, 1 ≤ j ≤ d,
ri,j ∈ {1, . . . L}, (i, j) ∈ O} of discrete ratings by n users
on d items, where the possible ratings are 1 < . . . < L
and O is the set of pairs of users and items for which a
rating is available. D is a subset of the entries of a complete n × d rating matrix R. In practice, D contains only a
small fraction of the entries in R. We model the location of
the entries included in D using an n × d binary matrix X,
where xi,j = 1 if ri,j ∈ D (observed) and xi,j = 0 otherwise (missing). We first describe a probabilistic model for
the generation of R, then we describe another model that
generates X given R. The first model is the complete data
model (CDM) and the second one the missing data model
(MDM). Our new method for ordinal Matrix Factorization
with data Missing Not At Random (MF-MNAR) is formed
by combining these two models into a single model.
The factor graph for the distribution implied by MF-MNAR
is shown in Figure 2. In this graph the square nodes correspond to factors in the distribution and the circular nodes
represent random variables. The edges show dependencies
of factors on variables (Kschischang et al., 2001). A full
description of the factors in Figure 2 is given in the supple-

Probabilistic Matrix Factorization with Non-random Missing Data

Figure 1. High-level visualization of the components of MF-MNAR. The observed data RO is obtained by ‘masking’ (denoted by the
Hadamard product ◦) the complete data R with the binary matrix X. The CDM (top) generates R by filtering a low rank matrix and
heteroskedastic noise through the ordinal likelihood ΘB (·) with specific interval boundaries B. The MDM (bottom) generates X using
as input a global bias z, a low rank matrix, i.i.d. noise and the result of combining R with Λrow and Ψcol to produce an n × d matrix
(indicated by the function Il ). The resulting matrix is filtered through the Heaviside step function Θ.

mentary material. Figure 1 visualizes the generative model
of MF-MNAR, which is described in detail below.
3.1. The Complete Data Model
We describe now the CDM in the left half of Figure 2. Full
details are in the supplementary material. We propose a
matrix factorization model for the rating matrix R. This
model has three key features: i) an appropriate ordinal likelihood for rating data (rather than the usual Gaussian likelihood); ii) heteroskedastic noise, that is, variable noise for
each user and item; and iii) hierarchical priors to increase
robustness to selection of hyper-parameter values.
We assume that R is generated as a function of two low
rank latent matrices U ∈ Rn×h and V ∈ Rd×h , where
h  min(n, d). Each discrete rating ri,j in R is determined by i) the scalar uTi vj , where ui is the vector in the
i-th row of U and vj corresponds to the j-th row of V,
and ii) a partition of R into L − 1 contiguous intervals
with boundaries bj,0 < . . . < bj,L , where bj,0 = −∞ and
bj,L = ∞. The value of ri,j is obtained from the interval
in which uTi vj falls. Note that the interval boundaries are
different for each column (item) of R. In practice data is
noisy. We model this by adding zero-mean Gaussian noise
i,j to uTi vj before generating ri,j and introducing the latent variable ai,j = uTi vj + i,j . The probability of ri,j
given ai,j and bj = (bj,1 , . . . , bj,L−1 ) is
ri,j −1

p(ri,j |ai,j , bj ) =

Y
k=1

L−1
Y

Θ[ai,j − bj,k ]

L−1
Y

Θ[bj,k − ai,j ] =

k=ri,j

Θ [sign[ri,j − k − 0.5](ai,j − bj,k )] ,

(4)

k=1

where Θ is the Heaviside step function. This likelihood is
1 when ai,j ∈ (bri,j −1 , bri,j ] and 0 otherwise. The dependence of (4) on all the entries in bj and not only on bri,j −1
and bri,j allows us to learn bj . We put a prior on each

bj to learn these item specific boundaries, p(bj |b0 ) =
QL−1
k=1 N (bj,k |b0,k , v0 ), where b0 is a vector of base interval boundaries. To avoid specifyingQthese base boundL−1
0
aries, we use a hyper-prior p(b0 ) = k=1 N (b0,k |mb
k ,
b0
b0
v0 ), where m1 , . . . , mL−1 and v0 are hyper-parameters.
Real world rating matrices exhibit variable levels of noise
across rows and columns (Lakshminarayanan et al., 2011).
To model this heteroskedasticity, we allow the additive
noise i,j to be row and column dependent: i,j has a Gaussian prior with zero-mean and variance γirow × γjcol , where
γirow and γjcol govern the noise level in the i-th row and j-th
column of R, respectively. Define ci,j = uTi vj , then the
conditional distribution for ai,j given ci,j , γirow and γjcol is
p(ai,j |ci,j , γirow , γjcol ) = N (ai,j |ci,j , γirow γjcol ). To learn the
noise levels we put Inverse Gamma priors on γirow and γjcol .
We use a hierarchical Gaussian prior for the
two low-rank matrices
We select
Qn Qh U and V.
U
p(U|mU , vU ) = i=1 k=1 N (ui,k |mU
,
v
)
and
k
k
Qd Qh
V
V
V V
p(V|m , v ) = j=1 k=1 N (vj,k |mk , vk ), where
mU and mV are mean parameters for the rows of U
and V, respectively. These are given factorized Gaussian
hyper-priors. Similarly, vU and vV are variance parameters for the rows of U and V and are given factorized
Inverse Gamma hyper-priors. The parameters of these
Gaussian hyper-priors are given standard values and the
parameters of the Inverse Gamma hyper-priors are in the
supplementary material.
We collect the boundary vectors bj into the d × (L − 1)
matrix B, the ai,j and ci,j variables into the n × d matrices
A and C, and the row and column noise levels into
the n and d dimensional vectors γ row and γ col , respectively. The joint distribution for R and the parameters
Θ = {U, V, B, A, C, γ row , γ col , b0 , mU , mV , vU , vV }
is

Probabilistic Matrix Factorization with Non-random Missing Data

Figure 2. Factor graph for MF-MNAR. The graph includes one component for the CDM (left) and another one for the MDM (right).
They are connected through the rating variables ri,j , The nodes for ri,j are connected with a dotted line because they encode the same
variables. The node for xi,j is shaded in blue because xi,j is always known. This variable indicates whether ri,j is observed. Only a
few of the ri,j are observed, to indicate this we have shaded their nodes in orange.

p(Θ, R) = p(R|A, B)p(A|C, γ row , γ col )p(C|U, V)
p(U|mU , vU )p(V|mV , vV )p(B|b0 )p(b0 )
p(γ row )p(γ col )p(mU )p(mV )p(vU )p(vV ) .

(5)

This specifies the CDM. We now describe the MDM that
explains which entries of R are contained in the set of observed ratings D, as specified by the binary matrix X.
3.2. The Missing Data Model
The MDM generates the binary matrix X as a function of
R. We also assume a matrix factorization model for X.
However, our MDM has two main differences to the CDM.
Firstly, we use a likelihood for binary data and secondly, we
use additional variables to model the effect of each rating’s
value ri,j on its observation status xi,j .
We define two additional low rank matrices E ∈ Rn×h
and F ∈ Rd×h . X is obtained as a function of E,
F, R and some additive noise.
we assume
PL In particular,
col
xi,j = Θ{ei fjT + z + gi,j + l=1 (λrow
+
ψ
)I[r
i,j = l]},
j,l
i,l
where Θ{·} is the Heaviside step function, I[·] is the binary
indicator function, z ∈ R is a bias that governs the overall
sparsity of X and gi,j ∈ R is an i.i.d. noise variable with a
logistic c.d.f. σ(x) = 1/(1 + exp(−x)). This generative
process results in a logistic likelihood, which is commonly
used in binary classification tasks.
col
The parameters λrow
i,l , ψj,l ∈ R determine the influence of
the value of ri,j on whether ri,j is contained in D or not.
Importantly, this influence can vary across the rows and

col
columns of R. A large positive value of (λrow
i,l + ψj,l )
increases the probability that ri,j is included in D when
col
ri,j = l and a low value of (λrow
i,l + ψj,l ) reduces this probrow
ability. Thus λi,l captures effects such as some users i
being more likely to rate movies they like, and others ratcol
ing movies they dislike, while ψj,l
captures the analogous
col
effects for movies j. We collect the λrow
i,l and the ψj,l in two
row
col
n × L and d × L matrices Λ and Ψ . The likelihood
for the missing data model is

p(X|E, F, z, Λrow , Ψcol , R) =

 Y

σ{ei fjT + z +

(i,j):

L
X
l=1




col

(λrow
i,l + ψj,l )I[ri,j = l]}

xi,j =1





 Y

σ{−ei fjT − z −

(i,j):

L
X
l=1


col

(λrow
i,l + ψj,l )I[ri,j = l]} . (6)

xi,j =0

We use fully factorized standard Gaussian priors for all the
parameters in (6). Finally we introduce row and column
specific offset biases in E and F by setting to one all the
entries in one of the columns in E and in another of the
columns of F (details in the supplementary material).
Let Ω be the set of variables Ω = {E, F, z, Λrow , Ψcol }.
The joint distribution for X and Ω given R is
p(X, Ω|R) = p(X|E, F, z, Λrow , Ψcol , R)
p(E)p(F)p(z)p(Λrow )p(Ψcol ) .

(7)

Probabilistic Matrix Factorization with Non-random Missing Data

Algorithm 1 Approximate Inference in the Joint Model

3.3. The Joint Model
We obtain MF-MNAR by combining the MDM with the
CDM. Recall from Section 2 that RO is the set of observed entries in R and R¬O is the set of non-observed
entries. The posterior distribution over the parameters Θ
of the CDM, the parameters Ω of the MDM and the missing data itself R¬O given X (which ratings are observed)
and RO (the values of the observed ratings) is
p(X, Ω|R)p(Θ, R)
,
p(RO , X)

p(Θ, Ω, R¬O |RO , X) =

(8)
Q3 (R¬O ) =

O

where p(R , X) is a normalization constant. The factor
graph for the resulting model is shown in Figure 2. The
graph includes 19 factors, described in the supplementary
material. To predict the value of an entry ri,j in R¬O , we
have to marginalize (8) with respect to Ω, Θ and all of
the entries in R¬O . This is intractable and we have to use
computational approximations. We describe next how to
approximate the posterior with a tractable distribution.

4. Approximate Inference
As in most non-trivial Bayesian models, the posterior (8)
is intractable. Therefore we approximate this distribution using expectation propagation (EP) (Minka, 2001) and
variational Bayes (VB) (Ghahramani & Beal, 2001). We
approximate p(Θ, Ω, R¬O |RO ) with Q(Θ, Ω, R¬O ) =
Q1 (Θ)Q2 (Ω)Q3 (R¬O ), where Q1 , Q2 and Q3 are given
by
"
Q1 (Θ) =

h
Y

U

U

#"

IG(vkU |avk , avk )

k=1

"

d L−1
Y
Y

n Y
d
Y

b
N (bi,k |mbi,k , vi,k
)

d
Y

h
Y

#"
c
N (ci,j |mci,j , vi,j
)

h
Y

n
Y

#

n Y
h
Y

v
N (vj,k |mvj,k , vj,k

U
mU
N (mU
, vkm )
k |mk

h
Y

IG(γirow |aγi

, bγi

row

#"
)

i=1

d
Y

#
N (b0,k |mbk0 , vkb0 )
#

V
mV
N (mV
, vkm )
k |mk

col
col
IG(γjrow |aγj , bγj )

#
,

z

z

Q2 (Ω) = N (z|m , v )

n
Y

h
Y

d
Y

h
Y

#"
f
N (fj,k |mfj,k , vj,k
)

j=1 k=1

"

d Y
L
Y
j=1 k=1

col

(12)

e
N (ei,k |mei,k , vi,k
)

row

row

#

λ
λ
N (λrow
i,l |mi,l , vi,l )

i=1 k=1
col

The parameters of Q are fixed by running EP and VB on the
complete data model (CDM) and VB on the missing data
model (MDM). We use EP for the CDM as it performs well
in ordinal regression (Chu & Ghahramani, 2005). However, EP is known to perform poorly for factors corresponding to matrix factorizations, so we use VB for these. In particular, we use a stochastic version of VB called stochastic
variational inference (SVI) (Hoffman et al., 2013) so that
our computational cost scales with the number of observed
ratings |O| and not with the size of R, which can be very
large. Algorithm 1 summarizes our inference procedure.
We first adjust the components in Q for the two models independently. After that, we co-train the models. For this,
we iteratively re-adjust Q by jointly refining the approximate posterior of each model while taking into account
the predictions of the other. Full details can be found in
the supplementary material. The code for our MF-MNAR
method is publicly available at http://jmhl.org.

CDM MDM
p̃JM
i,j,l (xi,j ) ∝ p̃i,j,l p̃i,j,l (xi,j ) ,

n Y
L
Y

#

col
ψ
N (ψj,l
|mψ
j,l , vj,l ) ,

(11)

#

i=1 k=1

"

.

(i,j)∈O
/ l=1

(9)

j=1

"

=l]

Given Q, we can approximate the joint model’s posterior
probability pJM
i,j,l (xi,j ) that the entry in the i-th row and jth column of R takes value l, conditioned on a specific
value of xi,j . When xi,j = 0, we assume that the entry
was not selected by the MDM and it is missing. When
xi,j = 1, the entry was selected by the MDM and should
have been observed, but its value is unknown (for example,
if it was held out in a test set). The value of pJM
i,j,l (xi,j ) is
approximated using

#
u
N (ui,k |mui,k , vi,k

k=1
row

I[r

pi,j,li,j

4.1. Predictive Distribution in the Joint Model

k=1

#"

L
Y Y

#
a
N (ai,j |mai,j , vi,j
)

i=1 k=1

k=1

"

d
n Y
Y

# "L−1
Y

j=1 k=1

"

V

i=1 j=1

i=1 j=1

"

V

IG(vkV |avk , bvk )

k=1

#"

i=1 k=1

"

h
Y

Input: Rating dataset D.
Adjust Q using EP on CDM ignoring MDM.
Adjust Q using SVI on MDM ignoring CDM.
for t = 1 to T do
Adjust Q using SVI on MDM with CDM predictions.
Adjust Q using EP-SVI on CDM with MDM predictions.
end for
Output: Posterior approximation Q.

(10)

MDM
where p̃CDM
i,j,l and p̃i,j,l (xi,j ) are the individual predictions
of the CDM and the MDM generated using the approximate posterior Q, respectively, and their exact values can
be found in the supplementary material. In this formula,
CDM
p̃MDM
i,j,l (xi,j ) improves the prediction of the CDM, p̃i,j,l ,
by using the information available in xi,j .

Probabilistic Matrix Factorization with Non-random Missing Data

5. Related Work
Marlin & Zemel (2009) have proposed a method for modeling MNAR rating data in collaborative filtering. They
use a Mixture of Multinomials (MM) as their CDM. In the
MM model, there are K clusters of users and each item has
K multinomial rating distributions associated to it. To rate
item j, all the users in the k-th cluster sample from the k-th
multinomial distribution associated with the j-th item. The
EM algorithm is used to adjust the MM model with MAR
data. This MM CDM is simple enough to be extended to
MNAR data at a low computational cost. However, in practice, the MM model lacks flexibility and is often outperformed by more powerful MF approaches such as the MF
method described in Section 3.1.
Marlin & Zemel (2009) propose two different missing data
models for the MM CDM. The first, CPTv, assumes that
p(xi,j |ri,j ) = µri,j , where µ is an L-dimensional probability vector such that µl is the probability that ri,j is observed
if its value is l. The second MDM, Logitvd, assumes that
p(xi,j |ri,j ) = σ(κri,j + ωj ), where σ is the logistic function, ωj ∈ R models a per-item bias and the parameters
κ1 , . . . , κL ∈ R model the dependence of xi,j on the value
of ri,j . These MDMs can be jointly estimated with the MM
CDM using EM. The resulting method is computationally
efficient and has approximately the same cost as learning
the MM under the MAR assumption.
Our MDM (see Section 3.2) is more flexible than CPTv or
Logitvd. The variables Λrow and Ψcol allow us to encode
dependencies between ri,j and xi,j that can change across
users, items and values of ri,j . For example, we can capture
effects such as groups of users with different rating behaviors: users that rate what they like and others that rate what
they dislike; or certain items that are only rated by users
who dislike them. Furthermore, we use a MF component
to capture global effects that are independent of R. For example, a set of items that are strongly promoted and hence
observed with high frequency. We also do full Bayesian inference instead of MAP estimation as in Logitvd and CPTv.
This makes our MDM robust to overfitting problems. Finally, we obtain the same scalability as CPTv and Logitvd
by using stochastic inference methods (see Section 4 and
the supplementary material).
MNAR rating data is also considered by Steck (2010). This
method is similar to the BPR algorithm (Rendle et al.,
2009) and works by optimizing the parameters of a nonprobabilistic MF model with respect to a ranking-based
loss function. Steck (2010) does not learn a generative
model for the data and optimizes a metric that is robust
to MNAR data. The resulting method can be applied to
recommendation, but not to rating prediction tasks.
The previous pioneering works have addressed the problem

of modeling MNAR rating data. However, these early approaches are limited to relatively simple models. Bayesian
MF models are highly flexible and often yield state-of-theart predictive performance on rating data. We present the
first approach to extend these methods to the MNAR scenario.

6. Experiments
We analyze the performance of our Matrix Factorization
model with data Missing Not At Random (MF-MNAR)
in a series of experiments with synthetic and real-world
rating data. We compare MF-MNAR with several benchmark methods including i) a version of MF-MNAR that assumes data Missing At Random (MF-MAR); ii) a Mixture
of Multinomials model for MAR rating data (MM-MAR)
(Marlin & Zemel, 2009); the iii) CPTv and iv) Logitvd
models for MNAR rating data proposed by Marlin & Zemel
(2009); v) a state-of-the-art method for ordinal matrix factorization with MAR data (Paquet et al., 2012) (Paquet) and
finally, vi) an oracle method that always predicts labels according to their empirical frequencies in the test set (Oracle). In all MF methods we use a latent rank of size 20. In
the mixture of multinomials we use 20 components.
6.1. Datasets
Our first synthetic dataset is a toy example that illustrates
the differences between rating data missing at random
(MAR) and rating data missing not at random (MNAR)
(Steck, 2010). The dataset contains items that are horror
movies (items 1 to 50) or romance movies (items 51 to
100) and users who are romance-lovers (users 1 to 100 and
201 to 300) or horror-lovers (users 101 to 200 and 301 to
400). We consider discrete ratings with values from 1 to 5.
Romance-lovers (horror-lovers) will rate romance movies
(horror movies) by sampling from a multinomial with probability vector p = (0.05, 0.05, 0.05, 0.4, 0.45), where pi is
the probability of rating the movie with value i. Similarly,
romance-lovers (horror-lovers) will rate horror movies (romance movies) using a multinomial with probability vector
p0 , where p0i = p6−i . The left-hand plot in Figure 3 shows
the complete rating matrix for this dataset. We have considered two missing data mechanisms. In the first (MAR),
each matrix entry is observed independently with probability p ≈ 0.23. In the second (MNAR), users 1 to 200
are ‘positive’, they tend to rate what they like. Each rating
from these users is observed with probability pi , where i
is the value of the entry. However, users 201 to 400 are
‘negative’, rating what they do not like. Each rating with
value i is now observed with probability p0i . The plot in the
middle of Figure 3 shows the observed data for the MAR
setting, while the right-hand plot shows the observed data
for the MNAR setting. In the latter case, there is a clear

Probabilistic Matrix Factorization with Non-random Missing Data
Table 1. Characteristics of the datasets.
n
d #xi,j = 1 #xi,j = 0 Sparsity
943 1682 100000
0
0.063
6040 3706 1000209
0
0.045
3972 2043 106337
0
0.013
1053 1381 52721
0
0.036
15400 1000 311704
54000
0.020
500 500 12234
237766
0.049
500 500 31378
218622
0.126
400 100
9057
30943
0.226
400 100
9258
30742
0.231

Negative

Positive

Dataset
ML100K
ML1M
MTweet
NIPS
Yahoo
SMF-MNAR
SMF-MAR
SRH-MNAR
SRH-MAR

SMF: Synthetic Matrix Factorization dataset.
SRH: Synthetic Romance Horror movie dataset.

Figure 3. Toy synthetic dataset that illustrates the differences between MAR and MNAR. Left, full 400×100 rating matrix. Light
blue colors correspond to high rating values, while dark blue colors correspond to low rating values. Middle, observed data under
the MAR setting. Right, observed data under the MNAR setting.

pattern relating the value of an entry with its observation
status. Note that the pattern changes from rows 1-200 to
rows 201-400.
The second synthetic dataset is obtained by sampling from
a matrix factorization model. We generate two 500 × 10
matrices U and V with standard Gaussian i.i.d. entries. We
then generate C = UVT − 3 and partition R in contiguous
intervals with boundaries −∞, −6, −2, 2, 6, ∞. For each
ci,j , we create an ri,j ∈ { 1, . . . , 5} according to the interval in which ci,j lies. We generate two extra 500 × 10
matrices E and F with standard Gaussian i.i.d. entries. For
each ri,j we set the binary variable xi,j to 1 with probabilP5
ity σ(ei fjT −4+ l=1 zi I[ri,j = l]), where σ is the logistic
function and ei and fj are the i-th and j-th rows of E and
F. We consider two missing data mechanisms. In the first
(MAR), all the z1 , . . . , z5 are 0 when we generate the xi,j .
In the second (MNAR), (z1 , . . . , z5 ) = (−3, −3, −3, 3, 3).
In the latter case we expect to see many more ratings with
value 4 and 5 than in the MAR scenario.
We considered several real-world rating datasets. The
MovieLens 100k and 1M datasets1 include ratings from 1
to 5 on movies. The Yahoo! Web-scope R3 dataset2 . contains ratings from 1 to 5 on songs. We also analyzed a
dataset obtained from the reviewer bidding process for the
2013 NIPS conference. This dataset contains ratings from
1 to 4 given by reviewers on papers. Finally, the Movie
Tweetings dataset3 includes ratings on movies collected
from Twitter. We pre-processed this dataset to map the
original ratings from 0 to 10 to the interval [1, 5] using the
rule rmapped = droriginal /2e except for 0, which was mapped
1
2
3

http://grouplens.org/datasets/movielens/
http://webscope.sandbox.yahoo.com
https://github.com/sidooms/MovieTweetings

to 1. Furthermore, because this dataset is very sparse, we
only kept the users and movies that have at least 10 ratings.
Table 1 shows a summary of the datasets. Note that only
the Yahoo and the synthetic datasets include data entries
with xi,j = 0. These entries were collected for the Yahoo
dataset by eliciting ratings on items selected randomly, not
by the users themselves. In all the other real-world datasets
we only have the entries from the rating matrix R that are
selected by the missing data mechanism, that is, entries for
which xi,j = 1.
6.2. Experimental Protocol
For each dataset, we collect the available rating entries ri,j
with xi,j = 0 in a special test set; this set is only available for the Yahoo and synthetic datasets. After that, we
randomly split the observed ratings (the ri,j with xi,j = 1)
into a training set with 99% of the ratings and a standard
test set with the remaining 1%. We use small standard test
sets to avoid interfering with the actual missing data mechanism. When an ri,j with xi,j = 1 is added to the standard
test set we fix that xi,j to zero to indicate that the entry ri,j
is not observed. Each method is adjusted on each training
set and then evaluated on the corresponding standard and
special test sets by computing the average predictive loglikelihood of the ratings. The whole process is repeated
40 times. The supplementary material contains results for
other performance metrics: root mean squared error, mean
absolute error, and predictive accuracy.
The previous procedure focuses mainly on evaluating the
performance of the complete data model. However, we are
also interested in evaluating the missing data model. For
this we try to find the xi,j that initially took value one but
were flipped to zero during the generation of the standard
test sets. We use recall at N to measure performance as this
is a popular metric in recommendation tasks (Gunawardana
& Shani, 2009). For each row i of X, we use the missing
data model to compute the probability that the variables
xi,j with value zero in that row originally took value one.
We select the top N = 10 variables with highest probability and the fraction of variables xi,j with original value one
that are recovered and average the recall across rows.

Probabilistic Matrix Factorization with Non-random Missing Data
Table 2. Average Log-likelihood on the Standard Test Sets.
MF
MF MM
Dataset
MNAR MAR MAR
ML100K
-1.181 -1.186 -1.471
ML1M
-1.121 -1.125 -1.308
MTweet
-0.941 -0.946 -1.105
NIPS
-0.937 -0.956 -1.204
Yahoo
-1.172 -1.204 -1.278
SMF-MNAR -0.902 -0.937 -1.447
SMF-MAR -0.425 -0.417 -1.327
SRH-MNAR -1.055 -1.067 -0.987
SRH-MAR -1.317 -1.287 -1.272

CTPv
MNAR
-1.463
-1.436
-1.245
-1.170
-1.399
-1.336
-1.238
-0.962
-1.265

Logitvd
MNAR
-1.425
-1.380
-1.141
-1.167
-1.304
-1.326
-1.235
-0.963
-1.266

Paquet
MAR
-1.218
-1.162
-0.997
-0.995
-1.218
-1.000
-0.510
-1.143
-1.318

Table 3. Average Recall on the Standard Test Sets.

Oracle
-1.468
-1.456
-1.235
-1.329
-1.551
-1.331
-1.198
-1.392
-1.498

6.3. Results
Table 2 shows the average test log-likelihood obtained by
each method on the standard rating test sets. The best performing method is highlighted in bold and the statistically
indistinguishable results, according to a paired t-test, are
underlined. MF-MNAR is the best method on the realworld datasets and on the SMF-MNAR dataset. Furthermore, in accordance with intuition, MF-MNAR is better
than MF-MAR on the synthetic datasets with MNAR data,
while the opposite result occurs on the synthetic datasets
with MAR data. The models based on mixtures of multinomials, MM, CPTv and Logitvd, obtain the best performance on the SRH-MNAR and SRH-MNAR datasets. This
is because these datasets were generated by sampling from
multinomials, as assumed by these models. The oracle
method is generally outperformed by most methods since
it predicts the same values for all entries. Finally, Paquet’s
method is less accurate than MF-MNAR and MF-MAR.
Table 3 shows the average recall obtained by the missing
data models of those methods that assume MNAR data. In
this table we have also included the results obtained by the
missing data model of MF-MNAR (MDM) without coupling it with the complete data model. This allows us to
evaluate the gains produced in MDM by using the predictions of the complete data model. We also include the results of a baseline (Freq) that, for each row i of R, ranks
the variables xi,1 , . . . , xi,d with value zero by their empirical frequency across rows. The missing data model of MFMNAR obtains the best results in all cases, except again intuitively on the SMF-MAR and SHR-MAR datasets, where
the MAR assumption is appropriate. Overall, all the methods outperform Freq except CPTv, which performs worst.
Our sample of the SHR-MAR dataset seems to be particularly suited to Logitvd, which performs best in this case.
Finally, Table 4 shows the average test log-likelihood of
each technique on the special test sets for predicting ratings
when xi,j = 0. In this case, the Oracle baseline obtains the
best results on most datasets with MNAR data, assuming
that the Yahoo dataset is MNAR. The Oracle method always makes the same probabilistic prediction for each test
entry. This shows the difficulty of making accurate predictions on MNAR data. Despite this, MF-MNAR is better

Dataset
ML100K
ML1M
MTweet
NIPS
Yahoo
SMF-MNAR
SMF-MAR
SRH-MNAR
SRH-MAR

MF
MNAR
0.299
0.204
0.203
0.309
0.285
0.300
0.438
0.246
0.113

MDM CTPv
MNAR
0.295 0.093
0.204 0.041
0.199 0.127
0.309 0.011
0.283 0.145
0.280 0.038
0.445 0.038
0.245 0.209
0.115 0.134

Logitvd Freq
MNAR
0.130 0.119
0.068 0.077
0.142 0.143
0.009 0.013
0.198 0.182
0.052 0.051
0.051 0.047
0.157 0.121
0.143 0.131

Table 4. Average Log-likelihood on the Special Test Sets.
Dataset
Yahoo
SMF-MNAR
SMF-MAR
SRH-MNAR
SRH-MAR

MF
MF MM
MNAR MAR MAR
-1.578 -1.566 -1.558
-1.215 -1.287 -1.705
-0.446 -0.439 -1.330
-1.682 -1.718 -1.531
-1.298 -1.286 -1.263

CPTv
MNAR
-1.277
-1.770
-2.003
-2.335
-1.393

Logitvd
MNAR
-1.499
-1.724
-1.957
-2.320
-1.403

Paquet
MAR
-1.457
-1.182
-0.535
-1.755
-1.318

Oracle
-1.227
-1.152
-1.201
-1.531
-1.515

than MF-MAR in all of the datasets with MNAR data, except on the real-world Yahoo dataset, where MF-MAR performs better. In the MAR datasets, MF-MAR is better than
MF-MNAR, as expected. Note that Logitvd and CPTv do
not produce any improvement on the SRH-MNAR dataset
with respect to MM. This is because these models make incorrect assumptions about the missing data mechanism that
was used to generate this dataset.

7. Conclusions
We have presented the first practical implementation of
a probabilistic matrix factorization (MF) model for ordinal matrix data with entries missing not at random (MFMNAR). The missing data model in MF-MNAR is a MF
model for binary matrices in which the observation probability of a matrix entry depends on the entry’s value,
with different dependence strengths across rows and across
columns. The complete data model in MF-MNAR is an
ordinal MF method that generates state-of-the-art predictions on rating data on its own. Approximate Bayesian inference in MF-MNAR is implemented using expectation
propagation and variational Bayes. We achieve scalability
to large datasets by using stochastic inference methods that
randomly sub-sample missing matrix entries. The combination of the missing and complete data models in MFMNAR produces gains in both the modeling of the missing
data mechanism and the modeling of the ordinal ratings.

Acknowledgements NMTH is a recipient of the Google
Europe Fellowship in Statistical Machine Learning. JMH
acknowledges support from Infosys Labs, Infosys Limited
and from the Spanish Dirección General de Investigación,
project ALLS (TIN2010-21575-C02-02).

Probabilistic Matrix Factorization with Non-random Missing Data

References
Chu, Wei and Ghahramani, Zoubin. Gaussian processes
for ordinal regression. In Journal of Machine Learning
Research, pp. 1019–1041, 2005.
Ghahramani, Z. and Beal, M. J. Advanced Mean Field
Method—Theory and Practice, chapter Graphical models and variational methods, pp. 161–177. 2001.
Gunawardana, Asela and Shani, Guy. A survey of accuracy evaluation metrics of recommendation tasks. The
Journal of Machine Learning Research, 10:2935–2962,
2009.
Hoffman, Matthew D., Blei, David M., Wang, Chong, and
Paisley, John. Stochastic variational inference. Journal
of Machine Learning Research, 14:1303–1347, 2013.
Kschischang, Frank R., Frey, Brendan J., and Loeliger,
H.-A. Factor graphs and the sum-product algorithm.
IEEE Transactions on Information Theory, 47(2):498–
519, 2001.
Lakshminarayanan, Balaji, Bouchard, Guillaume, and Archambeau, Cedric. Robust Bayesian matrix factorisation. In International Conference on Artificial Intelligence and Statistics, pp. 425–433, 2011.
Little, R.J.A. and Rubin, D.B. Statistical Analysis With
Missing Data. Wiley Series in Probability and Statistics Applied Probability and Statistics Section Series. Wiley,
1987.
Marlin, Benjamin M. and Zemel, Richard S. Collaborative filtering and the missing at random assumption. In
Proceedings of the 23rd Conference on Uncertainty in
Artificial Intelligence, 2007.
Marlin, Benjamin M. and Zemel, Richard S. Collaborative
prediction and ranking with non-random missing data.
In Proceedings of the Third ACM Conference on Recommender Systems, RecSys ’09, pp. 5–12, 2009.
Minka, Thomas P. Expectation propagation for approximate bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in Artificial Intelligence, pp. 362–369, 2001.
Mnih, Andriy and Salakhutdinov, Ruslan. Probabilistic
matrix factorization. In Advances in neural information
processing systems, pp. 1257–1264, 2007.
Paquet, Ulrich, Thomson, Blaise, and Winther, Ole. A hierarchical model for ordinal matrix factorization. Statistics
and Computing, 22(4):945–957, 2012.

Rendle, Steffen, Freudenthaler, Christoph, Gantner, Zeno,
and Schmidt-Thieme, Lars. BPR: Bayesian personalized
ranking from implicit feedback. In Proceedings of the
Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 452–461, 2009.
Steck, Harald. Training and testing of recommender systems on data missing not at random. In Proceedings
of the 16th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’10, pp.
713–722, 2010.
Stern, David H, Herbrich, Ralf, and Graepel, Thore.
Matchbox: large scale online bayesian recommendations. In Proceedings of the 18th international conference on World wide web, pp. 111–120. ACM, 2009.

