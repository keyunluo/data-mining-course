Towards an optimal stochastic alternating direction method of multipliers

SAZADI 156@ GMAIL . COM

Samaneh Azadi
UC Berkeley, Berkeley, CA
School of ECE, Shiraz University, Shiraz, Iran
Suvrit Sra
Carnegie Mellon University, Pittsburgh
Max Planck Institute for Intelligent Systems, Tübingen, Germany

Abstract

The linear constraints in (1) allow us to decouple f and
h, and thereby consider sophisticated regularizers without
having to rely on carefully tuned proximity operators—
this can greatly simplify the overall optimization algorithm
and is a substantial gain (Ouyang et al., 2013; Boyd et al.,
2011).

We study regularized stochastic convex optimization subject to linear equality constraints.
This class of problems was recently also studied by Ouyang et al. (2013) and Suzuki (2013);
both introduced similar stochastic alternating direction method of multipliers (SADMM) algorithms. However, the analysis of both papers
led to suboptimal convergence rates. This paper presents two new SADMM methods: (i) the
first attains the minimax optimal rate of O(1/k)
for nonsmooth strongly-convex stochastic problems; while (ii) the second progresses towards
an optimal rate by exhibiting an O(1/k 2 ) rate
for the smooth part. We present several experiments with our new methods; the results indicate
improved performance over competing ADMM
methods.

Using linear constraints to “split variables” is an idea that
has been most impressively exploited by the alternating
direction method of multipliers (ADMM). As a consequence, ADMM leads to methods that are easy to implement, scale well, and are widely applicable—these benefits are eminently advocated in recent survey (Boyd et al.,
2011), and is also the primary motivation of (Ouyang et al.,
2013; Suzuki, 2013). Indeed, these benefits have also borne
through in applications such large-scale lasso (Boyd et al.,
2011), constrained image deblurring (Chan et al., 2013),
and matrix completion (Goldfarb et al., 2012), to name a
few.

1. Introduction
We study stochastic optimization problems of the form
min (f (x) := E[F (x, ξ)]) + h(y),
s.t. Ax + By = b, and x ∈ X , y ∈ Y,

(1)

where ξ ∫follows some distribution over a space Ξ, so that
f (x) = Ξ F (x, ξ)dP (ξ), and for each ξ, function F (·, ξ)
is closed and convex. The function h is assumed to be
closed and convex, while X and Y are compact convex sets.
Problem (1) enjoys great importance in machine learning: the function f (x) typically represents a loss over all
data, while h(y) enforces structure or regularizes the learning model and aids generalization (Srebro & Tewari, 2010).
st

SUVRIT @ TUEBINGEN . MPG . DE

Proceedings of the 31 International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

But despite their broad applicability, traditional ADMM
methods cannot handle stochastic optimization, a drawback recently circumvented by Ouyang et al. (2013)
and (Suzuki, 2013), who approached (1) using an ADMM
strategy combined with ideas from ordinary stochastic
convex optimization. Both (Ouyang et al., 2013; Suzuki,
2013) showed some experiments that suggested benefits
of combining stochastic ideas with an ADMM strategy.
A key benefit of such a combination is that it allows
one to tackle stochastic problems with sophisticated regularization penalties such as graph-structured norms and
overlapping group norms (Parikh & Boyd, 2013), more
easily than either batch or online proximal splitting
methods (Ghadimi & Lan, 2012; Duchi & Singer, 2009;
Beck & Teboulle, 2009).
We remark that (deterministic) ADMM family of methods are now widely used and substantial engineering effort has been invested into deploying them, both in research as well as industry (see e.g., (Boyd et al., 2011); and

Towards optimal stochastic ADMM

also (Kraska et al., 2013)). Thus, enriching ADMM to handle stochastic optimization problems may be of great practical value.
Contributions. The main contributions of this paper are:
• A new SADMM algorithm (Alg. 2) for strongly convex
stochastic optimization that achieves the minmax optimal O(1/k) convergence rate; this improves on the previously shown suboptimal O(log k/k) rates (Ouyang et al.,
2013; Suzuki, 2013).
• A new SADMM algorithm (Alg. 3) for stochastic
convex problems with Lipschitz continuous gradients;
this method achieves an O(1/k 2 ) rate in the smooth
component, improving on the previous O(1/k) rates
of Ouyang et al. (2013); Suzuki (2013).
Empirical results (§4) indicate that when f is strongly
convex or smooth, our new methods outperform basic
SADMM. We note in passing that our analysis extends to
yield high-probability bounds assuming light-tailed on the
errors; this extension is fairly routine, so we omit it for lack
of space (see §5 also for other possible extensions).

2. Background

Consider the classic convex optimization problem
f (x) + h(Ax − b),

f (x)+h(y),

x ∈ X,

(2)

x ∈ X , Ax−y−b = 0, y ∈ dom h.

ADMM considers the slightly more general problem
min

x∈X ,y∈Y

+ β2 ∥Ax + By − b∥22 +

1
2ηk ∥x

− xk ∥22 , (5)

where gk is a stochastic (sub)gradient of f , i.e., E[gk ] ∈
∂f (xk ), where gk ∈ ∂F (x, ξk+1 ). Replacing Lβ by this
iteration dependent Lkβ in Alg. 1 one obtains the SADMM
method of (Ouyang et al., 2013). The ∥x − xk ∥22 prox-term
ensures that (5) has a unique solution, even if the augmented Lagrangian (AL) fails to be strictly convex; it also
aids the convergence analysis.
1 Initialize: x0 , y0 , and λ0 .
2 for k ≥ 0 do
3
xk+1 ← argminx∈X {Lβ (x, yk , λk )}
4
yk+1 ← argminy∈Y {Lβ (xk+1 , y, λk )}
5
λk+1 ← λk − β(Azk+1 + Byk+1 − b)
6 end

f (x) + h(y),

Ax + By − b = 0,

Since SADMM borrows techniques from stochastic subgradient methods, it is natural to expect similarities in convergence guarantees. Previous authors (Ouyang et al., 2013;
Suzuki, 2013)
∑ showed that for uniformly averaged iterates
(x̄k := k1 j xj , etc.) one obtains the approximation
bound
E[f (x̄k )+h(ȳk )−f (x∗ )−h(y ∗ )+ρ∥Ax̄k + B ȳk − b∥2 ] = O( √1k ).

where f , h are closed convex functions and X is a closed
convex set. Introducing y = Ax − b, problem (2) becomes
min

Lkβ (x, y, λ) := f (xk )+⟨gk , x⟩+h(y)−⟨λ, Ax + By − b⟩

Algorithm 1: ADMM

ADMM is a special case of the DouglasRachford (Douglas & Rachford, 1956) splitting method,
which itself may be viewed as an instance of the proximalpoint algorithm (Rockafellar, 1976; Eckstein & Bertsekas,
1992). But before discussing SADMM, let us first recall
some material about ADMM.

minx

∫
For stochastic problems with f (x) = Ξ F (x, ξ)dP (ξ)
over a potentially unknown distribution P , the standard
ADMM scheme is no longer applicable. Ouyang et al.
(2013) suggest linearizing f (x) by considering a modified
augmented Lagrangian that now depends on subgradients
of f . Specifically, the augmented Lagrangian they use is

(3)

to solve which it introduces an augmented Lagrangian
Lβ (x, y, λ) := f (x) + h(y) − ⟨λ, Ax + By − b⟩
+ β2 ∥Ax + By − b∥22 . (4)
Here λ is the dual variable, and β is a penalty parameter.
ADMM minimizes (4) over x and y in a Gauss-Seidel manner, followed by a step that updates the dual variable. The
resulting method is summarized as Alg. 1.

Exploiting properties of the stochastic part f , we can obtain
more refined rates. Specifically, if f is strongly convex, the
rate E[·] = O( logk k ), and if f is Lipschitz smooth, the rate
E[·] = O( k1 ) + O( √1k ) was shown by both Ouyang et al.
(2013) and Suzuki (2013) (though for the RDA-ADMM
variant in (Suzuki, 2013), no refined rate for strongly convex losses was proved).
However, these rates are suboptimal. It is of great
interest to obtain optimal rates—see the long line of
work in stochastic optimization (Nemirovski et al., 2009;
Chen et al., 2012; Shamir & Zhang, 2013; Ghadimi & Lan,
2012), where impressive effort has been expended to
obtain optimal rates; this effort can even translate into
improved empirical performance. For deterministic settings, notable examples of optimal methods are given
by (Beck & Teboulle, 2009; Nesterov, 2007), which often
substantially outperform their non-optimal counterparts.
In light of this background, we are now ready to present
new SADMM methods, which achieve the minimax opti-

Towards optimal stochastic ADMM

mal O( k1 ) rate for strongly convex losses. Without strong
convexity, the rate for the nonsmooth and stochastic parts
is O( k1 )+O( √1k ), with a more refined (and optimal) O( k12 )
contribution from the smooth part of the objective.

3. SADMM
We begin by stating our key structural assumptions:
1. Bounded subgradients: E[∥gk ∥22 ] ≤ G2 (for the
strongly convex case)
2. Bounded noise variance: E[∥gk − ∇f (x)∥22 ] ≤ σ 2
(for the smooth case).
3. Compactness of X , Y; bounded dual variables.

Following He & Yuan (2012) we introduce the notation

u, v ∈ dom F.

(6)

∆k := ∥xk − x∥22 , k = 0, 1, . . . ,

y∈Y

Here, ∆k is related to the diameter of the primal variable
x ∈ X , Ak measures how well the linear constraints are
satisfied, Lk measures distance between dual variables, DY
measures a diameter-like term for the primal variable y ∈
Y, while ρ is a parameter that bounds the dual variables.

Lemma 1. Let f be µ-strongly convex, and let xk+1 , yk+1
and λk+1 be computed as per Alg. 2. For all x ∈ X and
y ∈ Y, and w ∈ Ω, it holds for k ≥ 0 that
f (xk ) − f (x) + h(yk+1 ) − h(y) + ⟨wk+1 − w, F (wk+1 )⟩

(7)

Moreover, for an optimal w∗ ∈ Ω, and any w ∈ Ω, we have
f (x)−f (x∗ )+h(y)−h(y ∗ )+⟨w − w∗ , F (w∗ )⟩ ≥ 0. (8)
Therefore, a vector w̄ ∈ Ω is ϵ-optimal for the deterministic
ADMM problem (for ϵ > 0) if it satisfies
f (x̄)−f (x)+h(ȳ)−h(y)+⟨w̄ − w, F (w)⟩ ≤ ϵ,

We begin our analysis by introducing some more notation

Lemma 1 is a key result that describes progress made at one
step. Upon taking suitable expectations, it leads to Thm. 2.

The operator F (·) satisfies a simple but useful property
⟨u − v, F (u) − F (v)⟩ = 0,

xk+1 ← argminx∈X {Lkβ (x, yk , λk )}
5
yk+1 ← argminy∈Y {Lkβ (xk+1 , y, λk )}
6
λk+1 ← λk − β(Axk+1 + Byk+1 − b)
7 end
Algorithm 2: Stochastic ADMM (strongly convex)
4

Ak := ∥Ax + Byk − b∥22 , Lk := ∥λ − λk ∥22
DY := sup ∥B(y − y ∗ )∥2 , ∥λk ∥2 ≤ ρ.

Unfortunately, we must impose somewhat stricter assumptions than ordinary SADMM—this seems to be the price
that we have to pay for faster convergence—the discussion
in (Chambolle & Pock, 2011) sheds more light on these aspects (especially Assumption 3).

w := [xT ; y T ; λT ]T , wk := [xTk ; ykT ; λTk ]T ,


 
−AT λ
X
F (w) :=  −B T λ  , Ω :=  Y  .
Rm
Ax + By − b

1 Initialize: x0 , y0 , and λ0
2 for k ≥ 0 do
3
Obtain stochastic gradient gk ; build Lkβ via (5)

∀w ∈ Ω.

≤ η2k ∥gk ∥22 − µ2 ∆k +

1
2ηk [∆k

− ∆k+1 ] + β2 [Ak − Ak+1 ]

1
+ 2β
[Lk − Lk+1 ] + ⟨δk , xk − x⟩.

To use Lemma 1 to obtain an optimal SADMM method, we
use an idea that has also found success for the stochastic
subgradient method—see e.g., (Lacoste-Julien et al., 2012;
Shamir & Zhang, 2013)—the idea is to use nonuniform averaging of the iterates where more recent iterates are given
higher weight. For SADMM, some care is required to ensure that the nonuniform weighting does not conflict with
the augmented Lagrangian (AL) terms.

As in (He & Yuan, 2012), Ouyang et al. (2013) also use
this variational characterization of optimality and seek to
bound it in expectation. We too use this characterization;
we first estimate it after one step of our SADMM algorithm
to eventually bound it in expectation. We are now ready to
describe the our SADMM algorithms (§3.1 and §3.2).

We propose to use the following weighted iterates:1
∑k−1
∑k
2
2
x̄k := k(k+1)
(j + 1)xj , ȳk := k(k+1)
jyj ,
j=0
j=1
∑k
2
λ̄k := k(k+1)
jλj .
(9)

3.1. SADMM for strongly convex f

It is important to note that these weighted averages can be
maintained in an online manner. Indeed, given x̄k−1 , we

When f is µ-strongly convex, we use essentially the same
SADMM method as in (Ouyang et al., 2013) (shown as
Alg. 2). The key difference lies in how the iterates generated by the Alg. 2 are averaged to obtain an optimal rate.

1
Since x is treated asymmetrically from y by all SADMM variants (optimizing x involves subgradients, y does not), it is no surprise that the weighted average of the previous iterates that we use
for x is slightly different from what we use for y.

j=1

Towards optimal stochastic ADMM

can update the weighted average as
x̄k = (1 − θk )x̄k−1 + θk xk ,

k ≥ 1,

(10)

where θk = 2/(k + 2); similar updates apply for ȳk and λ̄k .
These weighted averages in combination with Lemma 1
help prove our main theorem on SADMM.
2
Theorem 2. Let f be µ-strongly convex. Let ηk = µ(k+2)
,
let x, yj , λj be generated by Alg. 2, and x̄k , ȳk , λ̄k computed by (9). Let x∗ , y ∗ be the optimal; then for k ≥ 1,

We begin our analysis by again stating a key lemma that
measures per-step progress; here we use slightly different
notation by redefining in w and wk (6) as
w := [z T , y T , λT ]T ,

Lemma 3. Let xk+1 , yk+1 , zk+1 be generated by Alg. 3.
For x ∈ X , y ∈ Y and w ∈ Ω, and with ηk = (L + αk )−1
the following bound holds for all k ≥ 0:
f (xk+1 ) + θk [h(yk+1 ) − h(y)]
+ θk ⟨wk+1 − w, F (wk+1 )⟩
≤ (1 − γk )f (xk ) + γk f (x) +

E[f (x̄k ) − f (x∗ ) + h(ȳk ) − h(y ∗ ) + ρ∥Ax̄k + B ȳk − b∥2 ]
≤

2G2
µ(k+1)

+

β
2
2(k+1) DY

+

Obtaining an optimal version of SADMM for Lipschitzsmooth f ∈ CL1 proves considerably harder.

1
2
3
4
5
6
7
8
9

+

2ρ2
β(k+1) .

3.2. SADMM for smooth f

Input: Sequence (γk ) of interpolation parameters;
(ηk = (L + αk )−1 ), stepsizes
Initialize: x0 = z0 , y0 .
for k ≥ 0 do
pk ← (1 − γk )xk + γk zk
Get stochastic gradient gk s.t. E[gk ] = ∇f (pk )
zk+1 ← argminx∈X {L̂kβ (x, yk , λk )}
xk+1 ← (1 − γk )xk + γk zk+1
yk+1 ← argminy∈Y {L̂kβ (zk+1 , y, λk )}
λk+1 ← λk − β(Azk+1 + Byk+1 − b)
end
Algorithm 3: SADMM for smooth f (x)

Alg. 3 depends on several careful modifications to the basic SADMM scheme. First, it uses interpolatory sequences
(pk ) and (zk ), as well as “stepsizes” γk (this is inspired
by techniques from fast-gradient methods (Tseng, 2008;
Nesterov, 2004)). Second, x is updated (cf. Line 4 in
Alg. 2) by first computing zk+1 , which in turn uses a
weighted prox-term that enforces proximity to zk instead of
to xk . Third, the update to y uses an AL term that depends
on zk+1 instead of xk+1 —this change is for simplifying
the analysis; one could continue to use an AL term based
on xk+1 , but at the expense of much more tedious analysis. Finally, an important modification is to the augmented
Lagrangian, which is now defined as

wk := [zkT , ykT , λTk ]T .

+

2
1
2αk ∥δk ∥2 + γk ⟨δk ,
γk
2β [Lk − Lk+1 ] .

zk − x⟩

γk2
2ηk [∆k − ∆k+1 ]
+ βγ2 k [Ak − Ak+1 ]

The proof of this inequality is lengthy and tedious, so we
leave it in the supplement. Lemma 3 proves crucial for
doing the induction to obtain the next main step towards our
convergence proof. Let R := supx∈X ∥x − x∗ ∥2 . Then,
Lemma 4. Using the notation of Lemma 3, for
1
γk2

and x such that f (xk ) ≥ f (x) (∀k), we have
1
(f (xk+1 )
γk2

− f (x)) +

∑k

1
j=1 γj

1−γk+1
2
γk+1

≤

[h(yj+1 ) − h(y)]

θ

≤
+

L+αk 2
2 R

+

∑k

β
2

+ γjj [⟨wj+1 − w, F (wj+1 )⟩]
∑k
∑k
1
Aj + 2β
Lj
j=1

1
∥δj ∥22
2
j=1 γj αj

+

j=1

1
γj ⟨δj ,

zj − x⟩.

As before, to state our convergence theorem, we average
the iterates generated by Alg. 3 non-uniformly. This technique is borrowed from the analysis of accelerated methods,
see e.g., (Ghadimi & Lan, 2012). To use Lemma 4 to obtain
our main convergence result, we introduce weighted candidate solution vectors. For k ≥ 0, we define the weighted
iterates (it is important to note that these weighted averages can be easily maintained in an online manner (cf. formula (10)):
∑k
x̄k := xk+1 , ȳk :=
νj yj+1 ,
j=1
(12)
∑k
∑k
z̄k :=
νj zj+1 , λ̄k :=
νj λj ,
j=1

j=1

where νj = 2(j + 1)/(k + 1)(k + 2). Since f (x) is smooth,
it turns out that in (12) we do not need to average over xk+1 ,
thereby obtaining “non-ergodic” convergence in expectation for the smooth part. This is an interesting technical
L̂kβ (x, y, λ) := f (xk )+⟨gk , x⟩+h(y)−θk ⟨λ, Ax + By − b⟩
difference from nonsmooth f (x), where one needs to averγk
2
age over the x iterates too unless one is willing to pay an
∥x
−
z
∥
,
(11)
+ βθ2k ∥Ax + By − b∥22 + 2η
k 2
k
additional log k penalty (Shamir & Zhang, 2013). Finally,
for suitable parameters (θk , γk ).
we have the following theorem.

Towards optimal stochastic ADMM

Theorem 5. Let x̄k , z̄k , ȳk , and λ̄k be as defined in (12).
Then for θj = 1 and k ≥ 0,
1
2 E[f (x̄k )
γk

− f (x∗ ) + h(ȳk ) − h(y ∗ ) + ρ∥Az̄k + B ȳk − b∥2 ]

k
(L + αk )R2
β(k + 1) 2
(k + 1) 2 ∑ σ 2
≤
+
DY +
ρ +
.
2
2
β
γ 2 αj
j=1 j

training sample ξ. This formulation is a special case of (1)
with A = F , B = −I and b = 0. The corresponding steps
of Alg. 3 assume the form
pk ← (1 − γk )xk + γk zk ,

gk ← L′ (pk , ξk+1 ) + γk pk

zk+1 ← ( γηkk I + βθk F T F )−1 [θk F T (βyk + λk )
+

γk
ηk zk

An immediate corollary is our refined result on the convergence rate of SADMM with a smooth stochastic objective
(notice that h is assumed to be nonsmooth):

xk+1 ← (1 − γk )xk + γk zk

Corollary 6. Let αj = c−1 σ(j + 1)3/2 (for a constant c),
and γj = 2/(j + 1); then

λk+1 ← λk − βF zk+1 + βyk+1 ,

E[f (x̄k ) − f (x∗ ) + h(ȳk ) − h(y ∗ ) + ρ∥Az̄k + B ȳk − b∥2 ]
≤

2
2βDY
2LR2
2ρ2
2σ(c−1 + c)
√
+
+
+
.
(k + 1)2
k+1
β(k + 1)
k+1

Observe that when there is no noise (σ = 0), our analysis
can be slightly modified to yield the bound
E[f (x̄k ) − f (x∗ ) + h(ȳk ) − h(y ∗ ) + ρ∥Az̄k + B ȳk − b∥2 ]
≤

2
2βDY
2LR2
2ρ2
+
+
.
(k + 1)2
k+1
β(k + 1)

4. Experiments
In this section we present experiments that illustrate performance of our SADMM variants. The results indicate that
our methods converge faster (on the generalization error)
than previous SADMM approaches. We note that for all all
experiments, we set the AL parameter β = 1, as also done
in Ouyang et al. (2013).
4.1. GFLasso with smooth loss
Our first experiment follows Ouyang et al. (2013), wherein
we consider the Graph-guided fused lasso (GFlasso). This
problem uses a graph-based regularizer where variables are
considered as vertices of the graph and the difference between two adjacent variables is penalized according to the
edge weight. This leads to the optimization problem:
∑
min E[L(x, ξ)] + λ∥x∥1 + ν
wij |xi − xj |,
{i,j}∈E

(13)
where E is the set of edges in the graph, and wij is the
weight for the edge between xi and xj . To verify performance of Alg. 3 we consider the following “large-margin”
modification to (13):
min E[L(x, ξ)] + λ2 ∥x∥22 + ν∥y∥1 ,

s.t.

F x − y = 0,

where Fij = wij , Fji = −wij for all edges {i, j} ∈ E, and
L(x, ξ) = 21 (l − xT s)2 for (s, l) feature label pair in the

yk+1 ← S βθν (F zk+1 −
k

− gk ]

λk
β )

where Sα (x) denotes the standard soft-thresholding operator As in Ouyang et al. (2013), we obtain F by sparse inverse covariance selection Banerjee et al. (2008) to determine the adjacency matrix of the graph by thresholding the
sparsity pattern of the inverse covariance matrix.
We
compare
the
following
methods:
SADMM (Ouyang et al., 2013), Alg. 3 (called OptimalSADMM2 ), ordinary stochastic gradient descent (SGD),
proximal-SGD (aka FOBOS (Duchi & Singer, 2009)), and
online RDA (Xiao, 2010). We compare these methods on
a version of the well-known 20newsgroups dataset3 . This
dataset consists of binary occurrence data of 100 words
for 16,242 instances, and the samples are labeled into four
categories for which one can do classification by one-vs
rest scheme multiclass classification. In Fig. 1, we show
prediction accuracy on test data (20% of samples) and
the training performance as measured by the objective
function value.
To implement proximal-SGD and online-RDA, the two
methods that require computing the proximity operator
1
2
2 ∥x − y∥2 + λ∥F x∥1 , we implemented an inexact QPsolver that solves the corresponding dual problem4 :
min ∥F T u − y∥22

s.t.

∥u∥∞ ≤ β.

If u∗ is the optimal dual solution, we can recover the primal
solution by setting x∗ = y − f T u∗ .
Fig. 1 shows that on the training data SADMM and
Optimal-SADMM converge faster than the other methods.
The classification performance of all methods is similar,
except Optimal-SADMM which achieves higher test accuracy (notice #-iterations refers to number of training data
points seen). Also, once we made a single pass through the
training data we terminate all the methods.
2

We refer to both our SADMM variants as ‘OptimalSADMM’.
3
Obtained from http://www.cs.nyu.edu/ roweis/data.html
4
This dual problem is just a box-constrained quadratic program, which we solved using the well-known freely available implementation of the LBFGS-B method.

Towards optimal stochastic ADMM
Smooth f: Accuracy %

Smooth f: Accuracy %
90

Classification Accuracy %

Classification Accuracy %

85

75

SGD
Proximal SGD
Online-RDA
RDA-Admm
SADMM
optimal-SADMM
65
0

1300

3250

5200

− li xT si )2 + γ2 ||x||22 + ν||F x||1
1
i 2 (1

P
1

train

N

5

10

15

20

25

30

0.75

SGD
Proximal SGD
Online-RDA
RDA-Admm
SADMM
optimal-SADMM

0.7

80

CPU time (s)
Smooth f: Objective Function

1

0.8

85

75
0

6500

# of iterations
Smooth f: Objective Function
0.9

RDA-Admm
SADMM
Optimal SADMM

RDA-Admm
SADMM
Optimal SADMM

0.65

0.6

0.55

0.5
0.4

0.45

0.3

0.35

0.2
0.1
0

1300

3250

5200

6500

# of iterations

0.25
0

5

10

15

20

25

30

CPU time (s)

Figure 1. Graph Regularization for 20newsgroups dataset with
smooth loss. #iterations refers to #-training data points seen. Upper figure shows the test data accuracy (after seeing about 1300
training points, Optimal-SADMM outperforms the other methods); the lower one shows training data objectives.

Figure 2. Graph Regularization for adult dataset with smooth loss.
For this problem SADMM and Optimal-SADMM perform similarly; both substantially outperform RDA-ADMM.

4.2. Overlapped group lasso

classification task in which f (x, ξ) and h(y) are defined as:
∑10
f (x, ξ) = 0.1
L(x, ξj ),
(15)
j=1
(
)
1
h(y) = C ∥x(1) ∥1 + √123
∥x(2) ∥block ,

In our second experiment, we present overlapped group
lasso results, as explained in (Suzuki, 2013). Here,

where
L(x, ξ∑
= log(1 + e−lj sj x ), ∥x∥block =
j)
∑
j ∥X.,j ∥2 where X denotes a reshaped veri ∥Xi,. ∥2 +
sion of x as a square matrix; observe that L(x, ξ) is a logistic loss and h(y) is the overlapping group lasso regularizer.
T

h(x) = C

∑

∥xg ∥ =: C∥x∥G ,

(14)

g∈G

where G is a set of groups of indices. Feature selection using non-overlapping groups of features by the Lasso can
be extended to the group Lasso. But using only nonoverlapping groups limits the discoverable structures in
practice. One of the solutions to handle this problem
is to allow overlapping groups accompanied with the following settings. We divide G into m non-overlapping
subsets G1 , · · · , Gm , and let Ax be a concatenation of
m-repetitions
of x. Thus, h([x; · · · ; x]) = h(Ax) =
∑m
C i=1 ∥x∥Gi . With this formulation, we can easily solve
the optimization problem using a proximal operation for
each subset; see (Qin & Goldfarb, 2012) for more details.
We applied our optimal SADMM on a dataset for a binary

We used the dataset ‘adult’5 which contains 123 dimensional feature vectors. Following Suzuki (2013) we also
augmented the feature space by taking products of features
resulting in (123 + 1232 ) dimensions. Vector x(1) in (15) is
related to the 123-first elements of x, while x(2) represents
the rest of x. Hyperparameter C is set to 0.01. Moreover,
we used mini-batches of size 10 for each iteration.
We present plots on the test data classification accuracy as
well as the training data objective functions. We compare
ordinary SADMM, Optimal-SADMM, and RDA-ADMM.
On this task, the difference between SADMM and OptimalSADMM is not remarkable, but both substantially outperform RDA-SADMM, as seen from Fig. 2.
5

Obtained from the LIBSVM datasets webpage.

Towards optimal stochastic ADMM
Strongly Convex f: Accuracy %

Strongly Convex f: Accuracy %
Classification Accuracy %

85

65

55

SGD
Proximal SGD
SADMM
optimal-SADMM
1300

3250

5200

1

train
N

75

65

SGD
Proximal SGD
SADMM
optimal-SADMM

55
0

6500

# of iterations
Strongly Convex f: Objective Function

0.5

1

1.5

2

2.5

3

4
# of iterations
x 10
Strongly Convex f: Objective Function

1

5

SGD
Proximal SGD
SADMM
optimal-SADMM

f(x, ξ) + h(y)

0.9

SGD
Proximal SGD
SADMM
optimal-SADMM

4.5

1

0.7

train

P

i

0.8

P

1
i 2 (li

− xT si )2 + γ2 ||x||22 + ν||F x||1

45
0

N

Classification Accuracy %

75

4
3.5
3
2.5
2
1.5
1

0.6
0

1300

3250

5200

6500

# of iterations

0.5
0

0.5

1

1.5

2

2.5

# of iterations

3
4
x 10

Figure 3. Graph Regularization for 20newsgroups dataset with
strongly convex loss.

Figure 4. Group Regularization for adult dataset with strongly
convex loss.

4.3. Strongly Convex Loss Functions
Now we show a more detailed comparisons between
SGD, Proximal SGD, strongly convex SADMM
from (Ouyang et al., 2013), and our Optimal-SADMM
version of Alg. 2. In this case, we compare the mentioned algorithms on a nonsmooth but strongly convex
GFLasso and Group Lasso problems, which use the
hinge loss for L(x, ξ) in (4.1) and (15), respectively
(L(x, ξ) = max{0, 1 − lsT x}). Other terms remain the
same. The closed form updates are similar to those in the
previous section, except that xk is used instead of zk ; also
x̄k is computed as per (10). Step size equal to k1 is used for
the SGD and proximal SGD methods. The classification
accuracy on hold out test data and the objective function
value on the training data are plotted in Figures 3 and 4.
The plots indicate that the proposed algorithm significantly
outperforms other methods, both in terms of training
objective value and classification accuracy except for the
objective function value on the training data in Fig. 3 in
which our optimal-SADMM training performance dips a
bit in comparison with SGD and proximal-SGD methods.

varies. For this experiment, we generated a synthetic
dataset, and performed classification using the smooth formulation (4.1). The data is generated as follows: we sample a matrix of m samples with n features following a
multivariate Gaussian with a random covariance matrix.
The true weight vector x∗ is chosen from an i.i.d. standard normal; the labels are defined according to lm =
sgn(sTm x∗ +ϵm ) in which ϵm is a mean zero Gaussian noise
with standard deviation of 2.

4.4. Experiment on Synthetic Data
Here, we intend to explore the behavior of smooth
SADMM variants as the number of features in the data

Fig. 5 reports percentage improvement of OptimalSADMM over SADMM in terms of classification accuracy
as a function of number of features. This experiment suggests that our optimal SADMM may use features more efficiently, especially with increasing feature dimension. Exploring this phenomenon more closely is ongoing work.

5. Conclusions and future work
We presented two new accelerated versions the stochastic
ADMM (Ouyang et al., 2013). In particular, we presented a
variant that attains the theoretically optimal O(1/k) convergence rate for strongly convex stochastic problems. When
the stochastic part is smooth, we showed another SADMM
algorithm that has an optimal O(1/k 2 ) dependence on the
smooth part.

Towards optimal stochastic ADMM
Accuracy improvement using Accelerated SADMM
60

Banerjee, O., Ghaoui, L. E., and d’Aspremont, A. Model
selection through sparse maximum likelihood estimation
for multivarite Gaussian or binary data. JMLR, 9:485–
516, 2008.

% improvement

50
40
30

Beck, A. and Teboulle, M. A Fast Iterative ShrinkageThresholding Algorithm for Linear Inverse Problems.
SIAM J. Imaging Sciences, 2(1):183–202, 2009.

20
10
0

References

10 50

100

200

300

500

Number of features

Figure 5. Feature efficiency of Optimal-SADMM vs SADMM
(smooth loss).

Our initial experiments reveal that our accelerated variants do exhibit notable performance gains over their
non-accelerated counterparts (see §4, also Fig. 3)—
though, obviously as also seen from the experiments
in (Lacoste-Julien et al., 2012; Shamir & Zhang, 2013),
gains in stochastic settings are less dramatic than in
the deterministic case (Beck & Teboulle, 2009). This is
not surprising, since accelerated methods are more sensitive to stochastic noise than their deterministic counterparts (Devolder et al., 2011).
There will be more results and details available in the
longer arXiv version of the paper. We mention below a
list of extensions to the present paper:
• Transfer the O(log k/k) convergence rate of the last
iterate as done for SGD by Shamir & Zhang (2013) to
the SADMM setting.
• Obtaining high-probability bounds under light-tailed
assumptions on the stochastic error.
• Incorporate the impact of sampling multiple stochastic gradients to decrease the variance in the gradient
estimates.
• Derive a mirror-descent version.
• Improve rate dependence of the augmented Lagrangian part to O(1/k 2 ) for smooth problems.
Most, except the last, of these extensions are easy (though
tedious) and follow by invoking standard techniques from
the analysis of stochastic convex optimization. We hope to
address these in a longer version of the present paper.
We conclude by highlighting that our empirical results are
encouraging and suggest that for strongly convex or smooth
losses, our accelerated SADMM variants outperform the
other known SADMM methods.

Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statistical learning via the alternating direction method of multiR in Machine Learning,
pliers. Foundations and Trends⃝
3(1):1–122, 2011.
Chambolle, A. and Pock, T. A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging. J. Math. Imaging Vis., 40(1):120–145, 2011.
Chan, R. H., Tao, M., and Yuan, X. Constrained total variation models and fast algorithms based on alternating direction method of multipliers. SIAM J. Imaging Sci., 6
(1), 2013.
Chen, Xi, Lin, Qihang, and Pena, Javier. Optimal regularized dual averaging methods for stochastic optimization.
In Advances in Neural Information Processing Systems
(NIPS-12), pp. 404–412, 2012.
Devolder, O., Glineur, F., and Nesterov, Yu. First-order
methods of smooth convex optimization with inexact oracle. Technical Report 2011/17, UCL, 2011.
Douglas, J. and Rachford, H. H. On the numerical solution
of the heat conduction problem in 2 and 3 space variables. Tran. Amer. Math. Soc., 82:421–439, 1956.
Duchi, J. and Singer, Y. Online and Batch Learning using Forward-Backward Splitting. J. Mach. Learning Res.
(JMLR), 2009.
Eckstein, J. and Bertsekas, D. P.
On the DouglasRachoford splitting method and the proximal point algorithm for maximal monotone operators. Math. Prog.,
55(3):292–318, 1992.
Ghadimi, S. and Lan, G. Optimal stochastic approximation
algorithms for strongly convex stochastic composite optimization, i: a generic algorithmic framework. SIAM J.
Optimization, 22:1469–1492, 2012.
Goldfarb, D., Ma, S., and Scheinberg, K. Fast alternating linearization methods for minimizing the sum of two
convex functions. Math. Prog. Ser. A, 2012.
He, B. and Yuan, X. On non-ergodic convergence rate of
Douglas-Rachford alternating direction method of multipliers. Unpublished, 2012.

Towards optimal stochastic ADMM

Kraska, T., Talwalkar, A., J.Duchi, Griffith, R., Franklin,
M., and Jordan, M.I. MLbase:A Distributed Machine
Learning System. In Conf. Innovative Data Systems Research, 2013.
Lacoste-Julien, S., Schmidt, M., and Bach, F. A simpler
approach to obtaining an O(1/t) convergence rate for
projected subgradient descent. arXiv:1212.2002, 2012.
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–
1609, 2009.
Nesterov, Y. Introductory Lectures on Convex Optimization: A Basic Course. Springer, 2004.
Nesterov, Yu. Gradient methods for minimizing composite
objective function. Technical Report 2007/76, Université
catholique de Louvain, Center for Operations Research
and Econometrics (CORE), 2007.
Ouyang, Hua, He, Niao, Tran, Long, and Gray, Alexander G. Stochastic alternating direction method of multipliers. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 80–88, 2013.
Parikh, N. and Boyd, S. Proximal Algorithms, volume 1.
NOW, 2013.
Qin, Z. and Goldfarb, D. Structured sparsity via alternating
direction methods. J. Machine Learning Research, 13:
1435–1468, 2012.
Rockafellar, R. T. Monotone Operators and the Proximal
Point Algorithm. SIAM J. Control Optim., 14(5):877–
989, 1976.
Shamir, Ohad and Zhang, Tong. Stochastic gradient descent for non-smooth optimization: Convergence results
and optimal averaging schemes. In Proceedings of
the 30th International Conference on Machine Learning
(ICML-13), pp. 71–79, 2013.
Srebro, N. and Tewari, A. Stochastic Optimization for Machine Learning. ICML 2010 Tutorial, 2010.
Suzuki, Taiji. Dual averaging and proximal gradient descent for online alternating direction multiplier method.
In Proceedings of the 30th International Conference on
Machine Learning (ICML-13), pp. 392–400, 2013.
Tseng, P. On accelerated proximal gradient methods for
convex-concave optimization. Unpublished, 2008.
Xiao, L. Dual averaging methods for regularized stochastic
learning and online optimization. JMLR, 11:2543–2596,
2010.

