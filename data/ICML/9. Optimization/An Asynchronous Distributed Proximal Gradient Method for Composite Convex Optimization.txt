An Asynchronous Distributed Proximal Gradient Method
for Composite Convex Optimization

N. S. Aybat
Z. Wang
Penn State University, University Park, PA 16802 USA

NSA 10@ PSU . EDU
ZXW 121@ PSU . EDU

G. Iyengar
Columbia University, New York, NY 10027 USA

Abstract
We propose a distributed first-order augmented
Lagrangian (DFAL) algorithm to minimize the
sum of composite convex functions, where each
term in the sum is a private cost function belonging to a node, and only nodes connected
by an edge can directly communicate with each
other. This optimization model abstracts a number of applications in distributed sensing and machine learning. We show that any limit point
of DFAL iterates is optimal; and for any ǫ >
0, an ǫ-optimal and ǫ-feasible solution can be
computed within O(log(ǫ−1 )) DFAL iterations,
ψ 1.5
which require O( dmax
ǫ−1 ) proximal gradient
min
computations and communications per node in
total, where ψmax denotes the largest eigenvalue
of the graph Laplacian, and dmin is the minimum degree of the graph. We also propose an
asynchronous version of DFAL by incorporating
randomized block coordinate descent methods;
and demonstrate the efficiency of DFAL on large
scale sparse-group LASSO problems.

1. Introduction
Let G = (N , E) denote a connected undirected graph of
N computing nodes where nodes i and j can communicate information only if (i, j) ∈ E. Each node i ∈ N :=
{1, . . . , N } has a private (local) cost function
Fi (x) := ρi (x) + γi (x),
(1)
where ρi : Rn → R is a possibly non-smooth convex function, and γi : Rn → R is a smooth convex function. We
assume that the proximal map
	

(2)
proxρi (x) := argmin ρi (y) + 12 ky − xk22
y∈Rn

is efficiently computable for i ∈ N .

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

GARUD @ IEOR . COLUMBIA . EDU

We propose a distributed augmented Lagrangian algorithm
for efficiently computing a solution for the convex problem:
N
X
(3)
Fi (x).
F ∗ := minn F (x) :=
x∈R

i=1

Clearly, (3) can be solved in a “centralized” fashion by
communicating all the private functions Fi to a central
node, and solving the overall problem at this node. However, such an approach can be very expensive both from
communication and computation perspectives. Suppose
(Ai , bi ) ∈ Rm×(n+1) and Fi (x) = kAi x−bi k22 +λkxk1 for
i ∈ N such that m ≪ n and N ≫ 1. Hence, (3) is a very
large scale LASSO problem distributed data. To solve (3)
in a centralized fashion the data {(Ai , bi ) : i ∈ N } needs
to be communicated to the central node. This can be prohibitively expensive, and may also violate privacy constraints. Furthermore, it requires that the central node have
large enough memory to be able to accommodate all the
data. On the other hand, at the expense of slower convergence, one can completely do away with a central node,
and seek for consensus among all the nodes on an optimal decision using “local” decisions communicated by the
neighboring nodes. In addition, for certain cases, computing partial gradients locally in an asynchronous manner
can be even more computationally efficient when compared
to computing the entire gradient at a central node. With
these considerations in mind, we propose decentralized algorithms that can compute solutions to (3) using only local
computations; thereby, circumventing all privacy, communication and memory issues. To facilitate the design of decentralized algorithms, we take advantage of the fact that
graph G is connected, and reformulate (3) as
N
o
nX
F
(x
)
:
x
=
x
,
∀
(i,
j)
∈
E
. (4)
min
i
i
i
j
n
xi ∈R , i∈N

i=1

Optimization problems of form (4) model a variety of very important applications, e.g., distributed
linear regression (Mateos et al., 2010), distributed
control (Necoara & Suykens, 2008), machine learning (McDonald et al., 2010), and estimation using sensor
networks (Lesser et al., 2003).

An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization

∇Fi

iter # for
ǫ-feas.
unknown
O(1)
unknown
O(1/ǫ)
O(1/ǫ)
√
O(1/ ǫ)

proxρ , ∇γi

O(1/ ǫ)

√
O(1/ ǫ)

proxρi , ∇γi

O(1/ǫ)

O(1/ǫ)

Reference

assumption on Fi

operation / iter.

Duchi et al. (2012)
Nedic & Ozdaglar (2009)
Wei & Ozdaglar (2012)
Makhdoumi & Ozdaglar (2014)
Wei & Ozdaglar (2013)

convex, Lipschitz cont.
convex
strictly convex
convex
convex
smooth convex
bounded ∇Fi
composite convex Fi = ρ + γi
bounded ∇γi
composite convex Fi = ρi + γi

subgrad., projection
subgrad.
proxFi
proxFi
proxFi

Jakovetic et al. (2011)
Chen & Ozdaglar (2012)
Our work

√

iter # for
ǫ-opt.
O(1/ǫ2 )
O(1/ǫ2 )
O(1/ǫ)
O(1/ǫ)
O(1/ǫ)
√
O(1/ ǫ)

comm. steps
ǫ-opt.
O(1/ǫ2 )
O(1/ǫ2 )
O(1/ǫ)
O(1/ǫ)
O(1/ǫ)
√
O(1/ ǫ)

no
no
no
no
yes

Can handle
constraints?
no
no
no
no
no

no

no

O(1/ǫ)

no

no

O(1/ǫ)

yes

yes

asnych.

Table 1. Comparison of our method with the previous work

We call a solution x̄ = 
(x̄i )i∈N ǫ-feasible
if the consen	
sus violation max(i,j)∈E kx̄i − x̄j k2 ≤ ǫ and ǫ-optimal
P
if  i∈N Fi (x̄i ) − F ∗  ≤ ǫ. In this work, we propose
a distributed first-order augmented Lagrangian (DFAL) algorithm, establish the following main result for the synchronous case in Section 2.2.3, and extend it to an asynchronous setting in Section 2.2.4.
Main Result. Let {x(k) }k∈Z+ denote the sequence of
P
(k)
DFAL iterates. Then F ∗ = limk∈Z+ i∈N Fi (xi ).
(k)
Furthermore, x
is ǫ-optimal and ǫ-feasible within
ψ 1.5 −1 
−1
O(log(ǫ )) DFAL iterations, requiring O dmax
ǫ
min
communications per node, and O(ǫ−1 ) gradient and proximal map computations for γi and ρi , respectively, where
ψmax denotes the largest eigenvalue of the Laplacian of G,
and dmin denotes the minimum degree over all nodes.
1.1. Previous work
Given the importance of (4), a number of different distributed optimization algorithms have been proposed to
solve (4). Duchi et al. (2012) proposed a dual averaging algorithm to solve (3) in a distributed fashion over
G when each Fi is convex. This algorithm computes
ǫ-optimal solution in O(1/ǫ2 ) iterations; however, they
do not provide any guarantees on the consensus violation
max{kx̄i − x̄j k2 : (i, j) ∈ E}. Nedic & Ozdaglar (2009)
developed a subgradient method with constant step size
α > 0 for distributed minimization of (3) where the network topology is time-varying. Setting α = O(ǫ) in their
method guarantees that consensus violation and suboptimality is O(ǫ) in O(1/ǫ2 ) iterations; however, since the
step size is constant none of the errors are not guaranteed
to decrease further. Wei and Ozdaglar (2012; 2013), and
recently Makhdoumi & Ozdaglar (2014) proposed an alternating direction method of multipliers (ADMM) algorithm that computes an ǫ-optimal and ǫ-feasible solution in
O(1/ǫ) proximal map evaluations for Fi . There are several problems where one can compute the proximal map
for ρi efficiently; however, computing the proximal map
for Fi = ρi + γi is hard -see Section 3 for an example. One
can overcome this limitation of ADMM by locally splitting
variables, i.e., setting Fi (xi , yi ) := ρi (xi ) + γi (yi ), and
adding a constraint xi = yi in (4). This approach dou-

bles local memory requirement; in addition, in order for
ADMM to be efficient, proximal maps for both ρi and γi
must be efficiently computable. When each Fi is smooth
and has bounded gradients, Jakovetic et al. (2011) devel√
oped a fast distributed gradient methods with O(1/ ǫ)
convergence rate. Note that for the quadratic loss, which is
one of the most commonly used loss functions, the gradient
is not bounded. Chen & Ozdaglar (2012) proposed an inexact proximal-gradient method for distributed minimization
of (3) that is able to compute ǫ-feasible and ǫ-optimal solution in O(ǫ−1/2 ) iterations which require O(ǫ−1 ) communications per node over a time-varying network topology when Fi = ρ + γi , assuming that the non-smooth
term ρ is the same at all nodes, and ∇γi is bounded for all
i ∈ N . In contrast, DFAL proposed in this paper is able to
asynchronously compute an ǫ-optimal ǫ-feasible solution in
O(ǫ−1 ) communications per node, allowing node specific
non-smooth functions ρi , and without assuming bounded
∇γi for any i ∈ N .
Aybat & Iyengar (2012) proposed an efficient first-order
augmented Lagrangian (FAL) algorithm for the basis pursuit problem minx∈Rn {kxk1 : Ax = b} to compute an
ǫ-optimal and ǫ-feasible solution to within O(κ2 (A)/ǫ)
matrix-vector multiplications, where A ∈ Rm×n such that
rank(A) = m, and κ(A) := σmax (A)/σmin (A) denotes
the condition number of A. In this work, we extend their
FAL algorithm to solve a more general version of (4) in
Section 2.2.1 and 2.2.2, and establish the Main Result for
(4) in Section 2.2.3. In Section 2.2.4, we propose an asynchronous version of DFAL. It is important to emphasize
that DFAL can be easily extended to solve (4) when there
are global constraints on network resources of the form
Ex − q ∈ K, where K is a proper cone, and none of the
algorithms discussed above can accommodate such global
conic constraints efficiently. Due to space limitations, we
do not discuss this extension here; however, the analysis
would be similar to (Aybat & Iyengar, 2013; 2014).

2. Methodology
Definition 1. (a) Let Γ be the set of convex functions
γ : Rn → R such that ∇γ is Lipschitz continuous with
constant Lγ , and γ(x) ≥ γ for all x ∈ Rn for some γ ∈ R.

An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization

(b) Let R be the set of convex functions ρ : Rn → R such
that subdifferential of ρ is uniformly bounded on Rn , i.e.,
there exists B > 0 such that kqk2 ≤ B for all q ∈ ∂ρ(x),
x ∈ Rn ; and τ kxk2 ≤ ρ(x) for all x ∈ Rn for some τ > 0.

Assumption 1. For all i ∈ N , we assume that γi ∈ Γ and
ρi ∈ R with corresponding constants Lγi , γi , Bi and τi .
Most of the important regularizers and loss functions used
in machine learning and statistics literature lie in R and Γ,
respectively. In particular, any norm, e.g., k · kα with α ∈
{1, 2, ∞}, group norm (see Section 3), nuclear norm, etc.,
weighted sum of these norms, e.g., sparse group norm (see
m×n
m
Section 3), all belong to R. Given A ∈ R
Pm andTb ∈ R ,
2
quadratic-loss kAx − bk2 , Huber-loss  i=1 h(ai x −
 bi )
Pm
x
−bi aT
i
(see Section 3), logistic-loss i=1 log 1 + e
, or
fair-loss (Blatt et al., 2007) functions all belong to Γ.

Throughout the paper, we adopt the notation x = (xi ; x−i )
with x−i = (xj )j6=i to denote a vector where xi and x−i
are treated as variable and parameter sub-vectors of x, respectively. Given f : RnN → R, ∇xi f (x) ∈ Rn denotes
the sub-vector of ∇f (x) ∈ RnN corresponding to components of xi ∈ Rn .
2.1. APG Algorithm for the Centralized Model
Consider the centralized version (3) where all the functions Fi are available at a central node, and all computations are carried out at this node. Suppose {ρi }i∈N and
PN
{γi }i∈N satisfy Assumption 1. Let ρ(x) := i=1 ρi (x)
PN
and γ(x) := i=1 γi (x). Lipschitz continuity of each ∇γi
with constant Lγi implies that ∇γ is also Lipschitz continPN
uous with constant Lγ = i=1 Lγi . When proxρ/Lγ can
be computed efficiently, the accelerated proximal gradient (APG) algorithm proposed in (Beck & Teboulle, 2009;
Tseng, 2008) guarantees that
0 ≤ F (x(ℓ) ) − F ∗ ≤

2Lγ
kx(0) − x∗ k22 ,
(ℓ + 1)2

(5)

where x(0) is the initial iterate and x∗ ∈ argminx∈Rn F (x)
–see Corollary 3 in (Tseng, 2008), and Theorem 4.4
in (Beck & Teboulle, 2009). Thus,pAPG can compute an
1
ǫ-optimal solution to (3) within O( Lγ ǫ− 2 ) iterations.

As discussed above, the centralized APG algorithm cannot be applied when the nodes are unwilling or unable to
communicate the privately known functions {Fi }i∈N to a
central node. There are many other setting where one may
want to solve (3) as a “distributed” problem. For instance,
although proxtρi can be computed efficiently for all t > 0
and i ∈ N , proxρ/Lγ may be hard to compute.
P As an example, consider a problem with ρ1 (X) = i,j |Xij | and
Prank(X)
ρ2 =
σi (X), where σ(X) denotes the vector
i=1
of singular values for X ∈ Rn1 ×n2 . Here, proxtρi is
easy to compute for all t > 0 and i ∈ {1, 2}; however,

proxt(ρ1 +ρ2 ) is hard to compute. Thus, the “centralized”
APG algorithm cannot be applied. In the rest of this paper,
we focus on decentralized algorithms.
2.2. DFAL Algorithm for the Decentralized Model
⊤ ⊤
nN
denotes a vector formed
Let x = x⊤
1 , . . . , xN ) ∈ R
by concatenating {xi }i∈N ⊂ Rn as a long column vector.
Consider the following optimization problem of the form:
n
o
F̄ ∗ := min F̄ (x) := ρ̄(x) + γ̄(x) s.t. Ax = b , (6)
x∈RnN

PN
PN
where ρ̄(x) :=
i=1 ρi (xi ), γ̄(x) :=
i=1 γi (xi ), and
A ∈ Rm×nN has rank(A) = m, i.e., the linear map is
surjective. In Section 2.2.3, we show that the distributed
optimization problem in (4) is a special case of (6), i.e., for
all connected G 1 , there exists a surjective A such that (4) is
equivalent to (6). In the rest of the section, we will use the
following notation: Let {Ai }i∈N ⊂ Rm×n such that A =
[A1 , A2 , . . . , AN ]; L̄ := maxi∈N Lγi , τ̄ := mini∈N τi .
We propose to solve (6) by inexactly solving the following
sequence of subproblems in a distributed manner:
(k)

x∗ ∈ argmin P (k) (x) := λ(k) ρ̄(x) + f (k) (x),

(7)

f (k) (x) := λ(k) γ̄(x) + 12 kAx − b − λ(k) θ(k) k22 ,

(8)

x∈RnN

for appropriately chosen sequences of penalty parameters
{λ(k) } and dual variables {θ(k) } such that λ(k) ց 0.
In particular, given {α(k) , ξ (k) } satisfying α(k) ց 0 and
ξ (k) ց 0, the iterate sequence {x(k) } is constructed such
that every x(k) satisfies one of the following conditions:
(a)
(b)

(k)

P (k) (x(k) ) − P (k) (x∗ ) ≤ α(k) ,
(k)
∃gi ∈ ∂xi P (k) (x)|x=x(k)
(k)
(k)
s.t. maxi∈N kgi k2 ≤ ξ√N ,

(9)

∂xi P (k) (x)|x=x̄ := λ(k) ∂ρi (xi )|xi =x̄i + ∇xi f (k) (x̄).
Note that ∇f (k) (x) is Lipschitz continuous in
2
x ∈ RnN with constant λ(k) L̄ + σmax
(A). Given
(0)
(0)
(0) (0)
{x , λ , α , ξ } and c ∈ (0, 1), we choose the
sequence {λ(k) , α(k) , ξ (k) , θ(k) } as shown in Fig. 1.


Algorithm DFAL λ(1) , α(1) , ξ (1)

Step 0: Set θ(1) = 0, k = 1
Step k: (k ≥ 1)
1. Compute x(k) such that (9)(a) or (9)(b) holds
(k)
2. θ(k+1) = θ(k) − Axλ(k)−b
3. λ(k+1) = cλ(k) , α(k+1) = c2 α(k) , ξ (k+1) = c2 ξ (k)

Figure 1. First-order Augmented Lagrangian algorithm

In Section 2.2.1, we show that DFAL can compute an ǫoptimal and ǫ-feasible xǫ to (6), i.e., kAxǫ − bk2 ≤ ǫ and
|F̄ (xǫ ) − F ∗ | ≤ ǫ, in at most O(log(1/ǫ)) iterations.
1

G can contain cycles.

An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization

Next, in Section 2.2.2, we show that computing
an ǫ-optimal, ǫ-feasible
 solution xǫ requires at most
3
(A)
σmax
−1
ǫ
floating point operations. Using
O mini∈N
2
σmin
(Ai )
this result, in Section 2.2.3 we establish that DFAL can
compute xǫ in a distributed manner within O(ǫ−1 ) communication steps, i.e., the Main Result stated in Section 1.
Finally, in Section 2.2.4 we show how to modify DFAL for
an asynchronous computation setting.
2.2.1. DFAL

(k)

We first show that {x } is a bounded sequence, and then
argue that this also implies boundedness of {θ(k) }. First,
we start with a technical lemma that will be used in establishing the main results of this section.
Lemma 1. Let ρ̄ : RnN → R be defined as ρ̄(x) =
P
i∈N ρi (xi ), where ρi ∈ R with uniform bound Bi on
its subdifferential for all x ∈ Rn and for all i ∈ N . Let
f : RnN → R denote a convex function such that there
exist constants {Li }N
i=1 ⊂ R++ that satisfy
N
X
Li kyi − ȳi k22
f (y) ≤ f (ȳ) + ∇f (ȳ)T (y − ȳ) +
2
i=1
nN

for all y, ȳ ∈ R . Given α, λ ≥ 0, and x̄ ∈ R
such
nN {λρ(x) + f (x)} ≤ α, it
that λρ(x̄) + f (x̄) − minx∈R
√
follows that k∇xi f (x̄)k2 ≤ 2Li α + λBi for all i ∈ N .
In Lemma 2 we show that function f (k) defined in (8) satisfies the condition given in Lemma 1.
Lemma 2. The function f (k) in (7) satisfies the condition
(k)
(k)
in Lemma 1 with the constants Li = Li , where Li :=
(k)
2
λ Lγi + σmax (A) for all i ∈ N .

Lemma 1 and Lemma 2 allow us to bound kθ(k+1) k2 in
(k)
terms of {k∇xi γ(xi )k2 }i∈N . We later use this bound in
an inductive argument to establish that the sequence {x(k) }
is bounded.
Lemma 3. Let {x(k) } be the DFAL iterate sequence, i.e.,
at least one of the conditions
in (9) hold for all
o k ≥ 1.
nq
(k)

Define Θi

:= max

(k)
(k) α(k)
, √1N λξ (k)
(λ(k) )2

2Li

Theorem 2. Suppose Assumption 1 holds and λ(1) and ξ (1)
are chosen according to Theorem 1. Then the primal-dual
iterate sequence {x(k) , θ(k) } generated by DFAL satisfy
(a) kAx(k) − bk2 ≤ 2Bθ λ(k) ,
(b) F̄ (x(k) ) − F̄ ∗ ≥ −λ(k) (kθ

ITERATION COMPLEXITY

nN

We are now ready to state a key result that will imply the
iteration complexity of DFAL.

+ Bi +

(k)
k∇γi (xi )k2 .

Then for all k ≥ 1, we have
)
(
(k)
Θi
(k+1)
.
kθ
k2 ≤ min
i∈N
σmin (Ai )

Theorem 1 establishes that the DFAL iterate sequence
{x(k) } is bounded whenever {ρi , γi }i∈N satisfy Assumption 1; therefore, the sequence of dual variables {θ(k) } is
bounded according to Lemma 3.
Theorem 1. Suppose Assumption 1 holds.
Then
there exist constants Bx , Bθ , λ̄ > 0 such that
(k)
max{kx∗ k2 , kx(k) k2 } ≤ Bx and kθ(k) k2 ≤ Bθ for
all k ≥ 1, whenever λ(1) and ξ (1) are chosen such that
(1)
0 < λ(1) ≤ λ̄ and λξ (1) < τ̄ .

(c) F̄ (x

(k)

∗

) − F̄ ≤ λ

(k)



∗

Bθ2
2

k2 +Bθ )2
2

+

max{α(1) , ξ (1) Bx }

(λ(1) )

2


,

where θ∗ denotes any optimal dual solution to (6).
Corollary 1. The DFAL iterates x(k) are ǫ-feasible, i.e.,
kAx(k) − bk2 ≤ ǫ, and ǫ-optimal, i.e., |F̄ (x(k) ) − F̄ ∗ | ≤ ǫ,
for all k ≥ N (ǫ) and N (ǫ) = log 1c ( C̄ǫ ) for some C̄ > 0.
2.2.2. OVERALL COMPUTATIONAL COMPLEXITY FOR
THE SYNCHRONOUS ALGORITHM

Efficiency of DFAL depends on the complexity of the oracle for Step 1 in Fig. 1. In this section, we construct
an oracle MS-APG that computes an x(k) satisfying (9)
within O(1/λ(k) ) gradient and prox computations. This
result together with Theorem 2 guarantees that for any
ǫ > 0, DFAL can compute
an ǫ-optimal and ǫ-feasible

iterate within O ǫ−1 floating point operations. Following lemma gives the iteration complexity of the oracle MSAPG displayed in Fig. 2.
Lemma 4. Let ρ̄ : RnN → R such that ρ̄(x) =
P
n
i∈N ρi (xi ), where ρi : R → R is a convex function
nN
for all i ∈ N , and f : R
→ R be a convex function such that it satisfies the condition in Lemma 1 for
N
some constants {Li }i=1
⊂ R++ . Suppose that y∗ ∈
argmin Φ(y) := ρ̄(y) + f (y). Then the MS-APG iter2, satisfies
ate sequence {y(ℓ) }ℓ∈Z+ , computed
PN as in Fig.
(0)
∗ 2
(ℓ)
i=1 2Li kyi − yi k2
.
0 ≤ Φ(y ) − min Φ(y) ≤
(ℓ + 1)2
y∈RnN
(10)
Proof. (10) follows from adapting the proof of Theorem 4.4 in Beck & Teboulle (2009) for the case here.
Algorithm MS-APG ( ρ̄, f, y(0) )
Step 0: Take ȳ(1) = y(0) , t(1) = 1
Step ℓ: (ℓ ≥ 1)


(ℓ)
(ℓ)
1. yi = proxρi /Li ȳi − ∇yi f (ȳ(ℓ) )/Li
q
2
2. t(ℓ+1) = (1 + 1 + 4 (t(ℓ) ) )/2


(ℓ)
−1
y(ℓ) − y(ℓ−1)
3. ȳ(ℓ+1) = y(ℓ) + tt(ℓ+1)

∀i ∈ N

Figure 2. Multi Step - Accelerated Prox. Gradient (MS-APG) alg.

An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization

Consider the problem Φ∗ = min Φ(y) := ρ̄(y) + f (y)
defined in Lemma 4. Note that ∇f is Lipschitz continuous with constant L = maxi∈N Li . In MS-APG algorithm, the step length 1/Li ≥ 1/L is different for
each i ∈ N . Instead, if one were to use the APG algorithm (Beck & Teboulle, 2009; Tseng, 2008), then the step
length would have been 1/L for all i ∈ N . When {Li }i∈N
are close to each other, the performances of MS-APG and
i∈N Li
APG are on par; however, when max
mini∈N Li ≫ 1, APG can
only take very tiny steps for all i ∈ N ; hence, MS-APG is
likely to converge much faster in practice.
Since the subproblem (7) is in the form given in Lemma 4,
the following result immediately follows.
Lemma 5. The iterate sequence {y(ℓ) }ℓ∈Z+ gener
ated when we call MS-APG λ(k) ρ̄, f (k) , x(k−1) satis(k)
fies P (k) (y(ℓ) ) − P (k) (x∗ ) ≤ α(k) , for all ℓ ≥
q
PN

(k)

i=1

(k−1)

(k)

−x∗i k22
kxi
− 1,
α(k)
(k)
and x∗i represents the
(k)

2Li

Lemma 2
one can compute x
APG iterations.

(k)

where Li

is defined in
(k)

i-th block of x∗ . Hence,
satisfying (9) within O(1/λ(k) ) MS-

Theorem 2 and Lemma 5 together imply that DFAL can
compute an ǫ-feasible, and ǫ-optimal solution to (6) within
O(1/ǫ) MS-APG iterations. Due to space considerations,
we will only state and prove this result for the case where
∇γ̄ is bounded in RnN since the bounds Bθ and Bx are
more simple for this case. Note that Huber-loss, logisticloss, and fair-loss functions indeed have bounded gradients.
Theorem 3. Suppose that ∃Gi > 0 such that k∇γi (x)k2 ≤
o
Gi for all x ∈ Rn and for all i ∈ N . Let NDFAL
(ǫ) and
f
NDFAL (ǫ) denote the number of DFAL-iterations to compute an ǫ-optimal, and an ǫ-feasible solutions to (6), respectively. Let N (k) denote MS-APG iteration number required to compute x(k) satisfying at least one of the conditions in (9). Then
o

PNDFAL
(ǫ)
N (k) = O Θ2 σmax (A)ǫ−1 ,
k=1
f

PNDFAL
(ǫ)
N (k) = O Θσmax (A)ǫ−1 ,
k=1
where Θ =

σmax (A)
mini∈N σmin (Ai ) .

2.2.3. S YNCHRONOUS A LGORITHM FOR DISTRIBUTED
OPTIMIZATION

In this section, we show that the decentralized optimization
problem (4) is a special case of (6); therefore, Theorem 3
establishes the Main Result stated in the Introduction. We
also show that the steps in DFAL can be further simplified
in this context.
Construct a directed graph by introducing an arc (i, j)
where i < j for every edge (i, j) in the undirected graph
G = (N , E). Then the constraints xi − xj = 0 for all
(i, j) ∈ E in the distributed optimization problem (4) can

be reformulated as Cx = 0, where C ∈ Rn|E|×nN is a
block matrix such that the block C(i,j),l ∈ Rn×n corresponding to the edge (i, j) ∈ E and node l ∈ N , i.e.,
C(i,j),l is equal to In if l = i, −In if l = j, and 0n otherwise, where In and 0n denote n × n identity and zero
matrices, respectively. Let Ω ∈ RN ×N be the Laplacian of
G, i.e., for all i ∈ N , Ωii = di , and for all (i, j) ∈ N × N
such that i 6= j, Ωij = −1 if either (i, j) ∈ E or (j, i) ∈ E,
where di denotes the degree of i ∈ N . Then it follows that
Ψ := C T C = Ω ⊗ In ,

where ⊗ denotes the Kronecker product. Let ψmax :=
ψ1 ≥ ψ2 ≥ . . . ≥ ψN be the eigenvalues of Ω. Since
G is connected, rank(Ω) = N − 1, i.e., ψN −1 > 0 and
ψN = 0. From the structure of Ψ it follows that that
{ψi }N
i=1 are also the eigenvalues of Ψ, each with algebraic
multiplicity n. Hence, rank(C) = n(N − 1).

Let C = U ΣV T denote the reduced singular value decomposition (SVD) of C, where U ∈ Rn|E|×n(N −1) , Σ =
n(N −1)
, and V ∈ RnN ×n(N −1) . Note
diag(σ), σ ∈ R++
2
2
that σmax (C) = ψmax , and σmin
(C) = ψN −1 . Define
T
n(N −1)×nN
A := ΣV . A ∈ R
has linearly independent rows; more importantly, AT A = C T C = Ψ; hence,
2
2
(A) = ψmax , and σmin
σmax
(A) = ψN −1 . We also have
nN
: Ax = 0} = {x ∈ RnN : Cx = 0}. Hence,
{x ∈ R
the general problem in (6) with A := ΣV T and b = 0 ∈
Rn(N −1) is equivalent to (4). Let Ai ∈ Rn(N −1)×n and
Ci ∈ Rn|E|×n be the submatrices of A and C, respectively, corresponding to xi , i.e., A = [A1 , A2 , . . . , AN ],
and C = [C1 , C2 , . . . , CN ]. Clearly, it follows
√ from the
definition of C that σmax (Ci ) = σmin (Ci ) = di for all
i ∈ N . Using the property of SVD, it can also√be shown
for A = ΣV T that σmax (Ai ) = σmin (Ai ) = di for all
i ∈ N . Thus, Theorem 3 establishes the Main Result.
We now show that we do not have to compute the SVD of
C, or A, or even the dual multipliers θ(k) when DFAL is
used to solve (4). In DFAL the matrix A is used in Step 1
(i.e. within the oracle MS-APG) to compute ∇f (k) , and
in Step 2 to compute θ(k+1) . Since θ(1) = 0, Step 2
Pk
(t)
in DFAL and (8) imply that θ(k+1) = − t=1 Ax
,
λ(t)
(k)
(k)
T
(k) (k)
and ∇f (x) = λ ∇γ̄(x) + A (Ax −
λ θ ) =
Pk−1 1 (t) 
x
. Moreover,
λ(k) ∇γ̄(x) + Ψ x + λ(k) t=1 λ(t)
from the definition of Ψ, it follows that
∇xi f (k) (x) =


 X

(k)
(k)
λ(k) ∇γi (xi ) + di xi + x̄i
−
xj + x̄j
,
j∈Oi

(k)

λ(k) (t)
t=1 λ(t) x ,

Pk−1

where x̄
:=
and Oi denotes the set of
nodes adjacent to i ∈ N . Thus, it follows that Step 1 of
MS-APG can be computed in a distributed manner by only
communicating with the adjacent nodes without explicitly
computing θ(k) in Step 2 of DFAL.

An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization

In particular, for the k-th DFAL iteration, each node i ∈ N
(k)
(k)
stores x̄i and {x̄j }j∈Oi , which can be easily computed
(t)

locally if {xj }j∈Oi is transmitted to i at the end of Step 1
of the previous DFAL iterations 1 ≤ t ≤ k − 1. Hence,
during the ℓ-th iteration of MS-APG λ(k) ρ̄, f (k) , x(k−1)
call, each node i ∈ N can compute ∇yi f (k) (ȳℓ ) locally
(t)
if {ȳj }j∈Oi is transmitted to i at the end of Step 3 in
MS-APG. It is important to note that every node can in(k)
dependently check (9)(b), i.e., ∃gi ∈ ∂ρi (xi )|xi =x(k) +
(k)

i

∇xi f (k) (x(k) ) for all i ∈ N such that maxi∈N kgi k2 ≤
(k)
ξ
√ .
N

Hence, nodes can reach a consensus to move to
the next DFAL iteration without communicating their pri(k)
vaterinformation. If (9)(b) does not hold for ℓmax :=
2

P

(k)
i∈N Li
α(k)

Bx
MS-APG iterations, then Lemma 5 implies that (9)(a) must be true. Hence, all the nodes move
(k)
to next DFAL iteration after ℓmax many MS-APG updates.
For implementable version of DFAL, see Figure 3, where
Bx is the bound in Theorem 1, and Ni := Oi ∪ {i}.

Algorithm DFAL (x(0) , λ(1) , α(1) , ξ (1) , Bx , ψmax )
(1)

1: k ← 1, x̄i ← 0, ∀i ∈ N
2: while k ≥ 1 do
3:
ℓ ← 1, t(1) ← 1, STOP ← false
(k−1)
(1)
(k−1)
(0)
, ∀i ∈ N
, ȳi ← xi
4:
yi ← x i
(k)
(k)
Lγi + ψmax , ∀i ∈ N
5:
Li ← λ r
(k)

2

P

(k)

Li

6:

ℓmax ← Bx

7:
8:
9:

while STOP = false do
for i ∈ N do

 P


(k)
(ℓ)
(ℓ)
(ℓ)
+ j∈Ni Ωij ȳj + x̄j
qi ← λ(k) ∇γi ȳi


(ℓ)
(ℓ)
(k)
(ℓ)
yi ← proxλ(k) ρ /L(k) ȳi − qi /Li

10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:

i∈N

2.2.4. A SYNCHRONOUS IMPLEMENTATION
Here we propose an asynchronous version of DFAL. Due
to limited space, and for the sake of simplicity of the exposition, we only consider a simple randomized block coordinate descent (RBCD) method, which will lead to an asynchronous implementation of DFAL that can compute an ǫoptimal and
 ǫ-feasible
 solution to (4) with probability 1 − p
1
within O ǫ2 log p1
RBCD iterations. In Section 4.9 of
the supplementary

  file, we discuss how to improve this rate
to O 1ǫ log p1
using an accelerated RBCD.
Algorithm RBCD ( ρ̄, f, y(0) )

Step ℓ: (ℓ ≥ 0)
1. i ∈ N is realized withprobability
2.

3.

i

end for


(ℓ)
(ℓ)
s.t. max kgi k2 ≤
if ∃gi ∈ qi + λ(k) ∂ρi ȳi
i∈N

ξ√(k)
N

then
(k)
(ℓ)
STOP ← true, xi ← ȳi , ∀i ∈ N
(k)
else if ℓ = ℓmax then
(k)
(ℓ)
STOP ← true, xi ← yi , ∀i ∈ N
end if
q
2
t(ℓ+1) ← (1 + 1 + 4 (t(ℓ) ) )/2


(ℓ)
(ℓ+1)
(ℓ)
(ℓ)
(ℓ−1)
−1
ȳi
← yi + tt(ℓ+1)
yi − yi
, ∀i ∈ N
ℓ←ℓ+1
end while
(k+1)
(k)
(k+1)
λ(k+1) ← cλ(k) , α
← c2 α
← c2 ξ (k)
 , ξ

(k)
(k)
(k+1)
λ(k+1)
, ∀i ∈ N
x̄i
← λ(k) x̄i + xi
k ←k+1
end while

Figure 3. Dist. First-order Aug. Lagrangian (DFAL) alg.

= proxρi /Li

(ℓ)
yi

1
N

− ∇yi f (y(ℓ) )/Li

(ℓ)

= y−i



Figure 4. Randomized Block Coordinate Descent (RBCD) alg.

Nesterov (2012) proposed an RBCD method for solving miny∈RnN f (y), where f is convex with block Lipschitz continuous gradient, i.e., ∇yi f (yi ; y−i ) is Lipschitz continuous in yi with constant Li for all i. Later,
Richtárik & Takác̆ (2012) extended
PN the convergence rate
results to miny∈RnN Φ(y) :=
i=1 ρi (yi ) + f (y), such
that proxtρi can be computed efficiently for all t > 0 and
i ∈ N , andestablishedthat given α > 0, and p ∈ (0, 1), for
ℓ ≥ 2NαC 1 + log p1 , the iterate sequence {y(ℓ) } computed by RBCD displayed in Fig. 4 satisfies

αk

i

(ℓ+1)
yi
(ℓ+1)
y−i

P(Φ(y(ℓ) ) − Φ∗ ≤ α) ≥ 1 − p,

(11)

where C := max{R2L (y(0) ), Φ(y(0) ) − Φ∗ }, R2L (y(0) ) :=
	
 PN
∗ 2
(0)
), y∗ ∈ Y ∗ ,
max
i=1 Li kyi − yi k2 : Φ(y) ≤ Φ(y
∗
y,y

and Y ∗ denotes the set of optimal solutions. RBCD is
significantly faster in practice for very large scale problems, particularly when the partial gradient ∇yi f (y) can
be computed more efficiently as compared to the full gradient ∇f (y). The RBCD algorithm can be implemented for
the distributed minimization problem when the nodes in G
work asynchronously. Assume that for any y = (yi )i∈N ∈
RnN , each node i is equally likely to be the first to complete computing proxρi /Li (yi − ∇yi f (y)/Li ), i.e., each
node has an exponential clock with equal rates. Suppose
node i ∈ N is the first node to complete Step 2 of RBCD.
Then, instead of waiting for the other nodes to finish, node
i sends a message to its neighbors j ∈ Oi to terminate
(ℓ+1)
their computations, and shares yi
with them. Note that
RBCD can be easily incorporated into DFAL as an oracle
to solve subproblems in (7) by replacing (9)(a) with


 1

(k) 
P P (k) x(k) − P (k) x∗ ≤ α(k) ≥ 1 − p N (ǫ) ,
(12)

An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization

 
where N (ǫ) = log 1c C̄ǫ defined in Corollary 1. Since
 1
1 − p N (ǫ) ≤ 1 − Np(ǫ) for p ∈ (0, 1), the total number
of RBCD iterarions for the k-th subproblem is bounded:


1
1
log Np(ǫ)
log p1 +
= O α(k)
N (k) ≤ O α(k)

log log 1ǫ
. Hence, Corollary 1 and (11) imply that
asynchronous DFAL, i.e., (9)(a) replaced with (12), can
compute an ǫ-optimal andǫ-feasible
solution to (4) with
 
1
1
probability 1 − p within O ǫ2 log p
RBCD iterations.
These results can be extended to the case where each node
has different clock rates using (Qu & Richtárik, 2014).

3. Numerical results
In this section, we compared DFAL with an ADMM
method proposed in (Makhdoumi & Ozdaglar, 2014) on
the sparse group LASSO problem with Huber loss:
mi
N
X
X

minn
β1 kxk1 +β2 kxkGi +
h δ aT
i (j)x − bi , (13)
x∈R

i=1

j=1

where β1 , β2 > 0, Ai ∈ Rmi ×n , bi ∈ Rmi , aT
i (j)
denotes the j-th row of Ai , the Huber loss function
h (x) := max{tx − t2 /2 : t ∈ [−δ, δ]}, and kxkGi :=
Pδ K
k=1 kxgi (k) k2 denotes the group norm with respect to
the partition Gi of [1, n] := {1, · · · , n} for all i ∈ N ,
SK
i.e., Gi = {gi (k)}K
k=1 such that
k=1 gi (k) = [1, n],
and gi (j)P
∩ gi (k) = ∅ for all j 6= k. In this case,
mi
h δ aT
γi (x) := j=1
i (j)x − bi and ρi (x) := β1 kxk1 +
β2 kxkGi . Next, we briefly describe the ADMM algorithm
in (Makhdoumi & Ozdaglar, 2014), and propose a more efficent variant, SADMM, for (13).
Algorithm SADMM ( c, x

(0)

(k)

Initialization: y(0) = x(0) , pi = p̃i = 0, i ∈ N
Step ℓ: (ℓ ≥ 0) For i ∈ N compute


(k+1)
(k)
= prox
1. xi
1
ρi x̃i
2
c(d +di +1)
i


(k+1)
(k)
= prox
2. yi
1
γi ỹi
2
c(d +di +1)
i
P
(k+1)
(k+1)
/(di + 1)
= j∈Ni Ωij xj
3. si
(k)

(k+1)

(k+1)

(k)

= p̃i

(k+1)

(k+1)

+ s̃i

3.1. A distributed ADMM Algorithm
Let Ω ∈ RN ×N denote the Laplacian of the graph G =
(N , E), Oi denote the set of neighboring nodes of i ∈ N ,
di +1
and
:
P define Ni := Oi ∪ {i}. Let Zi := {zi ∈ R
j∈Ni zij = 0}. Makhdoumi & Ozdaglar (2014) show
that (4) can be equivalently written as
PN
min
i=1 Fi (xi ) := ρi (xi ) + γi (xi )
n
s.t.

Ωij xj = zij ,

Ωij xj = zij ,

i ∈ N , j ∈ Ni .

i ∈ N , j ∈ Ni

Ωij yj = z̃ij , i ∈ N , j ∈ Ni
xi = q i , y i = q i , i ∈ N .

ADMM algorithm for this formulation is displayed in
Fig. 5, where c > 0 denotes the penalty parameter. Steps of
SADMM can be derived by minimizing the augmented Lagrangian alternatingly in (x, y), and in (z, z̃, q) while fixing the other. As in (Makhdoumi & Ozdaglar, 2014), computing (z, z̃, q) can be avoided by exploiting the structure
of optimality conditions. Prox centers in SADMM are
(k)

= xi

(k)

= yi

x̃i
ỹi

(k)

−

(k)

−

P

(k)
(k)
(k)
(k)
+(xi −yi )/2
2
di +di +1
P
(k)
(k)
(k)
(k)
(k)
j∈Ni Ωji (s̃j +p̃j )−ri −(xi −yi )/2
2
di +di +1
(k)

j∈Ni

(k+1)

Ωji (sj +pj )+ri

(k)

= ri

(k+1)

+ (xi

(k+1)

− yi

,
,

)/2.

The following lemma shows that in DFAL implementation,
each node i ∈ N can check (9)(b) very efficiently. For
x ∈ R, define sgn(x) as -1, 0 and 1 when x < 0, x = 0,
and x > 0, respectively; and for x ∈ Rn , define sgn(x) =
[sgn(x1 ), sgn(x2 ), . . . , sgn(xn )]T .

Figure 5. Split ADMM algorithm

xi ∈R ,zi ∈Zi

s.t.

3.2. Implementation details and numerical results

= p i + si
4. pi
P
(k+1)
(k+1)
/(di + 1)
= j∈Ni Ωij yj
5. s̃i

6. p̃i

xi ,yi ∈R ,
zi ,z̃i ∈Zi

respectively; and ri

)

(k)

Only (14) is penalized when forming the augmented Lagrangian, which is alternatingly minimized in x ∈ RnN ,
T
zT = [z1T , . . . , zN
], where Zi ∋ zi = [zij ]j∈Ni ∈ Rdi +1 .
Makhdoumi & Ozdaglar (2014) establish that suboptimality and consensus violation converge to 0 with a rate
O(1/k), and in each iteration every node communicates
3n scalars. From now on, we refer to this algorithm that
directly works with Fi as ADMM. Computing proxFi for
each i ∈ N is the computational bottleneck in each iteration of ADMM. Note that computing proxFi for (13) is
almost as hard as solving the problem. To deal with this
issue, we considered the following reformulation:
P
min n
i∈N ρi (xi ) + γi (yi )

(14)

Lemma 6. Let f : Rn → R be a differentiable function,
G = {g(k)}K
k=1 be a partition of [1, n]. For β1 , β2 > 0, define P = λρ + f , where ρ(x) := β1 kxk1 + β2 kxkG . Then,
for all x̄ ∈ Rn and ξ > 0, there exists ν ∈ ∂P (x)|x=x̄
such that kνk2 ≤ ξ if and only if kπ ∗ + ω ∗ + ∇f (x̄)k2 ≤
ξ for π ∗ , ω ∗ such that for each k, if x̄g(k) 6= 0, then
∗
πg(k)
= λβ1 sgn(x̄g(k) ) + 1 − sgn(|x̄g(k) |) ⊙ ηg(k) , and
x̄

g(k)
∗
ωg(k)
= λβ2 kx̄g(k)
k2 ; otherwise, if x̄g(k) = 0, then
∗
∗
πg(k) = ηg(k) , and ωg(k) equals
(
)


λβ
2
∗
− πg(k)
+ ∇xg(k) f (x̄) min 1,
,
∗
kπg(k)
+ ∇xg(k) f (x̄)k2

where ⊙ denotes componentwise

 multiplication, and
	
ηg(k) = − sgn ∇xg(k) f (x̄) ⊙ min |∇xg(k) f (x̄)|, λβ1 .

An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization

Table 2. Comparison of DFAL, AFAL (Asynchronous DFAL), ADMM, and SADMM. (Termination time T=1800 sec.)
Size

Alg.

Rel. Suboptimality

Consensus Violation (CV)

CPU Time (sec.)

Iterations

Case 1

Case 2

Case 1

Case 2

Case 1

Case 2

Case 1

Case 2

ng = 100
N =5

SDPT3 (C)
APG (C)
DFAL (D)
AFAL (D)
ADMM (D)
SADMM (D)

0
1E-3
6E-4, 7E-4
3E-4, 8E-4
6E-5, 5E-5
1E-4, 3E-4

0
N/A
4E-4, 6E-4
3E-4, 6E-4
7E-5, 7E-5
1E-4, 3E-4

0
0
5E-5, 2E-5
3E-6, 5E-6
1E-4, 1E-4
1E-4, 1E-4

0
N/A
5E-5, 3E-5
2E-6, 5E-6
1E-4, 1E-4
1E-4, 1E-4

28
10
5, 5
16, 11
1125, 808
771, 784

85
N/A
5, 5
17, 11
1090, 771
772, 804

24
2173
1103, 1022
9232, 9083
353, 253
592, 606

22
N/A
1105, 1108
9676, 9844
363, 261
593, 623

ng = 100
N = 10

SDPT3 (C)
APG (C)
DFAL (D)
AFAL (D)
ADMM (D)
SADMM (D)

0
1E-3
4E-4, 7E-4
4E-4, 8E-4
9E-3, 2E-2
3E-4, 9E-3

0
N/A
4E-4, 3E-4
2E-4, 8E-4
8E-3, 2E-2
4E-4, 1E-2

0
0
6E-5, 1E-5
7E-6, 2E-6
9E-4, 5E-4
2E-4, 1E-3

0
N/A
6E-5, 2E-5
8E-6, 2E-6
8E-4, 4E-4
2E-4, 1E-3

28
10
14, 12
48, 65
T, T
T, T

89
N/A
14, 13
49, 62
T, T
T, T

24
2173
1794, 1439
20711, 41125
354, 353
867, 883

22
N/A
1812, 1560
21519, 41494
373, 372
865, 879

ng = 300
N =5

SDPT3 (C)
APG (C)
DFAL (D)
AFAL (D)
ADMM (D)
SADMM (D)

0
1E-3
2E-4, 5E-4
1E-4, 6E-4
5E-2, 1E-3
2E-2, 7E-2

0
N/A
2E-4, 4E-4
2E-5, 6E-4
5E-2, 1E-3
2E-2, 8E-2

0
0
6E-5, 4E-5
5E-7, 2E-5
5E-3, 1E-3
2E-3, 3E-3

0
N/A
5E-5, 5E-5
6E-8, 2E-5
5E-3, 1E-3
2E-3, 3E-3

806
253
77, 64
164, 99
T, T
T, T

1653
N/A
80, 65
273, 99
T, T
T, T

26
8663
1818, 1511
21747, 8760
109, 118
269, 274

29
N/A
1897, 1535
37212, 8736
109, 118
268, 273

ng = 300
N = 10

SDPT3 (C)
APG (C)
DFAL (D)
AFAL (D)
ADMM (D)
SADMM (D)

0
1E-3
1E-4, 6E-4
2E-4, 7E-4
5E-2, 8E-2
3E-1, 3E+0

0
N/A
6E-4, 1E-3
6E-4, 1E-3
5E-2, 8E-2
3E-1, 3E+0

0
0
7E-5, 4E-5
8E-7, 1E-5
7E-3, 9E-3
4E-3, 2E-2

0
N/A
9E-5, 5E-5
3E-7, 1E-5
7E-3, 9E-3
4E-3, 2E-2

806
253
130, 80
350, 294
T, T
T, T

1641
N/A
122, 82
437, 288
T, T
T, T

26
8663
2942, 1721
48214, 29946
114, 124
255, 269

29
N/A
2794, 1769
63110, 30371
113, 123
256, 268

Both DFAL and SADMM call for proxρi . In Lemma 7,
we show that it can be computed in closed form. On the
other hand, when ADMM, and SADMM are implemented
on (13), one needs to compute proxFi and proxγi , respectively; however, these proximal operations do not assume closed form solutions. Therefore, in order to be fair,
we computed them using an efficient interior point solver
MOSEK (ver. 7.1.0.12).
Lemma 7. Let ρ(x) = β1 kxk1 + β2 kxkG . For t > 0
and x̄ ∈ Rn , xp = proxtρ (x̄) is given by xpg(k) =
o
n
tβ2
′
,
0
, for 1 ≤ k ≤ K, where
ηg(k)
max 1 − kηg(k)
k2
η ′ = sgn(x̄) ⊙ max{|x̄| − tβ1 , 0}.

In our experiments, the network was either a star tree or
a clique with either 5 or 10 nodes. The remaining problem parameters defining {ρi , γi }i∈N were set as follows.
We set β1 = β2 = N1 , δ = 1, and K = 10. Let
n = Kng for ng ∈ {100, 300}, i.e., n ∈ {1000, 3000}.
We generated partitions {Gi }i∈N in two different ways.
For test problems in CASE 1, we created a single partition
G = {g(k)}K
k=1 by generating K groups uniformly at random such that |g(k)| = ng for all k; and set Gi = G for
all i ∈ N , i.e., ρi (x) = ρ(x) := β1 kxk1 + β2 kxkG for
all i ∈ N . For the test problems in CASE 2, we created a
different partition Gi for each node i, in the same manner
n
as in Case 1. For all i ∈ N , mi = 2N
, and the elements of
mi ×n
Ai ∈ R
are i.i.d. with standard Gaussian, and we set
bi = Ai x̄ for x̄j = (−1)j e−(j−1)/ng for j ∈ [1, n].
We solved the distributed optimization problem (4) using
DFAL, AFAL (asynchronous version of DFAL with accelerated RBCD -see the supplementary file for details),

ADMM, and SADMM for both cases, on both star trees,
and cliques, and for N ∈ {5, 10} and ng ∈ {100, 300}.
For each problem setting, we randomly generated 5 instances. For benchmarking, we solved the centralized problem (3) using SDPT3 for both cases. Note that for Case 1,
P
can
i∈N ρi (x) = kxk1 + kxkG and its prox mapping
P
be computed efficiently, while for Case 2,
ρ
i∈N i (x)
does not assume a simple prox map. Therefore, for
the first case we were also able to use APG, described
in Section 2.1, to solve (3) by exploiting the result of
Lemma 7. All the algorithms are terminated when the relative suboptimality, |F (k) − F ∗ |/|F ∗ |, is less then 10−3 ,
and consensus violation,
CV(k) , is less than 10−4 , where
P
(k)
(k)
F
equals to i∈N Fi (x
 i ) for DFAL and ADMM,
(k)
(k)
P
xi +yi
for SADMM; CV(k) equals
and to
Fi
2
i∈N
√
(k)
(k)
to max(ij)∈E kxi − xj k2 / n for DFAL, and ADMM,
(k)

(k)

(k)

and to max{max(ij)∈E kxi − xj k2 , maxi∈N kxi −
√
(k)
yi k2 }/ n for SADMM. If the stopping criteria are not
satisfied in 30min., we terminated the algorithm and report
the statistics corresponding to the iterate at the termination.
In Table 2, ’xxx (C)’ stands for “algorithm xxx is used to
solve the centralized problem”. Similarly, ’xxx (D)’ for the
decentralized one. For the results separated by comma, the
left and right ones are for the star tree and clique, resp. Table 2 displays the means over 5 replications for each case.
The number of iterations in each case clearly illustrates the
topology of the network plays an important role in the convergence speed of DFAL, which coincides to our analysis
in Section 2.2.2.

An Asynchronous Distributed Proximal Gradient Method for Composite Convex Optimization

References
Aybat, N. S. and Iyengar, G. A first-order augmented lagrangian method for compressed sensing. SIAM Journal
on Optimization, 22(2):429–459, 2012.
Aybat, Necdet Serhat and Iyengar, Garud. An augmented
lagrangian method for conic convex programming. arXiv
preprint arXiv:1302.6322, 2013.
Aybat, Necdet Serhat and Iyengar, Garud. A unified approach for minimizing composite norms. Mathematical
Programming, 144(1-2):181–226, 2014.
Beck, A. and Teboulle, M. A fast iterative shrinkagethresholding algorithm for linear inverse problems.
SIAM J. Img. Sci., 2(1):183–202, March 2009. ISSN
1936-4954.
Blatt, D., Hero, A., and Gauchman, H. A convergent incremental gradient method with a constant step size. SIAM
Journal on Optimization, 18(1):29–51, 2007.
Chen, Annie I and Ozdaglar, Asuman. A fast distributed
proximal-gradient method. In Communication, Control,
and Computing (Allerton), 2012 50th Annual Allerton
Conference on, pp. 601–608. IEEE, 2012.
Duchi, J. C., Agarwal, A., and Wainwright, M. J. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Trans. Automat. Contr.,
57(3):592–606, 2012.

McDonald, Ryan, Hall, Keith, and Mann, Gideon. Distributed training strategies for the structured perceptron.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Association for Computational Linguistics, pp. 456–464.
Association for Computational Linguistics, 2010.
Necoara, Ion and Suykens, Johan AK. Application of a
smoothing technique to decomposition in convex optimization. Automatic Control, IEEE Transactions on, 53
(11):2674–2679, 2008.
Nedic, Angelia and Ozdaglar, Asuman. Distributed subgradient methods for multi-agent optimization. Automatic
Control, IEEE Transactions on, 54(1):48–61, 2009.
Nesterov, Y. Efficiency of coordinate descent methods on
huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341–362, 2012.
Qu, Zheng and Richtárik, Peter. Coordinate descent with
arbitrary sampling i: Algorithms and complexity. arXiv
preprint arXiv:1412.8060, 2014.
Richtárik, P. and Takác̆, M. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. forthcoming, Mathematical
Programming, Series A, 2012. DOI: 10.1007/s10107012-0614-z.
Tseng, Paul. On accelerated proximal gradient methods for
convex-concave optimization. submitted to SIAM Journal on Optimization, 2008.

Fercoq, Olivier and Richtárik, Peter. Accelerated, parallel and proximal coordinate descent. arXiv preprint
arXiv:1312.5799, 2013.

Wei, Ermin and Ozdaglar, Asuman. Distributed alternating
direction method of multipliers. 2012.

Grone, R. and Merris, R. The laplacian spectrum of a
graph. ii. SIAM J. Discrete Math., 7(2):221–229, 1994.

Wei, Ermin and Ozdaglar, Asuman. On the o (1/k) convergence of asynchronous distributed alternating direction
method of multipliers. arXiv preprint arXiv:1307.8254,
2013.

Jakovetic, Dusan, Xavier, Joao, and Moura, J. Fast distributed gradient methods. 2011.
Lesser, Victor, Ortiz Jr, Charles L, and Tambe, Milind. Distributed sensor networks: A multiagent perspective, volume 9. Springer, 2003.
Makhdoumi, Ali and Ozdaglar, Asuman. Broadcast-based
distributed alternating direction method of multipliers.
In Communication, Control, and Computing (Allerton),
2014 52nd Annual Allerton Conference on, pp. 270–277.
IEEE, 2014.
Mateos, Gonzalo, Bazerque, Juan Andrés, and Giannakis,
Georgios B. Distributed sparse linear regression. Signal
Processing, IEEE Transactions on, 58(10):5262–5276,
2010.

