On Greedy Maximization of Entropy

Dravyansh Sharma
IIT Delhi, New Delhi, India

CS 1110214@ CSE . IITD . AC . IN

Amit Deshpande
Microsoft Research, Bangalore, India

AMITDESH @ MICROSOFT. COM

Ashish Kapoor
Microsoft Research, Redmond, USA

AKAPOOR @ MICROSOFT. COM

Abstract
Submodular function maximization is one of the
key problems that arise in many machine learning tasks. Greedy selection algorithms are the
proven choice to solve such problems, where
prior theoretical work guarantees (1 − 1/e) approximation ratio. However, it has been empirically observed that greedy selection provides almost optimal solutions in practice. The main
goal of this paper is to explore and answer why
the greedy selection does significantly better than
the theoretical guarantee of (1 − 1/e). Applications include, but are not limited to, sensor selection tasks which use both entropy and mutual
information as a maximization criteria. We give
a theoretical justification for the nearly optimal
approximation ratio via detailed analysis of the
curvature of these objective functions for Gaussian RBF kernels.

1. Introduction
Consider a real-world scenario where the task is to sense
a certain physical phenomenon of interest, e.g., temperature, in an area (Krause et al., 2008) with a limited number
of sensors. Another scenario is selecting a subset of data
points to be labeled from a large corpus, for the purposes
of supervised learning (Settles, 2010). Similarly, the task
could consist of determining what tests to run on a medical
patient for diagnosing ailments (Kapoor & Horvitz, 2009).
The key underlying question in all these scenarios is how
to choose a subset of actions that would provide the most
useful information pertaining to the task at hand.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

All of the above scenarios can be considered as a subset
selection problem, where the goal is to determine which
subset maximizes a given objective function defined over
the subsets. Prior works have considered criteria such as
mutual information and entropy, which make the objective function submodular. Traditionally, these submodular maximization problems are solved via greedy selection
and often these prior works point to a result by Nemhauser
et al. (Nemhauser et al., 1978) which guarantees that the
greedy solution will be at least (1 − 1/e) times the optimum. While there exist submodular functions for which
the (1 − 1/e) bound is tight, in several practical instances it
has been observed that the greedy algorithm performs significantly better than (1 − 1/e) times the optimum. For example, we reproduce the figure (see Figure 1) from Krause
et al. (Krause et al., 2008), where the greedy method obtains an approximation ratio of over 0.95. While the greedy
selection algorithms are popular in such subset selection
problems, a better analysis explaining their empirical nearoptimal performance is an unexplored direction to the best
of our knowledge.
1.1. Our results
In this paper, we aim to answer why greedy selection results in nearly optimal solutions. We specifically focus
on the popular kernels generated by Gaussian Radial Basis Functions (RBFs), and show that the greedy selection
of points achieves an approximation ratio close to 1 that is
much superior than the traditional guarantee of (1 − 1/e).
The key insight here is that the Gaussian RBF kernel matrices for well-separated points have a very dominant diagonal, making the submodular objective function close
to linear (i.e., modular). Intuitively, it means that even
though the objective function is submodular and it satisfies the diminishing returns property, the returns diminish
only marginally even as we add more and more points.
Our main technical contribution is Theorem 5, where we

On Greedy Maximization of Entropy

Figure 1. Plot depicting comparison of greedy algorithm with optimal solutions in (Krause et al., 2008). The data set consisted of
16 locations of the Intel Berkeley temperature data. As noted in
the same work, the greedy algorithm is always within 95% of the
optimal value, although the best known theoretical guarantee is
about 63%.

bound the curvature of the subset selection criterion in
terms of the bandwidth of the kernel, and the underlying
dimensionality of the data points. An important consequence of Theorem 5 is that the curvature is nearly zero,
which leads to an approximation ratio close to 1. A key
challenge in our analysis is the non-monotonicity of these
functions. Our work circumvents this using a monotone,
submodular proxy to the objective function, that preserves
the greedy selection choice at every step while having nearzero curvature. We provide empirical evidence that validates and highlights the key ideas in our result and their
consequences.
1.2. Related work

tor approximation to the optimum for both these objective
functions, and more generally, for any non-negative, monotone, submodular function (Nemhauser et al., 1978). Conforti and Conuejols (Conforti & Cornuejols, 1984) gave a
tighter analysis to prove that the greedy selection actually
gives (1 − e−c )/c factor approximation when the objective
function is monotone and has curvature c, which means
that the approximation ratio tends to 1 as c tends to 0. Intuitively, it means that the approximation ratio of greedy
selection tends to 1 as the function gets closer to being linear, as one would expect. This was the state of the art until recently, when Sviridenko, Vondrak and Ward showed
that both a modified continuous greedy and local search
give almost (1 − c/e) factor approximation, and this is essentially the best possible in the value oracle model (Sviridenko et al., 2015). However, both these algorithms, modified continuous greedy as well as local search, are computationally quite expensive and not as practical as the usual
greedy selection. Note that curvature is defined only for
monotone, submodular functions, whereas entropy and mutual information are submodular but not necessarily monotone.
Krause et al. (Krause et al., 2008) get around nonmonotonicity of mutual information by showing that it is
approximately monotone over small sets, under some reasonable assumptions on the underlying kernel as well as the
discretization of the underlying space. This allows them
to show an almost (1 − 1/e) approximation. This guarantee holds for any non-negative, monotone, submodular
function, and no better analysis is known even for special cases such as Gaussian RBF kernels. In this work, we
prove near-optimal approximation guarantee for the maximum entropy sampling problem on Gaussian RBF kernels,
and also establish exact monotonicity of mutual information over small subsets.

Gaussian process (GP) model (Rasmussen & Williams,
2006) is a non-parametric generalization of linear regression, where the prediction error minimization reduces to
a problem of maximum entropy sampling. The model
is of fundamental significance to the problem of sensor
placements, and is known to outperform classical models
based on geometric assumptions, which turn out to be too
strong in practice (Krause et al., 2008). Greedy algorithm
is known to work well for the problem of maximum entropy sampling (Cressie, 1991) (Shewry & Wynn, 1987).
However, for the problem of sensor placement, (Krause
et al., 2008) propose a modified criterion of maximizing the
mutual information of selected sensor locations, for which
again a greedy procedure gives good approximation.

Finally we note that several fundamental problems in disparate domains can be effectively formulated as sensor selection problems. Among the more prominent ones are the
problem of state estimation in linear dynamical systems
(Shamaiah et al., 2010), target localization and tracking
(Wang et al., 2004), (Wang et al., 2005), (Isler & Bajcsy,
2005), graphical models (Krause & Guestrin, 2012), coverage problems and mission assignment schemes (Rowaihy
et al., 2007). Information-theoretic approaches, including
both entropy and mutual-information based methods, have
been widely acknowledged as prominent heuristics for sensor placement and other active learning problems.

Both the entropy and the mutual information maximization
problems are known to be NP-hard (Ko et al., 1995; Krause
et al., 2008). However, the greedy selection gives an efficient, polynomial time algorithm with at least (1−1/e) fac-

The outline of our paper is as follows. In Section 2 we describe submodular functions and their key properties such
as monotonicity and curvature. In Section 3 we describe

1.3. Outline

On Greedy Maximization of Entropy

the algorithms or pseudo-codes for maximum entropy sampling as well as mutual information maximization. In Section 4 we provide a better analysis of greedy for the maximum entropy sampling on Gaussian RBF kernels. The key
ideas here are
• A lower bound on the smallest eigenvalue of a Gaussian RBF kernel matrix, depending only on the intrinsic dimensionality of the data points and their minimum inter-point separation but independent of the total number of points.
• An upper bound on the Euclidean length of any row of
a Gaussian RBF kernel matrix, with a similar dependence as above.

Now we show that if X has its smallest eigenvalue at least
1 then the entropy log det (X[S, S]) is monotone.
Proposition 2. Given any symmetric X ∈ Rn×n with
λmin (X) ≥ 1, the function f (S) = log det (X[S, S]) is
monotone.
Proof. For monotonicity, it suffices to show that f (S) ≤
f (S ∪ {i}), for all S and i ∈
/ S. As in Proposition 1
f (S ∪ {i}) − f (S)


det (X[S ∪ {i}, S ∪ {i}])
= log
det (X[S, S])

= − log X[S ∪ {i}, S ∪ {i}]−1 ii by Cramer’s rule
≥ log λmin (X[S ∪ {i}, S ∪ {i}])

In Section 5 we prove exact monotonicity of mutual information over small subsets in Gaussian RBF kernels,
improving upon the approximate monotonicity of (Krause
et al., 2008).

2. Submodular functions and their properties
2.1. Submodularity, monotonicity, and curvature
We index a given set of n points by [n] = {1, 2, . . . , n} and
denote the set of all subsets of [n] by 2[n] .
Definition 1. A function f : 2[n] → R is submodular if
f (S ∪ {i}) − f (S) ≥ f (T ∪ {i}) − f (T ), for all S ⊆ T
and i ∈
/ T.
In other words, submodular functions exhibit the property
of diminishing returns.

≥ log λmin (X)
≥0

using λmin (X) ≥ 1.

See (Strang, 2009) for Cramer’s rule and the elementary
fact that the smallest eigenvalue of a principal submatrix is
at least the smallest eigenvalue of the bigger matrix.
Thus, it is easy to make entropy function monotone just
by scaling the matrix up so that its minimum eigenvalue is
at least 1. For monotone, submodular functions, one can
define their curvature as follows.
Definition 3. The curvature c(f ) ∈ [0, 1] of a monotone,
submodular function f : 2[n] → R is defined as
c(f ) = 1 −

min

S([n],i∈S
/

f (S ∪ {i}) − f (S)
,
f ({i}) − f (∅)

Given a matrix X ∈ Rn×n , we use X[S, T ] to denote its
|S|×|T | submatrix with row indices in S ⊆ [n] and column
indices in T ⊆ [n]. We denote the complement [n]\S by S̄,
and we abbreviate X[S, {i}] as X[S, i] and X[S, [n] \ {i}]
as X[S, ī], respectively, for convenience.

or equivalently, by submodularity, we can define

Maximizing entropy and mutual information in Gaussian
processes are known to be equivalent to maximizing certain functions of submatrices of given RBF kernels (Krause
et al., 2008), so we directly define them by their corresponding linear algebraic problems.

Notice that submodularity along with curvature gives a
tighter control on f as f ({i})−f (∅) ≥ f (S∪{i})−f (S) ≥
(1 − c(f )) (f ({i}) − f (∅)), for all S ( [n] and i ∈
/ S.
Curvature c(f ) = 0 means that the function f is linear.
Therefore, small c(f ) is desirable and easier to handle.

Proposition 1. Given any symmetric, positive semidefinite
matrix X ∈ Rn×n the entropy f (S) = log det (X[S, S])
and the mutual information
F (S) = log det (X[S, S]) +

log det X[S̄, S̄] are both submodular functions, where
X[S, S] denotes the |S|×|S| principal submatrix of X with
row and column indices in S ⊆ [n].

Scaling the matrix up by α makes the new entropy
log det (X[S, S]) + α |S|, which also helps reduce the curvature.

Proof. See Krause et al. (Krause et al., 2008).
Definition 2. A submodular function f : 2[n] → R is
monotone if, whenever S ⊆ T , we have f (S) ≤ f (T ).

c(f ) = 1 − min
i∈[n]

f ([n]) − f ([n] \ {i})
.
f ({i}) − f (∅)

Proposition 3. Let f : 2[n] → R be a monotone, submodular function, and let g(S) = f (S) + α |S|, for some fixed
α > 0. Then g is also a monotone, submodular function
with c(g) < c(f ).
Proof. Submodularity and monotonicity are easy to verify
by observing g(S∪{i})−g(S) = f (S∪{i})+α |S ∪ {i}|−

On Greedy Maximization of Entropy

f (S) − α |S| ≥ f (S ∪ {i}) − f (S).

Algorithm 1 Greedy(f, k)
Initialize S ← ∅
for t = 1 to k do
imax ← argmaxf (S ∪ {i}) − f (S)

g([n]) − g([n] \ {i})
g({i}) − g(∅)
i∈[n]
f ([n]) − f ([n] \ {i}) + α
= 1 − min
f ({i}) − f (∅) + α
i∈[n]
f ([n]) − f ([n] \ {i})
≤ 1 − min
f ({i}) − f (∅)
i∈[n]

c(g) = 1 − min

i∈S
/

S ← S ∪ {imax }
end for
Output S

= c(f ).

The scaling trick does not work for mutual information
 because after scaling by α we get an additive α |S| + α S̄  =
αn, for all S. Now we try to bound the curvature of
log det (X[S, S]) in terms of the row-lengths of X and
λmin (X). The reason being that the minimum eigenvalue
and the row-lengths can each be bounded independently of
n for Gaussian RBF kernels of well separated points (see
Lemma 7 and Lemma 8).
Proposition 4. For any positive semidefinite matrix X ∈
Rn×n with λmin (X) ≥ 1, if maxi∈[n] kX[ī, i]k ≤
λmin (X) , then the curvature c(f ) of the function f (S) =
log det (X[S, S]) can be upper bounded as follows.
2

λmin (X)−2 max kX[ī, i]k
i∈[n]

c(f ) ≤

log λmin (X)

.

Proof. By the definition of curvature and the well-known
identity for determinant of block matrices



A B
det
= det (A) det D − CA−1 B
C D

We have used λmin (X[S, S]) ≥ λmin (X) ≥ 1, for any
S ⊆ [n], which gives λmin (X[ī, ī]) ≥ λmin (X) as well
as X[i, i] ≥ λmin (X). We also used maxi∈[n] kX[ī, i]k ≤
λmin (X) to ensure that the expression inside log is nonnegative.
Note that our bound is stronger than the c(f ) ≤ 1 − 1/λmin
bound mentioned in (Sviridenko et al., 2015).

3. Greedy algorithm and its variants
In each step, the greedy algorithm (see Algorithm 1) picks
the element that maximizes the marginal gain. The approximation guarantees of (1 − 1/e) by Nemhauser et al.
(Nemhauser et al., 1978) and (1 − e−c )/c by Conforti
and Cornuejols (Conforti & Cornuejols, 1984) discussed in
Subsection 1.2 also hold for monotone, submodular maximization over subsets of a predetermined size k.
In practice, it is possible to run a faster version of greedy
selection while not losing on the approximation guarantee.
Such algorithms include Lazy-Greedy (Krause et al., 2008)
and Stochastic-Greedy (Mirzasoleiman et al., 2015). Our
analysis of the curvature can be extended to these settings
as well, and will be included in the full version.

if A is invertible, we have
c(f ) = 1 − min
i∈[n]

f ([n]) − f ([n] \ {i})
f ({i}) − f (∅)

log X[i, i] − X[ī, i]T X[ī, ī]−1 X[ī, i]
= 1 − min
log X[i, i]
i∈[n]


X[ī, i]T X[ī, ī]−1 X[ī, i]
− log 1 −
X[i, i]
= max
log X[i, i]
i∈[n]
!
2
λmin (X[ī, ī])−1 kX[ī, i]k
− log 1 −
X[i, i]
≤ max
log X[i, i]
i∈[n]


2
− log 1 − λmin (X)−2 max kX[ī, i]k
i∈[n]
≤
log λmin (X)
λmin (X)−2 max kX[ī, i]k
≤

i∈[n]

log λmin (X)

4. Greedy maximization of entropy


Now we are ready to show that the greedy selection gives
close to optimal solution for maximum entropy sampling
on Gaussian RBF kernels satisfying a reasonable condition
on the bandwidth parameter, the inter-point separation, and
the dimension of the underlying space but independent of
the number of points n.
Theorem 5. Let X ∈ Rn×n be a Gaussian
 RBF kernel ma
2
trix, that is, its ij-th entry X[i, j] = exp −γ kxi − xj k ,
for given points x1 , x2 , . . . , xn ∈ Rd and γ > 0. If
the minimum separation δ = mini6=j kxi − xj k, the bandwidth parameter γ > 0 and the dimension d satisfy
 
1
and γδ 2 ≥ 10d log d
d ≥ log


2

then Greedy(f, k) on f (S) = log det (X[S, S]) outputs
.

f (S) ≥ (1 − )f (O) − k,

On Greedy Maximization of Entropy

where O = argmaxf (S) is the optimal solution.
|S|=k

Proof. Consider Y = 2λmin (X)−1 X. λmin (Y ) = 2 and
g(S) = log det (Y [S, S]) = f (S) + |S| log(2λmin (X)−1 )
is a monotone, submodular function by Proposition 2.
For a fixed cardinality constraint |S| = k, g(S)
 is a
fixed translation of f (S) by k log 2λmin (X)−1 , and
hence, optimal solutions
coincide, giving g(O) = f (O) +

k log 2λmin (X)−1 . Moreover, the choices made by
Greedy(f, k) and Greedy(g, k) are the same. Therefore, we
analyze Greedy(g, k) instead to infer about Greedy(f, k).
This helps in two ways. Firstly, even though f is not monotone and does not have a well-defined curvature in [0, 1], g
is monotone with curvature c(g) ∈ [0, 1]. Moreover, c(g)
can be bounded using Proposition 4 as follows.
2

c(g) ≤

λmin (Y )−2 maxi∈[n] kY [ī, i]k
log λmin (Y )
2

=

maxi∈[n] kY [ī, i]k
4 log 2
−2

λmin (X)

≥ (1 − exp(−d log d)) f (O) − exp(−d log d)kd log d
≥ (1 − )f (O) − k,
for large enough d ≥ log (1/).

The main point here is that we can get an approximation
guarantee very close to 1, and the approximation guarantee and the required bandwidth parameter do not depend
on the total number of points n at all. It may be noted that
the constant 10 in the requirement γδ 2 ≥ 10d log d can be
made close to 1 with the same analysis but done more carefully, which gives
 interesting values for minimum separation (exp −γδ 2 ≈ exp (−d log d)) for the case of small
d (for d = 2, the value is 0.25).
4.1. Condition numbers of RBF kernels

2

maxi∈[n] kX[ī, i]k
log 2

≤ C exp d log(dγδ 2 ) − γδ 2 ,

=

1 − e−c(g)
f (O) − c(g) k log(2λmin (X)−1 )
c(g)


c(g)
f (O) − c(g)kd log d
≥ 1−
2
≥

by Lemma 7 and Lemma 8, and the condition
maxi∈[n] kY [ī, i]k = λmin (X)−1 maxi∈[n] kX[ī, i]k ≤
2 = λmin (Y ) of Proposition 4 is also satisfied. Here C is
the constant from the big-Oh notation in Lemma 8. Notice that since γδ 2 ≥ 10d
 log d, we can bound c(g) ≤
C exp d log(dγδ 2 ) − γδ 2 ≤ exp(−d log d).
Secondly, adding α |S| to a monotone submodular function, for a fixed α > 0, decreases its curvature (see Proposition 3). Therefore, if we could control the curvature in
terms of α, we may be able to exploit better approximation
guarantees for smaller curvature. That is exactly our strategy. Notice that we actually scale X up slightly beyond the
λmin (Y ) ≥ 1 condition for monotonicity. Thus, if S is the
output of Greedy(f, k) then
f (S) = g(S) − k log(2λmin (X)−1 )
≥

1 − e−c(g)
g(O) − k log(2λmin (X)−1 )
c(g)

=

1 − e−c(g)
c(g)


f (O) + k log(2λmin (X)−1 )

Schoenberg (Schoenberg, 1937) showed a striking result
that if x1 , x2 , . . . , xn are distinct points in a Hilbert space
then the matrix (kxi − xj k)ij is invertible, which gave
rise to radial basis interpolation methods. To implement
such methods, it is important that this matrix be wellconditioned, in particular its eigenvalues be bounded away
from 0. Keith Ball (Ball, 1992) showed such a bound, independent of n.
Proposition 6. For any points x1 , x2 , . . . , xn ∈ Rd with
minimum separation δ, all the eigenvalues of the 
matrix
√
(kxi − xj k)ij have absolute value at least Ω δ/ d .
This idea was later generalized to various RBF kernel matrices by Narcowich and Ward (Narcowich & Ward, 1992).
The case of interest to us is that of Gaussian RBF kernels, although similar results hold for other RBF kernels
too, e.g., exponential RBF kernel. Here we state a simple
corollary of Theorem 2.3 from (Narcowich & Ward, 1992).
Lemma 7. Let X ∈ Rn×n be a Gaussian
 RBF kernel ma
2
trix, that is, its ij-th entry X[i, j] = exp −γ kxi − xj k ,
for given points x1 , x2 , . . . , xn ∈ Rd and γ > 0. If the
minimum separation δ = mini6=j kxi − xj k and satisfies

γδ 2 ≥ 6d then λmin (X) = Ω exp(− d2 log(dγδ 2 )) .

− k log(2λmin (X)−1 )
=

1 − e−c(g)
f (O)
c(g)
+

1 − c(g) − e−c(g)
k log(2λmin (X)−1 )
c(g)

Proof. By Theorem 2.3 of Narcowich-Ward (Narcowich &
Ward, 1992) mentioned above,
λmin (X) ≥ Cd γ −d/2

 −d
2 δ −2 −1
δ
e−α ( 2 ) γ ,
2

On Greedy Maximization of Entropy

where
α = 12

πΓ2 1 +
9

d
2

1
 ! d+1

≈d

1
d+1



d
2e

and
α2
 ≈ d3/2
Cd = d+1
2 Γ 1 + d2



2d
e

d
 d+1

≈ d,

using γδ 2 ≥ d. Notice that the terms decay rapidly with
t and can be
 upper bounded by a geometric progression
exp −tγδ 2 , giving the final exp(−γδ 2 ) upper bound (up
to constants).

5. Extending to mutual information

− d2
,

up to constants. Plugging in and simplifying gives





d
dγδ 2
4d2
3/2
,
λmin (X) = Ω d exp − log
− 2
2
4
γδ

which becomes Ω exp(− d2 log(dγδ 2 )) if γδ 2 ≥ 6d.
4.2. Off-diagonal row-lengths in Gaussian RBF kernels
Now we show that the off-diagonal entries of Gaussian
RBF kernels decay rapidly if the points satisfy a minimum
separation condition. This helps us bound the row-lengths
of Gaussian RBF kernels independent of n.
Lemma 8. Let X ∈ Rn×n be a Gaussian
 RBF kernel ma
2
trix, that is, its ij-th entry X[i, j] = exp −γ kxi − xj k ,
for given points x1 , x2 , . . . , xn ∈ Rd and γ > 0. If the
minimum separation δ = mini6=j kxi − xj k and γδ 2 ≥ d
then

2
kX[ī, i]k = O exp(−γδ 2 ) , for all i ∈ [n].
Proof. For any fixed i, define


tδ
(t + 1)δ
Ct (i) = j :
≤ kxi − xj k ≤
.
2
2
Let B (x, r) denote the ball of radius r centered at
x. For any j ∈ Cj , the ball B (xj , δ/2) lies outside
B (xi , (t − 1)δ/2) and inside B (xi , (t + 2)δ/2). Moreover, the balls B (xj , δ/2) are all disjoint because δ is the
minimum separation between all pairs. Thus,
 

 

− vol B xi , (t−1)δ
vol B xi , (t+2)δ
2
2
|Ct (i)| ≤
vol (B (xj , δ/2))

Instead of using entropy for sensor placement, GuestrinKrause-Singh (Krause et al., 2008) use mutual information,
which is another submodular function.
Proposition 9. Given any symmetric, positive semidefinite matrix X ∈ Rn×n the mutual information
F (S) = log det (X[S, S]) + log det X[S̄, S̄] is submodular, where X[S, S] denotes the |S| × |S| principal submatrix of X with row and column indices in S, and S̄ denotes
the complement [n] \ S.
Proof. See (Krause et al., 2008).
Now we show that mutual information is monotone over
sets of small size for Gaussian RBF kernels satisfying the
same conditions we used in the previous section about entropy.
Proposition 10. Let X ∈ Rn×n be a Gaussian RBF
kernel matrix satisfying the conditions as in Theorem 5.
Then the mutual information F (S) = log det (X[S, S]) +
log det X[S̄, S̄] is monotone over sets of size k  n.
Proof. For any S ⊆ [n] and i ∈
/ S,


det (X[S ∪ {i}, S ∪ {i}])
F (S ∪ {i}) − F (S) = log
det (X[S, S])
 !
det X[S̄, S̄]

− log
det X[S̄ \ {i}]
However, we can show
det (X[S ∪ {i}, S ∪ {i}])
det (X[S, S])
= X[i, i] − X[S, i]T X[S, S]−1 X[S, i]

2
≥ 1 − λmax X[S, S]−1 kX[S, i]k

≤ (t + 2)d − (t − 1)d

2

≥ 1 − λmin (X)−1 kX[S, i]k

≥ 1 − O exp −γδ 2 .

≤ exp(d log t).
2

We can bound kX[ī, i]k as
2

kX[ī, i]k ≤

∞
X
t=1
∞
X

and


|Ct (i)| exp

2 2

−γt δ
4





γt2 δ 2
≤
exp d log t −
4
t=1



∞
X
t2
,
≤
exp γδ 2 log t −
4
t=1


det X[S̄, S̄]
 = X[i, i]−
det X[S̄ \ {i}]
X[S̄ \ {i}, i]T X[S̄ \ {i}, S̄ \ {i}]−1 X[S̄ \ {i}, i]
2

≤ 1 − λmin X[S̄ \ {i}, S̄ \ {i}]−1 X[S̄ \ {i}, i]

≤ 1 − λmax (X)−1 (n − k − 1) exp −γ∆2

≤ 1 − exp −γ∆2 ,

On Greedy Maximization of Entropy

using λmax (X) ≤ tr (X) = n and k  n, where ∆ =
maxi6=j kxi − xj k is the maximum separation.
Therefore,
F (S ∪ {i}) − F (S) ≥ log

 !
1 − O exp −γδ 2
≥ 0.
1 − exp (−γ∆2 )

The main difficulty in obtaining improved performance
bounds for mutual information based greedy algorithm is
the lack of monotonicity which makes it impossible to use
the notion of curvature here. Our result on monotonicity
for small k is a first step in removing this difficulty. Empirically mutual information is also known to exhibit the
near-optimal performance of the greedy approach, and it
should be interesting to theoretically establish the same under reasonable assumptions.
Table 1. Data sets for multivariate classification with real attributes, chosen to demonstrate our analysis.
DATA SET
I RIS
S ONAR M INES
C LOUD

# I NSTANCES

# ATTRIBUTES

147
111
1024

4
60
10

the Gaussian kernel zero and our results cannot be studied (λmin = δ = 0). This step only reduced the size of
the Iris data set by 3, and the other data sets remained unchanged. The features were then normalized to lie in the
interval [0, 1]. These normalized features were finally used
to generate Gaussian kernels with carefully chosen bandwidth parameter, γ.
6.2. Variation of approximation ratio with bandwidth
parameter
d
suffices for the greedy
As in Lemma 7, γ ≥ γ0 = d log
δ2
algorithm to have near optimal approximation ratio. In
the following we experimentally observe the transition of
the greedy algorithm’s approximation ratio to near optimal
values as the bandwidth parameter is incremented to γ0
from below. Figure 2(a) depicts these plots for the three
chosen data sets. Note that in order to overlay the plots for
different data sets on the same figure, the horizontal scale
has been normalized for the different data sets to range
over the interval [log γ0 − 9, log γ0 ].
An interesting observation from the plot is that the
transition to near-optimality is rather steep and occurs at
approximately the same value of log γ for different data
sets, of about log γ0 − 4. This indicates that our results
qualitatively capture the requirement for near-optimal ratios quite accurately, and are probably tight up to constant
factors in the worst case.

6. Experiments

6.3. Variation of approximation ratio with scaling

In this section, we empirically verify the applicability of
our analysis on three real world data sets as tabulated in Table 1. The data sets have been chosen to capture variation
of both number of examples and number of features. They
have been used to generate Gaussian kernels of different
sizes and in different dimensions, over which our results
have been studied. The experiments essentially consist of
construction of these Gaussian kernels with appropriately
chosen parameters and comparison of entropy of the greedy
algorithm with the entropy of the optimal subset, computed
for some small value of k. The experiments were repeated
for mutual information based optimization, with exact optimality of the greedy algorithm for large range of parameters and small values of k.

Scaling increases the curvature, and hence the multiplicative term in the approximation inequality of Theorem 5,
but also increases the negative additive term. The net
effect can be seen as a sub-logarithmic increase in the
approximation ratio, as in Figure 2(b). For the plots, we
fix the value of log γ at log γ0 − 6, i.e. slightly before the
transition to optimal. Two useful observations can be made
from the plot. Scaling improves the approximation ratio
of the Gaussian kernel, although rather slowly. Even an
exponential increase in scaling seems to improve the ratio
only in a sub-logarithmic fashion. Also, the qualitative
nature of effect of scaling the matrix on the approximation
ratio does not seem to vary significantly over the different
data sets or choices of the bandwidth parameter.

6.1. Pre-processing

We repeated the same sets of experiments for the mutual
information criterion and interestingly observed that the
greedy algorithm is able to find the optimal subset for the
entire range of bandwidth parameter and the scaling parameter for each of the data sets.

1

Data sets were first cleaned to remove duplicate instances
as presence of duplicates makes the smallest eigenvalue of
1
It is easy to argue that introduction of duplicates does not
change either the greedy set or the optimal set of sensors, so long
as total number of chosen sensors, k is less than the total number
of distinct instances.

0.97
0.94
0.91
Iris
Sonar
Cloud

0.88
0.85

−8

−6
−4
−2
log(bandwidth−parameter)

0

1

greedy−to−optimal ratio

1
greedy−to−optimal ratio

greedy−to−optimal ratio

On Greedy Maximization of Entropy

0.99
0.98
0.97
0.96
0.95

2

4
6
log(scaling−factor)

(a)

8

Iris
Sonar
Cloud
10

1
0.97
0.94
0.91
0.88
0.85
1

Iris
Sonar
Cloud

(b)

2

3
4
size of subset selected

5

(c)

Figure 2. (a) Plot of approximation ratio of the greedy algorithm against logarithm of Gaussian kernel bandwidth parameter. Zero
reference for the x-axis corresponds to the critical bandwidth (log γ0 ) for each data set. Size of subset selected, k = 3. (b) Plot of
approximation ratio of the greedy algorithm against logarithm of factor with which the kernel is scaled. The bandwidth parameter of
the kernel is fixed at log γ = log γ0 − 6 for each data set. Size of subset selected, k = 3. (c) Plot of approximation ratio of the greedy
algorithm against the size k of the subset selected. The bandwidth parameter is the same as in (b), and no scaling is used.

The variation of the approximation ratio for the greedy algorithm with k, the size of subset to be selected, is relatively more complex. The decrease due to the small negative additive term in Theorem 5 is countered by increase in
f (O) with k to an indeterminate extent. Thus, for reasonably small k the approximation ratio does not decrease or
vary significantly with increase in the number of sensors,
as in Figure 2(c). For the experiments, the value of log γ
was fixed at log γ0 − 6 and the scaling was fixed at unity.
6.5. Variation of λmin with d
Finally we evaluate the lower bound of Ω (exp (−d log d))
on the minimum eigenvalue (Lemma 7) used in our analysis by using the Sonar data set. To generate the plot we
randomly sample d features from the data set and determine the minimum eigenvalue for the corresponding Gaussian kernel. We observe that the blue curve representing
our bound is a rather pessimistic bound for the minimum
eigenvalue (shown in red in Figure 3), even though it suffices in our analysis to establish near-optimality.
This indicates that it might be possible to obtain significantly better lower bounds for decrease in the minimum
eigenvalue with increasing dimensions for real world data
sets. Plugging them in our analysis will give stronger approximation guarantees on the greedy performance, by allowing for weaker assumptions than those used here in order to establish near-optimality. it should be interesting
to obtain an understanding of the properties that the data
points satisfy which could be exploited to get such an improvement.

0

min. eigenvalue of kernel

6.4. Variation of approximation ratio with k

10

−3

10

actual value

−6

10

−9

10

lower bound used

−12

10

−15

10

−18

10

2

4

6
8
10
12
dimension of data set, d

14

Figure 3. Plot of minimum eigenvalue of the Gaussian kernel obtained using subset of Sonar data set by randomly sampling d
features and taking a minimum over a few iterations for smoothness. A logarithmic scale is used on the vertical axis for sake of
clarity.

7. Conclusion and Future Work
The main result of this paper is to establish a theoretical basis for the empirically observed near optimal performance
of the greedy algorithm for maximum entropy sampling,
which has important applications in sensor placement. This
is the first improvement over the general (1 − 1/e) bound
for submodular optimization, and holds for the extremely
common case of Gaussian RBF kernels.
There is great scope for extension of this result to similar
results for other kernels and also to other optimization criteria like mutual information. In fact, it seems that greedy
performs close to the optimal result even with random kernels, with overwhelming probability.

References
Ball, Keith. Eigenvalues of euclidean distance matrices.
Journal of Approximation Theory, 68:74–82, 1992.

On Greedy Maximization of Entropy

Conforti, Michele and Cornuejols, Gerard. Submodular
set functions, matroids and the greedy algorithm: Tight
worst-case bounds and some generalizations of the radoedmonds theorem. Discrete Applied Mathematics, 7(3):
251–274, 1984.
Cressie, Noel. Statistics for spatial data. John Wiley and
Sons Inc., New York, NY, 1991.
Isler, Volkan and Bajcsy, Ruzena. The sensor selection
problem for bounded uncertainty sensing models. In
Proceedings of the 4th international symposium on Information processing in sensor networks, pp. 20. IEEE
Press, 2005.
Kapoor, Ashish and Horvitz, Eric. Breaking boundaries:
Active information acquisition across learning and diagnosis. Advances in Neural Information Processing Systems (NIPS), 2009.
Ko, C., Lee, J., and Queyranne, M. An exact algorithm for
maximum entropy sampling. Operations Research, 43
(4):684–691, 1995.
Krause, Andreas and Guestrin, Carlos E. Near-optimal
nonmyopic value of information in graphical models.
arXiv preprint arXiv:1207.1394, 2012.
Krause, Andreas, Singh, Ajit, and Guestrin, Carlos. Nearoptimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. The Journal of Machine Learning Research, 9:235–284, 2008.
Mirzasoleiman, Baharan, Badanidiyuru, Ashwinkumar,
Karbasi, Amin, Vondrak, Jan, and Krause, Andreas.
Lazier than lazy greedy. In Proc. Conference on Artificial Intelligence (AAAI), 2015.
Narcowich, Francis J. and Ward, Joseph D. Norm estimates
for the inverses of a general class of scattered-data radialfunction interpolation matrices. Journal of Approximation Theory, 69:84–109, 1992.
Nemhauser, George L, Wolsey, Laurence A, and Fisher,
Marshall L. An analysis of approximations for maximizing submodular set functionsi. Mathematical Programming, 14(1):265–294, 1978.
Rasmussen, Carl Edward and Williams, Christopher K. I.
Gaussian Processes for Machine Learning. MIT Press,
2006.
Rowaihy, Hosam, Eswaran, Sharanya, Johnson, Matthew,
Verma, Dinesh, Bar-Noy, Amotz, Brown, Theodore, and
La Porta, Thomas. A survey of sensor selection schemes
in wireless sensor networks. In Defense and Security
Symposium, pp. 65621A–65621A. International Society
for Optics and Photonics, 2007.

Schoenberg, I. J. On certain metric spaces arising from euclidean space by a change of metric and their imbedding
in hilbert space. Annals of Math, 38:787–793, 1937.
Settles, Burr. Active learning literature survey. University
of Wisconsin, Madison, 52(55-66):11, 2010.
Shamaiah, Manohar, Banerjee, Siddhartha, and Vikalo,
Haris. Greedy sensor selection: Leveraging submodularity. In Decision and Control (CDC), 2010 49th IEEE
Conference on, pp. 2572–2577. IEEE, 2010.
Shewry, Michael C and Wynn, Henry P. Maximum entropy
sampling. Journal of Applied Statistics, 14(2):165–170,
1987.
Strang, Gilbert. Introduction to Linear Algebra (4th edition). Wellesley-Cambridge Press, 2009.
Sviridenko, Maxim, Vondrák, Jan, and Ward, Justin. Optimal approximation for submodular and supermodular
optimization with bounded curvature. In Proceedings of
the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 1134–1148, 2015.
Wang, Hanbiao, Yao, Kung, Pottie, Greg, and Estrin, Deborah. Entropy-based sensor selection heuristic for target localization. In Proceedings of the 3rd international symposium on Information processing in sensor
networks, pp. 36–45. ACM, 2004.
Wang, Hanbiao, Yao, Kung, and Estrin, Deborah.
Information-theoretic approaches for sensor selection
and placement in sensor networks for target localization
and tracking. Communications and Networks, Journal
of, 7(4):438–449, 2005.

