Outlier Path: A Homotopy Algorithm for Robust SVM

Shinya SUZUMURA
suzumura.mllab.nit@gmail.com
Nagoya Institute of Technology Gokiso-cho, Showa-ku, Nagoya, Aichi 466–8555 Japan
Kohei OGAWA
ogawa.mllab.nit@gmail.com
Nagoya Institute of Technology Gokiso-cho, Showa-ku, Nagoya, Aichi 466–8555 Japan
Masashi Sugiyama
Tokyo Institute of Technology O-okayama, Meguro-ku, Tokyo 152-8552, Japan

sugi@cs.titech.ac.jp

Ichiro Takeuchi
takeuchi.ichiro@nitech.ac.jp
Nagoya Institute of Technology Gokiso-cho, Showa-ku, Nagoya, Aichi 466–8555 Japan

Abstract
In recent applications with massive but less reliable data (e.g., labels obtained by a semisupervised learning method or crowdsourcing),
non-robustness of the support vector machine
(SVM) often causes considerable performance
deterioration. Although improving the robustness of SVM has been investigated for long
time, robust SVM (RSVM) learning still poses
two major challenges: obtaining a good (local)
solution from a non-convex optimization problem and optimally controlling the robustnessefficiency trade-off. In this paper, we address
these two issues simultaneously in an integrated
way by introducing a novel homotopy approach
to RSVM learning. Based on theoretical investigation of the geometry of RSVM solutions,
we show that a path of local RSVM solutions
can be computed efficiently when the influence
of outliers is gradually suppressed as simulated
annealing. We experimentally demonstrate that
our algorithm tends to produce better local solutions than the alternative approach based on
the concave-convex procedure, with the ability
of stable and efficient model selection for controlling the influence of outliers.

1. Introduction
The support vector machine (SVM) is one of the most
popular classification algorithms that has achieved signifProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

positive
negative

(a) Standard SVM

positive
negative

(b) Robust SVM (RSVM)

Figure 1. Illustrative examples of (a) standard SVM and (b) robust
SVM (RSVM) on a toy dataset. In RSVM, the classification result
is not sensitive to the two red outliers in the right-hand side of the
graphs.

icant empirical success in various real-world applications
(Vapnik, 1996). However, SVM was known to be sensitive to outliers which limits the usability of SVM in recent applications with massive but less reliable data (e.g.,
automatically labeled data by semi-supervised learning or
manually labeled data in crowdsourcing). In order to alleviate adverse influence of outliers, various robust extensions of SVM (robust SVM; RSVM) have been proposed
(Masnadi-Shiraze & Vasconcelos, 2000; Shen et al., 2003;
Krause & Singer, 2004; Liu et al., 2005; Liu & Shen, 2006;
Xu et al., 2006; Collobert et al., 2006; Wu & Liu, 2007;
Masnadi-Shirazi & Vasconcelos, 2009; Freund, 2009; Yu
et al., 2010). Figure 1 illustrates the robust behavior of
RSVM.
When we use RSVM in practice, we encounter two major difficulties. The first one is the non-convexity of the
RSVM optimization problem, which results in obtaining
only a local optimal solution. Another difficulty is the control of the robustness of the solution. In RSVM, the ro-

Outlier Path

bustness of the solution is controlled by a hyper-parameter,
and we usually change the hyper-parameter value gradually
and find the best one by cross-validation. However, due to
the non-convexity, the RSVM solutions with slightly different hyper-parameter values can be significantly different,
which makes model selection by cross-validation highly
challenging.
In this paper, we introduce a novel approach to RSVM
learning to address these issues. Our basic idea is to use
the homotopy methods (Allgower & George, 1993; Gal,
1995; Ritter, 1984; Best, 1996) to trace a path of local optimal solutions when the influence of outliers is gradually
decreased by changing the hinge loss to more robust ones.
Figure 2 illustrates two different ways to gradually robustify the hinge loss. So far, homotopy-like methods have
been (often implicitly) employed in sparse modeling and
semi-supervised learning (Zhang, 2010; Mazumder et al.,
2011; Zhou et al., 2012; Ogawa et al., 2013). However, to
the best of our knowledge, this is the first work that applies
the homotopy method to RSVM.
After problem formulation in § 2, we derive in § 3 the necessary and sufficient conditions for an RSVM solution to
be locally optimal, and show that there exist a finite number of discontinuous points in the local solution path. We
then propose an efficient algorithm in § 4 that can precisely
detect such discontinuous points and jump to find a strictly
better local optimal solution. In § 5, we experimentally
demonstrate that our proposed method, named the outlier
path, outperforms the existing RSVM algorithm based on
the concave-convex procedure or the difference-of-convex
programming (Shen et al., 2003; Krause & Singer, 2004;
Liu et al., 2005; Liu & Shen, 2006; Collobert et al., 2006;
Wu & Liu, 2007). Finally, we conclude in § 6.

2. Parameterized RSVM
Let us consider a binary classification problem with n instances and d features. We denote the training set as
{(xi , yi )}i∈Nn where xi ∈ X is the input vector in the input space X ⊂ Rd , yi ∈ {−1, 1} is the binary class label,
and the notation Nn := {1, . . . , n} represents the set of natural numbers up to n. We write the decision function as
f (x) := w> φ(x), where φ is the feature map implicitly defined by a kernel K, w is a vector in the feature space, and
>
denotes the transpose of vectors and matrices.
We introduce the following class of optimization problems
parameterized by θ and s:
X
1
min kwk2 + C
`(yi f (xi ); θ, s),
w 2
i=1
n

(1)

where C > 0 is the regularization parameter. The loss function ` is characterized by a pair of parameters θ ∈ [0, 1] and

s ≤ 0 in the following way:
(
`(z; θ, s) :=

[0, 1 − z]+ , z ≥ s,
1 − θz − s, z < s,

(2)

where [z]+ := max{0, z}. We refer to θ and s as homotopy
parameters. Figure 2 shows the loss functions for several
θ and s. The first homotopy parameter θ can be interpreted
as the weight for an outlier: θ = 1 indicates that the influences of outliers and inliers are same, while θ = 0 indicates
that outliers are completely ignored. The second homotopy
parameter s ≤ 0 is interpreted as the threshold for outliers.
In the following sections, we consider two types of homotopy methods. In the first method, we fix s = 0, and gradually change θ from 1 to 0 (see the top five plots in Figure 2).
In the second method, we fix θ = 0 and gradually change s
from −∞ to 0 (see the bottom five plots in Figure 2). Note
that the loss function is reduced to the hinge loss for the
standard (convex) SVM when θ = 1 or s = −∞. Therefore, each of the above two homotopy methods can be interpreted as the process of tracing a sequence of solutions
when the optimization problem is gradually modified from
convex to non-convex. We expect to find good local optimal solutions because such a process can be interpreted as
simulated annealing (Hromkovic, 2001). In addition, we
can adaptively control the degree of robustness by selecting
the best θ or s based on some model selection scheme.

3. Local Optimality of RSVM
In order to use the homotopy approach, we need to clarify the continuity of the local solution path. To this end,
we investigate several properties of RSVM local solutions,
and derive the necessary and sufficient conditions. Interestingly, our analysis reveals that the local solution path has a
finite number of discontinuous points. The theoretical results presented here form the basis of our novel homotopy
algorithm given in the next section that can properly handle
the above discontinuity issue.
3.1. Conditionally Optimal Solutions
The basic idea of our theoretical analysis is to reformulate the RSVM learning problem as a combinatorial optimization problem. We consider a partition of the instances
Nn := {1, . . . , n} into two disjoint sets I and O. The instances in I and O are defined as Inliers and Outliers,
respectively. Here, we restrict that the margin yi f (xi ) of
an inlier should be larger than s, while that of an outlier should be smaller than s. We denote the partition as
P := {I, O} ∈ 2Nn , where 2Nn is the power set of Nn . Given
a partition P, the above restrictions define the feasible re-

Outlier Path

1
0.5

1.5
1
0.5

0

0
0.5
yi f(xi )

1

1.5

−0.5
−2

2

θ = 1, s = 0
3

0
0.5
yi f(xi )

1

1.5

−0.5
−2

2

3
0−1 Loss
Hinge Loss

1
0.5

1.5
1
0.5

0

0
0.5
yi f(xi )

1

1.5

−0.5
−2

2

1

0

−1.5

−1

−0.5

0
0.5
yi f(xi )

1

1.5

−0.5
−2

2

1

1.5

2

−0.5
−2

θ = 0, s = −∞

−1

−0.5

0
0.5
yi f(xi )

1

1.5

2

−0.5
−2

1.5
1

−1

−0.5

0
0.5
yi f(xi )

1

1.5

2

−0.5
−2

1

1.5

2

0−1 Loss
Robust Loss

1.5
1
0.5

0

−1.5

0
0.5
yi f(xi )

2

0.5

0

−1.5

−0.5

2.5

2

1

−1

3
0−1 Loss
Robust Loss

2.5

1.5

−1.5

θ = 0, s = 0

3
0−1 Loss
Robust Loss

0.5

0

−0.5

0
0.5
yi f(xi )

2
Loss (yi f(xi ))

Loss (yi f(xi ))

1.5

−1

−0.5

2.5

2

−1.5

−1

1.5

0.5

0

−1.5

3
0−1 Loss
Robust Loss

2.5

2
Loss (yi f(xi ))

−0.5

1

θ = 0.75, s = 0
θ = 0.5, s = 0
θ = 0.25, s = 0
(a) Homotopy computation with decreasing θ from 1 to 0.

2.5

−0.5
−2

−1

1.5

0.5

0

−1.5

2

Loss (yi f(xi ))

−0.5

1

Loss (yi f(xi ))

−1

1.5

0−1 Loss
Robust Loss

2.5

2

0.5

0

−1.5

3
0−1 Loss
Robust Loss

2.5

2
Loss (yi f(xi ))

1.5

3
0−1 Loss
Robust Loss

2.5

2
Loss (yi f(xi ))

Loss (yi f(xi ))

2

−0.5
−2

3
0−1 Loss
Robust Loss

2.5

Loss (yi f(xi ))

3
0−1 Loss
Hinge Loss

Loss (yi f(xi ))

3
2.5

0

−1.5

−1

−0.5

0
0.5
yi f(xi )

1

1.5

2

−0.5
−2

θ = 0, s = −1.5
θ = 0, s = −1
θ = 0, s = −0.5
(b) Homotopy computation with decreasing s from −∞ to 0.

−1.5

−1

−0.5

0
0.5
yi f(xi )

1

1.5

2

θ = 0, s = 0

Figure 2. Robust loss functions for various homotopy parameters θ and s.

gion of the solution f in the form of a convex polytope1 :
( 
)
 y f (xi ) ≥ s, i ∈ I
pol(P; s) := f  i
.
(3)
 yi f (xi ) ≤ s, i ∈ O
Using the notion of the convex polytopes, the optimization
problem (1) can be rewritten as
min

min

P∈2Nn f ∈pol(P;s)

!
JP ( f ; θ) ,

(4)

where {α∗j } j∈Nn are the optimal Lagrange multipliers. The
following lemma summarizes the KKT optimality conditions of the conditionally optimal solution fP∗ .
Lemma 2 The KKT conditions of the convex problem (5)
is written as

1
JP ( f ; θ) := ||w||22

2
X

X

[1 − yi f (xi )]+  .
+ C  [1 − yi f (xi )]+ + θ
i∈O

When the partition P is fixed, it is easy to confirm that the
inner minimization problem in (4) is convex.
Definition 1 (Conditionally optimal solutions) Given a
partition P, the solution of the following convex problem
is said to be the conditionally optimal solution:
fP∗ := argmin JP ( f ; θ).
f ∈pol(P;s)

(5)

The formulation in (4) is interpreted as a combinatorial optimization problem of finding the best solution from all the
Note that an instance with the margin yi f (xi ) = s can be the
member of either I or O.
2
Note that we omitted the constant terms irrelevant to the optimization problem.
1

Using the representer theorem or convex optimization theory, we can show that any conditionally optimal solution
can be written as
X
fP∗ (x) :=
α∗j y j K(x, x j ),
(6)
j∈Nn

where the objective function JP is defined as2

i∈I

2n conditionally optimal solutions fP∗ corresponding to all
possible 2n partitions3 .

s<

yi fP∗ (xi ) > 1 ⇒ α∗i = 0,

(7a)

=1 ⇒
<1 ⇒

∈ [0, C],
= C,

(7b)
(7c)

≥ C,
≤ Cθ,
= Cθ.

(7d)
(7e)
(7f)

yi fP∗ (xi )
yi fP∗ (xi )

yi fP∗ (xi ) = s, i ∈ I ⇒
yi fP∗ (xi ) = s, i ∈ O ⇒
yi fP∗ (xi ) < s ⇒

α∗i
α∗i
α∗i
α∗i
α∗i

The proof is omitted because it can be easily derived based
on standard convex optimization theory (Boyd & Vandenberghe, 2004).
3.2. The necessary and sufficient conditions for local
optimality
From the definition of conditionally optimal solutions, it
is clear that a local optimal solution must be conditionally
3
For some partitions P, the convex problem (5) might not have
any feasible solutions.

Outlier Path

optimal within the convex polytope pol(P; s). However,
the conditional optimality does not necessarily indicate the
local optimality as the following theorem suggests.
Theorem 3 For any θ ∈ [0, 1) and s ≤ 0, consider a situation where a conditionally optimal solution fP∗ is at the
boundary of the convex polytope pol(P; s), i.e., there exists
at least an instance such that yi fP∗ (xi ) = s. In this situation,
if we define a new partition P̃ := {Ĩ, Õ} as
Ĩ ← I\{i ∈ I|yi f ∗ (xi ) = s}∪{i ∈ O|yi f ∗ (xi ) = s},
Õ ← O\{i ∈ O|yi f ∗ (xi ) = s}∪{i ∈ I|yi f ∗ (xi ) = s},

(a) Local solution path

(b) Local optimum

(c) Not local optimum

(d) Local optimum

(8a)
(8b)

then the new conditionally optimal solution fP̃∗ is strictly
better than the original conditionally optimal solution fP∗ ,
i.e.,
JP̃ ( fP̃∗ ; θ) < JP ( fP∗ ; θ).

(9)

The proof is presented in Appendix A. Theorem 3 indicates that if fP∗ is at the boundary of the convex polytope
pol(P; s), i.e., if there is one or more instances such that
yi fP∗ (xi ) = s, then fP∗ is NOT locally optimal because there
is a strictly better solution in the opposite side of the boundary.
The following theorem summarizes the necessary and sufficient conditions for local optimality. Note that, in nonconvex optimization problems, the KKT conditions are
necessary but not sufficient in general.

Figure 3. Solution space of RSVM. (a) The arrows indicate a local
solution path when θ is gradually moved from θ1 to θ5 (see § 4 for
more details). (b) fP∗ is locally optimal if it is at the strict interior
of the convex polytope pol(P; s). (c) If fP∗ exists at the boundary, then fP∗ is feasible, but not locally optimal. A new convex
polytope pol(P̃; s) defined in the opposite side of the boundary is
shown in yellow. (d) A strictly better solution exists in pol(P̃; s).

yi f ∗ (xi ) > 1
yi f ∗ (xi ) = 1

⇒
⇒

α∗i = 0,
α∗i ∈ [0, C],

(10a)
(10b)

s < yi f ∗ (xi ) < 1
yi f ∗ (xi ) < s

⇒
⇒

α∗i = C,
α∗i = Cθ,

a situation. If the local solution path arrives at the boundary, it can jump to the new conditionally optimal solution
fP̃∗ which is located on the opposite side of the boundary.
This jump operation is justified because the new solution is
shown to be strictly better than the previous one. Figure 3
(c) and (d) illustrate such a situation.

(10c)
(10d)

4. Outlier Path Algorithm

Theorem 4 For θ ∈ [0, 1) and s ≤ 0,

yi f ∗ (xi ) , s,

∀i ∈ Nn ,

(10e)

are necessary and sufficient for f ∗ to be locally optimal.
The proof is presented in Appendix B. The condition (10e)
indicates that the solution at the boundary of the convex
polytope is not locally optimal. Figure 3 illustrates when a
conditionally optimal solution can be locally optimal with
a certain θ or s.
Theorem 4 suggests that, whenever the local solution path
computed by the homotopy approach encounters a boundary of the current convex polytope at a certain θ or s, the
solution is not anymore locally optimal. In such cases, we
need to somehow find a new local optimal solution at that θ
or s, and restart the local solution path from the new one. In
other words, the local solution path has discontinuity at that
θ or s. Fortunately, Theorem 3 tells us how to handle such

Based on the analysis presented in the previous section,
we develop a novel homotopy algorithm for RSVM. We
call the proposed method the outlier-path (OP) algorithm.
For simplicity, we consider homotopy path computation involving either θ or s, and denote the former as OP-θ and the
latter as OP-s. OP-θ computes the local solution path when
θ is gradually decreased from 1 to 0 with fixed s = 0, while
OP-s computes the local solution path when s is gradually
increased from −∞ to 0 with fixed θ = 0.
4.1. Overview
The main flow of the OP algorithm is described in Algorithm 1. The solution f is initialized by solving the standard (convex) SVM, and the partition P := {I, O} is defined to satisfy the constraints in (3). The algorithm mainly
switches over the two steps called the continuous step (C-

Outlier Path

Algorithm 1 Outlier Path Algorithm
1: Initialize the solution f by solving the standard SVM.
2: Initialize the partition P := {I, O} as follows:

Algorithm 2 Continuous Step (C-step)
1: while (yi f (xi ) , s ∀ i ∈ Nn ) do
2:
Solve the sequence of convex problems,

I ← {i ∈ Nn |yi f (xi ) ≤ s},
O ← {i ∈ Nn |yi f (xi ) > s}.
3: θ ← 1 for OP-θ; s ← mini∈Nn yi f (xi ) for OP-s.
4: while θ > 0 for OP-θ; s < 0 for OP-s do
5:
if (yi f (xi ) , s ∀ i ∈ Nn ) then
6:
Run C-step.
7:
else
8:
Run D-step.
9:
end if
10: end while

min

f ∈pol(P;s)

JP ( f ; θ),

for gradually decreasing θ in OP-θ or gradually increasing s in OP-s.
3: end while
Algorithm 3 Discontinuous Step (D-step)
1: Update the partition P := {I, O} as follows:
I ← I \ {i ∈ I|yi f (xi ) = s} ∪ {i ∈ O|yi f (xi ) = s},
O ← O \ {i ∈ O|yi f (xi ) = s} ∪ {i ∈ I|yi f (xi ) = s}.
2: Solve the following convex problem for fixed θ and s:

step) and the discontinuous step (D-step).
In the C-step (Algorithm 2), a continuous path of local solutions is computed for a sequence of gradually decreasing
θ (or increasing s) within the convex polytope pol(P; s) defined by the current partition P. If the local solution path
encounters a boundary of the convex polytope, i.e., if there
exists at least an instance such that yi f (xi ) = s, then the
algorithm stops updating θ (or s) and enters the D-step.
In the D-step (Algorithm 3), a better local solution is obtained for fixed θ (or s) by solving a convex problem defined over another convex polytope in the opposite side of
the boundary (see Figure 3(d)). If the new solution is again
at a boundary of the new polytope, the algorithm repeatedly calls the D-step until it finds the solution in the strict
interior of the current polytope.
The C-step can be implemented by any homotopy algorithms for solving a sequence of quadratic problems (QP).
In OP-θ, the local solution path can be exactly computed
because the path within a convex polytope can be represented as piecewise-linear functions of the homotopy parameter θ. In OP-s, the C-step is trivial because the optimal
solution is shown to be constant within a convex polytope.
In § 4.2 and § 4.3, we will describe the details of our implementation of the C-step for OP-θ and OP-s, respectively.
In the D-step, we only need to solve a single quadratic
problem (QP). Any QP solver can be used in this step. We
note that the warm-start approach (DeCoste & Wagstaff,
2000) is quite helpful in the D-step because the difference
between two conditionally optimal solutions in adjacent
two convex polytopes is typically very small. In § 4.4, we
describe the details of our implementation of the D-step.
Figure 4 illustrates an example of the local solution path
obtained by OP-θ.
In Algorithm 1, If the conditionally optimal solution is at
the boundary, we again enters to the D-step. The objective

min

f ∈pol(P;s)

JP ( f ; θ).

function JP strictly decreases each time as shown in Theorem 3. Since any local optimal solutions must be in the
strict interior as shown in Theorem 4, and the number of
convex polytopes is finite, the algorithm will finally find a
local optimal solution in finite time.
4.2. Continuous-Step for OP-θ
In the C-step, the partition P := {I, O} is fixed, and our task
is to solve a sequence of convex quadratic problems (QPs)
parameterized by θ within the convex polytope pol(P; s).
It has been known in optimization literature that a certain
class of parametric convex QP can be exactly solved by exploiting the piecewise linearity of the solution path (Best,
1996). We can easily show that the local solution path
of OP-θ within a convex polytope is also represented as
a piecewise-linear function of θ. The algorithm presented
here is similar to the SVM regularization path algorithm in
Hastie et al. (2004).
Let us consider a partition of the inliers in I into the following three disjoint sets:
R

:=

E :=
L :=

{i|1 < yi f (xi )},
{i|yi f (xi ) = 1},
{i|s < yi f (xi ) < 1}.

For a given fixed partition {R, E, L, O}, the KKT conditions
of the convex problem (5) indicate that
αi = 0 ∀ i ∈ R, αi = C ∀ i ∈ L, αi = Cθ ∀ i ∈ O.
The KKT conditions also imply that the remaining Lagrange multipliers {αi }i∈E must satisfy the following linear

Outlier Path

s that changes the partition P. Such s can be simply found
as

200

α

150

s ← min yi f (xi ).

100

i∈L

50

4.4. Discontinuous-Step (for Both OP-θ and OP-s)
0
0

0.2

0.4

0.6

0.8

1

θ

Figure 4. An example of the local solution path by OP-θ on a simple toy data set (with C = 200). The paths of five Lagrange multipliers α∗1 , · · · , α∗4 are plotted in the range of θ ∈ [0, 1]. Open
circles represent the discontinuous points in the path. In this simple example, we had experienced three discontinuous points at
θ = 0.37, 0.67 and 0.77.

Given that the difference between the two solutions fP∗ and
fP̃∗ is typically small, the D-step can be efficiently implemented by a technique used in the context of incremental
learning (Cauwenberghs & Poggio, 2001).
Let us define

system of equations:
X
yi f (xi ) =
α j yi y j K(xi , x j ) = 1 ∀ i ∈ E

∆I→O := {i ∈ I | yi fP (xi ) = s},
∆O→I := {i ∈ O | yi fP (xi ) = s},

j∈Nn

⇔ QEE αE = 1 − QEL 1C − QEO 1Cθ,

As mentioned before, any convex QP solver can be used
for the D-step. When the algorithm enters the D-step, we
have the conditionally optimal solution fP∗ for the partition
P := {I, O}. Our task here is to find another conditionally
optimal solution fP̃∗ for P̃ := {Ĩ, Õ} given by (8).

(11)

where Q ∈ Rn×n is an n × n matrix whose (i, j)th entry is
defined as Qi j := yi y j K(xi , x j ). Here, a notation such as
QEL represents a submatrix of Q having only the rows in
the index set E and the columns in the index set L. By
solving the linear system of equations (11), the Lagrange
multipliers αi , i ∈ Nn , can be written as an affine function
of θ.
P
Noting that yi f (xi ) = j∈Nn α j yi y j K(xi , x j ) is also represented as an affine function of θ, any changes of the partition {R, E, L} can be exactly identified when the homotopy parameter θ is continuously decreased. Since the solution path linearly changes for each partition of {R, E, L},
the entire path is represented as a continuous piecewiselinear function of the homotopy parameter θ. We denote the
points in θ ∈ [0, 1) at which members of the sets {R, E, L}
change as break-points θBP .
Using the piecewise-linearity of yi f (xi ), we can also identify when we should switch to the D-step. Once we detect
an instance satisfying yi f (xi ) = s, we exit the C-step and
enter the D-step.
4.3. Continuous-Step for OP-s
Since θ is fixed to 0 in OP-s, the KKT conditions (7) yields
αi = 0 ∀ i ∈ O.
This means that outliers have no influence on the solution and thus the conditionally optimal solution fP∗ does not
change with s as long as the partition P is unchanged. The
only task in the C-step for OP-s is therefore to find the next

and α(bef) be the corresponding α at the beginning of the
D-Step. Then, we consider the following parameterized
problem with parameter µ ∈ [0, 1]:
fP̃ (xi ; µ) := fP̃ (xi ) + µ∆ fi ∀ i ∈ Nn ,
where
∆ fi := yi

h

Ki,∆I→O

Ki,∆O→I


i  α(bef) − 1Cθ
 ∆I→O
 α(bef) − 1C
∆O→I



 .

We can show that fP̃ (xi ; µ) is reduced to fP (xi ) when
µ = 1, while it is reduced to fP̃ (xi ) when µ = 0 for
all i ∈ Nn . By using a similar technique to incremental learning (Cauwenberghs & Poggio, 2001), we can efficiently compute the path of solutions when µ is continuously changed from 1 to 0. This algorithm behaves similarly to the C-step in OP-θ. The implementation detail of
the D-step is described in Appendix C.

5. Numerical Experiments
In this section, we compared the proposed outlier-path (OP)
algorithm with the concave-convex procedure (CCCP)
(Yuille & Rangarajan, 2002). In most of the existing
RSVM studies, CCCP or a variant called difference of convex (DC) programming are used for optimizing RSVM
(Shen et al., 2003; Krause & Singer, 2004; Liu et al., 2005;
Liu & Shen, 2006; Collobert et al., 2006; Wu & Liu, 2007).
Setup We used the 10 benchmark data sets listed in Table 1. We randomly divided each data set into the training
(40%), validation (30%), and test (30%) sets for training,
model selection (including the selection of θ or s), and performance evaluation, respectively. In the training and validation data, we flipped 15% of the labels as outliers.

Outlier Path

Table 1. Benchmark data sets. n and d denote the number of instances and the input dimensionality, respectively.
D1
D2
D3
D4
D5
D6
D7
D8
D9
D10

Data
BreastCancerDiagnostic
AustralianCreditApproval
German.Numer
SVMGuide1
Spambase
Musk
Gisette
w5a
a6a
a7a

n
569
690
1000
3089
4601
6598
6000
9888
11220
16100

d
30
14
24
4
57
166
5000
300
122
122

Table 2. The mean of test error and standard deviation (linear).
Smaller test error is better. The numbers in bold face indicate the
better method in terms of the test error.
Data
D1
D2
D3
D4
D5
D6
D7
D8
D9
D10

C-SVM
.056(.016)
.151(.018)
.281(.028)
.066(.007)
.108(.010)
.072(.005)
.185(.013)
.020(.002)
.173(.004)
.173(.008)

CCCP-θ
.050(.014)
.145(.007)
.270(.033)
.047(.007)
.088(.009)
.058(.006)
.184(.010)
.020(.003)
.181(.009)
.176(.006)

OP-θ
.049(.016)
.151(.018)
.270(.023)
.047(.005)
.088(.009)
.064(.003)
.184(.010)
.020(.002)
.173(.005)
.173(.007)

CCCP-s
.055(.018)
.145(.007)
.262(.013)
.053(.010)
.088(.010)
.061(.007)
.184(.010)
.021(.003)
.165(.004)
.160(.004)

OP-s
.050(.016)
.152(.010)
.266(.013)
.042(.006)
.084(.007)
.060(.003)
.184(.010)
.020(.003)
.164(.004)
.161(.005)

Table 3. The mean of test error and standard deviation (RBF).

Generalization Performance First, we compared the
generalization performance. We used the linear kernel
and the radial basis function
 (RBF) kernel defined as
K(xi , x j ) = exp −γkxi − x j k2 , where γ is a kernel parameter fixed to γ = 1/d with d being the input dimensionality. Model selection was carried out by finding the best
hyperparameter combination that minimizes the validation
error. We have a pair of hyperparameters in each setup.
In all the setups, the regularization parameter C was chosen from {0.01, 0.1, 1, 10, 100}, while the candidates of the
homotopy parameter θ or s were set as follows:
• In OP-θ, all the break-points θBP were considered as
the candidates (note that the local solutions at each
break-point have been already computed in the homotopy computation).
• In OP-s, all the break-points for sBP between sinit :=
mini∈Nn yi f (xi ) and 0 are considered as the candidates.
• In CCCP-θ (which is compared with OP-θ), the
homotopy parameter θ was selected from θ ∈
{1, 0.75, 0.5, 0.25, 0}.
• In CCCP-s (which is compared with OP-s), the homotopy parameter s was selected from
s ∈ {sinit , 0.75sinit , 0.5sinit , 0.25sinit , 0}.
Note that both OP and CCCP were initialized by using the
standard SVM.
Tables 2 and 3 represent the average and the standard deviation of the test errors on 10 different random data splits.
These results indicate that OP could find better local solutions and the degree of robustness was appropriately controlled.
Computational Time Finally, we compared the computational costs of the entire model-building process of each
method. The results are shown in Figure 5. Note that the
computational cost of the OP algorithm does not depend on

Data
D1
D2
D3
D4
D5
D6
D7
D8
D9
D10

C-SVM
.055(.017)
.149(.010)
.276(.024)
.052(.009)
.117(.012)
.046(.007)
.044(.003)
.022(.003)
.169(.003)
.163(.003)

CCCP-θ
.043(.022)
.148(.010)
.267(.026)
.048(.009)
.109(.013)
.045(.007)
.044(.003)
.022(.003)
.170(.005)
.163(.003)

OP-θ
.042(.017)
.147(.010)
.266(.024)
.044(.006)
.107(.012)
.045(.007)
.044(.003)
.022(.003)
.169(.004)
.163(.003)

CCCP-s
.037(.016)
.146(.013)
.271(.015)
.047(.008)
.107(.011)
.045(.007)
.044(.003)
.022(.003)
.168(.005)
.162(.002)

OP-s
.038(.013)
.142(.013)
.261(.020)
.040(.005)
.094(.008)
.043(.006)
.044(.003)
.021(.002)
.162(.003)
.160(.004)

the number of hyperparameter candidates of θ or s, because
the entire path of local solutions has already been computed
with the infinitesimal resolution in the homotopy computation. On the other hand, the computational cost of CCCP
depends on the number of hyperparameter candidates. In
our implementation of CCCP, we used the warm-start approach, i.e., we initialized CCCP with the previous solution
for efficiently computing a sequence of solutions. The results indicate that the proposed OP algorithm enables stable and efficient control of robustness, while CCCP suffers
a trade-off between model selection performance and computational costs.

6. Conclusions
In this paper, we proposed a novel robust SVM learning
algorithm based on the homotopy approach that allows efficient computation of the sequence of local optimal solutions when the influence of outliers is gradually deemphasized. The algorithm is built on our theoretical findings
about the geometric property and the optimality conditions
of an RSVM local solution. Experimental results indicate
that our algorithm tends to find better local solutions possibly due to the simulated annealing-like effect and the stable
control of robustness. One of the important future works
is to adopt scalable homotopy algorithms or approximate
parametric programming algorithms (Giesen et al., 2012)
as the building block of our algorithm to further improve
the computational efficiency.

Outlier Path
CCCP-θ(num of candidates: 5)
CCCP-θ(num of candidates: 10)
CCCP-θ(num of candidates: 15)
OP-θ

100000
10000

10000
Elapsed time [sec]

Elapsed time [sec]

CCCP-s(num of candidates: 5)
CCCP-s(num of candidates: 10)
CCCP-s(num of candidates: 15)
OP-s

100000

1000
100
10

1000
100
10

1
0.1

1

0.01

0.1
D8

D9

D10

D8

D9

D10

D7

D6

D5

D4

D3

D2

D1

D10

D9

D8

D7

D6

D5

D4

D3

D2

D1

(a) Elapsed time for CCCP and proposed OP (linear)
CCCP-θ(num of candidates: 5)
CCCP-θ(num of candidates: 10)
CCCP-θ(num of candidates: 15)
OP-θ

10000

1000
Elapsed time [sec]

1000
Elapsed time [sec]

CCCP-s(num of candidates: 5)
CCCP-s(num of candidates: 10)
CCCP-s(num of candidates: 15)
OP-s

10000

100
10

100
10

1

1

0.1

0.1

0.01

0.01
D7

D6

D5

D4

D3

D2

D1

D10

D9

D8

D7

D6

D5

D4

D3

D2

D1

(b) Elapsed time for CCCP and proposed OP (RBF)

Figure 5. Elapsed time when the number of (θ, s)-candidates is increased. Changing the number of hyperparameter candidates affects
the computation time of CCCP, but not OP because the entire path of solutions is computed with the infinitesimal resolution.

Acknowledgments
The authors thank anonymous reviewers for their fruitful
comments. MS was supported by JST CREST program. IT
was also supported by JST CREST program, and MEXT
Kakenhi 26280083 and 26106513.

Outlier Path

References
Allgower, E. L. and George, K. Continuation and path following. Acta Numerica, 2:1–63, 1993.
Best, M. J. An algorithm for the solution of the parametric
quadratic programming problem. Applied Mathemetics
and Parallel Computing, pp. 57–76, 1996.

Masnadi-Shirazi, H. and Vasconcelos, N. On the design
of loss functions for classification: theory, robustness to
outliers, and savageboost. In Advances in Neural Information Processing Systems, volume 22, pp. 1049–1056,
2009.

Boyd, S. and Vandenberghe, L. Convex Optimization.
Cambridge University Press, 2004.

Mazumder, R., Friedman, J. H., and Hastie, T. Sparsenet:
coordinate descent with non-convex penalties. Journal
of the American Statistical Association, 106:1125–1138,
2011.

Cauwenberghs, G. and Poggio, T. Incremental and decremental support vector machine learning. In Advances in
Neural Information Processing Systems, volume 13, pp.
409–415. 2001.

Ogawa, K., Imamura, M., Takeuchi, I., and Sugiyama, M.
Infinitesimal annealing for training semi-supervised support vector machines. In Proceedings of the 30th International Conference on Machine Learning, 2013.

Collobert, R., Sinz, F., Weston, J., and Bottou, L. Trading
convexity for scalability. In Proceedings of the 23rd International Conference on Machine Learning, pp. 201–
208, 2006.

Ritter, K. On parametric linear and quadratic programming
problems. mathematical Programming: Proceedings of
the International Congress on Mathematical Programming, pp. 307–335, 1984.

DeCoste, D. and Wagstaff, K. Alpha seeding for support vector machines. In Proceeding of the Sixth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, 2000.

Shen, X., Tseng, G., Zhang, X., and Wong, W. H. On ψlearning. Journal of the American Statistical Association, 98(463):724–734, 2003.

Freund, Y.
A more robust boosting algorithm.
arXiv:0905.2138, 2009.
Gal, T. Postoptimal Analysis, Parametric Programming,
and Related Topics. Walter de Gruyter, 1995.
Giesen, J., Jaggi, M., and Laue, S. Approximating parameterized convex optimization problems. ACM Transactions on Algorithms, 9, 2012.
Hastie, T., Rosset, S., Tibshirani, R., and Zhu, J. The entire
regularization path for the support vector machine. Journal of Machine Learning Research, 5:1391–415, 2004.
Hromkovic, J. Algorithmics for Hard Problems. Springer,
2001.
Krause, N. and Singer, Y. Leveraging the margin more
carefully. In Proceedings of the 21st International Conference on Machhine Learning, pp. 63–70, 2004.

Vapnik, V. N. The Nature of Statistical Learning Theory.
Springer, 1996.
Wu, Y. and Liu, Y. Robust truncated hinge loss support
vector machines. Journal of the American Statistical Association, 102:974–983, 2007.
Xu, L., Crammer, K., and Schuurmans, D. Robust support vector machine traiing via convex outlier ablation.
In Proceedings of the National Conference on Artificial
Intelligence (AAAI), 2006.
Yu, Y., Yang, M., Xu, L., White, M., and Schuurmans, D.
Relaxed clipping: a global training method for robust
regression and classification. In Advances in Neural Information Processing Systems, volume 23, 2010.
Yuille, A. L. and Rangarajan, A. The concave-convex procedure (cccp). In Advances in Neural Information Processing Systems, volume 14, 2002.

Liu, Y. and Shen, X. Multicategory ψ-learning. Journal of
the American Statistical Association, 101:98, 2006.

Zhang, C. H. Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38:894–942,
2010.

Liu, Y., Shen, X., and Doss, H. Multicategory ψ-learning
and support vector machine: Computational tools. Journal of Computational and Graphical Statistics, 14:219–
236, 2005.

Zhou, H., Armagan, A., and Dunson, D. B. Path following
and empirical Bayes model selection for sparse regression. arXiv:1201.3528, 2012.

Masnadi-Shiraze, H. and Vasconcelos, N. Functional gradient techniques for combining hypotheses. In Advances
in Large Margin Classifiers, pp. 221–246. MIT Press,
2000.

