A Lower Bound for the Optimization of Finite Sums

Alekh Agarwal
Microsoft Research NYC, New York, NY.

ALEKHA @ MICROSOFT. COM

Léon Bottou
Facebook AI Research, New York, NY.

Abstract
This paper presents a lower bound for optimizing a finite sum of n functions, where each function is L-smooth and the sum is µ-strongly convex. We show that no algorithm can reach an error ε in minimizing all
p functions from this class
in fewer than Ω(n + n(κ − 1) log(1/ε)) iterations, where κ = L/µ is a surrogate condition
number. We then compare this lower bound to
upper bounds for recently developed methods
specializing to this setting. When the functions
involved in this sum are not arbitrary, but based
on i.i.d. random data, then we further contrast
these complexity results with those for optimal
first-order methods to directly optimize the sum.
The conclusion we draw is that a lot of caution is
necessary for an accurate comparison, and identify machine learning scenarios where the new
methods help computationally.

1. Introduction
Many machine learning setups lead to the minimization a
convex function of the form
x∗f = arg min f (x), with f (x) =
x∈X

µ
1 n
kxk2 + ∑ gi (x), (1)
2
n i=1

where X is a convex, compact set. When the functions
gi are also convex, then the overall optimization problem
is convex, and can in principle be solved using any offthe-shelf convex minimization procedure. In the machine
learning literature, two primary techniques have typically
been used to address such convex optimization problems.
The first approach (called the batch approach) uses the ability to evaluate the function f along with its gradients, Hessian etc. and applies first- and second-order methods to
Proceedings of the 32nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

LEON @ BOTTOU . ORG

minimize the objective. The second approach (called the
stochastic approach) interprets the average in Equation (1)
as an expectation and uses stochastic gradient methods,
randomly sampling a gi and using its gradient and Hessian
information as unbiased estimates for those of the function f .1 Both these classes of algorithms have extensive
literature on upper bounds for the complexities of specific
methods. More fundamentally, there are also lower bound
results on the minimum black-box complexity of the bestpossible algorithm to solve convex minimization problems.
In several broad problem classes, these lower bounds further coincide with the known upper bounds for specific
methods, yielding a rather comprehensive general theory.
However, a recent line of work in the machine learning literature, recognizes that the specific problem (1) of interest has additional structure beyond a general convex minimization problem. For instance, the average in defining the
function f is over a fixed number n of functions, whereas
typical complexity results on stochastic optimization allow
for the expectation to be with respect to a continuous random variable. Recent works (Le Roux et al., 2012; ShalevShwartz & Zhang, 2013; Johnson & Zhang, 2013) make
further assumptions that the functions gi involved in this
sum are smooth, and the function f is of course strongly
convex by construction. Under these conditions, the algorithms studied in these works have the following properties: (i) the cost of each iteration is identical to stochastic optimization methods, and (ii) the convergence rate of
the method is linear.2 The results are surprising since the
existing lower bounds on stochastic optimization dictate
that the error can decrease no faster than Ω(1/k) after k
iterations under such assumptions (Nemirovsky & Yudin,
1983), leaving an exponential gap compared to these new
results. It is of course not a contradiction due to the finite
1 There

is a body of literature that recognizes the ability of
stochastic optimization to minimize testing error rather than training error in machine learning contexts (see e.g. Bottou & Bousquet, 2008), but we will focus on training error for this paper.
2 An optimization algorithm is linearly convergent if it reduces
the sub-optimality by a constant factor at each iteration.

A Lower Bound for the Optimization of Finite Sums

sum structure of the problem (1) (following the terminology of Bertsekas (2012), we will call the setup of optimizing a finite sum incremental optimization hereafter).
Given this recent and highly interesting line of work, it
is natural to ask just how much better can one do in
this model of minimizing finite sums. Put another way,
can we specialize the existing lower bounds for stochastic or batch optimization, to yield results for this new family of functions. The aim of such a result would be to
understand the fundamental limits on any possible algorithm for this family of problems, and whether better algorithms are possible at all than the existing ones. Answering such questions is the goal of this work. To this
end, we define the Incremental First-order Oracle (IFO)
complexity model, where an algorithm picks an index
i ∈ {1, 2, . . . , n} and a point x ∈ X and the oracle returns
g0i (x). We consider the setting where each function gi is
L-smooth (that is, it has L-Lipschitz continuous gradients).
In this setting, we demonstrate that no method can achieve
kxK − x∗f k ≤ εkx∗f k for all functions f of the form (1), withp

out performing K = Ω n + n(L/µ − 1) log(1/ε) calls
to the IFO. As we will discuss following this main result,
this lower bound is not too far from upper bounds for IFO
methods such as SAG, SVRG and SAGA (Schmidt et al.,
2013; Johnson & Zhang, 2013; Defazio et al., 2014) whose
iteration complexity is O((n + L/µ) log(1/ε)). Some dual
coordinate methods such as ASDCA and SPDC (ShalevShwartz & Zhang, 2014; Zhang & Xiao, 2014) get even
closer to the lower bound, but are not IFO algorithms.
Overall, there is no method with a precisely matching upper bound on its complexity, meaning that there is further
room for improving either the upper or the lower bounds
for this class of problems.
Following the statement of our main result, we will also
discuss the implications of these lower bounds for the typical machine learning problems that have inspired this line
of work. In particular, we will demonstrate that caution is
needed in comparing the results between the standard firstorder and IFO complexity models, and worst-case guarantees in the IFO model might not adequately capture the performance of the resulting methods in typical machine learning settings. We will also demonstrate regimes in which
different IFO methods as well as standard first-order methods have their strengths and weaknesses.
Recent work of Arjevani (2014) also studies the problem of
lower bounds on smooth and strongly convex optimization
methods, although their development focuses on certain restricted subclasses of first-order methods (which includes
SDCA but not the accelerated variants, for instance). Discussion on the technical distinctions in the two works is
presented following our main result.
As a prerequisite for our result, we need the result on

black-box first-order complexity of minimizing smooth and
strongly convex functions. We provide a self-contained
proof of this result in our paper in Appendix A which might
be of independent interest. In fact, we establish a slight
variation on the original result, in order to help prove our
main result. Our main result will invoke this construction
multiple times to design each of the components gi in the
optimization problem (1).
The remainder of this paper is organized as follows. The
next section formally describes the complexity model and
the structural assumptions. We then state the main result,
followed by a discussion of consequences for typical machine learning problems. The proofs are deferred to the
subsequent section, with the more technical details in the
supplement.

2. Setup and main result
Let us begin by formally describing the class of functions
we will study in this paper. Recall that a function g is called
L-smooth, if it has L-Lipschitz continuous gradients, that is
∀ x, y ∈ X

kg0 (x) − g0 (y)k∗ ≤ Lkx − yk ,

where k · k∗ is the norm dual to k · k. In this paper, we
will only concern ourselves with scenarios where X is a
convex subset of a separable Hilbert space, with k · k being
the (self-dual) norm associated with the inner product. A
function g is called µ-strongly convex if
∀ x, y ∈ X



 µ
g(y) ≥ g(x) + g0 (x), y − x + kx − yk2 .
2

Given these definitions, we now define the family of functions being studied in this paper.
µ,L

Definition 1 Let Fn (Ω) denote the class of all convex
functions f with the form (1), where each gi is (L − µ)smooth and convex.
Note that f is µ-strongly convex and L-smooth by conµ,L
struction, and hence Fn (Ω) ⊆ S µ,L (Ω) where S µ,L (Ω)
is the set of all µ-strongly convex and L-smooth functions.
However, as we will see in the sequel, it can often be a
much smaller subset, particularly when the smoothness of
the global function is much better than that of the local
functions. We now define a natural oracle for optimization of functions with this structure, along with admissible
algorithms.
Definition 2 (Incremental First-order Oracle (IFO))
µ,L
For a function f ∈ Fn (Ω), the Incremental First-order
Oracle (IFO) takes as input a point x ∈ X and index
i ∈ {1, 2, . . . , n} and returns the pair (gi (x), g0i (x)).

A Lower Bound for the Optimization of Finite Sums

Definition 3 (IFO Algorithm) An optimization algorithm
is an IFO algorithm if its specification does not depend on
the cost function f other than through calls to an IFO.
For instance, a standard gradient algorithm would take the
current iterate xk , and invoke the IFO with (xk , i) in turn
with i = {1, 2, . . . , n}, in order to assemble the gradient of
f . A stochastic gradient algorithm would take the current
iterate xk along with a randomly chosen index i as inputs to
IFO. Most interesting to our work, the recent SAG, SVRG
and SAGA algorithms (Le Roux et al., 2012; Johnson &
Zhang, 2013; Defazio et al., 2014) are IFO algorithms. On
the other hand, dual coordinate ascent algorithms require
access to the gradients of the conjugate of fi , and therefore
are not IFO algorithms.
We now consider IFO algorithms that invoke the oracle K
times (at x0 , . . . , xK−1 ) and output an estimate xK of the
minimizer x∗f . Our goal is to bound the smallest number of queries K needed for any method to ensure an erµ,L
ror kxK − x∗f k ≤ εkx∗f k, uniformly for all f ∈ Fn (Ω).
This complexity result will depend on the ratio κ = L/µ
which is analogous to the condition number that usually appears in complexity bounds for the optimization of smooth
and strongly convex functions. Note that κ is strictly an
upper bound on the condition number of f , but also the
best one in general given the structural information about
µ,L
f ∈ Fn (Ω).
In order to demonstrate our lower bound, we will make
a specific choice of the problem domain X . Let `2 be
the Hilbert space of real sequences x = (x[i])∞
i=1 with finite
2
norm kxk2 = ∑∞
i=1 x[i] , and equipped with the standard inner product hx, yi = ∑∞
i=1 x[i]y[i]. We are now in a position
to state our main result over the complexity of optimization
µ,L
for the function class Fn (`2 ).
Theorem 1 Consider an IFO algorithm for problem (1)
that performs K ≥ 0 calls to the oracle and output a solution xK . Then, for any γ > 0, there exists a function
µ,L
f ∈ Fn (`2 ) such that kx∗f k = γ and
q
kx∗f

− xK k ≥ γq


and with t =

2t

0
K/n

with q = q

1 + κ−1
n −1
1+

κ−1
n

+1

,κ=

L
,
µ

if K < n.
otherwise.

In order to better interpret the result of the theorem, we
state the following direct corollary which lower bounds the
number of steps need to attain an accuracy of εkx∗f k.
Corollary 1 Consider an IFO algorithm for problem (1)
that guarantees kx∗f − xK k ≤ εkx∗f k for any ε < 1. Then

µ,L

there is a function f ∈ Fn (`2 )pon which the algorithm
must perform at least K = Ω(n+ n(κ − 1) log(1/ε)) IFO
calls.
The first term in the lower bound simply asserts that any
optimization method needs to make at least one query per
gi , in order to even see each component of f which is
clearly necessary. The second term, which is more important since it depends on the desired accuracy ε, asserts that the problem becomes harder as the number of
elements n in the sum increases or as the problem conditioning worsens. Again, both these behaviors are qualitatively expected. Indeed as n → ∞, the finite sum approaches
an integral, and the IFO becomes equivalent to a generic
stochastic-first order oracle for f , under the constraint that
the stochastic gradients are also Lipschitz continuous. Due
to Ω(1/ε) complexity of stochastic strongly-convex optimization (with no dependence on n), we do not expect the
linear convergence of Corollary 1 to be valid as n → ∞.
Also, we certainly expect the problem to get harder as the
ratio L/µ degrades. Indeed if all the functions gi were
identical, whence the IFO becomes equivalent to a standard
first-order oracle,
√ the optimization complexity similarly depends on Ω( κ − 1 log(1/ε)).
Whenever presented with a lower bound, it is natural to ask
how it compares with the upper bounds for existing methods. We now compare our lower bound to upper bounds
for standard optimization schemes for S µ,L (`2 ) as well as
µ,L
specialized ones for Fn (`2 ). We specialize to kx∗f k = 1
for this discussion.

Comparison with optimal gradient methods: As menµ,L
tioned before, Fn (`2 ) ⊆ S µ,L (`2 ), and hence standard
methods for optimization of smooth and strongly convex
objectives apply. These methods need n calls to the IFO
for getting the gradient of f , followed by an update. Using Nesterov’s optimal
√ gradient method (Nesterov, 2004),
one needs at most O( κ log(1/ε)) gradient evaluations to
reach ε-optimal
solution for f ∈ S µ,L (`2 ), resulting in at
√
most O(n κ log(1/ε)) calls to the IFO. Comparing with
√
our lower bound, there is a suboptimality of at most O( n)
in this result. Since this is also the best possible complexity
for minimizing a general f ∈ S µ,L (`2 ), we conclude that
there might indeed be room for improvement by exploiting the special structure here. Note that there is an important caveat in this comparison. For f of the form (1), the
smoothness constant for the overall function f might be
much smaller than L, and the strong convexity term might
be much higher than µ due to further contribution from the
gi . In such scenarios. the optimal gradient methods will
face a much smaller condition number κ in their complexity. This issue will be discussed in more detail in Section 3.

A Lower Bound for the Optimization of Finite Sums

Comparison with the best known algorithms: At least
three algorithms recently developed for problem setting (1)
offer complexity guarantees that are close to our lower
bound. SAG, SVRG and SAGA (Le Roux et al., 2012;
Johnson & Zhang, 2013; Defazio et al., 2014) all reach
an optimization error ε after less than O((n + κ) log(1/ε))
calls to the oracle. There are two differences from our
lower bound. The first term of n multiplies the log(1/ε)
term in the upper bounds, and the condition
number de√
pendence is O(κ) as opposed to O( nκ). This suggests
that there is room to either improve the lower bound, or
for algorithms with a better complexity. As observed earlier, the ASDCA and SPDC methods (Shalev-Shwartz &
Zhang, 2014; Zhang
p & Xiao, 2014) reach a closer upper
bound of O((n + n(κ − 1)) log(1/ε)), but these methods
are not IFO algorithms.
Room for better lower bounds? One natural question to
ask is whether there is a natural way to improve the lower
bound. As will become clear from the proof, a better lower
bound is not possible for the hard problem instance which
we construct. Indeed for the quadratic problem we construct, conjugate gradient descent can be used to solve the
problem with a nearly matching upper bound. Hence there
is no hope to improve the lower bounds without modifying
the construction.
It might appear odd that the lower bound is stated in the infinite dimensional space `2 . Indeed this is essential to rule
out methods such as conjugate gradient descent solving the
problem exactly in a finite number of iterations depending
on the dimension only (without scaling with ε). An alternative is to rule out such methods, which is precisely the
approach Arjevani (2014) takes. On the other hand, the resulting lower bounds here are substantially stronger, since
they apply to a broader class of methods. For instance, the
restriction to stationary methods in Arjevani (2014) makes
it difficult to allow any kind of adaptive sampling of the
component functions fi as the optimization progresses, in
addition to ruling out methods such as conjugate gradient.

3. Consequences for optimization in machine
learning
With all the relevant results in place now, we will compare
the efficiency of the different available methods in the context of solving typical machine learning problems. Recall
the definitions of the constants L and µ from before. In
general, the full objective f (1) has its own smoothness and
strong convexity constants, which need not be the same as
L and µ. To that end, we define L f to be the smoothness
constant of f , and µ f to the strong convexity of f . It is
immediately seen that L provides an upper bound on L f ,
while µ provides a lower bound on µ f .

Algorithm

Batch complexity

Adaptive?

ASDCA, SDPC
(Shalev-Shwartz &
Zhang, 2014)
(Zhang & Xiao, 2014)

SAG
(Schmidt et al., 2013)

AGM†
(Nesterov, 2007)

Õ

q



1
1 + L−µ
µn log ε

no




1 + µLf n log ε1

to µ f

Õ

Õ

q

Lf
µf

log ε1



to µ f , L f

Table 1. A comparison of the batch complexities of different
methods. A method is adaptive to µ f or L f , if it does not need
the knowledge of these parameters to run the algorithm and obtain the stated complexity upper bound. † Although the simplest
version of AGM does require the specification of µ f and L f , Nesterov also discusses an adaptive variant with the same bound up
to additional logarithmic factors.

In order to provide a meaningful comparison for incremental as well as batch methods, we follow Zhang & Xiao
(2014) and compare the methods in terms of their batch
complexity, that is, how many times one needs to perform
n calls to the IFO in order to ensure that the optimization
error for the function f is smaller than ε. When defining
batch complexity, Zhang & Xiao (2014) observed that the
incremental and batch methods have dependence on L versus L f , but did not consider the different strong convexities
that play a part for different algorithms. In this section,
we also include the dual coordinate methods in our comparison since they are computationally interesting for the
problem (1) even though they are not admissible in the IFO
model. Doing so, the batch complexities can be summarized as in Table 1.
Based on the table, we see two main points of difference.
First, the incremental methods rely on the smoothness of
the individual components. That this is unavoidable is
clear, since even the worst case lower bound of Theorem 1
depends on L and not L f . As Zhang & Xiao (2014) observe,
L f can in general be much smaller than L. They attempt to
address the problem to some extent by using non-uniform
sampling, thereby making sure that the best of the gi and
the worst of the gi have a similar smoothness constant under the reweighing. This does not fully bridge the gap between L and L f as we will show next. However, more striking is the difference in the lower curvature across methods.
To the best of our knowledge, all the existing analyses of
coordinate ascent require a clear isolation of strong convexity, as in the function definition (1). These methods then
rely on using µ as an estimate of the curvature of f , and
cannot adapt to any additional curvature when µ f is much
larger than µ. Our next example shows this can be a serious
concern for many machine learning problems.

A Lower Bound for the Optimization of Finite Sums

In order to simplify the following discussion we restrict
ourselves to perhaps the most basic machine learning optimization problem, the regularized least-squares regression:
µ
1 n
kxk2 + ∑ gi (x) with gi (x) = (hai , xi − bi )2 ,
2
n i=1
(2)
where ai is a data point and bi is a scalar target for prediction. It is then easy to see that g00i (x) = ai a>
i so that
µ,L
f ∈ Fn (Ω) with L = maxi (µ + kai k2 ). To simplify the
comparisons, assume that ai ∈ Rd are drawn independently
from a distribution defined on the sphere kai k = R. This ensures that L = µ + R2 . Since each function gi has the same
smoothness constant, the importance sampling techniques
of Zhang & Xiao (2014) cannot help.
f (x) =

In order to succinctly compare algorithms, we use the notation ΓALG to represent the batch complexity of ALG without the log(1/ε) term, which is common across all methods. Then we see that the upper bound for ΓASDCA is
s
r
κ −1
R2
= 1+
.
(3)
ΓASDCA = 1 +
n
µn

we obtain the following bounds on the eigenvalues of the
sample covariance matrix :

	
µ f ≥ max µ, µ + λmin − λmax max z, z2 ≥ µ+λ2 min ,

	
max )
,
L f ≤ min L, µ + λmax + λmax max z, z2 ≤ 3(µ+λ
2
Using these estimates in the bounds of Table 1 gives
ΓSAG
ΓAGM

L
2(µ + R2 )
= 1+
≤ 1+
= O(1) , (5)
µf n
n(µ + λmin )
s
p
p
Lf
≤ 3κ f = O( κ f ) .
(6)
=
µf

Table 2 compares the three methods under assumption (4)
depending on the growth of κ.

Let Σ = E[ai a>
i ] be the second moment matrix of the ai distribution. Let λmin and λmax be its lowest and highest eigenvalues. Let us define the condition number of the penalized
population objective
∆

κf =

µ + λmax
.
µ + λmin

Equation (5.26) in (Vershynin, 2012) then implies that there
are universal constants c and C such that the following inequality holds with probability 1 − δ :
r
r

d
log(2/δ )
2
kΣ− Σ̂k ≤ kΣk max z, z with z = c
+C
.
n
n
Let us weaken the above inequality slightly to use µ + kΣk
instead of kΣk in the bound, which is minor since we typically expect µ  λmax for statistical consistency. Then
assuming we have enough samples to ensure that
d
log(d/δ )
1
c2 +C2
≤ 2,
n
n
8κ f

(4)


1

ASDCA, SPDC (Eq. (3))

Õ log ε

SAG (Eq. (5))

Õ log ε1

AGM (Eq. (6))
In order to follow the development of Table 1 for SAG and
AGM, we need to evaluate the constants µ f and L f . Note
that in this special case, the constants L f and µ f are given
by the upper and lower eigenvalues respectively of the matrix µI + Σ̂, where Σ̂ = ∑ni=1 ai a>
i /n represents the empirical covariance matrix. In order to understand the scaling of
this empirical covariance matrix, we shall invoke standard
results on matrix concentration.

κ = O(n)

Algorithm

Õ

κ n
Õ

κ
n

log ε1

Õ log ε1




√
κ f log ε1

q

Õ






√
κ f log ε1

Table 2. A comparison of the batch complexities of different
methods for the regularized least squares objective (2) when the
number of examples is sufficiently large (4). Observe how the ASDCA complexity bound can be significantly worse than the SAG
complexity bound, despite its better worst case guarantee.

Problems with κ = O(n): This setting is quite interesting for machine learning, since it corresponds roughly to
using µ = O(n) when R2 is a constant. In this regime,
all the incremental methods seem to enjoy the best possible convergence rate of Õ(log(1/ε)). When the population
problem is relatively well conditioned, AGM obtains a similar complexity since κ f = O(1). However, for poorly conditioned problems, the population condition number might
scale with the dimension d. We conclude that there is indeed a benefit from using the incremental methods over the
batch methods in these settings, but it seems hard to distinguish between the complexities of accelerated methods
like ASDCA and SPDC compared with SAG or SVRG.
Problems with large κ: In this setting, the coordinate ascent methods seem to be at a disadvantage, because the average loss term provides additional strong convexity, which
is exploited by both SAG and AGM, but not by ASDCA or
SPDC methods. Indeed, we find that the complexity term
ΓASDCA can be made arbitrarily large as κi grows large.
However, the contraction factors for both SAG and AGM
do not grow with n in this setting, leading to a large gap

A Lower Bound for the Optimization of Finite Sums

between the complexities. Between SAG and AGM, we
conclude that SAG has a better bound when the population
problem is poorly conditioned.
High-dimensional settings (n/d  1) : In this setting,
the global strong convexity can not really be larger than µ
for the function (2), since the Hessian of the averaged loss
has a non-trivial null space. It would appear then, that SAG
is forced to use the same problem dependent constants as
ASDCA/SPDC, while AGM gets no added benefit in strong
convexity either. However, in such high-dimensional problems, one is often enforcing a low-dimensional structure in
machine learning settings for generalization. In such structures, the global Hessian matrix can still satisfy restricted
versions of strong convexity and smoothness conditions,
which are often sufficient for batch optimization methods
to succeed (Agarwal et al., 2012). In such situations, the
comparison might once again resemble that of Table 2, and
we leave such development to the reader.
In a nutshell, the superiority of incremental algorithms for
the optimization of training error in machine learning is
far more subtle than suggested by their worst case bounds.
Among the incremental algorithms, SAG has favorable
complexity results in all regimes despite the fact that both
ASDCA and SPDC offer better worst case bounds. This is
largely due to the adaptivity of SAG to the curvature of the
problem. This might also explain in some part the empirical observation of Schmidt et al. (2013), who find that on
some datasets SDCA (without acceleration) performed significantly poorly compared with other methods (see Figure
2 in their paper for details). Finally, we also observe that
SAG does indeed improve upon the complexity of AGM
after taking the different problem dependent constants into
account, when the population problem is ill-conditioned
and the data are appropriately bounded.
It is worth observing that all our comparisons are ignoring
constants, and in some cases logarithmic factors, which of
course play a role in the running time of the algorithms in
practice. Note also that the worst case bounds for the incremental methods account for the worst possible choice
of the n functions in the sum. Better results might be possible when they are based on i.i.d. random data. Such results
would be of great interest for machine learning.

4. Proof of main result
In this section, we provide the proof of Theorem 1. Our
high-level strategy is the following. We will first construct
the function f : `2 7→ R such that each gi acts on only the
projection of a point x onto a smaller basis, with the bases
being disjoint across the gi . Since the gi are separable, we
then demonstrate that optimization of f under an IFO is
equivalent to the optimization of each gi under a standard

first-order oracle. The functions gi will be constructed so
that they in turn are smooth and strongly convex with appropriate constants. Hence, we can invoke the known result
for the optimization of smooth and strongly convex objectives under a first-order oracle, obtaining a lower bound on
the complexity of optimizing f . We will now formalize this
intuitive sketch.
4.1. Construction of a separable objective
We start with a simple definition.
Definition 4 Let e1 , e2 , . . . denote the canonical basis vectors of `2 , and let Qi , i = 1 . . . n, denote the orthonormal
families Qi = [ ei , en+i , e2n+i , . . . , ekn+i , . . .] .
For ease of presentation, we also extend the transpose notation for matrices over operators in `2 in the natural manner
(to avoid stating adjoint operators each time).
Definition 5 Given a finite or countable orthonormal family S = [s1 , s2 , . . . ] ⊂ `2 and x ∈ `2 , let
∞

Sx =

∑ x[i] si

and S> x = (hsi , xi)∞
i=1 ,

i=1

where si is assumed to be zero when i is greater than the
size of the family.
Remark 1 Both S x and S> x are square integrable and
therefore belong to `2 .
Using the above notation, we first establish some simple
identities for the operators Qi defined above.
Lemma 1 Simple calculus yields the following identities:
n

n

i=1

i=1

>
2
> 2
2
Q>
i Qi = ∑ Qi Qi = I, and kQi xk = ∑ kQi xk = kxk .

Proof We start with the first claim. For any basis vector
e j , it is easily checked that Qi e j = e( j−1)n+i . By definition
>
of Q>
i , it further follows that Qi e( j−1)n+i = e j . Linearity
>
now yields Qi Qi x = x for any x ∈ `2 , giving the first claim.
For the second claim, we observe that Qi Q>
i e j = 0 unless
mod( j, n) = i, in which case Qi Q>
e
=
e
. This implies
j
j
i
the second claim. The third
 claim now
follows
from the

2 . Similarly
first one, since hQi x, Qi xi = x, Q>
Q
x
=
kxk
i
i
the final claim follows from the second claim.

We now define the family of separable functions that will
be used to establish our lower bound.
f (x) =

1 n
µ
0,L−µ
kxk2 + ∑ hi (Q>
(`2 ) (7)
i x) , hi (x) ∈ S
2
n i=1

A Lower Bound for the Optimization of Finite Sums
µ,L

Proposition 1 All functions (7) belong to Fn (`2 ).
Proof We simply need to prove that the functions
0,L−µ (` ). Using g0 (x) =
gi (x) = hi (Q>
2
i x) belong to S
i
0
>
Qi hi (Qi x) and Lemma 1, we can write kg0i (x) − g0i (y)k2 =


 2
0
>
0
>
2
Qi h0i (Q>
 = kh0i (Q>
i x) − hi (Qi y)
i x) − hi (Qi y)k ≤
2
>
2
2
2
(L − µ) kQi (x − y)k ≤ (L − µ) kx − yk .

4.2. Decoupling the optimization across components
We would like to assert that the separable structure of f
allows us to reason about optimizing its components separately. Since the hi are not strongly convex by themselves,
we first rewrite f as a sum of separated strongly convex
functions. Using Lemma 1,
1 n
µ
kxk2 + ∑ hi (Q>
i x)
2
n i=1

f (x) =
=
=

µ
2

n

1

n

∑ kQ>i xk2 + n ∑ hi (Q>i x)

i=1
n h

i=1

i
n
1
nµ > 2
∆ 1
fi (Q>
kQi xk + hi (Q>
∑
∑
i x) ,
i x) =
n i=1 2
n i=1

By construction, the functions fi belong to S nµ,L−µ+nµ and
are applied to disjoint subsets of the x coordinates. Therefore, when the function is known to have form (7), problem (1) can be written as
n

x∗ = ∑ Qi xi∗
i=1

xi∗ = arg min fi (x) .

(8)

x∈`2

Any algorithm that solves optimization problem (1) therefore implicitly solves all the problems listed in (8).
We are almost done, but for one minor detail. Note that we
want to obtain a lower bound where the IFO is invoked for
>
a pair (i, x) and responds with hi (Q>
i x) and ∂ hi (Qi x)/∂ x.
In order to claim that this suffices to optimize each fi separately, we need to argue that a first-order oracle for fi can be
obtained from this information, knowing solely the structure of f and not the functions hi . Since the strong convexity constant µ is assumed to be known to the algorithm, the
additional (nµ/2)kxk2 in defining fi is also known to the
algorithm. As a result, given an IFO for f , we can construct
a first-order oracle for any of the fi by simply returning
> 2
>
>
hi (Q>
i x) + (nµ/2)kQi xk and ∂ hi (Qi x)/∂ x + nµQi Qi x).
Furthermore, an IFO invoked with the index i reveals no
information about f j for any other j based on the separable nature of our problem. Hence, the IFO for f offers no
additional information beyond having a standard first-order
oracle for each fi .
4.3. Proof of Theorem 1
Based on the discussion above, we can pick any i ∈ {1 . . . n}
and view our algorithm as a complicated setup whose sole

purpose is to optimize function fi ∈ S nµ,L−µ+nµ . Indeed,
given the output xK of an algorithm using an IFO for the
function f , we can declare xKi = Q>
i xK as our estimate for
xi∗ . Lemma 1 then yields
n

n

i=1

i=1

∗ 2
i
∗ 2
kxK − x∗f k2 = ∑ kQ>
i (xK − x f )k = ∑ kxK − xi k .

In order to establish the theorem, we now invoke the classical result on the black-box optimization of functions using
a first-order oracle. The specific form of the result stated
here is proved in Appendix A.
Theorem 2 (Nemirovsky-Yudin) Consider a first order
black box optimization algorithm for problem (9) that performs K ≥ 0 calls to the oracle and returns an estimate
xK of the minimum. For any γ > 0, there exists a function
f ∈ S µ,L (`2 ) such that kx∗f k = γ and
√
κ −1
L
and κ = .
kx∗f − xK k ≥ γ q2K with q = √
µ
κ +1
At a high-level, our oracle will make an independent choice
of one of the functions that witness the lower bound in Theorem 2 for each fi . At a high-level, each function fi will
be chosen to be a quadratic with an appropriate covariance
structure such that Ki queries to the function fi result in the
estimation of at most Ki + 1 coordinates of xi∗ . By ensuring
that the remaining entries still have a substantial norm, a
lower bound for such functions is immediate. The precise
details on the construction of these functions can be found
in Appendix A.3
Suppose the IFO is invoked Ki times on each index i, with
K = K1 + K2 + . . . + Kn . We first establish the theorem for
the case K < n in which the algorithm cannot query each
functions fi at least once. After receiving the response xK ,
we are still free to arbitrarily choose fi for any index i that
was never queried. No non-trivial accuracy is possible in
this case.
Proposition 2 Consider an IFO algorithm that satisfies
the conditions of Theorem 1 with K < n. Then there is a
µ,L
function f ∈ Fn (`2 ) such that kx∗f − xK k ≥ γ.
Proof Let us execute the algorithm assuming that all the
fi are equal to the function f of Theorem 2 that attains
the lower bound with γ = 0. Since K < n, there is at
least one function f j for which K j = 0. Since the IFO
has not revealed anything about this function, we can construct function f by redefining function f j to ensure that
3 The main difference with the original result of Nemirovsky
and Yudin is the dependence on γ instead of kx0 − x∗f k. This is
quite convenient in our setting, since it eliminates any possible
interaction amongst the starting values of different coordinates for
the different functions fi .

A Lower Bound for the Optimization of Finite Sums

kxKj −x∗j k ≥ kx∗j k = γ j . Since x∗j is the only part of x∗ which
is non-zero, we also get γ j = γ.

We can now assume without loss of generality that Ki > 0
for each i. Appealing to Theorem 2 for each fi in turn,
kxK − x∗f k2 =

n

∑ kxKi − xi∗ k2 ≥

i=1

n

∑ γi2 q4Ki

i=1

n

n
2
2
γ2
= γ 2 ∑ i2 q4Ki ≥ γ 2 q∑i=1 γi 4Ki /γ ,
γ
i=1

where the last inequality results from Jensen’s inequality
applied to the convex function q4α for α ≥ 1. Finally, since
the oracle has no way to discriminate amongst
√ the γi values
when Ki > 0, it will end up setting γi = γ/ n. With this
setting, we now obtain the lower bound
kxK − x∗f k2 ≥ γ 2 q4K/n ,
for K > n, along with kxK − x∗f k2 ≥ γ 2 for K < n.
This completes the proof of the Theorem. In order to further establish Corollary 1, we need an additional technical
lemma.
√

x−1
−2
.
Lemma 2 ∀x > 1 , log √
> √
x+1
x−1
√ 
2
Proof The function φ (x) = log √x−1
is contin+ √x−1
x+1
uous and decreasing on (1, +∞) because

Of course, there is another and a possibly more important
aspect of optimization in machine learning which we do not
study in this paper. In typical machine learning problems,
the goal of optimization is not just to minimize the objective f —usually called the training error—to a numerical
precision. In most problems, we eventually want to reason
about test error, that is the accuracy of the predictions we
make on unseen data. There are existing results (Bottou &
Bousquet, 2008) which highlight the optimality of singlepass stochastic gradient optimization methods, when test
error and not training error is taken into consideration. So
far, we do not have any clear results comparing the efficacy
of methods designed for the problem (1) in minimizing test
error directly. We believe this is an important question for
future research, and one that will perhaps be most crucial
for the adoption of these methods in machine learning.
We believe that there are some important open questions
for future works in this area, which we will conclude with:
1. Is there a fundamental gap between the best IFO methods and the dual coordinate methods in the achievable
upper bounds? Or is there room to improve the upper bounds on the existing IFO methods. We certainly
found it tricky to do the latter in our own attempts.

1
1
√
√
√ −
φ 0 (x) = √
( x − 1)( x + 1) x (x − 1) x − 1
1
1
√
√ −
<0.
=
(x − 1) x (x − 1) x − 1
The result follows because limx→∞ φ (x) = 0.

find that the worst-case near-optimal methods like ASDCA
can often be much worse than other methods like SAG and
SVRG. However, IFO methods like SAG certainly improve
upon optimal first-order methods agnostic of the finite sum
structure, in ill-conditioned problems. In general, we observe that the problem dependent constants that appear in
different methods can be quite different, even though this
is not always recognized. We believe that accounting for
these opportunities might open door to more interesting algorithms and analysis.



Now we observe that we have at least n queries due to the
precondition ε < 1 and Proposition 2, which yields the first
term in the lower bound. Based on Theorem 1 and this
lemma, the corollary is now immediate.

5. Discussion
The results in this paper were motivated by recent results
and optimism on exploiting the structure of minimizing finite sums, a problem which routinely arises in machine
learning. Our main result provides a lower bound on the
limits of gains that might be possible in this setting, allowing us to do a more careful comparison of this setting
with regular first-order black box complexity results. As
discussed in Section 3, the results seem mixed when the
sum consists of n functions based on random data drawn
i.i.d. from a distribution. In this statistical setting, we

2. Is it possible to obtain better complexity upper bounds
when the n functions involved in the sum (1) are based
on random data, rather than being n arbitrary functions? Can the incremental methods exploit global
rather than local smoothness properties in this setting?
3. What are the test error properties of incremental methods for machine learning problems? Specifically, can
one do better than just adding up the optimization and
generalization errors, and follow a more direct approach as the stochastic optimization literature?

Acknowledgements
We would like to thank Lin Xiao, Sham Kakade and Rong
Ge for helpful discussions regarding the complexities of
various methods. We also thank the anonymous reviewer
who pointed out that the dual coordinate are not valid IFO
algorithms.

A Lower Bound for the Optimization of Finite Sums

References
Agarwal, Alekh, Negahban, Sahand, and Wainwright, Martin J. Fast global convergence of gradient methods for
high-dimensional statistical recovery. The Annals of
Statistics, 40(5):2452–2482, 2012.
Arjevani, Y. On Lower and Upper Bounds in Smooth
Strongly Convex Optimization - A Unified Approach via
Linear Iterative Methods. ArXiv e-prints, 2014.
Bertsekas, Dimitri P. Incremental gradient, subgradient,
and proximal methods for convex optimization: A survey. In Sra, S., Nowozin, S., and Wright, S. J. (eds.),
Optimization for Machine Learning, pp. 85–119. MIT
Press, 2012. Extended version: LIDS report LIDSP2848, MIT, 2010.
Bottou, Léon and Bousquet, Olivier.
The tradeoffs
of large scale learning. In Platt, J.C., Koller, D.,
Singer, Y., and Roweis, S. (eds.), Advances in Neural Information Processing Systems, volume 20, pp.
161–168. NIPS Foundation (http://books.nips.cc), 2008.
URL
http://leon.bottou.org/papers/
bottou-bousquet-2008.
Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
Saga: A fast incremental gradient method with support
for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, 2014.
Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. In
Burges, C.J.C., Bottou, L., Welling, M., Ghahramani, Z.,
and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 26, pp. 315–323. 2013.
Le Roux, Nicolas, Schmidt, Mark, and Bach, Francis. A
stochastic gradient method with an exponential convergence rate for finite training sets. In Pereira, F., Burges,
C.J.C., Bottou, L., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 25, pp.
2663–2671. 2012.
Nemirovsky, Arkadi and Yudin, David B. Problem Complexity and Method Efficiency in Optimization. Interscience Series in Discrete Mathematics. Wiley, 1983.
Nesterov, Yurii. Introductory Lectures on Convex Optimization. Kluwer Academic Publisher, 2004.
Nesterov, Yurii. Gradient methods for minimizing composite objective function, 2007.
Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Minimizing finite sums with the stochastic average gradient.
arXiv preprint arXiv:1309.2388, 2013.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual
coordinate ascent methods for regularized loss. The
Journal of Machine Learning Research, 14(1):567–599,
2013.
Shalev-Shwartz, Shai and Zhang, Tong. Accelerated proximal stochastic dual coordinate ascent for regularized loss
minimization. In Proceedings of the 31th International
Conference on Machine Learning, ICML 2014, Beijing,
China, 21-26 June 2014, pp. 64–72, 2014.
Vershynin, Roman. Introduction to the non-asymptotic
analysis of random matrices. In Eldar, Yonina C. and
Kutyniok, Gitta (eds.), Compressed Sensing, pp. 210–
268. Cambridge University Press, 2012.
Zhang, Yuchen and Xiao, Lin. Stochastic primal-dual coordinate method for regularized empirical risk minimization. Technical Report MSR-TR-2014-123, September
2014.

