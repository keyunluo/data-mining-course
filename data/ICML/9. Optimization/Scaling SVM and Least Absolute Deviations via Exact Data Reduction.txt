Scaling SVM and Least Absolute Deviations via Exact Data Reduction

Jie Wang
Arizona State University, Tempe, AZ 85287 USA

JIE . WANG . USTC @ ASU . EDU

Peter Wonka
King Abdullah University of Science and Technology, Thuwal, Saudi Arabia
Arizona State University, Tempe, AZ 85287 USA
Jieping Ye
Arizona State University, Tempe, AZ 85287 USA

Abstract
The support vector machine (SVM) is a widely
used method for classification. Although many
efforts have been devoted to develop efficient
solvers, it remains challenging to apply SVM to
large-scale problems. A nice property of SVM is
that the non-support vectors have no effect on the
resulting classifier. Motivated by this observation, we present fast and efficient screening rules
to discard non-support vectors by analyzing the
dual problem of SVM via variational inequalities
(DVI). As a result, the number of data instances
to be entered into the optimization can be substantially reduced. Some appealing features of
our screening method are: (1) DVI is safe in the
sense that the vectors discarded by DVI are guaranteed to be non-support vectors; (2) the data set
needs to be scanned only once to run the screening, and its computational cost is negligible compared to that of solving the SVM problem; (3)
DVI is independent of the solvers and can be integrated with any existing efficient solver. We
also show that the DVI technique can be extended to detect non-support vectors in the least absolute deviations regression (LAD). To the best
of our knowledge, there are currently no screening methods for LAD. We have evaluated DVI on
both synthetic and real data sets. Experiments indicate that DVI significantly outperforms the existing state-of-the-art screening rules for SVM,
and it is very effective in discarding non-support
vectors for LAD. The speedup gained by DVI
rules can be up to two orders of magnitude.
st

Proceedings of the 31 International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

PWONKA @ GMAIL . COM

JIEPING . YE @ ASU . EDU

1. Introduction
The support vector machine is one of the most popular classification tools in machine learning. Many efforts
have been devoted to developing efficient solvers for SVM
(Hastie et al., 2004; Joachims, 2006; Shalev-Shwartz et al.,
2007; Hsieh et al., 2008; Fan et al., 2008). However, the
applications of SVM to large-scale problems still pose significant challenges. To address this issue, one promising
approach is by â€œscreeningâ€. The key idea of screening is
motivated by a well-known feature of SVM; that is, the resulting classifier is determined only by the so-called â€œsupport vectorsâ€. If we first identify non-support vectors via
screening, and then remove them from the optimization,
we may experience substantial savings in computational
cost and memory. Another useful tool in machine learning and statistics is the least absolute deviations regression
(LAD) (Powell, 1984; Wang et al., 2006; Chen et al., 2008;
Rao et al., 2008) or the `1 method. When the protection
against outliers is a major concern, LAD provides a useful
and plausible alternative to the classical least squares or `2
method for linear regression. In this paper, we study both
SVM and LAD under a unified framework.
The idea of screening has been successfully applied to a
large class of `1 -regularized problems (El Ghaoui et al.,
2012; Xiang et al., 2011; Tibshirani et al., 2012; Wang
et al., 2013; Liu et al., 2014), including Lasso, `1 regularized logistic regression, elastic net, and more general convex problems. Those methods are able to discard
a large portion of â€œinactiveâ€ features which have 0 coefficients in the optimal solution, and the speedup can be several orders of magnitude.
Recently, Ogawa et al. (2013) proposed a â€œsafe screeningâ€
rule to identify non-support vectors for SVM; in this paper,
we refer to this method as SSNSV for convenience. We
notice that, the former approaches for `1 -regularized prob-

Scaling SVM and Least Absolute Deviations via Exact Data Reduction

lems aim to discard inactive â€œfeaturesâ€, while SSNSV is
used to identify non-support â€œvectorsâ€. This essential difference makes SSNSV a nontrivial extension of the existing feature-screening methods. Although there exist many
methods for data reduction for SVM (Achlioptas et al.,
2002; Yu et al., 2003; Cao & Boley, 2006), they are not
safe, in the sense that the resulting classification model may
be different. To the best of our knowledge, SSNSV is the
only existing safe screening method (Ogawa et al., 2013) to
identify non-support vectors for SVM. However, in order to
run the screening, SSNSV needs to determine an appropriate parameter value iteratively and an associated feasible
solution, which can be very time consuming.
In this paper, we develop novel efficient and effective
screening rules, called â€œDVIâ€, for a class of supervised
learning problems including SVM and LAD (Buchinsky,
1998; Jin et al., 2001). The proposed method, DVI, shares
the same advantage as SSNSV (Ogawa et al., 2013), that
is, both rules are safe in the sense that the discarded vectors
are guaranteed to be non-support vectors. The proposed DVI identifies the non-support vectors by estimating a lower
bound of the inner product between each vector and the optimal solution, which is unknown. The more accurate the
estimation is, the more non-support vectors can be detected. However, the estimation turns out to be non-trivial since
the optimal solution is not available. To overcome this difficulty, we propose a novel framework to estimate accurately
the optimal solution via the estimation of the â€œdual optimal
solutionâ€, as the primal and dual optimal solutions can be
related by the KKT conditions (GuÌˆler, 2010). Our main
technical contribution is to estimate the dual optimal solution via so-called â€œvariational inequalitiesâ€ (GuÌˆler, 2010).
Our experiments on both synthetic and real data demonstrate that DVI can identify far more non-support vectors
than can SSNSV. Moreover, by using the same technique,
that is, variational inequalities, we can improve SSNSV in
its ability to identify non-support vectors. Our results also
show that DVI is very effective in discarding non-support
vectors for LAD. The speedup gained by DVI rules can be
up to two orders of magnitude.
The rest of this paper is organized as follows. In Section
2, we study the SVM and LAD problems under a unified
framework. We then introduce our DVI rules in detail for
the general formulation in Sections 3 and 4. In Sections
5 and 6, we extend the DVI rules derived in Section 4 to
SVM and LAD respectively. In Section 7, we evaluate our
DVI rules for SVM and LAD using both synthetic and real
data. We conclude this paper in Section 8.
P
Notation: Throughout this paper, we use hx, yi = i xi yi
to denote the inner product of vectors x and y, and kxk2 =
hx, xi. For vector x, let [x]i be the ith component of x.
If M is a matrix, mi is the ith column of M and [M]i,j

is the (i, j)th entry of M. Given a scalar x, we denote
max{x, 0} by [x]+ . For the index set I := {1, . . . , l}, let
J := {j1 , . . . , jk } âŠ† I and J c := I \ J . For a vector x or a matrix M, let [x]J = ([x]j1 , . . . , [x]jk )T and
[M]J = (mj1 , . . . , mjk ). Moreover, let Î“0 (<n ) be the
class of proper and lower semicontinuous convex functions
from <n to (âˆ’âˆ, âˆ]. The conjugate of f âˆˆ Î“0 (<n ) is the
function f âˆ— âˆˆ Î“0 (<n ) given by
f âˆ— : <n â†’ (âˆ’âˆ, âˆ] : Î¸ 7â†’ sup xT Î¸ âˆ’ f (x).

(1)

xâˆˆ<n

The biconjugate of f âˆˆ Î“0 (<n ) is the function f âˆ—âˆ— âˆˆ
Î“0 (<n ) given by
f âˆ—âˆ— : <n â†’ (âˆ’âˆ, âˆ] : x 7â†’ sup xT Î¸ âˆ’ f âˆ— (Î¸).

(2)

Î¸âˆˆ<n

2. Basics and Motivation
In this section, we study the SVM and LAD problems under a unified framework. Then, we motivate the general
screening rules via the KKT conditions. Consider convex
optimization problems of the following form:
min

wâˆˆ<n

1
kwk2 + CÎ¦(w),
2

(3)

where Î¦ : <n â†’ < is a convex function but not necessarily differentiable and C > 0 is a regularization parameter.
Notice that the function Î¦ is generally referred to as the empirical loss. More specifically, suppose that we have a set
of observations, {xi , yi }li=1 , where xi âˆˆ <n and yi âˆˆ <
are the ith data instance and the corresponding response,
respectively. We focus on the following function class:
Î¦(w) =

Xl
i=1


Ï• wT (ai xi ) + bi yi ,

(4)

where Ï• : < â†’ <+ is a nonconstant continuous sublinear
function, and ai , bi are scalars. We provide the definition
of sublinear function as follows.
Definition 1. (Hiriart-Urruty & LemareÌchal, 1993) A
function Ïƒ : <n â†’ (âˆ’âˆ, âˆ] is said to be sublinear if
it is convex and positively homogeneous, i.e.,
Ïƒ(tx) = tÏƒ(x), âˆ€x âˆˆ <n and t > 0.

(5)

We will see that SVM and LAD are both special cases of
problem (3) in Sections 5.1 and 6, respectively. A nice
property of the function Ï• is that the biconjugate Ï•âˆ—âˆ— is
exactly Ï• itself, as stated in Lemma 1.
Lemma 1. For the function Ï• : < â†’ <+ which is continuous and sublinear, we have Ï• âˆˆ Î“0 (<) and thus Ï•âˆ—âˆ— = Ï•.
It is straightforward to check the statement in Lemma 1 by
verifying the requirements of the function class Î“0 (<). For

Scaling SVM and Least Absolute Deviations via Exact Data Reduction

self-completeness, we provide a proof in the supplement.
According to Lemma 1, problem (3) can be rewritten as

Let wâˆ— (C) and Î¸âˆ— (C) be the optimal solutions of (3) and
(11), respectively. Eq. (7) implies that

l
X
wâˆ— (C) = âˆ’CZT Î¸âˆ— (C).
(13)

1
min kwk2 + C
Ï•âˆ—âˆ— wT (ai xi ) + bi yi
(6)
w 2
i=1
The KKT conditions1 of problem (12) are

l 
X
ï£±


1
sup Î¸i wT (ai xi ) + bi yi âˆ’ Ï•âˆ— (Î¸i )
= min kwk2 + C
ï£´
if âˆ’ hwâˆ— (C), ai xi i < bi yi ;
ï£²Î²,
w 2
Î¸
âˆˆ<
i
i=1
Î¸âˆ— (C)]i âˆˆ [Î±, Î²], if âˆ’ hwâˆ— (C), ai xi i = bi yi ;
(14)
ï£´
l
ï£³
X
âˆ—



	
1
Î±,
if
âˆ’
hw
(C),
a
x
i
>
b
y
;
i i
i i
Î¸i wT (ai xi ) + bi yi âˆ’ Ï•âˆ— (Î¸i )
= sup min kwk2 + C
w 2
Î¸
i
=
1,
. . . , l.
i=1

= sup âˆ’C
Î¸

l
X

1
Ï•âˆ— (Î¸i ) + minn kwk2 + ChZw + yÌ„, Î¸i,
wâˆˆ< 2
i=1
T

âˆ‚`(w)
âˆ‚w

= 0, we have
wâˆ— = âˆ’CZT Î¸,

(7)

and thus
min `(w) = `(wâˆ— ) = âˆ’
w

C2 T 2
kZ Î¸k + ChyÌ„, Î¸i.
2

(8)

Hence, Eq. (6) becomes
sup âˆ’C
Î¸âˆˆ<l

Xl
i=1

Ï•âˆ— (Î¸i ) âˆ’

C2 T 2
kZ Î¸k + ChyÌ„, Î¸i.
2

(9)

Moreover, because Ï• âˆˆ Î“0 (<) is sublinear by Lemma 1,
we know that Ï•âˆ— is the indicator function for a closed convex set. In fact, we have the following result:
Lemma 2. For the nonconstant continuous sublinear function Ï• : < â†’ <+ , there exists a nonempty closed interval
IÏ• = [Î±, Î²] with Î±, Î² âˆˆ < and Î± < Î² such that
(
0, if t âˆˆ [Î±, Î²],
âˆ—
Ï• (t) := Î¹[Î±,Î²] =
(10)
âˆ, otherwise.
Let IÏ•l = [Î±, Î²]l . We can rewrite problem (9) as
sup âˆ’
l
Î¸âˆˆIÏ•

R = {i : âˆ’hwâˆ— (C), ai xi i > bi yi },

T

where Î¸ = (Î¸1 , . . . , Î¸l ) , Z = (ai xi , . . . , al xl ) and yÌ„ =
(b1 y1 , . . . , bl yl )T . Let `(w) := 12 kwk2 + ChZw + yÌ„, Î¸i.
The reason we can exchange the order of min and sup in
Eq. (6) is due to the strong duality of problem (3) (Boyd &
Vandenberghe, 2004).
By setting

For notational convenience, let

C2 T 2
kZ Î¸k + ChyÌ„, Î¸i.
2

E = {i : âˆ’hwâˆ— (C), ai xi i = bi yi },
L = {i : âˆ’hwâˆ— (C), ai xi i < bi yi }.
We call the vectors in set E the â€œsupport vectorsâ€. All the
other vectors in R and L are called â€œnon-support vectorsâ€.
The KKT conditions in (14) imply that, if some of the data
instances are known to be members of R and L, then the
corresponding components of Î¸âˆ— (C) can be set accordingly
and we only need the other components of Î¸âˆ— (C). More
precisely, we have the following result:
Lemma 3. Given index sets RÌ‚ âŠ† R and LÌ‚ âŠ† L, we have
1. [Î¸âˆ— (C)]RÌ‚ = Î± and [Î¸âˆ— (C)]LÌ‚ = Î².
S
2. Let SÌ‚ = RÌ‚ LÌ‚, |SÌ‚ c | be the cardinality of the set SÌ‚ c ,
GÌ‚11 = [ZT ]TSÌ‚ c [ZT ]SÌ‚ c , GÌ‚12 = [XT ]TSÌ‚ c [XT ]SÌ‚ and yÌ‚ =
ySÌ‚ c âˆ’ C GÌ‚12 [Î¸âˆ— (C)]SÌ‚ . Then, [Î¸âˆ— (C)]SÌ‚ c can be computed
by solving the following problem:
minc

Î¸Ì‚âˆˆ<|SÌ‚

|

c
C T
Î¸Ì‚ GÌ‚11 Î¸Ì‚ âˆ’ yÌ‚T Î¸Ì‚, s.t. Î¸Ì‚ âˆˆ [Î±, Î²]|SÌ‚ | .
2

(15)

Clearly, if |SÌ‚| is large compared to |I| = l, the computational cost for solving problem (15) can be much cheaper
than solving the full problem (12). To determine the membership of the data instances, Eq. (13) and (14) imply that
ChZT Î¸âˆ— (C), ai xi i > bi yi â‡’ [Î¸âˆ— (C)]i = Î± â‡” i âˆˆ R; (R1)
ChZT Î¸âˆ— (C), ai xi i < bi yi â‡’ [Î¸âˆ— (C)]i = Î² â‡” i âˆˆ L. (R2)

(11)

However, (R1) and (R2) are generally not applicable since
Î¸âˆ— (C) is unknown. To overcome this difficulty, we can
estimate a region Î˜ such that Î¸âˆ— (C) âˆˆ Î˜. As a result, we
obtain the relaxed version of (R1) and (R2):

Problem (11) is in fact the dual problem of (3). Moreover,
the â€œsupâ€ in problem (11) can be replaced by â€œmaxâ€ due to
ChZT Î¸, ai xi i > bi yi â‡’ [Î¸âˆ— (C)]i = Î± â‡” i âˆˆ R; (R10 )
the strong duality (Boyd & Vandenberghe, 2004) of prob- min
Î¸âˆˆÎ˜
lem (3). Since C > 0, problem (11) is equivalent to
max ChZT Î¸, ai xi i < bi yi â‡’ [Î¸âˆ— (C)]i = Î² â‡” i âˆˆ L. (R20 )
Î¸âˆˆÎ˜
C T 2
min kZ Î¸k âˆ’ hyÌ„, Î¸i.
(12)
l 2
Î¸âˆˆIÏ•
1
Please refer to the supplement for details.

Scaling SVM and Least Absolute Deviations via Exact Data Reduction

We note that (R10 ) and (R20 ) serve as the foundation of the
proposed DVI rules and the method in Ogawa et al. (2013).
In the subsequent sections, we first estimate the region Î˜,
which includes Î¸âˆ— (C), and then derive the screening rules
based on (R10 ) and (R20 ).
Method to solve problem (15) It is known that, problem
(15) can be efficiently solved by the dual coordinate descent method (Hsieh et al., 2008). Briefly speaking, the algorithm updates the components of Î¸Ì‚ one at a time, which
is equivalent to minimizing a 1D quadratic function over
a compact interval. For self-completeness, we provide a
more detailed review in the supplement.
In Section 3, we first give an accurate estimation of the
set Î˜, which includes Î¸âˆ— (C) as in (R10 ) and (R20 ) via the
variational inequalities. Then, in Section 4, we present the
novel DVI rules for problem (3) in detail.

3. Estimation of the Dual Optimal Solution
For problem (12), suppose that we are given two parameter
values 0 < C0 < C and that Î¸âˆ— (C0 ) is known. Then,
Theorem 5 shows that Î¸âˆ— (C) can be effectively bounded
in terms of Î¸âˆ— (C0 ). The main technique we use is the socalled variational inequality. For self-completeness, we cite
the definition of variational inequality as follows.
Theorem 4. (GuÌˆler, 2010) Let A âŠ† <n be a convex set,
and let h be a GaÌ‚teaux differentiable function on an open
set containing A. If xâˆ— is a local minimizer of h on A, then
hâˆ‡h(xâˆ— ), x âˆ’ xâˆ— i â‰¥ 0, âˆ€x âˆˆ A.

(16)

4. The Proposed DVI Rules
Given C > C0 > 0 and Î¸âˆ— (C0 ), we can estimate Î¸âˆ— (C) via
Theorem 5. Combining (R10 ), (R20 ) and Theorem (5), we
develop the basic screening rule for problem (3) as summarized in the following theorem:
Theorem 6. (DVI) For problem (12), suppose that we are
given Î¸âˆ— (C0 ). Then, for any C > C0 , we have [Î¸âˆ— (C)]i =
Î±, i.e., i âˆˆ R, if the following holds:
T âˆ—
C+C0
2 hZ Î¸ (C0 ), ai xi i

âˆ’

T âˆ—
Câˆ’C0
2 kZ Î¸ (C0 )kkai xi k

> bi yi .

Similarly, we have [Î¸âˆ— (C)]i = Î², i.e., i âˆˆ L, if
T âˆ—
C+C0
2 hZ Î¸ (C0 ), ai xi i

+

T âˆ—
Câˆ’C0
2 kZ Î¸ (C0 )kkai xi k

< bi yi .

The supplement includes the proof of Theorem 6. In real
applications, the optimal parameter value of C is unknown
and we need to estimate it. Commonly used model selection strategies such as cross validation and stability selection need to solve the optimization problems over a grid of
turning parameters 0 < C1 < C2 < . . . < CK to determine an appropriate value for C. This procedure is usually
very time consuming, especially for large scale problems.
To this end, we propose a sequential version of the proposed DVI below.
Corollary 7. (DVIâˆ—s ) For problem (12), suppose that we
are given a sequence of parameters 0 < C1 < C2 < . . . <
CK . Assume that Î¸âˆ— (Ck ) is known for an arbitrary integer
1 â‰¤ k < K. Then, for Ck+1 , we have [Î¸âˆ— (Ck+1 )]i = Î±,
i.e., i âˆˆ R, if the following holds:

Via the variational inequality, the following theorem shows Ck+12+Ck hZT Î¸âˆ— (Ck ), ai xi i âˆ’ Ck+12âˆ’Ck kZT Î¸âˆ— (Ck )kkai xi k > bi yi .
that Î¸âˆ— (C) can estimated in terms of Î¸âˆ— (C0 ).
Similarly, we have [Î¸âˆ— (Ck+1 )]i = Î², i.e., i âˆˆ L, if
Theorem 5. For problem (12), let C > C0 > 0. Then
kZT Î¸âˆ— (C) âˆ’

C0 +C T âˆ—
2C Z Î¸ (C0 )k

â‰¤

T âˆ—
Câˆ’C0
2C kZ Î¸ (C0 )k.

Proof. Let g(Î¸) be the objective function of problem (12).
The variational inequality implies that
hâˆ‡g(Î¸âˆ— (C0 )), Î¸ âˆ’ Î¸âˆ— (C0 )i â‰¥ 0, âˆ€Î¸ âˆˆ [Î±, Î²]l ;

(17)

hâˆ‡g(Î¸âˆ— (C)), Î¸ âˆ’ Î¸âˆ— (C)i â‰¥ 0, âˆ€Î¸ âˆˆ [Î±, Î²]l .

(18)

Notice that âˆ‡g(Î¸) = CZZT Î¸ âˆ’ yÌ„, and Î¸âˆ— (C0 ) âˆˆ
[Î±, Î²]l and Î¸âˆ— (C) âˆˆ [Î±, Î²]l . Plugging âˆ‡g(Î¸âˆ— (C)) and
âˆ‡g(Î¸âˆ— (C0 )) into (17) and (18) leads to
hC0 ZZT Î¸âˆ— (C0 ) âˆ’ yÌ„, Î¸âˆ— (C) âˆ’ Î¸âˆ— (C0 )i â‰¥ 0;

(19)

hCZZT Î¸âˆ— (C) âˆ’ yÌ„, Î¸âˆ— (C0 ) âˆ’ Î¸âˆ— (C)i â‰¥ 0.

(20)

We can see that the inequality in (20) is equivalent to
hyÌ„ âˆ’ CZZT Î¸âˆ— (C), Î¸âˆ— (C) âˆ’ Î¸âˆ— (C0 )i â‰¥ 0.

(21)

Then the statement follows by adding the inequalities in
(19) and (21) together.

Ck+1 +Ck
hZT Î¸âˆ— (Ck ), ai xi i
2

+

Ck+1 âˆ’Ck
kZT Î¸âˆ— (Ck )kkai xi k
2

< bi yi .

The main computational cost of DVIâˆ—s is due to the evaluation of hZT Î¸âˆ— (Ck ), ai xi i, kZT Î¸âˆ— (Ck )k and kai xi k. Let
G = ZZT . It is easy to see that
hZT Î¸âˆ— (Ck ), ai xi i = giT Î¸âˆ— (Ck ),
kZT Î¸âˆ— (Ck )k2 = Î¸âˆ— (Ck )T GÎ¸âˆ— (Ck ),
kxÌ„i k2 = [G]i,i ,
where gi is the ith column of G. Since G is independent
of Ck , it can be computed only once and thus the computational cost of DVIâˆ—s reduces to O(l2 ) to scan the entire data
set. Indeed, based on Eq. (13), we can reconstruct DVI
rules without the explicit computation of G.
Corollary 8. (DVIs ) For problem (3), suppose that we are
given a sequence of parameters 0 < C1 < C2 < . . . <
CK . Assume that wâˆ— (Ck ) is known for an arbitrary integer

Scaling SVM and Least Absolute Deviations via Exact Data Reduction

1 â‰¤ k < K. Then, for Ck+1 , we have [Î¸âˆ— (Ck+1 )]i = Î±,
i.e., i âˆˆ R, if the following holds:

5.2. Improving the existing method

We briefly describe how to improve SSNSV (Ogawa et al.,
> bi yi . 2013) by using the same technique used in DVI rules (refer
to the supplement for more details). In view of Eq. (13),
Similarly, we have [Î¸âˆ— (Ck+1 )]i = Î², i.e., i âˆˆ L, if
(R10 ) and (R20 ) can be rewritten as:
âˆ’Ck
k+1
âˆ’ Ck +C
hwâˆ— (Ck ), ai xi i + Ck+1
kwâˆ— (Ck )kkai xi k < bi yi .
2Ck
2Ck
min hw, xÌ„i i > 1 â‡’ [Î¸âˆ— (C)]i = 0 â‡” i âˆˆ R,
(R100 )
k+1
hwâˆ— (Ck ), ai xi i âˆ’
âˆ’ Ck +C
2Ck

Ck+1 âˆ’Ck
kwâˆ— (Ck )kkai xi k
2Ck

wâˆˆâ„¦

5. Screening Rules for SVM

maxhw, xÌ„i i < 1 â‡’ [Î¸âˆ— (C)]i = 1 â‡” i âˆˆ L,

In Section 5.1, we first present the sequential DVI rules for
SVM based on the results in Section 4. Then, in Section
5.2, we show how to improve SSNSV (Ogawa et al., 2013)
by the same technique used in DVI.
5.1. DVI rules for SVM
Given a set of observations {xi , yi }li=1 , where xi and
yi âˆˆ {1, âˆ’1} are the ith data instance and the corresponding class label, the SVM takes the form of:
Xl 

1
1 âˆ’ wT (yi xi ) + .
(22)
min kwk2 + C
i=1
w 2
It is easy to see that, if we set Ï•(t) = [t]+ and âˆ’ai = bi =
yi , problem (3) becomes the SVM problem. To construct
the DVI rules for SVM by Corollaries 7 and 8, we only
need to find Î± and Î². In fact, we have the following result:
Lemma 9. Let Ï•(t) = [t]+ , then Î± = 0 and Î² = 1, i.e.,
Ï•âˆ— (s) = Î¹[0,1] .

(23)

We omit the proof of Lemma 9 since it is a direct application of Eq. (1). Then, we immediately have the following
screening rules for the SVM problem. (For notational convenience, let xÌ„i = yi xi and X = (xÌ„1 , . . . , xÌ„l )T .)
Corollary 10. (DVIâˆ—s for SVM) For problem (22), suppose
that we are given a sequence of parameters 0 < C1 <
C2 < . . . < CK . Assume that Î¸âˆ— (Ck ) is known for an
arbitrary integer 1 â‰¤ k < K. Then, for Ck+1 , we have
[Î¸âˆ— (Ck+1 )]i = 0, i.e., i âˆˆ R, if the following holds:
T
Ck+1 +Ck
hX Î¸âˆ— (Ck ), xÌ„i i
2

âˆ’

T
Ck+1 âˆ’Ck
kX Î¸âˆ— (Ck )kkxÌ„i k
2

> 1.

âˆ—

Similarly, we have [Î¸ (Ck+1 )]i = 1, i.e., i âˆˆ L, if
T
Ck+1 +Ck
hX Î¸âˆ— (Ck ), xÌ„i i
2

+

T
Ck+1 âˆ’Ck
kX Î¸âˆ— (Ck )kkxÌ„i k
2

< 1.

Corollary 11. (DVIs for SVM) For problem (22), suppose
that we are given a sequence of parameters 0 < C1 <
C2 < . . . < CK . Assume that wâˆ— (Ck ) is known for an
arbitrary integer 1 â‰¤ k < K. Then, for Ck+1 , we have
[Î¸âˆ— (Ck+1 )]i = 0, i.e., i âˆˆ R, if the following holds:
Ck +Ck+1
hwâˆ— (Ck ), xÌ„i i
2Ck

âˆ’

Ck+1 âˆ’Ck
kwâˆ— (Ck )kkxÌ„i k
2Ck

> 1.

âˆ—

Similarly, we have [Î¸ (Ck+1 )]i = 1, i.e., i âˆˆ L, if
Ck +Ck+1
hwâˆ— (Ck ), xÌ„i i
2Ck

+

Ck+1 âˆ’Ck
kwâˆ— (Ck )kkxÌ„i k
2Ck

wâˆˆâ„¦

(R200 )

where â„¦ is a set that includes wâˆ— (C) (we have already set
âˆ’ai = bi = yi , Î± = 0 and Î² = 1). It is easy to see that the
smaller â„¦ is, the tighter the bounds are in (R100 ) and (R200 ).
Thus, more data instancesâ€™ membership can be identified.
Estimation of wâˆ— in SSNSV Ogawa et al. (2013) consider
the following equivalent formulation of SVM:
Xl
1
[1 âˆ’ yi wT xi ]+ â‰¤ s.
(24)
min kwk2 , s.t.
i=1
w 2
Pl
T
Let Fs = {w :
i=1 [1 âˆ’ yi w xi ]+ â‰¤ s}. Suppose
that we have two scalars sa > sb > 0, and Fsb 6= âˆ…,
wÌ‚(sb ) âˆˆ Fsb . Then, for s âˆˆ [sb , sa ], wâˆ— (s) is inside the
following region:


hwâˆ— (sa ), w âˆ’ wâˆ— (sa )i â‰¥ 0,
â„¦[sb ,sa ] := w :
(25)
kwk2 â‰¤ kwÌ‚(sb )k2 .
Estimation of wâˆ— via VI By using the same technique as in
DVI, we can see that wâˆ— (s) is inside the following region:


hwâˆ— (sa ), w âˆ’ wâˆ— (sa )i â‰¥ 0,
(26)
â„¦0[sb ,sa ] := w :
kw âˆ’ 21 wÌ‚(sb )k â‰¤ 12 kwÌ‚(sb )k.
We can see that â„¦0[sb ,sa ] âŠ‚ â„¦[sb ,sa ] , and thus SSNSV can
be improved by the estimation in (26). The rule based on
â„¦0 [sb , sa ] is presented in Theorem 18 in the supplement,
which is called the â€œenhancedâ€ SSNSV (ESSNSV).

6. Screening Rules for LAD
In this section, we extend DVI rules in Section 4 to the
least absolute deviations regression (LAD). Suppose that
we have a training set {xi , yi }li=1 , where xi âˆˆ <n and
yi âˆˆ <. The LAD problem takes the form of
Xl
1
min kwk2 + C
|yi âˆ’ wT xi |.
i=1
w 2

(27)

We can see that, if we set Ï•(t) = |t| and âˆ’ai = bi = 1,
problem (3) becomes the LAD problem. To construct the
DVI rules for LAD based on Corollaries 7 and 8, we need
to find Î± and Î². Indeed, we have the following result:
Lemma 12. Let Ï•(t) = |t|, then Î± = âˆ’1 and Î² = 1, i.e.,

< 1.

Ï•âˆ— (s) = Î¹[âˆ’1,1] .

(28)

Scaling SVM and Least Absolute Deviations via Exact Data Reduction

We again omit the proof of Lemma 12 because it is a direct
application of Eq. (1). Then, it is straightforward to derive
the sequential DVI rules for the LAD problem.
Corollary 13. (DVIâˆ—s for LAD) For problem (27), suppose that we are given a sequence of parameter values
0 < C1 < C2 < . . . < CK . Assume that Î¸âˆ— (Ck ) is known
for an arbitrary integer 1 â‰¤ k < K. Then, for Ck+1 , we
have [Î¸âˆ— (Ck+1 )]i = âˆ’1 or 1, i.e., i âˆˆ R or i âˆˆ L, if the
following holds, respectively:

in largely overlapping classes. We evaluate DVIs rules on
three synthetic data sets, i.e., Toy1, Toy2 and Toy3, plotted in the first row of Fig. 1. For each data set, we generate two classes. Each class has 1000 data points and is
generated from N ({Âµ, Âµ}T , 0.752 I), where I âˆˆ <2Ã—2 is
the identity matrix. For the positive classes (the red dots),
Âµ = 1.5, 0.75, 0.5, for Toy1, Toy2 and Toy 3, respectively;
and Âµ = âˆ’1.5, âˆ’0.75, âˆ’0.5, for the negative classes (the
blue dots). From the plots, we can observe that when |Âµ|
decreases, the two classes increasingly overlap and thus the
1. Ck+12+Ck hXT Î¸âˆ— (Ck ), xi i âˆ’ Ck+12âˆ’Ck kXT Î¸âˆ— (Ck )kkxi k > yi . number of data instances belonging to set L increases.
2. Ck+12+Ck hXT Î¸âˆ— (Ck ), xi i + Ck+12âˆ’Ck kXT Î¸âˆ— (Ck )kkxi k < yi .
Corollary 14. (DVIs for LAD) For problem (27), suppose that we are given a sequence of parameter values
0 < C1 < C2 < . . . < CK . Assume that wâˆ— (Ck ) is known
for an arbitrary integer 1 â‰¤ k < K. Then, for Ck+1 , we
have [Î¸âˆ— (Ck+1 )]i = âˆ’1 or 1, i.e., i âˆˆ R or i âˆˆ L, if the
following holds, respectively:
1.
2.

Ck+1 +Ck
hwâˆ— (Ck ), xi i
2Ck
Ck+1 +Ck
hwâˆ— (Ck ), xi i
2Ck

âˆ’
+

Ck+1 âˆ’Ck
kwâˆ— (Ck )kkxi k
2Ck
Ck+1 âˆ’Ck
kwâˆ— (Ck )kkxi k
2Ck

> yi ,
< yi .

To the best of our knowledge, ours are the first screening
rules for LAD.

7. Experiments
We evaluate DVI rules on both synthetic and real data sets.
To measure the performance of the screening rules, we
compute the rejection rates, i.e., the ratio of the number of
data instances whose membership can be identified by the
rules to the total number of data instances. We test the rules
along a sequence of 100 parameters of C âˆˆ [10âˆ’2 , 10] equally spaced in the logarithmic scale. In Section 7.1, we
compare the performance of DVI rules with SSNSV (Ogawa et al., 2013), which is the only existing method for
identifying non-support vectors in SVM. We note that both
DVI rules and SSNSV are safe in the sense that no support
vectors will be mistakenly discarded. We then evaluate DVI rules for LAD in Section 7.2.
7.1. DVI for SVM
In this experiment, we first apply DVIs to three simple 2D
synthetic data sets to illustrate the effectiveness of the proposed screening methods. Then, we compare the performance of DVIs , SSNSV and ESSNSV using (a) the IJCNN1 data set (Prokhorov, 2001); (b) the Wine Quality data
set (Cortez et al., 2009); and (c) the Forest Covertype data
set (Hettich & Bay, 1999). The original Forest Covertype
data set includes seven classes. We randomly pick two of
the seven classes to construct the data set used in this paper.
Synthetic Data Sets In this experiment, we show that DVIs
are very effective in discarding non-support vectors even

Table 1. Running time (in seconds) for solving SVM problems
with 100 parameter values by (a) â€œSolverâ€ (solver without screening); (b) â€œSolver+DVIs â€ (solver combined with DVIs ). â€œDVIs â€
is the total running time (in seconds) of the rule. â€œInit.â€ is the
running time to solve SVM with the smallest parameter value.

Toy1
Toy2
Toy3

Solver

Solver+DVIs

DVIs

Init.

Speedup

11.83
13.68
15.35

0.20
0.52
0.61

0.02
0.03
0.03

0.12
0.15
0.16

59.15
26.31
25.16

The second row of Fig. 1 presents the stacked area charte and Le be
s of the rejection rates. For convenience, let R
the indices of data instances that are identified by DVIs as
members of R and L, respectively. Then, the blue and red
e and |L|/l
e (recall that l is
regions present the ratios of |R|/l
the number of data instances, which is 2000 for this experiment). We can see that, for Toy1, the two classes are clearly
apart from each other and thus most of the data instances
belong to set R. The first chart in the second row of Fig. 1
indicates that the proposed DVIs can identify almost all of
the non-support vectors and thus the speedup is almost 60
times compared to the solver without screening (please refer to Table 1). When the two classes have a large overlap,
e.g., Toy3, the number of data instances in L significantly
increases. This will generally impose a great challenge on
the solver. But even for this challenging case, DVIs is still
able to identify a large portion of the non-support vectors
as indicated by the last charts in the second row of Fig. 1.
e is comparable to |R|.
e Table 1
Notice that, for Toy3, |L|
shows that the speedup gained by DVIs is about 25 times
for this challenging case. It is worthwhile to mention that
the running time of â€œSolver+DVIs â€ in Table 1 includes the
running time (the 5th column of Table 1) for solving SVM
with the smallest parameter value.
Real Data Sets In this experiment, we compare the performance of SSNSV, ESSNSV and DVIs in terms of the
rejection ratio; that is, the ratio of the number of data instances identified as members of R or L by the screening
rules to the number of total data instances. Fig. 2 shows
the rejection ratios of the three methods on three real data sets. We can observe that DVIs rules identify far more

4

2

2

2

0

âˆ’2

0

âˆ’2

âˆ’2

0
x1

2

âˆ’4
âˆ’4

4

0

âˆ’2

âˆ’2

0
x1

2

4

âˆ’4
âˆ’4

1
Ratio of Detected NSVs

1
0.8
0.6
0.4
0.2
0

0.1

1

C

0.8
0.6
0.4
0.2
0

10

(a) Toy1

âˆ’2

0
x1

2

4

1
Ratio of Detected NSVs

âˆ’4
âˆ’4

Ratio of Detected NSVs

x2

4

x2

x2

Scaling SVM and Least Absolute Deviations via Exact Data Reduction
4

0.1

1

C

0.8
0.6
0.4
0.2
0

10

0.1

(b) Toy2

1

C

10

(c) Toy3

Figure 1. DVIs for three 2D synthetic data sets. The first row shows the plots of the data. Cyan and magenta dotted lines are the resulting
decision functions at C = 10âˆ’2 and C = 10, respectively. The second row presents the rejection rates of DVIs with the given 100
parameter values.

0.8
0.6
0.4
SSNSV
ESSNSV
DVIs

0.2
0

0.1

C

1

0.8
0.6
0.4
SSNSV
ESSNSV
DVIs

0.2
0

10

1

(a) IJCNN1

0.1

1

C

Ratio of Detected NSVs

1
Ratio of Detected NSVs

Ratio of Detected NSVs

1

0.8
0.6
0.4
SSNSV
ESSNSV
DVIs

0.2
0

10

(b) Wine

0.1

C

1

10

(c) Forest Covertype

Figure 2. Comparison of the performance of SSNSV, ESSNSV and DVIs for SVM on three real data sets.

0.8
0.6
0.4
0.2

1
Ratio of Detected NSVs

1
Ratio of Detected NSVs

Ratio of Detected NSVs

1

0.8
0.6
0.4
0.2

DVI
0

s

0.1

C

1

10

(a) Magic Gamma Telescope

0.8
0.6
0.4
0.2

DVI
0

s

0.1

C

1

10

DVI
0

(b) Computer

s

0.1

C

1

10

(c) Houses

Figure 3. Rejection ratio of DVIs for LAD on three real data sets.

non-support vectors than do SSNSV and ESSNSV. For IJCNN1, about 80% of the data instances are identified as
non-support vectors by DVIs . Therefore, as indicated by
Table 2 the speedup gained by DVIs is about 5 times. For
the Wine data set, more than 80% of the data instances
are identified to belong to R or L by DVIs . As indicated in Table 2, the speedup is about six times that gained by

DVIs . For the Forest Covertype data set, almost all of data
instancesâ€™ membership can be determined by DVIs . Table 2 shows that the speedup gained by DVIs is almost 80
times faster, which is much higher than that of SSNSV and
ESSNSV. Moreover, Fig. 2 indicates that ESSNSV is more
effective in identifying non-support vectors than SSNSV,
which is consistent with our analysis.

Scaling SVM and Least Absolute Deviations via Exact Data Reduction
Table 2. Running time (in seconds) for solving the SVM problems along the 100 parameter values on three real data sets. In
â€œSolver+SSNSVâ€ and â€œSolver+ESSNSVâ€, â€œInit.â€ reports the running time for solving SVM at the smallest and the largest parameter values since they are required to run SSNSV and ESSNSV. In
â€œSolver+DVIs â€, â€œInit.â€ reports the running time for solving SVM
at the smallest parameter value that is sufficient to run DVIs . The
running time reported by Init. is included in the total running time
of the solver equipped with the screening methods.

Magic Gamma Telescope (l = 19020, n = 10)
Solver
Total
122.34
DVIs
0.28
Solver+DVIs
Init.
0.12
Total
12.41

Speedup
-

Speedup
-

3.01

Computer (l = 8192, n = 12)
Solver
Total
5.38
DVIs
0.08
Solver+DVIs
Init.
0.05
Total
0.28

5.64

Solver

Houses (l = 20640, n = 8)
Total
21.43
DVIs
0.06
Solver+DVIs
Init.
0.10
Total
0.19

Speedup
-

IJCNN1 (l = 49990, n = 22)
Solver
Total
4669.14
SSNSV
2.08
Solver+SSNSV
Init.
92.45
Total
2018.55
ESSNSV
2.09
Solver+ESSNSV
Init.
91.33
Total
1552.72
DVIs
0.99
Solver+DVIs
Init.
42.67
Total
828.02

Speedup
-

Wine (l = 6497, n = 12)
Solver
Total
SSNSV
Solver+SSNSV
Init.
Total
ESSNSV
Solver+ESSNSV
Init.
Total
DVIs
Solver+DVIs
Init.
Total

Speedup
-

76.52
0.02
1.56
21.85
0.03
1.60
17.17
0.01
0.67
11.62

Forest Covertype (l = 37877, n = 54)
Solver
Total
1675.46
SSNSV
2.73
Solver+SSNSV
Init.
35.52
Total
220.58
ESSNSV
2.89
Solver+ESSNSV
Init.
36.13
Total
156.23
DVIs
1.27
Solver+DVIs
Init.
12.57
Total
21.16

Table 3. Running time (in seconds) for solving the LAD problems
with the given 100 parameter values on three real data sets. In
â€œSolver+DVIs â€, â€œInit.â€ reports the running time for solving LAD
at the smallest parameter value which is required to run DVIs .
Init. is included in the total running time of Solver+DVIs .

2.31

3.50

4.47

6.59

Speedup
7.60

10.72

79.18

7.2. DVI for LAD
We evaluate the performance of DVIs for LAD on three real
data sets: (a) Magic Gamma Telescope data set (Bache &
Lichman, 2013); (b) Computer data set (Rasmussen et al.);
(c) Houses data set (Pace & Barry, 1997). Fig. 3 shows
the rejection ratio of DVIs rules for the three data sets. We
can observe that the rejection ratio of DVIs on the Magic
Gamma Telescope data set is about 90%, leading to a ten-

9.86

19.21

114.91

fold speedup as indicated in Table 3. For the Computer and
Houses data sets, the rejection rates are very close to 100%;
i.e., almost all of the data instancesâ€™ membership can be
determined by DVIs . As expected, Table 3 shows that the
resulting speedups are about 20-fold and 115-fold faster,
respectively. We note that the speedup for the Houses data
set is more than two orders of magnitude. These results
demonstrate the effectiveness of the proposed DVI rules.

8. Conclusion
In this paper, we develop new screening rules for a class
of supervised learning problems by studying their dual formulation with the variational inequalities. Our framework
includes two well-known models, i.e., SVM and LAD, as
special cases. The proposed DVI rules are very effective
in identifying the non-support vectors for both SVM and
LAD, and they thus result in substantial savings in computational cost and memory usage. Extensive experiments
on both synthetic and real data sets demonstrate the effectiveness of the proposed DVI rules. We plan to extend the
framework of DVI to other supervised learning problems, e.g., weighted SVM (Yang et al., 2005), RWLS (robust
weighted least squres) (Chatterjee & MaÌˆchler, 1997), robust PCA (Ding et al., 2006), robust matrix factorization
(Ke & Kanade, 2005).

Acknowledgments
This work was supported in part by NIH (LM010730)
and NSF (IIS-0953662, CCF-1025177). We would like to
thank Virginia Unkefer for proofreading.

Scaling SVM and Least Absolute Deviations via Exact Data Reduction

References
Achlioptas, D., Mcsherry, F., and SchoÌˆlkopf, B. Sampling
techniques for kernel methods. In NIPS, 2002.
Bache, K. and Lichman, M. UCI machine learning repository, 2013. URL http://archive.ics.uci.
edu/ml.
Boyd, S. and Vandenberghe, L. Convex Optimization.
Cambridge University Press, 2004.
Buchinsky, M. Recent advances in quantile regression
models. The Journal of Human Resources, 33:88â€“126,
1998.
Cao, D. and Boley, D. On approximate solutions to support
vector machines. In SDM, 2006.
Chatterjee, S. and MaÌˆchler, M. Robust regression: a
weighted least squares approach. Communications in Statistics: Theory and Methods, 26:1381â€“1394, 1997.
Chen, K., Ying, Z., Zhang, H., and Zhao, L. Analysis of
least absolute deviation. Biometrika, 95:107â€“122, 2008.
Cortez, P., Cerdeira, A., Almeida, F., Matos, T., and Reis, J.
Modeling wine preferences by data mining from physicochemical properties. Decision Support Systems, 47:
547â€“553, 2009.
Ding, C., Zhou, D., He, X., and Zha, H. R1 -PCA: Rotational invariant L1 -norm principle component analysis
for robust subspace factorization. In ICML, 2006.
El Ghaoui, L., Viallon, V., and Rabbani, T. Safe feature elimination in sparse supervised learning. Pacific Journal
of Optimization, 8:667â€“698, 2012.
Fan, R., Chang, K., Hsieh, C., Wang, X., and Lin, C. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871â€“1874, 2008.
GuÌˆler, O. Foundations of optimization. Springer, 2010.
Hastie, T., Rosset, S., Tibshirani, R., and Zhu, J. The entire
regularization path for the support vector machine. Journal of Machine Learning Research, 5:1391â€“1415, 2004.
Hettich, S. and Bay, S. UCI KDD Archive. 1999.
Hiriart-Urruty, J.-B. and LemareÌchal, C. Convex analysis and minimization algorithms volumn I, II. SpringerVerlag, 1993.
Hsieh, C. J., Chang, K. W., and Lin, C. J. A dual coordinate
descent method for large-scale linear SVM. In ICML,
2008.
Jin, Z., Ying, Z., and Wei, L. A simple resampling method
by perturbing the minimand. Biometrika, 88:381â€“390,
2001.

Joachims, T. Training linear SVMs in linear time. In ACM
KDD, 2006.
Ke, Q. and Kanade, T. Robust L1 norm factorization in
the presence of outliers and missing data by alternative
convex programming. In CVPR, 2005.
Liu, J., Zhao, Z., Wang, J., and Ye, J. Safe screening with
variational inequalities and its application to lasso. In
ICML, 2014.
Ogawa, K., Suzuki, Y., and Takeuchi, I. Safe screening of
non-support vectors in pathwise SVM computation. In
ICML, 2013.
Pace, R. and Barry, R. Sparse spatial autoregressions. Statistics and Probability Letters, 33:291â€“297, 1997.
Powell, J. Least absolute deviations estimation for the censored regression model. Journal of Econometrics, 25:
303â€“325, 1984.
Prokhorov, D. Slide presentation in IJCNN01. In IJCNN
2001 neural network competition, 2001.
Rao, C. R., Toutenburg, H., Shalabh, and Heumann, C. Linear Models and Generalizations: Least Squares and Alternatives. Springer, 2008.
Rasmussen, C., Neal, R., Hinton, G., Camp, D., Revow,
M., Ghahramani, Z., Kustra, R., and Tibshirani, R. Data
for evaluating learning in valid experiments.
RuszczynÌski, A. Nonlinear Optimization. Princeton University Press, 2006.
Shalev-Shwartz, S., Singer, Y., and Srebro, N. Pegasos:
primal estimated sub-gradient solver for SVM. In ICML,
2007.
Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N.,
Taylor, J., and Tibshirani, R. Strong rules for discarding
predictors in lasso-type problems. Journal of the Royal
Statistical Society Series B, 74:245â€“266, 2012.
Wang, J., Zhou, J., Wonka, P., and Ye, J. Lasso screening
rules via dual polytope projection. In NIPS, 2013.
Wang, L., Gordon, M., and Zhu, J. Regularized least absolute deviations regression and an efficient algorithm for
parameter tuning. In ICDM, 2006.
Xiang, Z. J., Xu, H., and Ramadge, P. J. Learning sparse
representation of high dimensional data on large scale
dictionaries. In NIPS, 2011.
Yang, X., Song, Q., and Cao, A. Weighted support vector
machine for data classification. In IJCNN, 2005.
Yu, H., Yang, J., and Han, J. Classifying large data sets
using SVMs with hiearchical clusters. In KDD, 2003.

