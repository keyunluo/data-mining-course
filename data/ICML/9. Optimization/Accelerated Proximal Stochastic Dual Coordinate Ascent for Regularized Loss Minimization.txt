Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss
Minimization
Shai Shalev-Shwartz
School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel
Tong Zhang
Department of Statistics, Rutgers University, NJ, USA, and Baidu Inc., Beijing, China

Abstract
We introduce a proximal version of the stochastic dual coordinate ascent method and show how
to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the
framework and obtain rates that improve stateof-the-art results for various key machine learning optimization problems including SVM, logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical
findings.

1. Introduction
We consider the following generic optimization problem
associated with regularized loss minimization of linear predictors: Let X1 , . . . , Xn be matrices in Rd√ók (referred to
as instances), let œÜ1 , . . . , œÜn be a sequence of vector convex functions defined on Rk (referred to as loss functions),
let g(¬∑) be a convex function defined on Rd (referred to as
a regularizer), and let Œª ‚â• 0 (referred to as a regularization
parameter). Our goal is to solve:
" n
#
1X
>
min P (w) where P (w) =
œÜi (Xi w) + Œªg(w) .
n i=1
w‚ààRd
(1)
For example, in ridge regression the regularizer is g(w) =
1
2
2 kwk2 , the instances are column vectors, and for every i
the i‚Äôth loss function is œÜi (a) = 12 (a‚àíyi )2 , for some scalar
yi .
Let w‚àó = argminw P (w) (we will later make assumptions
that imply that w‚àó is unique). We say that w is -accurate if
P (w) ‚àí P (w‚àó ) ‚â§ . Our main result is a new algorithm for
solving (1). If g is 1-strongly convex and each œÜi is (1/Œ≥)Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

SHAIS @ CS . HUJI . AC . IL

TONGZ @ RCI . RUTGERS . EDU

smooth (meaning that its gradient is (1/Œ≥)-Lipschitz), then
our algorithm finds, with probability of at least 1 ‚àí Œ¥, an
-accurate solution to (1) in time
 


r
1
n
OÃÉ d n + min
,
.
ŒªŒ≥
ŒªŒ≥
This applies, for example, to ridge regression and to logistic
regression with L2 regularization. The OÃÉ notation hides
constants and logarithmic terms.
1
Intuitively, we can think of ŒªŒ≥
as the condition number of
the problem. If the condition number is O(n) then our
runtime becomes OÃÉ(dn). This means that the runtime is
nearly linear in the data size. This matches the recent result
of Shalev-Shwartz & Zhang [21], Le Roux et al. [13], but
our setting is significantly more general. When the condition q
number is much larger than n, our runtime becomes
OÃÉ(d ŒªnŒ≥ ). This significantly improves over the result of

[21, 13]. It also significantly improves over the runtime of
accelerated
q gradient descent due to Nesterov [16], which is
OÃÉ(d n Œª1Œ≥ ).
By applying a smoothing technique to œÜi , we also derive
a method that finds an -accurate solution to (1) assuming
that each œÜi is O(1)-Lipschitz, and obtain the runtime
r 
 

1
n
.
OÃÉ d n + min
,
Œª
Œª
This applies, for example, to SVM with the hinge-loss. It
d
significantly improves over the rate Œª
of SGD (e.g. [23]),
1
when Œª  n.
We can also apply our results to non-strongly convex regularizers (such as the L1 norm regularizer), or to nonregularized problems, by adding a slight L2 regularization.
For example, for L1 regularized problems, and assuming
that each œÜi is (1/Œ≥)-smooth, we obtain the runtime of
 

r 
n
1
OÃÉ d n + min
,
.
Œ≥
Œ≥

Accelerated Proximal Stochastic Dual Coordinate Ascent

This applies, for example, to the Lasso problem, in which
the goal is to minimize the squared loss plus an L1 regularization term.
To put our results in context, in Table 1 we specify the
runtime of various algorithms (while ignoring constants
and logarithmic terms) for three key machine learning applications; SVM in which œÜi (a) = max{0, 1 ‚àí a} and
g(w) = 21 kwk22 , Lasso in which œÜi (a) = 12 (a ‚àí yi )2
and g(w) = œÉkwk1 , and Ridge Regression in which
œÜi (a) = 21 (a ‚àí yi )2 and g(w) = 12 kwk22 . Additional applications, and a more detailed runtime comparison to previous work, are given in Section 4. In the table, SGD stands
for Stochastic Gradient Descent, and AGD stands for Accelerated Gradient Descent.
Technical contribution: Our algorithm combines two
ideas. The first is a proximal version of stochastic dual coordinate ascent (SDCA).1 In particular, we generalize the
recent analysis of [21] in two directions. First, we allow the
regularizer, g, to be a general strongly convex function (and
not necessarily the squared Euclidean norm). This allows
us to consider non-smooth regularization function, such as
the L1 regularization. Second, we allow the loss functions, œÜi , to be vector valued functions which are smooth
(or Lipschitz) with respect to a general norm. This generalization is useful in multiclass applications.
As
in [21],
 
1
the runtime of this procedure is OÃÉ d n + ŒªŒ≥ . This
would be a nearly linear time (in the size of the data) if
1
1
ŒªŒ≥ = O(n). Our second idea deals with the case ŒªŒ≥  n
by iteratively approximating the objective function P with
objective functions that have a stronger regularization. In
particular, each iteration of our acceleration procedure involves approximate minimization of P (w) + Œ∫2 kw ‚àí yk22 ,
with respect to w, where y is a vector obtained from previous iterates and Œ∫ is order of 1/(Œ≥n). The idea is that
the addition of the relatively strong regularization makes
the runtime of our proximal stochastic dual coordinate ascent procedure be OÃÉ(dn). And, with a proper choice of y
at each iteration, we show that the sequence of solutions
of the problems with the q
added regularization converge to
1
the minimum of P after ŒªŒ≥n
iterations. This yields the
q
n
overall runtime of d ŒªŒ≥
.
Additional related work: As mentioned before, our first
contribution is a proximal version of the stochastic dual co1

Technically speaking, it may be more accurate to use the term
randomized dual coordinate ascent, instead of stochastic dual coordinate ascent. This is because our algorithm makes more than
one pass over the data, and therefore cannot work directly on distributions with infinite support. However, following the convention in the prior machine learning literature, we do not make this
distinction.

ordinate ascent method and extension of the analysis given
in Shalev-Shwartz & Zhang [21]. Stochastic dual coordinate ascent has also been studied in Collins et al. [3] but
in more restricted settings than the general problem considered in this paper. One can also apply the analysis of
stochastic coordinate descent methods given in Richt√°rik &
Tak√°cÃå [17] on the dual problem. However, here we are interested in understanding the primal sub-optimality, hence
an analysis which only applies to the dual problem is not
sufficient.
The generality of our approach allows us to apply it for
multiclass prediction problems. We discuss this in detail
later on in Section 4. Recently, [11] derived a stochastic
coordinate ascent for structural SVM based on the FrankWolfe algorithm. Although with different motivations, for
the special case of multiclass problems with the hinge-loss,
their algorithm ends up to be the same as our proximal dual
ascent algorithm (with the same rate). Our approach allows
to accelerate the method and obtain an even faster rate.
The proof of our acceleration method adapts Nesterov‚Äôs
estimation sequence technique, studied in Devolder et al.
[6], Schmidt et al. [18], to allow approximate and stochastic proximal mapping. See also [1, 5]. In particular, it relies
on similar ideas as in Proposition 4 of [18]. However, our
specific requirement is different, and the proof presented
here is different and significantly simpler than that of [18].
There have been several attempts to accelerate stochastic
optimization algorithms. See for example [10, 9, 4] and the
references therein. However, the runtime of these methods
have a polynomial dependence on 1/ even if œÜi are smooth
and g is Œª-strongly convex, as opposed to the logarithmic
dependence on 1/ obtained here. As in [13, 21], we avoid
the polynomial dependence on 1/ by allowing more than
a single pass over the data.

2. Preliminaries
All the functions we consider in this paper are proper convex functions over a Euclidean space. We use R to denote
the set of real numbers and to simplify our notation, when
we use R to denote the range of a function f we in fact
allow f to output the value +‚àû.
Given a function f : Rd ‚Üí R we denote its conjugate function by f ‚àó (y) = supx [y > x ‚àí f (x)] . Given a
norm k ¬∑ kP we denote the dual norm by k ¬∑ kD where
kykD = supx:kxkP =1 y > x. We use k ¬∑ k or k ¬∑ k2 to denote
P
the L2 norm, kxk = x> x. We also use kxk1 = i |xi |
and kxk‚àû = maxi |xi |. The operator norm of a matrix X with respect to norms k ¬∑ kP , k ¬∑ kP 0 is defined as
kXkP ‚ÜíP 0 = supu:kukP =1 kXukP 0 .
A function f : Rk ‚Üí Rd is L-Lipschitz with respect to a

Accelerated Proximal Stochastic Dual Coordinate Ascent

Problem

Algorithm

Runtime
d

SGD [23]
SVM

Œª
q
dn Œª1

AGD [15]
This paper

Lasso

d n + min{ Œª1 ,

SGD and variants (e.g. [25, 24, 19])

d
2

Stochastic Coordinate Descent [20, 14]

dn

q

dn

FISTA [16, 2]
This paper
Exact
Ridge Regression

SGD [13], SDCA [21]
AGD [16]
This paper

pn 
Œª }

1


d n + min{ 1 ,

pn 
}

d2 n + d3

d n + Œª1
q
dn Œª1
p 
d n + min{ Œª1 , nŒª }

Table 1. The runtime of various algorithms for three key machine learning problems.

norm k ¬∑ kP , whose dual norm is k ¬∑ kD , if for all a, b ‚àà Rd ,
we have kf (a) ‚àí f (b)kD ‚â§ L ka ‚àí bkP . A function f :
Rd ‚Üí R is (1/Œ≥)-smooth with respect to a norm k ¬∑ kP if
it is differentiable and its gradient is (1/Œ≥)-Lipschitz with
respect to k ¬∑ kP . An equivalent condition is that for all
1
a, b ‚àà Rd , we have f (a) ‚â§ f (b)+‚àáf (b)> (a‚àíb)+ 2Œ≥
ka‚àí
2
d
bkP . A function f : R ‚Üí R is Œ≥-strongly convex with
respect to k¬∑kP if f (w +v) ‚â• f (w)+‚àáf (w)> v + Œ≥2 kvk2P .
It is well known that f is Œ≥-strongly convex with respect to
k ¬∑ kP if and only if f ‚àó is (1/Œ≥)-smooth with respect to the
dual norm, k ¬∑ kD .
The dual problem of (1) is to maximize D(Œ±) over Œ± ‚àà
Rk√ón where
" n
!#
n
X
1X
1
D(Œ±) =
‚àíœÜ‚àói (‚àíŒ±i ) ‚àí Œªg ‚àó Œªn
Xi Œ±i
,
n i=1
i=1
(2)
where Œ±i is the i‚Äôth column of the matrix Œ±, which forms a
vector in Rk .
We will assume that g is strongly convex which implies that
g ‚àó (¬∑) is continuous differentiable. If we define
n

v(Œ±) =

1 X
Xi Œ±i
Œªn i=1

and

w(Œ±) = ‚àág ‚àó (v(Œ±)),

(3)
then it is known that w(Œ±‚àó ) = w‚àó , where Œ±‚àó is an optimal solution of (2). It is also known that P (w‚àó ) = D(Œ±‚àó )
which immediately implies that for all w and Œ±, we have
P (w) ‚â• D(Œ±), and hence the duality gap defined as
P (w(Œ±)) ‚àí D(Œ±) can be regarded as an upper bound on

both the primal sub-optimality, P (w(Œ±)) ‚àí P (w‚àó ), and
on the dual sub-optimality, D(Œ±‚àó ) ‚àí D(Œ±).

3. Main Results
In this section we describe our algorithms and their analysis. We start in Section 3.1 with a description of our proximal stochastic dual coordinate ascent procedure (ProxSDCA). Then, in Section 3.2 we show how to accelerate
the method by calling Prox-SDCA on a sequence of problems with a strong regularization. Throughout this section
we assume that the loss functions are smooth. The case of
non-smooth but Lipschitz loss functions can be tackled by
applying a ‚Äúsmoothing‚Äù technique (see Nesterov [15]).
Due to the lack of space, all proofs are omitted from this extended abstract and can be found in the long version of the
paper [22]. The long version also contains detailed pseudocode of all the algorithms.
3.1. Proximal Stochastic Dual Coordinate Ascent
We now describe our proximal stochastic dual coordinate
ascent procedure for solving (1). Our results in this subsection holds for g being a 1-strongly convex function with
respect to some norm k ¬∑ kP 0 and every œÜi being a (1/Œ≥)smooth function with respect to some other norm k ¬∑ kP .
The corresponding dual norms are denoted by k ¬∑ kD0 and
k ¬∑ kD respectively.
The dual objective in (2) has a different dual vector associated with each example in the training set. At each iteration
of dual coordinate ascent we only allow to change the i‚Äôth

Accelerated Proximal Stochastic Dual Coordinate Ascent

column of Œ±, while the rest of the dual vectors are kept intact. We focus on a randomized version of dual coordinate
ascent, in which at each round we choose which dual vector
to update uniformly at random.
P
(t‚àí1)
At step t, let v (t‚àí1) = (Œªn)‚àí1 i Xi Œ±i
and let
w(t‚àí1) = ‚àág ‚àó (v (t‚àí1) ). We will update the i-th dual vari(t)
(t‚àí1)
able Œ±i = Œ±i
+ ‚àÜŒ±i , in a way that will lead to a
sufficient increase of the dual objective. For the primal
problem, this would lead to the update v (t) = v (t‚àí1) +
(Œªn)‚àí1 Xi ‚àÜŒ±i , and therefore w(t) = ‚àág ‚àó (v (t) ) can also
be written as
#
"
!
n
X
(t)
>
‚àí1
(t)
n
Xi Œ±i
+ Œªg(w) .
w
= argmin ‚àíw
w

i=1

Note that this particular update is rather similar to the update step of proximal-gradient dual-averaging method (see
for example Xiao [24]). The difference is on how Œ±(t) is
updated.
The goal of dual ascent methods is to increase the dual
objective as much as possible, and thus the optimal way
to choose ‚àÜŒ±i would be to maximize the dual objective,
namely, we shall let ‚àÜŒ±i be the maximizer of
1
‚àí œÜ‚àói (‚àí(Œ±i + ‚àÜŒ±i )) ‚àí Œªg ‚àó (v (t‚àí1) + (Œªn)‚àí1 Xi ‚àÜŒ±i ).
n

3.2. Acceleration
The Prox-SDCA procedure described 
in the previous
sub
R2
section has the iteration bound of OÃÉ n + ŒªŒ≥ . This is
a nearly linear runtime whenever the condition number,
R2 /(ŒªŒ≥), is O(n). In this section we show how to improve
the dependence on the condition number by an acceleration procedure. In particular, throughout this section we
2
assume that 10 n < R
ŒªŒ≥ . We further assume throughout this
subsection that the regularizer, g, is 1-strongly convex with
respect to the Euclidean norm, i.e. kukP 0 = k ¬∑ k2 . This
also implies that kukD0 is the Euclidean norm. A generalization of the acceleration technique for strongly convex
regularizers with respect to general norms is left to future
work.
The main idea of the acceleration procedure is to iteratively run the Prox-SDCA procedure, where at iteration t
we call Prox-SDCA with the modified objective, PÃÉt (w) =
P (w) + Œ∫2 kw ‚àí y (t‚àí1) k2 , where Œ∫ is a relatively large
regularization parameter and the regularization is centered
around the vector
y (t‚àí1) = w(t‚àí1) + Œ≤(w(t‚àí1) ‚àí w(t‚àí2) )
for some Œ≤ ‚àà (0, 1). That is, our regularization is centered
around the previous solution plus a ‚Äúmomentum term‚Äù
Œ≤(w(t‚àí1) ‚àí w(t‚àí2) ).

However, for a complex g ‚àó (¬∑), this optimization problem
may not be easy to solve. To simplify the optimization
problem we can rely on the smoothness of g ‚àó (with respect
to a norm k ¬∑ kD0 ) and instead of directly maximizing the
dual objective function, we try to maximize a proximal objective which is a lower bound of the dual objective. This
yields maximization of the expression:

The values of Œ≤ and Œ∫ are set by our theoretical analysis as follows: Œ∫ = R2 /(Œ≥n) ‚àí Œª, and Œ≤ = 1‚àíŒ∑
1+Œ∑ where
p
‚àí1
Œ∑
= ‚àí1 + Œ∫/Œª. At each ‚Äúouter‚Äù iteration of the acceleration procedure, we apply Prox-SDCA for approximately solving PÃÉt (w). We initialize the dual solution to
be the dual solution from the previous iteration, and we
require the accuracy of Prox-SDCA at iteration t to be
Œ∑
‚àí2
)(P (0) ‚àí D(0)) and
2(1+Œ∑ ‚àí1 ) Œæt‚àí1 where Œæ1 = (1 + Œ∑
1
‚àó
(t‚àí1) >
2
‚àí œÜi (‚àí(Œ±i + ‚àÜŒ±i )) ‚àí w
Xi ‚àÜŒ±i ‚àí
kXi ‚àÜŒ±i kD0 . Œæ = (1 ‚àí Œ∑/2)t‚àí1 Œæ .
t
t‚àí1
2Œªn

In general, this optimization problem is still not necessarily simple to solve because œÜ‚àó may also be complex. We
will thus also propose alternative update rules for ‚àÜŒ±i of
(t‚àí1)
the form ‚àÜŒ±i = s(‚àí‚àáœÜi (Xi> w(t‚àí1) ) ‚àí Œ±i
) for an
appropriately chosen step size parameter s > 0. Our analysis shows that setting s = R2ŒªnŒ≥
+ŒªnŒ≥ , for R being an upper
bound on kXi kD‚ÜíD0 , still leads to a sufficient increase in
the dual objective. A detailed pseudo-code can be found in
[22].
The theorem below provides an upper bound on the number
of iterations required by our prox-SDCA procedure.
Theorem 1. The expected runtime required to minimize P
up to accuracy  using procedure Prox-SDCA is



 
D(Œ±‚àó ) ‚àí D(Œ±(0) )
R2
¬∑ log
.
O d n+
ŒªŒ≥


A detailed pseudo-code of the algorithm is given in [22].
All the parameters of the algorithm are determined by our
theory.
Theorem 2. The total runtime required by accelerated
Prox-SDCA to guarantee an -accurate solution with probability of at least 1 ‚àí Œ¥ is
 
 s 2
nR
1
¬∑ log
¬∑
O d
ŒªŒ≥
Œ¥
 2   2 


R
R
P (0) ‚àí D(0)
log
log
+ log
.
ŒªŒ≥ n
ŒªŒ≥n


4. Applications
In this section we specify our algorithmic framework to
several popular machine learning applications. In Section 4.1 we start by describing several loss functions and

Accelerated Proximal Stochastic Dual Coordinate Ascent

deriving their conjugate. In Section 4.2 we describe several
regularization functions. Finally, in the rest of the subsections we specify our algorithm for Ridge regression, SVM,
and Lasso.
4.1. Loss functions
Squared loss: œÜ(a) =
conjugate function is

1
2 (a

‚àí y)2 for some y ‚àà R. The

1
1
œÜ‚àó (b) = max ab ‚àí (a ‚àí y)2 = b2 + yb
a
2
2
Hinge loss: œÜ(a) = [1 ‚àí a]+ := max{0, 1 ‚àí a}. The
conjugate function is
(
b
if b ‚àà [‚àí1, 0]
‚àó
œÜ (b) = max ab ‚àí max{0, 1 ‚àí a} =
a
‚àû otherwise

L1 regularization: Another popular regularization we
consider is the L1 regularization,
f (w) = œÉ kwk1 .
This is not a strongly convex regularizer and therefore we
will add a slight L2 regularization to it and define the L1 L2 regularization as
g(w) =

1
kwk22 + œÉ 0 kwk1 ,
2

(5)

where œÉ 0 = œÉŒª for some small Œª. Note that Œªg(w) =
Œª
2
2 kwk2 + œÉkwk1 , so if Œª is small enough (as will be formalized later) we obtain that Œªg(w) ‚âà œÉkwk1 .
The conjugate of g is


1
>
2
0
g (v) = max w v ‚àí kwk2 ‚àí œÉ kwk1 .
w
2
‚àó

Smooth hinge loss: This loss is obtained by smoothing
the hinge-loss. This loss is parameterized by a scalar Œ≥ > 0
and is defined as:
Ô£±
Ô£¥
a‚â•1
Ô£≤0
œÜÃÉŒ≥ (a) = 1 ‚àí a ‚àí Œ≥/2 a ‚â§ 1 ‚àí Œ≥
(4)
Ô£¥
Ô£≥1
2
(1
‚àí
a)
o.w.
2Œ≥

The maximizer is also ‚àág ‚àó (v) and we now show how to
calculate it. We have


1
2
0
‚àó
>
‚àág (v) = argmax w v ‚àí kwk2 ‚àí œÉ kwk1
2
w


1
2
0
= argmin kw ‚àí vk2 + œÉ kwk1
2
w

The conjugate function is
(
b + Œ≥2 b2
‚àó
œÜÃÉŒ≥ (b) =
‚àû

A sub-gradient of the objective of the optimization problem
above is of the form w ‚àí v + œÉ 0 z = 0, where z is a vector
with zi = sign(wi ), where if wi = 0 then zi ‚àà [‚àí1, 1].
Therefore, if w is an optimal solution then for all i, either
wi = 0 or wi = vi ‚àí œÉ 0 sign(wi ). Furthermore, it is easy to
verify that if w is an optimal solution then for all i, if wi 6=
0 then the sign of wi must be the sign of vi . Therefore,
whenever wi 6= 0 we have that wi = vi ‚àí œÉ 0 sign(vi ). It
follows that in that case we must have |vi | > œÉ 0 . And, the
other direction is also true, namely, if |vi | > œÉ 0 then setting
wi = vi ‚àí œÉ 0 sign(vi ) leads to an objective value whose i‚Äôth
component is

if b ‚àà [‚àí1, 0]
otherwise

It follows that œÜÃÉ‚àóŒ≥ is Œ≥ strongly convex and œÜÃÉ is (1/Œ≥)smooth. In addition, if œÜ is the vanilla hinge-loss, we have
for every a that œÜ(a) ‚àí Œ≥/2 ‚â§ œÜÃÉ(a) ‚â§ œÜ(a) .
4.2. Regularizers
L2 regularization: The simplest regularization is the
squared L2 regularization
1
kwk22 .
2
This is a 1-strongly convex regularization function whose
conjugate is g ‚àó (Œ∏) = 12 kŒ∏k22 . We also have ‚àág ‚àó (Œ∏) = Œ∏ .
g(w) =

For our acceleration procedure, we also use the L2 regularization plus a linear term, namely,
1
kwk2 ‚àí w> z ,
2
for some vector z. The conjugate of this function is


1
1
‚àó
>
2
g (Œ∏) = max w (Œ∏ + z) ‚àí kwk = kŒ∏ + zk2 .
w
2
2
g(w) =

1 0 2
1
(œÉ ) + œÉ 0 (|vi | ‚àí œÉ 0 ) ‚â§ |vi |2 ,
2
2
where the right-hand side is the i‚Äôth component of the objective value we will obtain by setting wi = 0. This leads
to the conclusion that ‚àái g ‚àó (v) = sign(vi ) [|vi | ‚àí œÉ 0 ]+ . It
2
P
follows that g ‚àó (v) = 21 i [|vi | ‚àí œÉ 0 ]+ .
4.3. Ridge Regression
In ridge regression, we minimize the squared loss with L2
regularization. That is, g(w) = 12 kwk2 and for every i we
have that xi ‚àà Rd and œÜi (a) = 12 (a‚àíyi )2 for some yi ‚àà R.
The primal problem is therefore
n

We also have
‚àág ‚àó (Œ∏) = Œ∏ + z .

P (w) =

1 X >
Œª
(x w ‚àí yi )2 + kwk2 .
2n i=1 i
2

Accelerated Proximal Stochastic Dual Coordinate Ascent

8]). However, all of these variants share the runtime of
O(dR2 B 2 /2 ), which is much slower than our runtime
when  is small.

The runtime of Prox-SDCA for ridge regression is
 

R2
OÃÉ d n +
,
Œª
where R = maxi kxi k. This matches the recent results
of [13, 21]. If R2 /Œª  n we can apply the accelerated
procedure and obtain the improved runtime
!
r
nR2
.
OÃÉ d
Œª
4.4. Lasso
In the Lasso problem, the loss function is the squared loss
but the regularization function is L1 . That is, we need to
solve the problem:
"
#
n
1 X >
2
(x w ‚àí yi ) + œÉkwk1 ,
min
(6)
w
2n i=1 i
with a positive regularization parameter œÉ ‚àà R+ .
Consider the optimization problem of minimizing
n

P (w) =

1 X >
(x w ‚àí yi )2 + Œª
2n i=1 i



1
œÉ
kwk22 + kwk1
2
Œª



Another relevant approach is the FISTA algorithm of [2].
The shrinkage operator of FISTA is the same as the gradient
of g ‚àó used in our approach. It is a batch algorithm using
Nesterov‚Äôs accelerated gradient technique.For the squared

q
R2 B 2
.
loss function, the runtime of FISTA is O d n

This
‚àö bound is worst than our bound by a factor of at least
n.
Another approach to solving (6) is stochastic coordinate descent over the primal problem.
[19]
 showed that the run
dnB 2
time of this approach is O
, under the assumption

that kxi k‚àû ‚â§ 1 for all i. Similar results can also be found
in [14].
For our method, the runtime depends on R2 = maxi kxi k22 .
If R2 = O(1) then the runtime of our method is much better than that of [19]. In the general case, if maxi kxi k‚àû ‚â§
1 then R2 ‚â§ d, which yields the runtime of
(
)!!
r
dB 2
n dB 2
OÃÉ d n + min
,
.



,

(7)
for some Œª > 0. This problem fits into our framework,
since now the regularizer is strongly convex. Furthermore,
if w‚àó is an (/2)-accurate solution to the problem in (7),
then it is easy to verify that setting Œª = (œÉ/yÃÑ)2 guarantees
that w‚àó is an  accurate solution to the original problem
given in (6).
Let us now discuss the runtime of the resulting method.
Denote RP= maxi kxi k and for simplicity, assume that
n
1
2
2
yÃÑ = 2n
i=1 yi = O(1). Choosing Œª = (œÉ/yÃÑ) , the
runtime of our method becomes
(
)!!
r
R2
nR2
.
OÃÉ d n + min
,
 œÉ2
 œÉ2
It is also convenient to write the bound in terms of B =
kwÃÑk2 , where, as before, wÃÑ is the optimal solution of the L1
regularized problem. With this parameterization, we can
set Œª = /B 2 and the runtime becomes
(
)!!
r
R2 B 2
n R2 B 2
OÃÉ d n + min
,
.


The runtime of standard SGD is O(dR2 B 2 /2 ) even in
the case of smooth loss functions such as the squared loss.
Several variants of SGD, that leads to sparser intermediate solutions, have been proposed (e.g. [12, 19, 24, 7,

This is the same or better than [19] whenever d = O(n).
4.5. Linear SVM
Support Vector Machines (SVM) is an algorithm for learning a linear classifier. Linear SVM (i.e., SVM with linear
kernels) amounts to minimizing the objective
n

P (w) =

1X
Œª
2
[1 ‚àí x>
i w]+ + kwk ,
n i=1
2

where [a]+ = max{0, a}, and for every i, xi ‚àà Rd .
This can be cast as the objective given in (1) by letting
the regularization be g(w) = 21 kwk22 , and for every i,
œÜi (a) = [1 ‚àí a]+ , is the hinge-loss.

1
Let R = maxi kxi k2 . SGD enjoys the rate of O Œª
.
Many software
packages
apply
SDCA
and
obtain
the
rate

1
OÃÉ n + Œª
. We now show how our accelerated proximal
pn
SDCA enjoys the rate OÃÉ n + Œª
. This is significantly
better than the rate of SGD when Œª < 1/n. We note that
a default setting for Œª, which often works well in practice,
is Œª = 1/n. In this case, Œª = /n  1/n.
Our first step is to smooth the hinge-loss. Let Œ≥ =  and
consider the smooth hinge-loss as defined in (4). Recall
that the smooth hinge-loss satisfies, for every a, œÜ(a) ‚àí
Œ≥/2 ‚â§ œÜÃÉ(a) ‚â§ œÜ(a). Let PÃÉ be the SVM objective while
replacing the hinge-loss with the smooth hinge-loss. Therefore, for every w0 and w, P (w0 )‚àíP (w) ‚â§ PÃÉ (w0 )‚àí PÃÉ (w)+

Accelerated Proximal Stochastic Dual Coordinate Ascent

Œ≥/2 . It follows that if w0 is an (/2)-optimal solution for
PÃÉ , then it is -optimal solution for P .
Denote R = maxi kxi k. Then, the runtime of the resulting
method is
s
)!!
(
nR2
R2
,
.
OÃÉ d n + min
Œ≥Œª
Œ≥Œª
In particular, choosing Œ≥ =  we obtain a solution to the
original SVM problem in runtime of
)!!
(
r
nR2
R2
.
,
OÃÉ d n + min
Œª
Œª
As mentioned before, this is better than SGD when
n.

1
Œª



is relatively large, but it converges much slower when Œª is
small. This is consistent with our theory. Finally, the relative performance of FISTA and Prox-SDCA depends on the
ratio between Œª and n, but in all cases, Accelerated-ProxSDCA is much faster than FISTA. This is again consistent
with our theory.

6. Discussion and Open Problems
We have described and analyzed a proximal stochastic dual
coordinate ascent method and have shown how to accelerate the procedure. The overall runtime of the resulting
method improves state-of-the-art results in many cases of
interest.
There are two main open problems that we leave to future
research.

5. Experiments
In this section we compare Prox-SDCA, its accelerated version Accelerated-Prox-SDCA, and the FISTA algorithm of
[2], on L1 ‚àí L2 regularized loss minimization problems.
The experiments were performed on three large datasets
with very different feature counts and sparsity, which were
kindly provided by Thorsten Joachims (the datasets were
also used in [21]). These are binary classification problems, with each xi being a vector which has been normalized to be kxi k2 = 1, and yi being a binary class label of
¬±1. We multiplied each xi by yi and following [21], we
employed the smooth hinge loss, œÜÃÉŒ≥ , as in (4), with Œ≥ = 1.
The optimization problem we need to solve is therefore to
minimize
n

P (w) =

1X
Œª
2
œÜÃÉŒ≥ (x>
i w) + kwk2 + œÉkwk1 .
n i=1
2

In the experiments, we set œÉ = 10‚àí5 and vary Œª in the
range {10‚àí6 , 10‚àí7 , 10‚àí8 , 10‚àí9 }.
The convergence behaviors are plotted in Figure 1. In all
the plots we depict the primal objective as a function of
the number of passes over the data (often referred to as
‚Äúepochs‚Äù). For FISTA, each iteration involves a single pass
over the data. For Prox-SDCA, each n iterations are equivalent to a single pass over the data. And, for AcceleratedProx-SDCA, each n inner iterations are equivalent to a single pass over the data. For Prox-SDCA and AcceleratedProx-SDCA we implemented their corresponding stopping
conditions and terminate the methods once an accuracy of
10‚àí3 was guaranteed.
It is clear from the graphs that Accelerated-Prox-SDCA
yields the best results, and often significantly outperform
the other methods. Prox-SDCA behaves similarly when Œª

1
ŒªŒ≥

is larger than n, the runtime
 q 
n
. Is it possible to
of our procedure becomes OÃÉ d ŒªŒ≥
 
q 
1
derive a method whose runtime is OÃÉ d n + ŒªŒ≥
?

Open Problem 1. When

Open Problem 2. Our Prox-SDCA procedure and its analysis works for regularizers which are strongly convex with
respect to an arbitrary norm. However, our acceleration
procedure is designed for regularizers which are strongly
convex with respect to the Euclidean norm. Is is possible to
extend the acceleration procedure to more general regularizers?

Acknowledgements
The authors would like to thank Fen Xia for careful proofreading of the paper which helped us correct numerous typos. Shai Shalev-Shwartz is supported by the following
grants: Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and ISF 598-10. Tong Zhang
is supported by the following grants: NSF IIS-1016061,
NSF DMS-1007527, and NSF IIS-1250985.

Accelerated Proximal Stochastic Dual Coordinate Ascent

Œª

astro-ph

cov1
AccProxSDCA
ProxSDCA
FISTA

0.5
0.4
0.3

10

‚àí6

0.2

CCAT

0.5

AccProxSDCA
ProxSDCA
FISTA

0.45

AccProxSDCA
ProxSDCA
FISTA

0.5
0.4

0.4

0.3

0.35

0.2

0.1
0.3
0

20

40

60

80

100

AccProxSDCA
ProxSDCA
FISTA

0.5
0.4
0.3

10

‚àí7

0.2

0.1
0

20

40

60

0.5

80

AccProxSDCA
ProxSDCA
FISTA

0.45

0

100

20

40

60

80

100

AccProxSDCA
ProxSDCA
FISTA

0.5
0.4

0.4

0.3

0.35

0.2

0.1
0.3
0

0

20

40

60

80

100

AccProxSDCA
ProxSDCA
FISTA

0.5
0.4
0.3

10

‚àí8

0.2

0.1
0

20

40

60

0.5

80

100

AccProxSDCA
ProxSDCA
FISTA

0.45

0

20

40

60

80

100

AccProxSDCA
ProxSDCA
FISTA

0.5
0.4

0.4

0.3

0.35

0.2

0.1
0.3
0

0

20

40

60

80

100

AccProxSDCA
ProxSDCA
FISTA

0.5
0.4
0.3

10‚àí9

0.2

0.1
0

20

40

60

0.5

80

100

AccProxSDCA
ProxSDCA
FISTA

0.45

0

20

40

60

80

100

AccProxSDCA
ProxSDCA
FISTA

0.5
0.4

0.4

0.3

0.35

0.2

0.3

0.1

0.1
0

0

20

40

60

80

100

0

20

40

60

80

100

0

20

40

60

80

100

Figure 1. Comparing Accelerated-Prox-SDCA, Prox-SDCA, and FISTA for minimizing the smoothed hinge-loss (Œ≥ = 1) with L1 ‚àí L2
regularization (œÉ = 10‚àí5 and Œª varies in {10‚àí6 , . . . , 10‚àí9 }). In each of these plots, the y-axis is the primal objective and the x-axis is
the number of passes through the entire training set. The three columns corresponds to the three data sets described in [21]. The methods
are terminated either if stopping condition is met (with  = 10‚àí3 ) or after 100 passes over the data.

Accelerated Proximal Stochastic Dual Coordinate Ascent

References
[1] Baes, Michel. Estimate sequence methods: extensions and approximations. Institute for Operations
Research, ETH, Z√ºrich, Switzerland, 2009.
[2] Beck, A. and Teboulle, M. A fast iterative shrinkagethresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183‚Äì202,
2009.
[3] Collins, M., A. Globerson, Koo, T., Carreras, X., and
Bartlett, P. Exponentiated gradient algorithms for
conditional random fields and max-margin markov
networks. Journal of Machine Learning Research, 9:
1775‚Äì1822, 2008.
[4] Cotter, Andrew, Shamir, Ohad, Srebro, Nathan, and
Sridharan, Karthik. Better mini-batch algorithms
via accelerated gradient methods. arXiv preprint
arXiv:1106.4574, 2011.
[5] d‚ÄôAspremont, Alexandre. Smooth optimization with
approximate gradient. SIAM Journal on Optimization, 19(3):1171‚Äì1183, 2008.
[6] Devolder, Olivier, Glineur, Francois, and Nesterov,
Yuri. First-order methods of smooth convex optimization with inexact oracle. Technical Report 2011/2,
CORE, 2011.
[7] Duchi, J. and Singer, Y. Efficient online and batch
learning using forward backward splitting. The Journal of Machine Learning Research, 10:2899‚Äì2934,
2009.
[8] Duchi, John, Shalev-Shwartz, Shai, Singer, Yoram,
and Tewari, Ambuj. Composite objective mirror descent. In Proceedings of the 23rd Annual Conference
on Learning Theory, pp. 14‚Äì26, 2010.
[9] Ghadimi, Saeed and Lan, Guanghui.
Optimal
stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework. SIAM Journal on Optimization,
22(4):1469‚Äì1492, 2012.
[10] Hu, Chonghai, Pan, Weike, and Kwok, James T. Accelerated gradient methods for stochastic optimization and online learning. In Advances in Neural Information Processing Systems, pp. 781‚Äì789, 2009.
[11] Lacoste-Julien, S., Jaggi, M., Schmidt, M., and
Pletscher, P. Stochastic block-coordinate frank-wolfe
optimization for structural svms. arXiv preprint
arXiv:1207.4747, 2012.

[12] Langford, J., Li, L., and Zhang, T. Sparse online
learning via truncated gradient. In NIPS, pp. 905‚Äì
912, 2009.
[13] Le Roux, Nicolas, Schmidt, Mark, and Bach, Francis. A Stochastic Gradient Method with an Exponential Convergence Rate for Strongly-Convex Optimization with Finite Training Sets. arXiv preprint
arXiv:1202.6258, 2012.
[14] Nesterov, Y. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341‚Äì362, 2012.
[15] Nesterov, Yurii. Smooth minimization of non-smooth
functions. Mathematical Programming, 103(1):127‚Äì
152, 2005.
[16] Nesterov, Yurii. Gradient methods for minimizing
composite objective function, 2007.
[17] Richt√°rik, Peter and Tak√°cÃå, Martin.
Iteration
complexity of randomized block-coordinate descent
methods for minimizing a composite function. Mathematical Programming, pp. 1‚Äì38, 2012.
[18] Schmidt, Mark, Roux, Nicolas Le, and Bach, Francis. Convergence rates of inexact proximal-gradient
methods for convex optimization. Technical Report
arXiv:1109.2415, arXiv, 2011.
[19] Shalev-Shwartz, S. and Tewari, A. Stochastic methods for l 1-regularized loss minimization. The Journal
of Machine Learning Research, 12:1865‚Äì1892, 2011.
[20] Shalev-Shwartz, Shai and Tewari, Ambuj. Stochastic methods for l1 regularized loss minimization. In
ICML, pp. 117, 2009.
[21] Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual coordinate ascent methods for regularized
loss minimization. Journal of Machine Learning Research, 14:567‚Äì599, Feb 2013.
[22] Shalev-Shwartz, Shai and Zhang, Tong. Accelerated
proximal stochastic dual coordinate ascent for regularized loss minimization. arxiv:1309.2375, 2013.
[23] Shalev-Shwartz, Shai, Singer, Yoram, and Srebro,
Nathan. Pegasos: Primal Estimated sub-GrAdient
SOlver for SVM. In ICML, pp. 807‚Äì814, 2007.
[24] Xiao, Lin. Dual averaging method for regularized
stochastic learning and online optimization. Journal
of Machine Learning Research, 11:2543‚Äì2596, 2010.
[25] Zhang, Tong. On the dual formulation of regularized
linear systems. Machine Learning, 46:91‚Äì129, 2002.

