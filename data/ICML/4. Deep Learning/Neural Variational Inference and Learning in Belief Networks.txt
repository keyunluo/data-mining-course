Neural Variational Inference and Learning in Belief Networks

Andriy Mnih
Karol Gregor
Google DeepMind

AMNIH @ GOOGLE . COM
KAROLG @ GOOGLE . COM

Abstract
Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the
approximate inference methods that have been
applied to them scale well. We propose a fast
non-iterative approximate inference method that
uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are
trained jointly by maximizing a variational lower
bound on the log-likelihood. Although the naive
estimator of the inference network gradient is too
high-variance to be useful, we make it practical by applying several straightforward modelindependent variance reduction techniques. Applying our approach to training sigmoid belief
networks and deep autoregressive networks, we
show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.

1. Introduction
Compared to powerful globally-normalized latent variable
models, such as deep belief networks (Hinton et al., 2006)
and deep Boltzmann machines (Salakhutdinov & Hinton,
2009a), which can now be trained on fairly large datasets,
their purely directed counterparts have been left behind due
to the lack of efficient learning algorithms. This is unfortunate, because their modularity and ability to generate observations efficiently make them better suited for integration into larger systems.
Training highly expressive directed latent variable models on large datasets is a challenging problem due to the
difficulties posed by inference. Although the generality
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

of Markov Chain Monte Carlo (MCMC) methods makes
them straightforward to apply to models of this type (Neal,
1992), they tend to suffer from slow mixing and are usually
too computationally expensive to be practical in all but the
simplest models. Such methods are also difficult to scale to
large datasets because they need to store the current state of
the latent variables for all the training observations between
parameter updates.
Variational methods (Jordan et al., 1999) provide an
optimization-based alternative to the sampling-based
Monte Carlo methods, and tend to be more efficient. They
involve approximating the exact posterior using a distribution from a more tractable family, often a fully factored
one, by maximizing a variational lower bound on the loglikelihood w.r.t. the parameters of the distribution. For a
small class of models, using such variational posteriors allows the expectations that specify the parameter updates to
be computed analytically. However, for highly expressive
models such as the ones we are interested in, these expectations are intractable even with the simplest variational posteriors. This difficulty is usually dealt with by lower bounding the intractable expectations with tractable one by introducing more variational parameters, as was done for sigmoid belief nets by Saul et al. (1996). However, this technique increases the gap between the bound being optimized
and the log-likelihood, potentially resulting in a poorer fit
to the data. In general, variational methods tend to be more
model-dependent than sampling-based methods, often requiring non-trivial model-specific derivations.
We propose a new approach to training directed graphical models that combines the advantages of the samplingbased and variational methods. Its central idea is using a
feedforward network to implement efficient exact sampling
from the variational posterior for the given observation. We
train this inference network jointly with the model by maximizing the variational lower bound on the log-likelihood,
estimating all the required gradients using samples from
the inference network. Although naive estimate of the gradient for the inference network parameters is unusable due
to its high variance, we make the approach practical by applying several straightforward and general variance reduc-

Neural Variational Inference and Learning in Belief Networks

tion techniques. The resulting training procedure for the
inference network can be seen as an instance of the REINFORCE algorithm (Williams, 1992). Due to our use
of stochastic feedforward networks for performing inference we call our approach Neural Variational Inference and
Learning (NVIL).
Compared to MCMC methods, where many iterations over
the latent variables are required to generate a sample from
the exact posterior and successive samples tend to be highly
correlated, NVIL does not suffer from mixing issues as
each forward pass through the inference network generates
an independent exact sample from the variational posterior.
In addition to being much faster than MCMC, our approach
has the additional advantage of not needing to store the
latent variables for each observation and thus is not only
more memory efficient but also applicable to the pure online learning setting, where each training case is seen once
before being discarded.
In contrast to other work on scaling up variational inference, NVIL can handle both discrete and continuous latent
variables (unlike Kingma & Welling (2013); Rezende et al.
(2014)) as well variational posteriors with complex dependency structures (unlike Ranganath et al. (2013)). Moreover, the variance reduction methods we employ are simple and model-independent, unlike the more sophisticated
model-specific control variates of Paisley et al. (2012).
Though the idea of training an inference model by following the gradient of the variational bound has been considered before, it was dismissed as infeasible (Dayan &
Hinton, 1996). Our primary contribution is to show how
to reduce the variance of the naive gradient estimator to
make it practical without narrowing its range of applicability. We also show that the resulting method trains sigmoid belief networks better than the wake-sleep algorithm
(Hinton et al., 1995), which is the only algorithm we are
aware of that is capable of training the same range of models efficiently. Finally, we demonstrate the effectiveness
and scalability of NVIL by using it to achieve state-of-theart results on the Reuters RCV1 document dataset.

2. Neural variational inference and learning
2.1. Variational objective
Suppose we are interested in training a latent variable
model Pθ (x, h) with parameters θ. We assume that exact inference in the model is intractable and thus maximum
likelihood learning is not an option. For simplicity, we will
also assume that all the latent variables in the model are discrete, though essentially the same approach applies if some
or all of the variables are continuous.
We will train the model by maximizing a variational

lower bound on the marginal log-likelihood. Following
the standard variational inference approach (Jordan et al.,
1999), given an observation x, we introduce a distribution
Qφ (h|x) with parameters φ, which will serve as an approximation to its exact posterior Pθ (h|x). The variational posterior Q will have a simpler form than the exact posterior
and thus will be easier to work with.
The contribution of x to the log-likelihood can then be
lower-bounded as follows (Jordan et al., 1999):
X
log Pθ (x) = log
Pθ (x, h)
h

≥

X

Qφ (h|x) log

h

Pθ (x, h)
Qφ (h|x)

= EQ [log Pθ (x, h) − log Qφ (h|x)]

(1)

= L(x, θ, φ).
By rewriting the bound as
L(x, θ, φ) = log Pθ (x) − KL(Qφ (h|x), Pθ (h|x)), (2)
we see that its tightness is determined by the KullbackLeibler (KL) divergence between the variational distribution and the exact posterior. Maximizing the bound with
respect to the parameters φ of the variational distribution
makes the distribution a better approximation to the posterior (w.r.t. the KL-divergence) and tightens the bound.
In contrast to most applications of variational inference
where the variational posterior for each observation is defined using its own set of variational parameters, our approach does not use any local variational parameters. Instead, we use a flexible feedforward model to compute the
variational distribution from the observation. We call the
model mapping x to Qφ (h|x) the inference network. The
architecture of the inference network is constrained only by
the requirement that Qφ (h|x) it defines has to be efficient
to evaluate and sample from. Using samples from the inference network we will be able to compute gradient estimates
for the model and inference network parameters for a large
class of highly expressive architectures, without having to
deal with architecture-specific approximations.
Given a training set D, consisting of observations
x1 , ..., xD , wePtrain the model by (locally) maximizing
L(D, θ, φ) = i L(xi , θ, φ) using gradient ascent w.r.t. to
the model and inference network parameters. To ensure
scalability to large datasets, we will perform stochastic optimization by estimating gradients on small minibatches of
randomly sampled training cases.
2.2. Parameter gradients
The gradient of the variational bound for a single observation x w.r.t. to the model parameters is straightforward to

Neural Variational Inference and Learning in Belief Networks

2.3.1. C ENTERING THE LEARNING SIGNAL

derive and has the form
∇θ L(x) = EQ [∇θ log Pθ (x, h)] ,

(3)

where we left θ and φ off the list of the arguments of L to
simplify the notation. The corresponding gradient w.r.t. to
the inference network parameters is somewhat more involved:
∇φ L(x) = EQ [(log Pθ (x, h) − log Qφ (h|x))
× ∇φ log Qφ (h|x)],

(4)

We give its derivation in the supplementary material.
As both gradients involve expectations which are intractable in all but a handful of special cases, we will estimate them with Monte Carlo integration, using samples
from the inference network. Having generated n samples
h(1) , ..., h(n) from Qφ (h|x), we compute
n

∇θ L(x) ≈

1X
∇θ log Pθ (x, h(i) )
n i=1

(5)

Inspecting Eq. 4, we see that we are using
lφ (x, h) = log Pθ (x, h) − log Qφ (h|x)

(7)

as the learning signal for the inference network parameters,
and thus are effectively fitting log Qφ (h|x) to log Pθ (x, h).
This might seem surprising, given that we want the inference network Qφ (h|x) to approximate the posterior
distribution Pθ (x|h), as opposed to the joint distribution
Pθ (x, h). It turns out however that using the joint instead of
the posterior distribution in Eq. 4 does not affect the value
of the expectation. To see that we start by noting that


∇φ Qφ (h|x)
EQ [∇φ log Qφ (h|x)] = EQ
Qφ (h|x)
= ∇φ EQ [1] = 0.

(8)

Therefore we can subtract any c that does not depend on h
from the learning signal in Eq. 4 without affecting the value
of the expectation:
EQ [(lφ (x, h) − c)∇φ log Qφ (h|x)]
= EQ [lφ (x, h)∇φ log Qφ (h|x)] − cEQ [∇φ log Qφ (h|x)]

and

= EQ [lφ (x, h)∇φ log Qφ (h|x)].

n

1X
(log Pθ (x, h(i) ) − log Qφ (h(i) |x))
∇φ L(x) ≈
n i=1
× ∇φ log Qφ (h(i) |x).

(6)

The above gradient estimators are unbiased and thus can be
used to perform stochastic maximization of the variational
objective using a suitable learning rate annealing schedule.
The speed of convergence of this procedure, however, depends heavily on the variance of the estimators used, as we
will see in Section 4.2.
The model gradient estimator (5) is well-behaved and does
not pose a problem. The variance of the inference network
gradient estimator (6), however, can be very high due to
the scaling of the gradient inside the expectation by a potentially large term. As a result, learning variational parameters with updates based on this estimator can be unacceptably slow. In fact, it is widely believed that learning variational parameters using gradient estimators of the
form (6) is infeasible (Hinton & Zemel, 1994; Dayan &
Hinton, 1996; Kingma & Welling, 2013). In the next section we will show how to make this approach practical by
applying variance reduction techniques.
2.3. Variance reduction techniques
Though gradient estimates computed using Eq. 6 are usually too noisy to be useful in practice, it is easy to reduce
their variance to a manageable level with the following
model-independent techniques.

(9)

And as log Pθ (x, h) = log Pθ (h|x) + log Pθ (x) and
log Pθ (x) does not depend on h, using Pθ (h|x) in Eq. 4
in place of Pθ (x, h) does not affect the value of the expectation.
This equivalence allows us to compute the learning signal efficiently, without having to evaluate the intractable
Pθ (h|x) term. The price we pay for this tractability is
the much higher variance of the estimates computed using Eq. 6. Fortunately, Eq. 9 suggests that we can reduce
the variance by subtracting a carefully chosen c from the
learning signal. The simplest option is to make c a parameter and adapt it as learning progresses. However, c
will not be able capture the systematic differences in the
learning signal for different observations x, which arise in
part due to the presence of the log Pθ (x) term. Thus we
can reduce the gradient variance further by subtracting an
observation-dependent term Cψ (x) to minimize those differences. Doing this does not affect the expected value of
the gradient estimator because Cψ (x) does not depend on
the latent variables. Borrowing a name from the reinforcement learning literature we will refer to c and Cψ (x) as
baselines. We will elaborate on this connection in Section 3.4.
We implement the input-dependent baseline Cψ (x) using a
neural network and train it to minimize the expected square
of the centered learning signal EQ [(lφ (x, h)−Cψ (x)−c)2 ].
Though this approach to fitting the baseline does not result in the maximal variance reduction, it is simpler and in

Neural Variational Inference and Learning in Belief Networks

our experience works as well as the optimal approach of
Weaver & Tao (2001) which requires taking into account
the magnitude of the gradient of the inference network parameters. We also experimented with per-parameter baselines but found that they did not improve on the global ones.
Finally, we note that incorporating baselines into the learning signal can be seen as using simple control variates. In
contrast to the more elaborate control variates (e.g. of Paisley et al. (2012)), baselines do not depend on the form of
the model or of the variational distribution and thus are easier to use.
2.3.2. VARIANCE NORMALIZATION
Even after centering, using lφ (x, h) as the learning signal
is non-trivial as its average magnitude can change dramatically, and not necessarily monotonically, as training progresses. This variability makes training an inference network using a fixed learning rate difficult. We address this
issue by dividing the centered learning signal by a running
estimate of its standard deviation. This normalization ensures that the signal is approximately unit variance, and can
be seen as a simple and efficient way of adapting the learning rate. To ensure that we stop learning when the magnitude of the signal approaches zero, we apply variance normalization only when the estimate of the standard deviation is greater than 1. The algorithm for computing NVIL
parameter updates using the variance reduction techniques
described so far is provided in the supplementary material.
2.3.3. L OCAL LEARNING SIGNALS
So far we made no assumptions about the structure of the
model or the inference network. However, by taking advantage of their conditional independence properties we can
train the inference network using simpler and less noisy local learning signals instead of the monolithic global learning signal lφ (x, h). Our approach to deriving a local signal
for a set of parameters involves removing all the terms from
the global signal that do not affect the value of the resulting
gradient estimator.
We will derive the layer-specific learning signals for the
common case of both the model and the inference network
having n layers of latent variables. The model and the variational posterior distributions then naturally factor as
Pθ (x, h) =Pθ (x|h1 )

Yn−1

Qφ (h|x) =Qφ1 (h1 |x)

Pθ (hi |hi+1 )Pθ (hn ),

(10)

Qφi+1 (hi+1 |hi ),

(11)

i=1
Yn−1
i=1

where hi denotes the latent variables in the ith layer and φi
the parameters of the variational distribution for that layer.
We will also use hi:j to denote the latent variables in layers
i through j.

To learn the parameters of the the variational distribution
for layer i , we need to compute the following gradient:
∇φi L(x) = EQ(h|x) [lφ (x, h)∇φi log Qφi (hi |hi−1 )].
Using the law of iterated expectation we can rewrite the
expectation w.r.t. Q(h|x) as
∇φi L(x) = EQ(h1:i−1 |x) [
EQ(hi:n |hi−1 ) [lφ (x, h)∇φi log Qφi (hi |hi−1 )]|hi−1 ]],
where we also used the fact that under the variational posterior, hi:n is independent of h1:i−2 and x, given hi−1 . As
a consequence of Eq. 9, when computing the expectation
w.r.t. Q(hi:n |hi−1 ), all the terms in the learning signal that
do not depend on hi:n can be safely dropped without affecting the result. This gives us the following local learning
signal for layer i:
lφi (x, h) = log Pθ (hi−1:n ) − log Qφ (hi:n |hi−1 ).

(12)

To get the signal for the first hidden layer we simply use x
in place of h0 , in which case we simply recover the global
learning signal. For hidden layers i > 1, however, the local signal involves fewer terms than lφ (x, h) and thus can
be expected to be less noisy. As we do not assume any
within-layer structure, Eq. 12 applies to models and inference network whether or not Qφ (hi |hi−1 ) and Pθ (hi |hi+1 )
are factorial.
Since local signals can be significantly different from each
other, we use separate baselines and variance estimates for
each signal. For layers i > 1, the input-dependent baseline
Cψ (x) is replaced by Cψi i (hi−1 ).
In some cases, further simplification of the learning signal
is possible, yielding a different signal per latent variable.
We leave exploring this as future work.

3. Related work
3.1. Feedforward approximations to inference
The idea of training an approximate inference network
by optimizing a variational lower bound is not new. It
goes back at least to Hinton & Zemel (1994), who derived the variational objective from the Minimum Description Length (MDL) perspective and used it to train linear
autoencoders. Their probabilistic encoder and decoder correspond to our inference network and model respectively.
However, they computed the gradients analytically, which
was possible due to the simplicity of their model, and dismissed the sampling-based approach as infeasible due to
noise.
Salakhutdinov & Larochelle (2010) proposed using a feedforward “recognition” model to perform efficient inputdependent initialization for the mean field inference algorithm in deep Boltzmann machines. As the recognition

Neural Variational Inference and Learning in Belief Networks

model is trained to match the marginal probabilities produced by mean field inference, it inherits the limitations
of the inference procedure, such as the inability to model
structured posteriors. In contrast, in NVIL the inference
net is trained to match the true posterior directly, without
involving an approximate inference algorithm, and thus the
accuracy of the fit is limited only by the expressiveness of
the inference network itself.
Recently a method for training nonlinear models with continuous latent variables, called Stochastic Gradient Variational Bayes (SGVB), has been proposed by Kingma &
Welling (2013) and Rezende et al. (2014). Like NVIL, it
involves using feedforward models to perform approximate
inference and trains them by optimizing a sampling-based
estimate of the variational bound on the log-likelihood.
However, SGVB is considerably less general than NVIL,
because it uses a gradient estimator obtained by taking advantage of special properties of real-valued random variables and thus is not applicable to models with discrete
random variables. Moreover, unlike NVIL, SGVB method
cannot handle inference networks with nonlinear dependencies between latent variables. The ideas of the two
methods are complementary however, and NVIL is likely
to benefit from the SGVB-style treatment of continuousvalued variables, while SGVB might converge faster using
the variance reduction techniques we proposed.
Gregor et al. (2013) have recently proposed a related algorithm for training sigmoid belief network like models
based on the MDL framework. They also use a feedforward model to perform approximate inference, but concentrate on the case of a deterministic inference network and
can handle only binary latent variables. The inference network is trained by backpropagating through binary thresholding units, ignoring the thresholding nonlinearities, to approximately minimize the coding cost of the joint latentvisible configurations. This approach can be seen as approximately maximizing a looser variational lower bound
than (2) due to the absence of the entropy term.

rate set of variational parameters for each observation and
does not use an inference network. Moreover, BBVI uses a
fully-factorized mean field approximation to the posterior,
which limits its power.
3.3. The wake-sleep algorithm
NVIL shares many similarities with the wake-sleep algorithm (Hinton et al., 1995), which enjoys the same scalability and applicability to a wide range of models. This
algorithm was introduced for training Helmholtz machines
(Dayan et al., 1995), which are multi-layer belief networks
augmented with recognition networks. These recognition
networks are used for approximate inference and are directly analogous to NVIL inference networks. Wake-sleep
alternates between updating the model parameters in the
wake phase and the recognition network parameters in the
sleep phase. The model parameter update is based on the
samples generated from the recognition network on the
training data and is identical to the NVIL one (Eq. 5). However, in contrast to NVIL, the recognition network parameters are learned from samples generated by the model. In
other words, the recognition network is trained to recover
the hidden causes corresponding to the samples from the
model distribution by following the gradient
∇φ L(x) = EPθ (x,h) [∇φ log Qφ (h|x)] .

(13)

Unfortunately, this update does not optimize the same objective as the model parameter update, which means that
the wake-sleep algorithm does not optimize a well-defined
objective function and is not guaranteed to converge. This
is the algorithm’s main weakness, compared to NVIL,
which optimizes a variational lower bound on the loglikelihood.

An inference network for efficient generation of samples
from the approximate posterior can also be seen as a probabilistic generalization of the approximate feedforward inference methods developed for sparse coding models in the
last few years (Kavukcuoglu et al., 2008; Bradley & Bagnell, 2008; Gregor & LeCun, 2010).

The wake-sleep gradient for recognition network parameters does have the advantage of being much easier to estimate than the corresponding gradient of the variational
bound. In fact, the idea of training the recognition networks using the gradient of the bound was mentioned in
(Hinton & Zemel, 1994) and (Dayan & Hinton, 1996) but
not seriously considered due concerns about the high variance of the estimates. In Section 4.2 we show that while
the naive estimator of the gradient given in Eq. 6 does exhibit high variance, the variance reduction techniques from
Section 2.3 improve it dramatically and make it practical.

3.2. Sampling-based variational inference

3.4. REINFORCE

Like NVIL, Black Box Variational Inference (BBVI, Ranganath et al., 2013) learns the variational parameters of
the posterior by optimizing the variational bound using
sampling-based gradient estimates, which makes it applicable to a large range of models. However, unlike NVIL,
BBVI follows the traditional approach of learning a sepa-

Using the gradient (4) to train the inference network can
be seen as an instance of the REINFORCE algorithm
(Williams, 1992) from reinforcement learning (RL), which
adapts the parameters of a stochastic model to maximize
the external reward signal which depends on the model’s
output. Given a model Pθ (x) and a reward signal r(x),

Neural Variational Inference and Learning in Belief Networks

REINFORCE updates the model parameters using the rule
∆θ ∝ EP [(r(x) − b)∇θ log Pθ (x)].

(14)

We can view NVIL as an application of REINFORCE on
the per-training-case basis, with the inference network corresponding to the stochastic model, latent state h to the output, and the learning signal lφ (x, h) to the reward. The term
b in Eq. 14, called a baseline in the RL literature, is a hyperparameter that can be adapted to reduce the variance of
the parameter update. Thus it serves the same function as
c and Cψ (x) that we subtract from the learning signal to
center it in Section 2.3.1. The considerable body of work
on baselines and other variance reduction methods done in
the RL community (e.g. Greensmith et al., 2004) is likely
to contain additional techniques relevant for training inference networks.

4. Experimental results
We performed two sets of experiments, with the first set
intended to evaluate the effectiveness of our variance reduction techniques and to compare NVIL’s performance to
that of the wake-sleep algorithm. In the second set of experiments, we demonstrate NVIL’s ability to handle larger
real-world datasets by using it to train generative models of
documents.
4.1. Experimental protocol
We trained all models using stochastic gradient ascent using minibatches of 20 observations sampled randomly from
the training data. The gradient estimates were computed
using a single sample from the inference network. For each
dataset, we created a validation set by removing a random
subset of 100 observations from the training set. The only
form of regularization we used was early stopping based on
the validation bound, implemented by keeping track of the
parameter configuration with the best validation score seen
so far. We implemented each input-dependent baseline using a neural network with a single hidden layer of 100 tanh
units.
We used fixed learning rates because we found them to
produce superior results to the annealing schedules we experimented with. The learning rates we report were selected based on the validation set performance in preliminary experiments with smaller models. We always make
the learning rate for inference network five times smaller
than for the model (which is the one we report), as we found
this to improve performance. We used inference networks
with layered structure given by Eq. 11, without dependencies within each layer except in the experiment with autoregressive inference networks. All multi-layer inference
networks were trained using layer-specific learning signals
from Section 2.3.3.

As the models we train are intractable, we cannot compute
the exact log-likelihoods for them. Instead we report the
estimates of the variational bound (2) computed using 10
samples from the inference network, which we found to be
sufficient to get the accurate bound estimates. We expect
this approach to underestimate the log-likelihood considerably, but leave finding more direct and thus less pessimistic
evaluation methods as future work.
4.2. Modelling images of digits
Our first set of experiments was performed on the binarized
version of the MNIST dataset, which has become the standard benchmark for evaluating generative models of binary
data. The dataset consists of 70,000 28 × 28 binary images of handwritten digits, partitioned into a 60,000-image
training set and 10,000-image test set. We used the binarization of Salakhutdinov & Murray (2008), which makes
our scores directly comparable to those in the literature.
We used 3 × 10−4 as the learning rate for training models with NVIL on this dataset. Centering the input vectors
by subtracting the mean vector was essential for making
the inference networks and input-dependent baselines work
well.
To demonstrate the importance of variance reduction techniques, we trained two SBNs using a range of variance control settings. The first SBN had a single layer of 200 latent
variables, while the second one had two layers of 200 variables each. Figure 1 shows the estimate of the variational
objective on the validation set plotted against the number
of parameter updates. For both models, it is clear that using all three techniques – the input-dependent and inputindependent baselines along with variance normalization –
is essential for best performance. However, of the three
techniques, the input-dependent baseline appears to be the
least important. Comparing the plots for the two models
suggests that variance reduction becomes more important
for larger models, with the gap between the best combination and the others (excluding the very worst one) widening. For both models, learning with all three variance reduction techniques disabled makes barely any progress and
is clearly infeasible.
We found that disabling layer-specific learning signals had
little effect on the performance of the resulting model. The
difference was about 0.4 nats for an SBN with two or three
layers of latent variables.
We next compared NVIL to the wake-sleep algorithm,
which is its closest competitor in terms of scalability and
breadth of applicability, by training a range of models using both algorithms. Wake-sleep training used a learning
rate of 1 × 10−4 , as we found this algorithm to be more
sensitive to the choice of the learning rate than NVIL, per-

Neural Variational Inference and Learning in Belief Networks
SBN 200−200
−100

−120

−120

−140

−140
Validation set bound

Validation set bound

SBN 200
−100

−160

−180

−200

−180

−200

Baseline, IDB, & VN
Baseline & VN
Baseline only
VN only
No baselines & no VN

−220

−240

−160

0

200

400

600

800
1000
1200
Number of parameter updates

1400

1600

1800

2000

Baseline, IDB, & VN
Baseline & VN
Baseline only
VN only
No baselines & no VN

−220

−240

0

200

400

600

800
1000
1200
Number of parameter updates

1400

1600

1800

2000

Figure 1. Bounds on the validation set log-likelihood for an SBN with (Left) one and (Right) two layers of 200 latent variables. Baseline
and IDB refer to the input-independent and the input-dependent baselines respectively. VN is variance normalization.

Table 1. Results on the binarized MNIST dataset. “Dim” is the
number of latent variables in each layer, starting with the deepest
one. NVIL and WS refer to the models trained with NVIL and
wake-sleep respectively. NLL is the negative log-likelihood for
the tractable models and an estimate of it for the intractable ones.
M ODEL
SBN
SBN
SBN
SBN
SBN
F DARN
F DARN
F DARN
DARN
NADE
RBM (CD3)
RBM (CD25)
MOB

D IM
200
500
200-200
200-200-200
200-200-500
200
500
400
400
500
500
500
500

T EST NLL
NVIL
WS
113.1 120.8
112.8 121.4
99.8 107.7
96.7 102.2
97.0 102.3
92.5
95.9
90.7
97.2
96.3
93.0
88.9
105.5
86.3
137.6

forming considerably better with lower learning rates. The
results, along with some baselines from the literature, are
shown in Table 1. We report only the means of the bound
estimates as their standard deviations were all very small,
none exceeding 0.1 nat. We can see that models trained
with NVIL have considerably better bounds on the loglikelihood, compared to their wake-sleep counterparts, with
the difference ranging from 3.4 to 8.6 nats. Additional layers make SBNs perform better, independently of the training method. Interestingly, single-layer fDARN (Gregor
et al., 2013) models, which have autoregressive connections between the latent variables, perform better than any
of the SBN models trained using the same algorithm. Comparing to results from the literature, we see that all the SBN

and fDARN models we trained perform much better than a
mixture of 500 factorial Bernoulli distributions (MoB) but
not as well as the deterministic Neural Autoregressive Distribution Estimator (NADE) (Larochelle & Murray, 2011).
The NVIL-trained fDARN models with 200 and 500 latent
variables also outperform the fDARN (as well as the more
expressive DARN) model with 400 latent variables from
(Gregor et al., 2013), which were trained using an MDLbased algorithm. The fDARN and multi-layer SBN models trained using NVIL also outperform a 500-hidden-unit
RBM trained with 3-step contrastive divergence (CD), but
not the one trained with 25-step CD (Salakhutdinov & Murray, 2008). However, both sampling and CD-25 training in
an RBM is considerably more expensive than sampling or
NVIL training for any of our models.
The sampling-based approach to computing gradients allows NVIL to handle variational posteriors with complex
dependencies. To demonstrate this ability, we retrained
several of the SBN models using inference networks with
autoregressive connections within each layer. These networks can capture the dependencies between variables
within layers and thus are considerably more expressive
than the ones with factorial layers. Results in Table 2 indicate that using inference networks with autoregressive
connections produces better models, with the single-layer
models exhibiting large gains.
4.3. Document modelling
We also applied NVIL to the more practical task of document modelling. The goal is to train a generative model
of documents which are represented as vectors of word
counts, also known as bags of words. We trained two sim-

Neural Variational Inference and Learning in Belief Networks
Table 2. The effect of using autoregressive connections in the inference network. “Dim” is the number of latent variables in each
layer, starting with the deepest one. “Test NLL” is an estimate
of the lower bound on the log-likelihood on the MNIST test set.
”Autoreg” and “Factorial” refer to using inference networks with
and without autoregressive connections respectively.
M ODEL
SBN
SBN
SBN
SBN

D IM
200
500
200-200-200
200-200-500

T EST NLL
AUTOREG FACTORIAL
103.8
113.1
104.4
112.8
94.5
96.7
96.0
97.0

ple models on the 20 Newsgroups and Reuters Corpus Volume I (RCV1-v2) datasets, which have been used to evaluate similar models in (Salakhutdinov & Hinton, 2009b;
Larochelle & Lauly, 2012). 20 Newsgroups is a fairly small
dataset of Usenet newsgroup posts, consisting of about 11K
training and 7.5K test documents. RCV1 is a much larger
dataset of Reuters newswire articles, with about 794.4K
training and 10K test documents. We use the standard preprocessed versions of the datasets from Salakhutdinov &
Hinton (2009b), which have vocabularies of 2K and 10K
words respectively.
We experimented with two simple document models, based
on the SBN and DARN architectures. Both models had a
single layer of latent variables and a multinomial visible
layer and can be seen as directed counterparts of the Replicated Softmax model (Salakhutdinov & Hinton, 2009b).
We used the same training procedure as on MNIST with
the exception of the learning rates which were 3 × 10−5 on
20 Newsgroups and 10−3 on RCV1.
The established evaluation metric for such models is the perplexity 
per word, computed as
P
exp − N1 n L1n log P (xn ) , where N is the number of documents, Ln is the length of document n, and
P (xn ) the probability of the document under the model.
As we cannot compute log P (xn ), we use the variational
lower bound in its place and thus report an upper bound on
perplexity.
The results for our models, along with ones for the Replicated Softmax and DocNADE models from (Salakhutdinov
& Hinton, 2009b) and (Larochelle & Lauly, 2012) respectively, are shown in Table 3. We can see that the SBN and
fDARN models with 50 latent variables perform well, producing better scores than LDA and Replicated Softmax on
both datasets. Their performance is also competitive with
that of DocNADE on 20 Newsgroups. The score of 724 for
fDARN with 50 latent variables on RCV1 is already better than DocNADE’s 742, the best published result on that
dataset. fDARN with 200 hidden units, however, performs

Table 3. Document modelling results. “Dim” is the number of
latent variables in the model. The third and the fourth columns
report the estimated test set perplexity on the 20 Newsgroups and
Reuters RCV1 datasets respectively.
M ODEL
SBN
F DARN
F DARN
LDA
LDA
R EP S OFT M AX
D OC NADE

D IM
50
50
200
50
200
50
50

20 N EWS
909
917
1091
1058
953
896

R EUTERS
784
724
598
1437
1142
988
742

even better, setting a new record with 598.

5. Discussion and future work
We developed, NVIL, a new training method for intractable
directed latent variable models which is general and easy to
apply to new models. We showed that NVIL consistently
outperforms the wake-sleep algorithm at training sigmoidbelief-network-like models. Finally, we demonstrated the
potential of our approach by achieving state-of-the-art results on a sizable dataset of documents (Reuters RCV1).
As the emphasis of this paper is on the training method, we
applied it to some of the simplest possible model and inference network architectures, which was sufficient to obtain promising results. We believe that considerable performance gains can be made by using more expressive architectures, such as those with nonlinearities between layers of
stochastic variables. Applying NVIL to models with continuous latent variables is another promising direction since
binary latent variables are not always appropriate.
We expect NVIL to be also applicable to training conditional latent variable models for modelling the distribution
of observations given some context, which would require
making the inference network take both the context and
the observation as input. This would make it an alternative to the importance-sampling training method of Tang
& Salakhutdinov (2013) for conditional models with structured high-dimensional outputs.
We hope that the generality and flexibility of our approach
will make it easier to apply powerful directed latent variable models to real-world problems.
ACKNOWLEDGEMENTS
We thank Koray Kavukcuoglu, Volodymyr Mnih, and
Nicolas Heess for their helpful comments. We thank Ruslan Salakhutdinov for providing us with the preprocessed
document datasets.

Neural Variational Inference and Learning in Belief Networks

References
Bradley, David M and Bagnell, J Andrew. Differential
sparse coding. In Advances in Neural Information Processing Systems, volume 20, 2008.
Dayan, Peter and Hinton, Geoffrey E.
Varieties of
helmholtz machine. Neural Networks, 9(8):1385–1403,
1996.
Dayan, Peter, Hinton, Geoffrey E, Neal, Radford M, and
Zemel, Richard S. The helmholtz machine. Neural computation, 7(5):889–904, 1995.
Greensmith, Evan, Bartlett, Peter L., and Baxter, Jonathan.
Variance reduction techniques for gradient estimates in
reinforcement learning. Journal of Machine Learning
Research, 5:1471–1530, 2004.
Gregor, Karol and LeCun, Yann. Learning fast approximations of sparse coding. In Proc. International Conference
on Machine learning (ICML’10), 2010.
Gregor, Karol, Mnih, Andriy, and Wierstra, Daan. Deep autoregressive networks. arXiv preprint arXiv:1310.8499,
2013.
Hinton, Geoffrey E and Zemel, Richard S. Autoencoders,
minimum description length, and Helmholtz free energy.
In Advances in Neural Information Processing Systems,
1994.
Hinton, Geoffrey E, Dayan, Peter, Frey, Brendan J, and
Neal, Radford M. The "wake-sleep" algorithm for unsupervised neural networks. Science, 268(5214):1158–
1161, 1995.
Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee Whye.
A fast learning algorithm for deep belief nets. Neural
Computation, 18(7):1527–1554, 2006.
Jordan, Michael I., Ghahramani, Zoubin, Jaakkola,
Tommi S., and Saul, Lawrence K. An introduction
to variational methods for graphical models. Machine
Learning, 37(2):183–233, 1999.
Kavukcuoglu, Koray, Ranzato, Marc’Aurelio, and LeCun,
Yann. Fast inference in sparse coding algorithms with
applications to object recognition. Technical report,
Courant Institute, NYU, 2008.
Kingma, Diederik P and Welling, Max. Auto-encoding
variational bayes. arXiv preprint arXiv:1312.6114,
2013.
Larochelle, Hugo and Lauly, Stanislas. A neural autoregressive topic model. In Advances in Neural Information
Processing Systems, pp. 2717–2725, 2012.

Larochelle, Hugo and Murray, Iain. The neural autoregressive distribution estimator. JMLR: W&CP, 15:29–37,
2011.
Neal, Radford M. Connectionist learning of belief networks. Artificial intelligence, 56(1):71–113, 1992.
Paisley, John William, Blei, David M., and Jordan,
Michael I. Variational bayesian inference with stochastic
search. In ICML, 2012.
Ranganath, Rajesh, Gerrish, Sean, and Blei, David M.
Black box variational inference.
arXiv preprint
arXiv:1401.0118, 2013.
Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic back-propagation and variational inference in deep latent gaussian models. arXiv preprint
arXiv:1401.4082, 2014.
Salakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltzmann machines. In International Conference on Artificial Intelligence and Statistics, pp. 448–455, 2009a.
Salakhutdinov, Ruslan and Hinton, Geoffrey E. Replicated
softmax: an undirected topic model. In Advances in neural information processing systems, 2009b.
Salakhutdinov, Ruslan and Larochelle, Hugo. Efficient
learning of deep boltzmann machines. In International
Conference on Artificial Intelligence and Statistics, pp.
693–700, 2010.
Salakhutdinov, Ruslan and Murray, Iain. On the quantitative analysis of Deep Belief Networks. In Proceedings
of the 25th Annual International Conference on Machine
Learning (ICML 2008), 2008.
Saul, Lawrence K., Jaakkola, Tommi, and Jordan,
Michael I. Mean field theory for sigmoid belief networks. Journal of Artificial Intelligence Research, 4:61–
76, 1996.
Tang, Yichuan and Salakhutdinov, Ruslan. Learning
stochastic feedforward neural networks. In Advances in
Neural Information Processing Systems, 2013.
Weaver, Lex and Tao, Nigel. The optimal reward baseline
for gradient-based reinforcement learning. In In Proceedings of the Seventeenth Conference on Uncertainty
in Artificial Intelligence, 2001.
Williams, Ronald J. Simple statistical gradient-following
algorithms for connectionist reinforcement learning.
Machine learning, 8(3-4):229–256, 1992.

