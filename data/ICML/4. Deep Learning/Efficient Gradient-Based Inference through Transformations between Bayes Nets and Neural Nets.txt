Efficient Gradient-Based Inference through
Transformations between Bayes Nets and Neural Nets

Diederik P. Kingma
Max Welling
Machine Learning Group, University of Amsterdam

Abstract
Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly
perceived as two separate types of models. We
show that either of these types of models can often be transformed into an instance of the other,
by switching between centered and differentiable
non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each
parameterization is preferred and show how inference can be made robust. In the non-centered
form, a simple Monte Carlo estimator of the
marginal likelihood can be used for learning the
parameters. Theoretical results are supported by
experiments.

1. Introduction
Bayesian networks (also called belief networks) are probabilistic graphical models where the conditional dependencies within a set of random variables are described by a
directed acyclic graph (DAG). Many supervised and unsupervised models can be considered as special cases of
Bayesian networks.
In this paper we focus on the problem of efficient inference in Bayesian networks with multiple layers of continuous latent variables, where exact posterior inference is intractable (e.g. the conditional dependencies between variables are nonlinear) but the joint distribution is differentiable. Algorithms for approximate inference in Bayesian
networks can be roughly divided into two categories: sampling approaches and parametric approaches. Parametric
approaches include Belief Propagation (Pearl, 1982) or the
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

D . P. KINGMA @ UVA . NL
M . WELLING @ UVA . NL

more recent Expectation Propagation (EP) (Minka, 2001).
When it is not reasonable or possible to make assumptions
about the posterior (which is often the case), one needs
to resort to sampling approaches such as Markov Chain
Monte Carlo (MCMC) (Neal, 1993). In high-dimensional
spaces, gradient-based samplers such as Hybrid Monte
Carlo (Duane et al., 1987) and the recently proposed noU-turn sampler (Hoffman & Gelman, 2011) are known for
their relatively fast mixing properties. When just interested
in finding a mode of the posterior, vanilla gradient-based
optimization methods can be used. The alternative parameterizations suggested in this paper can dramatically improve the efficiency of any of these algorithms.
1.1. Outline of the paper
After reviewing background material in 2, we introduce
a generally applicable differentiable reparameterization
of continuous latent variables into a differentiable noncentered form in section 3. In section 4 we analyze the
posterior dependencies in this reparameterized form. Experimental results are shown in section 6.

2. Background
Notation. We use bold lower case (e.g. x or y) notation
for random variables and instantiations (values) of random
variables. We write pÎ¸ (x|y) and pÎ¸ (x) to denote (conditional) probability density (PDF) or mass (PMF) functions
of variables. With Î¸ we denote the vector containing all
parameters; each distribution in the network uses a subset
of Î¸â€™s elements. Sets of variables are capitalized and bold,
matrices are capitalized and bold, and vectors are written
in bold and lower case.
2.1. Bayesian networks
A Bayesian network models a set of random variables V
and their conditional dependencies as a directed acyclic
graph, where each variable corresponds to a vertex and
each edge to a conditional dependency. Let the distribu-

Transformations between Bayes Nets and Neural Nets

variables can be easily acquired using the equality:

j

âˆ‡z log pÎ¸ (z|x) = âˆ‡z log pÎ¸ (x, z)
Â·Â·Â·

zj

Â·Â·Â·

zj

Â·Â·Â·

(a)

Â·Â·Â·

Figure 1. (a) The centered parameterization (CP) of a latent variable zj . (b) The differentiable non-centered parameterization
(DNCP) where we have introduced an auxiliary â€™noiseâ€™ variable j âˆ¼ pÎ¸ (j ) such that zj becomes deterministic: zj =
gj (paj , j , Î¸). This deterministic variable has an interpretation
of a hidden layer in a neural network, which can be differentiated
efficiently using the backpropagation algorithm.

tion of each variable vj be pÎ¸ (vj |paj ), where we condition on vj â€™s (possibly empty) set of parents paj . Given
the factorization property of Bayesian networks, the joint
distribution over all variables is simply:
N
Y
j=1

pÎ¸ (vj |paj )

N
X
j=1

(b)

pÎ¸ (v1 , . . . , vN ) =

=

(1)

âˆ‡z log pÎ¸ (vj |paj )

(3)

In words, the gradient of the log-posterior w.r.t. the latent
variables is simply the sum of gradients of individual factors w.r.t. the latent variables. These gradients can then be
followed to a mode if one is interested in finding a MAP
solution. If one is interested in sampling from the posterior then the gradients can be plugged into a gradient-based
sampler such as Hybrid Monte Carlo (Duane et al., 1987);
if also interested in learning parameters, the resulting samples can be used for the E-step in Monte Carlo EM (Wei &
Tanner, 1990) (MCEM).
Problems arise when strong posterior dependencies exist
between latent variables. From eq. (3) we can see that the
Hessian H of the posterior is:
H = âˆ‡z âˆ‡Tz log pÎ¸ (z|x) =

N
X
j=1

âˆ‡z âˆ‡Tz log pÎ¸ (vj |paj )
(4)

Let the graph consist of one or more (discrete or continuous) observed variables xj and continuous latent variables zj , with corresponding conditional distributions
pÎ¸ (xj |paj ) and pÎ¸ (zj |paj ). We focus Ron the case where
both the marginal likelihood pÎ¸ (x) = z pÎ¸ (x, z) dz and
the posterior pÎ¸ (z|x) are intractable to compute or differentiate directly w.r.t. Î¸ (which is true in general), and where
the joint distribution pÎ¸ (x, z) is at least once differentiable,
so it is still possible to efficiently compute first-order partial
derivatives âˆ‡Î¸ log pÎ¸ (x, z) and âˆ‡z log pÎ¸ (x, z).
2.2. Conditionally deterministic variables
A conditionally deterministic variable vj with parents paj
is a variable whose value is a (possibly nonlinear) deterministic function gj (.) of the parents and the parameters:
vj = gj (paj , Î¸). The PDF of a conditionally deterministic variable is a Dirac delta function, which we define as a
Gaussian PDF N (.; Âµ, Ïƒ) with infinitesimal Ïƒ:
pÎ¸ (vj |paj ) = lim N (vj ; gj (paj , Î¸), Ïƒ)
Ïƒâ†’0

(2)

Suppose a factor log pÎ¸ (zi |zj ) connecting two scalar latent
variables zi and zj exists, and zi is strongly dependent on
2
pÎ¸ (z|x)
zj , then the Hessianâ€™s corresponding element âˆ‚ log
âˆ‚zi âˆ‚zj
will have a large (positive or negative) value. This is bad
for gradient-based inference since it means that changes
âˆ‚ log pÎ¸ (zi |zj )
in zj have a large effect on the gradient
âˆ‚zi
and changes in zi have a large effect on the gradient
âˆ‚ log pÎ¸ (zi |zj )
. In general, strong conditional dependencies
âˆ‚zj
lead to ill-conditioning of the posterior, resulting in smaller
optimal stepsizes for first-order gradient-based optimization or sampling methods, making inference less efficient.

3. The differentiable non-centered
parameterization (DNCP)
In this section we introduce a generally applicable transformation between continuous latent random variables and
deterministic units with auxiliary parent variables. In rest
of the paper we analyze its ramifications for gradient-based
inference.

which equals +âˆ when Rvj = gj (paj , Î¸) and equals 0
everywhere else such that vj pÎ¸ (vj |paj ) dvj = 1.

3.1. Parameterizations of latent variables

2.3. Inference problem under consideration

Let zj be some continuous latent variable with parents paj ,
and corresponding conditional PDF:

We are often interested in performing posterior inference,
which most frequently consists of either optimization (finding a mode argmaxz pÎ¸ (z|x)) or sampling from the posterior pÎ¸ (z|x). Gradients of the log-posterior w.r.t. the latent

zj |paj âˆ¼ pÎ¸ (zj |paj )

(5)

This is also known in the statistics literature as the centered
parameterization (CP) of the latent variable zj . Let the

Transformations between Bayes Nets and Neural Nets

differentiable non-centered parameterization (DNCP) of
the latent variable zj be:
zj = gj (paj , j , Î¸) where

j âˆ¼ p(j )

(6)

where gj (.) is some differentiable function. Note that in
the DNCP, the value of zj is deterministic given both paj
and the newly introduced auxiliary variable j which is distributed as p(j ). See figure 1 for an illustration of the two
parameterizations.
By the change of variables, the relationship between the
original PDF pÎ¸ (zj |paj ), the function gj (paj , j ) and the
PDF p(j ) is:
p(j ) = pÎ¸ (zj = gj (paj , j , Î¸)|paj ) |det(J)|

(7)

where det(J) is the determinant of Jacobian of gj (.) w.r.t.
j . If zj is a scalar variable, then j is also scalar and
âˆ‚z
|det(J)| = | âˆ‚jj |.
In the DNCP, the original latent variable zj has become
deterministic, and its PDF pÎ¸ (zj |paj , j ) can be described
as a Dirac delta function (see section 2.2).
The joint PDF over the random and deterministic variables
can be integrated w.r.t. the determinstic variables. If for
simplicity we assume that observed variables are always
leaf nodes of the network, and that all latent variables are
reparameterized such that the only random variables left
are the observed and auxiliary variables x and , then the
marginal joint pÎ¸ (x, ) is obtained as follows:
Z
pÎ¸ (x, ) = pÎ¸ (x, z, ) dz
Z Y z
Y
Y
=
pÎ¸ (xj |paj )
pÎ¸ (zj |paj , j )
p(j ) dz
z j

=

Y

=

Y

j

j

j

pÎ¸ (xj |paj )

Y

pÎ¸ (xj |paj )

Y

where

p(j )

2. For any â€location-scaleâ€ family of distributions (with
differentiable log-PDF) we can choose the standard
distribution (with location = 0, scale = 1) as the auxiliary variable j , and let gj (.) = location + scale Â· j .
Examples: Gaussian, Uniform, Laplace, Elliptical,
Studentâ€™s t, Logistic and Triangular distributions.
3. Composition: It is often possible to express variables
as functions of component variables with different
distributions. Examples: Log-Normal (exponentiation of normally distributed variable), Gamma (a sum
over exponentially distributed variables), Beta distribution, Chi-Squared, F distribution and Dirichlet distributions.
When the distribution is not in the families above, accurate differentiable approximations to the inverse CDF can
be constructed, e.g. based on polynomials, with time complexity comparable to the CP (see e.g. (Devroye, 1986) for
some methods).
For the exact approaches above, the CP and DNCP forms
have equal time complexities. In practice, the difference in
CPU time depends on the relative complexity of computing
derivatives of log pÎ¸ (zj |paj ) versus computing gj (.) and
derivatives of log p(j ), which can be easily verified to be
similar in most cases below. Iterations with the DNCP form
were slightly faster in our experiments.
3.3. DNCP and neural networks

j

Z Y
z j

j

F âˆ’1 (zj |paj ; Î¸) be the inverse CDF of the conditional
distribution. Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal, Gompertz,
Gumbel and Erlang distributions.

pÎ¸ (zj |paj , j ) dz

p(j )

j

zk = gk (pak , k , Î¸)
(8)

In the last step of eq. (8), the inputs paj to the factors of observed variables pÎ¸ (xj |paj ) are defined in terms of functions zk = gk (.), whose values are all recursively computed from auxiliary variables .
3.2. Approaches to DNCPs
There are a few basic approaches to transforming CP of a
latent variable zj to a DNCP:
1. Tractable and differentiable inverse CDF. In this
case, let j âˆ¼ U(0, 1), and let gj (zj , paj , Î¸) =

It is instructive to interpret the DNCP form of latent variables as â€hidden unitsâ€ of a neural network. The network
of hidden units together form a neural network with inserted noise , which we can differentiate efficiently using
the backpropagation algorithm (Rumelhart et al., 1986).
There has been recent increase in popularity of deep neural networks with stochastic hidden units (e.g. (Krizhevsky
et al., 2012; Goodfellow et al., 2013; Bengio, 2013)). Often, the parameters Î¸ of such neural networks are optimized towards maximum-likelihood objectives. In that
case, the neural network can be interpreted as a probabilistic model log pÎ¸ (t|x, ) computing a conditional distribution over some target variable t (e.g. classes) given some
input x. In (Bengio & Thibodeau-Laufer, 2013), stochastic hidden units are used for learning the parameters of
a Markov chain transition operator that samples from the
data distribution.
For example, in (Hinton et al., 2012) a â€™dropoutâ€™ regularization method is introduced where (in its basic ver-

Transformations between Bayes Nets and Neural Nets

sion) the activation of hidden units zj is computed as zj =
j Â· f (paj ) with j âˆ¼ p(j ) = Bernoulli(0.5), and where
the parameters are learned by following
 the gradient of the

log-likelihood lower bound: âˆ‡Î¸ E log pÎ¸ (t(i) |x(i) , ) ;
this gradient can sometimes be computed exactly (Maaten
et al., 2013) and can otherwise be approximated with a
Monte Carlo estimate (Hinton et al., 2012). The two parameterizations explained in section 3.1 offer us a useful
new perspective on â€™dropoutâ€™. A â€™dropoutâ€™ hidden unit
(together with its injected noise ) can be seen as the
DNCP of latent random variables, whose CP is zj |paj âˆ¼
pÎ¸ (zj = j Â· f (paj )|paj )). A practical implication is that
â€™dropoutâ€™-type neural networks can therefore be interpreted
and treated as hierarchical Bayes nets, which opens the
door to alternative approaches to learning the parameters,
such as Monte Carlo EM or variational methods.
While â€™dropoutâ€™ is designed as a regularization method,
other work on stochastic neural networks exploit the
power of stochastic hidden units for generative modeling,
e.g. (Frey & Hinton, 1999; Rezende et al., 2014; Tang &
Salakhutdinov, 2013) applying (partially) MCMC or (partically) factorized variational approaches to modelling the
posterior. As we will see in sections 4 and 6, the choice
of parameterization has a large impact on the posterior dependencies and the efficiency of posterior inference. However, current publications lack a good justification for their
choice of parameterization. The analysis in section 4 offers some important insight in where the centered or noncentered parameterizations of such networks are more appropriate.

1

2

3

z1

z2

z3

z1

z2

z3

x1

x2

x3

x1

x2

x3

(a)

(b)

Figure 2. (a) An illustrative hierarchical model in its centered parameterization (CP). (b) The differentiable non-centered parameterization (DNCP), where z1 = g1 (1 , Î¸), z2 = g2 (z1 , 2 , Î¸)
and z3 = g3 (z2 , 3 , Î¸), with auxiliary latent variables k âˆ¼
pÎ¸ (k ). The DNCP exposes a neural network within the hierarchical model, which we can differentiate efficiently using backpropagation.

Table 1. Limiting behaviour of squared correlations between z
and its parent yi when z is in the centered (CP) and non-centered
(DNCP) parameterizaton.
Ï2yi ,z (CP)

Ï2yi ,e (DNCP)

limÏƒâ†’0

1

0

limÏƒâ†’+âˆ

0

Î²wi2
Î²wi2 +Î±

limÎ²â†’0
limÎ²â†’âˆ’âˆ
limÎ±â†’0
limÎ±â†’âˆ’âˆ

wi2
wi2 âˆ’Î±Ïƒ 2

0

0
1

1
1âˆ’Î²Ïƒ 2

Î²Ïƒ 2
Î²Ïƒ 2 âˆ’1

0

0

3.4. A differentiable MC likelihood estimator
We showed that many hierarchical continuous latentvariable models can be transformed into a DNCP pÎ¸ (x, ),
where all latent variables (the introduced auxiliary variables ) are root nodes (see eq. (8)). This has an important
implication for learning since (contrary to a CP) the DNCP
can be used to form a differentiable Monte Carlo estimator
of the marginal likelihood:
L

1 XY
(l)
pÎ¸ (xj |paj )
log pÎ¸ (x) ' log
L
j
l=1

(l)

where the parents paj of the observed variables are either root nodes or functions of root nodes whose values
are sampled from their marginal: (l) âˆ¼ p(). This MC
estimator can be differentiated w.r.t. Î¸ to obtain an MC estimate of the log-likelihood gradient âˆ‡Î¸ log pÎ¸ (x), which
can be plugged into stochastic optimization methods such
as Adagrad for approximate ML or MAP. When performed
one datapoint at a time, we arrive at our on-line Maximum
Monte Carlo Likelihood (MMCL) algorithm.

4. Effects of parameterizations on posterior
dependencies
What is the effect of the proposed reparameterization on the
efficiency of inference? If the latent variables have linearGaussian conditional distributions, we can use the metric
of squared correlation between the latent variable and any
of its children in their posterior distribution. If after reparameterization the squared correlation is decreased, then in
general this will also result in more efficient inference.
For non-linear Gaussian conditional distributions, the logPDF can be locally approximated as a linear-Gaussian using a second-order Taylor expansion. Results derived for
the linear case can therefore also be applied to the nonlinear case; the correlation computed using this approximation is a local dependency between the two variables.
Denote by z a scalar latent variable we are going to reparameterize, and by y its parents, where yi is one of the
parents. The log-PDF of the corresponding conditional dis-

Transformations between Bayes Nets and Neural Nets

the equation above, the squared correlation can be computed from the elements of the Hessian matrix:

Ïƒz = 0.02 (Ï â‰ˆ 1.00)

z2

Ïƒz = 1 (Ï â‰ˆ 0.41)

z2

z2

Ïƒz = 50 (Ï â‰ˆ 0.00)

2
2 2
Ï2 = (ÏƒAB
)2 /(ÏƒA
ÏƒB )

= (HAB /det(H))2 /((âˆ’HA /det(H))(âˆ’HB /det(H))
z1
Ïƒz = 0.02 (Ï â‰ˆ âˆ’0.01)

2
= HAB
/(HA HB )

e1

e1

e1

H = âˆ‡z âˆ‡Tz log pÎ¸ (z|x) = âˆ‡z âˆ‡Tz log pÎ¸ (x, z)

Figure 3. Plots of the log-posteriors of the illustrative linearGaussian model discussed in sec. 4.4. Columns: different choices
of Ïƒz , ranging from a low prior dependency (Ïƒz = 50) to a high
prior dependency (Ïƒz = 0.02). First row: CP form. Second row:
DNCP form. The posterior correlation Ï between the variables
is also displayed. In the original form a larger prior dependency
leads to a larger posterior dependency (see top row). The dependency in the DNCP posterior is inversely related to the prior
dependency between z1 and z2 (bottom row).

z = the variable to be reparameterized
y = zâ€™s parents
L(z) = log pÎ¸ (z|y) (zâ€™s factor)
L

A reparameterization of z using an auxiliary variable  is
z = g(.) = (wT y + b) + Ïƒ where  âˆ¼ N (0, 1). With (7)
it can be confirmed that this change of variables is correct:
 
 
 âˆ‚z 
 âˆ‚z 
pÎ¸ (z|y) Â·   = pÎ¸ (z = g(.)|y) Â·  
âˆ‚
âˆ‚
= N (wT y + b + Ïƒ|wT y + b, Ïƒ 2 ) Â· Ïƒz
âˆš
= âˆ’ exp(2 /2)/ 2Ï€ = N (0, 1)

= p()

âˆ‚ 2 L(\z)
âˆ‚yi âˆ‚yi
âˆ‚ 2 L(zâ†’)
Î²=
âˆ‚zâˆ‚z

4.1. Squared Correlations
4.1.1. C ENTERED CASE

(9)

The covariance C between two jointly Gaussian distributed
variables A and B equals the negative inverse of the Hessian matrix of the log-joint PDF:


= the factors of zâ€™s children

In the CP case, the relevant Hessian elements are as follows:

First we will derive expressions for the squared correlations
between z and its parents, for the CP and DNCP case, and
subsequently show how they relate.

2
ÏƒAB
2
ÏƒB

(all factors minus zâ€™s factor)

Î±=

= âˆ’(z âˆ’ wT y âˆ’ b)2 /(2Ïƒ 2 ) + constant

C=

L = log pÎ¸ (x, z) (sum of all factors)

(zâ†’)

log pÎ¸ (z|y) = log N (z|wT y + b, Ïƒ 2 )

2
ÏƒA
2
ÏƒAB

The following shorthand notation is used in this section:

L(\z) = L âˆ’ L(z)

tribution is



(10)

Important to note is that derivatives of the log-posterior
w.r.t. the latent variables are equal to the derivatives of logjoint w.r.t. the latent variables, therefore,

e2

e2

z1
Ïƒz = 1 (Ï â‰ˆ âˆ’0.41)

e2

z1
Ïƒz = 50 (Ï â‰ˆ âˆ’0.58)

âˆ’1

= âˆ’H

1
=
det(H)



âˆ’HB
HAB

HAB
âˆ’HA



The correlation Ï between two jointly Gaussian distributed
2
variables A and B is given by: Ï = ÏƒAB
/(ÏƒA ÏƒB ). Using

âˆ‚2L
âˆ‚ 2 L(z)
=Î±+
= Î± âˆ’ wi2 /Ïƒ 2
âˆ‚yi âˆ‚yi
âˆ‚yi âˆ‚yi
âˆ‚2L
âˆ‚ 2 L(z)
=
=Î²+
= Î² âˆ’ 1/Ïƒ 2
âˆ‚zâˆ‚z
âˆ‚zâˆ‚z
âˆ‚2L
âˆ‚ 2 L(z)
=
=
= wi /Ïƒ 2
(11)
âˆ‚yi âˆ‚z
âˆ‚yi âˆ‚z

Hyi yi =
Hzz
Hyi z

Therefore, using eq. (10), the squared correlation between
yi and z is:
Ï2yi ,z =

(Hyi z )2
wi2 /Ïƒ 4
=
Hyi yi Hzz
(Î± âˆ’ wi2 /Ïƒ 2 )(Î² âˆ’ 1/Ïƒ 2 )

(12)

Transformations between Bayes Nets and Neural Nets

4.1.2. N ON - CENTERED CASE

Table 2. Effective Sample Size (ESS) for different choices of
latent-variable variance Ïƒz , and for different samplers, after taking 4000 samples. Shown are the results for HMC samplers using
the CP and DNCP parameterizations, as well as a robust HMC
sampler.

In the DNCP case, the Hessian elements are:
âˆ‚ âˆ‚L(zâ†’)
âˆ‚2L
=Î±+
âˆ‚yi âˆ‚yi
âˆ‚yi âˆ‚yi


âˆ‚
âˆ‚L(zâ†’)
=Î±+
wi
= Î± + wi2 Î²
âˆ‚yi
âˆ‚z

Hyi yi =

log Ïƒz

âˆ‚2L
âˆ‚ 2 L(zâ†’)
âˆ‚ 2 log p()
=
+
= Ïƒ2 Î² âˆ’ 1
âˆ‚âˆ‚
âˆ‚âˆ‚
âˆ‚âˆ‚
âˆ‚2L
=
= Ïƒwi Î²
(13)
âˆ‚yi âˆ‚

H =
Hyi 

The squared correlation between yi and  is therefore:
Ï2yi , =

(Hyi  )2
Ïƒ 2 wi2 Î² 2
=
Hyi yi H
(Î± + wi2 Î²)(Ïƒ 2 Î² âˆ’ 1)

(14)

4.2. Correlation inequality
We can now compare the squared correlation, between z
and some parent yi , before and after the reparameterization.
Assuming Î± < 0 and Î² < 0 (i.e. L(\z) and L(zâ†’) are
concave, e.g. exponential families):
Ï2yi ,z > Ï2yi ,
wi2 /Ïƒ 4
Ïƒ 2 wi2 Î² 2
>
(Î± âˆ’ wi2 /Ïƒ 2 )(Î² âˆ’ 1/Ïƒ 2 )
(Î± + wi2 Î²)(Ïƒ 2 Î² âˆ’ 1)
2
4
wi /Ïƒ
wi2 Î² 2
>
(Î± âˆ’ wi2 /Ïƒ 2 )(Î² âˆ’ 1/Ïƒ 2 )
(Î± + wi2 Î²)(Î² âˆ’ 1/Ïƒ 2 )
4
Î²2
1/Ïƒ
>
(Î± âˆ’ wi2 /Ïƒ 2 )
(Î± + wi2 Î²)
Ïƒ âˆ’2 > âˆ’Î²

(15)

Thus we have shown the surprising fact that the correlation inequality takes on an extremely simple form where
the parent-dependent values Î± and wi play no role; the inequality only depends on two properties of z: the relative
strenghts of Ïƒ (its noisiness) and Î² (its influence on childrenâ€™s factors). Informally speaking, if the noisiness of zâ€™s
conditional distribution is large enough compared to other
factorsâ€™ dependencies on z, then the reparameterized form
is beneficial for inference.
4.3. A beauty-and-beast pair
Additional insight into the properties of the CP and DNCP
can be gained by taking the limits of the squared correlations (12) and (14). Limiting behaviour of these correlations is shown in table 1. As becomes clear in these limits, the CP and DNCP often form a beauty-and-beast pair:
when posterior correlations are high in one parameterization, they are low in the other. This is especially true in the

-5
-4.5
-4
-3.5
-3
-2.5
-2
-1.5
-1

CP

DNCP

ROBUST

2
26
10
225
386
542
406
672
1460

305
348
570
417
569
608
972
1078
1600

640
498
686
624
596
900
935
918
1082

limits of Ïƒ â†’ 0 and Î² â†’ âˆ’âˆ, where squared correlations
converge to either 0 or 1, such that posterior inference will
be extremely inefficient in either CP or DNCP, but efficient
in the other. This difference in shapes of the log-posterior
is illustrated in figure 3.
4.4. Example: Simple Linear Dynamical System
Take a simple model with scalar latent variables z1
and z2 , and scalar observed variables x1 and x2 .
The joint PDF is defined as p(x1 , x2 , z1 , z2 ) =
p(z1 )p(x1 |z1 )p(z2 |z1 )p(x2 |z2 ), where p(z1 ) = N (0, 1),
p(x1 |z1 ) = N (z1 , Ïƒx2 ), p(z2 |z1 ) = N (z1 , Ïƒz2 ) and
p(x2 |z2 ) = N (z2 , Ïƒx2 ). Note that the parameter Ïƒz determines the dependency between the latent variables, and
Ïƒx determines the dependency between latent and observed
variables.
We reparameterize z2 such that it is conditionally deterministic given a new auxiliary variable 2 . Let p(2 ) =
N (0, 1). let z2 = g2 (z1 , 2 , Ïƒz ) = z1 + Ïƒz Â· 2 and let
1 = z1 . See figure 3 for plots of the original and auxiliary
posterior log-PDFs, for different choices of Ïƒz , along with
the resulting posterior correlation Ï.
For what choice of parameters does the reparameterization
yield smaller posterior correlation? We use equation (15)
and plug in Ïƒ â† Ïƒz and âˆ’Î² â† Ïƒxâˆ’2 , which results in:
Ï21 ,2 > Ï2z1 ,z2

â‡’

Ïƒz2 < Ïƒx2

i.e. the posterior correlation in DNCP form Ï21 ,2 is smaller
when the latent-variable noise parameter Ïƒz2 is smaller than
the oberved-variable noise parameter Ïƒx2 . Less formally,
this means that the DNCP is preferred when the latent variable is more strongly coupled to the data (likelihood) then
to its parents.

Transformations between Bayes Nets and Neural Nets

pling is tractable, efficient sampling is possible by interleaving between centered and non-centered parameterizations, as was shown in (Yu & Meng, 2011).

4
3
2
1
0
âˆ’1
âˆ’2

0

500

1000

1500

1.0
0.8
0.6
0.4
0.2
0.0
2000 âˆ’600 âˆ’300

0

300

600

(a) Centered Parameterization (CP)

3
2
1
0
âˆ’1
âˆ’2
âˆ’3
âˆ’4

0

500

1000

(b) Differentiable
(DNCP)

1500

1.0
0.8
0.6
0.4
0.2
0.0
âˆ’0.2
2000 âˆ’600 âˆ’300

Non-Centered

Auxiliary variables are used for data augmentation (see
(Van Dyk & Meng, 2001) or slice sampling (Neal, 2003))
where, in contrast with our method, sampling is performed
in a higher-dimensional augmented space. Auxiliary variables are used in a similar form under the name exogenous variables in Structural Causal Models (SCMs) (Pearl,
2000). In SCMs the functional form of exogenous variables
is more restricted than our auxiliary variables. The concept
of conditionally deterministic variables has been used earlier in e.g. (Cobb & Shenoy, 2005), although not as a tool
for efficient inference in general Bayesian networks with
continuous latent variables. Recently, (Raiko et al., 2012)
analyzed the elements of the Hessian w.r.t. the parameters
in neural network context.
The differentiable reparameterization of latent variables in
this paper was introduced earlier in (Kingma & Welling,
2013) and independently in (Bengio, 2013), but these publications lacked a theoretic analysis of the impact on the
efficiency of inference. In (Kingma & Welling, 2013), the
reparameterization trick was used in an efficient algorithm
for stochastic variational inference and learning.

0

300

600

6. Experiments
Parameterization

Figure 4. Auto-correlation of HMC samples of the latent variables
for a DBN in two different parameterizations. Left on each figure are shown 2000 subsequent HMC samples of three randomly
chosen variables in the dynamic Bayesian network model. On the
right are shown the corresponding HMC sample auto-correlation
graphs. The DNCP resulted in much lower posterior dependencies and a dramatic drop in HMC sample auto-correlation.

5. Related work
This is, to the best of our knowledge, the first work to investigate the implications of the different differentiable noncentered parameterizations on the efficiency of gradientbased inference. However, the topic of centered vs noncentered parameterizations has been investigated for efficient (non-gradient based) Gibbs Sampling in work by Papaspiliopoulos et al. (2003; 2007), which also discusses
some strategies for constructing parameterization for those
cases. There have been some publications for parameterizations of specific models; (Gelfand et al., 1995), for example, discusses parameterizations of mixed models, and
(Meng & Van Dyk, 1998) investigate several rules for
choosing an appropriate parameterization for mixed-effects
models for faster EM. In the special case where Gibbs sam-

6.1. Nonlinear DBN
From the derived posterior correlations in the previous sections we can conclude that depending on the parameters of
the model, posterior sampling can be extremely inefficient
in one parameterization while it is efficient in the other.
When the parameters are known, one can choose the best
parameterization (w.r.t. posterior correlations) based on the
correlation inequality (15).
In practice, model parameters are often subject to change,
e.g. when optimizing the parameters with Monte Carlo
EM; in these situations where there is uncertainty over the
value of the model parameters, it is impossible to choose
the best parameterization in advance. The â€beauty-beastâ€
duality from section 4.3 suggests a solution in the form of
a very simple sampling strategy: mix the two parameterizations. Let QCP (z0 |z) be the MCMC/HMC proposal distribution based on pÎ¸ (z|x) (the CP), and let QDN CP (z0 |z)
be the proposal distribution based on pÎ¸ (|x) (the DNCP).
Then the new MCMC proposal distribution based on the
mixture is:
Q(z0 |z) = Ï Â· QCP (z0 |z) + (1 âˆ’ Ï) Â· QDN CP (z0 |z)
(16)
where we use Ï = 0.5 in experiments. The mixing efficiency might be half that of the oracle solution (where the

Transformations between Bayes Nets and Neural Nets

We applied a Hybrid Monte Carlo (HMC) sampler to a
Dynamic Bayesian Network (DBN) with nonlinear transition probabilities with the same structure as the illustrative
model in figure 2. The prior and conditional probabilities are: z1 âˆ¼ N (0, I), zt |ztâˆ’1 âˆ¼ N (tanh(Wz ztâˆ’1 +
bz ), Ïƒz2 I) and xt |zt âˆ¼ Bernoulli(sigmoid(Wx ztâˆ’1 )).
The parameters were intialized randomly by sampling from
N (0, I). Based on the derived limiting behaviour (see table 1, we can expect that such a network in CP can have
very large posterior correlations if the variance of the latent
variables Ïƒz2 is very small, resulting in slow sampling.
To validate this result, we performed HMC inference with
different values of Ïƒz2 , sampling the latent variables while
holding the parameters fixed. For HMC we used 10
leapfrog steps per sample, and the stepsize was automatically adjusted while sampling to obtain a HMC acceptance
rate of around 0.9. At each sampling run, the first 1000
HMC samples were thrown away (burn-in); the subsequent
4000 HMC samples were kept. To estimate the efficiency
of sampling, we computed the effective sample size (ESS);
see e.g. (Kass et al., 1998) for a discussion on ESS.
Results. See table 2 and figure 4 for results. It is clear that
the choice of parameterization has a large effect on posterior dependencies and the efficiency of inference. Sampling
was very inefficient for small values of Ïƒz in the CP, which
can be understood from the limiting behaviour in table 1.
6.2. Generative multilayer neural net
As explained in section 3.4, a hierarchical model in DNCP
form can be learned using a MC likelihood estimator which
can be differentiated and optimized w.r.t. the parameters
Î¸. We compare this Maximum Monte Carlo Likelihood
(MMCL) method with the MCEM method for learning the
parameters of a 4-layer hierarchical model of the MNIST
dataset, where x|z3 âˆ¼ Bernoulli(sigmoid(Wx z3 + bx ))
and zt |ztâˆ’1 âˆ¼ N (tanh(Wi ztâˆ’1 +bi ), Ïƒz2t I). For MCEM,
we used HMC with 10 leapfrog steps followed by a weight
update using Adagrad (Duchi et al., 2010). For MMCL,
we used L âˆˆ {10, 100, 500}. We observed that DNCP
was a better parameterization than CP in this case, in terms
of fast mixing. However, even in the DNCP, HMC mixed
very slowly when the dimensionality of latent space become too high. For this reason, z1 and z2 were given
a dimensionality of 3, while z3 was 100-dimensional but
noiseless (Ïƒz21 = 0) such that only z3 and z2 are random
variables that require posterior inference by sampling. The
model was trained on a small (1000 datapoints) and large
(50000 datapoints) version of the MNIST dataset.

âˆ’100
Marginal log-likelihood

optimal parameterization is known), nonetheless when taking into account the uncertainty over the parameters, the
expected efficiency of the mixture proposal can be better
than a single parameterization chosen ad hoc.

Ntrain = 1000

Ntrain = 50000
MCEM (train)
MCEM (test)
MMCL (500 samples) (train)
MMCL (500 samples) (test)
MMCL (10 samples) (train)
MMCL (10 samples) (test)
MMCL (100 samples) (train)
MMCL (100 samples) (test)

âˆ’150

âˆ’120

âˆ’160

âˆ’140

âˆ’170

âˆ’160

âˆ’180

âˆ’180
âˆ’200 0
10

âˆ’140

âˆ’190
101
102
Training time (Ã— 5 minutes)

âˆ’200 0
10

101

102

Figure 5. Performance of MMCL versus MCEM in terms of the
marginal likelihood, when learning the parameters of a generative
multilayer neural network (see section 6.2).

Results. We compared train- and testset marginal likelihood. See figure 5 for experimental results. As was expected, MCEM attains asymptotically better results. However, despite its simplicity, the on-line nature of MMCL
means it scales better to large datasets, and (contrary to
MCEM) is trivial to implement.

7. Conclusion
We have shown how Bayesian networks with continuous
latent variables and generative neural networks are related
through two different parameterizations of the latent variables: CP and DNCP. A key result is that the differentiable
non-centered parameterization (DNCP) of a latent variable
is preferred, in terms of its effect on decreased posterior
correlations, when the variable is more strongly linked to
the data (likelihood) then to its parents. Through theoretical analysis we have also shown that the two parameterizations are complementary to each other: when posterior correlations are large in one form, they are small in the other.
We have also illustrated that this theoretical result can be
exploited in practice by designing a MCMC strategy that
mixes between both parameterizations, making it robust to
situations where MCMC can otherwise be inefficient.

Acknowledgments
The authors thank the reviewers for their excellent feedback and Joris Mooij, Ted Meeds and Taco Cohen for invaluable discussions and input.

Transformations between Bayes Nets and Neural Nets

References
Bengio, Yoshua. Estimating or propagating gradients through
stochastic neurons. arXiv preprint arXiv:1305.2982, 2013.
Bengio, Yoshua and Thibodeau-Laufer, EÌric. Deep generative
stochastic networks trainable by backprop. arXiv preprint
arXiv:1306.1091, 2013.
Cobb, Barry R and Shenoy, Prakash P. Nonlinear deterministic
relationships in Bayesian networks. In Symbolic and Quantitative Approaches to Reasoning with Uncertainty, pp. 27â€“38.
Springer, 2005.
Devroye, Luc. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference on Winter simulation, pp. 260â€“265. ACM, 1986.
Duane, Simon, Kennedy, Anthony D, Pendleton, Brian J, and
Roweth, Duncan. Hybrid Monte Carlo. Physics letters B, 195
(2):216â€“222, 1987.

Minka, Thomas P. Expectation propagation for approximate
bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pp. 362â€“369.
Morgan Kaufmann Publishers Inc., 2001.
Neal, Radford M. Probabilistic inference using Markov Chain
Monte Carlo methods. 1993.
Neal, Radford M. Slice sampling. Annals of statistics, pp. 705â€“
741, 2003.
Papaspiliopoulos, Omiros, Roberts, Gareth O, and SkoÌˆld, Martin. Non-centered parameterisations for hierarchical models
and data augmentation. In Bayesian Statistics 7: Proceedings
of the Seventh Valencia International Meeting, pp. 307. Oxford
University Press, USA, 2003.
Papaspiliopoulos, Omiros, Roberts, Gareth O, and SkoÌˆld, Martin.
A general framework for the parametrization of hierarchical
models. Statistical Science, pp. 59â€“73, 2007.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic optimization.
Journal of Machine Learning Research, 12:2121â€“2159, 2010.

Pearl, Judea. Reverend Bayes on inference engines: A distributed
hierarchical approach. Cognitive Systems Laboratory, School
of Engineering and Applied Science, University of California,
Los Angeles, 1982.

Frey, Brendan J and Hinton, Geoffrey E. Variational learning in
nonlinear Gaussian belief networks. Neural Computation, 11
(1):193â€“213, 1999.

Pearl, Judea. Causality: models, reasoning and inference, volume 29. Cambridge Univ Press, 2000.

Gelfand, AE, Sahu, SK, and Carlin, BP. Efficient parameterisations for normal linear mixed models. Biometrika, 82:479â€“488,
1995.

Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep learning
made easier by linear transformations in perceptrons. In International Conference on Artificial Intelligence and Statistics,
pp. 924â€“932, 2012.

Goodfellow, Ian J, Warde-Farley, David, Mirza, Mehdi, Courville,
Aaron, and Bengio, Yoshua. Maxout networks. arXiv preprint
arXiv:1302.4389, 2013.

Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan.
Stochastic back-propagation and variational inference in deep
latent gaussian models. arXiv preprint arXiv:1401.4082, 2014.

Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan R. Improving neural networks by preventing co-adaptation of feature detectors.
arXiv preprint arXiv:1207.0580, 2012.

Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J.
Learning representations by back-propagating errors. Nature,
323(6088):533â€“536, 1986.

Hoffman, Matthew D and Gelman, Andrew. The no-U-turn sampler: Adaptively setting path lengths in Hamiltonian Monte
Carlo. arXiv preprint arXiv:1111.4246, 2011.
Kass, Robert E, Carlin, Bradley P, Gelman, Andrew, and Neal,
Radford M. Markov chain Monte Carlo in practice: A
roundtable discussion. The American Statistician, 52(2):93â€“
100, 1998.
Kingma, Diederik P and Welling, Max. Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114, 2013.
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Imagenet
classification with deep convolutional neural networks. In
Advances in Neural Information Processing Systems 25, pp.
1106â€“1114, 2012.
Maaten, Laurens, Chen, Minmin, Tyree, Stephen, and Weinberger, Kilian Q. Learning with marginalized corrupted features. In Proceedings of the 30th International Conference on
Machine Learning (ICML-13), pp. 410â€“418, 2013.
Meng, X-L and Van Dyk, David. Fast EM-type implementations
for mixed effects models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 60(3):559â€“578, 1998.

Tang, Yichuan and Salakhutdinov, Ruslan. Learning stochastic
feedforward neural networks. In Advances in Neural Information Processing Systems, pp. 530â€“538, 2013.
Van Dyk, David A and Meng, Xiao-Li. The art of data augmentation. Journal of Computational and Graphical Statistics, 10
(1), 2001.
Wei, Greg CG and Tanner, Martin A. A Monte Carlo implementation of the EM algorithm and the poor manâ€™s data augmentation
algorithms. Journal of the American Statistical Association, 85
(411):699â€“704, 1990.
Yu, Yaming and Meng, Xiao-Li. To Center or Not to Center: That Is Not the Questionâ€“An Ancillarity-Sufficiency
Interweaving Strategy (ASIS) for Boosting MCMC Efficiency. Journal of Computational and Graphical Statistics, 20(3):531â€“570, 2011.
doi: 10.1198/jcgs.2011.
203main. URL http://amstat.tandfonline.com/
doi/abs/10.1198/jcgs.2011.203main.

