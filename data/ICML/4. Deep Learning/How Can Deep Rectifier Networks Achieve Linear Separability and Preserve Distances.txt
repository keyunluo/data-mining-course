How Can Deep Rectifier Networks Achieve Linear Separability and
Preserve Distances?
Senjian An
SENJIAN . AN @ UWA . EDU . AU
School of Computer Science and Software Engineering, The University of Western Australia, Australia
Farid Boussaid
FARID . BOUSSAID @ UWA . EDU . AU
School of Electrical, Electronic and Computer Engineering, The University of Western Australia, Australia
Mohammed Bennamoun
MOHAMMED . BENNAMOUN @ UWA . EDU . AU
School of Computer Science and Software Engineering, The University of Western Australia, Australia

Abstract
This paper investigates how hidden layers of
deep rectifier networks are capable of transforming two or more pattern sets to be linearly separable while preserving the distances with a guaranteed degree, and proves the universal classification power of such distance preserving rectifier
networks. Through the nearly isometric nonlinear transformation in the hidden layers, the margin of the linear separating plane in the output
layer and the margin of the nonlinear separating
boundary in the original data space can be closely
related so that the maximum margin classification in the input data space can be achieved approximately via the maximum margin linear classifiers in the output layer. The generalization performance of such distance preserving deep rectifier neural networks can be well justified by
the distance-preserving properties of their hidden
layers and the maximum margin property of the
linear classifiers in the output layer.

1. Introduction
With the exponential increase in computing power and
the development of efficient training techniques (Hinton
et al., 2006; Glorot & Bengio, 2010; Sutskever, 2013),
deep learning networks have achieved impressive successes
across a wide variety of domains such as speech recognition ((Seide et al., 2011; Hinton et al., 2012; Deng et al.,
2013), handwritten digit recognition (Ciresan et al., 2012),
object recognition (Krizhevsky et al., 2012; Zeiler & FerProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

gus, 2014; Lee et al., 2014; He et al., 2015) and face verification (Taigman et al., 2014; Sun et al., 2014).
Deep rectifier networks, wherein the rectifier (i.e.
max(0, x)) acts as the nonlinear activation function, are
among the most successful deep learning networks. The
training advantages and improved performance of rectifiers
over sigmodal activation functions have been shown in a
number of recent deep learning networks (Nair & Hinton,
2010; Zeiler et al., 2013; Krizhevsky et al., 2012; Bengio,
2013; Maas et al., 2013; Glorot et al., 2011), with rectifier networks providing some of the best results on several
benchmark problems for object classification (Krizhevsky
et al., 2012) and speech recognition (Dahl et al., 2013).
While there is a vast body of empirical evidence for the
excellent generalization performance of deep neural networks, there is only a limited body of works that have
sought to provide a theoretical justification of such performance. Most of the theoretical works have focused on the
universal approximation power of deep neural networks for
functions (Hornik et al., 1989) or for probability distributions (Le Roux & Bengio, 2010; Montufar & Ay, 2011).
Recently, several publications have investigated the superior expressive power of deep networks against shallow networks (i.e., with a single hidden layer). (Delalleau & Bengio, 2011) showed that the deep network representation of
a certain family of polynomials can be much more compact
(i.e., with less hidden units) than that provided by a shallow
network. Similarly, with the same number of hidden units,
deep networks are able to separate their input space into
much more regions of linearity than their shallow counterparts (Pascanu et al., 2014; Montúfar et al., 2014). However, the universal approximation power and the superior
expressive power are not enough to explain the superior
generalization performance of deep neural networks. In
fact, there is currently no clear theoretical justification of

the excellent empirical performance of deep networks with
a huge number of parameters.
Motivated by the fact that rectifier neural networks perform
linear classification in the output layer and the generalization performance of linear classifiers (e,g. Support Vector Machines (SVM)(Cortes & Vapnik, 1995)) can be well
justified by their maximum margin property, this paper investigates the distance preserving properties of the rectifier
hidden layers in achieving linear separability, and establishes the link between the margin of the linear separating
boundary in the output layer and the margin of the nonlinear separating boundary in the input data space. If the distances are preserved perfectly in the transformation from
the data space to the output of the topmost hidden layer,
the area bounded by two parallel hyperplanes in the output
layer would correspond to an area bounded by two parallel
manifolds in the input data space.
The link between the output and input of a hidden
layer is a rectified linear transformation (RLT), namely,
max(0, W T x + b). The only difference between linear
transformations and RLTs lies in that the rectifier forces all
the negative outputs to be zero. However, this seemingly
small change makes all the big differences between linear
transformations and RLTs. We will prove that RLT can
make any disjoint data linearly separable through a cascade
of two RLTs, and consequently, two-hidden-layer rectified
feedforward networks are universal classifiers. Our proof
is constructive, with the aid of a new proposed data model,
and explains the strategies of RLTs in transforming linearly
inseparable data to be linearly separable. Furthermore,
we
√
2
will show how RLT can preserve at least 2 ≈ 70.7% of
the distance of any two vectors in the input space, and for
two-hidden-layer rectifier networks, half of the distances
can thus be preserved. The separating boundary area with
a margin γ, bounded by two parallel hyperplanes from a
linear SVM in the output layer, is related to a separating
boundary area, in the original data space, bounded by two
manifolds with margins varying from γ to 2γ. The generalization performance of such deep rectifier networks can
be well justified by their approximate distance preserving
properties and the maximum margin properties of the linear
classifiers in the output layer.
The main contributions of this paper include: 1) Disjoint Convex Hull Decompositions of Data−A new data
model is proposed to construct the rectified linear units
that can transform linearly inseparable data to be linearly
separable; 2) Bidirectional RLT−A new type of ReLU,
which splits the positive and negative components of linear units into two separate features, is introduced for sake
of distance preservation; 3) Distance Preserving Rectified
Networks–A special type of rectifier networks is identified to have both universal classification power and distance

preserving properties. It is shown that, through a cascade
of two orthogonal bidirectional RLTs, any two or more disjoint pattern sets can be transformed to be linearly separable under the constraint that the distance distortions are
within factors from 0.5 to 1.
Notations: Throughout the paper, we use capital letters to
denote matrices, lower letters for scalar numbers, and bold
lower letters for vectors. Given an integer m, we use [m]
to denote the integer set from 1 to m, I the identity matrix
with proper dimensions and 0 a vector with all elements being 0. Given a finite number of points xi (i ∈ [m]) in Rn , a
convex combination of these points is a linear combination
of them in which all coefficients are non-negative and sum
to 1. The convex hull of a set X , denoted by CH(X ), is a
set of all convex combinations of the points in X .
The rest of this paper is organised as follows. In Section 2,
we introduce the disjoint convex hull decomposition models of data, and then use this model, in Section 3, to address
the power of RLTs in transforming linearly inseparable data
to be linearly separable. The bidirectional RLTs are introduced and their distance preserving properties are investigated in Section 4. Section 5 addresses distance preserving
rectifier networks and their universal classification power,
while Section 6 concludes the paper with a discussion on
the related works and future research directions.

2. Decomposition of Multiple Pattern Sets
Two pattern sets, namely X1 and X2 , are called linearly separable if there exist w and b such that wT x + b > 0, ∀ x ∈
X1 and wT x + b ≤ 0, ∀ x ∈ X2 . It is well known that two
pattern sets are linearly separable if and only if their convex hulls are disjoint. In order to investigate how linearly
inseparable pattern sets can be transformed to be linearly
separable, we model each pattern set with several subsets
so that the convex hulls of these subsets are disjoint across
different classes of patterns. More precisely, we define the
disjoint convex hull decomposition model of data as below.
Definition 1 (Disjoint Convex Hull Decomposition) Let
Xk , (k ∈ [m]), be m disjoint subsets in Rn . A decompoSLk i
sition of Xk , namely, Xk = i=1
Xk ,, is called a disjoint
convex hull decompositionSif the unions of the convex hulls
Lk
of Xki , denoted by X̂k , i=1
CH(Xki ), are still disjoint,
i.e.,
X̂k ∩ X̂l = ∅, ∀ k 6= l
(1)
or equivalently, for all i ∈ [Lk ], j ∈ [Ll ],
CH(Xki ) ∩ CH(Xlj ) = ∅, ∀ k 6= l.

(2)

For finite pattern sets Xk , a trivial disjoint convex hull decomposition is to select each point as a subset. Hence,

any disjoint pattern sets have at least one disjoint convex
hull decomposition. The complexity of the decomposition
model can be characterised by its size, i.e., the number of
involved linearly separable subsets. To generate a small
size disjoint convex hull decomposition, one can
SL1proceed
X1i so
as follows. First, we decompose X1 as X1 = i=1
that the size L1 is minimal and
!
m
\ [
i
CH(X1 )
Xl = ∅, ∀ i ∈ [L1 ].
(3)

3.1. From Convex Separability to Linear Separability
Theorem 2 Let X1 and X2 be two convexly separable pattern sets with a finite number of points in Rn , CH(X1 ) ∩
L2
[
X2 = ∅, X2 =
X2j with CH(X2j ) ∩ CH(X1 ) = ∅, and
j=1

let wjT x + bj be the linear classifiers of X2j and X1 such
that, for any j ∈ [L2 ],
wjT x + bj
wjT x + bj

l=2

SL 2
X2j so that the size
Then, we decompose X2 as X2 = j=1
L2 is minimal and
!
m
[
T
j
CH(X2 )
Xl
= ∅, ∀ j ∈ [L2 ]
l=3
!
L1
[
T
j
i
CH(X2 )
CH(X1 )
= ∅, ∀ j ∈ [L2 ].

≤ 0, ∀ x ∈ X1
> 0, ∀ x ∈ X2j .

(5)

Denote
, [w1 , w2 , · · · , wL2 ]
, [b1 , b2 , · · · , bL2 ]T
, {z = max(0, W T x + b) : x ∈ Xk }, k = 1, 2.
(6)
Then Z1 and Z2 are linearly separable.
W
b
Zk

i=1

(4)
Sequentially, we can obtain the decompositions for all Xk
and form a disjoint convex hull decomposition.
According to the characteristics of disjoint convex hull decomposition models, pattern sets can be categorized into
three typical categories:
1) Linearly Separable Pattern Sets: Two linearly separable pattern sets have a disjoint decomposition convex
model with L1 = L2 = 1, i.e., CH(X1 ) ∩ CH(X2 ) =
∅;
2) Convexly Separable Pattern Sets: Two pattern sets are
called convexly separable if they have a disjoint convex hull decomposition with min(L1 , L2 ) = 1. They
are referred to as convexly separable because there exists a convex region which can separate one class from
the other, i.e., CH(X1 )∩X2 = ∅ or CH(X2 )∩X1 = ∅;
3) Convexly Inseparable Pattern Sets: If any disjoint
convex hull decomposition of X1 and X2 satisfies
min(L1 , L2 ) > 1, then they are called convexly inseparable.
Given that the classification of linear inseparable patterns
is more challenging, it is desirable to transform the linearly
inseparable pattern sets into linearly separable ones since
the latter have been well investigated and there are many
well-developed solutions (e.g linear SVM) for them.

3. From Linear Inseparability to Linear
Separability: The Roles of the Rectifier
In this section, we investigate how RLTs can transform linearly inseparable pattern sets to be linearly separable.

Proof: From the definition of Z1 in (6) and(5), we have
Z1 = {0}. Next we show that 0 6∈ CH(Z2 ). Let z be any
vector in Z2 . From the definition of Z2 in (6) and(5), it
follows that z 6= 0, all the entries of z are non-negative and
at least one of them is strictly positive. Note that Z2 is a
finite set, we have 0 6∈ CH(Z2 ), and therefore, CH(Z1 ) ∩
CH(Z2 ) = ∅. That is, Z1 and Z2 are linearly separable
and the proof is completed.

From Theorem 2, we can see that two convexly separable
pattern sets can be transformed to be linearly separable by
squeezing one pattern set into one point while keeping this
point away from the convex hull of the other class patterns.
The minimal number of required rectified linear units is no
larger than the minimal number of subsets into which X2
can be decomposed so that each subset is linearly separable
from X1 .
3.2. From Convex Inseparability to Convex
Separability
Theorem 3 Let X1 and X2 be two convexly inseparable
pattern sets, and
X1

=

L1
[
i=1

X1i , X2

=

L2
[

X2j

(7)

j=1

be one of their disjoint convex hull decompositions (L1 >
T
1, L2 > 1), and let wij
x + bij be the linear classifiers of
j
i
X2 and X1 such that, for any i ∈ [L1 ] and j ∈ [L2 ],
T
wij
x + bij
T
wij x + bij

≤ 0, ∀ x ∈ X1i
> 0, ∀ x ∈ X2j .

(8)

Denote
Wi
bi
W
b
Zk
Z1i

[wi1 , wi2 , · · · , wiL2 ]
[bi1 , bi2 , · · · , biL2 ]T
[W1 , W2 , · · · , WL1 ]
[bT1 , bT2 , · · · , bTL1 ]T
{z = max(0, W T x + b) : x ∈ Xk }, k = 1, 2.
{z = max(0, W T x + b) : x ∈ X1i }, i ∈ [L1 ].
(9)
!
L1
\ [
i
CH(Z2 )
CH(Z1 ) = ∅
(10)

,
,
,
,
,
,

Then

i=1

which implies that Z1 , Z2 are convexly separable.
Proof: Denote, for i ∈ [L1 ] and t ∈ [L1 ],
Z2t
i
Z1t

,
,

{z = max(0, WtT x + bt ) : x ∈ X2 }
{z = max(0, WtT x + bt ) : x ∈ X1i }.

(11)

Note that CH(X1i ) ∩ X2 = ∅. Apply Theorem 2 on X1i , X2
i
and their images, Z1i
and Z2i respectively, under the transformation max(0, WiT + bi ). Then we have
i
CH(Z1i
) ∩ CH(Z2i ) = ∅, i ∈ [L1 ]

(12)

which implies that
CH(Z1i ) ∩ CH(Z2 ) = ∅, i ∈ [L1 ].

(13)

This implication is due to the fact that, according to the
definitions in (9), Wi is a submatrix of W , bi is a subvector
i
of b and therefore the points in the sets Z1i
and Z2i are
the projections, into a lower dimensional subspace, of the
points in the sets Z1i and Z2 respectively.
i
1
Note that Z1 ⊂ ∪L
i=1 CH(Z1 ), we have Z1 ∩ CH(Z2 ) = ∅
and thus, Z1 and Z2 are convexly separable.

Now let V = [v1 , v2 , · · · , vL1 ], c = [c1 , c2 , · · · , cL1 ]T ,
y = max(0, V T z + c) and define
Yk

,
=

{y = max(0, V T z + c) : z ∈ Zk }


	
y = max 0, V T max{0, W T x + b} + c : x ∈ Xk

(15)
for k = 1, 2. Then from Theorem 2, we have CH(Y1 ) ∩
CH(Y2 ) = ∅ and thus Y1 , Y2 are linearly separable.
Hence, any two disjoint subsets, namely X1 and X2 , can be
transformed to be linearly separable through a cascade of
two RLTs, which require L1 (L2 + 1) or less rectified linear
units if X1 and X2 have a disjoint convex hull decomposition with L1 subsets of X1 and L2 subsets of X2 . The above
results can be summarised by the following Theorem:
Theorem 4 Any two disjoint subsets in Rn can be transformed to be linearly separable through a cascade of two
RLTs.
3.4. Multiple Sets with Pairwise Linear Separability
Given m pattern sets Xi in Rn , they are said to be linearly
separable if each set, namely Xi , is linearly separable from
the union of the other sets. They are said to be pairwise
linearly separable if every pair of them, namely Xi and Xj ,
are linearly separable. Linear separability is much stronger
than pairwise linear separability for multiple sets. Next, we
will show that any multiple sets with pairwise linear separability can be transformed to be linearly separable through
an RLT, and Section 3.5 will show that any disjoint multiple sets can be transformed to be linearly separable by a
cascade of two RLTs.
Let Xk be m pattern sets with pairwise linear separability,
T
i.e., CH(Xi ) ∩ CH(Xj ) = ∅, ∀ i 6= j, and let wi,j
x + bi,j
be the linear classifiers of Xi and Xj , satisfying wj,i =
−wi,j , bj,i = −bi,j and
T
wi,j
x + bi,j
T
wi,j
x + bi,j


Theorem 3 shows that any two disjoint subsets X1 , X2 in
Rn can be transformed convexly separable through an RLT.
The minimal number of required rectified linear units is no
larger than the minimal value of L1 L2 such that a disjoint
convex hull decomposition of X1 , X2 exists with L1 subsets
of X1 and L2 subsets of X2 .
3.3. From Convex Inseparability to Linear Separability
Let W, b, Wi , bi , Z1 , Z2 , Z1i , i ∈ [L1 ], be defined as in
Theorem 3, and z = max(0, W T x + b) be the RLT. From
(10), Z2 and Z1i are linearly separable. Let viT z + ci be the
linear classifiers of Z2 and Z1i such that
viT z + ci
viT z + ci

≤
>

0, ∀ z ∈ Z2
0, ∀ z ∈ Z1i .

(14)

≤ 0, ∀ x ∈ Xi ,
> 0, ∀ x ∈ Xj .

(16)

Denote
Wi
bi
W
b
Zji
Zi

,
,
,
,
,
,

[wi,1 , · · · , wi,i−1 , wi,i+1 , · · · , wi,m ]
[bi,1 , · · · , bi,i−1 , bi,i+1 , · · · , bi,m ]
[W1 , W2 , · · · , Wm ]
[bT1 , bT2 , · · · , bTm ]T
{max(0, WiT x + bi ) : x ∈ Xj }
{max(0, W T x + b) : x ∈ Xi }.

(17)

for i ∈ [m] and j ∈ [m]. Apply Theorem 2 on Xi , ∪j6=i Xj
(as two convexly separable sets in Theorem 2) with the RLT
z = max(0, WiT x + bi ). Then we have


CH Zii ∩ CH ∪j6=i Zji = ∅

(18)

which, similar to the implication from (12) to (13), implies
that
CH (Zi ) ∩ CH (∪j6=i Zj ) = ∅.
(19)
Hence, the multiple sets Zi , transformed from Xi , are linearly separable and we have the following Theorem
Theorem 5 Any m subsets in Rn with pairwise linear
separability can be transformed to be linearly separable
through an m(m − 1) dimensional RLT.
3.5. Multiple Disjoint Sets
Let Xk be m disjoint subsets, X̂k ,
Xk

=

SL k

i=1

Xki , X̂k

=

Sm

l=1,l6=k

SL̂k

j=1

Xl , and

X̂kj

(20)

be a disjoint convex hull decomposition of Xk and X̂k . That
is, CH(Xki ) ∩ CH(X̂kj ) = ∅ and there exist linear classifiers
T
wkij
x + bkij such that
T
wkij
x + bkij
T
wkij x + bkij

≤ 0, ∀ x ∈ Xki
> 0, ∀ x ∈ X̂kj .

(21)

Next, we construct another RLT on the sets Zk and Ẑk to
T
transform the pattern sets linearly separable. Let vki
z + cki
i
be the linear separator of Ẑk and Zk such that
T
vki
z + cki
T
vki
z + cki

≤
>

0, ∀ z ∈ Ẑk
0, ∀ z ∈ Zki

Wki
bki
Wk
bk
W
b

,
,
,
,
,
,

[wki1 , wki2 , · · · , wkiL̂k ]
[bki1 , bki2 , · · · , bkiL̂k ]T
[Wk1 , Wk2 , · · · , WkLk ]
[bTk1 , bTk2 , · · · , bTkLk ]T
[W1 , W2 , · · · , Wm ]
[bT1 , bT2 , · · · , bTm ]T

Vk
ck
V
c
Ykk
Ŷkk
Yk
Ŷk

,
,
,
,
,
,
,
,

[vk1 , vk2 , · · · , vkLk ]
[ck1 , ck2 , · · · , ckLk ]T
[V1 , V2 , · · · , Vm ]
[cT1 , cT2 , · · · , cTm ]T
{y = max(0, VkT z + ck ) : z ∈ Zk }
{y = max(0, VkT z + ck ) : z ∈ Ẑk }
{y = max(0, V T z + c) : z ∈ Zk }
{y = max(0, V T z + c) : z ∈ Ẑk }.

{z = max(0, WkT x + bk ) : x ∈ Xk }
{z = max(0, WkT x + bk ) : x ∈ Xki }
{z = max(0, WkT x + bk ) : x ∈ X̂k }
{z = max(0, W T x + b) : x ∈ Xk }
{z = max(0, W T x + b) : x ∈ Xki }
{z = max(0, W T x + b) : x ∈ X̂k }.

(27)

Then from Theorem 2, we have
(28)

which implies that
(22)

CH(Yk ) ∩ CH(Ŷk ) = ∅.

(29)

By the definitions of Yk , Ŷk in (27) and the definitions of
Zk , Ẑk in (23), we know that the points of Yk , Ŷk correspond to those of Xk and X̂k through the following transfor
mation: y = max 0, V T max(0, W T x + b) + c . Thus,
we have the following Theorem:

and define the following sets, for i ∈ [Lk ], k ∈ [m],
,
,
,
,
,
,

(26)

and define

CH(Ykk ) ∩ CH(Ŷkk ) = ∅

Denote

Zkk
Zkki
Ẑkk
Zk
Zki
Ẑk

Therefore, Ẑk is linearly separable from Zki for each i ∈
[Lk ], and thus convexly separable from Zk .

(23)

Theorem 6 Any multiple disjoint sets can be transformed
to be linearly separable through a cascade of two RLTs.

4. Bidirectional RLTs and Their Distance
Preserving Properties

(24)

Since the rectifier of a vector discards the information of
the negative elements, it does not preserve distances and
can transform two very different vectors into an identical
vector. For sake of distance preservation, we introduce a
new type of rectifier, which keeps both the information of
the positive and the negative elements.

which, similar to the implication from (12) to (13), implies
that
!
L1
\ [
i
CH(Ẑk )
CH(Zk ) = ∅.
(25)

Definition 7 The bidirectional rectifier of a vector x ∈ Rn
is defined by


max(0, x)
z=
.
(30)
max(0, −x)

By applying Theorem 3 on Xk and Xˆk (corresponding to X1 and X2 respectively) with the transformation
max(0, WkT x + bk ), we have
CH(Ẑkk )

\

L1
[

!
CH(Zkki )

=∅

i=1

i=1

Unlike the rectifier, the bidirectional rectifier can preserve
distances with guaranteed degrees. Let x1 , x2 be any
two vectors in Rn and z1 , z2 be their bidirectional rectifications. For brevity of notations, hereafter, we denote
x+ = max(0, x), x− = max(0, −x), and denote the k th
element of x by x(k). Then x+ , x− are non-negative vectors, x = x+ − x− , (x+ )T x− = 0, and therefore
+ 2
−
− 2
= kx+
1 − x2 k + kx1 − x2 k
+
−
+
− 2
= k(x1 − x1 ) − (x2 − x2 )k
+ 2
−
− 2
= kx+
1 − x2 k + kx1 − x2 )k
+
+ T
−
−
−2(x1 − x2 ) (x1 − x2 )
− T +
T −
= kz1 − z2 k2 + 2(x+
1 ) x2 + 2(x1 ) x2
2
≥ kz1 − z2 k .
(31)
The equality kz1 − z2 k = kx1 − x2 k holds if and only
− T +
T −
if (x+
1 ) x2 = 0 and (x1 ) x2 = 0, or equivalently
x1 (k)x2 (k) ≥ 0, k ∈ [n], that is, the k th elements of x1
and x2 have the same sign for all k ∈ [n]. Only the elements of x1 , x2 with different signs contribute to the loss
of the Euclidean distance in bidirectional rectification.

kz1 − z2 k2
kx1 − x2 k2

Now we derive the upper bound of kx1 − x2 k.
−
+
− 2
= k(x+
1 − x1 ) − (x2 − x2 )k
+
+
−
− 2
= k(x1 − x2 ) − (x1 − x2 )k
+ 2
−
− 2
= k(x+
1 − x2 )k + k(x1 − x2 )k
+
+ T
−
−
−2(x1 − x2 ) (x1 − x2 )
+ 2
−
− 2
= 2k(x+
1 − x2 )k + 2k(x1 − x2 )k
+
+
−
− 2
−k(x1 − x2 ) + (x1 − x2 )k
+ 2
−
− 2
≤ 2kx+
1 − x2 k + 2kx1 − x2 k
2
= 2kz1 − z2 k .
(32)
Furthermore, the equality 2kz1 − z2 k2 = kx1 − x2 k2 holds
−
+
−
if and only if (x+
1 + x1 ) − (x2 + x2 ) = 0, or equivalently
|x1 (k)| = |x2 (k)| for all k ∈ [n].

kx1 − x2 k2

The distance preserving properties of the bidirectional rectifier can be summarised as below.
Proposition 8 Let x1 6= x2 be any two different vectors in
Rn and z1 , z2 be their corresponding bidirectional rectifications. Then we have
√

2
2 kx1

− x2 k ≤ kz1 − z2 k ≤ kx1 − x2 k.

(33)

That is, the bidirectional
rectifier is a contract mapping and
√
preserves at least 22 ≈ 70.7% of the Euclidean distance
of any two different vectors.
Next, we define the bidirectional rectified linear transformation and investigate its distance preserving properties.
Definition 9 A bidirectional RLT is a mapping from Rn to
R2d and is defined as


max(0, W T x + b)
z=
(34)
max(0, −W T x − b)

where W ∈ Rn×d and b ∈ Rd .
A bidirectional RLT is called singular, nonsingular, or orthogonal if Q = W W T is singular, nonsingular and orthogonal respectively.
Note that
kW T x1 − W T x2 k2 = (x1 − x2 )T Q(x1 − x2 )

(35)

where Q = W W T . Then kW T x1 − W T x2 k = kx1 − x2 k
if Q is orthogonal (i.e., QT Q = I); and W T x1 6=
W T x2 ⇔ x1 6= x2 if Q is nonsingular. Then, from Proposition 8, we have
Proposition 10 Let the bidirectional RLT be defined as in
(34), x1 , x2 be any two different vectors in Rn and z1 , z2 be
their responses of the transform. Then the following statements are correct:
1). Orthogonal bidirectional
RLT is a contract mapping
√
2
and preserves at least 2 ≈ 70.7% of the Euclidean distance of any two different vectors, more precisely, the inequalities
√
2
kx1 − x2 k ≤ kz1 − z2 k ≤ kx1 − x2 k
(36)
2
hold if Q = W W T is orthogonal.
2). Nonsingular bidirectional RLT preserves the disjointness of any two disjoint points, i.e.,
x1 6= x2 ⇔ z1 6= z2 , if Q = W W T is nonsingular.
(37)

5. Universal Classification Power of Distance
Preserving Rectifier Networks
In Section 3, we have shown that a cascade of two RLTs
are capable of transforming any disjoint pattern sets to be
linearly separable. Next, we show that a cascade of two orthogonal bidirectional RLTs are also capable of achieving
linear separability for any two or multiple disjoint pattern
sets, with additional distance preserving property due to the
bidirectional rectifier and the orthogonality constraint on
the weight matrix.
Theorem 11 The following three statements are true for
orthogonal bidirectional RLTs:
1). Any two convexly separable sets can be transformed to
be linearly separable through an orthogonal bidirectional
RLT.
2). Any multiple disjoint sets with pairwise linear separability can be transformed to be linearly separable through
an orthogonal bidirectional RLT.

3). Any two or more disjoint sets can be transformed to
be linearly separable through a cascade of two orthogonal
bidirectional RLTs.

Note that W̄ W̄ T = I and max(0, Ŵ T x + b̂) is a subvector of z, the bidirectional RLT (40) is orthogonal and
transforms the data to be linearly separable.

To prove Theorem 11, it suffices to prove the following
Lemma 12, because Theorem 11 follows from Theorem 2,
Theorem 4, Theorem 5, Theorem 6, and Lemma 12.

2). Let y = max{0, V T max(0, W T x + b) + c} be a
cascade of RLTs, which transform the disjoint sets Xk (k ∈
[m]) to be linearly separable. Assume that W ∈ Rn×d1 and
V ∈ Rd1 ×d2 . Let Σ1 be a diagonal matrix with positive diagonals such that I−W Σ21 W T is positive definite and there
exists U ∈ Rn×n such that W Σ21 W T + U U T = I. Denote
Ŵ = W Σ1 , b̂ = Σ1 b, W̄ = [Ŵ , U ] ∈ Rn×(d1 +n) . Then
W̄ W̄ T = I and the following bidirectional RLT


 
max(0, Ŵ T x + b̂)
z1

 z2  
max(0, U T x)

 
(41)
z=
 z3  =  max(0, −Ŵ T x − b̂) 
z4
max(0, −U T x)

Lemma 12 Let X1 and X2 be any two disjoint subsets in
Rn . The following statements are true:
1). If X1 and X2 can be transformed to be linearly separable by an RLT, then an orthogonal bidirectional RLT exists
to transform them to be linear separable.
2). If X1 and X2 can be transformed to be linearly separable by a cascade of RLTs, then a cascade of orthogonal
bidirectional RLTs exist to transform them to be linear separable.
Proof: The proofs of 1) and 2) are proceeded as follows:
first, we scale the RLTs, which are assumed to transform
the two pattern sets to be linearly separable, so that the resulted RLTs are contract mappings; then we add some more
linear units so that the resulted bidirectional RLTs are orthogonal. Note that these operations do not change the linear separability of pattern sets, the constructed orthogonal
bidirectional RLTs are capable of transforming the data to
be linearly separable.
1). Assume that the transformation z = max(0, W T x + b)
transforms X1 and X2 to be linearly separable and wT z + b
be the linear classifier such that
wT max(0, W T x + b) + b ≤ 0; ∀ x ∈ X1
wT max(0, W T x + b) + b > 0; ∀ x ∈ X2 .

(38)

Let Σ be a diagonal matrix with positive diagonals such that
the largest eigenvalue of W Σ2 W T be less than 1. Denote
Ŵ = W Σ, b̂ = Σb and ŵ = Σ−1 w. Then it follows
that wT max(0, W T x + b) = ŵT max(0, Ŵ T x + b̂) and
therefore
ŵT max(0, Ŵ T x + b̂) + b
ŵT max(0, Ŵ T x + b̂) + b

≤ 0; ∀ x ∈ X1
> 0; ∀ x ∈ X2

is orthogonal. Let Zk denote the response sets of
Xk by the orthogonal bidirectional RLT (41). Note
that y = max{0, V T max(0, W T x + b) + c} =
max{0, V̂ T max(0, Ŵ T x + b̂) + c}, where V̂ = Σ−1
1 V,
is a cascade of two RLTs, which transform the sets Xk
to be linearly separable. Consequently, the transformation
y = max(0, V̂ T z1 + c) = max(0, V̄ T z + c) transforms
the sets Zk to be linearly separable, where z, z1 are defined
in (41) and V̄ = [V̂ T , 0T , 0T , 0T ]. Then from statement 1)
(already proved), Zk can be transformed to be linearly separable by an orthogonal bidirectional RLT and therefore,
Xk , k ∈ [m] can be transformed to be linearly separable
by a cascade of two orthogonal bidirectional RLTs and the
proof is completed.

(39)

which imply that the transformation z = Ŵ T x + b̂ turns
the pattern sets linearly separable.
Since the largest eigenvalue of Ŵ Ŵ T is less than 1, I −
Ŵ Ŵ T is positive definite and there exists U ∈ Rn×n such
that U U T = I − Ŵ Ŵ T . Denote W̄ = [Ŵ , U ], b̄ =
[b̂T , 0Tn ]T and define the following bidirectional RLT

max(0, Ŵ T x + b̂)


T

max(0, W̄ x + b̄)
max(0, U T x)
z=
=
T
 max(0, −Ŵ T x − b̂)
max(0, −W̄ x − b̄)
max(0, −U T x)
(40)


Distance Preserving Rectifier Networks. An orthogonal
bidirectional RLT is related to a hidden layer with a weight
matrix Ŵ = [W, −W ], satisfying W W T = I, and a bias
vector b̂ = [bT , −bT ]T . Such hidden layers are referred
to as distance preserving hidden layers since they have the
same distance preserving property as the orthogonal bidirectional RLTs. One can formulate rectifier networks by
using one or more distance preserving hidden layers, and
these rectifier networks are referred to as distance preserving rectifier networks. From Theorem 11, we know that
two-hidden-layer distance preserving rectifier neural networks are capable of separating any disjoint pattern sets
and thus are universal classifiers. From Proposition
10, we
√
2
know that each hidden layer preserves at least 2 of the
distances and therefore two hidden layers preserve at least

half of the distances in the original data space. That is, for
 any two vectors, namely x1 and x2 , and their outputs z1
.
 and z2 in the second hidden layer, we have
1
kx1 − x2 k ≤ kz1 − z2 k ≤ kx1 − x2 k.
2

(42)

Remarks. 1). It is worth noting that the orthogonality of
the weight matrices W in the hidden layers is not the orthogonality of the linear units (i.e., W T W = I), but the
orthogonality of the weight matrix across data dimensions
(i.e., W W T = I). While there are at most n orthogonal linear units for n dimensional input data, one can have
any large number of linear units which satisfy the orthogonality constraint W W T = I. Moreover, for a randomly
selected matrix W , the more columns (corresponding to
linear units) it has, the closer it is to satisfy W W T = I. 2).
Motivated by the fact that the large distances of patterns
in the input data space may not worth perfect preservation,
and for sake of computational efficiency, one may conduct
locality preserving projection (Niyogi, 2004) on the input
data and/or the hidden layer outputs so that the number of
hidden nodes can be significantly reduced while preserving the most important distances of the nearby points and
ensuring that the hidden layers can still be able to transform the data to be linearly separable. To achieve the distance preservation in a certain subspace spanned by H with
H T H = Ir , where r < n and n is the number of rows of
W , one can choose the weight matrix W = HV such that
V V T = Ir . By this selection of W , only the distances in
the space spanned by H will be preserved.

6. Discussion
This paper has shown how two hidden layers of rectifier
networks can transform any two or more disjoint pattern
sets to be linearly separable while preserving the distance
of any two vectors in the data space with a factor ranging
from 0.5 to 1. This nearly isometric property makes the
maximum margins achieved by the linear classifiers in the
output layer closely related to the separating margins in the
original data space, and makes the generalization performance of such rectifier networks well justified.
Related Work. Our work on the universal classification
power of deep rectifier networks is related to the works
on the universal approximation powers of deep neural networks (Hornik et al., 1989; Le Roux & Bengio, 2010;
Montufar & Ay, 2011). These works consider the universal
approximation power of arbitrary neural networks with one
or more hidden layers. In particular, (Hornik et al., 1989)
proves that any Borel measurable function can be approximated by a single hidden layer neural network. However,
this proof only show the existence of such approximations
for classifiers. In fact, it is not yet clear whether any disjoint
pattern sets can be perfectly separated by a single hidden
layer neural network. In contrast, our proof is constructive and this paper identifies a special type of two-hiddenlayer deep rectifier networks which can serve as universal
classifiers and whose generalization performance can also
be well justified by the distance preserving property of the

hidden layers and the maximum margin property of the linear classifiers in the output layer. Our effort to provide
theoretical justifications for deep rectifier network’s generalization performance is related to several recent publications (Delalleau & Bengio, 2011; Pascanu et al., 2014;
Montúfar et al., 2014) on the superior expressive powers of
deep networks against shallow networks (i.e., with a single hidden layer). These works apply the Occam’s razor
rule, which favours simple solutions over complex ones,
and use the smaller number of required hidden units to
justify the superior performance of deep rectifier networks
against their shallow counterpart. However, this cannot explain why many practical deep neural networks have thousands of hidden units, with millions of parameters, but exhibit excellent generalization performance. Our work is the
first to justify the generalization performance by exploring
the distance preserving properties of hidden layers and establishing the link between the separating boundary margins in the output layer and those in the input data space.
Although the state-of-the-art learnt deep rectifier networks
may not preserve the distances as the proposed distance
preserving rectifier network does, they usually have a large
number of hidden units and the weight matrices are likely
to be approximately orthogonal so that the hidden layers
are capable of preserving the distances in certain degrees.
The way of the splitting in bidirectional rectifiers was used
in (Coates & Ng, 2011) to split the positive and negative
components of the sparse codes into separate features and
allow classifiers to weigh positive and negative responses
differently.
Implications to Practical Training of Rectified Networks. Our analysis suggests that the distance preserving rectifier networks’ generalization performance can be
well justified even though they may have a large number
of hidden units. In practical training of rectifier networks,
one may add constraints on the weight matrix or add a cost
function for each hidden layer to promote distance preservations.
Limitations and Future Directions. This work has focused on theoretically analysing the universal classification power and the distance preserving properties of rectifier networks, and providing an insightful explanation for
the recent successes of rectifier networks in practice. However, there are other factors involved in their generalization performance. Further work is needed to investigate the
properties of convolutional neural networks, as well as the
training of distance preserving deep rectifier networks.
Acknowledgements: This work was supported by ARC
grants (DP150100294, DP150104251), and by the Western
Australia Office of Science through Applied Research Program (ARP) Shark Hazard Mitigation.

References
Bengio, Yoshua. Deep learning of representations: Looking forward. In Statistical Language and Speech Processing, 2013.
Ciresan, Dan, Meier, Ueli, and Schmidhuber, Jürgen.
Multi-column deep neural networks for image classification. In Computer Vision and Pattern Recognition
(CVPR), 2012 IEEE Conference on, pp. 3642–3649.
IEEE, 2012.

Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E,
Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath,
Tara N, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four
research groups. Signal Processing Magazine, IEEE, 29
(6):82–97, 2012.
Hornik, Kurt, Stinchcombe, Maxwell, and White, Halbert.
Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366, 1989.

Coates, Adam and Ng, Andrew Y. The importance of
encoding versus training with sparse coding and vector
quantization. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pp. 921–
928, 2011.

Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing
systems, pp. 1097–1105, 2012.

Cortes, Corinna and Vapnik, Vladimir. Support-vector networks. Machine learning, 20(3):273–297, 1995.

Le Roux, Nicolas and Bengio, Yoshua. Deep belief networks are compact universal approximators. Neural
computation, 22(8):2192–2207, 2010.

Dahl, George E, Sainath, Tara N, and Hinton, Geoffrey E.
Improving deep neural networks for LVCSR using rectified linear units and dropout. In 2013 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 8609–8613. IEEE, 2013.

Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang,
Zhengyou, and Tu, Zhuowen. Deeply-supervised nets.
arXiv preprint arXiv:1409.5185, 2014.

Delalleau, Olivier and Bengio, Yoshua. Shallow vs. deep
sum-product networks. In Advances in Neural Information Processing Systems, pp. 666–674, 2011.
Deng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu,
Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoffrey, He, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.
In Acoustics, Speech and Signal Processing (ICASSP),
2013 IEEE International Conference on, pp. 8604–8608.
IEEE, 2013.
Glorot, Xavier and Bengio, Yoshua. Understanding the
difficulty of training deep feedforward neural networks.
In International conference on artificial intelligence and
statistics, pp. 249–256, 2010.
Glorot, Xavier, Bordes, Antoine, and Bengio, Yoshua.
Deep sparse rectifier networks. In Proceedings of the
14th International Conference on Artificial Intelligence
and Statistics. JMLR W&CP Volume, volume 15, pp.
315–323, 2011.

Maas, Andrew L, Hannun, Awni Y, and Ng, Andrew Y.
Rectifier nonlinearities improve neural network acoustic
models. In ICML Workshop on Deep Learning for Audio,
Speech, and Language Processing, 2013.
Montufar, Guido and Ay, Nihat. Refinements of universal
approximation results for deep belief networks and restricted boltzmann machines. Neural Computation, 23
(5):1306–1319, 2011.
Montúfar, Guido, Pascanu, Razvan, Cho, Kyunghyun, and
Bengio, Yoshua. On the number of linear regions of deep
neural networks. arXiv preprint arXiv:1402.1869, 2014.
Nair, Vinod and Hinton, Geoffrey E. Rectified linear units
improve restricted boltzmann machines. In Proceedings
of the 27th International Conference on Machine Learning (ICML-10), pp. 807–814, 2010.
Niyogi, X. Locality preserving projections. In Neural information processing systems, volume 16, pp. 153, 2004.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification. arXiv
preprint arXiv:1502.01852, 2015.

Pascanu, Razvan, Montufar, Guido, and Bengio, Yoshua.
On the number of inference regions of deep feed forward networks with piece-wise linear activations. In
International Conference on Learning Representations
2014(Conference Track), April 2014. URL http://
arxiv.org/abs/1312.6026.

Hinton, Geoffrey, Osindero, Simon, and Teh, Yee-Whye.
A fast learning algorithm for deep belief nets. Neural
computation, 18(7):1527–1554, 2006.

Seide, Frank, Li, Gang, and Yu, Dong. Conversational
speech transcription using context-dependent deep neural networks. In Interspeech, pp. 437–440, 2011.

Sun, Yi, Chen, Yuheng, Wang, Xiaogang, and Tang,
Xiaoou. Deep learning face representation by joint
identification-verification. In Advances in Neural Information Processing Systems, pp. 1988–1996, 2014.
Sutskever, Ilya. Training recurrent neural networks. PhD
thesis, University of Toronto, 2013.
Taigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and
Wolf, Lior. Deepface: Closing the gap to human-level
performance in face verification. In Computer Vision and
Pattern Recognition (CVPR), 2014 IEEE Conference on,
pp. 1701–1708. IEEE, 2014.
Zeiler, Matthew D and Fergus, Rob. Visualizing and understanding convolutional networks. In Computer Vision–
ECCV 2014, pp. 818–833. Springer, 2014.
Zeiler, Matthew D, Ranzato, M, Monga, Rajat, Mao, M,
Yang, K, Le, Quoc Viet, Nguyen, Patrick, Senior, A,
Vanhoucke, Vincent, Dean, Jeffrey, et al. On rectified
linear units for speech processing. In Acoustics, Speech
and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 3517–3521. IEEE, 2013.

