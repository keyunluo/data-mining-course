A Bayesian Wilcoxon signed-rank test based on the Dirichlet process

A. Benavoli, F. Mangili, G. Corani, M. Zaffalon
IPG IDSIA, Manno, Switzerland
F. Ruggeri
CNR IMATI, Milano, Italy

Abstract
Bayesian methods are ubiquitous in machine
learning. Nevertheless, the analysis of empirical results is typically performed by frequentist
tests. This implies dealing with null hypothesis
significance tests and p-values, even though the
shortcomings of such methods are well known.
We propose a nonparametric Bayesian version of
the Wilcoxon signed-rank test using a Dirichlet
process (DP) based prior. We address in two different ways the problem of how to choose the
infinite dimensional parameter that characterizes
the DP. The proposed test has all the traditional
strengths of the Bayesian approach; for instance,
unlike the frequentist tests, it allows verifying
the null hypothesis, not only rejecting it, and
taking decisions which minimize the expected
loss. Moreover, one of the solutions proposed to
model the infinite-dimensional parameter of the
DP allows isolating instances in which the traditional frequentist test is guessing at random. We
show results dealing with the comparison of two
classifiers using real and simulated data.

1. Introduction
The field of machine learning is constantly growing. Many
novel approaches in classification, regression etc. are constantly proposed, raising the issue of assessing and comparing these new methods with the state-of-the-art. A proper
way to perform such comparison is by means of statistical
procedures. Tutorials on the use of parametric and nonparametric statistical tests as a methodology for comparing
algorithms have been presented (Demšar, 2006; Trawiński
et al., 2010; Derrac et al., 2011) on different areas of machine learning. In all these papers, the Wilcoxon signedProceedings of the 31st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

{ALESSIO , FRANCESCA , GIORGIO , ZAFFALON}@ IDSIA . CH
FABRIZIO @ MI . IMATI . CNR . IT

rank test is indicated as the nonparametric statistical procedure for the analysis of two paired samples.
Let us consider classification as a case-study. After having
assessed the accuracy (or the AUC, or any other indicator)
of two competing classifiers on multiple data sets, one has
to formally check whether the difference among the two
classifiers is significant. The Wilcoxon signed-rank is used
for this purpose because, thanks to its nonparametric nature, it solves some major problems of the t-test: it does not
assume commensurability of the measures across different
data sets; it does not assume normality of the sample mean
of the accuracy; it is robust w.r.t. outliers. The signed-rank
test is moreover preferable also to the sign test, which is
nonparametric but has much lower power.
However the Wilcoxon signed-rank test is affected by all
the drawbacks which characterize the null-hypothesis significance tests (NHST). Such tests “allow one either to reject the null hypothesis or to fail to reject it, but they do not
provide any measure of evidence for the null hypothesis”
(Raftery, 1995). This prevents associating a cost to Type I
and Type II errors and taking decisions by minimizing the
expected loss. Instead, decision are taken on the basis of
the chosen significance α , namely the probability of rejecting the null hypothesis when it is true. In principle, one
should balance significance and power of the test. Yet, a
principled way of doing this is lacking. Hence, decisions
are simply taken by setting α =0.01 or 0.05, without considering the probability of Type II errors. Moreover, the
p-value and thus the outcome of the test depend on the intention of the person who has collected the data (Goodman,
1999; Kruschke, 2010).
Bayesian tests of hypothesis constitute an appealing alternative to the NHST analogous. They return the posterior probability of the null and the alternative hypotheses, which are thus fully characterized in terms of mean,
variance, credible interval and density function. While
the frequentist test can only reject the null hypothesis,
the Bayesian one can also accept the null hypothesis, on
the basis of the estimated posterior probability. Once

The Bayesian Wilcoxon signed-rank test

the costs of Type I and Type II errors are specified, a
Bayesian test allows taking decisions by minimizing the
expected loss; in this way, the size of the test can be better adapted to the actual need. The computation does not
depend on the intention of the person who collected the
data. Bayesian tests have been widely considered for clinical practice (Spiegelhalter et al., 1994) and cognitive sciences (Kruschke, 2010). They are however very rarely used
in machine learning, despite the abundance of Bayesian algorithms in this area. To the best of our knowledge, no
Bayesian version of the Wilcoxon signed-rank has been
proposed so far. We fill this gap by proposing a nonparametric Bayesian version of the Wilcoxon signed-rank test
based on the Dirichlet process (DP) (Ferguson, 1973). In
his seminal paper on the Dirichlet process, Ferguson provided a Bayesian justification of many classic nonparametric estimators (Mann-Whitney statistics, median, etc.).
Similar results were derived by other authors that, employing DP as prior model, were able to naturally obtain estimators related to the frequentist ones, e.g., Kaplan-Meier
(Susarla & Ryzin, 1976), Kendall’s tau (Dalal & Phadia,
1983). Recently there has been an increasing interest in
the development of Bayesian nonparametric procedures for
hypothesis testing focusing in particular on the two (or k)
sample problem (Borgwardt & Ghahramani, 2009; Holmes
et al., 2009; Ma & Wong, 2011; Chen & Hanson, 2014).
Two sample tests deal with unpaired samples, while in
machine learning we often work with paired observations,
e.g., when we want to compare the accuracy of two classifiers on the same collection of datasets. Moreover, an open
problem in all these procedures is how to choose the infinite dimensional parameter of the nonparametric prior in
case of lack of prior information.
Here, we address the problem of how to choose the infinite
dimensional parameter that characterizes the DP by means
of two models corresponding to two different choices of the
prior parameter: the noninformative DP prior (Dp(s = 0))
and the prior ignorance model (IDP). Dp(s = 0) is the nonparametric analogue of a Bayesian noninformative prior,
while IDP consists of a set of DPs priors and is based on
the techniques developed in Bayesian robustness (Berger
et al., 2000; 1994; Pericchi & Walley, 1991; Walley, 1991)
for modelling lack of prior information within parametric
models.
By means of simulations on artificial and real world data,
we use our test to decide if a certain classifier is significantly better than another. We show that the Bayesian test
incurs much lower costs than the frequentist one for a wide
variety of costs of Type I and Type II errors. We moreover show that the IDP test is more robust, in the sense that
it acknowledges when the decision is prior-dependent. In
other words, the IDP test suspends the judgment and becomes indeterminate when the option which minimizes the

expected loss depends on the prior. This behavior is analogous to that observed in credal classifiers (Corani & Zaffalon, 2008), which suspend the judgment when the classification is prior-dependent, namely when the most probable class varies with the prior used to induce the classifier.
The little reliability of prior-dependent decisions is confirmed by the fact that when the IDP test is indeterminate,
the Wilcoxon signed-rank and the Dp(s = 0) tests are virtually behaving as random guessers. Since IDP has all the
positive features of a Bayesian test and it is more reliable
than Wilcoxon and Dp(s = 0), we propose IDP as a new
test for comparing classifiers and other methods in machine
learning. Finally, notice that the proposed test is applicable
to many other fields of research where it has the potential
of reducing the misleading results of NHST by avoiding the
use of a significance parameter which does not represent a
correct measure of the evidence provided by data (Johnson,
2013). The IDP test developed in this work can currently
be used online (or downloaded as R or Matlab code) at
http://ipg.idsia.ch/software/IDP.php.

2. Dirichlet Process
The Dirichlet process Dp(α ) has been proposed by (Ferguson, 1973) as a probability measure on the set of probability measures on some space Z (for this paper we can
assume Z = R). It has an infinite dimensional parameter α (·), which is a positive finite measure over Z , i.e.,
α (A) > 0 (positive) for any (measurable) set A ∈ Z and
α (Z ) < ∞ (finite). Assuming that a probability measure
is drawn from DP, i.e., P ∼ Dp(α ), the characteristic of
DP is that, for any (measurable) partition B1 , . . . , Bm of Z ,
the finite vector (P(B1 ), P(B2 ), . . . , P(Bm )) is Dirichlet distributed Dir(α (B1 ), α (B2 ), . . . , α (Bm )). From the Dirichlet
distribution, we can thus derive the prior mean (E [P(Bi )] =
α (Bi )/α (Z )) and variance (E [(P(Bi ) − E [P(Bi )])2 ] =
α (Bi )(α (Z ) − α (Bi ))/α 2 (Z )(α (Z ) + 1) of P(Bi ) w.r.t.
the DP for any Bi ∈ Z .1 This shows that the normalized measure α (·)/α (Z ) of DP reflects the prior expectation of P, while the scaling parameter α (Z ) controls the
variance of P around α (·)/α (Z ). The normalized measure α (·)/α (Z ) is a probability measure, therefore, when
Z = R, it can be completely characterized by the cumulative distribution function G(z) = α (−∞, z]/α (Z ). We can
then denote the Dirichlet process by Dp(α (Z ), G).
DP is a conjugate prior in the sense that, given a sample
Z1 , . . . , Zn from F ∼ Dp(α (Z ), G) of n observations which
are conditionally independent given F, and fixed the prior
parameters α (Z ) = s and G = G0 , the posterior distribution of the cumulative distribution function F of P is still
1 We

will use calligraphic letters, E , P, to denote expectation
and probability w.r.t. the DP.

The Bayesian Wilcoxon signed-rank test

Dp(αn (Z ), Gn ) with

F w.r.t. its median M and computes the statistic:

where IA (z) is the indicator function: it is one when z ∈
A and zero otherwise. Thus, a-posteriori we have that
E [P(Z ≤ z)|Z1 , . . . , Zn ] = (sG0 (z) + n<z )/(s + n), where
n<z = ∑ni=1 I[Zi ,∞) (z) is the number of observations Zi falling
in (−∞, z].
An issue in the use of the DP as prior measure on P is how
to choose the infinite dimensional parameter G0 in case of
lack of prior information. There are two avenues that we
can follow. The first assumes that prior ignorance can be
modelled satisfactorily by a so-called noninformative prior.
In the DP setting, the only noninformative prior that has
been proposed so far is the limiting DP obtained for s → 0,
which has been introduced by (Ferguson, 1973) and discussed by (Rubin, 1981). The second approach suggests
that lack of prior information should be expressed in terms
of a set of probability distributions. This approach is known
as Bayesian robustness (Berger et al., 2000; 1994; Pericchi & Walley, 1991; Walley, 1991; Coolen-Schrijner et al.,
2009) and it has been extensively applied to model lack of
prior information in parametric models. In this paper, we
implement the limiting DP obtained for s → 0 and we also
extend the Bayesian robust approach to the nonparametric
setting by considering a set of DPs obtained by fixing s to
a strictly positive value and letting G0 span the set of all
distributions.

3. The Dirichlet Process-based Wilcoxon test
Let X n = (X1 , . . . , Xn ) and Y n = (Y1 , . . . ,Yn ) be two sequences of paired observations representing the accuracies
of classifiers X and Y on n different data sets. Define
Zi = Yi − Xi and assume for the moment that there are no
ties of type Zi = −Z j . We will discuss how to manage ties
and zeros in Section 3.2. Let F be the distribution of Z and
M its median. A one-sided test contrasts the null hypothesis M ≤ 0 against the alternative hypothesis M > 0. The
easiest test about the median of a distribution is the sign test
which counts the number of positive differences Zi > 0; this
statistic is an estimator of the probability P(Z > 0). Then,
a Bayesian analogous of the sign test would consider the
posterior probability of P(Z > 0). However, a shortcoming
of the sign-test is its low power.
The Wilcoxon signed-rank test is more powerful than the
sign test, as it accounts not only for the sign but also for the
size of the differences Zi . It does so by ranking the absolute value of the differences and then comparing the ranks
of the positive and negative differences (Demšar, 2006).
The Wilcoxon signed-rank test assumes the symmetry of

∑

T+ =

s
1 n
αn (Z ) = s + n, Gn =
G0 +
∑ I[Zi ,∞) ,
s+n
s + n i=1

{i: Zi ≥0}

ri (|Zi |) =

∑

1≤i≤ j≤n

Ti+j ,

(1)

where ri (|Zi |) is the rank of |Zi | and

1 if Zi ≥ −Z j ,
Ti+j =
0 otherwise.
The decision is taken by comparing the observed value of
2T +
against its critical value, which
the statistic R+ = n(n+1)
depends on the significance α . For a large number of data,
the distribution of R+ under the null hypothesis is approximately normal with mean 1/2. Then, in practice, considering for example a one-sided test evaluating M ≤ 0 against
M > 0, the null hypothesis is rejected when the observed
value of R+ is significantly larger (according to the significance level α ) than 1/2. Based on the definition of Ti+j , one
can interpret the statistic R+ as an estimator of the probability that, given two independent observations Z and Z ′ from
F, Z ≥ −Z ′ . This probability can be written as
P(Z ≥ −Z ′ ) = P(Z ≤ 0, Z ′ > 0, |Z ′ | ≥ |Z|)

+ P(Z > 0, Z ′ ≤ 0, |Z| ≥ |Z ′ |) + P(Z > 0, Z ′ > 0),

from which it can be noticed that P(Z ≥ −Z ′ ) considers
at the same time the probability that Z is positive and the
probability that negative differences are smaller than the
positive ones; for this reason the Wilcoxon statistic is more
sensitive than the sign test statistic to the presence of a bias
(positive if P(Z ≥ −Z ′ ) > 1/2 or negative if P(Z ≥ −Z ′ ) <
1/2) in the differences Zi . In analogy with the Wilcoxon
signed-rank test, we propose a Bayesian test based on the
probability
P(Z ≥ −Z ′ ) =

ZZ

I[−z′ ,∞) (z)d(F(z)F(z′ )) = E[I[−Z ′ ,∞) (Z)].

The test compares the hypothesis P(Z ≥ −Z ′ ) ≤ 1/2
against P(Z ≥ −Z ′ ) > 1/2. Notice that the Wilcoxon
signed-rank test needs to assume the symmetry of F to be
able to specify the distribution of R+ under the null hypothesis. In this context, another advantage of the Bayesian approach is that it does not require the symmetry assumption
since all inferences are derived from the posterior distribution of P(Z ≥ −Z ′ ) > 1/2 which follows directly from
the prior distribution for F and the sequence of observations Z n = (Z1 , . . . , Zn ). We propose the Dirichlet process
as prior for F.
Theorem 1 If F ∼ Dp(α (Z ), G), then

 ZZ
E P(Z ≥ −Z ′ ) =
I[−z′ ,∞) (z)dE [F(z)F(z′ )]
=

ZZ

I[−z′ ,∞) (z)

d [G(min(z, z′ )) + α (Z ) G(z)G(z′ )]
.
α (Z ) + 1

(2)

The Bayesian Wilcoxon signed-rank test

This result is similar to that in Lemma 2.1 of (Dalal & Phadia, 1983) for the Kendall’s tau. Its proof and that of the
next theorems can be found in the appendix (supplementary material). To use the DP for evaluating the posterior
probability of P(Z ≥ −Z ′ ) > 1/2, we must choose the base
CDF G0 . We focus on the case where we have no prior
information about the functional form of F (which would
justify the use of a nonparametric test), and about the value
of P(Z ≥ −Z ′ ), and propose a model which is capable of
modeling a situation of complete prior ignorance about the
expectation of P(Z ≥ −Z ′ ). For any choice of s and G0 the
prior and posterior expectation of P(Z ≥ −Z ′ ) can be derived from Theorem 1, by taking, respectively, α (Z ) = s
and G = G0 for the prior and α (Z ) = s + n and G = Gn
for the posterior. Since the form of G0 does not affect the
posterior for s → 0, this is a frequent choice for modeling
a noninformative prior. This prior has been introduced under the name of Bayesian Bootstrap by (Rubin, 1981). The
prior and posterior expectations, in this case, are given by
the following theorem.

be used for predictive inferences. In case of lack of prior
information, a more natural way to model prior ignorance
may be to consider the set of all distributions G0 (Walley,
1991), (Walley, 1996). In other words, we keep s fixed and
assume that G0 ∈ Γ = {all distributions}, and then compute
the lower and upper expectations for all the functions of interest in the statistical analysis. We call this model prior
near-ignorance DP (IDP).
Theorem 3 Given the DP prior Dp(s, G0 ), with G0 ∈ Γ,
the prior lower and upper expectations are obtained for
dG0 = δZ0 with Z0 < 0 (lower) and Z0 > 0 (upper), and are
E [P(Z ≥ −Z ′ )] = 0, E [P(Z ≥ −Z ′ )] = 1;

the posterior lower expectation of P(Z ≥ −Z ′ ) is obtained
for dG0 (z) = δZ0 with Z0 < − max |Zi | and is
n

E [P(Z ≥ −Z ′ )|Z n ] =

E [P(Z ≥ −Z ′ )|Z n ] =

1
n(n+1)

n

n

n

(3)
#

∑ ∑ I[−Z ,∞) (Z j ) + ∑ I[0,∞) (Z j )

i=1 j=1

i

j=1

(4)

It can be easily seen that the posterior expectation obtained
for s → 0 is equal to R+ . This shows that E [P(Z ≥ −Z ′ )
is closely related to the Wilcoxon signed-rank statistic R+ .
This result extends to the Wilcoxon signed-rank a similar
result obtained by (Ferguson, 1973) concerning the relationship between E [P(X ≤ Y )] (with X and Y representing
independent unpaired samples) and the Mann-Whitney U
statistics. Note that, although in (2) the posterior means of
P(Z ≥ −Z ′ ) and R+ appear to be closely related, the posterior distribution of P(Z ≥ −Z ′ ) is, in general, different from
that assumed for R+ under the null hypothesis (although, if
the null hypothesis is true they converge to the same distribution for large n), and thus one should not expect the
frequentist and Bayesian tests to make the same decisions
even for s → 0. The prior expectation for s → 0 depends
on the choice of the prior base measure G0 . For example, by choosing G0 symmetric around zero, we obtain a
prior expectation of 1/2. However, in a situation of complete prior ignorance, we have no reason to assign, a priori,
any specific value to the probability P(Z ≥ −Z ′ ). Moreover, Rubin has highlighted a second critical point: the
Bayesian bootstrap assigns zero posterior probability to any
set that does not include the observations, since for s → 0,
E [P(A)|Z n ] = (α (A) + nA )/(s + n) → 0 when nA = 0, i.e.,
whenever there are not observations in the set A (Rubin,
1981). This is not suitable for a Bayesian model that can

n

∑ ∑ I[−Zi ,∞) (Z j )

i=1 j=1

(s+n)(s+n+1)

n

∑ I[0,∞) (Z j )

j=1

+ (s+n)(s+n+1) ;

(6)
the posterior upper expectation is obtained for dG0 (x) =
δZ0 with Z0 > max |Zi | and is

Theorem 2 Given that F ∼ Dp(s, G0 ), for s → 0 one has

 Z
E P(Z ≥ −Z ′ ) = I[0,∞) (z)dG0 (z),
"

(5)

E [P(Z ≥ −Z ′ )|Z n ] =
n

n

∑ ∑ I[−Zi ,∞) (Z j )

i=1 j=1

.

(s+n)(s+n+1)

n

∑ I[0,∞) (Z j )

2

(7)

s +2ns+s
+ (s+n)(s+n+1) + (s+n)(s+n+1)
.
j=1

From Theorem 3 it follows that, given the prior
Dp(α (Z ), G0 ), with α (Z ) ≤ s, the posterior expectation
of P(Z ≥ −Z ′ ) will be bounded by E [P(Z ≥ −Z ′ )|Z n ] and
E [P(Z ≥ −Z ′ )|Z n ] whatever is the choice of G0 . Thus, one
should not worry about the fact the upper and lower expectations are obtained by extreme priors (Dp(s, δZ0 )), since
they are only used to identify the range of values where
the expectation provided by any other (smoother) prior will
fall. The imprecision, defined as the difference between
the upper and lower expectations, can be derived from
Theorem 3 as E [P(Z ≥ −Z ′ )|Z n ] − E [P(Z ≥ −Z ′ )|Z n ] =
s2 +2ns+s
(s+n)(s+n+1) , and goes to zero for large n. Thus, E [P(Z ≥
−Z ′ )|Z n ], E [P(Z ≥ −Z ′ )|Z n ] tend to the asymptotic limit of
R+ for n → ∞. To perform the hypothesis test, we need to
know the posterior probability of P(Z ≥ −Z ′ ) > 1/2. The
next theorem gives an important result which can be used
to efficiently approximate it by Monte Carlo sampling, in
correspondence of the atomic priors that give the upper and
lower distributions.
Theorem 4 Consider one of the limiting priors that give
the posterior lower and upper expectations in (6)–(7). Let
dFn be sampled from its posterior; then dFn = w0 δZ0 +
∑nj=1 w j δZ j , where (w0 , w1 , . . . , wn ) ∼ Dir(s, 1, . . . , 1) and,
for any a ∈ [0, 1], it holds that




P P(Z ≥ −Z ′ ) > a|Z n = P g(w· , Z n ) > a ,
(8)


P P(Z ≥ −Z ′ ) > a|Z n = P [g(w· , Z n ) > a] ,

The Bayesian Wilcoxon signed-rank test
n

n

with g(w· , Z n ) = ∑ ∑ wi w j I[−Zi ,∞) (Z j ) and
i=1 j=1

g(w·

, X n)

n

n

= w0 (2 − w0 ) + ∑ ∑ wi w j I[−Zi ,∞) (Z j ),
i=1 j=1

and P is computed w.r.t. this Dirichlet distribution.
Based on this theorem, we can numerically approximate
P and P by Monte Carlo sampling the vector of weights
(w0 , w1 , . . . , wn ) from the Dirichlet distribution. This
means that we do not need stick-breaking or other sampling
techniques specific for DP.
Let a1 be the decision of preferring classifier Y to X and a0
its opposite; we can formulate the Bayesian test in terms
of a loss function which assigns loss l1 to an error of type
I (taking the decision a1 when Y is not better than X) and
loss l0 to an error of type II (taking the decision a0 when Y
is actually better than X). To minimize the expected loss,
the decision a0 should be preferred if
l0 P[P(Z ≥ −Z ′ ) > 21 |Z n ] ≤ l1 P[P(Z ≥ −Z ′ ) ≤ 21 |Z n ],
that is
P[P(Z ≥ −Z ′ ) > 21 |Z n ] ≤

l1
,
l0 + l1

(9)

where the probability P[P(Z ≥ −Z ′ ) > 1/2|Z n] is evaluated from the posterior distribution of F. Notice that this
decision problem is the Bayesian analogous of a frequentist one-sided test with α = l0 /(l0 + l1 ) where we have used
the posterior probability of P(Z ≥ −Z ′ ) > 1/2 given the
data in place of the likelihood of the data given the null
hypothesis. In the IDP model, both the lower and upper
probabilities of P(Z ≥ −Z ′ ) > 1/2 are compared with the
threshold l1 /(l0 + l1 ) and the decision is made based on the
following rules: (i) if P > l1 /(l0 + l1 ) we prefer classifier
Y ; (ii) if P < l1 /(l0 + l1 ) we prefer classifier X; (iii) if
P > l1 /(l0 + l1 ) and P < l1 /(l0 + l1 ) we are not able to
make a decision which yields minimum expected loss for
any choice of the prior measure G0 .
Thus, the IDP test can return a determinate decision only
in the first two cases, whereas in the third case we are in
an indeterminate situation where it is not possible to reach
a decision. Notice that the fact of preferring classifier X
does not imply that X is better than Y , but only that one can
expect a smaller loss by choosing X. Indeed, if l1 > l0 , one
may prefer X even when Y is likely to be better, if the evidence in favor of Y is not sufficiently large to compensate
the larger cost in case of error. As an illustrative example,
Figure 1 shows the posterior upper and lower distributions
of P(Z ≥ −Z ′ ) obtained from n = 20 observations Zi sampled from a standard normal distribution. Based on these
posterior estimates, the test will decide in favor of classifier
Y if l1 /(l0 + l1 ) < 0.76, and conversely in favor of classifier
X if l1 /(l0 + l1 ) > 0.87. It will be instead indeterminate if
0.76 ≤ l1 /(l0 + l1 ) ≤ 0.87.

Figure 1. Posterior distributions of P(Z ≥ −Z ′ ). Area lower (upper) gives the value of P[P(Z ≥ −Z ′ ) > 1/2|Z n ] (P[P(Z ≥
−Z ′ ) > 1/2|Z n ]), i.e. the integral of the lower (upper) distribution from 1/2 to ∞.

Finally, by exploiting the results in (Janssen, 1994), it is
possible to show that the above test is asymptotically consistent as a test for P(Z ≥ −Z ′ ), in the sense that the posterior lower and upper distributions of P(Z ≥ −Z ′ ) converge
to the asymptotic distribution of the statistic R+ (a Normal distribution). Conversely, the Wilcoxon-signed rank
test is asymptotically consistent only under the assumption
that the distribution F is symmetric; when this is not the
case the test is not calibrated and, thus, asymptotically inconsistent. Consider this example: P(Z ≥ −Z ′ ) = 0.5 but
the median m is positive and, thus F is asymmetric (for instance F = wU[1, 5]+ (1 − w)U[−12, 5], with w = 0.46514,
m = 1 and U[a, b] is the uniform distribution on the interval [a, b]), which can happen if a classifier is slightly better
than the other except in a few cases where it is worse. In
this case, is one classifier better than the other? Note that
the frequentist test would neither be calibrated if the answer
is yes, nor powerful if it is not (the probability of rejecting
H0 converges to 6.5% for frequentist test). In this case our
test coherently declares that no classifier can be preferred
(it is asymptotically calibrated).
3.1. How to choose s in IDP
The value of s determines how quickly lower and upper
posterior expectations converge as the number of observations increases. A way to select a value of s is by imposing that the degree of imprecision E [P(Z ≥ −Z ′ )|Z1 ] −
E [P(Z ≥ −Z ′ )|Z1 ] is reduced to a fraction of its prior value
(E [P(Z ≥ −Z ′ )] − E [P(Z ≥ −Z ′ )] = 1) after the first observation Z1 = Y1 − X1 . A degree of imprecision close to
1 after the first observation increases the probability of an
indeterminate outcome of the test, whereas, a value close
to 0 makes the test less reliable (in fact the limiting value
of 0 corresponds to the Bayesian bootstrap which will be
shown in the next section to be less reliable than IDP).
Then the intermediate value of 1/2 is a frequent choice in
prior-ignorance modeling (Pericchi & Walley, 1991; Walley, 1996). Although this is a subjective way to choose the

The Bayesian Wilcoxon signed-rank test

(l0 , l1 ). Fig. 2 reports as a function of ∆ and for two different values of (l0 , l1 ): (i) the loss of the Dp(s = 0) test; (ii)
the loss of the Wilcoxon test; (iii) the loss of the IDP test
when it is determinate; (iv) the indeterminacy of the IDP
test, i.e., the number of runs it returns an indeterminate response divided by the total number of Monte Carlo runs.
Let us start comparing Wilcoxon versus Dp(s = 0). From
s=0

So far, it has been assumed that there is zero probability
of ties (Zi = −Z j ) and zeros (Zi = 0). Notice that the zeros can be interpreted as ties, since Zi = 0 = −Zi . If ties
are possible, the common approach to account for them is
to consider the probability [P(Z ≥ −Z ′ ) + 12 P(Z = −Z ′ )]
(Sidak et al., 1999). Note that P(Z ≥ −Z ′ ) + 12 P(Z = −Z ′ )
is equal to E[I(−z′,∞) (z) + 12 I{−z′ } (z)] which in turns is equal
to E[H(z + z′ )], where H(·) denotes the Heaviside step
function, i.e., H(z) = 1 for z > 0, H(z) = 1/2 for z = 0
and H(z) = 0 for z < 0. The procedure presented in this
section is easily extended to the case of ties by substituting I[−Zi ,∞) (Z j ) with H(Zi + Z j ) in the computation of




P P(Z ≥ −Z ′ ) > 1/2|Z n and P P(Z ≥ −Z ′ ) > 1/2|Z n .

4. Numerical Simulations

Consider a Monte Carlo experiment in which paired values
of accuracies Xi , Yi are generated for n = 30 multiple data
sets based on the Gaussian models:



 

0
Xi
σ 2 ρσ 2
∼N
,
,
(10)
∆
Yi
ρσ 2 σ 2
for i = 1, . . . , n, with ∆ (difference in accuracy) ranging
from −0.07 to 0.07 and σ = 0.12. We have selected these
values on the basis of extensive classification experiments
performed using WEKA (Witten et al., 2011). Hereafter,
due to the limited space, we only report the results for
the case ρ = 0; the results obtained with correlation (e.g.,
ρ = 0.95) lead to similar conclusions. The aim of this section is to compare three methods to evaluate if the classifier Y is better than classifier X (i.e., ∆ > 0): (i) onesided Wilcoxon signed-ranks test; (ii) Bayesian Bootstrap
Dp(s = 0); (iii) prior ignorance Dirichlet process (IDP)
model. The one-sided Wilcoxon test has been implemented
according to the conventional decision criterion: p-value
less than α = 0.05. To evaluate the performance of the
tests, we have considered the average loss produced by
each method (i.e. the proportion of wrong decisions multiplied by the corresponding loss) with different values of

Loss

3.2. Managing ties

Freq

IP−Dp determinate

1

1

0.8

0.8

0.6

0.6

Loss

degree of conservativeness (indeterminacy), it represents a
reasonable trade-off between the reliability and indeterminacy of the decision. From (6)–(7) it follows that E [P(Z ≥
s2 +3s
. Thus, by im−Z ′ )|Z1 ] − E [P(Z ≥ −Z ′ )|Z1 ] = (s+1)(s+2)
√
2
s +3s
posing that (s+1)(s+2)
= 21 , we obtain s = ( 17 − 3)/2. Observe that the lower and upper probabilities produced by a
value of s are always contained in the probability intervals
produced by larger values of s. Then whenever we are undecided for s1 we are also for s2 > s1 . Nonetheless, for
large n the distance between the upper and lower probabilities goes to zero, then also the indeterminacy goes to zero.

0.4

0.4

0.2

0.2

0

−0.05

0
∆

0.05

0

−0.05

IP−Dp indeterminacy

0
∆

0.05

Figure 2. Loss for l1 = 4 (left) and l1 = 19 (right).

the plot relative to the loss (l0 = 1, l1 = 4), it can be seen
that Wilcoxon is too conservative towards the null hypothesis. When ∆ > 0 it has a greater loss than Dp(s = 0) and,
thus, a lower power. This conservativeness can be quantified by computing the areas under the curves in Fig. 2
for the Wilcoxon and Dp(s = 0) tests, i.e., the average loss
(averaged over the Monte Carlo runs and the values of ∆).
The results are shown in Table 1 for different values of
(l0 , l1 ). It is evident that Dp(s = 0) has always lower average loss than Wilcoxon, (see in particular (l0 , l1 ) = (1, 4)
which corresponds to the plot in Fig. 2 (left)). The only
exception is for (l0 , l1 ) = (1, 19) (Fig. 2 (right)). In this
case, the DP-based tests declares that classifier Y is better
than classifier X when the posterior probability of the hypothesis ∆ > 0 is greater than 1 − α = 0.95. Thus for this
choice of the loss, Wilcoxon and DP are closely matched.
However, the choice (l0 , l1 ) = (1, 19) is extremely conservative, implying that the cost associated to a Type I error
is 19-times greater than the cost associated to a Type II error. Consider, for instance, that clinical trials (Spiegelhalter et al., 1994) are usually designed to have significance
of 5% and power 80 or 90% (the computation of power requires doing a number of assumptions). We can roughly
infer that in these cases a Type II error is regarded about
two or four times worse than a Type I error. The advantage of the Bayesian test is that one can make decisions
minimizing the expected loss. Conversely, in the frequentist Wilcoxon test, one always makes the myopic choice
α = 0.05 regardless of the loss. Table 1 shows that for
a wide variety of values of cost configurations, including
(l0 , l1 ) = (1, 2) and (l0 , l1 ) = (1, 4), the Bayesian test incurs much lower loss than the Wilcoxon signed-rank. To

The Bayesian Wilcoxon signed-rank test
Loss, l0 = 1

l1 = 1

l1 = 2

l1 = 4

l1 = 9

l1 = 19

Loss, l0 = 1

l1 = 1

l1 = 2

l1 = 4

l1 = 9

l1 = 19

Dp(s = 0)
Wilcoxon

.025
.048

.034
.049

.044
.050

.053
.054

.061
.061

IDP
Dp(s = 0)
Wilcoxon

.023
.023
.047

.031
.031
.047

.040
.040
.048

.049
.049
.051

.057
.057
.057

Table 1. Total average loss.

compare Wilcoxon and Dp(s = 0) with IDP, we distinguish
two cases: (i) the instances in which IDP is determinate;
(ii) the instances in which IDP is indeterminate. The loss
(averaged w.r.t. ∆ and the Monte Carlo runs) for the first
case is shown in Table 2, while for the second case Table 3
reports the percentage of times the Wilcoxon and Dp(s = 0)
tests have returned a wrong decision in the two cases where
the truth is H0 or, respectively, H1 . From Tables 2–3, it
can respectively be seen that: (i) in the IDP determinate instances the loss of Dp(s = 0) coincides with that of IDP;
(ii) in the IDP indeterminate instances Dp(s = 0) is almost
a random guesser. For l1 < 19, Wilcoxon test has always
greater loss than that of Dp(s = 0) and IDP in the determinate instances and it always returns H0 in the indeterminate
instances. The only exception is the case l1 = 19 where the
losses coincide in the determinate instances while, in the
indeterminate ones, Wilcoxon is a perfect random guesser.
From Fig. 2 (right) it can be seen that the percentage of runs
in which IDP is indeterminate is high (e.g., about 16% for
∆ = 0.05); this means that Dp(s = 0) and Wilcoxon are issuing an almost random answer in 16% of the cases, which
is a large percentage (a similar comment can be done for
Dp(s = 0) in the case l1 ≤ 19, see in particular l1 = 4 in
Fig. 2 (left)). For (l0 , l1 ) = (1, 19), since in the determinate instances Wilcoxon and IDP have the same loss and in
the indeterminate ones Wilcoxon is a random guesser, we
could paradoxically design a new test that coincides with
IDP in the IDP determinate instances and issues a random
answer in the indeterminate ones that overall has the same
loss of Wilcoxon. This shows that IDP is more reliable
than Wilcoxon. In fact, assume that one is trying to compare the accuracy of two classifiers to determine if “Y is
better than X” and that, given the available data, IDP is indeterminate. In such a situation the Wilcoxon test always
issues a determinate response (pretending to be able to conclude whether “Y is better than X” or not), but its response
is simply random (like tossing a coin). On the other side,
the IDP acknowledges the impossibility of making a decision (I do not know whether “Y is better than X”). In such
cases one knows that (i) her/his posterior decisions would
depend on the choice of G0 ; (ii) reaching a decision given
the observed data is difficult, and in fact the Wilcoxon behaves like a random guesser. Based on the indeterminate
outcome of the IDP test, one can for example decide to run
the classifiers on additional datasets to eliminate the indeterminacy (in fact when the number of observations goes to
infinity the indeterminacy goes to zero).

Table 2. Average loss in the IDP determinate cases.
% H0 /H1

l1 = 1

l1 = 2

l1 = 4

l1 = 9

l1 = 19

Dp(s = 0)
Wilcoxon

45 / 45
100 / 0

43 / 48
100/ 0

43/ 50
100/ 0

43/ 54
100/ 0

42/ 55
50/ 50

Table 3. % of H0 /H1 decisions in the IDP indeterminate instances
averaged over ∆ for l0 = 1 and different values of l1 .

4.1. Practical case studies
We consider three different classifiers: naive Bayes (NB)
and two variants of the tree-augmented naive Bayes (TAN)
(Friedman et al., 1997) which differ as for the score used
for learning the TAN structure. We denote such two variants of TAN as TANBDeu and TANmdl . We run the WEKA
implementation (Witten et al., 2011) of such classifiers on
70 data sets from the UCI repository: 54 classification data
sets and 16 regression data sets, which we use for classification having discretized into 4 bins the target variable.
We evaluate via 10 folds cross-validation the accuracy of
each classifier on each data set. Then we compare pairs
of classifiers via the Wilcoxon signed-rank test and its two
novel Bayesian variants (Table 4.1). We run the tests in a
one-sided fashion. When comparing NBC with a TAN the
null hypothesis is that the median accuracy of NBC is no
smaller than that of TAN; the alternative hypothesis is that
the median accuracy of TAN is instead greater than that
of NBC. The three tests consistently identify both TANs
Pair
of classifiers

Wilcoxon
p-value

DP(s=0)
P(H1 |D)

IDP
[P(H1 |D), P(H1 |D)]

NBC-TANmdl
NBC-TANBDeu
TANBDeu -TANmdl

1e-06
1e-07
.79

1
1
.26

[1, 1]
[1, 1]
[.23. .30]

Table 4. Statistical comparison of pair of classifiers. We report the
posterior probability of H1 for the test DP(s=0) and the interval of
the posterior probability of H1 for IDP.

as significantly more accurate than NBC. Indeed, TAN is
well-known to perform better than naive Bayes (Friedman
et al., 1997). For both TANs, the DP(s=0) returns probability 1 for the alternative hypothesis. Given the large sample
size (n=70), the upper and lower posterior probability of the
alternative hypothesis computed by IDP collapse on a single point, namely 1. On the other hand, the three tests consistently report no significant difference between the two
TANs. The lower and upper posteriors for this last case are
shown in Fig. 3.

The Bayesian Wilcoxon signed-rank test

returning either H0 or H1 depending on the prior. We separately evaluate the replicability of the decisions made by
the tests when the IDP test is determinate and indeterminate, as reported in Tab. 5. Strikingly, a sharp drop of
replicability affects both the Wilcoxon and the DP(s=0) test
when the IDP test becomes indeterminate. For both tests,
when assessing both pairs of classifiers, the replicability
drops from about 90% to about 50%. In practice both the
Wilcoxon and the DP(s=0) test behave as random guessers
when the IDP is indeterminate.
Figure 3. Posterior probability for TANBDeu -TANmdl .

4.2. Replicability analysis
According to (Bouckaert, 2004), a desirable test has low
Type I error, high power and high replicability. The replicability is the probability that the same conclusion is achieved
in two experiments involving the same pair of classifiers
(i.e., the null hypothesis is accepted or rejected in both
cases). We follow the experimental setup of (Demšar,
2006). We randomly draw 15 data sets among the 70 available. We repeat the drawing 1000 times. Every time we
run the statistical tests to compare the accuracy of classifiers on the drawn data sets. We consider two pairs of classifiers: NB–TANmdl and NB–TANBDeu . This yields 1000
experiments for each pair of classifiers. In the following
we describe how to measure replicability of a hypothesis
test. The outcome ei of the hypothesis test in the i-th experiment is 0 or 1 depending on whether the null hypothesis is
accepted or rejected. The replicability R is defined as:
R = 1−2

∑i (ei − e)2
,
n−1

where e is the mean outcome of the hypothesis test over
the n=1000 repetitions. Thus R ranges between 0.5 (random decisions) and 1 (perfectly repeatable decisions). To
allow a fair measure of repeatability we focus on the loss
(l0 , l1 ) = (1, 19), in which case the decisions of the frequentist (α =0.05) and that of the Bayesian tests are closely
matched, as already discussed. The IDP test suspends
Pair

NB-TANmdl
TANmdl - TANBDeu

%H1

%Ind

87
6

14
8

Replicability
Wilcoxon
DP (s=0)
Det Ind Det Ind
.88
.94

.50
.51

.88
.95

.56
.50

The behavior of the IDP test cannot be mimicked by a reject option, which would suspend the judgment whenever
the p-value of the frequentist test is close to 0.05. The
IDP test checks whether the decision to be taken is priordependent, yielding a more complex behavior than a reject
option. On the one hand the IDP test does not always gets
indeterminate when the p-value is close to 0.05; on the
other hand, in some cases it does get indeterminate when
the p-value is quite far from 0.05. This means that the indeterminacy of IDP does not only depend on the p-value ,
but on the observations (not only the statistic). However,
the median p-value of the cases in which IDP suspends
the decision is close to 0.05. Moreover, the p-values of
the cases in which IDP suspends the judgment are (almost)
symmetrically distributed around 0.05. This explains why
the replicability of the Wilcoxon test drops down to 50% in
the IDP indeterminate instances.

5. Conclusions
We have proposed a novel Bayesian method based on
the Dirichlet Processes (DP) for performing the Wilcoxon
signed-rank test. We have developed two tests: one based
on a noninformative prior and one based on a conservative
model of prior ignorance (IDP). The Bayesian approach
is more flexible than the frequentist one, as it allows for
taking decision which minimize the expected loss. Experimental results show that the prior ignorance method is more
reliable than both the frequentist test and the noninformative Bayesian one, being able to isolate instances in which
these tests are almost guessing at random. We plan to extend this approach to implement Bayesian versions of multiple nonparametric tests such as for instance the Friedman
test. In the long run, our aim is to build a statistical package
for Bayesian nonparametric tests.

Acknowledgments
Table 5. Replicability results. We denote by %H1 the proportion
of times in which the Wilcoxon test rejects the null hypothesis and
by %Ind the proportion of times in which the IDP test becomes
indeterminate.

the judgment becoming indeterminate when the decision
is prior-dependent, namely when the loss is minimized by

This work was partly supported by the Swiss NSF grants
nos. 200021 146606 / 1 and 200020 137680 / 1.

The Bayesian Wilcoxon signed-rank test

References
Berger, J. O., Moreno, E., Pericchi, L. R., Bayarri, M. J.,
Bernardo, et al. An overview of robust Bayesian analysis. Test, 3(1):5–124, 1994.
Berger, James O., Rios Insua, David, and Ruggeri, Fabrizio. Bayesian robustness. In Robust Bayesian Analysis, volume 152 of Lecture Notes in Statistics, pp. 1–32.
Springer New York, 2000.
Borgwardt, Karsten M and Ghahramani, Zoubin. Bayesian
two-sample tests. arXiv preprint arXiv:0906.4032, 2009.
Bouckaert, Remco R. Estimating replicability of classifier
learning experiments. In Proc. of the twenty-first International Conference on Machine Learning, pp. 15–22,
2004.
Chen, Yuhui and Hanson, Timothy E. Bayesian nonparametric k-sample tests for censored and uncensored data.
Computational Statistics and Data Analysis, 71(C):335–
346, 2014.
Coolen-Schrijner, Pauline, Coolen, Frank PA, Troffaes,
Matthias CM, and Augustin, Thomas. Imprecision in
statistical theory and practice. Journal of Statistical Theory and Practice, 3(1):1–9, 2009.

Holmes, C.C, Caron, F., Griffin, J.E., and Stephens, D.A.
Two-sample Bayesian nonparametric hypothesis testing.
arXiv preprint arXiv:0910.5060, 2009.
Janssen, Paul. Weighted bootstrapping of U–statistics.
Journal of statistical planning and inference, 38(1):31–
41, 1994.
Johnson, Valen E. Revised standards for statistical evidence. Proceedings of the National Academy of Sciences, 110(48):19313–19317, 2013.
Kruschke, John K. Bayesian data analysis. Wiley Interdisciplinary Reviews: Cognitive Science, 1(5):658–676,
2010.
Ma, Li and Wong, Wing Hung. Coupling optional Pólya
trees and the two sample problem. Journal of the American Statistical Association, 106(496), 2011.
Pericchi, L. R. and Walley, P. Robust Bayesian credible
intervals and prior ignorance. International Statistical
Review, pp. 1–23, 1991.
Raftery, Adrian E. Bayesian model selection in social research. Sociological methodology, 25:111–164, 1995.
Rubin, Donald B. The Bayesian Bootstrap. The Annals of
Statistics, 9(1):pp. 130–134, 1981. ISSN 00905364.

Corani, Giorgio and Zaffalon, Marco. Learning reliable
classifiers from small or incomplete data sets: the naive
credal classifier 2. The Journal of Machine Learning
Research, 9:581–621, 2008.

Sidak, Z., Sen, P.K., and Hajek, J. Theory of Rank Tests.
Probability and Mathematical Statistics. Elsevier Science, 1999. ISBN 9780080519104.

Dalal, S.R. and Phadia, E.G. Nonparametric Bayes inference for concordance in bivariate distributions. Communications in Statistics-Theory and Methods, 12(8):947–
963, 1983.

Spiegelhalter, David J, Freedman, Laurence S, and Parmar,
Mahesh KB. Bayesian approaches to randomized trials.
Journal of the Royal Statistical Society. Series A (Statistics in Society), pp. 357–416, 1994.

Demšar, Janez. Statistical comparisons of classifiers over
multiple data sets. The Journal of Machine Learning
Research, 7:1–30, 2006.

Susarla, V. and Ryzin, J. Van. Nonparametric bayesian
estimation of survival curves from incomplete observations. Journal of the American Statistical Association,
71(356):pp. 897–902, 1976. ISSN 01621459.

Derrac, Joaquı́n, Garcı́a, Salvador, Molina, Daniel, and
Herrera, Francisco. A practical tutorial on the use of
nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms.
Swarm and Evolutionary Computation, 1(1):3–18, 2011.
Ferguson, Thomas S. A Bayesian Analysis of Some Nonparametric Problems. The Annals of Statistics, 1(2):pp.
209–230, 1973. ISSN 00905364.

Trawiński, B., Graczyk, M., Telec, Z, and Lasota, T. Nonparametric statistical analysis of machine learning algorithms for regression problems. In Knowledge-Based
and Intelligent Information and Engineering Systems,
pp. 111–120. Springer, 2010.
Walley, P. Statistical Reasoning with Imprecise Probabilities. Chapman and Hall, New York, 1991.

Friedman, Nir, Geiger, Dan, and Goldszmidt, Moises.
Bayesian network classifiers. Machine learning, 29(23):131–163, 1997.

Walley, P. Inferences from multinomial data: learning
about a bag of marbles. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):3–57, 1996.

Goodman, Steven N. Toward evidence-based medical
statistics. 1: The p–value fallacy. Annals of internal
medicine, 130(12):995–1004, 1999.

Witten, Ian H, Frank, Eibe, and Hall, Mark A. Data Mining: Practical Machine Learning Tools and Techniques.
Elsevier, 2011.

