On Robustness and Regularization of Structural Support Vector Machines

MohamadAli Torkamani
Daniel Lowd
Computer and Information Science Department, University of Oregon

Abstract
Previous analysis of binary support vector machines
(SVMs) has demonstrated a deep connection between
robustness to perturbations over uncertainty sets and
regularization of the weights. In this paper, we explore
the problem of learning robust models for structured
prediction problems. We first formulate the problem
of learning robust structural SVMs when there are perturbations in the sample space, and show how we can
construct corresponding bounds on the perturbations
in the feature space. We then show that robustness to
perturbations in the feature space is equivalent to additional regularization. For an ellipsoidal uncertainty
set, the additional regularizer is based on the dual norm
of the norm that constrains the ellipsoidal uncertainty.
For a polyhedral uncertainty set, the robust optimization problem is equivalent to adding a linear regularizer in a transformed weight space related to the linear constraints of the polyhedron. We also show that
these constraint sets can be combined and demonstrate
a number of interesting special cases. This represents
the first theoretical analysis of robust optimization of
structural support vector machines. Our experimental results show that our method outperforms the nonrobust structural SVMs on real world data when the
test data distribution has drifted from the training data
distribution.

1. Introduction
Traditional machine learning methods assume that training
and test data are drawn from the same distribution. However, in many real-world applications, the distribution is
constantly changing. In some cases, such as spam filtering and fraud detection, an adversary may be actively manipulating it to defeat the learned model. In others, such as
news and political discussions, the concept changes quickly
over time and we want to be robust to these unpredictable
changes. In both scenarios, it is beneficial to optimize the
model’s performance on not just the training data but on the
worst-case manipulation of the training data, where the manipulations are constrained to some domain-specific uncerProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

ALI @ CS . UOREGON . EDU
LOWD @ CS . UOREGON . EDU

tainty set. For example, in an image classification problem,
the uncertainty set could include minor translations, rotations, noise, or color shifts of the training data. This type
of robust optimization leads to models that perform well on
points that are “close” to those in the training data.
In general, robust optimization addresses optimization
problems in which some degree of uncertainty governs the
known parameters of the model (Ben-Tal & Nemirovski,
1998; 1999; 2000; 2001; Bertsimas & Sim, 2004). Robust
linear programming is central to many of the existing formulations. For example, Bertsimas et al. (2004) show that
when the disturbance of the inputs is restricted to an ellipsoid around the true values defined by some norm, then
the robust linear programming problem can be reduced to
a convex cone program. A number of other authors have
explored the application of robust optimization to classification problems (e.g., (Lanckriet et al., 2003; El Ghaoui
et al., 2003; Bhattacharyya et al., 2004; Shivaswamy et al.,
2006)). Recently, Xu et al. (2009) showed that regularization of support vector machines (SVMs) can be derived
from a robust formulation. However, robustness for structured prediction models has remained largely unexplored.
Structured prediction problems are characterized by an exponentially large space of possible outputs, such as parse
trees or graph labelings, making this a much more challenging problem.
In this paper, we develop a general-purpose technique
for learning robust structural SVMs (Tsochantaridis et al.,
2004). Our basic approach is to consider the worst-case
corruption of the input data within some uncertainty set
and use this to define a robust formulation. This optimization problem is often much harder than standard training of
structural SVMs when written directly; we overcome this
obstacle by transforming the robust optimization problem
into a standard structural SVM learning problem with an
additional regularizer. This gives us both robustness and
computational efficiency in the structured prediction setting, as well as establishing an elegant relationship between
robustness and regularization for structural SVMs.
We demonstrate our approach on a new dataset consisting of snapshots of political blogs from 2003 through

On Robustness and Regularization of Structural Support Vector Machines

2013, based on the political blogs dataset from Adamic and
Glance (2005). Blogs are classified as liberal or conservative using both their words and link structure. To make this
more challenging, we train on blogs from 2004 but evaluate on every year, from 2003 to 2013. In this domain, we
define an uncertainty set, show how to construct an appropriate regularizer, and show that this regularization can lead
to substantially lower test error than a non-robust model.

3. Robust Structural SVMs

2. Notation and Background

Adversaries might have a wide range of goals, but in
the worst case they will antagonistically try to reduce
the accuracy of the predictive model. For structural
SVMs, the predicted output is chosen by solving ỹ =
arg maxỹ wT φ(x, ỹ), where wT φ(x, ỹ) is the classification score. Thus, an adversary’s antagonistic goal would
be to replace the true input x with a manipulated version x̃
that maximizes the classification loss ∆(y, ỹ). If the highest scoring label is not unique, we assume the adversary
tries to maximize the minimum loss in the set:

x and y denote the vectorized input and the representation
of the structured output in the training data, respectively.
For simplicity of notation, we assume a single training example, such as a single social network graph, but our results
easily extend to a set of training examples.
The feature vector φ(x, y) is a function of both inputs and
outputs (and also manipulated input or alternate outputs,
when used as the input argument). We use ∆φ(x, y, ỹ)
to refer to the difference between two feature vectors with
different outputs y and ỹ:

In this section, we motivate and define a robust formulation of structural SVMs. We begin by considering how an
adversary might modify an input in order to maximize the
prediction error, and use this to derive a definition of a robust structural SVM in sample space and feature space.
3.1. Worst-Case/Adversarial Data Manipulation

maximize min ∆(y, ỹ),

ỹ ∈ arg max wT φ(x̃, ỹ)

∆φ(x, y, ỹ) ≡ φ(x, ỹ) − φ(x, y).

ỹ6=y

x̃ ∈ S(x, y)

T

The value of w φ(x, ỹ) is called the score of labeling x
as ỹ, for the given model weights w.
∆(y, ỹ) is a scalar distance function, such as Hamming
distance, which is a measure of dissimilarity between the
true and alternate outputs.
We use k.k to refer to a general norm function and k.k∗ for
the dual norm of k.k, where kyk∗ = sup{y T x|kxk ≤ 1}.
In this paper, we focus on the derivation of robust formulations for 1-slack structural SVM (Joachims et al., 2009).
(With minor changes, the results of this paper can be applied to n-slack structural SVMs as well, but we skip them
here.) The optimization program of a 1-slack structural
SVM is:
minimize f (w) + Cζ
w,ζ

subject to

subject to

ỹ

x̃

(1)

S(x, y) is a domain-specific uncertainty set, which constrains the set of possible corrupt inputs x̃. We always assume that x ∈ S(x, y), which means x can remain unchanged. The set S(x, y) can contain a wide range of possible variations, such as the amount of affordable/possible
change in an attribute, or the restrictions that are enforced
on combinations of changes among several attributes.
The bi-level optimization program in (2) is not tractable in
general, especially when x and y have integer components.
A slightly more tractable solution is to relax the program
and only require that ỹ be scored higher than the true output
y:
maximize ∆(y, ỹ),
x̃,ỹ

ỹ

f (w) is a regularization function that penalizes “large”
weights. Depending on the application, f (w) can be any
convex function in general. Semi-homogeneous functions,
such as norms or powers of norms with power value equal
to or greater than 1, are a common choice. (A function f (z)
is semi-homogeneous if and only if f (az) = aα f (z) for
some positive α.) f (w) = 12 wT w is the most commonly
used regularization function.

subject to

wT φ(x̃, y) ≤ wT φ(x̃, ỹ)

ζ ≥ max wT ∆φ(x, y, ỹ) + ∆(y, ỹ)
where x is the vector of all input variables, y is the desired
structured query variables, and w is the vector of the model
parameters. The goal is to learn w.

(2)

x̃ ∈ S(x, y)

(3)

The maximization in (3) might be infeasible, but its Lagrangian relaxation is always feasible:
maximize λwT ∆φ(x̃, y, ỹ) + ∆(y, ỹ)
x̃,ỹ

subject to

x̃ ∈ S(x, y)

(4)

We want to attract the reader’s attention to the similarity of
(4), and the nested max operation in the constraint of (1).
In fact, λwT ∆φ(x̃, y, ỹ) + ∆(y, ỹ) is a component of the
loss function that the learner wants to minimize. In the next

On Robustness and Regularization of Structural Support Vector Machines

subsection, we reformulate the standard 1-slack structural
SVM so that the effect of adversarial manipulation of input
data will be minimized.
3.2. Robust Formulation in Sample Space
Our goal is to find a set of model parameters that perform well against the worst-case manipulated input x̃ in the
uncertainty set. We formulate this by replacing the lossaugmented margin in (1) with the worst-case adversarial
loss obtained by (4):

Note that we are not introducing any error; both functions
δ(x̃, y) and δ(x̃, ỹ) contain as many high-order approximation terms as needed for achieving infinitesimal error
introduction, although we never unpack these functions. In
fact, the difference between δ(x̃, y) and δ(x̃, ỹ) is particularly important; let δỹ (x, y, x̃) = δ(x̃, ỹ) − δ(x̃, y), then:
φ(x̃, ỹ) − φ(x̃, y)
= φ(x + ∆x, ỹ) − φ(x + ∆x, y)
= φ(x, ỹ) − φ(x, y) + δ(x̃, ỹ) − δ(x̃, y)
= ∆φ(x, y, ỹ) + δỹ (x, y, x̃)

minimize Cf (w) +
w

sup

Lλ (w, x̃, ỹ, y) (5)

x̃∈S(x,y),ỹ

where Lλ (w, x̃, ỹ, y) = λwT ∆φ(x̃, y, ỹ)+∆(y, ỹ). We
replace the maximization with a sup operator to indicate
that the maximum value might not be achieved. Both λ
and C are tunable parameters that can be determined by
cross-validation. In the following lemma we show that it
is possible to tune only one of them by performing a reparameterization.
Lemma 3.1. For semi-homogeneous f (.), the problem (5)
can be equivalently re-written in the following form:
minimize Cf (w) +
w

sup

L(w, x̃, ỹ, y) (6)

x̃∈S(x,y),ỹ

where L(w, x̃, ỹ, y) = wT ∆φ(x̃, y, ỹ) + ∆(y, ỹ)
Proof. Let w0 = λw, and C 0 = λCα . Then, for a semihomogeneous f (.), where f (aw) = aα f (w), we have
Cf (w) = λCα f (λw). Therefore, by re-parameterization
of w as w0 and C as C 0 , (5) can be rewritten as (6).
Problem (6) is similar in form to a standard structural SVM,
except that the inner maximization is done over both x̃ and
ỹ. This is potentially much harder than simply maximizing over ỹ, since the input often has a much higher dimension than the output. For example, when labeling a set of
1000 web pages, there are only 1000 labels to predict but
1,000,000 possible hyperlinks that the adversary could add
or remove. In the next subsection, we show that we can
avoid the above-mentioned computational complexity by
instead restricting the variations in the feature space.

(7)

Therefore, the manipulation of the input data affects the
margin L(.) in (6) through δỹ (x, y, x̃). In the rest of the
paper, we will use δ i to refer to the ith element of the vector
δỹ (x, y, x̃).
Clearly, δỹ depends on the specific choice of the alternate
labeling ỹ, as well as x̃, x, and y. Let ∆2 Φ(x, y) be the
set of all variations, over all ỹ and x̃:
∆2 Φ(x, y) ≡ {δ = δỹ (x, y, x̃)| ∀x̃ ∈ S(x, y), ỹ}
Note that ∆2 Φ(x, y) is independent of ỹ. In the next section, we introduce some mechanical procedures for calculating ∆2 Φ(x, y) from S(x, y), for certain choices of
S(x, y) and φ(x, y).
Lemma 3.2. Let L1 (w, x̃, ỹ, y) = wT ∆φ(x̃, y, ỹ) +
∆(y, ỹ), and L2 (w, δ, ỹ) = wT (∆φ(x, y, ỹ) + δ) +
∆(y, ỹ). Then we will have:
L2 (w, δ, ỹ) ≥

sup
δ∈∆2 Φ(x,y),ỹ

sup

L1 (w, x̃, ỹ, y)

x̃∈S(x,y),ỹ

Proof sketch. The left-hand side of the inequality is equal
to the right-hand side except that the supremum is taken
over a superset of function values. Thus, the left-hand side
cannot be any less than the right-hand side.
Now, we can rewrite the robust formulation in (6) over variations in the feature space:
minimize Cf (w) +
w

sup

L(w, δ, ỹ) (8)

δ∈∆2 Φ(x,y),ỹ

where L(w, δ, ỹ) = wT (∆φ(x, y, ỹ) + δ) + ∆(y, ỹ).
3.3. Robustness in Feature Space
Let ∆x be the disturbance in the sample space such that:
x̃ = x + ∆x. Then, by finite difference approximation1 :
φ(x̃, y) = φ(x + ∆x, y) = φ(x, y) + δ(x̃, y)
φ(x̃, ỹ) = φ(x + ∆x, ỹ) = φ(x, ỹ) + δ(x̃, ỹ)
1
For more on finite difference approximations, refer to
Smith (1985).

By Lemma 3.2, the objective of (8) is an upper-bound for
the objective of (6); therefore, the formulation of the problem in (8) is an approximate, but more tractable, solution
for (6).
In the next section, we will show that for a wide class of
uncertainty sets ∆2 Φ(x, y), problem (8) reduces to an optimization program which can be solved as efficiently as an
ordinary 1-slack structural SVM.

On Robustness and Regularization of Structural Support Vector Machines

4. Mapping the Uncertainty Sets

ple space, according to the following inequality:

In many real world problems, there exists some expert
knowledge about the uncertainty sets in the sample space.
For example, for the web page classification problem, a
spammer can modify web pages by adding and removing
words and links, but is constrained by the cost of compromising legitimate web pages, which takes time and effort,
or obfuscating spam pages, which may make them less effective at gaining clicks. We can approximate this with a
simple budget on the number of words and links the adversary can change over the entire dataset. Even when such
information is not readily available, it may be possible to
infer an uncertainty set from training data. For example, if
our dataset contains outliers, we can pair each outlier (x̃)
with the most similar non-outlier (x) and take the differences as possible directions of manipulation: ∆x = x̃ − x.
The convex hull of these difference vectors (or an approximation thereof) can be used to define an uncertainty set for
any instance.
Lemma 3.2 states that the robust formulation in feature
space is a reasonable approximation for the robust formulation in the sample space, but it does not suggest any mechanical procedure for calculating the uncertainty sets in
feature space from the ones in the sample space. We now
derive such procedures for certain types of uncertainty sets
and feature functions.
Many features of interest, including logical conjunctions,
can be represented as products of several variables. We define a multinomial feature function as a sum of many such
products:
φC (x, y) =

X

Y

(cx ,cy )∈C i∈cx

xi

Y

|δ C |p
αC |C|

(10)

cx ∈C i∈cx

cx ∈C

ables in cx ; and |C| is the number of different sets cx in
C.
The proof can be found in the supplementary material.
Now we show how to apply Lemma 4.1 to obtain bounds
over all features simultaneously. The next theorem is the
main result of this section.
Theorem 4.2. For multinomial feature functions and
spherical uncertainty sets in the sample space S(x, y) =
{x̃ | kx̃ − xkp ≤ B} (with p ≥ 1), one can construct an
ellipsoidal uncertainty set in the feature space:
∆2 Φ(x, y) = {δ| kM δkp ≤ 1}

(11)
1

where M is a diagonal matrix with

1

1

at the

B(dαi ) p |Ci | q

(i, i)th position. d, αi , and |Ci | are appropriate constants.
Proof. Assume that P = {C1 , . . . , CL } is a set of cliques
that covers all variable xi ’s. Note that such a set should exist; otherwise, some variables are never used in the model.
For each of the cliques, we form a corresponding difference
in the feature function from Eq. (7), and apply Lemma 4.1.
By adding all of the resulting inequalities, we obtain:
dim(x)

X |δ Ci |p
αi |Ci |

p
q

(9)

≤d

X

|x̃i − xi |p

i=1

= dkx̃ − xkpp ≤ dB p

i∈cy

p

Ci ∈P

⇒

|δ Ci |p

X

⇒
where C is a set of variable groups and (cx , cy ) are the index sets of the attribute and output variables in each group.
Using terminology from Markov networks, we refer to each
of these variable groups as a clique. The summation groups
together many products that share the same pattern into a
single, aggregate feature so that they may be considered
collectively. For example,
in web page classification, the
P
multinomial feature i xi,j yi could represent the number
of web pages with label 1 that contain word j. This is
equivalent to having many features with tied weights.

|x̃i − xi |p

where p ≥ 1 is an arbitrary power value and p1 + 1q = 1;
α = max|cx |(p−1) ; |cx | is the number of evidence vari-

Ci ∈P

yi

X X

≤

p
q

1

X

1
p

B p dαi |Ci | q
!p

B(dαi ) |Ci |

Ci ∈P

1
q

|δ Ci |

≤1

≤1

where αi = max |cx |(p−1) , and |cx | is the number of varicx ∈Ci

ables in cx . Since it is possible that cliques cover overlapping sets of variables, the coefficient d ≥ 1 will be used to
maintain the inequality.
Now let

1
1

1

be the diagonal entry in matrix M

B(dαi ) p |Ci | q

To relate uncertainty sets in sample space to uncertainty
sets in feature space, we begin with the following lemma,
which bounds the disturbance of a single feature.

that corresponds to feature disturbance δ Ci . For this choice
of M , kM δkp ≤ 1.

Lemma 4.1. If the feature function φC (x, y) is multinomial with 0 ≤ x, y ≤ 1, then its disturbance δ C can be
upper-bounded by a function of the variations in the sam-

We have an example of applying Theorem 4.2 in Section
6.2, which will show how this construction works in practice.

On Robustness and Regularization of Structural Support Vector Machines

Corollary 4.3. If S(x, y) = {x̃ | kx̃ − xk1 ≤ B}, then
1
M can be constructed by setting Bd
as its (i, i)th element,
which results in a tighter upper bound.

symmetric; therefore, kM −T wk∗ = kM −1 wk∗ .
= kM −1 wk∗ + sup wT ∆φ(x, y, ỹ) + ∆(y, ỹ)
ỹ

The proof can be found in the supplementary material.
By substitution, the rest of the proof is straightforward.

5. Robust Optimization Programs
Our main contribution in this paper is achieving robust formulations that can be efficiently solved. We do this by
demonstrating a connection between robustness to certain
perturbations in feature space and certain types of weight
regularization. In this section we derive formulations for
achieving robust weight learning in structural SVMs when
∆2 Φ(x, y) is an ellipsoid, a polyhedron, or the intersection
of an ellipsoid and a polyhedron.
5.1. Ellipsoidal Constrained Uncertainty
We first consider the case when the uncertainty set
∆2 Φ(x, y) is ellipsoidal. Recall that any ellipsoid can be
represented in the form of {t | kM tk ≤ 1}, where k.k is
the relevant norm.
Theorem 5.1. For ∆2 Φ(x, y) = {δ | kM δk ≤ 1} where
M is positive definite, the optimization program of the robust structural SVM in (8) reduces to the following regularized formulation of the ordinary 1-slack structural SVM:
minimize Cf (w) + kM −1 wk∗ + ζ
w,ζ

Note that Theorem 5.1 can still be applied when M is not
positive definite by using the Moore-Penrose inverse of M
instead of the regular inverse. The result in Theorem 5.1
uses the technique of robust linear programming with arbitrary norms that is introduced in Bertsimas et al. (2004).
This theorem can also be seen as a generalization of Theorem 3 in Xu et al. (2009) to structural SVMs. Theorem 5.1
shows the direct connection between the robust formulation and regularization of the non-robust formulation for
structural SVMs.
Corollary 5.2. For disturbances of the form kδk ≤ B in
the feature space, with B being a maximum budget for the
applicable changes and k.k being an arbitrary norm, robustness can be achieved by adding the regularization function Bkwk∗ to the objective.
Proof. Since kδk/B ≤ 1 ⇒ k B1 δk = k B1 Iδk ≤ 1.
Let M = B1 I, then M −1 = BI. Thus, kM −1 wk∗ =
kBIwk∗ = Bkwk∗ . By Theorem 5.1, Bkwk∗ is the appropriate regularization function.

(12)

subject to
ζ ≥ sup wT ∆φ(x, y, ỹ) + ∆(y, ỹ)

Note that M can also be seen as a tuning parameter. In
particular, if there is a low-dimensional representation of
M , then tuning M might be an option.

ỹ

where k.k∗ is the dual norm of k.k.
Proof. We begin with the robust formulation of a structural
SVM from (8), where the uncertainty set of δ is defined by
the ellipsoid kM δk ≤ 1:
minimize Cf (w) +
w

sup

L(w, δ, ỹ)

kM δk≤1,ỹ

=

wT (∆φ(x, y, ỹ) + δ) + ∆(y, ỹ)

sup
kM δk≤1,ỹ

=

sup wT δ + sup wT ∆φ(x, y, ỹ) + ∆(y, ỹ)
kM δk≤1

=

T

sup w M
kνk≤1

Corollary 5.3. If f (w) = 0, then setting M = C1 I and
k.k = k.k2 will recover the commonly used L2 -regularized
structural SVM.

L(w, δ, ỹ)

kM δk≤1,ỹ

Let ν = M δ, so that δ = M −1 ν. Then we will have:
sup

The commonly used L2 regularization can be in fact interpreted as a regularization function that enforces robustness
to disturbances in the feature space that are restricted to a
hypersphere.

Proof. If M = C1 I, then M −1 = CI. Note that the L2
norm is dual to itself. Therefore, f (w) + kM −1 wk∗2 =
0 + kCIwk2 = Ckwk2 .
Corollary 5.4. Robustness to√variations restricted by a
Mahalanobis norm kδkS = δ T Sδ ≤ 1, where S is
positive definite, is equivalent
to adding the regularization
√
function kwkS −1 = wT S −1 w to the objective.

ỹ
−1

ν + sup wT ∆φ(x, y, ỹ) + ∆(y, ỹ)
ỹ

By definition of the dual norm, supkνk≤1 (wT M −1 )ν =
kM −T wk∗ . Since M −1 is also a definite matrix, it is

Proof. Let S = U ΛU T be the spectral decomposition
1
of S. Set M =√ U Λ 2 U T and √
the norm k.k √to k.k2 .
Then kM δk2 = δ T M T M δ = δ T M 2 δ = δ T Sδ.
Therefore the resulting regularization
function will be
√
kM −1 wk∗2 = kM −1 wk2 =
wT M −T M −1 w =

On Robustness and Regularization of Structural Support Vector Machines

p
√
−1
−1
T
T
T
wT U Λ−1 U T w =
√ w UΛ 2 U UΛ 2 U w =
T
−1
w S w = kwkS −1 , Note that U T U = I because
U is a unitary matrix.
5.2. Polyhedral Constrained Uncertainty
For some problems, an ellipsoid may not be a good representation of the uncertainty set, but almost any convex
uncertainty set can be approximated by a polyhedron. In
this subsection we consider the situations in which we are
aware of the shape of the polyhedral constraints on the variations in the feature space; i.e., ∆2 Φ(x, y) = {δ|Aδ ≤
b}. The next theorem shows that polyhedral uncertainty
sets are equivalent to linear regularization in a transformed
feature space. We begin with a supporting lemma.
Lemma 5.5. If x ∈ S(x, y), then for the corresponding
∆2 Φ(x, y) = {δ|Aδ ≤ b}, b is a non-negative vector.
Proof. x ∈ S(x, y), and φ(x̃, ỹ) − φ(x̃, y) = φ(x, ỹ) −
φ(x, y) + δ. Therefore, when x̃ = x then δ = 0, so
we should have 0 ∈ ∆2 Φ(x, y). Therefore, for δ = 0,
Aδ = A0 ≤ b; i.e., b ≥ 0.
Theorem 5.6. For ∆2 Φ(x, y) = {δ|Aδ ≤ b}, the optimization program of the robust structural SVM in (8) reduces to the following ordinary 1-slack structural SVM
minimize Cf (AT λ) + λT b + ζ
λ≥0,ζ

(13)

subject to ζ ≥ sup λT A∆φ(x, y, ỹ) + ∆(y, ỹ)
ỹ

Proof. By substituting the uncertainty set ∆2 Φ(x, y) =
{δ|Aδ ≤ b} into the optimization program (8), we obtain:
minimize Cf (w) + sup L(w, δ, ỹ)
w≥0

(14)

Aδ≤b,ỹ

We can rewrite sup L(w, δ, ỹ) as:
Aδ≤b,ỹ

sup wT (∆φ(x, y, ỹ) + δ) + ∆(y, ỹ)
Aδ≤b,ỹ

=

sup wT δ + sup wT ∆φ(x, y, ỹ) + ∆(y, ỹ)
Aδ≤b

ỹ

We perform a Lagrangian relaxation on Aδ ≤ b:
=

inf sup(wT δ − λT Aδ + λT b)

λ≥0 δ

T

+ sup w ∆φ(x, y, ỹ) + ∆(y, ỹ)


inf λT b + sup(wT − λT A)δ

λ≥0

δ
T

+ sup w ∆φ(x, y, ỹ) + ∆(y, ỹ)
ỹ

δ

unless w = AT λ, therefore:


inf λT b + sup [wT ∆φ(x, y, ỹ) + ∆(y, ỹ)]

λ≥0
ỹ
=
if w = AT λ



+∞
otherwise.
Therefore (14) can be rewritten as:
minimize Cf (w) + inf λT b +
λ≥0

w≥0

T

sup w ∆φ(x, y, ỹ) + ∆(y, ỹ)
ỹ

subject to w = AT λ

(15)

By substituting w with AT λ, (15) can be equivalently written as (13). Note that by Lemma (5.5), the value of b is is
always non-negative, so no value of λ can lead the value
of the objective in the outer minimization to negative infinity.
It is a known fact that maximization (or minimization) of
L1 and L∞ norms of affine functions can be converted to
linear programs (Boyd & Vandenberghe, 2004). In the following proposition, we state that both Theorem 5.1 and
Theorem 5.6 will lead to equivalent optimization programs
in these cases.
Proposition 5.7. If the disturbances in the feature space
are restricted by some ellipsoid that is defined by L1 or L∞
norms, then the optimization program that is generated by
Theorem 5.1 can be equivalently transformed to one that is
generated by Theorem 5.6
The proof can be found in the supplementary materials.
5.3. Ellipsoidal/Polyhedral Conjunction
In some cases, the uncertainty set in feature space may resemble an ellipsoid but with additional linear constraints.
We can model this as the intersection of an ellipsoid and
a polyhedron. The following theorem describes how such
uncertainty sets can be transformed into regularizers.
Theorem 5.8. For ∆2 Φ(x, y) = {δ|kM δk ≤ 1, Aδ ≤
b}, the optimization program of the robust structural SVM
in (8) reduces to the following ordinary 1-slack structural
SVM:
minimize Cf (w) + kM −1 (w − AT λ)k∗ + bT λ + ζ
w,λ≥0,ζ

subject to
ζ ≥ sup wT ∆φ(x, y, ỹ) + ∆(y, ỹ)

ỹ

=

Note that the value of the sup(wT − λT A)δ will be +∞,

(16)

ỹ

The proof of Theorem 5.8 is a combination of the proofs
of Theorems 5.1 and 5.6. First, we perform the Lagrangian

On Robustness and Regularization of Structural Support Vector Machines

relaxation as in the proof of 5.6, and then we add the dual of
M −1 (w−AT λ) (the coefficient of δ) as the regularization
term.
The results in Theorems 5.1, 5.6, and 5.8 apply to binary
and multi-class SVMs as well simply by restricting the
space of y to a small set of values. For Theorem 5.1, this
reduces to results proved by Xu et al. (2009). For the later
theorems, we are not aware of any analogous previous work
for binary or multi-class SVMs.
Some limiting cases of Theorem 5.8 are also interesting.
For example, for a (geometrically) infinitely large polyhedron Aδ ≤ b (e.g., elements of the vector b are infinitely
large), λ must be 0, which recovers the regularization term
kM −1 wk∗ introduced in Theorem 5.1.
Let λ1 , . . . , λm be the eigenvalues of M . If min(λi ) →
+∞ (for example, a diagonal matrix with very large numbers on the diagonal), then as a result δ → 0 in the robust
formulation. Intuitively, this means that the uncertainty set
only contains the unmodified input x. In this case, M −1
approaches the zero matrix, and as a result the regularization term kM −1 (w − AT λ)k∗ fades as expected. On the
other hand, if max(λi ) → 0, then kM −1 (w − AT λ)k∗ ≈
kLM I(w−AT λ)k∗ = LM k(w−AT λ)k∗ , where LM →
+∞. Therefore, the constraint w = AT λ must be satisfied, leading to (13).

is evolving significantly, we might expect a robust model to
outperform a non-robust model when trained and tested on
different years.2
6.2. Problem Formulation
In our experiments, we use both word features and link features. We construct one multinomial
feature for each word
P
w
i and label k, φik (x, y) = j xw
ji yjk , where xji = 1 if the
jth blog contains the ith word, and yjk = 1 if the jth blog
has label k. WeP
also construct a link feature for each label
k: φk (x, y) = ij xeij yik yjk , where xeij = 1 if there is a
link from the ith blog to the jth blog.
For our constraints, we assume that the number of words
added or removed is bounded by some budget, Bw , and the
number of edges by another budget, Be . Thus, letting xw
be vector of all word-related variables, kx̃w −xw k1 ≤ Bw .
Similarly, kx̃e − xe k1 ≤ Be .
In order to construct the uncertainty set in the feature space,
we follow the construction procedure in Theorem 4.2 and
then apply Corollary 4.3. For the word features φik and
edge features φk we can construct separate uncertainty sets:
|δik |

We demonstrate the utility of our approach by applying it
to a collective classification problem.
6.1. Dataset
We introduce a new dataset based on the political blogs
dataset collected by Adamic and Glance (2005). The original dataset consists of 1490 blogs and their network structure from the 2004 presidential election period. Each blog
is labeled as liberal or conservative. We expanded this
dataset by crawling the actual blog texts in different years
to obtain a vector of 250 word features for each blog in each
yearly snapshot from 2003 to 2013. We used the internet
archive website (https://archive.org/web/) to obtain snapshots of each blog in each year. We selected the snapshot
closest to October 10th of each year and removed blogs that
were inactive for an 8 month window (4 months before and
after October 10th).
The political affiliation of a blog can thus be inferred from
both the words on the blog and its hyperlink relationships
to other blogs, which are likely to have similar political
views. Since political topics evolve quickly over time, we
expect a significant amount of concept drift over the years,
especially over the word features. Since the test distribution

X

≤

X

≤

X

w
|x̃w
ik − xik |

i

⇒

6. Experiments

≤

X

|δik |

k

w
w
w
|x̃w
ik − xik | = kx̃ − x k ≤ Bw

i,k

|δek |

|x̃eij − xeij | = kx̃e − xe k ≤ Be

i,j

In our domain there are two classes,
and conservaP1 liberal
P |δ
ik |
≤ 1, and
tive, so k ∈ {0, 1}. As a result: k=0 i 2B
w
P1 |δek |
k=0 2Be ≤ 1. Summing the equalities and dividing by
two:
X |δik | X |δek |
+
4Bw
4Be
i,k

≤

1

k

Finally, let δ = [δ11 , . . . , δnm , δe0 , δe1 ]T , where m =
250 is the number of word attributes that are chosen
from training data, and n is the number of the nodes in
the graph. Then, M is a diagonal matrix with entries
1
[ 4B1w , . . . , 4B1w , 4B
, 1 ], so we will have kM δk1 ≤ 1.
e 4Be
Note that, in this uncertainty translation, the base case of
Lemma 4.1 holds in the first place, so the inequality is in
its tightest form.
2
The expanded political blogs dataset and our robust
SVM implementation can be downloaded from the following URL: http://ix.cs.uoregon.edu/˜lowd/
robustsvmstruct.

On Robustness and Regularization of Structural Support Vector Machines

6.3. Methods and Results

Standard structural SVMs have one parameter C that
needs to be tuned. The robust method has an additional regularization parameter C 0 = 1/Be = 1/Bw
which scales the strength of the robust regularization.3
We chose these parameters from the semi-logarithmic set
{0, .001, .002, .005, .1, . . . , 10, 20, 50}. We intentionally
added 0 to this set to allow removing one of the regularization terms. We learned parameters using a cutting plane
method, implemented using the Gurobi optimization engine 5.60 (2014) for running all integer and quadratic programs. We ran for 50 iterations and selected the weights
from the iteration with the best performance on the tuning
set.
Figure 1 shows the average error rate of the robust and
non-robust formulations in each year. In 2004, both have
very similar accuracy. This is not surprising, since they
were tuned for this particular year. In years before and after 2004, the error rate increases for both models. However, the error rate of the robust model is often substantially lower than the non-robust model. We attribute this
to the fact that the robust model has additional L∞ regularization (since L∞ is the dual of the L1 uncertainty set
used). This prevents the model from relying too much on
a small set of features that may change, such as a particular political buzzword that might go out of fashion. These
results demonstrate that robust methods for learning structural SVMs can lead to large improvements in accuracy,
even when we do not have an explicit adversary or a perfect model of the perturbations.

7. Related Work
In this paper, the big picture of our formulation for robustness in the presented algorithms is based on a minimax formulation, where the learner minimizes a loss function and,
at the same time, the antagonistic adversary tries to maximize the same quantity. Some related work has focused
on designing classifiers that are robust to adversarial perturbation of the input data in a minimax formulation. For
example, Globerson and Roweis (2006) introduce a clas3
In general, Be and Bw could be tuned separately, but we did
not do this in our experiments.

40
Robust Model
Non−robust model

35

Prediction Error (%)

We partitioned the blogs into three separate sub-networks
and used three-way cross-validation, training on one subnetwork, using the next as a validation set for tuning parameters, and evaluating on the third. We used mutual information to select the 250 most informative words separately for each training set. However, rather than training,
tuning, and testing on the same year, we trained and tuned
on the snapshot from 2004 and evaluated the models on
every snapshot from 2003 to 2013.

30

25

20

15

10
2003

2004

2005

2006

2007

2008 2009
Test Year

2010

2011

2012

2013

Figure 1. Average prediction error of robust and non-robust models trained on year 2004 and evaluated on years 2003-2013.

sifier that is robust to feature deletion. Teo et al. (2008)
extend this to any adversarial manipulation that can be efficiently simulated. Livni et al. (2012) show that a minimax formulation of robustness in the presence of stochastic adversaries results in L2 (Frobenius for matrix weights)
regularization, and for the multi-class case results in twoinfinity regularization of the model weights. Torkamani
and Lowd (2013), show that for associative Markov networks, robust weight learning for collective classification
can be efficiently done with a convex quadratic program.
Xu et al.’s work on robustness and regularization (2009) is
the most related previous work, which analyzes the connection between robustness and regularization in binary
SVMs. Our work goes well beyond these results (and the
ones mentioned in the introduction) by analyzing arbitrary
structural SVMs and showing how they can be made robust
without directly simulating the adversary, by choosing the
appropriate regularization function.

Acknowledgments
We thank the anonymous reviewers for useful comments.
This research was partly funded by ARO grant W911NF08-1-0242 and NSF grant OCI-0960354. The views and
conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of
ARO, NSF, or the U.S. Government.

References
Adamic, L.A. and Glance, N. The political blogosphere
and the 2004 us election: divided they blog. In Proceedings of the 3rd International Workshop on Link Discovery, pp. 36–43. ACM, 2005.

On Robustness and Regularization of Structural Support Vector Machines

Ben-Tal, A. and Nemirovski, A. Robust convex optimization. Mathematics of Operations Research, 23(4):769–
805, 1998.
Ben-Tal, A. and Nemirovski, A. Robust solutions of uncertain linear programs. Operations Research Letters, 25
(1):1–13, 1999.
Ben-Tal, A. and Nemirovski, A. Robust solutions of linear programming problems contaminated with uncertain data. Mathematical Programming, 88(3):411–424,
2000.
Ben-Tal, A. and Nemirovski, A. On polyhedral approximations of the second-order cone. Mathematics of Operations Research, 26(2):193–205, 2001.
Bertsimas, D. and Sim, M. The price of robustness. Operations research, 52(1):35–53, 2004.
Bertsimas, D., Pachamanova, D., and Sim, M. Robust linear optimization under general norms. Operations Research Letters, 32(6):510–516, 2004.
Bhattacharyya, C., Pannagadatta, KS, and Smola, A. A second order cone programming formulation for classifying
missing data. Advances in Neural Information Processing Systems, 17:153–160, 2004.
Boyd, S. and Vandenberghe, L. Convex optimization. Cambridge University Press, 2004.
El Ghaoui, L., Lanckriet, G.R.G., and Natsoulis, G. Robust classification with interval data. Computer Science
Division, University of California, 2003.
Globerson, A. and Roweis, S. Nightmare at test time:
robust learning by feature deletion. In Proceedings of
the Twenty-Third International Conference on Machine
Learning, pp. 353–360, Pittsburgh, PA, 2006. ACM
Press.
Gurobi Optimization, Inc. Gurobi optimizer reference
manual, 2014. URL http://www.gurobi.com.
Joachims, T., Finley, T., and Yu, C.-N. J. Cutting-plane
training of structural SVMs. Machine Learning, 77(1):
27–59, 2009.
Lanckriet, G.R.G., Ghaoui, L.E., Bhattacharyya, C., and
Jordan, M.I. A robust minimax approach to classification. The Journal of Machine Learning Research, 3:
555–582, 2003.
Livni, R., Crammer, K., and Globerson, A. A simple
geometric interpretation of svm using stochastic adversaries. In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS),
volume 22, pp. 722–730, 2012.

Shivaswamy, Pannagadatta K, Bhattacharyya, C., and
Smola, A. Second order cone programming approaches
for handling missing and uncertain data. The Journal of
Machine Learning Research, 7:1283–1314, 2006.
Smith, G. Numerical solution of partial differential equations: finite difference methods. Oxford University
Press, 1985.
Teo, C.H., Globerson, A., Roweis, S., and Smola, A. Convex learning with invariances. In Advances in Neural
Information Processing Systems 21, 2008.
Torkamani, M. and Lowd, D. Convex adversarial collective
classification. In Proceedings of the 30th International
Conference on Machine Learning (ICML-13), pp. 642–
650, 2013.
Tsochantaridis, I., Hofmann, T., Joachims, T., and Altun, Y.
Support vector machine learning for interdependent and
structured output spaces. In Proceedings of the TwentyFirst International Conference on Machine Learning,
pp. 104. ACM, 2004.
Xu, H., Caramanis, C., and Mannor, S. Robustness and
regularization of support vector machines. The Journal
of Machine Learning Research, 10:1485–1510, 2009.

