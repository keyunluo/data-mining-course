Dimension-free Concentration Bounds on Hankel Matrices for Spectral
Learning

FrancÌ§ois Denis
Mattias Gybels
Aix Marseille UniversiteÌ, CNRS, LIF, 13288 Marseille Cedex 9, FRANCE

FRANCOIS . DENIS @ LIF. UNIV- MRS . FR
MATTIAS . GYBELS @ LIF. UNIV- MRS . FR

Amaury Habrard
AMAURY. HABRARD @ UNIV- ST- ETIENNE . FR
UniversiteÌ Jean Monnet de Saint-Etienne, CNRS, LaHC, 42000 Saint-Etienne Cedex 2, FRANCE

Abstract
Learning probabilistic models over strings is an
important issue for many applications. Spectral
methods propose elegant solutions to the problem of inferring weighted automata from finite
samples of variable-length strings drawn from
an unknown target distribution. These methods
rely on a singular value decomposition of a matrix HS , called the Hankel matrix, that records
the frequencies of (some of) the observed strings.
The accuracy of the learned distribution depends
both on the quantity of information embedded
in HS and on the distance between HS and its
mean Hr . Existing concentration bounds seem
to indicate that the concentration over Hr gets
looser with its size, suggesting to make a tradeoff between the quantity of used information and
the size of Hr . We propose new dimensionfree concentration bounds for several variants of
Hankel matrices. Experiments demonstrate that
these bounds are tight and that they significantly
improve existing bounds. These results suggest
that the concentration rate of the Hankel matrix
around its mean does not constitute an argument
for limiting its size.

1. Introduction
Many applications in natural language processing, text
analysis or computational biology require learning probabilistic models over finite variable-size strings such as
probabilistic automata, Hidden Markov Models (HMM),
or more generally, weighted automata. Weighted automata
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

exactly model the class of rational series, and their algebraic properties have been widely studied in that context (Droste et al., 2009). In particular, they admit algebraic
representations that can be characterized by a set of finitedimensional linear operators whose rank corresponds to the
minimum number of states needed to define the automaton.
From a machine learning perspective, the objective is then
to infer good estimates of these linear operators from finite
samples. In this paper, we consider the problem of learning
the linear representation of a weighted automaton, from a
finite sample, composed of variable-size strings i.i.d. from
an unknown target distribution.
Recently, the seminal papers of Hsu et al. (2009) for
learning HMM and Bailly et al. (2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions
over strings represented by finite state models (Siddiqi
et al., 2010; Song et al., 2010; Balle et al., 2012; Balle &
Mohri, 2012). Extensions to probabilistic models for treestructured data (Bailly et al., 2010; Parikh et al., 2011; Cohen et al., 2012), transductions (Balle et al., 2011) or other
graphical models (Anandkumar et al., 2012c;b;a; Luque
et al., 2012) have also attracted a lot of interest.
Spectral methods suppose that the main parameters of a
model can be expressed as the spectrum of a linear operator
and estimated from the spectral decomposition of a matrix
that sums up the observations. Given a rational series r, the
values taken by r can be arranged in a matrix Hr whose
rows and columns are indexed by strings, such that the linear operators defining r can be recovered directly from the
right singular vectors of Hr . This matrix is called the Hankel matrix of r.
In a learning context, given a learning sample S drawn from
a target distribution p, an empirical estimate HS of Hp is
built and then, a rational series pÌƒ is inferred from the right
singular vectors of HS . However, the size of HS increases
drastically with the size of S and state of the art approaches

Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning

consider smaller matrices HSU,V indexed by limited subset
of strings U and V . It can be shown that the above learning scheme, or slight variants of it, are consistent as soon
as the matrix HrU,V has full rank (Hsu et al., 2009; Bailly,
2011; Balle et al., 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||HSU,V âˆ’ HpU,V ||2 between the empirical Hankel matrix and its mean (Hsu et al., 2009; Bailly, 2011).
On the one hand, limiting the size of the Hankel matrix
avoids prohibitive calculations. Moreover, most existing
concentration bounds on sum of random matrices depend
on their size and suggest that ||HSU,V âˆ’ HpU,V ||2 may become significantly looser with the size of U and V , compromising the accuracy of the inferred model.
On the other hand, limiting the size of the Hankel matrix implies a drastic loss of information: only the strings
of S compatible with U and V will be considered. In
order to limit the loss of information when dealing with
restricted sets U and V , a general trend is to work with
other functionsP
than the target p, such as the prefix funcp(u)
=
b =
tion
vâˆˆÎ£âˆ— p(uv) or the factor function p
P
v,wâˆˆÎ£âˆ— p(vuw) (Balle et al., 2013; Luque et al., 2012).
These functions are rational, they have the same rank as p,
a representation of p can easily be derived from representations of p or pb and they allow a better use of the information
contained in the learning sample.
A first contribution is to provide a dimension free concentration inequality for ||HSU,V âˆ’HpU,V ||2 , by using recent results on tail inequalities for sum of random matrices showing that restricting the dimension of H is not mandatory.
However, these results cannot be directly applied to the
prefix and factor series, since the norm of the corresponding random matrices are unbounded. A second contribution of the paper is to define two classes of parametrized
functions, pÎ· and pbÎ· , that constitute continuous intermediates between p and p (resp. pb), and to provide analogous
dimension-free concentration bounds for these classes.
These bounds are evaluated on a benchmark made of 11
problems extracted from the PAutomaC challenge (Verwer
et al., 2012). These experiments show that the bounds derived from our theoretical results are quite tight - compared
to the exact values- and that they significantly improve existing bounds, even on matrices of fixed dimensions.
These results have two practical consequences for spectral learning: (i) the concentration of the empirical Hankel
matrix around its mean does not highly depend on its dimension and the only reason not to use all the information
contained in the sample should only rely on computing resources limitations. In that perspective, using random techniques to perform singular values decomposition on huge
Hankel matrices should be considered (Halko et al., 2011);

(ii) by constrast, the concentration is weaker for the prefix and factor functions, and smoothed variants should be
used, with an appropriate parameter.
The paper is organized as follows. Section 2 introduces
the main notations, definitions and concepts. Section 3
presents a first dimension free-concentration inequality for
the standard Hankel matrices. Then, we introduce the prefix and the factor variants and provide analogous concentration results. Section 4 describes some experiments before
the conclusion presented in Section 5.

2. Preliminaries
2.1. Singular Values, Eigenvalues and Matrix Norms
Let M âˆˆ RmÃ—n be a m Ã— n real matrix. The singular
values of M are the square roots of the eigenvalues of the
matrix M T M , where M T denotes the transpose of M :
Ïƒmax (M ) and Ïƒmin (M ) denote the largest and smallest
singular value of M , respectively.
In this paper, we mainly use the spectral norms || Â· ||k induced by the corresponding vector norms on Rn and dex||k
fined by ||M ||k = maxx6=0 ||M
||x||k :
â€¢ ||M ||1 = M ax1â‰¤jâ‰¤n

Pm

â€¢ ||M ||âˆ = M ax1â‰¤iâ‰¤m

i=1

Pn

|M [i, j]|,

j=1

|M [i, j]|,

â€¢ ||M ||2 = Ïƒmax (M ).
We have: ||M ||2 â‰¤

p

||M ||1 ||M ||âˆ .

These norms can be extended, under certain conditions, to
infinite matrices and the previous inequalities remain true
when the corresponding norms are defined.
2.2. Rational stochastic languages and Hankel matrices
Let Î£ be a finite alphabet. The set of all finite strings over
Î£ is denoted by Î£âˆ— , the empty string is denoted by , the
length of string w is denoted by |w| and Î£n (resp. Î£â‰¤n )
denotes the set of all strings of length n (resp. â‰¤ n). For
any string w, let Pref(w) = {u âˆˆ Î£âˆ— |âˆƒv âˆˆ Î£âˆ— w = uv}.
A series is a mapping r : Î£âˆ— P
7â†’ R. A series r is convergent
if the sequence r(Î£â‰¤n ) =
wâˆˆÎ£â‰¤n r(w) is convergent;
its limit is denoted by r(Î£âˆ— ). A stochastic language p is
a probability distribution over Î£âˆ— , i.e. a series taking non
negative values and converging to 1.
Let n â‰¥ 1 and M be a morphism defined from Î£âˆ— to
M(n), the set of n Ã— n matrices with real coefficients. For
all u âˆˆ Î£âˆ— , let us denote M (u) by Mu and Î£xâˆˆÎ£ Mx by
MÎ£ . A series r over Î£ is rational if there exists an integer n â‰¥ 1, two vectors I, T âˆˆ Rn and a morphism M :
Î£âˆ— 7â†’ M(n) such that for all u âˆˆ Î£âˆ— , r(u) = I T Mu T .

Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning

The triplet hI, M, T i is called an n-dimensional linear representation of r. The vector I can be interpreted as a vector
of initial weights, T as a vector of terminal weights and the
morphism M as a set of matrix parameters associated with
the letters of Î£. A rational stochastic language is thus a
stochastic language admitting a linear representation.
Let U, V âŠ† Î£âˆ— , the Hankel matrix HrU,V , associated with
a series r, is the matrix indexed by U Ã— V and defined
by HrU,V [u, v] = r(uv), for any (u, v) âˆˆ U Ã— V . If
U = V = Î£âˆ— , HrU,V , simply denoted by Hr , is a bi-infinite
matrix. In the following, we always assume that  âˆˆ U
and that U and V are ordered in quasi-lexicographic order:
strings are first ordered by increasing length and then, according to the lexicographic order. It can be shown that a
series r is rational if and only if the rank of the matrix Hr
is finite. The rank of Hr is equal to the minimal dimension
of a linear representation of r.
Let r be a non negative convergent rational series and let
hI, M, T i be a minimal d-dimensional linear representation
of r. Then, the sum Id +MÎ£ +. . .+MÎ£n +. . . is convergent
and r(Î£âˆ— ) = I T (Id âˆ’ MÎ£ )âˆ’1 T where Id is the identity
matrix of size d.
Several convergent rational series can be naturally associated with a stochastic language p:
P

â€¢ p, defined by p(u) = vâˆˆÎ£âˆ— p(uv), the series associated with the prefixes of the language,
P
â€¢ pb, defined by pÌ‚(u) = v,wâˆˆÎ£âˆ— p(vuw), the series associated with the factors of the language.
It can be noticed that p(u) = p(uÎ£âˆ— ), the probability that a
string begins with u, but that in general, pb(u) â‰¥ p(Î£âˆ— uÎ£âˆ— ),
the probability that a string contains u as a substring.
If hI, M, T i is a minimal d-dimensional linear representation of p, then hI, M, (Id âˆ’ MÎ£ )âˆ’1 T i (resp. h[I T (Id âˆ’
MÎ£ )âˆ’1 ]T , M, (Id âˆ’ MÎ£ )âˆ’1 T i) is a minimal linear representation of p (resp. of pb). Any linear representation of
these variants of p can be reconstructed from the others.
For any integer k â‰¥ 1, let
X
Sp(k) =
p(u1 u2 . . . uk ) = I T (Id âˆ’ MÎ£ )âˆ’k T.
u1 u2 ...uk

U,V

â€¢ H w [u, v] = 1uvâˆˆP ref (w) and
b U,V [u, v] = P
â€¢ H
w
x,yâˆˆÎ£âˆ— 1xuvy=w
for any (u, v) âˆˆ U Ã— V . For any sample of strings S, let
P
P
U,V
U,V
1
1
U,V
= |S|
and
HSU,V = |S|
wâˆˆS Hw , H S
wâˆˆS H w
P
U,V
1
U,V
b
b
H
=
Hw .
S

|S|

wâˆˆS

For example, let S = {a, ab}, U = V = {, a, b}. We have
U,V

HS

ï£«
ï£¶
0 0.5 0
= ï£­0.5 0 0.5ï£¸ ,
0
0
0

U,V

HS

ï£«
ï£¶
1 1 0
= ï£­ 1 0 0.5ï£¸ ,
0 0 0

ï£«
ï£¶
2.5 1 0.5
b U,V = ï£­ 1 0 0.5ï£¸ .
H
S
0.5 0 0

2.3. Spectral Algorithm for Learning Rational
Stochastic Languages
Rational series admit a canonical linear representation determined by their Hankel matrix. Let r be a rational series
âˆ—
of rank d and U âŠ‚ Î£âˆ— such that the matrix HrU Ã—Î£ (denoted by H in the following) has rank d.
â€¢ For any string s, let Ts be the constant matrix whose
rows and columns are indexed by Î£âˆ— and defined by
Ts [u, v] = 1 if v = us and 0 otherwise.
â€¢ Let E be a vector indexed by Î£âˆ— whose coordinates
are all zero except the first one equals to 1: E[u] =
1u= and let P be the vector indexed by Î£âˆ— defined
by P [u] = r(u).
â€¢ Let H = LDRT be a reduced singular value decomposition of H: R (resp. L) is a matrix whose columns
form a set of orthonormal vectors - the right (resp.
left) singular vectors of H - and D is a d Ã— d diagonal matrix, composed of the singular values of H.
Then, hRT E, (RT Tx R)xâˆˆÎ£ , RT P i is a linear representation of r (Bailly et al., 2009; Bailly, 2011; Balle et al.,
2012). A quick proof can be found in (Denis et al., 2013).
The basic spectral algorithm for learning rational stochastic
languages aims at identifying the canonical linear representation of the target p determined by its Hankel matrix Hp .
Let S be a sample independently drawn according to p:

âˆˆÎ£âˆ—
(1)

(2)

(3)

Clearly, p(Î£âˆ— ) = Sp = 1, p(Î£âˆ— ) = Sp and pb(Î£âˆ— ) = Sp .
P
P
(2)
Note that Sp = 1 + u |u|p(u), where u |u|p(u) is the
average length of a string drawn according to p.
Let U, V âŠ† Î£âˆ— . For any string w âˆˆ Î£âˆ— , let us define the
U,V
b wU,V by
matrices HwU,V , H w and H
â€¢ HwU,V [u, v] = 1uv=w ,

â€¢ Choose sets U, V âŠ† Î£âˆ— and build the Hankel matrix
HSU Ã—V ,
â€¢ choose a rank d and compute a reduced SVD of
HSU Ã—V truncated at rank d,
â€¢ build
the
canonical
linear
representation
hRST E, (RST Tx RS )xâˆˆÎ£ , RST PS i from the right
singular vectors RS and the empirical distribution pS
defined from S.

Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning

Alternative learning strategies consist in learning p or pb, using the same algorithm, and then to compute an estimate of
p. In all cases, the accuracy of the learned representation
mainly depends on the estimation of R. The Stewart formula (Stewart, 1990) bounds the principle angle Î¸ between
the spaces spanned by the right singular vectors of R and
RS :
||HSU Ã—V âˆ’ HrU Ã—V ||2
.
|sin(Î¸)| â‰¤
Ïƒmin (HrU Ã—V )
According to this formula, the concentration of the Hankel matrix around its mean is critical and the question of
limiting the sizes of U and V naturally arises. Note that
the Stewart inequality does not give any clear indication
on the impact or on the interest of limiting these sets. Indeed, Weylâ€™s inequalities can be used to show that both the
numerator and the denominator of the right part of the inequality increase with U and V .

3. Concentration Bounds for Hankel Matrices
Let p be a rational stochastic language over Î£âˆ— , let Î¾ be a
random variable distributed according to p, let U, V âŠ† Î£âˆ—
and let Z(Î¾) âˆˆ R|U |Ã—|V | be a random matrix. For instance,
U,V
b U,V .
Z(Î¾) may be equal to HÎ¾U,V , H Î¾ or H
Î¾
Concentration bounds for sum of random matrices can be
used to estimate the spectral distance between the empirical matrix ZS computed on the sample S and its mean
(see (Hsu et al., 2011) for references). However, most of
classical inequalities depend on the dimensions of the matrices. For example, it can be proved that with probability
at least 1 âˆ’ Î´ (Kakade, 2010):
6M
||ZS âˆ’ EZ||2 â‰¤ âˆš
N

r
p

log d +

1
log
Î´

!
(1)

where N is the size of S, d is the minimal dimension of the
matrix Z and ||Z||2 â‰¤ M almost surely. If Z = HÎ¾U,V ,
U,V

then M = 1; if Z = H Î¾ , M = â„¦(D1/2 ) in the worst
b U,V , ||Z||2 is generally unbounded.
case; if Z = H
Î¾
These concentration bounds get worse with both sizes of
the matrices. Coming back to the discussion at the end of
Section 2, they suggest to limit the size of the sets U and V ,
and therefore, to design strategies to choose optimal sets.
We then use recent results (Tropp, 2012; Hsu et al., 2011)
to obtain dimension-free concentration bounds for Hankel
matrices. More precisely, we extend a Bernstein bound for
unbounded random matrices from (Hsu et al., 2011) to non
symmetric random matrices by using the dilation principle
(Tropp, 2012).
Let Z be a random matrix, the dilation of Z is the symmet-

ric random matrix X defined by



0 Z
ZZ T
2
X=
.
Then
X
=
T
Z
0
0

0



ZT Z

and ||X||2 = ||Z||2 , tr(X 2 ) = tr(ZZ T ) + tr(Z T Z) and
||X 2 ||2 â‰¤ M ax(||ZZ T ||2 , ||Z T Z||2 ).
We can then reformulate the result that we use as follows (Denis et al., 2013).
Theorem 1. Let Î¾1 , . . . , Î¾N be i.i.d. random variables, and
for i = 1, . . . , N , let Zi = Z(Î¾i ) be i.i.d. matrices and
Xi the dilation of Zi . If there exists b > 0, Ïƒ > 0, and
k > 0 such that E[X1 ] = 0, ||X1 ||2 â‰¤ b, ||E(X12 )||2 â‰¤
Ïƒ 2 and tr(E(X12 )) â‰¤ Ïƒ 2 k almost surely, then for all t > 0,
#
"
r
N
2Ïƒ 2 t
bt
1 X
Xi ||2 >
+
â‰¤ kÂ·t(et âˆ’tâˆ’1)âˆ’1 .
P r ||
N i=1
N
3N
We will then make use of this theorem to derive our new
concentration bounds. Section 3.1 deals with the standard
case, Section 3.2 with the prefix case and Section 3.3 with
the factor case.
3.1. Concentration Bound for the Hankel Matrix HpU,V
Let p be a rational stochastic language over Î£âˆ— , let S
be a sample independently drawn according to p, and let
U, V âŠ† Î£âˆ— . In this section, we compute a bound on
||HSU,V âˆ’ HpU,V ||2 which is independent from the sizes of
U and V and holds in particular when U = V = Î£âˆ— .
Let Î¾ be a random variable distributed according to p, let
Z(Î¾) = HÎ¾U,V âˆ’ HpU,V be the random matrix defined by
Zu,v = 1Î¾=uv âˆ’ p(uv) and let X be the dilation of Z.
Clearly, E(X) = 0. In order to apply Theorem 1, it is necessary to compute the parameters b, Ïƒ and k. We first prove
a technical lemma that will provide a bound on E(X 2 ).
Lemma 1. For any u, u0 âˆˆ U , v, v 0 âˆˆ V ,
|E(Zuv Zu0 v )| â‰¤ p(u0 v) and |E(Zuv Zuv0 )| â‰¤ p(uv 0 ).
Proof.
E(Zuv Zu0 v ) = E(1Î¾=uv 1Î¾=u0 v ) âˆ’ p(uv)p(u0 v)
X
=
p(w)1w=uv 1w=u0 v âˆ’ p(uv)p(u0 v)
wâˆˆÎ£âˆ—
0

= p(u v)[1u=u0 âˆ’ p(uv)]
and
|E(Zuv Zu0 v )| â‰¤ p(u0 v).
The second inequality is proved in a similar way.
Next lemma provides parameters b, Ïƒ and k needed to apply
Theorem 1.

Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning
(2)

Lemma 2. ||X||2 â‰¤ 2, E(T r(X 2 )) â‰¤ 2Sp
(2)
||E(X 2 )||2 â‰¤ Sp .

and

U,V

P
P
Proof. 1. âˆ€u âˆˆ U ,
vâˆˆV |Zu,v | =
vâˆˆV |1Î¾=uv âˆ’
p(uv)| â‰¤ 1 + p(uÎ£âˆ— ) â‰¤ 2. Therefore, ||Z||âˆ â‰¤ 2. In
a similar way, it can be shown that ||Z||1 â‰¤ 2. Hence,
p
||X||2 = ||Z||2 â‰¤ ||Z||âˆ ||Z||1 â‰¤ 2.
2. For all (u, u0 ) âˆˆ U 2 : ZZ T [u, u0 ] =

P

vâˆˆV

3.2. Bound for the prefix Hankel Matrix HpU,V

Zu,v Zu0 ,v .

Therefore,

The random matrix Z(Î¾) = H Î¾ âˆ’ HpU,V is defined
by Z u,v = 1uvâˆˆP ref (Î¾) âˆ’ p(uv). It can easily be shown
that ||Z||2 may be unbounded if U or V are unbounded:
||Z||2 = â„¦(|Î¾|1/2 ). Hence, Theorem 1 cannot be directly
applied, which suggests that the concentration of Z around
its mean could be far weaker than the concentration of Z.
For any Î· âˆˆ [0, 1], we define a smoothed variant of p by
pÎ· (u) =

E(T r(ZZ T )) = E(

X

ZZ T [u, u])

X

â‰¤

E(Zu,v Zu,v )

uâˆˆU,vâˆˆV

â‰¤ Sp(2) .
(2)

In a similar way, it can be proved that E(T r(Z T Z)) â‰¤ Sp
(2)
and therefore, E(T r(X 2 )) â‰¤ 2Sp .
3. For any u âˆˆ U ,
X
|E(ZZ T [u, u0 ])| â‰¤
u0 âˆˆU

X

|E(Zuv Zu0 v )|

X

p(u0 v)

u0 âˆˆU,vâˆˆV

â‰¤ Sp(2) .
(2)

Hence, ||ZZ T ||âˆ â‰¤ Sp . It can be proved, in a sim(2)
(2)
ilar way, that ||Z T Z||âˆ â‰¤ Sp , ||ZZ T ||1 â‰¤ Sp and
(2)
(2)
||Z T Z||1 â‰¤ Sp . Therefore, ||X 2 ||2 â‰¤ Sp .
We can now prove the main theorem of this section:
Theorem 2. Let p be a rational stochastic language and
let S be a sample of N strings drawn i.i.d. from p. For all
t > 0,
P r ï£°||HSU,V âˆ’ HpU,V ||2 >

Î· n p(uÎ£n ).

nâ‰¥0

Proposition 1. Let p be a rational stochastic language and
let hI, (Mx )xâˆˆÎ£ , T i be a minimal linear representation of
p. Let T Î· = (Id âˆ’ Î·MÎ£ )âˆ’1 T . Then, pÎ· is rational and
hI, (Mx )xâˆˆÎ£ , T Î· i is a linear representation of pÎ· .
P
Proof. For any string u, pÎ· (u) = nâ‰¥0 I T Mu Î· n MÎ£n T =
P
I T Mu ( nâ‰¥0 Î· n MÎ£n )T = I T Mu T Î· .

u0 âˆˆU,vâˆˆV

â‰¤

ï£®

X

Note that p1 = p, p0 = p and that p(u) â‰¤ pÎ· (u) â‰¤ p(u)
for any string u. Therefore, the functions pÎ· are natural intermediates between p and p. Moreover, when p is rational,
each pÎ· is also rational.

Zu,v Zu,v )

uâˆˆU,vâˆˆV

X

Î· |x| p(ux) =

xâˆˆÎ£âˆ—

uâˆˆU

= E(

X

s

(2)

ï£¹

2Sp t
2t ï£»
+
â‰¤ 2t(et âˆ’tâˆ’1)âˆ’1 .
N
3N

Proof. Let Î¾1 , . . . , Î¾N be N independent copies of Î¾, let
Zi = Z(Î¾i ) and let Xi be the dilation of Zi for i =
1, . . . , N . Lemma 2 shows that the 4 conditions of The(2)
orem 1 are fulfilled with b = 2, Ïƒ 2 = Sp and k = 2.

Note that T can be computed from T Î· when Î· and MÎ£ are
known and therefore, it is a consistent learning strategy to
learn pÎ· from the data, for some Î·, and next, to derive p.
For any 0 â‰¤ Î· â‰¤ 1, let Z Î· (Î¾) be the random matrix defined
by
Z Î· [u, v] =

Î· |x| 1Î¾=uvx âˆ’ pÎ· (uv)

xâˆˆÎ£âˆ—

=

X

Î· |x| (1Î¾=uvx âˆ’ p(uvx)).

xâˆˆÎ£âˆ—

for any (u, v) âˆˆ U Ã— V . It is clear that E(Z Î· ) = 0 and we
show below that ||Z Î· ||2 is bounded if Î· < 1.
(k)

The moments SpÎ· can naturally be associated with pÎ· . For
any 0 â‰¤ Î· â‰¤ 1 and any k â‰¥ 1, let
(k)

X

SpÎ· =

pÎ· (u1 u2 . . . uk ).

u1 u2 ...uk âˆˆÎ£âˆ—
(k)

We have SpÎ· = I T (Id âˆ’ MÎ£ )âˆ’k (Id âˆ’ Î·MÎ£ )âˆ’1 T and it is
(k)

This bound is independent from U and V . It can be noticed
that the proof also provides a dimension dependent bound
P
(2)
by replacing Sp with (u,v)âˆˆU Ã—V p(uv), which may result in a significative improvement if U or V are small.

X

(k)

clear that Sp0 = Sp

(k)

(k+1)

and Sp1 = Sp

Lemma 3.
||Z Î· ||2 â‰¤

1
(1)
+ SpÎ· .
1âˆ’Î·

.

Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning

Proof. Indeed, let u âˆˆ U .
X
X
X
Î· |x| 1Î¾=uvx +
Î· |x| p(uvx)
|Z Î· [u, v]| â‰¤
v,xâˆˆÎ£âˆ—

vâˆˆV

Therefore, we can apply the Theorem 1 with b =

(1)

+ SpÎ· . Similarly, ||Z Î· ||1 â‰¤

1
1âˆ’Î·

+

(1)

SpÎ· , which completes the proof.
When U and V are bounded, let l be the maximal length
of a string in U âˆª V . It can easily be shown that ||Z Î· ||2 â‰¤
(1)
l + 1 + SpÎ· and therefore, in that case,
||Z Î· ||2 â‰¤ M in(l + 1,

1
(1)
) + SpÎ·
1âˆ’Î·

Lemma 4. |E(Z Î· [u, v]Z Î· [u0 , v])| â‰¤ pÎ· (u0 v), for any
u, u0 , v âˆˆ Î£âˆ— .
Proof. We have E((1Î¾=w âˆ’ p(w))(1Î¾=w0 âˆ’ p(w0 ))) =
E(1Î¾=w 1Î¾=w0 ) âˆ’ p(w)p(w0 ). Therefore,
E(Z Î· [u, v]Z Î· [u0 , v])
X

Î·

|xx0 |

p(w)1w=u0 vx0 [1w=uvx âˆ’ p(uvx)]

X

0

Î· |x | p(u0 vx0 )[

x0

X

Î· |x| (1u0 vx0 =uvx âˆ’ p(uvx))]
0

Î· |x | p(u0 vx0 ) = pÎ· (u0 v)
and |E(Z Î· [u, v]Z Î· [u0 , v])| â‰¤
P |x| x0
since âˆ’1 â‰¤ âˆ’pÎ· (uv) â‰¤
(1u0 vx0 =uvx âˆ’ p(uvx)) â‰¤ 1
xÎ·

P

x

â‰¤ 2t(et âˆ’ t âˆ’ 1)âˆ’1 .
Remark that when Î· = 0 we find back the concentration
bound of Theorem 2, and that Inequality 2 provides a bound
when Î· = 1.
3.3. Bound for the factor Hankel Matrix HpbU,V
b
b U,V âˆ’ HpbU,V is defined by
The random matrix Z(Î¾)
=H
Î¾
X
Zbu,v =
1Î¾=xuvy âˆ’ pb(uv).
x,yâˆˆÎ£âˆ—

Î· |x| (1u0 vx0 =uvx âˆ’ p(uvx))| â‰¤ 1.

Lemma 5.
T

T

(2)

(2)

||E(Z Î· Z Î· )||2 â‰¤ SpÎ· and T r(E(Z Î· Z Î· )) â‰¤ SpÎ· .
T

T

(2)

(2)

||E(Z Î· Z Î· )||2 â‰¤ SpÎ· and T r(E(Z Î· Z Î· )) â‰¤ SpÎ· .
Proof. Indeed,
T

||E(Z Î· Z Î· )||âˆ â‰¤ M axu

X

|E(Z Î· [u, v]Z Î· [u0 , v])|

u0 ,v

â‰¤

X
u0 ,v,x0

We can also define smoothed variants of pb by
X
X
pbÎ· (u) =
Î· |xy| p(xuy) =
Î· m+n p(Î£m uÎ£n )
x,yâˆˆÎ£âˆ—

x

P

i.e. |

Theorem 3. Let p be a rational stochastic language, let
S be a sample of N strings drawn i.i.d. from p and let
0 â‰¤ Î· < 1. For all t > 0,
ï£¹
ï£®
s
(2)


2S
t
t
1
pÎ·
ï£¯ U,V
(1) ï£º
||2 >
+
+ SpÎ· ï£»
P r ï£°||H Î·,S âˆ’ HpU,V
Î·
N
3N 1 âˆ’ Î·

0

Î· |xx | p(u0 vx0 )[1u0 vx0 =uvx âˆ’ p(uvx)]

x,x0

=

+

b 2 is generally unbounded. Moreover, unlike the prefix
||Z||
b 2 can be unbounded even if U and V are finite.
case, ||Z||
Hence, the Theorem 1 cannot be directly applied either.

x,x0 ,w

X

1
1âˆ’Î·

and k = 2.

0

X

=

=

(2)
SpÎ·

Î· |xx | [E(1Î¾=uvx 1Î¾=u0 vx0 ) âˆ’ p(u0 vx0 )p(uvx)]

x,x0

=

(1)
SpÎ· , Ïƒ 2

(2)

which holds even if Î· = 1.

=

(2)

E(Z Î· [u, v]Z Î· [u, v]) â‰¤ SpÎ· .

Similar computations provide all the inequalities.

1
(1)
+ SpÎ· .
1âˆ’Î·
1
1âˆ’Î·

X
u,v

(1)

Hence, ||Z Î· ||âˆ â‰¤

T

T r(E(Z Î· Z Î· )) =

v,xâˆˆÎ£âˆ—

â‰¤ (1 + Î· + . . . + Î· |Î¾|âˆ’|u| ) + SpÎ·
â‰¤

In the same way,

0

(2)

Î· |x | p(u0 vx0 ) â‰¤ SpÎ· .

m,nâ‰¥0

which have properties similar to functions pÎ· :
â€¢ p â‰¤ pbÎ· â‰¤ pb, pb1 = pb and pb0 = p,
â€¢ if hI, (Mx )xâˆˆÎ£ , T i be a minimal linear representation of p then hIbÎ· , (Mx )xâˆˆÎ£ , T Î· i, where IbÎ· = (Id âˆ’
Î·MÎ£T )âˆ’1 I, is a linear representation of pÌ‚Î· .
However, proofs of the previous Section cannot be directly
extended to pbÎ· because p is bounded by 1, a property which
is often used in the proofs, while pb is not. Next lemma
provides a tool which allows to bypass this difficulty.
Lemma 6. Let 0 < Î· â‰¤ 1. For any integer n, (n + 1)Î· n â‰¤
KÎ· where

1
if Î· â‰¤ eâˆ’1
KÎ· =
âˆ’1
(âˆ’eÎ· ln Î·)
otherwise.

Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning

Proof. Let f (x) = (x + 1)Î· x . We have f 0 (x) = Î· x (1 +
(x + 1) ln Î·) and f takes its maximum for xM = âˆ’1 âˆ’
1/ ln Î·, which is positive if and only if Î· > 1/e. We have
f (xM ) = (âˆ’eÎ· ln Î·)âˆ’1 .
Lemma 7. Let w, u âˆˆ Î£âˆ— . Then,
X
Î· |xy| 1w=xuy â‰¤ KÎ· and pb(u) â‰¤ KÎ· p(Î£âˆ— uÎ£âˆ— ).
x,yâˆˆÎ£âˆ—

Proof. Indeed, if w = xuy, then |xy| = |w| âˆ’ |u| and u
appears at most |w| âˆ’ |u| + 1 times as a factor of w.
X
pb(u) =
Î· |xy| p(xuy)
x,yâˆˆÎ£âˆ—

X

=

X

p(w)

Î· |xy| 1w=xuvy

4. Experiments
The proposed bounds are evaluated on the benchmark of
PAutomaC (Verwer et al., 2012) which provides samples of strings generated from several probabilistic automata, designed to evaluate probabilistic automata learning. Eleven problems have been selected from that benchmark for which sparsity of the Hankel matrices makes the
use of standard SVD algorithms available from NumPy or
SciPy possible. The size N of the samples is 20, 000 except for the problem 4 where N = 100, 000. Table 1 shows
some target properties of the selected problems: the size of
(k)
the alphabets and the exact values of Sp computed for the
different targets p. Figure 1 shows the typical behavior of
(1)
(1)
SpÎ· and SpbÎ· , similar for all the problems.

x,yâˆˆÎ£âˆ—

wâˆˆÎ£âˆ— uÎ£âˆ—
âˆ—

âˆ—

â‰¤ KÎ· p(Î£ uÎ£ ).

For Î· âˆˆ [0, 1], let ZbÎ· (Î¾) be the random matrix defined by
X
ZbÎ· [u, v] =
Î· |xy| 1Î¾=xuvy âˆ’ pbÎ· (uv)
x,yâˆˆÎ£âˆ—

X

=

Î· |xy| (1Î¾=xuvy âˆ’ p(xuvy)).

x,yâˆˆÎ£âˆ—

and, for any k â‰¥ 0, let
(k)

SpbÎ· =

X

pbÎ· (u1 u2 . . . uk ).

u1 u2 ...uk âˆˆÎ£âˆ—
(k)

T
bÎ· ) = 0, S
It can easily be shown that E(Z
p
bÎ· = I (Id âˆ’
(k)

(k)

Î·MÎ£ )âˆ’1 (Id âˆ’ MÎ£ )âˆ’k (Id âˆ’ Î·MÎ£ )âˆ’1 T , Spb0 = Sp

(k)
Spb1

=

(1)

(1)

Figure 1. Behavior of SpÎ· and SpbÎ· for Î· âˆˆ [0; 1].

and

(k+2)
.
Sp

For each problem, the exact value of ||HSU,V âˆ’ HpU,V ||2
is computed for sets U and V of the form Î£â‰¤l , trying to
maximize l according to our computing resources. It is
(1)
âˆ’2
compared to the bounds provided by Theorem 2 and Equab
||ZÎ· ||2 â‰¤ (1 âˆ’ Î·) + SpbÎ· .
tion (1), with Î´ = 0.05 (Table 2). The optimized bound
(â€opt.â€), refers to the case where Ïƒ 2 has been calculated
Eventually, we can apply the Theorem 1 with b = (1 âˆ’
over U Ã— V rather than Î£âˆ— Ã— Î£âˆ— (see the remark at the end
(1)
(2)
Î·)âˆ’2 + SpbÎ· , Ïƒ 2 = KÎ· SpbÎ· and k = 2 (Denis et al., 2013).
of Section 3.1). Tables 3 and 4 show analog comparisons
Theorem 4. Let p be a rational stochastic language, let
for the prefix and the factor cases with different values of Î·.
S be a sample of N strings drawn i.i.d. from p and let
Similar results have been obtained for all the problems of
0 â‰¤ Î· < 1. For all t > 0,
PautomaC. We can remark that our dimension-free bounds
ï£®
ï£¹
s
are significantly more accurate than the one provided by
(2)


2KÎ· SpbÎ· t
t
1
ï£¯ b U,V
(1) ï£º
U,V
Equation (1). Notice that in the prefix case, the dimensionP r ï£°||H
âˆ’
H
||
>
+
+
S
ï£»
2
Î·,S
p
bÎ·
p
bÎ·
N
3N (1 âˆ’ Î·)2
free bound has a better behavior in the limit case Î· = 1
than the bound from Eq. (1). This is due to the fact that
â‰¤ 2t(et âˆ’ t âˆ’ 1)âˆ’1 .
in our bound, the term that bounds ||Z||2 appears in the N1
term while it appears in the âˆš1N term in the other one.
Remark that when Î· = 0 we find back the concentration
bound of Theorem 2. We provide experimental evaluation
Additional experiments confirm the implications of these
of the proposed bounds in the next Section.
results for spectral learning (see (Denis et al., 2013)).
It can be shown that ||ZbÎ· ||2 is bounded if Î· < 1.
Lemma 8. (Denis et al., 2013)

Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning

Problem number
|Î£|
(2)
Sp
(3)
Sp

3
4
8.23
57.84

4
4
6.25
31.06

Table 1. Properties of the problem set.
7
15
25
29
31
13
14
10
6
5
6.52
13.40
10.65
6.35
6.97
29.61 160.92 93.34 38.11 43.53

38
10
8.09
65.87

Table 2. Concentration values from various bounds for ||HSU,V âˆ’ HpU,V ||2
Problem number
3
4
7
15
25
29
31
l
8
9
8
5
5
9
7
||HSU,V âˆ’ HpU,V ||2 0.0052 0.0030 0.0064 0.0037 0.0033 0.0045 0.0051
Eq. (1)
0.1910 0.0857 0.1917 0.1909 0.1935 0.1908 0.1911
Th. 2 (dim. free)
0.0669 0.0260 0.0595 0.0853 0.0761 0.0588 0.0615
Th. 2 (opt. U, V )
0.0475 0.0228 0.0527 0.0284 0.0323 0.0472 0.0437

U,V

U,V
||2
Table 3. Concentration values from various bounds for ||H S âˆ’ Hp,Î·
lines and Î· = 1 for the last four lines.
Problem number
3
4
7
15
25
l
8
9
8
5
5
U,V
U,V
||2 0.0067 0.0035 0.0085 0.0043 0.0041
||H S âˆ’ Hp,Î·
Eq. (1)
0.7463 0.3326 0.7545 0.7515 0.7626
Th. 3 (dim. free)
0.0890 0.0339 0.0777 0.1162 0.1026
Th. 3 (opt. U, V )
0.0636 0.0299 0.0697 0.0398 0.0457
U,V

U,V
||2
âˆ’ Hp,Î·
Eq. (1)
Th. 3 (dim. free)
Th. 3 (opt. U, V )

||H S

0.0141
3.1011
0.1784
0.1281

0.0059
1.3079
0.0582
0.0518

0.0217
2.7839
0.1279
0.1166

0.0124
3.5129
0.2967
0.1062

0.0145
3.0283
0.2261
0.1057

39
14
8.82
90.81

40
14
9.74
111.84

for U = V = Î£â‰¤l .
38
39
4
6
0.0058 0.0049
0.1852 0.1925
0.0663 0.0692
0.0275 0.0325

We have provided dimension-free concentration inequalities for Hankel matrices in the context of spectral learning of rational stochastic languages. These bounds cover 3
cases, each one corresponding to a specific way to exploit
the strings under observation, paying attention to the strings
themselves, to their prefixes or to their factors. For the last
two cases, we introduced parametrized variants which allow a trade-off between the rate of the concentration and
the exploitation of the information contained in data.
A consequence of these results is that there is no a priori
good reason, aside from computing resources limitations,
to restrict the size of the Hankel matrices. This suggests
an immediate future work consisting in investigating recent
random techniques (Halko et al., 2011) to compute singular values decomposition on Hankel matrices in order to be

40
4
0.0037
0.1829
0.0728
0.0243

(prefix case) for U = V = Î£â‰¤l with Î· =

1
2

42
7
0.0054
0.1936
0.0634
0.0378

for the first four

29
9
0.0055
0.7250
0.0770
0.0621

31
7
0.0073
0.7369
0.0811
0.0577

38
4
0.0059
0.7051
0.0884
0.0366

39
6
0.0061
0.7068
0.0931
0.0432

40
4
0.0044
0.6753
0.0983
0.0317

42
7
0.0062
0.7146
0.0844
0.0498

0.0116
2.9286
0.1450
0.1175

0.0182
2.6695
0.1547
0.1099

0.0132
2.2395
0.1899
0.0778

0.0135
2.8524
0.2230
0.1020

0.0089
2.5132
0.2472
0.0761

0.0127
2.7863
0.1846
0.1077

b U,V âˆ’ H U,V ||2 (factor case) for U = V = Î£â‰¤l
Table 4. Concentration values from various bounds for ||H
S
pÌ‚,Î·
Problem number
3
4
7
15
25
29
31
38
39
l
6
7
5
4
4
6
6
4
4
b U,V âˆ’ H U,V ||2 0.0065 0.0031 0.0071 0.0042 0.0033 0.0051 0.0072 0.0061 0.0065
||H
S
p
b,Î·
Eq. (1)
0.9134 0.4107 0.9196 0.9466 0.9152 0.9096 0.9219 0.8765 0.8292
Th. 4 (dim. free)
0.0985 0.0374 0.0858 0.1292 0.1139 0.0849 0.0895 0.0979 0.1033
Th. 4 (opt. U, V )
0.0601 0.0300 0.0619 0.0364 0.0412 0.0559 0.0589 0.0405 0.0356

5. Conclusion

42
9
7.39
62.11

and Î· = 1/e.
40
42
4
5
0.0047 0.0060
0.8796 0.8565
0.1092 0.0934
0.0349 0.0444

able to deal with huge matrices. Then, a second aspect is to
evaluate the impact of these methods on the quality of the
models, including an empirical evaluation of the behavior
of the standard approach and its prefix and factor extensions, along with the influence of the parameter Î·.
Another research direction would be to link up the prefix
and factor cases to concentration bounds for sum of random
tensors and to generalize the results to the case where a
fixed number â‰¥ 1 of factors is considered for each string.

Acknowledments
This work was supported by the French National Agency
for Research (Lampada - ANR-09-EMER-007).

Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning

References
Anandkumar, A., Foster, D.P., Hsu, D., Kakade, S., and
Liu, Y.-K. A spectral algorithm for latent dirichlet allocation. In Proceedings of NIPS, pp. 926â€“934, 2012a.
Anandkumar, A., Hsu, D., Huang, F., and Kakade, S.
Learning mixtures of tree graphical models. In Proceedings of NIPS, pp. 1061â€“1069, 2012b.
Anandkumar, A., Hsu, D., and Kakade, S.M. A method of
moments for mixture models and hidden markov models. Proceedings of COLT - Journal of Machine Learning
Research - Proceedings Track, 23:33.1â€“33.34, 2012c.
Bailly, R. MeÌthodes spectrales pour lâ€™infeÌrence grammaticale probabiliste de langages stochastiques rationnels.
PhD thesis, Aix-Marseille UniversiteÌ, 2011.
Bailly, R., Denis, F., and Ralaivola, L. Grammatical inference as a principal component analysis problem. In
Proceedings of ICML, pp. 5, 2009.
Bailly, R., Habrard, A., and Denis, F. A spectral approach
for probabilistic grammatical inference on trees. In Proceedings of ALT, pp. 74â€“88, 2010.
Balle, B. and Mohri, M. Spectral learning of general
weighted automata via constrained matrix completion.
In Proceedings of NIPS, pp. 2168â€“2176, 2012.
Balle, B., Quattoni, A., and Carreras, X. A spectral learning
algorithm for finite state transducers. In Proceedings of
ECML/PKDD (1), pp. 156â€“171, 2011.
Balle, B., Quattoni, A., and Carreras, X. Local loss optimization in operator models: A new insight into spectral
learning. In Proceedings of ICML, 2012.
Balle, B., Carreras, X., Luque, F. M., and Quattoni, A.
Spectral learning of weighted automata: A forwardbackward perspective. To appear in Machine Learning,
2013.
Cohen, Shay B., Stratos, Karl, Collins, Michael, Foster,
Dean P., and Ungar, Lyle H. Spectral learning of LatentVariable PCFGs. In ACL (1), pp. 223â€“231. The Association for Computer Linguistics, 2012. ISBN 978-1937284-24-4.
Denis, F., Gybels, M., and Habrard, A. Dimension-free
Concentration Bounds on Hankel Matrices for Spectral
Learning (Long Version). arXiv:1312.6282, 2013.
Droste, M., Kuich, W., and Vogler, H. (eds.). Handbook of
Weighted Automata. Springer, 2009.

Halko, N., Martinsson, P.G., and Tropp, J.A. Finding
structure with randomness: Probabilistic algorithms for
constructing approximate matrix decompositions. SIAM
Rev., 53(2):217â€“288, 2011.
Hsu, D., Kakade, S.M., and Zhang, T. A spectral algorithm
for learning hidden markov models. In Proceedings of
COLT, 2009.
Hsu, D., Kakade, S. M., and Zhang, T. Dimension-free tail
inequalities for sums of random matrices. ArXiv e-prints,
2011.
Kakade, S. Multivariate analysis, dimensionality reduction,
and spectral methods. Lecture Notes (Matrix Concentration Derivations), 2010.
Luque, F.M., Quattoni, A., Balle, B., and Carreras, X.
Spectral learning for non-deterministic dependency parsing. In Proceedings of EACL, pp. 409â€“419, 2012.
Parikh, A.P., Song, L., and Xing, E.P. A spectral algorithm for latent tree graphical models. In Proceedings of
ICML, pp. 1065â€“1072, 2011.
Siddiqi, S., Boots, B., and Gordon, G.J. Reduced-rank hidden Markov models. In Proceedings of the Thirteenth
International Conference on Artificial Intelligence and
Statistics (AISTATS-2010), 2010.
Song, L., Boots, B., Siddiqi, S.M., Gordon, G.J., and
Smola, A.J. Hilbert space embeddings of hidden markov
models. In Proceedings of ICML, pp. 991â€“998, 2010.
Stewart, G. W. Perturbation theory for the singular value
decomposition. In SVD and Signal Processing II: Algorithms, Analysis and Applications, pp. 99â€“109. Elsevier,
1990.
Tropp, J.A. User-friendly tail bounds for sums of random
matrices. Foundations of Computational Mathematics,
12(4):389â€“434, 2012.
Verwer, S., Eyraud, R., and de la Higuera, C. Results of
the PAutomaC probabilistic automaton learning competition. Journal of Machine Learning Research - Proceedings Track, 21:243â€“248, 2012.

