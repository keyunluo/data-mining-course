Counterfactual Risk Minimization: Learning from Logged Bandit Feedback

Adith Swaminathan
Cornell University, Ithaca, NY 14853 USA

ADITH @ CS . CORNELL . EDU

Thorsten Joachims
Cornell University, Ithaca, NY 14853 USA

TJ @ CS . CORNELL . EDU

Abstract
We develop a learning principle and an efficient
algorithm for batch learning from logged bandit
feedback. This learning setting is ubiquitous in
online systems (e.g., ad placement, web search,
recommendation), where an algorithm makes a
prediction (e.g., ad ranking) for a given input
(e.g., query) and observes bandit feedback (e.g.,
user clicks on presented ads). We first address
the counterfactual nature of the learning problem
through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk
estimator. These constructive bounds give rise
to the Counterfactual Risk Minimization (CRM)
principle. We show how CRM can be used
to derive a new learning method – called Policy Optimizer for Exponential Models (POEM)
– for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efficient
stochastic gradient optimization. POEM is evaluated on several multi-label classification problems showing substantially improved robustness
and generalization performance compared to the
state-of-the-art.

1. Introduction
Log data is one of the most ubiquitous forms of data available, as it can be recorded from a variety of systems (e.g.,
search engines, recommender systems, ad placement) at little cost. The interaction logs of such systems typically contain a record of the input to the system (e.g., features describing the user), the prediction made by the system (e.g.,
a recommended list of news articles) and the feedback (e.g.,
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

number of ranked articles the user read) (Li et al., 2010).
The feedback, however, provides only partial information
– “bandit feedback”– limited to the particular prediction
shown by the system. The feedback for all the other predictions the system could have made is typically not known.
This makes learning from log data fundamentally different
from supervised learning, where “correct” predictions (e.g.,
the best ranking of news articles for that user) together with
a loss function provide full-information feedback.
We study the problem of batch learning from logged bandit feedback. Unlike online learning with bandit feedback,
batch learning does not require interactive experimental
control over the system. Furthermore, it enables the reuse
of existing data and offline cross-validation techniques for
model selection (e.g., “should we perform feature selection?”, “which learning algorithm to use?”, etc.).
To solve this batch-learning problem, we first need a counterfactual estimator (Bottou et al., 2013) of a system’s performance, so that we can estimate how other systems would
have performed if they had been in control of choosing predictions. Such estimators have been developed recently for
the off-policy evaluation problem (Langford et al., 2011),
(Li et al., 2011), (Li et al., 2014), where data collected from
the interaction logs of one bandit algorithm is used to evaluate another system.
Our approach to counterfactual learning centers around the
insight that, to perform robust learning, it is not sufficient to
have just an unbiased estimator of the off-policy system’s
performance. We must also reason about how the variances of these estimators differ across the hypothesis space,
and pick the hypothesis that has the best possible guarantee
(tightest conservative bound) for its performance. We first
prove generalization error bounds analogous to structural
risk minimization (Vapnik, 1998) for a stochastic hypothesis family using an empirical Bernstein argument (Maurer
& Pontil, 2009). The constructive nature of these bounds
suggests a general principle – Counterfactual Risk Minimization (CRM) – for designing methods for batch learning from bandit feedback.

Counterfactual risk minimization

Using the CRM principle, we derive a new learning algorithm – Policy Optimizer for Exponential Models (POEM)
– for structured output prediction. The training objective
is decomposed using repeated variance linearization, and
optimizing it using AdaGrad (Duchi et al., 2011) yields a
fast and effective algorithm. We evaluate POEM on several
multi-label classification problems, verify that its empirical
performance supports the theory, and demonstrate substantial improvement in generalization performance over the
state-of-the-art.
We review existing approaches in Section 2. The learning
setting is detailed in Section 3, and contrasted with supervised learning. In Section 4, we derive the Counterfactual
Risk Minimization learning principle and provide a rule of
thumb for setting hyper-parameters. In Section 5, we instantiate the CRM principle for structured output prediction
using exponential models and construct an efficient decomposition of the objective for stochastic optimization. Empirical evaluations are reported in Section 6 and we conclude with future directions and discussion in Section 7.

2. Related Work
Existing approaches for batch learning from logged bandit feedback fall into two categories. The first approach
is to reduce the problem to supervised learning. In principle, since the logs give us an incomplete view of the feedback for different predictions, one could first use regression
to estimate a feedback oracle for unseen predictions, and
then use any supervised learning algorithm using this feedback oracle. Such a two-stage approach is known to not
generalize well (Beygelzimer & Langford, 2009). More
sophisticated techniques using a cost weighted classification (Zadrozny et al., 2003) or the Offset Tree algorithm
(Beygelzimer & Langford, 2009) allow us to perform batch
learning when the space of possible predictions is small. In
contrast, our approach generalizes structured output prediction, with exponential-sized prediction spaces.
The second approach to batch learning from bandit feedback uses propensity scoring (Rosenbaum & Rubin, 1983)
to derive unbiased estimators from the interaction logs
(Bottou et al., 2013). These estimators are used for a small
set of candidate policies, and the best estimated candidate is
picked via exhaustive search. In contrast, our approach can
be optimized via gradient descent, over hypothesis families
(of infinite size) that are equally as expressive as those used
in supervised learning.
Our approach builds on counterfactual estimators that have
been developed for off-policy evaluation. The inverse
propensity scoring estimator can be optimal when we have
a good model of the historical algorithm (Strehl et al.,
2010), (Li et al., 2014), (Li et al., 2015), and doubly ro-

bust estimators are even more efficient when we additionally have a good model of the feedback (Langford et al.,
2011). In our work, we focus on the inverse propensity
scoring estimator, but the results we derive hold equally
for the doubly robust estimators. Recent work (Thomas
et al., 2015) has additionally developed tighter confidence
bounds for counterfactual estimators, which can be directly
co-opted in our approach to counterfactual learning.
In the current work, we concentrate on the case where
the historical algorithm was a stationary, stochastic policy.
Techniques like exploration scavenging (Langford et al.,
2008) and bootstrapping (Mary et al., 2014) allow us to
perform counterfactual evaluation even when the historical
algorithm was deterministic or adaptive.
Our strategy of picking the hypothesis with the tightest conservative bound on performance mimics similar successful approaches in other problems like supervised learning
(Vapnik, 1998), risk averse multi-armed bandits (Galichet
et al., 2013), regret minimizing contextual bandits (Langford & Zhang, 2008) and reinforcement learning (Garcia &
Fernandez, 2012).
Beyond the problem of batch learning from bandit feedback, our approach can have implications for several applications that require learning from logged bandit feedback
data: warm-starting multi-armed bandits (Shivaswamy &
Joachims, 2012) and contextual bandits (Strehl et al.,
2010), pre-selecting retrieval functions for search engines
(Hofmann et al., 2013), and policy evaluation for contextual bandits (Li et al., 2011), to name a few.

3. Learning Setting: Batch Learning with
Logged Bandit Feedback
Consider a structured output prediction problem that takes
as input x ∈ X and outputs a prediction y ∈ Y. For example, in multi-label document classification, x could be a
news article and y a bitvector indicating the labels assigned
to this article. The inputs are assumed drawn from a fixed
i.i.d.
but unknown distribution Pr(X ), x ∼ Pr(X ). Consider
a hypothesis space H of stochastic policies. A hypothesis
h(Y | x) ∈ H defines a probability distribution over the
output space Y, and the hypothesis makes predictions by
sampling, y ∼ h(Y | x). Note that this definition also includes deterministic hypotheses, where the distributions assign probability 1 to a single y. For notational convenience,
denote h(Y | x) by h(x), and the probability assigned by
h(x) to y as h(y | x).
In interactive learning systems, we only observe feedback
δ(x, y) for the y sampled from h(x). In this work, feedback
δ : X ×Y 7→ R is a cardinal loss that is only observed at the
sampled data points. Small values for δ(x, y) indicate user

Counterfactual risk minimization
Table 1. Comparison of assumptions, hypotheses and learning principles for supervised learning and batch learning with bandit feedback.
Setting
Supervised
Batch w/bandit

Distribution

Data, D

Hypothesis, h

Loss

(x,y ∗ ) ∼ Pr(X ×Y)

{xi ,yi∗ }

y = h(x)

∆(y ∗ , ·) known

x ∼ Pr(X ), y ∼ h0 (x)

{xi ,yi ,δi ,pi }

y ∼ h(Y | x)

δ(x, ·) unknown

Learning principle
argminh R̂(h) + C · Reg(H)
q
argminh R̂M (h) + λ · V ar(h)
n

satisfaction with y for x, while large values indicate dissatisfaction. The expected loss – called risk – of a hypothesis
R(h) is defined as,

policy, we keep track of the propensity, h0 (y | x) of the historical system to generate y for x. From these propensityaugmented logs

R(h) = Ex∼Pr(X ) Ey∼h(x) [δ(x, y)] .

D = {(x1 ,y1 ,δ1 ,p1 ), . . . , (xn ,yn ,δn ,pn )},

The goal of the system is to minimize risk, or equivalently,
maximize expected user satisfaction. The aim of learning
is to find a hypothesis h ∈ H that has minimum risk.

where pi ≡ h0 (yi | xi ), we can derive an unbiased estimate
of R(h) via Monte Carlo approximation,

We wish to re-use the interaction logs of these systems for
batch learning. Assume that its historical algorithm acted
according to a stationary policy h0 (x) (also called logging
policy). The data collected from this system is
D = {(x1 , y1 , δ1 ), . . . , (xn , yn , δn )},
where yi ∼ h0 (xi ) and δi ≡ δ(xi , yi ).
Sampling bias. D cannot be used to estimate R(h) for a
new hypothesis h using the estimator typically used in supervised learning. We ideally need either full information
about δ(xi , ·) or need samples y ∼ h(xi ) to directly estimate R(h). This explains why, in practice, model selection
over a small set of candidate systems is typically done via
A/B tests, where the candidates are deployed to collect new
data sampled according to y ∼ h(x) for each hypothesis
h. A relative comparison of the assumptions, hypotheses,
and principles used in supervised learning vs. our learning setting is outlined in Table 1. Fundamentally, batch
learning with bandit feedback is hard because D is both biased (predictions favored by the historical algorithm will
be over-represented) and incomplete (feedback for other
predictions will not be available) for learning.

4. Learning Principle: Counterfactual Risk
Minimization
The distribution mismatch between h0 and any hypothesis
h ∈ H can be addressed using importance sampling, which
corrects the sampling bias as:
R(h) = Ex∼Pr(X ) Ey∼h(x) [δ(x, y)]


h(y | x)
.
= Ex∼Pr(X ) Ey∼h0 (x) δ(x, y)
h0 (y | x)
This motivates the propensity scoring approach (Rosenbaum & Rubin, 1983). During the operation of the logging

n

R̂(h) =

1 X h(yi | xi )
δi
.
n i=1
pi

(1)

At first thought, one may think that directly estimating
R̂(h) over h ∈ H and picking the empirical minimizer is
a valid learning strategy. Unfortunately, there are several
potential pitfalls.
First, this strategy is not invariant to additive transformations of the loss and will give degenerate results if the loss
is not appropriately scaled. In Section 4.1, we develop intuition for why this is so, and derive the optimal scaling of
δ. For now, assume that ∀x, ∀y, δ(x, y) ∈ [−1, 0].
Second, this estimator has unbounded variance, since pi '
0 in D can cause R̂(h) to be arbitrarily far away from the
true risk R(h). This problem can be fixed by “clipping” the
importance sampling weights (Ionides, 2008)



h(y | x)
RM (h) = Ex Ey∼h0 (x) δ(x, y) min M,
,
h0 (y | x)


n
1X
h(yi | xi )
M
R̂ (h) =
δi min M,
.
n i=1
pi
M > 0 is a hyper-parameter chosen to trade-off bias and
variance in the estimate, where smaller values of M induce
larger bias in the estimate. Optimizing R̂M (h) through exhaustive enumeration over H yields the Inverse Propensity
Scoring (IPS) training objective (Bottou et al., 2013)
n
o
ĥIP S = argmin R̂M (h) .
(2)
h∈H

Third, importance sampling typically estimates R̂M (h) of
different hypotheses h ∈ H with vastly different variances.
Consider two hypotheses h1 and h2 , where h1 is similar to
h0 , but where h2 samples predictions that were not well explored by h0 . Importance sampling gives us low-variance
estimates for R̂M (h1 ), but highly variable estimates for

Counterfactual risk minimization

R̂M (h2 ). Intuitively, if we can develop variance-sensitive
confidence bounds over the hypothesis space, optimizing a
conservative confidence bound should find a h whose R(h)
will not be much worse, with high probability.

and observed losses δ1 , . . . , δn , for n ≥ 16 and a stochastic hypothesis space H with capacity N∞ ( n1 , FH , 2n),
p
∀h ∈ H : R(h) ≤ R̂M (h) + 18V arh (u)QH (n, γ)/n
+ M · 15Q(n, γ)/(n − 1).

Generalization error bound. A standard analysis would
give a bound that is agnostic to variance introduced by importance sampling. Following our intuition above, we derive a higher order bound that includes the variance term
using empirical Bernstein bounds (Maurer & Pontil, 2009).
To develop such a generalization error bound, we first need
a concept of capacity for stochastic hypothesis classes. For
any stochastic class H, define an auxiliary function class
FH = {fh : X ×Y 7→ [0, 1]}. Each h ∈ H corresponds to a
function fh ∈ FH ,


h(y | x)
δ(x, y)
min M,
. (3)
fh (x, y) = 1 +
M
h0 (y | x)
fh is a deterministic, bounded function, and satisfies
Ex Ey∼h0 (x) [fh (x, y)] = 1 + RM (h)/M.

(4)

Hence, we can use classic notions of capacity for FH to
reason about the convergence of R̂M (h) → RM (h).
Recall the covering number N∞ (, F, n) for a function
class F (refer (Anthony & Bartlett, 2009), (Maurer & Pontil, 2009) and the references therein). Define an −cover
N (, A, k·k∞ ) for a set A ⊆ Rn to be the size of the smallest cardinality subset A0 ⊆ A such that A is contained in
the union of balls of radius  centered at points in A0 , in
the metric induced by k · k∞ . The covering number is,
N∞ (, F, n) =

sup

N (, F({(xi , yi )}), k · k∞ ),

(xi ,yi )∈(X ×Y)n

where F({(xi , yi )}) is the function class conditioned on
sample {(xi , yi )},
F({(xi , yi )}) = {(f (x1 , y1 ), . . . , f (xn , yn )) : f ∈ F}.
Our measure for the capacity of our stochastic class H to
“fit” a sample of size n shall be N∞ ( n1 , FH , 2n).
Theorem 1. For a compact notation, define
uh i ≡ δi min{M, h(yi | xi )/pi },

uh ≡

n
X

uh i /n,

i=1
n
X
V arh (u) ≡
(uh i − uh )2 /(n − 1),
i=1

1
QH (n, γ) ≡ log(10 · N∞ ( , FH , 2n)/γ),
n

0 < γ < 1.

With probability at least 1 − γ in the random vector
i.i.d.
(x1 , y1 ) · · · (xn , yn ), with xi ∼ Pr(X ) and yi ∼ h0 (xi ),

Proof. Follow the proof of Theorem 6 of (Maurer & Pontil,
2009) with the function class as FH . Use Equations (3), (4)
to translate from fh (x, y) to RM (h). R̂M (h) = M · fˆh −1,
RM (h) = M · fh − 1, and M 2 V arh (u) = V arfh (u).
Finally, since δ(·,·) ≤ 0, hence R(h) ≤ RM (h).
CRM Principle. This generalization error bound is constructive, and it motivates a general principle for designing
machine learning methods for batch learning from bandit
feedback. In particular, a learning algorithm following this
principle should jointly optimize the estimate R̂M (h) as
well as its empirical standard deviation, where the latter
serves as a data-dependent regularizer.
)
(
r
V
ar
(u)
h
. (5)
ĥCRM = argmin R̂M (h) + λ
n
h∈H
M > 0 and λ ≥ 0 are regularization hyper-parameters.
When λ = 0, we recover the Inverse Propensity Scoring
objective of Equation (2). In analogy to Structural Risk
Minimization (Vapnik, 1998), we call this principle Counterfactual Risk Minimization, since both pick the hypothesis with the tightest upper bound on the true risk R(h).
4.1. Optimal Loss Scaling
When performing supervised learning with true labels y ∗
and a loss function ∆(y ∗ , ·), empirical risk minimization
using the standard estimator is invariant to additive translation and multiplicative scaling of ∆. The risk estimators
R̂(h) and R̂M (h) in bandit learning, however, crucially require δ(·, ·) ∈ [−1, 0].
Consider, for example, the case of δ(·, ·) ≥ 0. The training
objectives in Equation (2) (IPS) and Equation (5) (CRM)
become degenerate! A hypothesis h ∈ H that completely
avoids the sample D (i.e. ∀i = 1, . . . , n, h(yi | xi ) = 0)
trivially achieves the best possible R̂M (h) (= 0) with 0
empirical variance. This degeneracy arises because when
δ(·, ·) ≥ 0, the optimization objectives are a lower bound
on R(h), whereas what we need is an upper bound.
For any bounded loss δ(·, ·) ∈ [5, 4], we have, ∀x


h(y | x)
Ey∼h(x) [δ(x, y)] ≤ 4+ Ey∼h0 (x) (δ(x, y)−4)
.
h0 (y | x)
We assert that this is the tightest possible upper bound possible without additional assumptions. Since the optimization objectives in Equations (2),(5) are unaffected by a constant scale factor (e.g., 4−5), we should transform δ 7→ δ 0

Counterfactual risk minimization

to derive a conservative training objective w.r.t. δ 0 ,
δ 0 ≡ {δ − 4}/{4 − 5}.
4.2. Selecting hyper-parameters
We propose selecting the hyper-parameters M > 0 and
λ ≥ 0 via validation. However, we must be careful not to set M too small or λ too big. The estimated risk R̂M (h) ∈ [−M, 0], while the variance penalty
q
i
h
V arh (u)
M
√
. If M is too small, all hypothe∈
0,
n
2 n
ses will have the same biased estimate of risk M R̂M (h0 ),
since all the importance sampling weights will be clipped.
Similarly, if λ  0, a hypothesis h ∈ H that completely
avoids D achieves the best possible training objective of
0. As a rule of thumb, we can calibrate M and λ so that
the estimator is unbiased and objective is negative for some
h
M ' max{pi }/ min{pi } and
 ∈ H. Whenqh0 ∈ H, 
V
ar
(u)
h0
R̂M (h0 ) + λ
< 0 are natural choices.
n
4.3. When is counterfactual learning possible?
The bounds in Theorem 1 are with respect to the randomness in h0 . Known impossibility results for counterfactual
evaluation using h0 (Langford et al., 2008) also apply to
counterfactual learning. In particular, if h0 was deterministic, or even stochastic but without full support over Y, it is
easy to engineer examples involving the unexplored y ∈ Y
that guarantee sub-optimal learning even as |D| → ∞.
Also, a stochastic h0 with heavier tails need not always allow more effective learning. From importance sampling
theory (Owen, 2013), what really matters is how well h0
explores the regions of Y with favorable losses.

5. Learning Algorithm: POEM
We now use the CRM principle to derive an efficient algorithm for structured output prediction using linear rules.
Classic models in supervised learning (e.g., structured support vector machines (Tsochantaridis et al., 2004) and conditional random fields (Lafferty et al., 2001)) predict using
(6)

y∈Y

where w is a d−dimensional weight vector, and φ(x, y) is
a d−dimensional joint feature map. For example, in multilabel document classification, for a news article x and a
possible assignment of labels y represented as a bitvector, φ(x, y) could simply be a concatenation of the bag-ofwords features of the document (x), one copy for each of
the assigned labels in y, x ⊗ y. Several efficient inference
algorithms have been developed to solve Equation (6).
Consider

the

following

stochastic

family

Hlin ,

Hlin

hw (y | x) = exp(w · φ(x, y))/Z(x).
P
Z(x) = y0 ∈Y exp(w · φ(x, y 0 )) is the partition function.
This can be thought of as the “soft-max” variant of the
“hard-max” rules from Equation (6). Additionally, for a
temperature multiplier α > 1, w 7→ αw induces a more
“peaked” distribution hαw that preserves the modes of hw ,
and intuitively is a “more deterministic” variant of hw .
hw lies in the exponential family of distributions, and has a
simple gradient,

	
∇hw(y | x) = hw(y | x) φ(x,y)−Ey0∼hw(x) [φ(x,y 0 )] .
Consider a bandit-feedback structured-output dataset D =
{(x1 , y1 , δ1 , p1 ), . . . , (xn , yn , δn , pn )}. In multi-label document classification, this data could be collected from an
interactive labeling system, where each y indicates the labels predicted by the system for a document x. The feedback δ(x, y) is how many labels (but not which ones) were
correct. To perform learning, first we scale the losses as
outlined in Section 4.1. Next, instantiating the CRM principle (Equation (5)) for Hlin , (using notation analogous to
that in Theorem 1, adapted for Hlin ), yields the POEM
training objective.
POEM Training Objective:
r
∗

w = argmin uw + λ
w∈Rd

V arw (u)
,
n

(7)
n

uw i ≡ δi min{M,
V arw (u) ≡

hsup
w (x) = argmax {w · φ(x, y)} ,

∈

parametrized by w. A hypothesis hw (x)
samples y from the distribution

n
X

X
exp(w · φ(xi , yi ))
uw i /n,
}, uw ≡
pi · Z(xi )
i=1

(uw i − uw )2 /(n − 1).

i=1

While the objective in Equation (7) is not convex in w
(even for λ = 0), prior work (Yu et al., 2010), (Lewis &
Overton, 2013) has established theoretically sound modifications to L-BFGS for non-zmooth non-convex optimization. We find that batch gradient descent (e.g., L-BFGS out
of the box) and the stochastic gradient approach introduced
below find local optima that have good generalization error.
Software implementing POEM is available at http://www.
cs.cornell.edu/∼adith/poem/ for download, as is all the
code and data needed to run each of the experiments reported in Section 6.
5.1. Iterated Variance Majorization
The POEM training
pobjective in Equation (7), specifically
the variance term V arw (u), resists stochastic gradient

Counterfactual risk minimization

optimization in the presented form. To remove this obstacle, we now develop a Majorization-Minimization scheme,
similar in spirit to recent approaches to multi-class SVMs
(van den Burg & Groenen, 2014) that can be shown to
converge to a local optimum of the POEM training objective.
In particular, we will show how to decompose
p
V
ar
(u)
of differentiable functions (e.g.,
w
P
P as a isum
i
2
i uw or
i {uw } ) so that we can optimize the overall
training objective at scale using stochastic gradient descent.
Proposition 1. For any w0 ,
p

V arw (u) ≤ Aw0

n
X

uw i + Bw0

i=1

n
X

{uw i }2 + Cw0

i=1

= Q(w; w0 ).
p
Aw0 ≡ −uw0 /{(n − 1) V arw0 (u)},
p
Bw0 ≡ 1/{2(n − 1) V arw0 (u)},
p
V arw0 (u)
n{uw0 }2
p
Cw0 ≡
.
+
2
2(n − 1) V arw0 (u)
Proof. Consider a first √
order Taylor approximation of
p
V arw (u) around w0 , · is concave. Again Taylor approximate −{uw }2 , noting that −{·}2 is concave.
Iteratively minimizing wt+1 = argminw Q(w; wt ) ensures
1
t+1
that the sequence
are successive
p of iterates w , . . . , w
minimizers of V arw (u). Hence, during an epoch t,
POEM proceeds by sampling uniformly i ∼ D, computing uw i , ∇uw i and, for learning rate η, updating
√
w ← w − η{∇uw i + λ n(Awt ∇uw i + 2Bwt uw i ∇uw i )}.
After each epoch, wt+1 ← w, and iterated minimization
proceeds until convergence.

6. Experiments
We now empirically evaluate the prediction performance
and computational efficiency of POEM. Consider multilabel classification with input x ∈ Rp and prediction y ∈
{0, 1}q . Popular supervised algorithms that solve this problem include Structured SVMs (Tsochantaridis et al., 2004)
and Conditional Random Fields (Lafferty et al., 2001). In
the simplest case, CRF essentially performs logistic regression for each of the q labels independently. As outlined in Section 5, we use a joint feature map: φ(x, y) =
x ⊗ y. We conducted experiments on different multi-label
datasets collected from the LibSVM repository, with different ranges for p (features), q (labels) and n (samples)
represented as summarized in Table 2.
Experiment methodology. We employ the Supervised
7→ Bandit conversion (Agarwal et al., 2014) method. Here,

Table 2. Corpus statistics for different multi-label datasets from
the LibSVM repository. LYRL was post-processed so that only
top level categories were treated as labels.
Name
Scene
Yeast
TMC
LYRL

p(# features)
294
103
30438
47236

q(# labels)
6
14
22
4

ntrain
1211
1500
21519
23149

ntest
1196
917
7077
781265

we take a supervised dataset D∗ = {(x1 , y1∗ ) . . . (xn , yn∗ )}
and simulate a bandit feedback dataset from a logging policy h0 by sampling yi ∼ h0 (xi ) and collecting feedback
∆(yi∗ , yi ). In principle, we could use any arbitrary stochastic policy as h0 . We choose a CRF trained on 5% of D∗
as h0 using default hyper-parameters, since they provide
probability distributions amenable to sampling. In all the
multi-label experiments, ∆(y ∗ , y) is the Hamming loss between the supervised label y ∗ vs. the sampled label y for
input x. Hamming loss is just the number of incorrectly assigned labels (both false positives and false negatives). To
create bandit feedback D = {(xi , yi , δi ≡ ∆(yi∗ , yi ), pi ≡
h0 (yi | xi ))}, we take four passes through D∗ and sample
labels from h0 . Note that each supervised label is worth
' |Y| = 2q bandit feedback labels. We can explore different learning strategies (e.g., IPS, CRM, etc.) on D and
obtain learnt weight vectors wips , wcrm , etc. On the supervised test set, we then
loss per inP report the expected
1
∗
stance R(w) = ntest
i Ey∼hw (xi ) ∆(yi , y) and compare
the generalization performance of these learning strategies.
Baselines and learning methods. The expected Hamming loss of h0 is the baseline to beat. Lower loss is
better. The naı̈ve, variance-agnostic approach to counterfactual learning (Bottou et al., 2013) can be generalized
to handle parametric multilabel classification (Equation (7)
with λ = 0). We optimize it either using L-BFGS (IPS(B))
or stochastic optimization (IPS(S)). POEM(S) uses our
Iterative-Majorization approach to variance regularization
as outlined in Section 5.1, while POEM(B) is a L-BFGS
variant. Finally, we report results from a supervised CRF
as a skyline, despite its unfair advantage of having access
to the full-information examples.
We keep aside 25% of D as a validation set – we use the
unbiased counterfactual estimator from Equation (1) for se∗
lecting hyper-parameters. λ = cλ∗ , where
 λ−6is the cali
bration factor from Section 4.2 and c ∈ 10 , . . . , 1 in
multiples of 10. The clipping constant M is similarly set
to the ratio of the 90%ile to the 10%ile propensity score
observed in the training set of D. For all methods, when
optimizing any objective over w, we always begin the optimization from w = 0 (⇒ hw = uniform(Y)). We use
mini-batch AdaGrad (Duchi et al., 2011) with batch size
= 100 to adapt our learning rates for the stochastic approaches and use progressive validation (Blum et al., 1999)

Counterfactual risk minimization

and gradient norms to detect convergence. Finally, the entire experiment set-up is run 10 times (i.e. h0 trained on
randomly chosen 5% subsets, D re-created, and test set performance of different approaches collected) and we report
the averaged test set expected error across runs.

Table 4. Average time in seconds for each validation run for different approaches to multi-label classification. CRF is the scikitlearn implementation (Pedregosa et al., 2011). On all datasets,
stochastic approaches are substantially faster than batch gradients.
IPS(B)
IPS(S)
POEM(B)
POEM(S)
CRF

6.1. Does variance regularization improve
generalization?

Table 3. Test set Hamming loss for different approaches to multilabel classification on different datasets, averaged over 10 runs.
POEM is significantly better than IPS on each dataset and each
optimization variant (one-tailed paired difference t-test at significance level of 0.05).
h0
IPS(B)
POEM(B)
IPS(S)
POEM(S)
CRF

Scene
1.543
1.193
1.168
1.519
1.143
0.659

Yeast
5.547
4.635
4.480
4.614
4.517
2.822

TMC
3.445
2.808
2.197
3.023
2.522
1.189

LYRL
1.463
0.921
0.918
1.118
0.996
0.222

Yeast
47.61
2.86
94.16
5.02
3.28

TMC
136.34
49.12
949.95
276.13
99.18

LYRL
21.01
13.66
561.12
120.09
62.93

require computation of the partition function Z(x) which
can be expensive in structured output prediction. From Table 5, we see that the loss of the deterministic predictor is
typically not far from the loss of the stochastic policy, but
often slightly better.
Table 5. Mean Hamming loss of MAP predictions from the policies in Table 3. POEMmap is not significantly worse than POEM
(one-sided paired difference t-test, significance level 0.05).
Scene
1.143
1.143

POEM(S)
POEM(S)map

Yeast
4.517
4.065

TMC
2.522
2.299

LYRL
0.996
0.880

6.4. How does generalization improve with size of D?

4
R(w)

Results are reported in Table 3. We statistically test the performance of POEM against IPS (batch variants are paired
together, and the stochastic variants are paired together)
using a one-tailed paired difference t-test at significance
level of 0.05 across 10 runs of the experiment, and find
POEM to be significantly better than IPS on each dataset
and each optimization variant. Furthermore, on all datasets
POEM learns a hypothesis that substantially improves over
the performance of h0 . This suggests that the CRM principle is practically useful for designing learning algorithms,
and that the variance regularizer is indeed beneficial.

Scene
2.58
1.65
75.20
4.71
4.86

h0
CRF
POEM(S)

3.5
3

6.2. How computationally efficient is POEM?
Table 4 shows the time taken (in CPU seconds) to run each
method on each dataset, averaged over different validation
runs when performing hyper-parameter grid search. Some
of the timing results are skewed by outliers, e.g., when under very weak regularization, CRFs tend to take a lot longer
to converge. In aggregate, it is clear that the stochastic variants are able to recover good parameter settings in a fraction of the time of batch L-BFGS optimization, and this is
even more pronounced when the number of labels grows
(the run-time is dominated by computation of Z(xi )).
6.3. Can MAP predictions derived from stochastic
policies perform well?
For the policies learnt by POEM as shown in Table 3, Table 5 reports the averaged performance of the deterministic
predictor derived from them. For a learnt weight vector w,
this simply amounts to applying Equation (6). In practice,
this method of generating predictions can be substantially
faster than sampling since computing the argmax does not

20

21

22

23 24 25
ReplayCount

26

27

28

Figure 1. Generalization performance of POEM(S) as a function
of n on the Yeast dataset. Even with ReplayCount = 28 ,
POEM(S) is learning from much less information than the CRF
(each supervised label conveys 214 bandit label feedbacks).

As we collect more data under h0 , our generalization error
bound indicates that prediction performance should eventually approach that of the optimal hypothesis in the hypothesis space. We can simulate n → ∞ by replaying the
training data multiple times, collecting samples y ∼ h0 (x).
In the limit, we would observe every possible y in the bandit feedback dataset, since h0 (x) has non-zero probability
of exploring each prediction y. However, the learning rate
may be slow, since the exponential model family has very
thin tails, and hence may not be an ideal logging distribution to learn from. Holding all other details of the experiment setup fixed, we vary the number of times we re-

Counterfactual risk minimization

played the training set (ReplayCount) to collect samples
from h0 , and report the performance of POEM(S) on the
Yeast dataset in Figure 1.

deterministic (α ≥ 24 ), performance of POEM(S) simply
recovers h0 map, suggesting that the CRM principle indeed achieves robust learning.

6.5. How does quality of h0 affect learning?
1.4
1.2
1
2−1
h0
POEM(S)

R(w)

6

4
0.4

0.6

0.8

20

21

22
α

23

24

25

Figure 3. Performance of POEM(S) on the LYRL dataset as h0
becomes more deterministic. For α ≥ 25 , h0 ≡ h0 map (within
machine precision).

5

0.2

h0
POEM(S)
h0 map

1.6
R(w)

In this experiment, we change the fraction of the training
set f · ntrain that was used to train the logging policy; as f
is increased, the quality of h0 improves. Intuitively, there’s
a trade-off: better h0 probably samples correct predictions
more often and so produces a higher quality D to learn
from, but it should also be harder to beat h0 . We vary f

1

f
Figure 2. Performance of POEM(S) on the Yeast dataset as h0 is
improved. The fraction f of the supervised training set used to
train h0 is varied to control h0 ’s quality. h0 performance does not
reach CRF when f = 1 because we do not tune hyper-parameters,
and we report its expected loss, not the loss of its MAP prediction.

from 1% to 100% while keeping all other conditions identical to the original experiment setup in Figure 2, and find
that POEM(S) is able to consistently find a hypothesis at
least as good as h0 . Moreover, even D collected from a
poor quality h0 (0.5 ≤ f ≤ 0.2) allows POEM(S) to effectively learn an improved policy.
6.6. How does stochasticity of h0 affect learning?
Finally, the theory suggests that counterfactual learning is
only possible when h0 is sufficiently stochastic (the generalization bounds hold with high probability in the samples drawn from h0 ). Does CRM degrade gracefully when
this assumption is violated? We test this by introducing
the temperature multiplier w 7→ αw, α > 0 (as discussed
in Section 5) into the logging policy. For h0 = hw0 , we
scale w0 7→ αw0 , to derive a “more deterministic” variant of h0 , and generate D ∼ hαw0 . We report the performance of POEM(S) on the LYRL dataset in Figure 3 as
we change α ∈ [0.5, . . . , 32], compared against h0 , and the
deterministic predictor – h0 map – derived from h0 . So
long as there is some minimum amount of stochasticity in
h0 , POEM(S) is still able to find a w that improves upon
h0 and h0 map. The margin of improvement is typically
greater when h0 is more stochastic. Even when h0 is too

We observe the same trends (Figures 1, 2 and 3) across
all datasets and optimization variants. They also remain
unchanged when we include l2−regularization (analogous
to supervised CRFs to capture the capacity of Hlin ).

7. Conclusion
Counterfactual risk minimization serves as a robust principle to design algorithms that can learn from a batch of
bandit feedback interactions. The key insight for CRM is
to expand the classical notion of a hypothesis class to include stochastic policies, reason about variance in the risk
estimator, and derive a generalization error bound over this
hypothesis space. The practical take-away is a simple, datadependent regularizer that guarantees robust learning. Following the CRM principle, we developed POEM for structured output prediction. POEM can optimize over rich policy families (exponential models corresponding to linear
rules in supervised learning), and deal with massive output
spaces as efficiently as classical supervised methods.
The CRM principle more generally applies to supervised
learning with non-differentiable losses, since the objective
does not require the gradient of the loss function. We also
foresee extensions of this work that relax some of the assumptions, e.g., to handle noisy δ(·, ·), and ordinal or coactive feedback, or adaptive h0 etc.

Acknowledgement
This research was funded in part through NSF Awards IIS1247637 and IIS-1217686, the JTCII Cornell-Technion Research Fund, and a gift from Bloomberg. We thank Chenhao Tan, Karthik Raman and Vikram Rao for proofreading
our manuscript, and the anonymous reviewers of ICML for
their constructive feedback.

Counterfactual risk minimization

References
Agarwal, Alekh, Hsu, Daniel, Kale, Satyen, Langford,
John, Li, Lihong, and Schapire, Robert. Taming the
monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st International Conference
on Machine Learning, pp. 1638–1646, 2014.
Anthony, Martin and Bartlett, Peter L. Neural Network
Learning: Theoretical Foundations. Cambridge University Press, New York, NY, USA, 2009.
Beygelzimer, Alina and Langford, John. The offset tree for
learning with partial labels. In Proceedings of the 15th
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 129–138, 2009.
Blum, Avrim, Kalai, Adam, and Langford, John. Beating
the hold-out: Bounds for k-fold and progressive crossvalidation. In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, pp. 203–208,
1999.
Bottou, Léon, Peters, Jonas, Candela, Joaquin Q., Charles,
Denis X., Chickering, Max, Portugaly, Elon, Ray, Dipankar, Simard, Patrice Y., and Snelson, Ed. Counterfactual reasoning and learning systems: the example of
computational advertising. Journal of Machine Learning
Research, 14(1):3207–3260, 2013.
Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research,
12:2121–2159, 2011.
Galichet, Nicolas, Sebag, Michèle, and Teytaud, Olivier.
Exploration vs exploitation vs safety: Risk-aware multiarmed bandits. In Asian Conference on Machine Learning, pp. 245–260, 2013.
Garcia, J. and Fernandez, F. Safe exploration of state and
action spaces in reinforcement learning. Journal of Artificial Intelligence Research, 45:515–564, 2012.
Hofmann, Katja, Schuth, Anne, Whiteson, Shimon, and
de Rijke, Maarten. Reusing historical interaction data
for faster online learning to rank for IR. In Sixth ACM
International Conference on Web Search and Data Mining, pp. 183–192, 2013.
Ionides, Edward L. Truncated importance sampling. Journal of Computational and Graphical Statistics, 17(2):
295–311, 2008.
Lafferty, John D., McCallum, Andrew, and Pereira, Fernando C. N. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.
In Proceedings of the 18th International Conference on
Machine Learning, pp. 282–289, 2001.

Langford, John and Zhang, Tong. The epoch-greedy algorithm for multi-armed bandits with side information.
In Proceedings of the 21st Annual Conference on Neural
Information Processing Systems, pp. 817–824, 2008.
Langford, John, Strehl, Alexander, and Wortman, Jennifer.
Exploration scavenging. In Proceedings of the 25th International Conference on Machine Learning, pp. 528–
535, 2008.
Langford, John, Li, Lihong, and Dudı́k, Miroslav. Doubly
robust policy evaluation and learning. In Proceedings of
the 28th International Conference on Machine Learning,
pp. 1097–1104, 2011.
Lewis, Adrian S. and Overton, Michael L. Nonsmooth optimization via quasi-newton methods. Mathematical Programming, 141(1-2):135–163, 2013.
Li, Lihong, Chu, Wei, Langford, John, and Schapire,
Robert E. A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th
International Conference on World Wide Web, pp. 661–
670, 2010.
Li, Lihong, Chu, Wei, Langford, John, and Wang, Xuanhui.
Unbiased offline evaluation of contextual-bandit-based
news article recommendation algorithms. In Proceedings of the 4th ACM International Conference on Web
Search and Data Mining, pp. 297–306, 2011.
Li, Lihong, Chen, Shunbao, Kleban, Jim, and Gupta,
Ankur. Counterfactual estimation and optimization of
click metrics for search engines. CoRR, abs/1403.1891,
2014.
Li, Lihong, Munos, Remi, and Szepesvari, Csaba. Toward
minimax off-policy value estimation. In Proceedings of
the 18th International Conference on Artificial Intelligence and Statistics (AISTATS), 2015.
Mary, Jérémie, Preux, Philippe, and Nicol, Olivier. Improving offline evaluation of contextual bandit algorithms via bootstrapping techniques. In Proceedings of
the 31st International Conference on Machine Learning,
pp. 172–180, 2014.
Maurer, Andreas and Pontil, Massimiliano. Empirical
bernstein bounds and sample-variance penalization. In
Proceedings of the 22nd Conference on Learning Theory, 2009.
Owen, Art B. Monte Carlo theory, methods and examples.
2013.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,

Counterfactual risk minimization

Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.
Rosenbaum, Paul R. and Rubin, Donald B. The central
role of the propensity score in observational studies for
causal effects. Biometrika, 70(1):41–55, 1983.
Shivaswamy, Pannagadatta K. and Joachims, Thorsten.
Multi-armed bandit problems with history. In Proceedings of the 15th International Conference on Artificial
Intelligence and Statistics, pp. 1046–1054, 2012.
Strehl, Alexander L., Langford, John, Li, Lihong, and
Kakade, Sham. Learning from logged implicit exploration data. In Proceedings of the 24th Annual Conference on Neural Information Processing Systems, pp.
2217–2225, 2010.
Thomas, Philip S., Theocharous, Georgios, and
Ghavamzadeh, Mohammad.
High-confidence offpolicy evaluation. In Proceedings of the 29th AAAI
Conference on Artificial Intelligence, pp. 3000–3006,
2015.
Tsochantaridis, Ioannis, Hofmann, Thomas, Joachims,
Thorsten, and Altun, Yasemin. Support vector machine
learning for interdependent and structured output spaces.
In Proceedings of the 21st International Conference on
Machine Learning, pp. 104–, 2004.
van den Burg, G.J.J. and Groenen, P.J.F. GenSVM: A Generalized Multiclass Support Vector Machine. Technical Report EI 2014-33, Erasmus University Rotterdam,
Erasmus School of Economics (ESE), Econometric Institute, 2014.
Vapnik, V. Statistical Learning Theory. Wiley, Chichester,
GB, 1998.
Yu, Jin, Vishwanathan, S. V. N., Günter, Simon, and
Schraudolph, Nicol N. A quasi-Newton approach to nonsmooth convex optimization problems in machine learning. Journal of Machine Learning Research, 11:1145–
1200, 2010.
Zadrozny, Bianca, Langford, John, and Abe, Naoki. Costsensitive learning by cost-proportionate example weighting. In Proceedings of the Third IEEE International
Conference on Data Mining, pp. 435–, 2003.

