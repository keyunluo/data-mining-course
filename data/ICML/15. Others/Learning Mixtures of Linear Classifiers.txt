Learning Mixtures of Linear Classifiers

Yuekai Sun
YUEKAI @ STANFORD . EDU
Institute for Computational and Mathematical Engineering, Stanford University, 475 Via Ortega, Stanford, CA 94305
Stratis Ioannidis
Technicolor, 175 S San Antonio Rd, Los Altos, CA 94022

STRATIS . IOANNIDIS @ TECHNICOLOR . COM

Andrea Montanari
MONTANARI @ STANFORD . EDU
Dept. of Electrical Engineering & Dept. of Statistics, Stanford University, 350 Serra Mall, Stanford, CA 94305

Abstract
We consider a discriminative learning (regression) problem, whereby the regression function
is a convex combination of k linear classifiers.
Existing approaches are based on the EM algorithm, or similar techniques, without provable
guarantees. We develop a simple method based
on spectral techniques and a ‘mirroring’ trick,
that discovers the subspace spanned by the classifiers’ parameter vectors. Under a probabilistic
assumption on the feature vector distribution, we
prove that this approach has nearly optimal statistical efficiency.

1. Introduction
Since Pearson’s seminal contribution (Pearson, 1894), and
most notably after the introduction of the EM algorithm
(Dempster et al., 1977), mixture models and latent variable
models have played a central role in statistics and machine
learning, with numerous applications—see, e.g., McLachlan & Peel (2004), Bishop (1998), and Bartholomew et al.
(2011). Despite their ubiquity, fitting the parameters of a
mixture model remains a challenging task. The most popular methods (e.g., the EM algorithm or likelihood maximization by gradient ascent) are plagued by local optima
and come with little or no guarantees. Computationally efficient algorithms with provable guarantees are an exception in this area. Even the idealized problem of learning
mixtures of Gaussians has motivated a copious theoretical
literature (Arora & Kannan, 2001; Moitra & Valiant, 2010).
In this paper we consider the problem of modeling a regression function as a mixture of k components. Namely,
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

we are given labels Yi ∈ R and feature vectors Xi ∈ Rd ,
i ∈ [n] ≡ {1, 2, . . . , n}, and we seek estimates of the parameters of a mixture model

Pk
Yi X =x ∼ ℓ=1 pℓ f (yi |xi , uℓ ) .
(1)
i

i

Here k is the number of components, (pℓ )ℓ∈[k] are weights
of the components, and uℓ is a vector of parameters for
the ℓ-th component. Models of this type have been intensely studied in the neural network literature since the
early nineties (Jordan & Jacobs, 1994; Bishop, 1998). They
have also found numerous applications ranging from object
recognition (Quattoni et al., 2004) to machine translation
(Liang et al., 2006). These studies are largely based on
learning algorithms without consistency guarantees.
Recently, Chaganty & Liang (2013) considered mixtures
of linear regressions, whereby the relation between labels and feature vectors is linear within each component;
i.e., Yi = huℓ , Xi i + noise (here and below ha, bi denotes the standard inner product in Rm ). Equivalently,
f (yi |xi , uℓ ) = f0 (yi − hxi , uℓ i) with f0 ( · ) a density of
mean zero. Building on a new approach developed by Hsu
et al. (2012) and Anandkumar et al. (2012), these authors
propose an algorithm for fitting mixtures of linear regressions with provable guarantees. The main idea is to regress
Yiq , for q ∈ {1, 2, 3} against the tensors Xi , Xi ⊗ Xi ,
Xi ⊗ Xi ⊗ Xi . The coefficients of these regressions are
tensors whose decomposition yields the parameters uℓ , pℓ .
While the work of Chaganty & Liang (2013) is a significant
step forward, it leaves several open problems:
Statistical efficiency. Consider a standard scaling of the
feature vectors, whereby the components (Xi,j )j∈[p] are
of order one. Then, the mathematical guarantees of Chaganty & Liang (2013) require a sample size n ≫ d6 . This
is substantially larger than the ‘information-theoretic’ optimal scaling, and is an unrealistic requirement in highdimension (large d). As noted in (Chaganty & Liang,

Learning Mixtures of Linear Classifiers

2013), this scaling is an intrinsic drawback of the tensor
approach as it operates in a higher-dimensional space (tensor space) than the space in which data naturally live.
Linear regression versus classification. In virtually all
applications of the mixture model (1), labels Yi are
categorical—see, e.g., Jordan & Jacobs (1994), Bishop
(1998), Quattoni et al. (2004), Liang et al. (2006). In
this case, the very first step of Chaganty & Liang, namely,
regressing Yi2 on Xi⊗2 and Yi3 on Xi⊗3 , breaks down.
Consider—to be definite—the important case of binary labels (e.g., Yi ∈ {0, 1} or Yi ∈ {+1, −1}). Then powers
of the labels do not provide additional information (e.g., if
Yi ∈ {0, 1}, then Yi = Yi2 ). Also, since Yi is non-linearly
related to uℓ , Yi2 does not depend only on u⊗2
ℓ .
Computational complexity. The method of Chaganty &
Liang (2013) solves a regularized linear regression in d3
dimensions and factorizes a third order tensor in d dimensions. Even under optimistic assumptions (finite convergence of iterative schemes), this requires O(d3 n + d4 ) operations.
In this paper, we develop a spectral approach to learning
mixtures of linear classifiers in high dimension. For the
sake of simplicity, we shall focus on the case of binary labels Yi ∈ {+1, −1}, but we expect our ideas to be more
broadly applicable. We consider regression functions of the
form f (yi |xi , uℓ ) = f (yi |hxi , uℓ i), i.e., each component
corresponds to a generalized linear model with parameter
vector uℓ ∈ Rd . In a nutshell, our method constructs a
symmetric matrix Q̂ ∈ Rd×d by taking a suitable empirical average of the data. The matrix Q̂ has the following
property: (d − k) of its eigenvalues are roughly degenerate. The remaining k eigenvalues correspond to eigenvectors that—approximately—span the same subspace as u1 ,
. . . , uk . Once this space is accurately estimated, the problem dimensionality is reduced to k; as such, it is easy to
come up with effective prediction methods (as a matter of
fact, simple K-nearest neighbors works very well).
The resulting algorithm is computationally efficient, as its
most expensive step is computing the eigenvector decomposition of a d × d matrix (which takes O(d3 ) operations).
Assuming Gaussian feature vectors Xi ∈ Rd , we prove
that our method is also statistically efficient, i.e., it only
requires n ≥ d samples to accurately reconstruct the subspace spanned by u1 , . . . , uk . This is the same amount of
data needed to estimate the covariance of the feature vectors Xi or a parameter vector u1 ∈ Rd in the trivial case
of a mixture with a single component, k = 1. It is unlikely that a significantly better efficiency can be achieved
without additional structure.
The assumption of Gaussian feature vectors Xi ’s is admit-

tedly restrictive. On one hand, as for the problem of learning mixtures of Gaussians (Arora & Kannan, 2001; Moitra
& Valiant, 2010), we believe that useful insights can be
gained by studying this simple setting. On the other, and as
discussed below, our proof does not really require the distribution of the Xi ’s to be Gaussian, and a strictly weaker
assumption is sufficient. We expect that future work will
succeed in further relaxing this assumption.
1.1. Technical contribution and related work
Our approach is related to the principal Hessian directions
(pHd) method proposed by Li (1992) and further developed
by Cook (1998) and co-workers. PHd is an approach to
dimensionality reduction and data visualization. It generalizes principal component analysis to the regression (discriminative) setting, whereby each data point consists of
a feature vector Xi ∈ Rd and a label Yi ∈ R. Summarizing,
Pn the idea is to form the ‘Hessian’ matrix Ĥ =
n−1 i=1 Yi Xi XiT ∈ Rd×d . (We assume here, for ease
of exposition, that the Xi ’s have zero mean and unit covariance.) The eigenvectors associated to eigenvalues with
largest magnitude are used to identify a subspace in Rd
onto which to project the feature vectors Xi ’s.
Unfortunately, the pHd approach fails in general for the
mixture models of interest here, namely, mixtures of linear classifiers. For instance, it fails when each component
of (1) is described by a logistic model f (yi = +1|z) =
(1 + e−z )−1 , when features are centered at E(Xi ) = 0;
a proof can be found in the extended version of this paper (Sun et al., 2013).
Our approach
overcomes this problem by constructing Q̂ =
Pn
n−1 i=1 Zi Xi XiT ∈ Rd×d . The Zi ’s are pseudo-labels
obtained by applying a ‘mirroring’ transformation to the
Yi ’s. Unlike with Ĥ, the eigenvector structure of Q̂ enables
us to estimate the span of u1 , . . . , uk .
As an additional technical contribution, we establish nonasymptotic bounds on the estimation error that allow to
characterize the trade-off between the data dimension d
and the sample size n. In contrast, rigorous analysis on
pHd is limited to the low-dimensional regime of d fixed as
n → ∞. It would be interesting to generalize the analysis
developed here to characterize the high-dimensional properties of pHd as well.

2. Problem Formulation
2.1. Model
Consider a dataset comprising n i.i.d. pairs (Xi , Yi ) ∈
Rd × {−1, +1}, i ∈ [n]. We refer to the vectors Xi ∈ Rd
as features and to the binary variables as labels. We assume
that the features Xi ∈ Rd are sampled from a Gaussian dis-

Learning Mixtures of Linear Classifiers
+1
-1

+1
-1

1.5

1.5

1.5

1.0

1.0

1.0

0.5

0.5

0.5

0.0

0.0

0.0

−0.5

−0.5

−0.5

−1.0

−1.0

−1.0

−1.5

−1.5

1.5
1.0
−1.0

−0.5

0.0

0.5

1.0
1.5

−1.5

1.5

1.5

1.0

0.5
−1.5

+1
-1

1.0

0.5
−1.5

0.0
−0.5
−1.0
−1.5

−1.0

−0.5

0.0

0.5

1.0
1.5

(a)

0.0
−0.5
−1.0
−1.5

0.5
−1.5

−1.0

−0.5

(b)

0.0

0.5

1.0
1.5

0.0
−0.5
−1.0
−1.5

(c)

Figure 1. The mirroring process applied to a mixture of two 3-dimensional classifiers. Figure (a) shows labels generated by two classifiers
in R3 ; the figure includes the parameter profiles as well as the corresponding classification surfaces. Figure (b) shows the mirroring
direction r̂ as a dashed vector, computed by (5), as well as the plane it defines; note that r̂ lies within the positive cone spanned by
the two classifier profiles, approximately. Finally, Figure (c) shows the result of the mirroring process: the region of points that was
predominantly positive has remained unaltered, while the region of points that was predominantly negative has been flipped.

tribution with mean µ ∈ Rd and a positive definite covariance Σ ∈ Rd×d . The labels Yi ∈ {−1, +1} are generated
by a mixture of linear classifiers, i.e.,
Pk
Pr(Yi = +1 | Xi ) = ℓ=1 pℓ f (huℓ , Xi i) .
(2)

Here, k ≥ 2 is the number of components in the mixture; (pℓ )ℓ∈[k] are the weights, satisfying of course pℓ > 0,
Pk
d
ℓ=1 pℓ = 1; and (uℓ )ℓ∈[k] , uℓ ∈ R are the normals to
the planes defining the k linear classifiers. We refer to each
normal uℓ as the parameter profile of the ℓ-th classifier; we
assume that the profiles uℓ , ℓ ∈ [k], are linearly independent, and that k < n/2.
We assume that the function f : R → [0, 1], characterizing
the classifier response, is analytic, non-decreasing, strictly
concave in [0, +∞), and satisfies:
lim f (t) = 1,

t→∞

lim f (t) = 0,

t→−∞

1−f (t) = f (−t). (3)

As an example, it is useful to keep in mind the logistic function f (t) = (1 + e−t )−1 . Fig. 1(a) illustrates a mixture of
k = 2 classifiers over d = 3 dimensions.
2.2. Subspace Estimation, Prediction and Clustering
Our main focus is the following task:
Subspace Estimation: After observing (Xi , Yi ),
i ∈ [n], estimate the subspace spanned by
the profiles of the k classifiers, i.e., U ≡
span(u1 , . . . , uk ).
b an estimate of U , we characterize performance via
For U
the principal angle between the two spaces, namely


b ) = max arccos hx,yi .
dP (U, U
kxkkyk
b
x∈U,y∈U

Notice that projecting the features Xi on U entails no loss
of information w.r.t. (2). This can be exploited to improve
the performance of several learning tasks through dimensionality reduction, by projecting the features to the estimate of the subspace U . Two such tasks are:
Prediction: Given a new feature vector Xn+1 ,
predict the corresponding label Yn+1 .
Clustering: Given a new feature vector and label pair (Xn+1 , Yn+1 ), identify the classifier that
generated the label.
As we will see in Section 5, our subspace estimate can be
used to significantly improve the performance of both prediction and clustering.
2.3. Technical Preliminary
We review here a few definitions used in our exposition.
The sub-gaussian norm of a random variable X is:
1
kXkψ2 = sup √ (E[|X|p ])1/p .
p
p≥1
We say X is sub-gaussian if kXkψ2 < ∞. We say that a
random vector X ∈ Rd is sub-gaussian if hy, Xi is subgaussian for any y on the unit sphere Sd−1 .
We use the following variant of Stein’s identity (Stein,
′
1973; Liu, 1994). Let X ∈ Rd , X ′ ∈ Rd be jointly Gaus′
sian random vectors, and consider a function h : Rd → R
that is almost everywhere (a.e.) differentiable and satisfies
E[|∂h(X ′ )/∂xi |] < ∞, i ∈ [d′ ]. Then, the following identity holds:
Cov(X, h(X ′ )) = Cov(X, X ′ )E[∇h(X ′ )].

(4)

Learning Mixtures of Linear Classifiers

Algorithm 1 S PECTRAL M IRROR
Input: Pairs (Xi , Yi ), i ∈ [n]
b
Output: Subspace estimate U
P⌊n/2⌋
1
1: µ̂ ← ⌊n/2⌋ i=1 Xi
P⌊n/2⌋
1
T
2: Σ̂ ← ⌈n/2⌉
i=1 (Xi − µ̂)(Xi − µ̂)
P
⌊n/2⌋
1
−1
(Xi − µ̂)
3: r̂ ← ⌊n/2⌋
i=1 Yi Σ̂
4: for each i ∈ {⌊n/2⌋ + 1, . . . , n}:
5: Q̂←

1
⌈n/2⌉

Mirroring. (Lines 3–4) We compute the vector:
r̂ =

Zi ← Yi sgnhr̂, Xi i

n
X

Zi Σ̂−1/2 (Xi − µ̂)(Xi − µ̂)T Σ̂−1/2

i=⌊n/2⌋+1

Pd

6: Find eigendecomposition ℓ=1 λℓ wℓ wℓT of Q̂
7: Let λ(1) , . . . , λ(k) be the k eigenvalues furthest from

the median.

8: Û ← span Σ̂−1/2 w(1) , . . . , Σ̂−1/2 w(k)



1 P⌊n/2⌋
Yi Σ̂−1 (Xi − µ̂) ∈ Rd .
⌊n/2⌋ i=1

(5)

We refer to r̂ as the mirroring direction. In Section 4, we
show that r̂ concentrates around its population (n = ∞)
version r ≡ E[Y Σ−1 (X − µ)]. Crucially, r lies in the interior of the convex cone spanned by the parameter profiles,
Pk
i.e., r = ℓ=1 αℓ uℓ , for some positive αℓ > 0, ℓ ∈ [k]
(see Lemma 2 and Fig. 1(b)). Using this r̂, we ‘mirror’ the
labels in the second part of the dataset:
Zi = Yi sgnhr̂, Xi i,

for ⌊n/2⌋ < i ≤ n.

In words, Zi equals Yi for all i in the positive half-space
defined by the mirroring direction; instead, all labels for
points i in the negative half-space are flipped (i.e., Zi =
−Yi ). This is illustrated in Figure 1(c).
Spectral Decomposition. (Lines 5–8) The mirrored labels are used to compute a weighted covariance matrix over
whitened features as follows:

3. Subspace Estimation
In this section, we present our algorithm for subspace estimation, which we refer to as S PECTRAL M IRROR. Our
main technical contribution, stated formally below, is that
the output Û of S PECTRAL M IRROR is a consistent estimator of the subspace U as soon as n ≥ C d, for a sufficiently
large constant C.
3.1. Spectral Mirror Algorithm

Q̂ =

1
⌈ n2 ⌉

n
X

i=⌊n/2⌋+1

Zi Σ̂−1/2 (Xi − µ̂)(Xi − µ̂)T Σ̂−1/2

The spectrum of Q̂ has a specific structure, that reveals the
span U . In particular, as we will see in Section 4, Q̂ converges to a matrix Q that contains an eigenvalue with multiplicity n − k; crucially, the eigenvectors corresponding
to the remaining k eigenvalues, subject to the linear transform Σ̂−1/2 , span the subspace U . As such, the final steps
of the algorithm amount to discovering the eigenvalues that
‘stand out’ (i.e., are different from the eigenvalue with multiplicity n − k), and rotating the corresponding eigenvectors to obtain Û . More specifically, let (λℓ , wℓ )ℓ∈[d] be the
eigenvalues and eigenvectors of Q̂. Recall that k < n/2.
The algorithm computes the median of all eigenvalues, and
identifies the k eigenvalues furthest from this median; these
are the ‘outliers’. The corresponding k eigenvectors, multiplied by Σ̂−1/2 , yield the subspace estimate Û .

We begin by presenting our algorithm for estimating the
subspace span U . Our algorithm consists of three main
steps. First, as pre-processing, we estimate the mean and
covariance of the underlying features Xi . Second, using
these estimates, we identify a vector r̂ that concentrates
near the convex cone spanned by the profiles (uℓ )ℓ∈[k] . We
use this vector to perform an operation we call mirroring:
we ‘flip’ all labels lying in the negative halfspace determined by r̂. Finally, we compute a weighted covariance
matrix Q̂ over all Xi , where each point’s contribution is
weighed by the mirrored labels: the eigenvectors of this
matrix, appropriately transformed, yield the span U .

The algorithm does not require knowledge of the classifier
response function f . Also, while we assume knowledge
of k, an eigenvalue/eigenvectors statistic (see, e.g., ZelnikManor & Perona (2004)) can be used to estimate k, as the
number of ‘outlier’ eigenvalues.

These operations are summarized in Algorithm 1. We discuss each of the main steps in more detail below:

3.2. Main Result

Pre-processing. (Lines 1–2) We split the dataset into two
halves. Using the first half (i.e., all Xi with 1 ≤ i ≤ ⌊ n2 ⌋),
we construct estimates µ̂ ∈ Rd and Σ̂ ∈ Rd×d of the feature mean and covariance, respectively. Standard Gaussian
(i.e., ‘whitened’) versions of features Xi can be constructed
as Σ̂−1/2 (Xi − µ̂).

Our main result states that S PECTRAL M IRROR is a consistent estimator of the subspace spanned by (uℓ )ℓ∈[k] . This is
true for ‘most’ µ ∈ Rd . Formally, we say that an event occurs for generic µ if adding an arbitrarily small random perturbation to µ, the event occurs with probability 1 w.r.t. this
perturbation.

Learning Mixtures of Linear Classifiers

Theorem 1. Denote by Û the output of S PECTRAL M IR ROR, and let Pr⊥ ≡ I − rr T /krk2 be the projector orthogonal to r, given by (6). Then, for generic µ, as well as for
µ = 0, there exists ǫ0 > 0 such that, for all ǫ ∈ [0, ǫ0 ),
nǫ2
).
Pr(dP (Pr⊥ U, Û ) > ǫ) ≤ C1 exp(−C2
d

In other words, Û provides an accurate estimate of Pr⊥ U
as soon as n is significantly larger than d. This holds for
generic µ, but we also prove that it holds for the specific
and important case where µ = 0; in fact, it also holds for
all small-enough µ. Note that this does not guarantee that
Û spans the direction r ∈ U ; nevertheless, as shown below, the latter is accurately estimated by r̂ (see Lemma 1)
and can be added to the span, if necessary. Moreover, our
experiments suggest this is rarely the case in practice, as Û
indeed includes the direction r (see Section 5).

4. Proof of Theorem 1
Recall that we denote by r the population (n = ∞) version
of r̂. Let g(s) ≡ 2f (s) − 1, for s ∈ R, and observe that
Pk
E[Y | X = x] = ℓ=1 pℓ g(huℓ , xi). Hence,
r = E Σ−1 (X − µ) ·

P

k
ℓ=1

pℓ g(huℓ , Xi)

i

.

(6)

Lemma 1. There exist an absolute constant C > 0 and
c1 , c′1 , c′2 that depend on kXkψ2 such that:
Pr(kr̂ − rk2 ≥ ǫ) ≤ Ce

c

2 nǫ
d

2

, c′1

√

nǫ−c′2

√

d

2 	

.

The proof of Lemma 1 relies on a large deviation inequality
for sub-Gaussian vectors, and is provided in (Sun et al.,
2013). Crucially, r lies in the interior of the convex cone
spanned by the parameter profiles:
Pk
Lemma 2. r = ℓ=1 αℓ uℓ for some αℓ > 0, ℓ ∈ [k].
Proof. From (6),
r=

Pk

ℓ=1

< ∞, as g is

Σ−1 E[(X − µ)g(hu, Xi)]
(4)

= Σ−1 Cov(X, hu, Xi)E[g ′ (X ′ )], where X ′ ∼ N (µ0 , σ02 )

= Σ−1 · Σu · E[g ′ (X ′ )] = E[g ′ (X ′ )] · u

and the lemma follows.
For r and (αℓ )ℓ∈[k] as in Lemma 2, define
z(x) = E[Y sgn(hr, Xi) | X = x]


P
P
k
k
p
g(hx,
u
i)
·
sgn
=
α
hx,
u
i
.
ℓ
ℓ
ℓ=1 ℓ
ℓ=1 ℓ

Observe that z(x) is the expectation of the mirrored label at
a point x presuming that the mirroring direction is exactly
r. Let Q ∈ Rd×d be the matrix:
Q = E[z(X)Σ−1/2 (X − µ)(X − µ)T Σ−1/2 ].

Then Q̂ concentrates around Q, as stated below.
Lemma 3. Let ǫ0 ≡ min{α1 , . . . , αk }σmin (U ), where
the αℓ > 0 are defined as per Lemma 2 and σmin (U )
is the smallest non-zero singular value of U . Then for
ǫ < min(ǫ0 , krk/2):

Pr(kQ̂ − Qk2 > ǫ) ≤ C exp{−F (ǫ2 )},
n
√ 2 o
√
2
′
′
,
c
nǫ
−
c
, C an abwhere F (ǫ) ≡ min c1 nǫ
1
2 d
d
solute constant, and c1 , c′1 , c′2 depend on µ, Σ, and krk.

Then, the following concentration result holds:

− min

1
E[X ′ g(X ′ )]
σ02

= Σ−1 · E[(X − µ)X T u] · E[g ′ (X ′ )]

Here C1 is an absolute constant, and C2 > 0 depends on
µ, Σ, f and (uℓ )ℓ∈[k] .

h

identity (4), E[g ′ (X ′ )] =
bounded. Hence:

pℓ Σ−1 E[(X − µ)g(huℓ , Xi)].

It thus suffices to show that Σ−1 E[(X − µ)g(hu, Xi)] =
αu, for some α > 0. Note that X ′ = hu, Xi is normal
with mean µ0 = uT µ and variance σ02 = uT Σu > 0.
Since f is analytic and non-decreasing, so is g; moreover,
g ′ ≥ 0. This, and the fact that g is non-constant, implies E[g ′ (X ′ )] > 0. On the other hand, from Stein’s

The proof of Lemma 3 is also provided in (Sun et al., 2013).
We again rely on large deviation bounds for sub-gaussian
random variables; nevertheless, our proof diverges from
standard arguments because r̂, rather than r, is used as a
mirroring direction. Additional care is needed to ensure
that (a) when r̂ is close enough to r, its projection to U lies
in the interior of the convex cone spanned by the profiles,
and (b) although r̂ may have a (vanishing) component outside the convex cone, the effect this has on Q̂ is negligible,
for n large enough.
An immediate consequence of Lemma 2 is that r reveals a
direction in the span U . The following lemma states that
the eigenvectors of Q, subject to a rotation, yield the remaining k − 1 directions:
Lemma 4. Matrix Q has at most k + 1 distinct eigenvalues. One eigenvalue, termed λ0 , has multiplicity d − k.
For generic µ, as well as for µ = 0, the eigenvectors
w1 , . . . , wk corresponding to the remaining eigenvalues
λ1 , . . . , λk are such that
Pr⊥ U = span(Pr⊥ Σ−1/2 w1 , . . . , Pr⊥ Σ−1/2 wk ),
where Pr⊥ is the projection orthogonal to r.

Learning Mixtures of Linear Classifiers

hand, range(R) ⊆ Ũ , for Ũ = span(ũ1 , . . . , ũℓ ), and
r̃T Rr̃ = 0 where r̃ ∈ Ũ \ {0}), so rank(R) = k − 1. The
latter also implies that range(R) = Pr̃⊥ Ũ , as range(R)⊥r̃,
range(R) ⊆ Ũ , and dim(range(R)) = k − 1.

Proof. Note that
Q = E[z(X)Σ
= E[z(Σ

1/2

− 12

(X − µ)(X − µ)T Σ
T

W + µ)W W ],

− 21

]

where W ∼ N (0, I)

=E

k
X

pℓ g(hΣ 2 W +µ,uℓ i) sgn(hΣ 2 W +µ,ri)W W T

=E

k
X

pℓ g(hW + µ̃, ũℓ i) sgn(hW + µ̃, r̃i)W W T

ℓ=1

ℓ=1

1

1





The above imply that Q has one eigenvalue of multiplicity
n − k, namely a. Moreover, the eigenvectors w1 , . . . , wk
corresponding to the remaining eigenvalues (or, the nonzero eigenvalues of Q − aI) are such that
Pr̃⊥ Σ1/2 U = Pr̃⊥ span(w1 , . . . , wk ).

−2
2
2
for
Pkũℓ ≡ Σ uℓ , r̃ ≡ Σ r, and µ̃ ≡ Σ µ. Hence Q =
ℓ=1 pℓ Qℓ where

The lemma thus follows by multiplying both sides of the
above equality with Pr⊥ Σ−1/2 , and using the fact that
Pr⊥ Σ−1/2 Pr̃⊥ = Pr⊥ Σ−1/2 .

By a rotation invariance argument, Qℓ can be written as

It remains to show that γℓ =
6 0, for all ℓ ∈ [k], when µ is
generic or 0. Note that

1

1

1

Qℓ = E[g(hũℓ , W + µ̃i) sgn(hr̃, W + µ̃i)W W T ].

Qℓ = aℓ I + bℓ (ũℓ r̃T + r̃ũTℓ ) + cℓ ũℓ ũTℓ + dℓ r̃r̃T

(7)

for some aℓ , bℓ , cℓ , dℓ ∈ R. To see this, let Q̃ℓ =
[q̃ij ]i,j∈[d] , and suppose first that
r̃ = [r̃1 , r̃2 , 0, . . . , 0] and ũℓ = [ũℓ1 , ũℓ2 , 0, . . . , 0].

(8)

Since W is whitened, its coordinates are independent.
Thus, under (8), q̃ij = 0 for all i 6= j s.t. i, j > 2, and
q̃ii = aℓ for i > 2, for some aℓ . Thus Q̃ℓ = aℓ I + B,
where B is symmetric and 0 everywhere except perhaps on
B11 , B12 , B21 , B22 (the top left block). Since the profiles
uℓ are linearly independent, so are ũℓ and r̃, by Lemma 2.
Hence, matrices ũℓ r̃T + r̃ũTℓ , ũℓ ũTℓ , r̃r̃T span all such B,
so (7) follows. Moreover, since W is whitened, Q̃ℓ is rotation invariant and thus (7) extends beyond (8); indeed, if
r̃′ = Rr̃, ũ′ℓ = Rũℓ , µ̃′ = Rµ̃ where R a rotation matrix
(i.e. RRT = I), then Q′ = RQRT . Hence, as (8) holds
for some orthonormal basis, (7) holds for all bases.
Pk
Let a = ℓ=1 pℓ aℓ . Then
Q − aI =

k
X

pℓ dℓ r̃r̃T + r̃(

+(

k
X

k
X

pℓ bℓ ũℓ )T +

ℓ=1

ℓ=1

ℓ=1

pℓ bℓ ũℓ )r̃T +

k
X

pℓ cℓ ũℓ ũTℓ .

ℓ=1

Let Pr̃⊥ be the projector orthogonal to r̃, i.e., Pr̃⊥ =
r̃r̃ T
⊥
I − kr̃k
2 . Let vℓ ≡ Pr̃ ũℓ . Lemma 2 and the linear in2
dependence of ũℓ imply that vℓ 6= 0, for all ℓ ∈ [k]. Define
Pk
R ≡ Pr̃⊥ (Q − aI)Pr̃⊥ = ℓ=1 γℓ vℓ vℓT , where γℓ = pℓ cℓ ,
ℓ ∈ [k]. We will show below that for generic µ, as well
as for µ = 0, γℓ 6= 0 for all ℓ ∈ [k]. P
This implies that
rank(R) = k − 1. Indeed, R = Pr̃⊥ γℓ ũℓ ũTℓ Pr̃⊥ =
Pr̃⊥ R̃Pr̃⊥ , where R̃ has rank k by the linear independence of profiles. As P⊥ is a projector orthogonal to a 1dimensional space, R has rank at least k − 1. On the other

(7)

cℓ hũℓ , vℓ i2 = hvℓ , (Qℓ − aℓ I)vℓ i =

(9)
2

Cov(g(hũℓ , W + µ̃i) sgn(hr̃, W + µ̃i); hW, vℓ i ) ≡ c̃ℓ

It thus suffices to show that c̃ℓ 6= 0. Lemma 2 implies that
ũℓ = vℓ + cr̃ for some c > 0, hence
c̃ℓ = Cov[g(X + cY + zℓ (µ)i) sgn(Y + z0 (µ)); X 2 ],
where X ≡ hvℓ , W i and Y ≡ hr̃, W i are independent
Gaussians with mean 0, and zℓ (µ) ≡ hũℓ , µ̃i, z0 (µ) ≡
hr̃, µ̃i. Hence, c̃ℓ = Cov[F (X); X 2 ] where
F (x) = EY [g(x + cY + zℓ (µ)) sgn(Y + z0 (µ))]
Z ∞
Z −z0 (µ)
= g(x+cy +zℓ (µ))φ(y)dy− g(x + cy + zℓ (µ)φ(y)dy
−z0 (µ)

−∞

where φ the normal p.d.f. Assume first that µ = 0.
By (3), g is anti-symmetric, i.e., g(−x) = −g(x). Thus,
Y ′ ≡−Y

F (−x) = EY [g(−x + cY ) sgn(Y )] = EY ′ [g(−x −
cY ′ ) sgn(−Y ′ )] = F (x), i.e., F is symmetric.
Further,
R∞
F ′ (x) = Ey [g ′ (x + cY ) sgn(Y )] = 0 (g ′ (x + cy) −
g ′ (x − cy))φ(y)dy. The strict concavity of g in [0, ∞)
implies that g ′ is decreasing in [0, +∞), and the antisymmetry of g implies that g ′ is symmetric. Take x > 0: if
x > cy ≥ 0, g ′ (x + cy) > g ′ (x − cy), while if x ≤ cy, then
g ′ (x − cy) = g ′ (cy − x) > g ′ (cy + x), so F ′ (x) is negative for x > 0. By the symmetry of F , F ′ (x) is positive
for x < 0. As such, F (x) = G(x2 ) for some strictly decreasing G, and c̃ℓ = Cov(G(Z); Z) for Z = X 2 ; hence,
c̃ℓ < 0, for all ℓ ∈ [k].

To see that c̃ℓ 6= 0 for generic µ, recall that f is analytic
and hence so is g. Hence, c̃ℓ is an analytic function of µ,
for every ℓ ∈ [k]; also, as c̃ℓ (µ) < 0 for µ = 0, it is not
identically 0. Hence, the sets {µ ∈ Rd : c̃ℓ (µ) = 0}, ℓ ∈
[k], have Lebesgue measure 0 (see, e.g., pg. 83 in (Krantz
& Parks, 2002)), and so does their union Z. As such, c̃ℓ 6=
0 for generic µ; if not, there exists a ball B ⊂ Rd such that
B ∩ Z has positive Lebesgue measure, a contradiction.

Learning Mixtures of Linear Classifiers

Note that the Gaussianity of X is crucially used in the fact
that the ‘whitened’ features W are uncorrelated, which in
turn yields Eq. (7). We believe that the theorem can be
extended to more general distributions, provided that the
1
transform Σ− 2 de-correlates the coordinates of X.

0.8
d = 10
d = 20
d = 30

0.6

0.4

0.2
1000

2000

3000
n

4000

Largest principal angle

Largest principal angle

0.8

5000

d = 10
d = 20
d = 30

0.6

0.4

0.2
0

(a) sin(dP ) vs. n

100

200
300
n/d

400

500

We conduct computational experiments to validate the performance of S PECRAL M IRROR on subspace estimation,
prediction, and clustering. We generate synthetic data using k = 2, with profiles uℓ ∼ N (0, I), ℓ = 1, 2 and
mixture weights pℓ sampled uniformly at random from
the k-dimensional simplex. Features are also Gaussian:
Xi ∼ N (0, I), i = 1, . . . , n; labels generated by the ℓ-th
classifier are given by yi = sgn(uTℓ Xi ), i = 1, . . . , n.

(b) sin(dP ) vs. n/d

Figure 2. Convergence of Û to U .
0.7
0.6

d = 10
d = 20
d = 30
Loss

Loss

0.5
0.4

0.5
0.4

0.3
0.2
1000

d = 10
d = 20
d = 30

0.6

2000

3000
n

(a) K =

√

4000

5000

n

1000

2000

3000
n

4000

5000

(b) K = log(n)

Figure 3. Predicting the expected label given features using KNN (RMSE). Dotted lines are for K-NN after projecting the features Xi onto Û .

Denote by λ0 the eigenvalue of multiplicity d − k in
Lemma 4. Let ∆ = minℓ∈[k] |λ0 − λℓ | be the gap between
λ0 and the remaining eigenvalues. Then, the following
lemma holds; this, along with Lemma 4, yields Theorem 1.


Pr(dP (U, Û ) > ǫ) ≤ C exp − F (∆ǫ) ,
where ǫ0 , F are defined as per Lemma 3.
Proof. If we ensure kQ̂ − Qk ≤ ∆/4, then, by Weyl’s theorem (Horn & Johnson, 2012), d − k eigenvalues of Q̂ are
contained in [λk+1 − ∆/4, λk+1 + ∆/4], and the remaining eigenvalues are outside this set, and will be detected
by S PECTRAL M IRROR. Moreover, by the Davis-Kahan
sin(θ) theorem,
kQ̂−Qk2

∆ − kQ̂−Qk2

=

1
∆
kQ̂−Qk2

−1

Thus the event dp (U, Û ) ≤ ǫ is implied by kQ̂ − Qk2 ≤
∆ǫ
1+ǫ ≤ ∆ǫ. Moreover, this implies that sufficient condition for kQ̂ − Qk2 ≤ ∆/4 (which is required for S PEC TRAL M IRROR to detect the correct eigenvalues) is that
ǫ ≤ 14 . The lemma thus follows from Lemma 3.

Convergence. We study first how well S PECTRAL M IR ROR estimates the span U . Figure 2(a) shows the convergence of Û to U in terms of (the sin of) the largest principal angle between the subspaces versus the sample size n.
We also plot the convergence versus the effective sample
size n/d (Figure 2(a)). The curves for different values of d
align in Figure 2, indicating that the upper bound in Thm. 1
correctly predicts the sample complexity as n ≈ Θ(d).
Though not guaranteed by Theorem 1, in all experiments
r was indeed spanned by Û , so the addition of r̂ to Û was
not necessary.
Prediction through K-NN. Next, we use the estimated
subspace to aid in the prediction of expected labels. Given
a new feature vector X, we use the average label of its
K nearest neighbors (K-NN) in the training set to predict
its expected label. We do this for two settings: once over
the raw data (the ‘ambient’ space), and once over data for
which the features X are first projected to Û , the estimated
span (of dimension 2). For
√ each n, we repeat this procedure 25 times with K = n and K = log n. We record the
average root mean squared error between predicted and expected labels over the 25 runs. Figures 3(a) and 3(b) show
that, despite the error in Û , using K-NN on this subspace
outperforms K-NN on the ambient space.

Lemma 5. Let Û be our estimate for U . If λ1 , . . . , λk
are separated from λ0 by at least ∆, then for ǫ ≤
min(ǫ0 /∆, 41 ), we have

dp (range(Q), range(Q̂)) ≤

5. Experiments

.

Prediction and Clustering through EM. We next study
the performance of prediction and clustering using the
Expectation-Maximization (EM) algorithm. We use EM
to fit the individual profiles both over the training set, as
well as on the dataset projected to the estimated subspace
Û . We conducted two experiments in this setting: (a) initialize EM close to the true profiles uℓ , ℓ ∈ [k], and (b)
randomly initialize EM and choose the best set of profiles
from 30 runs. For each n we run EM 10 times.
The first set of prediction experiments, we again compare
expected labels to the predicted labels, using for the latter profiles uℓ and mixture probabilities pℓ as estimated by

Learning Mixtures of Linear Classifiers
0.5
d = 10
d = 20
d = 30

0.5
Loss

Loss

0.3
0.2

d = 10
d = 20
d = 30

0.6

0.4

d = 10
d = 20
d = 30

0.4
Loss

0.4

0.3

0.3

0.2
0.1

0.1

0
2000

4000
n

6000

8000

(a) EM Prediction (close to gr. truth)

0.2
2000

4000
n

6000

8000

(b) EM Prediction (random)

2000

4000
n

6000

8000

(c) Clustering (random)

Figure 4. (a) Predicting the label given features and the classifier using using EM (normalized 0-1 loss) from a starting point close to
ground truth. Dotted lines are for kNN after projecting the features onto the estimated subspace. (b) Predicting the label given features
and the classifier using using EM (normalized 0-1 loss) from a random starting point. (c) Predicting the classifier given features and the
label.

EM. Figure 4(a) measures the statistical efficiency of EM
over the estimated subspace versus EM over the ambient
space, when EM is initialized close to the true profiles. The
second set of experiments, illustrated in Figure 4(b), aims
to capture the additional improvement due to the reduction
in the number of local minima in the reduced space. In
both cases we see that constraining the estimated profiles
to lie in the estimated subspace improves the statistical efficiency of EM; in the more realistic random start experiments, enforcing the subspace constraint also improves the
performance of EM by reducing the number of local minima. We also observe an overall improvement compared to
prediction through K-NN.
Finally, we use the fitted profiles uℓ to identify the classifier generating a label given the features and the label. To
do this, once the profiles uℓ have been detected by EM, we
use a logistic model margin condition to identify the classifier who generated a label, given the label and its features.
Figure 4(c) shows the result for EM initialized at a random
point, after choosing the best set of profiles from out of 30
runs. We evaluate the performance of this clustering procedure using the normalized 0-1 loss. Again, constraining the
estimated profiles to the estimated subspace significantly
improves the performance of this clustering task.

6. Conclusions
We have proposed S PECTRAL M IRROR, a method for discovering the span of a mixture of linear classifiers. Our
method relies on a non-linear transform of the labels, which
we refer to as ‘mirroring’. Moreover, we have provided
consistency guarantees and non-asymptotic bounds, that
also imply the near optimal statistical efficiency of the
method. Finally, we have shown that, despite the fact
that S PECTRAL M IRROR discovers the span only approximately, this is sufficient to allow for a significant improvement in both prediction and clustering, when the features
are projected to the estimated span.

We have already discussed several technical issues that remain open, and that we believe are amenable to further
analysis. These include amending the Gaussianity assumption, and applying our bounds to other pHd-inspired methods. An additional research topic is to further improve the
computational complexity of the estimation of the eigenvectors of the ‘mirrored’ matrix Q̂. This is of greatest interest in cases where the covariance Σ and mean µ are a priori known. This would be the case when, e.g., the method
is applied repeatedly and, although the features X are sampled from the same distribution each time, labels Y are generated from a different mixture of classifiers. In this case,
S PECTRAL M IRROR lacks the pre-processing step, that requires estimating Σ and is thus computationally intensive;
the remaining operations amount to discovering the spectrum of Q̂, an operation that can be performed more efficiently. For example, we can use a regularized M-estimator
to exploit the fact that Σ−1/2 Q̂Σ−1/2 should be the sum of
a multiple of the identity and a low rank matrix—see, e.g.,
Negahban et al. (2012).

References
Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., and
Telgarsky, M. Tensor decompositions for learning latent variable models, 2012. arXiv preprint, arXiv:
1210.7559.
Arora, S. and Kannan, R. Learning mixtures of arbitrary
Gaussians. In Proceedings of the 33rd annual ACM Symposium on Theory of Computing, pp. 247–257. ACM,
2001.
Bartholomew, D. J., Knott, M., and Moustaki, I. Latent
Variable Models and Factor Analysis: a Unified Approach, volume 899. Wiley & Sons, 2011.
Bishop, C. M. Latent variable models. In Learning in
Graphical Models, pp. 371–403. Springer, 1998.

Learning Mixtures of Linear Classifiers

Chaganty, A. T. and Liang, P. Spectral experts for estimating mixtures of linear regressions. In ICML, 2013.
Cook, R. D. Principal Hessian directions revisited. Journal
of the American Statistical Association, 93(441):84–94,
1998.
Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B
(Methodological), pp. 1–38, 1977.
Horn, R. A. and Johnson, C. R. Matrix Analysis. Cambridge University Press, 2012.
Hsu, D., Kakade, S. M., and Zhang, T. A spectral algorithm for learning hidden Markov models. Journal of
Computer and System Sciences, 78(5):1460–1480, 2012.
Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of
experts and the EM algorithm. Neural Computation, 6
(2):181–214, 1994.
Krantz, S. G. and Parks, H. R. A primer of real analysis
and functions. Springer, 2002.
Li, K.-C. On principal Hessian directions for data visualization and dimension reduction: another application
of Stein’s Lemma. Journal of the American Statistical
Association, 87(420):1025–1039, 1992.
Liang, P., Bouchard-Côté, A., Klein, D., and Taskar, B. An
end-to-end discriminative approach to machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Linguistics, pp. 761–768. Association for Computational Linguistics, 2006.
Liu, J. S. Siegel’s formula via Stein’s identities. Statistics
& Probability Letters, 21(3):247–251, 1994.
McLachlan, G. and Peel, D. Finite Mixture Models. Wiley
& Sons, 2004.
Moitra, A. and Valiant, G. Settling the polynomial learnability of mixtures of Gaussians. In 51st Annual IEEE
Symposium on Foundations of Computer Science, pp.
93–102. IEEE, 2010.
Negahban, S. N., Ravikumar, P., Wainwright, M. J., and Yu,
B. A unified framework for high-dimensional analysis
of m-estimators with decomposable regularizers. Statistical Science, 27(4):538–557, 2012.
Pearson, Karl. Contributions to the mathematical theory
of evolution. Philosophical Transactions of the Royal
Society of London. A, 185:71–110, 1894.

Quattoni, A., Collins, M., and Darrell, T. Conditional random fields for object recognition. In Advances in Neural
Information Processing Systems, pp. 1097–1104, 2004.
Stein, C. M. Estimation of the mean of a multivariate normal distribution. In Prague Symposium on Asymptotic
Statistics, 1973.
Sun, Y., Ioannidis, S., and Montanari, A. Learning mixtures of linear classifiers, 2013. arXiv:1311.2547.
Zelnik-Manor, L. and Perona, P. Self-tuning spectral clustering. In Advances in Neural Information Processing
Systems, 2004.

