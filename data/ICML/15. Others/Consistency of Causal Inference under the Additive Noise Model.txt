Consistency of Causal Inference under the Additive Noise Model

Samory Kpotufe
Toyota Technological Institute-Chicago

SAMORY @ TTIC . EDU

Eleni Sgouritsa
Max Planck Institute for Intelligent Systems

ELENI . SGOURITSA @ TUEBINGEN . MPG . DE

Dominik Janzing
Max Planck Institute for Intelligent Systems

DOMINIK . JANZING @ TUEBINGEN . MPG . DE

Bernhard SchoÌˆlkopf
Max Planck Institute for Intelligent Systems

BS @ TUEBINGEN . MPG . DE

Abstract
We analyze a family of methods for statistical causal inference from sample under the socalled Additive Noise Model. While most work
on the subject has concentrated on establishing
the soundness of the Additive Noise Model, the
statistical consistency of the resulting inference
methods has received little attention. We derive
general conditions under which the given family of inference methods consistently infers the
causal direction in a nonparametric setting.

1. Introduction
Drawing causal conclusions for a set of observed variables
given a sample from their joint distribution is a fundamental problem in science. Conditional-independence-based
methods (Pearl, 2000; Spirtes et al., 2000) estimate a set of
directed acyclic graphs, all entailing the same conditional
independences, from the data. However, these methods can
not distinguish between two graphs that entail the same
set of conditional independences, the so-called Markov
equivalent graphs. Consider for example the case of only
two observed dependent random variables. Conditionalindependence-based methods can not recover the causal
graph since X â†’ Y and Y â†’ X are Markov equivalent. An elegant basis for causal graphs is the framework
of structural causal models (SCMs) (Pearl, 2000), where
every observable is a function of its parents and an unobserved independent noise term. This allows us to formulate
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

an assumption on function classes which lets us infer the
causal direction in two-variable case.
A special case of SCMs is the Causal Additive Noise Model
(CAM) (Shimizu et al., 2006; Hoyer et al., 2009; Tillman
et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed
to cause Y if (i) Y can be obtained as a function of X plus
a noise term independent of X, but (ii) X cannot be obtained as a function of Y plus independent noise, then we
infer that X causes Y . In this case, where (i) and (ii) hold
simultaneously, the CAM is termed identifiable.
Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for
which (i) and (ii) hold simultaneously. Early work by
(Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f (X) + Î· is
linear, provided the independent noise Î· is not Gaussian.
Later, Hoyer et al. (2009), Zhang & HyvaÌˆrinen (2009) and
Peters et al. (2011a) showed that the CAM is identifiable
more generally even if f is nonlinear, the main technical
requirements being that the marginals PX , and PÎ· are absolutely continuous on R, with PÎ· having support R. Note
that Zhang & HyvaÌˆrinen (2009) also introduces a generalization of the CAM termed post-nonlinear models. Further work by Peters et al. (2011b) showed how to reduce
causal inference for a network of multiple variables under
the CAM to the case of two variables X and Y discussed
so far, by properly extending the conditions (i) and (ii) to
conditional distributions instead of marginals. Thus, the
soundness of the CAM being established by these various
works, the next natural question is to understand the statistical behavior of the resulting estimation procedures on
finite samples.

Consistency of Causal Inference under the Additive Noise Model

Current insights into this last question are mostly empirical.
Various works (Shimizu et al., 2006; Hoyer et al., 2009;
Peters et al., 2011a) have successfully validated procedures
based on the CAM (outlined in Section 1.1 below) on a
mix of artificial and real-world datasets where the causal
structure to be inferred is clear. However, on the theoretical side, it remains unclear whether these procedures can
infer causality from samples in general situations where
the CAM is identifiable. In the particular case where the
functional relation between X and Y is linear, HyvaÌˆrinen
et al. (2008) proposed a successful method shown to be
consistent. Two recent Arxived results, BuÌˆhlmann et al.
(2013); Nowzohour & BuÌˆhlmann (2013), show the consistency of maximum log-likelihood approaches to causal inference under the multi-variable network extension of Peters et al. (2011b).
While consistency has been shown for particular procedures, in this paper we are rather interested in general conditions under which common approaches, with various algorithmic instantiations, are consistent. We derive both algorithmic and distributional conditions for statistical consistency in general situations where the CAM is identifiable. The present work focuses on the case of two real
variables, allowing us to focus on the inherent difficulties
of achieving consistency with the common algorithmic approaches. These difficulties, described in Section 1.2 have
to do with estimating the degree of independence between
noise and input, while the noise is itself estimated from the
input and hence is inherently dependent on the input.
1.1. Inference Methods Under the Additive Noise
Model
Causal inference methods under the Additive Noise Model
typically follow the meta-procedure below. Assume f and
g are the best functional fits under some risk, respectively
Y â‰ˆ f (X) and X â‰ˆ g(Y ):

Fit Y as a function f (X), obtain the residuals
Î·Y,f = Y âˆ’ f (X), fit X as a function g(Y ),
obtain the residuals Î·X,g = X âˆ’ g(Y ), decide
X â†’ Y if Î·Y,f âŠ¥
âŠ¥ X but Î·X,g 6âŠ¥
âŠ¥ Y , decide Y â†’
X if the reverse holds true, abstain otherwise.

Instantiations thus vary in the regression procedures employed for function fitting, and in the independence measures employed. Our analysis concerns procedures employing an entropy-based independence measure, which is
cheaper than usual independence tests. These procedures
vary in the regression and entropy estimators employed.
They are presented in detail in Section 3.

1.2. Towards Consistency: Main Difficulties
Assume (i) and (ii) hold so that X causes Y under the
CAM. We want to detect this from sufficiently large finite
samples. This is consistency in a rough sense.
Establishing consistency of the above meta-procedure faces
many subtle difficulties. The above outlined algorithmic
approach consists of four interdependent statistical estimation tasks, namely two regression problems and two
independence-tests. Considered separately, the consistency
of such estimation tasks is well understood, but in the
present context the success of the independence tests is
contingent on successful regression.
The main difficulty is that although we are observing X
and Y , we are not observing the residuals Î·Y,f and Î·X,g ,
but empirical approximations Î·Y,fn and Î·X,gn obtained by
estimating f and g as fn and gn on a sample of size n.
For now, consider just detecting that Î·Y,f , f unknown,
is independent from X. A good estimator fn will ensure that fn and f are close, usually in an L2 sense (i.e.
2
E X |fn (X) âˆ’ f (X)| â‰ˆ 0). Hence Î·Y,fn is close to Î·Y,f ,
but unfortunately this does not imply that Î·Y,fn âŠ¥
âŠ¥ X if
Î·Y,f âŠ¥
âŠ¥ X. In fact it is easy to construct r.v.â€™s A, B, C
such that A âŠ¥
âŠ¥ B, |B âˆ’ C| < , for arbitrary , but C 6âŠ¥
âŠ¥ A.
Thus, the estimate Î·Y,fn might be close to Î·Y,f , yet it might
still appear dependent on X even if Î·Y,f is not. Complicating matters further, Î·Y,fn and Î·Y,f would only be close in
an average sense (instead of close for every value of X)
since fn and f are typically only close in an average sense
(e.g. close in L2 ).
Now consider the full causal discovery, i.e. consider also
detecting that Î·X,g depends on Y . To achieve consistency,
the independence test employed must detect more dependence between Î·X,gn and Y than between Î·Y,fn and X.
This will depend on how the particular independence test is
influenced by errors in the particular regression procedures
employed, and the relative rates at which these various procedures converge.
As previously mentioned, we will consider a family of
independence-tests based on comparing sums of entropies.
We will handle the above difficulties and derive conditions
for consistency by first understanding how the various estimated entropies converge as a function of regression convergence (L2 convergence).
We do not consider the question of finite-sample convergence rates for causal estimation under the CAM. In fact,
it is not even clear whether it is generally possible to establish such rates. This is because it is generally possible
that the Bayes best fits f (x) = E [Y |x] is smooth while
g(y) = E [X|y] is not even continuous; yet it is well known
that without smoothness or similar structural conditions, ar-

Consistency of Causal Inference under the Additive Noise Model

bitrarily bad rates of convergence are possible in regression
(see e.g. (Gyorfi et al., 2002), Theorem 3.1).
However, along the way of deriving consistency, we analyze the convergence of various quantities, which appear
to affect the finite-sample behavior of the meta-procedure.
In particular the tails of the additive noise and the richness
of the regression algorithms seem to have a strong effect
on convergence. This is verified in controlled simulations.
The theoretical details are discussed in Section 4.

2. Preliminaries

Note that whenever Î·Y,f , âŠ¥
âŠ¥ X, we have I(Î·Y,f , X) = 0.
Therefore, by the above lemma, if Î·Y,f , âŠ¥
âŠ¥ X then CXY ,
H(X)+H(Î·Y,f ) is smaller than CY X , H(Y )+H(Î·X,g ).
This yields a measure of independence which is relatively
cheap to estimate. In particular the test depends only on
the marginal distributions of the r.v.â€™s X, Y and functional
residuals, and does not involve estimating joint distributions or conditionals, as is implicit in most independence
tests. We analyze a family of procedures based on this idea.
This family is given in the next subsection.
3.2. Meta-Algorithm

2.1. Setup and Notation

n

We let H and I denote respectively differential entropy, and
mutual information (Cover et al., 1994). Given a density p
we will at times use the (abuse of) notation H(p) when a
r.v. is unspecified.
The distribution of a r.v. Z is denoted PZ , and its density
when it exists is denoted pZ .
Throughout the analysis we will be concerned with residuals from regression fits. We use the following notation.
Definition 1. For a function f : R 7â†’ R, we consider either
of the residuals: Î·Y,f , Y âˆ’f (X) and Î·X,f , X âˆ’f (Y ).
The Causal Additive Noise Model is captured as follows:
Definition 2 (CAM). Given r.v.â€™s X, Y , a function f : R 7â†’
f,Î·

R and a r.v. Î·, we write X âˆ’âˆ’â†’ Y if the following holds:
(i) PX,Y is generated as X âˆ¼ PX , and Y = f (X) + Î·,
where the noise r.v. Î· has 0 mean and Î· âŠ¥
âŠ¥ X;
(ii) for any g : R 7â†’ R, Î·X,g , X âˆ’ g(Y ) depends on X.
We write X â†’ Y when f and Î· are clear from context.

3. Causal Inference Procedures
3.1. Main Intuition
Lemma 1. Consider any absolutely continuous jointdistribution PX,Y on X, Y âˆˆ R. For any two functions
f, g : R 7â†’ R we have
H(X) + H(Î·Y,f ) = H(Y ) + H(Î·X,g )
âˆ’ {I(Î·X,g , Y ) âˆ’ I(Î·Y,f , X)} .
Proof. By the chain rule of differential entropy we have
H(X, Y ) = H(X) + H(Y |X) = H(X) + H(Î·Y,f |X)
= H(X) + H(Î·Y,f ) âˆ’ I(Î·Y,f , X), similarly
H(X, Y ) = H(Y ) + H(Î·X,g ) âˆ’ I(Î·X,g , Y ).
Equate the two r.h.s above and rearrange.

Let {(Xi , Yi )}1 = {(x1 , y1 ), . . . , (xn , yn )} be a finite
sample drawn from PX,Y . Let Hn (X) and Hn (Y ) be respective estimators of H(X) and H(Y ) based on the samn
ple {(Xi , Yi )}1 .
We consider the following family of inference procedures:
Given an i.i.d sample {(Xi , Yi )}n
1 from PX,Y , let fn
be returned by an algorithm which fits Y as fn (X) and
gn be returned by an algorithm which fits X as gn (Y ).
Let Hn denote an entropy estimator. Given a threshold
nâ†’âˆž
parameter Ï„n âˆ’âˆ’âˆ’âˆ’â†’ 0:
Decide X â†’ Y if
Hn (X) + Hn (Î·Y,fn ) + Ï„n â‰¤ Hn (Y ) + Hn (Î·X,gn ).
Decide Y â†’ X if
Hn (Y ) + Hn (Î·X,gn ) + Ï„n â‰¤ Hn (X) + Hn (Î·Y,fn ).
Abstain otherwise.

The analysis in this paper is carried with respect to the
L2,PX and L2,PY functional norms defined as follows.
Definition 3. For f : R 7â†’ R, and a measure Âµ on R, the
1/2
R
L2,Âµ norm is given as kf k2,Âµ = t f (t)2 dÂµ(t)
.
We assume the internal procedures fn , gn , Hn have the following consistency properties.
Assumption 1. The internal procedures are consistent:
â€¢ Suppose E Y 2 < âˆž. Let f (x) , E [Y |x]. Then
P
kfn âˆ’ f k2,PX âˆ’
â†’ 0.
â€¢ Suppose E X 2 < âˆž. Let g(y) , E [X|y]. Then
P
kgn âˆ’ gk2,PY âˆ’
â†’ 0.
â€¢ Suppose Z has bounded variance, and has continuous
density pZ such that âˆƒ T, C > 0, Î± > 1, âˆ€ |t| >
P
âˆ’Î±
T, pZ (t) â‰¤ C |t| . Then |Hn (Z) âˆ’ H(Z)| âˆ’
â†’ 0.
Many common nonparametric regression procedures (e.g.
kernel, k-NN, Kernel-SVM, spline regressors) are consistent in the above sense (Gyorfi et al., 2002). Also the consistency of a variety of entropy estimators (e.g. plug-in entropy estimators) is well established (Beirlant et al., 1997).

Consistency of Causal Inference under the Additive Noise Model

0.8

Coupled
Decoupled

CYX âˆ’ CXY

1
0.5
0
âˆ’0.5

Decoupled
Coupled

0.6

0.9

Coupled
Decoupled

0.8
0.7

0.4

0.6
0.5

0.2

0.4

âˆ’1
(0.03,0.03)

N = 1500, KernelRegr, b = 1

KernelRegr, b=1, q=1

N = 1500, KernelRegr, b = 0.1, q = 1, l = 1.5
1.5

(0.39,0.39)

(4.47,4.47) (50.97,50.97)

104*(sigX, sigY)

0
0

500 1000 1500 2000 2500

(a)

Sample size

(b)

0.4

0.6

0.8

1

1.2

q (controlling the tail of the noise)

(c)

Figure 1. Plots of the difference between the complexity measures (CY X âˆ’ CXY ) for coupled and decoupled-estimation in various
scenarios. Simulated data is generated as Y = bX 3 + X + Î·. X is sampled from a uniform distribution on the open interval
(âˆ’2.5, 2.5), while Î· is sampled as |N |q Â· sign(N ) where N is a standard normal. b controls the strength of the nonlinearity of the
function and q controls the non-Gaussianity of the noise: q = 1 gives a Gaussian, while q > 1 and q < 1 produces super-Gaussian and
sub-Gaussian distributions, respectively. For entropy estimation we employ a resubstitution estimate using a kernel density estimator
tuned against log-likelihood (Beirlant et al., 1997) and for regression estimator we use kernel regression (KR). For every combination of
the parameters, each experiment was repeated 10 times, and average results for (CY X âˆ’CXY ) are reported along with standard deviation
across repetitions. Plot (a): increasing kernel bandwidth of regressor geometrically (by factors of l = 1.5), i.e. decreasing richness of
the algorithm. When the capacity of the regression algorithm is too large, the variance of the causal inference is large for coupledestimation (due to overfitting) but remains low for decoupled-estimation. Plot (b): increasing sample size (bandwidth of KR tuned by
cross-validation). For tuned bandwidth, the variance of the causal inference is only due to the sample size, so the coupled-estimation
(which estimates everything on a larger sample) becomes the better procedure. Plot (c): increasing q, i.e. the tail of the noise is made
sharper (KR tuned by cross-validation). For faster decreasing tail of the noise, the causal inference becomes better. The experiments of
Figures (b) and (c) were repeated using kernel ridge regression (KRR) tuned by cross-validation (see appendix of the extended paper).
For properly tuned parameters, the selection of regression method does not seem to matter for the causal inference results.

4. Technical overview of results
We consider the following two versions of the above metaprocedure. The analysis (Section 5) is divided accordingly.
Definition 4 (Decoupled-estimation). fn and gn are
n
learned on half of the sample {(Xi , Yi )}1 , and the
Hn (Î·Y,fn ) and Hn (Î·X,gn ) are learned on the other half of
the sample (w.l.o.g. assume n is even). Hn (X) and Hn (Y )
could be learned on either half or on the entire sample.
Definition 5 (Coupled-estimation). All fn , gn and enn
tropies Hn are learned on the entire sample {(Xi , Yi )}1 .
Our most general consistency result (Theorem 1, Section
5.1) concerns decoupled-estimation. By decoupling regression and entropy estimations, we reduce the potential of
overfitting, during entropy estimation, the generalization
error of regression. This generalization error could be large
if the regression algorithms are too rich (e.g. ERM over
large functional classes). Our simulations show that, when
the regression algorithm is too rich, the variance of the
causal inference is large for coupled-estimation but remains
low for decoupled-estimation (Fig. 1(a)). By decreasing
the richness of the class (simulated by increasing the kernel bandwidth for a kernel regressor) the source of variance
shifts to the sample size, and coupled-estimation (which estimates everything on a larger sample) becomes the better
procedure and tends to converge faster (Fig. 1(b)).

For the consistency result of Theorem 1 we make no assumption on the richness of the regression algorithms, but
simply assume that they converge in L2 (Assumption 1).
The main technicality is to then show that entropies of
residuals are locally continuous relative to the L2 metric
in both causal and anticausal directions.
For coupled-estimation, the main difficulty is the following. Even though the entropy estimators are consistent for
a fixed distribution, the distribution of the residuals change
with fn and gn , thus with every random sample (this problem is alleviated by decoupling the estimation). However,
if the richness of the regression algorithms is controlled, in
other words if the set of potential fn and gn is not too rich,
then the entropy estimate for residuals might converge. We
show in Theorem 2 (Section 5.2) that if we employ kernel
regressors with properly chosen bandwidths, and kernelbased entropy estimators with sufficiently smooth kernels,
then the resulting method is consistent for causal inference.
Both consistency results of Theorem 1 and Theorem 2 rely
f,Î·

on tail assumptions on the additive noise Î· (where X âˆ’âˆ’â†’
Y ). We assume an exponentially decreasing tail for the
more difficult case of coupled-estimation, but need only a
mild assumption of polynomially decreasing tail in the case
of decoupled-estimation. Note that it is common to assume
that Î· has Gaussian tail, and our assumptions are milder in
that respect.

Consistency of Causal Inference under the Additive Noise Model

Interestingly, our analysis for Theorem 1 suggests that convergence of causal inference is likely faster if the noise Î·
has faster decreasing tail (see Lemma 3). This is verified in
our simulations where we vary the tail of Î· (Fig. 1(c)).

5. Analysis

Lemma 2 (Properties of induced densities). Suppose PX,Y
satisfies Assumption 2 for some f, Î·, and Î± > 1. We then
have the following: (i) pX,Y has a bounded gradient on R2 ,
(ii) consider functions f 0 , g : R 7â†’ R and suppose sup |f 0 |
and sup |g| are at most T0 for some T0 ; then there exists
T 0 > 0 depending on T0 , and C 0 > 0 such that âˆ€ |t| > T 0

5.1. Consistency for Decoupled-estimation
In this section we establish a general consistency result for
the meta-procedure above. The main technicality consists
of relating differential entropy of residuals to the L2 -norms
of residuals (i.e. to the error made in function estimation).
We henceforth let Î£ denote the Lebesgue measure.
The analysis in this section uses the following polynomial
tail assumption on Î·. We note that Assumption 2 satisfies
the idenfiability conditions of (Zhang & HyvaÌˆrinen, 2009).
Assumption 2 (Tail). PX,Y is generated as follows:
f,Î·

X âˆ’âˆ’â†’ Y for some bounded function f , with bounded
derivative on R. PX has bounded support, and both PX
and PÎ· have densities pX , pÎ· with bounded derivatives on
R. Furthermore, we assume Î· has bounded variance, and
pÎ· satisfies, for some T > 0, C > 0, and Î± > 1:
âˆ€ |t| > T,

pÎ· (t) â‰¤ C |t|

âˆ’Î±

,

(1)

Note that, since the unknown target functions are assumed
bounded, any consistent regressor can be appropriately
truncated while maintaining consistency. We therefore
have the following technical assumption on the regressors.
Assumption 3. The regression procedures return bounded
functions: limnâ†’âˆž max {kfn (t)kâˆž , kgn (t)kâˆž } < âˆž.
Theorem 1 (General consistency for decoupled-estima-

n
o
âˆ’Î±
pX,Y (Â·, t), pX,Y (t, Â·), pÎ·Y,f 0 (t), pÎ·X,g (t) â‰¤ C 0 |t| .

In particular, the above holds for g(y) , E [X|Y = y].
The next lemma relates the density of residuals to the L2
distance between functions. Notice, as discussed in Section 4, that the Lemma suggests that the densities of residuals converge faster the sharper the tails of the noise Î·: the
larger Î±, the sharper the bounds are in terms of the L2 distance between functions.
Lemma 3 (Density of residuals w.r.t. L2 distance). Suppose the joint distribution PX,Y satisfies Assumption 2 for
some f, Î· and Î± > 1. Let g(y) , E[X|Y = y]. Consider
functions f 0 , g 0 : R 7â†’ R. There exist a constant C 00 such
that for kf âˆ’ f 0 k2,PX and (respectively) kg âˆ’ g 0 k2,PY sufficiently small,




(Î±âˆ’1)/2Î±


sup pÎ·Y,f 0 (t) âˆ’ pÎ·Y,f (t) â‰¤ C 00 kf 0 âˆ’ f k2,PX
,
tâˆˆR




(Î±âˆ’1)/2Î±


sup pÎ·X,g0 (t) âˆ’ pÎ·X,g (t) â‰¤ C 00 kg 0 âˆ’ gk2,PY
.
tâˆˆR

f,Î·

tion). Suppose X âˆ’âˆ’â†’ Y for some f, Î·, and PX,Y satisfies
the tail Assumption 2. Suppose fn , gn , and Hn are consistent procedures satisfying Assumption 1 and 3. Let the
meta-algorithm be decoupled as in Definition 4.
Then the probability of correctly deciding X â†’ Y goes to
1 as n â†’ âˆž.
To prove the theorem, we have to understand how the estimated entropies converge as a function of the L2 error
in regression estimation. We will proceed by bounding the
distance between the densities pÎ·Y,f and pÎ·Y,f 0 of the residuals of functions f and f 0 in terms of the L2 distance between f and f 0 (Lemma 3); this will then be used to bound
the difference in the entropy of such residuals.
Given Assumption 2, the following lemma establishes
some useful properties of the distribution PX,Y and of the
distribution of certain residuals. It is easy to verify that under our assumptions, all distributions under consideration
in the lemma are absolutely continuous.

Proof. We start by bounding the difference between
pÎ·Y,f 0 (t) and pÎ·Y,f 0 (t). We note that the same ideas can
be used to bound the difference between pÎ·X,g0 (t) and
pÎ·X,g (t), since X and Y are interchangeable in the analysis
from this point on. This is because what follows does not
depend on how PX,Y is generated, just on the properties of
the induced distributions as stated in Lemma 2.
We will partition
the space R as follows.
First, let Ro> den
q
0
note the set x : |f (x) âˆ’ f (x)| > kf âˆ’ f 0 k2,PX . We
define the following interval U âŠ‚ R: let T 0 be defined as in
Lemma 2, and Ï„ > T 0 ; we have U , [âˆ’Ï„, Ï„ ].
For any t âˆˆ R we have by writing residual densities in
terms of the joint pX,Y (as in the proof of Lemma 2 in the

Consistency of Causal Inference under the Additive Noise Model





appendix of the extedned version) that pÎ·Y,f 0 (t) âˆ’ pÎ· (t)

Z


0

=  (pX,Y (x, t + f (x)) âˆ’ pX,Y (x, t + f (x))) dx
ZR





0
(pX,Y (x, t + f (x)) âˆ’ pX,Y (x, t + f (x))) dx
â‰¤
 R\D


Now, for kf âˆ’ f 0 k2,PX sufficiently small, we can pick Ï„ =

âˆ’1/2Î±
O kf âˆ’ f 0 k2,PX
to get the result.
As previously noted we can use the same
 ideas as above


to similarly bound pÎ·X,g0 (t) âˆ’ pÎ·X,g (t) for all t âˆˆ R. It
suffices to interchange X and Y in the above analysis.

(2)
Z

|pX,Y (x, t + f 0 (x)) âˆ’ pX,Y (x, t + f (x))| dx

+
U \R>

Z

+ 

R>

(3)


0
(pX,Y (x, t + f (x)) âˆ’ pX,Y (x, t + f (x))) dx .
(4)

To bound the first term (2), let yx denote either of t + f 0 (x)
or t + f (x), we have by Lemma 2 that
Z âˆž
Z âˆž
C 0 âˆ’(Î±âˆ’1)
pX,Y (x, yx ) dx â‰¤
C 0 xâˆ’Î± dx â‰¤
Ï„
,
Î±âˆ’1
Ï„
Ï„
0

C
so that the first term (2) is at most 2 Î±âˆ’1
Ï„ âˆ’(Î±âˆ’1) .

To bound the second term (3) we recall that pX,Y has a
bounded gradient on R2 (Lemma 2). Therefore there exists
C0 such that for every x, y,  âˆˆ R, pX,Y (x, y + ) differs
from pX,Y (x, y) by at most C0 Â· ||. It follows that the
second term (3) is at most
Z
q
C0 |f 0 (x) âˆ’ f (x)| dx â‰¤ 2Ï„ Â· C0 kf âˆ’ f 0 k2,PX .

Lemma 4. Let p1 , p2 be two densities such that there exist
T, C > 1 and Î± > 1, for all |t| > T , maxiâˆˆ[2] pi (t) <
âˆ’Î±
C |t| . Suppose sup
	 tâˆˆR |p1 (t) âˆ’ p2 (t)| <  for some  <
min 1/T 2 , 1/(3e) satisfying the further condition: âˆ€t >
âˆš
1/ , t(Î±âˆ’1)/2 > ln t. We then have for  sufficiently small
âˆš
4CÎ± (Î±âˆ’1)/4
|H(p1 ) âˆ’ H(p2 )| â‰¤ 18  ln(1/3) +

.
Î±âˆ’1

Proof. Forâˆšsimplicity of notation in what follows, let
Ï„ , 1/ . Let U , [âˆ’Ï„, Ï„ ] and let U2> ,
{t âˆˆ U, p2 (t) > 2}. Define Î³(u) = âˆ’u ln u for u > 0,
and Î³(0) = 0. We will use the fact that for the function
Î³(Â·) is increasing on [0, 1/e]. We have
Z

Z

H(p1 ) =

Î³(p1 (t)) dt +

U \R>

Î³(p1 (t)) dt
U2>

R\U

Z
+
Z

The third term (4) is equal to

Î³(p1 (t)) dt
Z
Î³(p1 (t)) dt +

U \U2>

â‰¤

|P (X âˆˆ R> , Y = t + f 0 (X)) âˆ’

U2>

R\U

P (X âˆˆ R> , Y = t + f (X)) | â‰¤ PX (R> ).
We next bound PX (R> ) while noting that kf âˆ’ f 0 k2,PX
could be 0. Let  > kf âˆ’ f 0 k2,PX . By Markovâ€™s inequality,

âˆš 	 kf âˆ’ f 0 k1,PX
âˆš
PX |f (X) âˆ’ f 0 (X)| >  â‰¤

kf âˆ’ f 0 k2,PX
âˆš
â‰¤
.


1
dt
p1 (t)

+ Î£ (U \ U2> ) Â· Î³(3),

(5)

since for t âˆˆ U \ U2> we have p1 (t) â‰¤ p2 (t) +  â‰¤ 3 â‰¤
1/e.
To bound the first term of (5), notice that
Z

âˆž

Z

âˆž


âˆ’Ctâˆ’Î± ln Ctâˆ’Î± dt

Î³(p1 (t)) dt â‰¤
Ï„

ZÏ„ âˆž

0

Thus, consider a sequence of  â†’ kf âˆ’ f k2,PX , by Faq
touâ€™s lemma we have PX (R> ) â‰¤ kf âˆ’ f 0 k2,PX .
Combining the above analysis we have that


C 0 âˆ’(Î±âˆ’1)


Ï„
pÎ·Y,f 0 (t) âˆ’ pÎ· (t) â‰¤2
Î±âˆ’1
q
+ (1 + 2Ï„ Â· C0 ) kf âˆ’ f 0 k2,PX .

p1 (t) ln

â‰¤

CÎ±tâˆ’Î± ln t dt

Ï„

Z
â‰¤

âˆž

CÎ±tâˆ’(Î±+1)/2 dt

Ï„

2CÎ± âˆ’(Î±âˆ’1)/2
Ï„
,
â‰¤
Î±âˆ’1
hence we have

0

R
R\U

Î³(p1 (t)) dt â‰¤ C 0 Ï„ âˆ’Î± , for C 0 , Î±0 > 0.

Consistency of Causal Inference under the Additive Noise Model
P

Next we bound the second term of (5) as follows:
Z
1
p1 (t) ln
dt
p1 (t)
U2>
Z
1
â‰¤
dt
(p2 (t) + ) ln
p2 (t) âˆ’ 
U2>
Z
1
=
p2 (t) ln
dt
p2 (t)(1 âˆ’ /p2 (t))
U2>
Z
1
dt
+
 ln
p2 (t) âˆ’ 
U2>
Z
1
â‰¤ H(p2 ) +
p2 (t) ln
dt
1 âˆ’ /p2 (t)
U2>
Z
1
+
 ln dt

U2>
Z
â‰¤ H(p2 ) +
p2 (t) ln(1 + 2/p2 (t)) dt

By Lemma 3, convergence of fn i.e. kfn âˆ’ f k2,PX âˆ’
â†’0
 P

â†’ 0; this in turn im(t) âˆ’ pÎ· (t) âˆ’
implies supt pÎ·
Y,fn

P

Thus all quantities (a)-(d) are at most  with probability
going to 1.
5.2. Coupled Regression and Residual-entropy
Estimation
Here we consider a coupled version of the meta-algorithm
where fn and gn are kernel regressors. This is described in
the next subsection.
5.2.1. K ERNEL INSTANTIATION OF THE
META - ALGORITHM
Regression: Although any kernel that is 0 outside a
bounded region will work for the regression, we focus here
(for simplicity) on the particular case where fn and gn are
box-kernel regressors defined as follows (interchange X
and Y to obtain gn (y)):

U2>

+ Î£ (U2> ) Â· Î³()
â‰¤ H(p2 ) + 2Î£ (U2> ) Â·  + Î£ (U2> ) Â· Î³().
Combining all the above, we have
0

H(p1 ) â‰¤H(p2 ) + 3Î£ (U) Â· Î³(3) + C 0 Ï„ âˆ’Î±
âˆš
0
=H(p2 ) + 18  ln(1/3) + C 0 Î± /2 .

fn (x) =

n
1 X

nx,h

Yi 1{|Xi âˆ’x|<h} ,

(7)

i=1

where nx,h = |i : |Xi âˆ’ x| < h| , for a bandwidth h.

Notice that p1 and p2 are interchangeable in the above argument. The result therefore follows.
We are now ready to prove the main theorem.
Proof of Theorem 1
Let f (x) , E [Y |x] and g(y) , E [X|y]. By Lemma 1,
H(X) + H (Î·Y,f ) > H(Y ) + H (Î·X,g ) + 8,

Y,f

plies by Lemma 4 that |H (Î·Y,fn ) âˆ’ H (Î·Y,f )| âˆ’
â†’ 0.

(6)

for some  > 0.
Thus we detect the right direction X â†’ Y if all quantities
(a) |Hn (Î·Y,fn ) âˆ’ H (Î·Y,f )|, (b) |Hn (Î·X,gn ) âˆ’ H (Î·Y,g )|,
(c) |Hn (X) âˆ’ H(X)|, and (d) |Hn (Y ) âˆ’ H(Y )|, are at
most .
By assumption, (c) and (d) both tend to 0 in probability.
The quantities (a) and (b) are handled as follows. We only
show the argument for (a), as the argument for (b) is the
same. We have:
|Hn (Î·Y,fn ) âˆ’ H (Î·Y,f )| â‰¤ |Hn (Î·Y,fn ) âˆ’ H (Î·Y,fn )|
+ |H (Î·Y,fn ) âˆ’ H (Î·Y,f )| .
Now Hn (Î·Y,fn ) is consistent for fn fixed (it easy to check
that PÎ·Y,fn satisfies the necessary conditions provided fn is
bounded) and fn is learned on an independent sample from
P
Hn , we have |Hn (Î·Y,fn ) âˆ’ H (Î·Y,fn )| âˆ’
â†’ 0.

n

Entropy estimation: Given a sequence  = {i }i=1 , and a
bandwidth Ïƒ, define pn, as follows:


n
1 X
i âˆ’ t
pn, (t) =
K
,
nh i=1
Ïƒ


Z

 d
where
K(u) du = 1,  K(u) < âˆž,
du
R
and K(u) = 0 for |u| â‰¥ 1.
Let Y,i = Yi âˆ’ fn (Xi ) and X,i = Xi âˆ’ gn (Yi ). The
residual entropy estimators are defined as:
Hn (Î·Y,fn ) , H (pn,Y ) and Hn (Î·X,gn ) , H (pn,X ) .
(8)
5.2.2. C ONSISTENCY RESULT FOR
COUPLED - ESTIMATION
We abuse notation and use h and Ïƒ to denote the bandwidth
parameters used to estimate either fn and Hn (Î·Y,fn ), or gn
and Hn (Î·X,gn ). We make the distinction clear whenever
needed.
The consistency result depends on the following quantities
bounded in Lemma 5.
Definition 6 (Expected
average excess risk). Define
Pn
Rn (fn ) , E n1 i=1 |fn (Xi ) âˆ’ f (Xi )| and similarly
Pn
Rn (gn ) , E n1 i=1 |gn (Yi ) âˆ’ g(Yi )|.

Consistency of Causal Inference under the Additive Noise Model

We assume in this section that the noise Î· has exponentially
decreasing tail:

Pn
Then we have E n1 1 |fn (Xi ) âˆ’ f (Xi )| â‰¤ c2 nâˆ’Î² , for
Î² , min {(1 âˆ’ Î±)/2, Î±}.

Definition 7. A r.v. Z has exponentially decreasing
tail if there exists C, C 0 > 0 such that for all t >
0
0, P (|Z âˆ’ E Z| > t) â‰¤ Ceâˆ’C t .

The main theorem of this sectin is proved in the long version of this paper.

The following consistency theorem hinges on properly
choosing the bandwidths parameters h and Ïƒ. Essentially
we want to choose h such that regression estimation is consistent, and we want to choose Ïƒ so as not to overfit regression error. If the bandwidth Ïƒ is too small relative to
regression error (captured by Rn ), then the entropy estimator (for the residual entropy) is only fitting this error. The
conditions on Ïƒ in the Theorem are mainly to ensure that Ïƒ
is not too small relative to regression error Rn .
f,Î·

Theorem 2 (Coupled estimation). Suppose X âˆ’âˆ’â†’ Y for
some f, Î·, and suppose PX,Y satisfies Assumption 2, and
Î· has exponentially decreasing tail. Let fn , gn , and Hn be
defined as in Section 5.2.1, and let both Hn (X) and Hn (Y )
be consistent as in Assumption 1.
Suppose that :
(i) For learning fn and Hn (Î·Y,fn ), we use h = c1 nâˆ’Î± for
some c1 > 0 and 0 < Î± < 1, and Ïƒ = c2 nâˆ’Î² for some
c2 > 0 and 0 < Î² < min {(1 âˆ’ Î±)/4, Î±/2}.
(ii) For learning gn and Hn (Î·X,gn ), h satisfies h â†’ 0
and nh â†’ âˆž, and Ïƒ satisfies Ïƒ â†’ 0, nÏƒ â†’ âˆž, and
Ïƒ = â„¦(Rn (gn )âˆ’Î³ ) for some 0 < Î³ < 1/2.
Then the probability of correctly detecting X â†’ Y goes to
1 as n â†’ âˆž.
The theorem relies on Lemma 5 which bounds the errors
f,Î·
Rn for both fn and gn . Suppose X âˆ’âˆ’â†’ Y , then if f is
smooth or continuously differentiable, Rn (fn ) â†’ 0, and in
fact we can obtain finite rates of convergence for Rn (fn ),
thus yielding advice on setting Ïƒ. The second part of the
Lemma corresponds to this situation.
However, as mentioned earlier in the paper introduction, a
smooth f does not ensure that g(y) , g(X|y) is smooth or
even continuous, so we do not have rates for Rn (gn ). We
can nonetheless show that Rn (gn ) would generally converge to 0, which is sufficient for there to be proper settings
for Ïƒ (i.e. Ïƒ larger than the error, but also tending to 0).
We note that the r.v.â€™s X and Y are interchangeable in this
lemma since it does not assume X â†’ Y . The proof is
given in the extended version of the paper.
Lemma 5. Let fn be defined as in (7). Let f (x) , E [Y |x].
Suppose (i) E Y 2 < âˆž and that f is bounded; h â†’ 0 and
Pn
nâ†’0
nh â†’ âˆž. Then E n1 1 |fn (Xi ) âˆ’ f (Xi )| âˆ’âˆ’âˆ’â†’ 0.
Suppose further (ii) that PX has bounded support and that
f is continuously differentiable; h = c1 nâˆ’Î± for some c1 >
0 and 0 < Î± < 1.

6. Final Remarks
We derived the first consistency results for an existing family of procedures for causal inference under the Additive
Noise Model. We obtained mild algorithmic requirements,
and various distributional tail conditions which guarantee
consistency. The present work focuses on the case of two
r.v.s X and Y , which captures the inherent difficulties of
consistency. We believe however that the insights developed should extend to the case of random vectors under
corresponding tail conditions. The details however are left
for future work.
Another interesting multivariate situation is that of a causal
network of r.v.s. as in Peters et al. (2011b) dicussed earlier. Extending our consistency results to this particular
multivariate case would primarily consist of extending our
distributional tail conditions to the tails of distributions resulting from conditioning on appropriate sets of variables
in the network. This is however a non-trivial extension as it
involves, e.g. for the convergence of conditional entropies,
some additional integration steps that have to be carefully
worked out.
A possible future direction of investigation is to understand
under what conditions finite sample rates can be obtained
for such procedures. For reasons explained earlier, we do
not believe that this is possible without less general distributional assumptions.

References
Beirlant, Jan, Dudewicz, Edward J, GyoÌˆrfi, LaÌszloÌ, and
Van der Meulen, Edward C. Nonparametric entropy estimation: An overview. International Journal of Mathematical and Statistical Sciences, 6:17â€“40, 1997.
BuÌˆhlmann, P., Peters, J., and Ernest, J. Cam: Causal additive models, high-dimensional order search and penalized regression. arXiv:1310.1533, 2013.
Cover, Thomas M, Thomas, Joy A, and Kieffer, John. Elements of information theory. SIAM Review, 36(3):509â€“
510, 1994.
Gyorfi, L., Kohler, M., Krzyzak, A., and Walk, H. A
Distribution Free Theory of Nonparametric Regression.
Springer, New York, NY, 2002.
Hoyer, Patrik O, Janzing, Dominik, Mooij, JM, Peters,
Jonas, and SchoÌˆlkopf, Bernhard. Nonlinear causal dis-

Consistency of Causal Inference under the Additive Noise Model

covery with additive noise models. Proceedings of Advances in Neural Processing Information Systems, 2009.
HyvaÌˆrinen, Aapo, Shimizu, Shohei, and Hoyer, Patrik O.
Causal modelling combining instantaneous and lagged
effects: an identifiable model based on non-gaussianity.
In Proceedings of the 25th international conference on
Machine learning, pp. 424â€“431. ACM, 2008.
Nowzohour, Christopher and BuÌˆhlmann, Peter. Scorebased causal learning in additive noise models. arXiv
preprint arXiv:1311.6359, 2013.
Pearl, Judea. Causality: models, reasoning and inference,
volume 29. Cambridge Univ Press, 2000.
Peters, Jonas, Janzing, Dominik, and Scholkopf, Bernhard.
Causal inference on discrete data using additive noise
models. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 33(12):2436â€“2450, 2011a.
Peters, Jonas, Mooij, Joris, Janzing, Dominik, and
SchoÌˆlkopf, Bernhard. Identifiability of causal graphs using functional models. Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence, 2011b.
Shimizu, Shohei, Hoyer, Patrik O, HyvaÌˆrinen, Aapo, and
Kerminen, Antti. A linear non-gaussian acyclic model
for causal discovery. The Journal of Machine Learning
Research, 7:2003â€“2030, 2006.
Spirtes, Peter, Glymour, Clark N, and Scheines, Richard.
Causation Prediction & Search 2e, volume 81. MIT
press, 2000.
Tillman, Robert, Gretton, Arthur, and Spirtes, Peter. Nonlinear directed acyclic structure learning with weakly additive noise models. Proceedings of Advances in Neural
Processing Information Systems, 22:1847â€“1855, 2009.
Zhang, Kun and HyvaÌˆrinen, Aapo. On the identifiability
of the post-nonlinear causal model. In Proceedings of
the Twenty-Fifth Conference on Uncertainty in Artificial
Intelligence, pp. 647â€“655. AUAI Press, 2009.

