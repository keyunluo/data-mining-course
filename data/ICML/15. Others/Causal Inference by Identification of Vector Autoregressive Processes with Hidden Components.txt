Causal Inference by Identification of Vector Autoregressive Processes with
Hidden Components

Philipp Geigera
PGEIGER @ TUEBINGEN . MPG . DE
Kun Zhanga,b
KZHANG @ TUEBINGEN . MPG . DE
Mingming Gongc
GONGMINGNJU @ GMAIL . COM
Dominik Janzinga
JANZING @ TUEBINGEN . MPG . DE
Bernhard Schölkopfa
BS @ TUEBINGEN . MPG . DE
a
Empirical Inference Department, Max Planck Institute for Intelligent Systems, Tübingen, Germany
b
Information Sciences Institute, University of Southern California, USA
c
Centre for Quantum Computation and Intelligent Systems, University of Technology, Sydney, Australia

Abstract
A widely applied approach to causal inference
from a time series X, often referred to as
“(linear) Granger causal analysis”, is to simply
regress present on past and interpret the regression matrix B̂ causally. However, if there is an
unmeasured time series Z that influences X, then
this approach can lead to wrong causal conclusions, i.e., distinct from those one would draw if
one had additional information such as Z. In this
paper we take a different approach: We assume
that X together with some hidden Z forms a first
order vector autoregressive (VAR) process with
transition matrix A, and argue why it is more
valid to interpret A causally instead of B̂. Then
we examine under which conditions the most important parts of A are identifiable or almost identifiable from only X. Essentially, sufficient conditions are (1) non-Gaussian, independent noise
or (2) no influence from X to Z. We present two
estimation algorithms that are tailored towards
conditions (1) and (2), respectively, and evaluate them on synthetic and real-world data. We
discuss how to check the model using X.

1. Introduction
Inferring the causal structure of a stochastic dynamical
system from a time series of measurements is an important problem in many fields such as economics (Lütkepohl,
2006) and neuroscience (Roebroeck et al., 2005; Besserve
et al., 2010).
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

In the present paper, we approach this problem as follows:
We assume that the measurements are a finite sample from
a random process X = (Xt )t∈Z which, together with another random process Z = (Zt )t∈Z , forms a first order vector autoregressive (VAR) process. That is, (X, Z)> obeys

 


Xt
B C
Xt−1
=
+ Nt ,
Zt
D E
Zt−1
for all t ∈ Z, some matrices B, C, D, E, and some i.i.d.
Ni , i ∈ Z. So far this is a purely statistical model. Now
we additionally assume that the variables in Z correspond
to real properties of the underlying system that are in principle measurable and intervenable. Based on this we consider B, C, D, E to have a causal meaning. More precisely,
we assume that B’s entries express the direct causal influences between the respective variables in X. And more
generally, we assume that for all variables in (X, Z)> the
matrices B, C, D, E capture the respective direct and indirect causal influences. Note that in this sense C is particularly interesting because it tells which components of X
are jointly influenced by an unmeasured quantity, i.e., have
a hidden confounder, and how strong the influence is.
This way causal inference on X is reduced to a statistical
problem: examining to what extent, i.e., under which assumptions, B as well as C, D, E are identifiable from the
distribution of the process X, and how they can be estimated from a sample of X. It is worth mentioning that this
approach can be justified in two different ways, following
either (Granger, 1969) or (Pearl, 2000; Spirtes et al., 2000).
We will briefly elaborate on this later (Section 4.2).
The first and main contribution of this paper is on the theoretical side: we present several results that show under
which conditions B and C are identifiable or almost (i.e.
up to a small number of possibilities) identifiable from only
the distribution of X. Generally we assume that Z has at

Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components

most as many components as X. Theorem 1 shows that
if the noise terms are non-Gaussian and independent, and
an additional genericity assumption holds true, then B is
uniquely identifiable. Theorem 2 states that under the same
assumption, those columns of C that have at least two nonzero entries are identifiable up to scaling and permutation
indeterminacies (because scale and ordering of the components of Z are arbitrary). Theorem 3 shows that regardless
of the noise distribution (i.e., also in the case of Gaussian
noise), if there is no influence from X to Z and an additional genericity assumption holds, then B is identifiable
from the covariance structure of X up to a small finite number of possibilities. In Propositions 1 and 2 we prove that
the additional assumptions we just called generic do in fact
only exclude a Lebesgue null set from the parameter space.
The second contribution is a first examination of how the
above identifiability results can be translated into estimation algorithms on finite samples of X. We propose two
algorithms. Algorithm 1, which is tailored towards the conditions of Theorems 1 and 2, estimates B and C by approximately maximizing the likelihood of a parametric VAR
model with a mixture of Gaussians as noise distribution.
Algorithm 2, which is tailored towards the conditions of
Theorem 3, estimates the matrix B up to finitely many possibilities by solving a system of equations somewhat similar to the Yule-Walker equations (Lütkepohl, 2006). Furthermore, we briefly examine how the model assumptions
that we make can to some extend be checked just based on
the observed sample of X. We examine the behavior of the
two proposed algorithms on synthetic and real-world data.
It should be mentioned that probably the most widely applied approach to causal inference from time series data
so far (Lütkepohl, 2006), which we refer to as practical
Granger causal analysis in this paper (often just called
“(linear) Granger causality”), is to simply perform a linear regression of present on past on the observed sample of
X and then interpret the regression matrix causally. While
this method may yield reasonable results in certain cases, it
obviously can go wrong in others (see Section 4.3 for details). We believe that the approach presented in this paper
may in certain cases lead to more valid causal conclusions.
The remainder of this paper is organized as follows. In
Section 2 we discuss related work. In Section 3 we introduce notation and definitions for time series. In Section 4 we state the statistical and causal model that we
assume throughout the paper. In Section 5 we introduce
the so-called generalized residual. Section 6 contains the
three main results on identifiability (Theorems 1 to 3) as
well as arguments for the genericity of certain assumptions
we need to make (Propositions 1 and 2). In Section 7 we
present the two estimation algorithms and discuss model
checking. Section 8 contains experiments for Algorithms 1
and 2. We conclude with Section 9.

2. Related Work
We briefly discuss how the present work is related to previous papers in similar directions.
Inference of properties of processes with hidden components: The work (Jalali & Sanghavi, 2012) also assumes
a VAR model with hidden components and tries to identify parts of the transition matrix. However their results
are based on different assumptions: they assume a “localglobal structure”, i.e., connections between observed components are sparse and each latent series interacts with
many observed components, to achieve identifiability. The
authors of (Boyen et al., 1999) - similar to us - apply a
method based on expectation maximization (EM) to infer
properties of partially observed Markov processes. Unlike
us, they consider finite-state Markov processes and do not
provide a theoretical analysis of conditions for identifiability. The paper (Etesami et al., 2012) examines identifiability of partially observed processes that have a certain
tree-structure, using so-called discrepancy measures.
Harnessing non-Gaussian noise for causal inference:
The paper (Hyvaerinen et al., 2010) uses non-Gaussian
noise to infer instantaneous effects. In (Hoyer et al., 2008),
the authors use the theory underlying overcomplete independent component analysis (ICA) (Kagan et al., 1973,
Theorem 10.3.1) to derive identifiability (up to finitely
many possibilities) of linear models with hidden variables,
which is somewhat similar to our Theorem 1. However,
there are two major differences: First, they only consider
models which consist of finitely many observables which
are mixtures of finitely many noise variables. Therefore
their results are not directly applicable to VAR models.
Second, they show identifiability only up to a finite number of possibilities, while we (exploiting the autoregressive
structure) prove unique identifiability.
Integrating several definitions of causation: The work
(Eichler, 2012) provides an overview over various definitions of causation w.r.t. time series, somewhat similar to
but more comprehensive than our brief discussion in Sections 4.2 and 4.3.

3. Time Series: Notation and Definitions
Here we introduce notation and definitions w.r.t. time series. We denote multivariate time series, i.e., families of
random vectors over the index set Z, by upper case letters
such as X. As usual, Xt denotes the t-th member of X,
and Xtk denotes the k-th component of the random vector
Xt . Slightly overloading terminology, we call the univariate time series X k = (Xtk )t∈Z the k-th component of X.
By PX we denote the distribution of the random process
X, i.e., the joint distribution of all Xt , t ∈ Z.
Given a KX -variate time series X and a KZ -variate time

Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components

series Z, (X, Z)> denotes the (KX + KZ )-variate series


(Xt1 , . . . , XtKX , Zt1 , . . . , ZtKZ )>
.
t∈Z

A K-variate time series W is a vector autoregressive process (of order 1), or VAR process for short, with VAR transition matrix A and noise covariance matrix Σ, if it allows
a VAR representation, i.e.,
Wt = AWt−1 + Nt ,

(1)

the absolute value of all eigenvalues of A is less than1 1,
and N is an i.i.d. noise time series such that Cov(N0 ) =
Σ. We say W is a diagonal-structural VAR process if in
the above definition the additional condition is met that
N01 , . . . , N0K are jointly independent.2

4. Statistical and Causal Model Assumptions
In this section we introduce the statistical model that we
consider throughout the paper and discuss based on which
assumptions its parameters can be interpreted causally.
Moreover, we give an example for how practical Granger
causal analysis can go wrong.
4.1. Statistical Model
Let KX be arbitrary but fixed. Let X be a KX -variate time
series. As stated in Section 1, X is the random process
from which we assume we measured a sample. In particular, the random variables in X have a meaning in reality
(e.g., X31 is the temperature measured in room 1 at time 3)
and we are interested in the causal relations between these
variables. Let X be related to a K-variate VAR process W ,
with transition matrix A, noise time series N , and noise
covariance matrix Σ, and a KZ -variate time series Z, as
follows: W = (X, Z)> and KZ ≤ KX . Furthermore, let


B C
A =:
,
(2)
D E
with B a KX × KX matrix. We call B, the most interesting part of A, the structural matrix underlying X. Furthermore, in case C 6= 0, we call Z a hidden confounder.
4.2. Causal Assumptions
As already mentioned in Section 1, throughout this paper
we assume that there is an underlying system such that all
variables in W correspond to actual properties of that system which are in principle measurable and intervenable.
While we assume that a finite part of X was in fact measured (Section 4.1), Z is completely unmeasured. Further1

We require all VAR processes to be stable (Lütkepohl, 2006).
Note that the notion “diagonal-structural” is a special case of
the more general notion of “structural” in e.g., (Lütkepohl, 2006).
2

more we assume that the entries of A, in particular the submatrix B, capture the actual non-instantaneous causal influences between the variables in W . We also mentioned
that there are two lines of thought that justify this assumption. We briefly elaborate on this here.
On the one hand, (Granger, 1969) proposed a definition of
causation between observables which we will refer to as
Granger’s ideal definition. Assume the statistical model
for the observed sample of X specified in Section 4.1. If
we additionally assume that Z correctly models the whole
rest of the universe or the “relevant” subpart of it, then according to Granger’s ideal definition the non-instantaneous
(direct) causal influences between the components of X are
precisely given by the entries of B. But this implies that
everything about B that we can infer from X can be interpreted causally, if one accepts Granger’s ideal definition
and the additional assumptions that are necessary (such as
KZ ≤ KX , which in fact may be a quite strong assumption
of course). This is one way to justify our approach.
On the other hand, (Pearl, 2000) does not define causation
based on measurables alone but instead formalizes causation by so-called structural equation models (SEMs) and
links them to observable distributions via additional assumptions. In this sense, let us assume that W forms a
causally sufficient set of variables, whose correct structural
equations are given by the VAR equations (1), i.e., these
equations represent actual causal influences from the r.h.s.
to the l.h.s.3 In particular these equations induce the correct (temporal) causal directed acyclic graph (DAG) for
(X, Z)> . Then, essentially following the above mentioned
author, everything about B that we can infer from the distribution of X can be interpreted causally. This is the
other way to justify our approach (in case the requirement
KZ ≤ KX and the other assumptions are met). It is important to mention that the usual interpretation of SEMs is that
they model the mechanisms which generate the data and
that they predict the outcomes of randomized experiments
w.r.t. the variables contained in the equations.
4.3. Relation to Practical Granger Causal Analysis and
How It Can Go Wrong
The above ideal definition of causation by Granger (Section 4.2) needs to be contrasted with what we introduced as
“practical Granger causal analysis” in Section 1. In practical Granger causal analysis, one just performs a linear regression of present on past on the observed X and then interprets the regression matrix causally.4 While making the
3

Note that here we ignore the fact that Pearl generally only
considers models with finitely many variables while the process
W is a family of infinitely many (real-valued) variables.
4
We are aware that nonlinear models (Chu & Glymour, 2008)
and nonparametric estimators (Schreiber, 2000) have been used
to find temporal causal relations. In this paper we focus on the

Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components

ideal definition practically feasible, this may lead to wrong
causal conclusions in the sense that it does not comply with
the causal structure that we would infer given we had more
information.5
Let us give an example for this. Let X be bivariate and Z
be univariate. Moreover, assume


0.5
0.9 0
0.8  ,
A =  0.1 0.1
0
0
0.9
and let the covariance matrix of Nt be the identity matrix.
To perform practical Granger causal analysis, we proceed
as usual: we fit a VAR model on only X, in particular compute, w.l.o.g. assuming zero mean, the transition matrix by


0.89 0.35
>
BpG := E(Xt Xt−1
)E(Xt Xt> )−1 =
0.08 0.65
(3)
(up to rounding) and interpret the coefficients of BpG as
causal influences. Although, based on A, Xt2 does in fact
1
not cause Xt+1
, BpG suggests that there is a strong causal
1
2
with the strength 0.35. It is even
effect Xt → Xt+1
2
stronger than the relation Xt1 → Xt+1
, which actually exists in the complete model with the strength 0.1.

5. The Generalized Residual: Definition and
Properties
In this section we define the generalized residual and discuss some of its properties. The generalized residual is
used in the proofs of the three main results of this paper,
Theorems 1 to 3.
For any KX × KX matrices U1 , U2 let
Rt (U1 , U2 ) := Xt − U1 Xt−1 − U2 Xt−2 .
We call this family of random vectors generalized residual.
Furthermore let


>
M1 := E Wt · (Xt> , Xt−1
) .
In what follows, we list some simple properties of the generalized residual. Proofs can be found in (Geiger et al.,
2015, Section A).
Lemma 1. We have
Rt (U1 , U2 ) = (B 2 + CD − U1 B − U2 )Xt−2

Lemma 2. If (U1 , U2 ) satisfies the equation



B C
(U1 , U2 )
= B 2 + CD, BC + CE , (5)
I 0
then Rt (U1 , U2 ) is independent of (Xt−2−j )∞
j=0 , and in
particular, for j ≥ 0,
Cov(Rt (U1 , U2 ), Xt−2−j ) = 0.

(6)

X
Let ΓX
i := Cov(Xt , Xt−i ) for all i. That is, Γi are the
autocovariance matrices of X. Note that equation (6), for
j = 0, 1, can equivalently be written as the single equation
 X


Γ1 ΓX
X
2
(U1 , U2 )
= ΓX
(7)
X
2 , Γ3 .
ΓX
Γ
0
1

Keep in mind that, as usual, we say a m × n matrix has full
rank if its (row and column) rank equals min{m, n}.
Lemma 3. Let M1 have full rank. If (U1 , U2 ) satisfies
equation (6) for j = 0, 1, then it satisfies equation (5).
Lemma 4. If K = KX or if C has full rank, then there
exists (U1 , U2 ) that satisfies equation (5).

6. Theorems on Identifiability and Almost
Identifiability
This section contains the main results of the present paper. We present three theorems on identifiability and almost identifiability of B and C (defined in Section 4.1), respectively, given X and briefly argue why certain assumptions we have to make can be considered as generic. Recall the definition of the matrix M1 in Section 5. Note that
the following results show (almost) identifiability of B for
all numbers KZ of hidden components simultaneously, as
long as 0 ≤ KZ ≤ KX (which contains the case of no
hidden components as a special case).
6.1. Assuming Non-Gaussian, Independent Noise
We will need the following assumptions for the theorems.
Assumptions. We define the following abbreviations for
the respective subsequent assumptions.
A1: All noise terms Ntk , k = 1, . . . , K, t ∈ Z, are nonGaussian.

+ (BC + CE − U1 C)Zt−2
X
Z
+ (B − U1 )Nt−1
+ CNt−1
+ NtX ,

if K > KX . In case K = KX , the same equation holds
except that one sets C := D := E := 0.

(4)

linear case.
5
Obviously, if one is willing to assume that X is causally sufficient already, then the practical Granger causation can be justified
along the lines of Section 4.2.

A2: W is a diagonal-structural VAR process (as defined in
Section 3).
G1: C (if it is defined, i.e., if K > KX ) and M1 have full
rank.

Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components

(We will discuss the genericity of G1 in Section 6.3.)

We will need the following assumptions for the theorem.

The following definition of F1 is not necessary for an intuitive understanding, but is needed for a precise formulation of the subsequent identifiability statements. Let F1
denote the set of all K 0 -variate VAR processes W 0 with
KX ≤ K 0 ≤ 2KX (i.e. W has at most as many hidden components as observed ones), which satisfy the following properties w.r.t. N 0 , C 0 , M10 (defined similarly to
N, C, M1 in Section 4): assumptions A1, A2 and G1 applied to N 0 , C 0 , M10 (instead of N, C, M1 ) hold true.

Assumptions. We define the following abbreviations for
the respective subsequent assumptions.

Theorem 1. If assumptions A1, A2 and G1 hold true, then
B is uniquely identifiable from only PX .
That is: There is a map f such that for each W 0 ∈ F1 , and
X 0 defined as the first KX components of W 0 , f (PX 0 ) =
B 0 iff B 0 is the structural matrix underlying X 0 .
A detailed proof can be found in (Geiger et al., 2015, Section B.1). The idea is to chose U1 , U2 such that Rt (U1 , U2 )
is a linear mixture of only finitely many noise terms, which
is possible based on Lemmas 1 to 4. Then, using the identifiability result underlying overcomplete ICA (Kagan et al.,
1973, Theorem 10.3.1), the structure of the mixing matrix
of (Rt (U1 , U2 ), Rt−1 (U1 , U2 ))> allows to uniquely determine B from it.
Again using (Kagan et al., 1973, Theorem 10.3.1), one can
also show the following result. For a matrix M let S(M )
denote the set of those columns of M that have at least two
non-zero entries, and if M is not defined, let S(M ) denote
the empty set. A proof can be found in (Geiger et al., 2015,
Section B.2).
Theorem 2. If assumptions A1, A2 and G1 hold true, then
the set of columns of C with at least two non-zero entries is
identifiable from only PX up to scaling of those columns.
In other words: There is a map f such that for each
W 0 ∈ F1 with K 0 components, X 0 defined as the first
KX components of W 0 , and C 0 defined as the upper right
KX ×(K 0 −KX ) submatrix of the transition matrix of W 0 ,
f (PX 0 ) coincides with S(C 0 ) up to scaling of its elements.
6.2. Assuming D = 0
In this section we present a theorem on the almost identifiability of B under different assumptions. In particular, we
drop the non-Gaussianity assumption. Instead, we make
the assumption that Z is not influenced by X, i.e., D = 0.
Given U = (U1 , U2 ), let
TU (Q) := Q2 − U1 Q − U2 ,

(8)

for all square matrices Q that have the same dimension
as U1 . Slightly overloading notation, we let TU (α) :=
TU (αI) for all scalars α. Note that det(TU (α)) is a univariate polynomial in α.

A3: D = 0.
G2: The transition matrix A is such that there exists
U = (U1 , U2 ) such that equation (5) is satisfied and
det(TU (α)) has 2KX distinct roots.
(We will discuss the genericity of G2 in Section 6.3.)
The following definition of F2 is not necessary for an
intuitive understanding, but is needed for a precise formulation of the subsequent identifiability statement. Let
F2 denote the set of all K 0 -variate VAR processes W 0
with KX ≤ K 0 ≤ 2KX , which satisfy the following
properties w.r.t. N 0 , A0 , C 0 , D0 , M10 (defined similarly to
N, A, C, D, M1 in Section 4): assumptions A3, G1 and G2
applied to N 0 , A0 , C 0 , D0 , M10 (instead of N, A, C, D, M1 )
hold true.
Theorem 3. If assumptions A3, G1 and G2 hold true, then
B is identifiable
from only the covariance structure of X

X
up to 2K
possibilities.
KX
In other words: There is a map f such that for each
W 0 ∈ F2 , and X 0 defined as the first
 KX components of
X
W 0 , f (X 0 ) is a set of at most 2K
many matrices, and
KX
B 0 ∈ f (PX 0 ) for B 0 the structural matrix underlying X 0 .
A detailed proof can be found in (Geiger et al., 2015, Section B.3). The proof idea is the following: Let L denote the
set of all (U, B̃), with U = (U1 , U2 ), that satisfy equation
(6) for j = 0, 1, as well as the equation
TU (B̃) = 0,

(9)

and meet the condition that det(TU (α)) has 2KX distinct
roots. L is non-empty and (U, B) is an element of it, for
the true B and some U , due to Lemmas 2 to 4. But L is
only defined
based on the covariance of X and has at most

2KX
elements
(based on (J. E. Dennis et al., 1976)).
KX
Note the similarity between equation (6), or its equivalent, equation (7), and the well-known Yule-Walker equation (Lütkepohl, 2006). The Yule-Walker equation (which
is implicitly used in equation (3)) determines B uniquely
under some genericity assumption and given C = 0.
6.3. Discussion on the Genericity of Assumptions G1
and G2
In this section we want to briefly argue why the assumptions G1 and G2 are generic. A detailed elaboration with
precise definitions and proofs can be found in (Geiger
et al., 2015, Section C). The idea is to define a natural parametrization of (A, Σ) and to show that the restrictions that assumptions G1 and G2, respectively, impose on

Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components

(A, Σ) just exclude a Lebesgue null set in the natural parameter space and thus can be considered as generic.
In this section, let K such that KX ≤ K ≤ 2KX be arbitrary but fixed. Let λk denote the k-dimensional Lebesgue
measure on Rk .
Let Θ1 denote the set of all possible parameters (A0 , Σ0 )
for a K-variate VAR processes W 0 that additionally satisfy
assumption A2, i.e., correspond to structural W 0 . Let S1
denote the subset of those (A0 , Σ0 ) ∈ Θ1 for which also
assumption G1 is satisfied. And let g denote the natural
parametrization of Θ1 which is defined in (Geiger et al.,
2015, Section C.1).

Proposition 1. We have λK 2 +K g −1 (Θ1 \ S1 ) = 0.
A proof can be found in (Geiger et al., 2015, Section C.1).
The proof idea is that g −1 (Θ1 \ S1 ) is essentially contained
in the union of the root sets of finitely many multivariate
polynomials and hence is a Lebesgue null set.
Let Θ2 denote the set of all possible parameters (A0 , Σ0 )
for the K-variate VAR processes W that additionally satisfy assumption A3, i.e., are such that the submatrix D of
A is zero. Let S2 denote the subset of those (A0 , Σ0 ) ∈ Θ2
for which also assumptions G1 and G2 are satisfied. Let h
denote the natural parametrization of Θ1 which is defined
in (Geiger et al., 2015, Section C.2). A proof for the following proposition (which is based on a similar idea as that
of Proposition 1) can also be found in (Geiger et al., 2015,
Section C.2).

Proposition 2. We have λ2K 2 −KX KZ h−1 (Θ2 \ S2 ) =
0.

7. Estimation Algorithms
In this section we examine how the identifiability results in
Section 6 can be translated into estimators on finite data.
We propose two algorithms.
7.1. Algorithm Based on Variational EM
Here we present an algorithm for estimating B and C
which is closely related to Theorems 1 and 2. Keep in mind
that the latter theorem in fact only states identifiability for
S(C) (defined in Section 6.2), up to scaling, not for the exact C. The idea is the following: We transform the model of
X underlying these theorems (i.e. the general model from
Section 4.1 together with assumptions A1, A2 and G1 from
Section 6.1) into a parametric model by assuming the noise
terms Ntk to be mixtures of Gaussians.6 Then we estimate all parameters, including B and C, by approximately
6

Obviously, Theorems 1 and 2 also imply identifiability of B
and (up to scaling) S(C) for this parametric model. We conjecture that this implies consistency of the (non-approximate) maximum likelihood estimator for that model under appropriate assumptions.

Algorithm 1 Estimate B, C using variational EM
1: Input: Sample x1:L of X1:L .
2: Initialize the transition matrix and the parameters of
the Gaussian mixture model, denoted as θ0 , set j ← 0.
3: repeat
4:
E step: Evaluate
X
Z
X
Z
q j (z1:L , v1:L
, v1:L
) = q j (z1:L )q j (v1:L
)q j (v1:L
),

5:
6:
7:
8:

which is the variational approx. to the true posterior
X
Z
q j (z1:L , v1:L
, v1:L
|x1:L ), by maximizing the variational lower bound, i.e., q j = arg maxq L(q, θj ).
M step: Evaluate θj+1 = arg maxθ L(q j , θ).
j ← j + 1.
until convergence
Output: The final θj , containing the estimated B, C.

Algorithm 2 Estimate B using covariance structure
1: Input: Sample x1:L of X1:L .
X
2: Solve the equation (7), with ΓX
i replaced by Γ̂i . Let
(Û1 , Û2 ) denote the solution.
3: Solve equation (9) with U := (Û1 , Û2 ) for B̃. Let
B̂1 , . . . , B̂n denote the solvents.
4: Output: B̂1 , . . . , B̂n .

maximizing the likelihood of the given sample of X using a variational expectation maximization (EM) approach
similar to the one in (Oh et al., 2005). (Directly maximizing the likelihood is intractable due to the hidden variables
(Z and mixture components) that have to be marginalized
out.) Let y1:L be shorthand for (y1 , . . . , yL ). The estimator is outlined by Algorithm 1, where (VtX , VtZ ) with
values (vtX , vtZ ) denote the vectors of mixture components
Z
X
|x1:L ) the
, v1:L
for NtX and NtZ , respectively; q j (z1:L , v1:L
Z
X
true posterior of Z1:L , V1:L , V1:L under the respective parameter vector θj (which comprises A, Σ as well as the
Gaussian mixture parameters) at step j; and L the variational lower bound. The detailed algorithm can be found in
(Geiger et al., 2015, Section D). Note that, if needed, one
may use cross validation as a heuristic to determine KZ
and the number of Gaussian mixture components.
7.2. Algorithm Based on the Covariance Structure
Now we present an algorithm, closely related to Theorem 3,
for estimating B up to finitely many possibilities. It relies
on the proof idea of that theorem, as we outlined it at the
end of Section 6.2, and it is meant to be applied for cases
where the conditions of that theorem are met. It uses only
the estimated autocovariance structure of X. Keep in mind
that Γ̂X
i denote the sample autocovariance matrices (similar
to the true autocovariances ΓX
i defined in Section 5). The
estimation algorithm is given by Algorithm 2.

Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components

7.3. Model Checking

For instance, to check (to a limited extent) the assumptions
underlying Theorems 1 and 2 and Algorithm 1, i.e., the
general statistical and causal model assumptions from Sections 4.1 and 4.2 together with A1, A2 and G1 from Section
6.1, we propose the following two tests: First, test whether
Rt (Û1 , Û2 ) is independent of (Xt−2−j )Jj=0 , for (Û1 , Û2 ) as
defined in Algorithm 2, and for say J = 2. (If Algorithm
2 finds no (Û1 , Û2 ) then the test is already failed.) Second,
check whether all components of Xt are non-Gaussian using e.g. the Kolmogorov-Smirnov test (Conover, 1971) for
Gaussianity.
Note that under the mentioned assumptions, both properties of X do in fact hold true. Regarding the independence
statement, this follows from Lemmas 4 and 2. W.r.t. the
non-Gaussianity statement, this follows from the fact (Ramachandran, 1967, Theorem 7.8) that the distribution of an
infinite weighted sum of non-Gaussian random variables is
again non-Gaussian. It should be mentioned that the first
test can also be used to check (to a limited extent) the assumptions underlying Theorem 3 and Algorithm 2.

8. Experiments
In this section we evaluate the two algorithms proposed in
Section 7 on synthetic and real-world data and compare
them to the practical Granger causation estimator. Keep in
mind that the latter is defined by replacing the covariances
in equation (3) by sample covariances.
8.1. Synthetic Data
We empirically study the behavior of Algorithms 1 and 2 on
simulated data, in dependence on the sample length. Note
that, based on theoretical considerations (see Section 4.3),
it can be expected that the error of the practical Granger
estimator is substantially bounded away from zero in the
generic case.
8.1.1. A LGORITHM 1
Here we evaluate Algorithm 1.
Experimental setup: We consider the case of a 2-variate
X and a 1-variate Z, i.e., KX = 2, KZ = 1. We consider sample lengths L = 100, 500, 1000, 5000 and for
each sample length we do 20 runs. In each run we draw the
matrix A uniformly at random from the stable matrices and

RMSE

Ideally we would like to know whether the various model
assumptions we make in this paper, most importantly the
one that the entries of B can in fact be interpreted causally,
are appropriate. Obviously, this is impossible to answer
just based on the observed sample of X. Nonetheless
one can check these assumptions to the extent they imply
testable properties of X.

Algorithm 1
Practical Granger

0.1

5 · 10−2

0

0

1,000

2,000

3,000

4,000

5,000

L

Figure 1. RMSE of Algorithm 1 and the practical Granger estimator as a function of sample length L.

then randomly draw a sample of length L from a VAR process W = (X, Z)> with A as transition matrix and noise
Ntk distributed according to a super-Gaussian mixtures of
Gaussians. Then we apply Algorithm 1 and the practical
Granger causation estimator on the sample of only X.
Outcome: We calculated the root-mean-square
error
P20
1
true 2
est
(RMSE) of Algorithm 1, i.e., 20
n=1 (Bn − Bn ) ,
where Bnest , Bntrue denotes the output of Algorithm 1 and the
true B, respectively, for each run n. The RMSE as a function of the sample length L is depicted in Figure 1, along
with the RMSE of the practical Granger algorithm.
Discussion: This suggests that for L → ∞ the error of
Algorithm 1 is negligible, although it may not converge
to zero. The error of the practical Granger estimator for
L → ∞ is still small but substantially bigger than that of
Algorithm 1.
8.1.2. A LGORITHM 2
Here we empirically establish the error of Algorithm 2,
more precisely the deviation between the true B and the
best out of the several estimates that Algorithm 2 outputs.
Obviously in general it is unknown which of the outputs of
Algorithm 2 is the best estimate. However here we rather
want to establish that asymptotically, the output of Algorithm 2 in fact contains the true B. Also we compare Algorithm 2 to the practical Granger estimator, although it needs
to be said, that the latter is usually not applied to univariate
time series.
Experimental setup: We consider the case of 1-variate X
and Z, i.e., KX = KZ = 1. We consider sample lengths
L = 101 , 102 , . . . , 107 and for each sample length we do
20 runs. In each run we draw the matrix A uniformly at
random from the stable matrices with the constraint that the
lower left entry is zero and then randomly draw a sample
of length L from a VAR process W = (X, Z)> with A as
transition matrix and standard normally distributed noise
N . Then we apply Algorithm 2 and the practical Granger

Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components

0.4

RMSE

to work reliably.

Algorithm 2
Practical Granger

Outcome: The outputs are:


AfG

0.2

BpG
0
101

102

103

104
L

105

106

107

ĀfA

Figure 2. RMSE of Algorithm 2 and the practical Granger estimator as a function of sample length L.

ÃpA


0.8381 0.0810 0.0375
=  0.0184 0.9592 −0.0473  ,
0.2318 0.0522 0.7446


0.8707 0.0837
=
,
−0.0227 0.9559

0.8809 0.1812 0.1016 −0.1595
 0.0221 1.0142 −0.0290 −0.0492
=
 0.2296 0.1291 0.8172 −0.1143
1.0761 0.6029 −0.7184 0.4226


0.9166 0.0513 −0.0067
=  −0.0094 0.9828 −0.0047  .
−0.0031 0.1441 −0.2365



,


causation estimator on the sample of only X.
Outcome: We calculated the Proot-mean-square error
20
1
best est
− Bntrue )2 ,
(RMSE) of Algorithm 2, i.e., 20
n=1 (Bn
best est
true
where Bn
, Bn denotes the best estimate for B returned by Algorithm 2 (i.e., the one out of the two outputs
that minimizes the RMSE) and true B for each run n, respectively. The RMSE as a function of the sample length L
is depicted in Figure 2, along with the RMSE of the practical Granger estimator.
Discussion: This empirically shows that the set of two outputs of Algorithm 2 asymptotically seem to contain the true
B. However, it takes at least 1000 samples to output reasonable estimates. As expected, the practical Granger estimator does not seem to converge against the true B.
8.2. Real-World Data
Here we examine how Algorithm 1 performs on a realworld data set.
Experimental setup: We consider a time series Y of
length 340 and the three components: cheese price Y 1 ,
butter price Y 2 , milk price Y 3 (recorded monthly from
January 1986 to April 2014, http://future.aae.
wisc.edu/tab/prices.html). We used the following estimators: We applied practical Granger estimation to
the full time series Y (i.e., considering X = Y ) and denote
the outcome by AfG . We applied practical Granger estimation to the reduced time series (Y 1 , Y 2 )> (i.e., considering
X = (Y 1 , Y 2 )> ) and denote the outcome by BpG . We applied Algorithm 1 to the full time series Y (i.e., considering
X = Y ), while assuming an additional hidden univariate
Z, and denote the outcome by ĀfA . We applied Algorithm
1 to the reduced time series (Y 1 , Y 2 )> (i.e., considering
X = (Y 1 , Y 2 )> ), while assuming an additional hidden
univariate Z, and denote the outcome by ÃpA . Furthermore
we do a model check as suggested in Section 7.3, although
the sample size may be too small for the independence test

The outcome of the model check, based on a significance
level of 5%, is the following: the hypothesis of Gaussianity is rejected. Also the independence hypothesis stated in
Section 7.3 is rejected. The latter implies that the model
assumptions underlying Algorithm 1 are probably wrong.
Discussion: We consider AfG as ground truth. Intuitively,
non-zero entries at positions (i, 3) can be explained by the
milk price influencing cheese/butter prices via production
costs, while non-zero entries at positions (3, j) can be explained by cheese/butter prices driving the milk price via
demand for milk. The explanation of non-zero entries at
positions (1, 2) an (2, 1) is less clear. One can see that the
upper left 2 × 2 submatrix of ÃpA is quite close to that of
AfG (the RMSE over all entries is 0.0753), which shows
that Algorithm 1 works well in this respect. Note that BpG
is even a bit closer (the RMSE is 0.0662). However, the
upper right 2 × 1 matrix of ÃpA is not close to a scaled
version of the upper right 2 × 1 submatrix of AfG (which
corresponds to C). This is in contrast to what one could expect based on Theorem 2. ĀfA can be seen as an alternative
ground truth. It is important to mention that the estimated
order (lag length) of the full time series Y is 3, according to
Schwarz’s criterion (SC) (Lütkepohl, 2006), which would
violate our assumption of a VAR process of order 1 (Section 4.1). The model check seems to detect this violation
of the model assumptions.

9. Conclusions
We considered the problem of causal inference from observational time series data. Our approach consisted of two
parts: First, we examined possible conditions for identifiability of causal properties of the underlying system from
the given data. Second, we proposed two estimation algorithms and showed that they work on simulated data under
the respective conditions from the first part.

Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components

Acknowledgements
Kun Zhang was supported in part by DARPA grant No.
W911NF-12-1-0034.

References

J. E. Dennis, Jr., Traub, J. F., and Weber, R. P. The algebraic theory of matrix polynomials. SIAM Journal on
Numerical Analysis, 13(6):831–845, 1976.
Jalali, A. and Sanghavi, S. Learning the dependence graph
of time series with latent factors. In Proceedings of
the 29th International Conference on Machine Learning,
Edinburgh, Scotland, UK, 2012.

Besserve, M., Schölkopf, B., Logothetis, N. K., and Panzeri, S. Causal relationships between frequency bands
of extracellular signals in visual cortex revealed by an
information theoretic analysis. Journal of Computational Neuroscience, 29(3):547–566, 2010. doi: 10.
1007/s10827-010-0236-5. URL http://dx.doi.
org/10.1007/s10827-010-0236-5.

Kagan, A. M., Linnik, Y. V., and Rao, C. R. Characterization Problems in Mathematical Statistics. Wiley, New
York, 1973.

Boyen, X., Friedman, N., and Koller, D. Discovering the
hidden structure of complex dynamic systems. In Proceedings of the 15 th Conference on Uncertainty in Artificial Intelligence, pp. 91–100. Morgan Kaufmann, San
Francisco, 1999.

Oh, S. M., Ranganathan, A., Rehg, J. M., and Dellaert,
F. A variational inference method for switching linear
dynamic systems. Technical report, 2005.

Chu, T. and Glymour, C. Search for additive nonlinear time
series causal models. Journal of Machine Learning Research, 9:967–991, 2008.

Ramachandran, B. Advanced theory of characteristic functions. Series in probability and statistics. Statistical Pub.
Society, 1967.

Conover, W.J. Practical Nonparametric Statistics. John
Wiley & Sons, 1971.

Roebroeck, A., Formisano, E., and Goebel, R. Mapping
directed influence over the brain using Granger causality
and fMRI. Neuroimage, 25:230–242, 2005.

Eichler, M. Causal inference in time series analysis. In
Berzuini, C., Dawid, A.P., and Bernardinelli, L. (eds.),
Causality, pp. 327–354. John Wiley and Sons, Ltd, 2012.
Etesami, J., Kiyavash, N., and Coleman, T.P. Learning minimal latent directed information trees. In IEEE International Symposium on Information Theory (ISIT), pp.
2726–2730, 2012.
Geiger, P., Zhang, K., Gong, M., Janzing, D., and
Schölkopf, B. Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components.
ArXiv e-prints, 2015. arXiv:1411.3972 [stat.ML].
Granger, C. W. J. Investigating causal relations by econometric models and cross-spectral methods. Econometrica, 37(3):pp. 424–438, 1969. ISSN 00129682.
Hoyer, P. O., Shimizu, S., Kerminen, A. J., and Palviainen, M. Estimation of causal effects using linear nongaussian causal models with hidden variables. International Journal of Approximate Reasoning, 49(2):362 –
378, 2008. doi: http://dx.doi.org/10.1016/j.ijar.2008.02.
006.
Hyvaerinen, A., Zhang, K., Shimizu, S., and Hoyer, P. O.
Estimation of a structural vector autoregression model
using non-gaussianity. Journal of Machine Learning Research, (11):1709–1731, 2010.

Lütkepohl, H. New Introduction to Multiple Time Series
Analysis. Springer, Berlin, Heidelberg, New York, 2006.

Pearl, J. Causality. Cambridge University Press, 2000.

Schreiber, T. Measuring information transfer. Physical Review Letters, 85:461–464, 2000.
Spirtes, P., Glymour, C., and Scheines, R. Causation, prediction, and search. MIT, Cambridge, MA, 2nd edition,
2000.

