Generalized Exponential Concentration Inequality
for Rényi Divergence Estimation

Shashank Singh
Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA 15213 USA

SSS 1@ ANDREW. CMU . EDU

Barnabás Póczos
Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA 15213 USA

BAPOCZOS @ CS . CMU . EDU

Abstract
Estimating divergences in a consistent way is
of great importance in many machine learning
tasks. Although this is a fundamental problem
in nonparametric statistics, to the best of our
knowledge there has been no finite sample exponential inequality convergence bound derived
for any divergence estimators. The main contribution of our work is to provide such a bound for
an estimator of Rényi-α divergence for a smooth
Hölder class of densities on the d-dimensional
unit cube [0, 1]d . We also illustrate our theoretical results with a numerical experiment.

1. Introduction
There are several important problems in machine learning
and statistics that require the estimation of the distance or
divergence between distributions. In the past few decades
many different kinds of divergences have been defined
to measure the discrepancy between distributions. They
include the Kullback–Leibler (KL) (Kullback & Leibler,
1951), Rényi-α (Rényi, 1961; 1970), Tsallis-α (Villmann
& Haase, 2010), Bregman (Bregman, 1967), Lp , maximum
mean discrepancy (Borgwardt et al., 2006), Csiszár’s-f divergence (Csiszár, 1967) and many others.
Most machine learning algorithms operate on finite dimensional feature vectors. Using divergence estimators one
can develop machine learning algorithms (such as regression, classification, clustering, and others) that can operate
on sets and distributions (Poczos et al., 2012; Oliva et al.,
2013). Under certain conditions, divergences can estimate
entropy and mutual information. Entropy estimators are
important in goodness-of-fit testing (Goria et al., 2005),
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

parameter estimation in semi-parametric models (Wolsztynski et al., 2005), studying fractal random walks (Alemany & Zanette, 1994), and texture classification (Hero
et al., 2002a;b). Mutual information estimators have been
used in feature selection (Peng & Dind, 2005), clustering
(Aghagolzadeh et al., 2007), optimal experimental design
(Lewi et al., 2007), fMRI data processing (Chai et al., 2009),
prediction of protein structures (Adami, 2004), boosting
and facial expression recognition (Shan et al., 2005). Both
entropy estimators and mutual information estimators have
been used for independent component and subspace analysis (Learned-Miller & Fisher, 2003; Szabó et al., 2007), as
well as for image registration (Kybic, 2006; Hero et al.,
2002a;b). For further applications, see Leonenko et al.
(2008).
In this paper we will focus on the estimation of Rényi-α
divergences. This important class contains the Kullback–
Leibler divergence as the α → 1 limit case and can also
be related to the Tsallis-α, Jensen-Shannon, and Hellinger
divergences.1 It can be shown that many information theoretic quantities (including entropy, conditional entropy, and
mutual information) can be computed as special cases of
Rényi-α divergence.
In our framework, we assume that the underlying distributions are not given explicitly. Only two finite, independent
and identically distributed (i.i.d.) samples are given from
some unknown, continuous, nonparametric distributions.
Although many of the above mentioned divergences were
defined a couple of decades ago, interestingly there are still
many open questions left to be answered about the properties of their estimators. In particular, even simple questions,
such as rates are unknown for many estimators, and to the
best of our knowledge no finite sample exponential concentration bounds have ever been derived for divergence
1

Some of the divergences mentioned in the paper are distances
as well. To simplify the treatment we will call all of them divergences.

Exponential Concentration for Divergence Estimation

estimators.
Our main contribution is to derive an exponential concentration bound for a particular consistent, nonparametric,
Rényi-α divergence estimator. We illustrate the behaviour
of the estimator with a numerical experiment.
Organization
In the next section we discuss related work (Section 2). In
Section 3 we formally define the Rényi-α divergence estimation problem, and introduce the notation and the assumptions used in the paper. Section 4 presents the divergence estimator that we study in the paper. Section 5
contains our main theoretical contibutions concerning the
exponential concentration bound of the divergence estimator. Section 6 contains the proofs of our theorems. To illustrate the behaviour of the estimator, we provide a simple
numerical experiment in Section 7. We draw conclusions
in Section 8.

2. Related Work
Probably the closest work to our contribution is Liu et al.
(2012), who derived exponential-concentration bound for
an estimator of the two-dimensional Shannon entropy. We
generalize these results in several aspects:
1. The estimator of Liu et al. (2012) operates in the
unit square [0, 1]2 . Our estimator operates on the ddimensional unit hypercube [0, 1]d .
2. In Liu et al. (2012) the exponential concentration inequality was proven for densities in the Hölder class
Σκ (2, L, 2), whereas our inequality applies for densities in the Hölder class Σκ (β, L, r) for any fixed
β ≥ 0, r ≥ 1 (see Section 3.3 for definitions of these
Hölder classes).
3. While Liu et al. (2012) estimated the Shannon entropy
using one i.i.d. sample set, in this paper we estimate
the Rényi-α divergence using two i.i.d. sample sets.
To the best of our knowledge, only very few consistent nonparametric estimators exist for Rényi-α divergences: Poczos & Schneider (2011) proposed a k-nearest neighbour
based estimator and proved the weak consistency of the estimator, but they did not study the convergence rate of the
estimator.
Wang et al. (2009) provided an estimator for the α → 1
limit case only, i.e., for the KL-divergence. They did not
study the convergence rate either, and there is also an apparent error in this work; they applied the reverse Fatou lemma
under conditions when it does not hold. This error originates in the work Kozachenko & Leonenko (1987) and can

also be found in other works. Recently, Pérez-Cruz (2008)
has proposed another consistency proof for this estimator,
but it also contains some errors: he applies the strong law
of large numbers under conditions when it does not hold
and also assumes that convergence in probability implies
almost sure convergence.
Hero et al. (2002a;b) also investigated the Rényi divergence
estimation problem but assumed that one of the two density
functions is known. Gupta & Srivastava (2010) developed
algorithms for estimating the Shannon entropy and the KL
divergence for certain parametric families.
Recently, Nguyen et al. (2010) developed methods for estimating f -divergences using their variational characterization properties. They estimate the likelihood ratio of
the two underlying densities and plug that into the divergence formulas. This approach involves solving a convex
minimization problem over an infinite-dimensional function space. For certain function classes defined by reproducing kernel Hilbert spaces (RKHS), however, they were
able to reduce the computational load from solving infinitedimensional problems to solving n-dimensional problems,
where n denotes the sample size. When n is large, solving
these convex problems can still be very demanding. They
studied the convergence rate of the estimator, but did not
derive exponential concentration bounds for the estimator.
Sricharan et al. (2010); Laurent (1996); Birge & Massart
(1995) studied the estimation of non-linear functionals of
density. They, however, did not study the Rényi divergence estimation and did not derive exponential concentration bounds either. Using ensemble estimators, Sricharan
et al. (2012) derived fast rates for entropy estimation, but
they did not investigate the divergence estimation problem.
Leonenko et al. (2008) and Goria et al. (2005) considered
Shannon and Rényi-α entropy estimation from a single
sample.2 In this work, we study divergence estimators using two independent samples. Recently, Pál et al. (2010)
proposed a method for consistent Rényi information estimation, but this estimator also uses one sample only and
cannot be used for estimating Rényi divergences.
Further information and useful reviews of several different divergences can be found, e.g., in Villmann & Haase
(2010).
2

The original presentations of these works contained some errors; Leonenko & Pronzato (2010) provide corrections for some
of these theorems.

Exponential Concentration for Divergence Estimation

3. Problem and Assumptions
3.1. Notation
We use the notation of multi-indices common in multivariable calculus to index several expressions. For example,
for analytic functions f : Rd → R,
f (~y ) =

X D~i f (~x)
~
(~y − ~x)i ,
~i!

~i∈Nd

where Nd is the set of d-tuples of natural numbers,
~i! :=

d
Y

~i

(~y − ~x) :=

ik !,

k=1

d
Y

(yk − xk )

ik

k=1

and
~

~

Di f :=

∂ |i| f
,
∂ i1 x1 · · · ∂ αd xd

|~i| :=

for

d
X

ik .

k=1

We also use the multinomial theorem, which states that,
∀k ∈ N, ~x ∈ Rd .
d
X
i=1

!k
xi

=

X k! ~
~xi .
~i!

(1)

|~i|=k

to be in different Hölder classes Σκp (βp , Lp , rp ) and
Σκq (βq , Lq , rq ), but the bounds we show depend, asymptotically, only on the weaker of the conditions on p and q
(i.e., min{βp , βq }, max{Lp , Lq }, etc.).
It is worth commenting on the case that p (similarly, q) is
γ times continuously differentiable for a positive integer
γ. Since X is compact, the γ-order derivatives of p are
bounded. Hence, since X is convex, the (γ − 1)-order
derivatives of p are Lipschitz, by the Mean Value Theorem. Consequently, any degree of continuous differentiability suffices for this assumption.
The existence of an upper bound κ2 is trivial, since p, q are
continuous and X is compact. The existence of a positive
lower bound κ1 for q is natural, as otherwise Rényi-α divergence may be ∞. The existence of κ1 for p is a technical
necessity due to certain singularities at 0 (see the Logarithm
Bound in Section 6.1). However, in the important special
case of Rényi-α entropy (i.e., q is the uniform distribution),
the assumption of κ1 for p can be dropped via an argument
using Jensen’s Inequality.
As explained later, we also desire p and q to be nearly
constant near the boundary ∂X = {~x ∈ X : xj ∈
{0, 1} for some j ∈ [d]} of X . Thus, we assume that, for
any sequence {~xn }∞
xn , ∂X ) → 0 as
n=1 ∈ X with dist(~
n → ∞, ∀~i ∈ Nd with 1 ≤ |~i| ≤ `,
~

n→∞

For a given d ≥ 1, consider random d-dimensional real
vectors X and Y in the unit cube X := [0, 1]d , distributed
according to densities p, q : X → R, respectively. For
a given α ∈ (0, 1) ∪ (1, ∞), we are interested in using a
random sample of n i.i.d. points from p and n i.i.d. points
from q to estimate the Rényi α-divergence
Z

1
log
pα (~x)q 1−α (~x) d~x .
Dα (pkq) =
α−1
X
3.3. Assumptions

n→∞

Kernel Assumptions:
We assume the kernel K : R → R is non-negative, with
bounded support [−1, 1] and the following properties (with
respect to the Lebesgue measure):
Z 1
Z 1
K(u) du = 1,
uj K(u) du = 0,
−1

−1

for all j ∈ {1, . . . , `}, and K has finite 1-norm, i.e.,
Z 1
|K(u)| du = kKk1 < ∞.

(3)

−1

Density Assumptions:
We assume that p and q are in the bounded Hölder class
Σκ (β, L, r). That is, for some β ∈ (0, ∞), if ` = bβc is
the greatest integer with ` < β, ∀~i ∈ Nd with |~i| = `, the
densities p and q each satisfy a β-Hölder condition in the
r-norm (r ∈ [1, ∞)):
~

~

lim Di q(~xn ) = lim Di p(~xn ) = 0.

3.2. Problem

~

|Di p(~x + ~v ) − Di p(~x)| ≤ Lk~v kβ−`
r
=L

d
X

!(β−`)/r
|vi |r

,

(2)

i=1

and, furthermore, there exist κ = (κ1 , κ2 ) ∈ (0, ∞)2
with κ1 ≤ p, q ≤ κ2 . We could take p and q

4. Estimator
Let [d] := {1, 2, . . . , d}, and let
S := {(S1 , S2 , S3 ) : S1 ∪ S2 ∪ S3 = [d],
Si ∩ Sj = ∅ for i 6= j}
denote the set of partitions of [d] into 3 distinguishable
parts. For a bandwidth h ∈ (0, 1) (to be specified later),
for each S ∈ S, define the region
CS = {x ∈ X : ∀i ∈ S1 , 0 ≤ xi ≤ h,
∀j ∈ S2 , h < xj < 1 − h,
∀k ∈ S3 , 1 − h ≤ xk ≤ 1}

Exponential Concentration for Divergence Estimation

and the regional kernel KS : [−1, 2]d × X → R by


 Y

Y
xj + yj
xj − yj
K
K
KS (~x, ~y ) :=
·
h
h
j∈S1
j∈S2


Y
xj − 2 + yj
K
·
.
h
j∈S3

5. Main Result
Rather than the usual decomposition of mean squared error into squared bias and variance, we decompose the error
|Dα (b
ph kb
qh ) − Dα (pkq)| of our estimatator into a bias
term and a variance-like term via the triangle inequality:
|Dα (b
ph kb
qh ) − Dα (pkq)| ≤ |Dα (b
ph kb
qh ) − EDα (b
ph kb
qh )|
{z
}
|

Note that {CS : S ∈ S} partitions X (as illustrated in
Figure 1), up to intersections of measure zero, andthat K
S
x +y
is supported only on [−1, 2]d × CS . The term K j h j
corresponds to reflecting
 y across
 the hyperplane xj = 0,

We will prove the “variance” bound

so that KS (~x, y) is the product kernel (in x), with uniform
bandwidth h, centered around a reflected copy of y.

k1 ε2 n
P (|Dα (b
ph , qbh ) − EDα (b
ph , qbh )| > ε) ≤ 2 exp −
kKk2d
1

whereas the term K

xj −2+yj
h

reflects y across xj = 1,

variance-like term

+ |EDα (b
ph kb
qh ) − Dα (pkq)| .
|
{z
}
bias term





and the bias bound
x2

h

1-2h



1
β
2β
|EDα (b
ph kb
qh ) − Dα (pkq)| ≤ k2 h + h +
,
nhd
R

where k1 , k2 are constant in the sample size n and bandwidth h (see (15) and (16) for exact values of these constants). The variance bound does not depend on h, while
1
the bias bound is minimized by h  n− d+β , giving the
convergence rate


β
|EDα (b
ph kb
qh ) − Dα (pkq)| ∈ O n− d+β .

x3

h

x1

(0,0,0)

Figure 1. Illustration of regions C(S1 ,S2 ,S3 ) with 3 ∈ S1 . The
region labeled R corresponds to S1 = {3}, S2 = {1}, S3 = {2}.

We define the “mirror image” kernel density estimator
peh (~x) =

n
1 XX
KS (~x, ~xi ),
nhd i=1
S∈S

where ~xi denotes the ith sample. Since the derivatives of p
and q vanish near ∂X , p and q are approximately constant
near ∂X , and so the mirror image estimator attempts to
reduce boundary bias by mirroring data across ∂X before
kernel-smoothing. We then clip the estimator at κ1 and κ2 :
pbh (x) = min(κ2 , max(κ1 , peh (x))).
Finally, we plug our clipped density estimate into the following plug-in estimator for Rényi α-divergence:
Z

1
α
1−α
Dα (pkq) :=
log
p (~x)q
(~x) d~x
α−1
Z X

1
=
log
f (p(~x), q(~x)) d~x
(4)
α−1
X
1−α
for f : [κ1 , κ2 ]2 → R defined by f (x1 , x2 ) := xα
.
1 x2
Our α-divergence estimate is then Dα (b
ph kb
qh ).

Note that we can use this exponential concentration bound
to obtain a bound on the variance of D(b
ph kb
qh ). If
F : [0, ∞) → R is the cummulative distribution of the
squared deviation of Dα (b
ph kb
qh ) from its mean,

1 − F (ε) = P (Dα (b
ph , qbh ) − EDα (b
ph , qbh ))2 > ε


k1 n
≤ 2 exp −
.
kKk2d
1
Thus,
2

V[Dα (b
ph kb
qh )] = E [Dα (b
ph , qbh ) − EDα (b
ph , qbh )]
Z ∞
=
(1 − F (ε)) dε


Z0 ∞
k1 nε
dε
≤
2 exp −
kKk2d
0
1
=2

kKk2d
1
n−1 .
k1

(5)

We then have a mean squared-error of
h
i


2β
2
E (D(b
ph kb
qh ) − D(pkq)) ∈ O n−1 + n− d+β .


2β
which is in O(n−1 ) if β ≥ d and in O n− d+β otherwise. This asymptotic rate is consistent with previous
bounds in density functional estimation (Birge & Massart,
1995; Sricharan et al., 2010).

,

Exponential Concentration for Divergence Estimation

6. Proof of Main Result

The desired result follows by induction on |~i|.

6.1. Lemmas

Bound on Integral of Mirrored Kernel: A key property
of the mirrored kernel is that the mass of the kernel over X
is preserved, even near the boundary of X , as the kernels
about the reflected data points account exactly for the mass
of the kernel about the original data point that is not in X .
In particular, ∀~y ∈ X ,
XZ
|KS (~x, ~y )| d~x = hd kKkd1 .
(9)

Bound on Derivatives of f : Let f be as in (4). Since
f is analytic on the compact domain [κ1 , κ2 ]2 , there is a
constant Cf ∈ R, depending only on κ, and α, such that,
∀ξ ∈ (κ1 , κ2 )2 ,
 
  2


  ∂f
 ∂ f 
 ∂f
,
,


(ξ)
(ξ)
(ξ)
 ∂x1   ∂x2   ∂x2  ,
1
 2
 

 ∂ f   ∂2f


,

(6)
(ξ)
(ξ)
 ∂x2   ∂x1 x2  ≤ Cf .
2

X

S∈S

Proof: For each S ∈ S, the change of variables

Cf can be computed explicitly by differentiating f and observing that the derivatives of f are monotone in each argument. We will use this bound later in conjunction with
the Mean Value and Taylor’s theorems.
Logarithm Bound: If g, ĝ : X → R with 0 < c ≤ g, ĝ for
some c ∈ R depending only on κ and α, then, by the Mean
Value Theorem, there exists CL depending only on κ and
α such that
 Z

Z



log
ĝ(~x) d~x − log
g(~x) d~x 

X
ZX
≤ CL
|ĝ(~x) − g(~x)| d~x.
(7)
X

We will use this bound to eliminate logarithms from our
calculations.
Bounds on Derivatives of p: Combining the assumption
that the derivatives of p vanish on ∂X and the Hölder condition on p, we bound the derivatives of p near ∂X . In
particular, we show that, if ~i ∈ Nd has 1 ≤ |~i| ≤ `, then,
∀~x ∈ B := {~x ∈ X : dist(~x, ∂X ) ≤ h}
~

|Di p(~x)| ≤

β−|~i|

Lh

(` − |~i|)!

.

(8)

Proof: We proceed by induction on |~i|, as |~i| decreases
from ` to 0. The case |~i| = ` is precisely the Hölder
assumption (2). Now suppose that we have the desired
bound for derivatives of order |~i| + 1. Let ~x ∈ ∂X ,
~u = (0, . . . , 0, ±1, 0, . . . , 0) ∈ Rd , where uj = ±1. If
~y + h~u ∈ X (any ~x ∈ B is clearly of this form, for some
j ∈ [d]), then

Z h
 ∂ ~i

~i


|D p(~y + ~u)| ≤
 ∂xj D p(~y + t~u) dt
0
Z h
~
Ltβ−(|i|+1)
≤
dt
0 (` − |~i| − 1)!
~

~

Lhβ−|i|
Lhβ−|i|
=
≤
.
(β − |~i|)(` − |~i| − 1)!
(` − |~i|)!



X
x1

Figure 2. A data point x1 ∈ C({1,2},∅,∅) ⊂ [0, 1]2 , along with its
three reflected copies. The sum of the integrals over X of the four
kernels (shaded) is kKk1 .

uj = −xj ,

j ∈ S1

uj = xj ,

j ∈ S2

uj = 2 − xj ,

j ∈ S3

returns the reflected data point created by KS back onto its
original data point. Applying this change of variables gives



Z
XZ
 d u−y 
 d~x,

|KS (~x, ~y )| d~x =

K
h
X
[−1,2]d
S∈S

where K d (~x) :=

d
Y

K(xi ) denotes the product kernel.

i=1

Rescaling, translating, and applying Fubini’s Theorem,
Z
XZ
|KS (~x, ~y )| d~x = hd
|K d (~x)| d~x
S∈S

= hd

[−1,1]d

X

Z

1

d
|K(u)| du = hd kKkd1 . 

−1

6.2. Bias Bound
The following lemma bounds the integrated square bias of
peh for an arbitrary p ∈ Σκ1 ,κ2 (β, L, r). We write the bias
of peh at ~x ∈ X as Bp (~x) = Ee
ph (~x) − p(~x).

Exponential Concentration for Divergence Estimation

Bias Lemma: There exists a constant C > 0 such that
Z
Bp2 (~x) d~x ≤ Ch2β .
(10)
X

We consider separately the interior I := (h, 1 − h)d and
boundary B (noting X = I ∪ B). By a standard result
for kernel density estimates of Hölder continuous functions
(see, for example, Proposition 1.2 of Tsybakov (2008)),
Z

on the derivatives of p near ∂X (as computed in (8)),



 X
~i

D p(x)
~i 

(~yS − ~x) 


1≤|~i|≤` ~i!




 X Lhβ−|~i|

~
|i| 

≤
(3h) 
~
~
|~i|≤` (` − |i|)!i!

= Lhβ

I

≤ Lhβ

B

Suppose S = (S1 , S2 , S3 ) ∈ S\{(∅, [d], ∅)} (as
C(∅,[d],∅) = I). We wish to bound |Bp (~x)| on CS . To
simplify notation, by geometric symmetry, we may assume S3 = ∅. Let ~u ∈ [−1, 1]d , and define ~yS ∈ X by
(yS )i = hui − xi , ∀i ∈ S1 and (yS )i = xi − hui , ∀i ∈ S2
(this choice arises from the change of variables we will use
in (14)). By the Hölder condition (2) and the choice of yS ,




X D~i p(x)

~i 
β
p(~yS ) −
(~
y
−
~
x
)
S

 ≤ Lk~yS − ~xkr
~
i!


|~i|≤`

β/r
X
X
= L
|2xj + huj |r +
|huj |r 
j∈S1

j∈S2

Since each |uj | ≤ 1 and, for each i ∈ S1 , 0 ≤ xj ≤ h,




X D~i p(x)

~i 
p(~yS ) −
(~yS − ~x) 

~i!


|α|≤`

β/r
X
X
= L
(3h)r +
hr 
j∈S1
r β/r


β
= L 3d1/r h .

Rewriting this using the triangle inequality
|p(~yS ) − p(~x)|



β  X Dα p(~x)

≤ L 3d1/r h + 
(~yS − ~x)α  . (11)
1≤|α|≤` α!



~

~

Observing that (~y − ~x)i ≤ (3h)|i| and applying the bound

(` − k)!~i!

`
X

X k!3|~i|
1
~i!
k!(` − k)!
k=0
~
|i|=k

Then, applying the multinomial theorem (1) followed by
the binomial theorem gives


 X

`
~i
X


D
p(x)
(3d)k
~
i
β

(~
y
−
~
x
)
≤
Lh
S


k!(` − k)!
1≤|~i|≤` ~i!

k=0
`

= Lhβ

`!
1X
(3d)k
`!
(` − k)!k!
k=0

= Lhβ

(3d + 1)`
.
`!

Combining this bound with (11) gives
|p(~yS ) − p(~x)| = C3 hβ ,

(12)

where C3 is the constant (in n and h)

β (3d + 1)` 
C3 := L 3d1/r +
.
(13)
`!
Pn
For x ∈ CS , we have peh (~x) = nh1 d i=1 KS (~x, ~xi ), and
thus, by a change of variables, recalling that K d (~x) denotes
the product kernel,
Z
1
KS (~x, ~u)p(~u) d~u
Ee
ph (~x) = d
h
Z X
=
K d (~v )p(~yS ) d~v ,
(14)
[−1,1]d

j∈S2

≤ L (d (3h) )

~

3|i|

k=0 |~i|=k

Bp2 (x) d~x ≤ C2 h2β .

L
(In particular, this holds for the constant C2 := kKkd1 .)
`!
Z
Bp2 (~x) d~x ≤ C32 h2β .
We now show that

` X
X

Z
Since

K d (~v ) d~v = 1, by the bound in (12),

[−1,1]

|Bp (x)| = |Ee
ph (x) − p(x)|
Z

Z




d
d
=
K (~v )p(~yS ) d~v −
K (~v )p(~x) d~v 
 [−1,1]d

[−1,1]d
Z
≤
K d (~v )|p(~yS ) − p(~x)| d~v
d
[−1,1]
Z
≤
K d (~v )C3 hβ d~v = C3 hβ .
[−1,1]d

Exponential Concentration for Divergence Estimation

R
Then, B Bp2 (x) d~x ≤ C32 h2β (since the measure of B is
less than 1), proving the Bias Lemma. 
By Taylor’s Theorem, ∀~x ∈ X , for some ξ : X → R2 on
the line segment between (b
ph (~x), qbh (~x)) and (p(~x), q(~x)),

∂f
(p(~x), q(~x))(b
qh (~x) − q(~x))
∂x2
 2
∂2f
1 ∂ f
(ξ)(b
ph (~x) − p(~x))2 +
(ξ)(b
qh (~x) − q(~x))2
+
2
2 ∂x1
∂x22


∂2f
+
(ξ)(b
ph (~x) − p(~x))(b
qh (~x) − q(~x)) 
∂x1 ∂x2
2

≤ Cf (|Bp (~x)| + |Bq (~x)| + E [b
ph (~x) − p(~x)]
2

+ E [b
qh (~x) − q(~x)] + |Bp (~x)Bq (~x))|) ,
where the last line follows from the triangle inequality and
(6). Thus, using (7),
|EDα (b
ph kb
qh ) − Dα (pkq)|


Z
 1

=
E log
f (b
ph (~x), qbh (~x)) d~x
α−1
X

Z

− log
f (p(~x), q(~x)) d~x 
X
Z
CL
≤
|Ef (b
ph (~x), qeh (~x)) − f (p(~x), q(~x)) d~x|
|α − 1| X
Z
Cf CL
2
≤
|Bp (~x)| + |Bq (~x)| + E [b
ph (~x) − p(~x)]
|α − 1| X
2

+ E [b
qh (~x) − q(~x)] + |Bp (~x)Bq (~x)| d~x.
By Hölder’s Inequality, we then have
|EDα (b
ph kb
qh ) − Dα (pkq)|
sZ
sZ
Cf CL
2
≤
Bp (~x) d~x +
Bq2 (~x) d~x
|α − 1|κ1
X
X
Z
2
2
+
E [b
ph (~x) − p(~x)] + E [b
qh (~x) − q(~x)] d~x
X
s
!
Z
Z
Bp2 (~x)

kKkd1
≤ (C2 + C3 ) hβ + C2 h2β + κ2
nhd


1
≤ C hβ + h2β +
,
nhd

(15)



6.3. Variance Bound

+

X

|EDα (b
ph kb
qh ) − Dα (pkq)|

for some C > 0 not depending on n or h.

|Ef (b
ph (~x), qbh (~x)) − f (p(~x), q(~x))|

 ∂f
(p(~x), q(~x))(b
ph (~x) − p(~x))
= E
∂x1

+

Tsybakov (2008)) gives

Bq2 (~x) d~x .

X

Consider i.i.d. samples ~x1 , . . . , ~xn ∼ p, ~y 1 , . . . , ~y n ∼ q.
In anticipation of using McDiarmid’s Inequality (McDiarmid, 1989), let pb0h (~x) denote our kernel density estimate
with the sample ~xj replaced by (~xj )0 . By (7),
|Dα (b
ph kb
qh ) − Dα (e
p0h ke
qh0 )|
 Z

1 
f
(b
p
(~
x
),
q
b
(~
x
))
d~
x
log
=
h
h
|α − 1| 
X
Z


0
− log
f (b
ph (~x), qbh (~x)) d~x 
X
Z
CL
≤
|f (b
ph (~x), qbh (~x)) − f (b
p0h (~x), qbh (~x))| d~x.
|α − 1| X
Then, applying the Mean Value Theorem followed by (6)
gives, for some ξ : X → R2 on the line segment between
(b
ph , qbh ) and (p, q),
|Dα (b
ph kb
qh ) − Dα (e
p0h ke
qh0 )|

Z 
 ∂f

CL
0

≤
(ξ(~x))(b
ph (~x) − pbh (~x)) dx

|α − 1| X ∂x1
Z
CL Cf
≤
|b
ph (~x) − pb0h (~x)| d~x.
|α − 1| X
Expanding pbh as per its construction gives
|Dα (b
ph kb
qh ) − Dα (e
p0h ke
qh0 )|
Z
CL Cf
≤
|e
ph (~x) − pe0h (~x)| d~x.
|α − 1| X
XZ 

CL Cf
KS (x, xj ) − KS (x, (xj )0 ) d~x
≤
|α − 1|nhd
S∈S X
XZ
2CL Cf
2CL Cf
≤
sup
|KS (x, y)| d~x =
kKkd1 ,
|α − 1|nhd y∈X
|α
−
1|n
X
S∈S

where the last line follows from the triangle inequality and
(9). An identical proof holds if we vary some ~y i rather than
~xi . Thus, since we have 2n independent samples, McDiarmid’s Inequality gives the bound,


C 2 ε2 n
,
P (|Dα (b
ph , qbh ) − EDα (e
ph , qeh )| > ε) ≤ 2 exp −
kKk2d
1
where

Applying Lemma 3.1 and a standard result in kernel density
estimation (see, for example, Propositions 1.1 and 1.2 of

depends only on κ and α.

C=


|α − 1|
2CL Cf

(16)

Exponential Concentration for Divergence Estimation

7. Experiment

References

We used our estimator to estimate the Rényi α-divergence
between two normal distributions in R3 restricted to the
unit cube. In particular, for

Adami, Christoph. Information theory in molecular biology. Physics of Life Reviews, 1:3–22, 2004.

 
 

0.3
0.7
0.2
µ
~ 1 = 0.3 , µ
~ 2 = 0.7 , Σ =  0
0.3
0.7
0

0
0.2
0



0
0 ,
0.2

p = N (~
µ1 , Σ), q = N (~
µ2 , Σ).
For each n ∈
{1, 2, 5, 10, 50, 100, 500, 1000, 2000, 5000}, n data points
were sampled according to each distribution and constrained (via rejection sampling) to lie within [0, 1]3 . Our
estimator was computed from these samples, for α = 0.8,
using the Epanechnikov Kernel K(u) = 34 (1 − u2 ) on
[−1, 1], with constant bandwidth h = 0.25. The true αdivergence was computed directly according to its definition on the (renormalized) distributions on [0, 1]3 . The bias
and variance of our estimator were then computed in the
usual manner based on 100 trials. Figure 3 shows the error
and variance of our estimator for each n.
We also compared our estimator’s empirical error to our
theoretical bound. Since the distributions used are infinitely differentiable, β = ∞, and so the estimator’s mean
squared error should converge as O(n−1 ). An appropriate
constant multiple was computed from (5), (13), and (15).
The resulting bound is also shown in Figure 3.

Aghagolzadeh, M., Soltanian-Zadeh, H., Araabi, B., and
Aghagolzadeh, A. A hierarchical clustering based on
mutual information maximization. In in Proc. of IEEE
International Conference on Image Processing, pp. 277–
280, 2007.
Alemany, P. A. and Zanette, D. H. Fractal random walks
from a variational formalism for Tsallis entropies. Phys.
Rev. E, 49(2):R956–R958, Feb 1994. doi: 10.1103/
PhysRevE.49.R956.
Birge, L. and Massart, P. Estimation of integral functions
of a density. The Annals of Statistics, 23:11–29, 1995.
Borgwardt, Karsten M., Gretton, Arthur, Rasch, Malte J.,
Kriegel, Hans-Peter, Schlkopf, Bernhard, and Smola,
Alex J. Integrating structured biological data by kernel
maximum mean discrepancy. Bioinformatics, 22(14):
e49–e57, 2006.
Bregman, L. The relaxation method of finding the common
points of convex sets and its application to the solution of
problems in convex programming. USSR Computational
Mathematics and Mathematical Physics, 7:79–86, 1967.
Chai, Barry, Walther, Dirk B., Beck, Diane M., and FeiFei, Li. Exploring functional connectivity of the human
brain using multivariate information analysis. In NIPS,
2009.
Csiszár, I. Information-type measures of differences of
probability distributions and indirect observations. Studia Sci. Math. Hungarica, 2:299–318, 1967.
Goria, M. N., Leonenko, N. N., Mergel, V. V., and Inverardi, P. L. Novi. A new class of random vector entropy
estimators and its applications in testing statistical hypotheses. Journal of Nonparametric Statistics, 17:277–
297, 2005.

Figure 3. Log-log plot of mean squared error (computed over 100
trials) of our estimator for various sample sizes n, alongside our
theoretical bound. Error bars indicate standard deviation of estimator over 100 trials.

Gupta, M. and Srivastava, S. Parametric bayesian estimation of differential entropy and relative entropy. Entropy,
12:818–843, 2010.

8. Conclusion

Hero, A. O., Ma, B., Michel, O., and Gorman, J. Alphadivergence for classification, indexing and retrieval,
2002a. Communications and Signal Processing Laboratory Technical Report CSPL-328.

In this paper we derived a finite sample exponential
concentration bound for a consistent, nonparametric, ddimensional Rényi-α divergence estimator. To the best of
our knowledge this is the first such exponential concentration bound for Renyi divergence.

Hero, A. O., Ma, B., Michel, O. J. J., and Gorman, J. Applications of entropic spanning graphs. IEEE Signal Processing Magazine, 19(5):85–95, 2002b.

Exponential Concentration for Divergence Estimation

Kozachenko, L. F. and Leonenko, N. N. A statistical estimate for the entropy of a random vector. Problems of
Information Transmission, 23:9–16, 1987.

Pérez-Cruz, F. Estimation of information theoretic measures for continuous random variables. In Advances in
Neural Information Processing Systems 21, 2008.

Kullback, S. and Leibler, R.A. On information and sufficiency. Annals of Mathematical Statistics, 22:79–86,
1951.

Poczos, B. and Schneider, J. On the estimation of alphadivergences. In International Conference on AI and
Statistics (AISTATS), volume 15 of JMLR Workshop and
Conference Proceedings, pp. 609–617, 2011.

Kybic, J. Incremental updating of nearest neighbor-based
high-dimensional entropy estimation. In Proc. Acoustics, Speech and Signal Processing, 2006.
Laurent, B. Efficient estimation of integral functionals of a
density. The Annals of Statistics, 24:659–681, 1996.
Learned-Miller, Erik G. and Fisher, John W. ICA using spacings estimates of entropy. Journal of Machine
Learning Research, 4:1271–1295, 2003.
Leonenko, N. and Pronzato, L. Correction of ‘a class of
Rényi information estimators for mulitidimensional densities’ Ann. Statist., 36(2008) 2153-2182, 2010.
Leonenko, Nikolai, Pronzato, Luc, and Savani, Vippal. A
class of Rényi information estimators for multidimensional densities. Annals of Statistics, 36(5):2153–2182,
2008.
Lewi, J., Butera, R., and Paninski, L. Real-time adaptive
information-theoretic optimization of neurophysiology
experiments. In Advances in Neural Information Processing Systems, volume 19, 2007.
Liu, H., Lafferty, J., and Wasserman, L. Exponential concentration inequality for mutual information estimation.
In Neural Information Processing Systems (NIPS), 2012.
McDiarmid, Colin. On the method of bounded differences.
Surveys in Combinatorics, 141:148–188, 1989.
Nguyen, X., Wainwright, M.J., and Jordan., M.I. Estimating divergence functionals and the likelihood ratio by
convex risk minimization. IEEE Transactions on Information Theory, To appear., 2010.
Oliva, J., Poczos, B., and Schneider, J. Distribution to distribution regression. In International Conference on Machine Learning (ICML), 2013.
Pál, D., Póczos, B., and Szepesvári, Cs. Estimation of
Rényi entropy and mutual information based on generalized nearest-neighbor graphs. In Proceedings of the
Neural Information Processing Systems, 2010.
Peng, H. and Dind, C. Feature selection based on mutual information: Criteria of max-dependency, maxrelevance, and min-redundancy. IEEE Trans On Pattern
Analysis and Machine Intelligence, 27, 2005.

Poczos, B., Xiong, L., Sutherland, D., and Schneider, J.
Nonparametric kernel estimators for image classification. In 25th IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2012.
Rényi, A. On measures of entropy and information. In
Fourth Berkeley Symposium on Mathematical Statistics
and Probability. University of California Press, 1961.
Rényi, A. Probability Theory. North-Holland Publishing
Company, Amsterdam, 1970.
Shan, Caifeng, Gong, Shaogang, and Mcowan, Peter W.
Conditional mutual information based boosting for facial expression recognition. In British Machine Vision
Conference (BMVC), 2005.
Sricharan, K., Raich, R., and Hero, A.
Empirical estimation of entropy functionals with confidence.
Technical Report, http://arxiv.org/
abs/1012.4188, 2010.
Sricharan, K., Wei, D., and Hero, A. Ensemble estimators for multivariate entropy estimation, 2012. http:
//arxiv.org/abs/1203.5829.
Szabó, Z., Póczos, B., and Lőrincz, A. Undercomplete
blind subspace deconvolution. Journal of Machine
Learning Research, 8:1063–1095, 2007.
Tsybakov, Alexandre B. Introduction to Nonparametric Estimation. Springer Publishing Company, Incorporated, 1st edition, 2008. ISBN 0387790519,
9780387790510.
Villmann, T. and Haase, S. Mathematical aspects of
divergence based vector quantization using Frechetderivatives, 2010. University of Applied Sciences Mittweida.
Wang, Qinq, Kulkarni, Sanjeev R., and Verdú, Sergio. Divergence estimation for multidimensional densities via
k-nearest-neighbor distances. IEEE Transactions on Information Theory, 55(5), 2009.
Wolsztynski, Eric, Thierry, Eric, and Pronzato, Luc.
Minimum-entropy estimation in semi-parametric models. Signal Process., 85(5):937–949, 2005. ISSN 01651684. doi: http://dx.doi.org/10.1016/j.sigpro.2004.11.
028.

