Estimating Latent-Variable Graphical Models using Moments and Likelihoods
Arun Tejasvi Chaganty
Percy Liang
Stanford University, Stanford, CA, USA

CHAGANTY @ CS . STANFORD . EDU
PLIANG @ CS . STANFORD . EDU

Abstract
Recent work on the method of moments enable
consistent parameter estimation, but only for certain types of latent-variable models. On the other
hand, pure likelihood objectives, though more
universally applicable, are difficult to optimize.
In this work, we show that using the method of
moments in conjunction with composite likelihood yields consistent parameter estimates for a
much broader class of discrete directed and undirected graphical models, including loopy graphs
with high treewidth. Specifically, we use tensor
factorization to reveal information about the hidden variables. This allows us to construct convex
likelihoods which can be globally optimized to
recover the parameters.

xa1

xb1

xa1

xb1

h1

h1

xa2

xa1

xa3

h2
xb2
xa4

xb1

h1

h2

h3

xb2

xb3

h4

xa1

xb1

h1

xa2

xa3

h3
xa2

xa3

h2
xb2

θ

xb3

h4
xa4

xb3

h3

xb4

xb4

1. G ET C ONDITIONALS

2. G ET M ARGINALS

3. G ET PARAMETERS

Figure 1. Overview of our approach: (i) we use tensor factorization to learn the conditional moments for each hidden variable;
(ii) we optimize a composite likelihood to recover the hidden
marginals; and (iii) we optimize another likelihood objective to
the model parameters. Both likelihood objectives are convex.

of models, and are thus not as broadly applicable as EM.

1. Introduction
Latent-variable graphical models provide compact representations of data and have been employed across many
fields (Ghahramani & Beal, 1999; Jaakkola & Jordan,
1999; Blei et al., 2003; Quattoni et al., 2004; Haghighi &
Klein, 2006). However, learning these models remains a
difficult problem due to the non-convexity of the negative
log-likelihood. Local methods such as expectation maximization (EM) are the norm, but are susceptible to local
optima.
Recently, unsupervised learning techniques based on the
spectral method of moments have offered a refreshing perspective on this learning problem (Mossel & Roch, 2005;
Hsu et al., 2009; Bailly et al., 2010; Song et al., 2011;
Anandkumar et al., 2011; 2012b;a; Hsu et al., 2012; Balle
& Mohri, 2012). These methods exploit the linear algebraic
properties of the model to factorize moments of the observed data distribution into parameters, providing strong
theoretical guarantees. However, they apply to a limited set
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

In this paper, we show that a much broader class of discrete directed and undirected graphical models can be consistently estimated: specifically those in which each hidden
variable has three conditionally independent observed variables (“views”). Our key idea is to leverage the method
of moments, not to directly provide a consistent parameter estimate as in previous work, but as constraints on a
likelihood-based objective. Notably, our method applies to
latent undirected log-linear models with high treewidth.
The essence of our approach is illustrated in Figure 1,
which contains three steps. First, we identify three views
for each hidden variable hi (for example, xa1 , xb1 and xa3
are conditionally independent given h1 ) and use the tensor
factorization algorithm of Anandkumar et al. (2013) to estimate the conditional moments P(xai | hi ) and P(xbi | hi )
for each i (Section 3). Second, we optimize a composite marginal likelihood to recover the marginals over subsets of hidden nodes (e.g., P(h2 , h3 , h4 )). Normally, such
a marginal likelihood objective would be non-convex, but
given the conditional moments, we obtain a convex objective, which can be globally optimized using EM (see Sections 4 and 4.2). So far, our method has relied only on the
conditional independence structure of the model and applies generically to both directed and undirected models.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

The final step of turning hidden marginals into model parameters requires some specialization. In the directed case,
this is simple normalization; in the undirected case, we
need to solve another convex optimization problem (Section 5).

2. Setup
Let G be a discrete graphical model with observed variables
x = (x1 , . . . , xL ) and hidden variables h = (h1 , . . . , hM ).
We assume that the domains of the variables are xv ∈ [d]
for all v ∈ [L] and hi ∈ [k] for all i ∈ [M ], where [n] =
{1, . . . , n}. Let X , [d]L and H , [k]M be the joint
domains of x and h, respectively.
For undirected models G, let G denote a set of cliques,
where each clique C ⊆ x ∪ h is a subset of nodes. The joint
distribution
is given by an exponential family: pθ (x, h) ∝
Q
>
exp(θ
φC (xC , hC )), where θ is the parameter vecC∈G
tor, and φC (xC , hC ) is the local feature vector which only
depends on the observed (xC ) and hidden (hC ) variables in
clique C. Also define N (a) = {b 6= a : ∃C ⊇ {a, b}} to be
the neighbors of variable a.
Q
For directed models G, define pθ (x, h) = a∈x∪h pθ (a |
Pa(a)), where Pa(a) ⊆ x ∪ h are the parents of a variable
a. The parameters θ are the conditional probability tables
of each variable, and the cliques are G = {{a} ∪ Pa(a) :
a ∈ x ∪ h}.
Problem statement This paper focuses on the problem
of parameter estimation: We are given n i.i.d. examples of
the observed variables D = (x(1) , . . . , x(n) ), where each
x(i) ∼ pθ∗ for some true parameters θ∗ . Our goal is to
produce a parameter estimate θ̂ that approximates θ∗ .
The standard estimation procedure is maximum likelihood:
X
X
X
Lunsup (θ) ,
log pθ (x) =
log
pθ (x, h). (1)
x∈D

x∈D

h∈H

Maximum likelihood is statistically efficient, but in general
computationally intractable because marginalizing over
hidden variables h yields a non-convex objective. In practice, one uses local optimization procedures (e.g., EM or LBFGS) on the marginal likelihood, but these can get stuck
in local optima. We will later return to likelihoods, but let
us first describe a method of moments approach for parameter estimation. To do this, let’s introduce some notation.
Notation We use the notation [·] to indicate indexing; for
example, M [i] is the i-th row of a matrix M and M [i, j]
is the (i, j)-th element of M . For a tensor T ∈ Rd×···×d
and a vector i = (i1 , . . . , i` ), define the projection T [i] =
T [i1 , . . . , i` ].
We use ⊗ to denote the tensor product: if u ∈ Rd , v ∈ Rk ,

then u ⊗ v ∈ Rd×k . For an `-th order tensor T ∈ Rd×...×d
and vectors v1 , · · · , v` ∈ Rd , define the application:
X
T (v1 , · · · , v` ) =
T [i]v1 [i1 ] · · · v` [i` ].
i

Analogously, for matrices M1 ∈ Rd×k , · · · , M` ∈ Rd×k :
X
T (M1 , · · · , M` )[j] =
T [i]M1 [i1 , j1 ] · · · M` [i` , j` ].
i

We will use P(·) to denote various moment tensors constructed from the true data distribution pθ∗ (x, h):
Mi , P(xi ), Mij , P(xi , xj ), Mijk , P(xi , xj , xk ).
Here, Mi , Mij , Mijk are tensors of orders 1, 2, 3 in
Rd , Rd×d , Rd×d×d . Next, we define the hidden marginals:
Zi , P(hi ),

Zij , P(hi , hj ),

Zijk , P(hi , hj , hk ).

These are tensors of orders 1, 2, 3 in Rk , Rk×k , Rk×k×k .
Finally, we define conditional moments O(v|i) , P(xv |
hi ) ∈ Rd×k for each v ∈ [L] and i ∈ [M ].
2.1. Assumptions
In this section, we state technical assumptions that hold for
the rest of the paper, but that we feel are not central to our
main ideas. The first one ensures that all realizations of
each hidden variable are possible:
Assumption 1 (Non-degeneracy). The marginal distribution of each hidden variable hi has full support: P(hi )  0.
Next, we assume the graphical model only has conditional
independences given by the graph:
Assumption 2 (Faithful). For any hidden variables
a, b, c ∈ h such that an active trail1 connects a and b conditioned on c, we have that a and b are dependent given
c.
Finally, we assume the graphical model is in a canonical
form in which all observed variables are leaves:
Assumption 3 (Canonical form). For each observed variable xv , there exists exactly one C ∈ G such that C =
{xv , hi } for some hidden node hi .
The following lemma shows that this is not a real assumption (see the appendix for the proof):
Lemma 1 (Reduction to canonical form). Every graphical
model can be transformed into canonical form. There is a
one-to-one correspondence between the parameters of the
transformed and original models.
Finally, for clarity, we will derive our algorithms using exact moments of the true distribution pθ∗ . In practice, we
would use moments estimated from data D.

1
See Koller & Friedman (2009) for a definition. We do not
condition on observed variables.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

3. Bottlenecks
We start by trying to reveal some information about the
hidden variables that will be used by subsequent sections. Specifically, we review how the tensor factorization
method of Anandkumar et al. (2013) can be used to recover
the conditional moments O(v|i) , P(xv | hi ). The key notion is that of a bottleneck:
Definition 1 (Bottleneck). A hidden variable hi is said to
be a bottleneck if (i) there exists three observed variables
(views), xv1 , xv2 , xv3 , that are conditionally independent
given hi (Figure 2(a)), and (ii) each O(v|i) , P(xv | hi ) ∈
Rd×k has full column rank k for each v ∈ {v1 , v2 , v3 }. We
say that a subset of hidden variables S ⊆ h is bottlenecked
if every h ∈ S is a bottleneck. We say that a graphical
model G is bottlenecked if all its hidden variables are bottlenecks.
For example, in Figure 1, xa1 , xb1 , xa2 are views of the bottleneck h1 , and xa2 , xb2 , xb1 are views of the bottleneck h2 .
Therefore, the clique {h1 , h2 } is bottlenecked. Note that
views are allowed to overlap.
The full rank assumption on the conditional moments
O(v|i) = P(xv | hi ) ensures that all states of hi “behave
differently.” In particular, the conditional distribution of
one state cannot be a mixture of that of other states.
Anandkumar et al. (2012a) provide an efficient tensor factorization algorithm for estimating P(xv | hi ):
Theorem 1 (Tensor factorization). Let hi ∈ h be a bottleneck with views xv1 , xv2 , xv3 . Then there exists an algorithm G ET C ONDITIONALS that returns consistent estimates of O(v|i) for each v ∈ {v1 , v2 , v3 } up to relabeling
of the hidden variables.
To simplify notation, consider the example in Figure 2(a)
where h1 = 1, v1 = 1, v2 = 2, v3 = 3. The observed
moments M12 , M23 , M13 and M123 can be factorized as
follows:
X
0
Mvv0 =
π (1) [h]O(v|1)> [h] ⊗ O(v |1)> [h]
h

M123 =

X
h

π (1) [h]O(1|1)> [h] ⊗ O(2|1)> [h] ⊗ O(3|1)> [h].

The G ET C ONDITIONALS algorithm first computes a
whitening matrix W ∈ Rd×k such that W > M12 W =
Ik×k , and uses W to transform M123 into a symmetric
orthogonal tensor. Then a robust tensor power method is
used to extract the eigenvectors of the whitened M123 ; unwhitening yields the columns of O(3|1) (up to permutation).
The other conditional moments can be recovered similarly.
The resulting estimate of O(v|i) based on n data points con1
verges at a rate of n− 2 with a constant that depends poly(v|i) −1
nomially on σk (O
) , the inverse of the k-th largest

x1

h1

h1
x2

h2

h4
h3

x1

x2

(a) Bottleneck

x3

x4

S

x3

(b) Exclusive views

Figure 2. (a) A bottleneck h1 has three conditionally independent
views x1 , x2 , x3 . (b) A bidependent subset S has exclusive views
{x1 , x2 , x3 , x4 }.

singular value of O(v|i) . Note that σk (O(v|i) ) can become
quite small if hi and xv are connected via many intermediate hidden variables.2
The tensor factorization method attacks the heart of the
non-convexity in latent-variable models, providing some
information about the hidden variables in the form of
the conditional moments O(v|i) = P(xv | hi ). Note
that G ET C ONDITIONALS only examines the conditional
independence structure of the graphical model, not its
parametrization.
If i is the single parent of v (e.g., P(xa1 | h1 ) in Figure 1),
then this conditional moment is a parameter of the model,
but this is in general not the case (e.g., P(xa2 | h1 )). Furthermore, there are other parameters (e.g., P(h4 | h2 , h3 ))
which we do not have a handle on yet. In general, there
is a gap between the conditional moments and the model
parameters, which we will address in the next two sections.

4. Recovering hidden marginals
Having recovered conditional moments O(v|i) , P(xv |
hi ), we now seek to compute the marginal distribution of
sets of hidden variables ZS , P(hS ).
Example To gain some intuition, consider the directed
grid model from Figure 1. We can express the observed
marginals M12 , P(xa1 , xa2 ) ∈ Rd×d as a linear function of
the hidden marginals Z12 , P(h1 , h2 ) ∈ Rk×k , where the
linear coefficients are based on the conditional moments
O(1|1) , O(2|2) ∈ Rd×k :
M12 = O(1|1) Z12 O(2|2)> .
We can then solve for Z12 by matrix inversion:
Z12 = O(1|1)† M12 O(2|2)†> .
2
To see this, suppose h1 has a view xv via a chain: h1 −
h2 · · · − ht − xv . In this example, if σk (P(hi+1 | hi )) = ak for
each i = 1, · · · , t − 1, then σk (O(v|1) ) = atk σk (O(v|t) ).

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

4.1. Exclusive views
For which subsets of hidden nodes can we recover the
marginals? The following definition offers a characterization:
Definition 2 (Exclusive views). Let S ⊆ h be a subset of
hidden variables. We say hi ∈ S has an exclusive view xv
if the two conditions hold: (i) there exists some observed
variable xv which is conditionally independent of the others S\{hi } given hi (Figure 2(b)), and (ii) the conditional
moment matrix O(v|i) , P(xv | hi ) has full column rank
k and can be recovered. We say that S has the exclusive
views property if every hi ∈ S has an exclusive view.
Estimating hidden marginals We now show that if a
subset of hidden variables S has the exclusive views property, then we can recover the marginal distribution P(hS ).
Consider any S = {hi1 , . . . , him } with the exclusive views
property. Let xvj be an exclusive view for hij in S and define V = {xv1 , . . . , xvm }. By the exclusive views property,
the marginal over the observed variables P(xV ) factorizes
according to the marginal over the hidden variables P(hS )
times the conditional moments:
MV , P(xV )
X
=
P(hS ) P(xv1 | hi1 ) · · · P(xvm | him )
hS

= ZS (O(v1 |i1 ) , . . . , O(vm |im ) )
= ZS (O),

where O = O(v1 |i1 ) ⊗ · · · ⊗ O(vm |im ) is the tensor product
of all the conditional moments. Vectorizing, we have that
m
m
m
m
ZS ∈ Rk , MV ∈ Rd , and O ∈ Rd ×k . Since each
O(v|i) has full column rank k, the tensor product O has full
column rank k m . Succinctly, MV (which can be estimated
directly from data) is a linear function of ZS (what we seek
to recover). We can solve for the hidden marginals ZS simply by multiplying MV by the pseudoinverse of O:
†

†

ZS = MV (O(v1 |i1 ) , · · · , O(vm |im ) ).
Algorithm 1 summarizes the procedure, G ET M ARGINALS.
Given ZS , the conditional probability tables for S can easily be obtained via renormalization.
Theorem 2 (Hidden marginals from exclusive views). If
S ⊆ x is a subset of hidden variables with the exclusive views property, then Algorithm 1 recovers the
marginals ZS = P(hS ) up to a global relabeling of
the hidden variables determined by the labeling from
G ET C ONDITIONALS.
Relationship to bottlenecks The bottleneck property allows recovery of conditional moments, and the exclusive

Algorithm 1 G ET M ARGINALS (pseudoinverse)
Input: Hidden subset S = {hi1 , . . . , him } with exclusive
views V = {xv1 , . . . , xvm } and conditional moments
O(vj |ij ) = P(xvj | hij ).
Output: Marginals ZS = P(hS ).
†
†
Return ZS ← MV (O(v1 |i1 ) , . . . , O(vm |im ) ).
views property allows recovery of hidden marginals. But
we will now show that the latter property is in fact implied
by the former property for special sets of hidden variables,
which we call bidependent sets (in analogy with biconnected components), in which conditioning on one variable
does not break the set apart:
Definition 3 (Bidependent set). We say that a subset of
nodes S is bidependent if conditioned on any a ∈ S, there
is an active trail between any other two nodes b, c ∈ S.
Note that all cliques are bidependent, but bidependent sets can have more conditional independences (e.g.,
{h1 , h2 , h3 } in Figure 2(b)). This will be important in Section 5.1.
Bidependent sets are significant because they guarantee exclusive views if they are bottlenecked:
Lemma 2 (Bottlenecked implies exclusive views). Let S ⊆
h be a bidependent subset of hidden variables. If S is bottlenecked, then S has the exclusive views property.
Proof. Let S be a bidependent subset and fix any h0 ∈ S.
Since h0 is a bottleneck, it has three conditionally independent views, say x1 , x2 , x3 without loss of generality. For
condition (i), we will show that at least one of the views
is conditionally independent of S\{h0 } given h0 . For the
sake of contradiction, suppose that each observed variable
xi is conditionally dependent on some hi ∈ S\{h0 } given
h0 , for i ∈ {1, 2, 3}. Then conditioned on h0 , there is an
active trail between h1 and h2 because S is biconnected.
This means there is also an active trail x1 − h1 − h2 − x2
conditioned on h0 . Since the graphical model is faithful
by assumption, we have x1 6⊥ x2 | h0 , contradicting the
fact that x1 and x2 are conditionally independent given
h0 . To show condition (ii), assume, without loss of generality, that x1 is an exclusive view. Then we can recover
O(1|0) = P(x1 | h0 ) via G ET C ONDITIONALS.
Remarks. Note that having only two independent views
for each hi ∈ S is sufficient for condition (i) of the exclusive views property, while three is needed for condition (ii).
The bottleneck property (Definition 1) can also be relaxed
if some cliques share parameters (see examples below).
Our method extends naturally to the case in which the observed variables are real-valued (xv ∈ Rd ), as long as the

Estimating Latent-Variable Graphical Models using Moments and Likelihoods
h1

h1

h2

h3

x1

x2

x3

h2

...
xa2

(a) Hidden Markov model

x2

xb2

xa3

h4
xb3

xa4

xb4

(b) Tree model

h1
x1

h3

h2
x3

x4

x5

(c) Noisy-or model
Figure 3. (a) and (b): graphical models that satisfy the exclusive
views property; (c) a graphical model that does not.

hidden variables remain discrete. In this setting, the conditional moments O(v|i) , E(xv | hi ) ∈ Rd×k would no
longer be distributions but general rank k matrices.
Example: hidden Markov model. In the HMM (Figure
3(a)), h2 is a bottleneck, so we can recover O , P(x2 |
h2 ). While the first hidden variable h1 is not a bottleneck, it still has an exclusive view x1 with respect to the
clique {h1 , h2 }, assuming parameter sharing across emissions (P(x1 | h1 ) = O).
Example: latent tree model. In the latent tree model
(Figure 3(b)), h1 is not directly connected to an observed
variable, but it is still a bottleneck, with views xa2 , xa3 , xa4 ,
for example. The clique {h1 , h2 } has exclusive views
{xa2 , xa3 }.
Non-example In Figure 3(c), h1 does not have exclusive
views. Without parameter sharing, the techniques in this
paper are insufficient. In the special case where the graphical model represents a binary-valued noisy-or network, we
can use the algorithm of Halpern & Sontag (2013), which
first learns h2 and subtracts off its influence, thereby making h1 a bottleneck.
4.2. Composite likelihood
So far, we have provided a method of moments estimator which used (i) tensor decomposition to recover conditional moments and (ii) matrix pseudoinversion to recover
the hidden marginals. We will now improve statistical efficiency by replacing (ii) with a convex likelihood-based
objective.
Of course, optimizing the original marginal likelihood
(Equation 1) is subject to local optima. However, we make

two changes to circumvent non-convexity: The first is that
we already have the conditional moments from tensor decomposition, so effectively a subset of the parameters are
fixed. However, this alone is not enough, for the full likelihood is still non-convex. The second change is that we will
optimize a composite likelihood objective (Lindsay, 1988)
rather than the full likelihood.
Consider a subset of hidden nodes S = {hi1 , . . . , him },
with exclusive views V = {xv1 , . . . , xvm }. The expected
composite log-likelihood over xV given parameters ZS ,
P(hS ) with respect to the true distribution MV can be written as follows:
Lcl (ZS ) , E[log P(xV )]
X
P(hS ) P(xV | hS )]
= E[log
hS

= E[log ZS (O(v1 |i1 ) [xv1 ], · · · , O(vm |im ) [xvm ])]

= E[log ZS (O[xV ])].

(2)

The final expression is an expectation over the log of a linear function of ZS , which is concave in ZS . Unlike maximum likelihood in fully-observed settings, we do not have
a closed-form solution, so we use EM to optimize it. However, since the function is concave, EM is guaranteed to
converge to the global maximum. Algorithm 2 summarizes
our algorithm.
Algorithm 2 G ET M ARGINALS (composite likelihood)
Input: Hidden subset S = {hi1 , . . . , him } with exclusive
views V = {xv1 , . . . , xvm } and conditional moments
O(vj |ij ) = P(xvj | hij ).
Output: Marginals ZS = P(hS ).
Return ZS = arg maxZS ∈∆km −1 E[log ZS (O[xV ])].
4.3. Statistical efficiency
We have proposed two methods for estimating the hidden
marginals ZS given the conditional moments O, one based
on computing a simple pseudoinverse, and the other based
on composite likelihood. Let ẐSpi denote the pseudoinverse
estimator and ẐScl denote the composite likelihood estimator.3
The Cramér-Rao lower bound tells us that maximum likelihood yields the most statistically efficient composite estimator for ZS given access to only samples of xV .4 Let
us go one step further and quantify the relative efficiency
3

For simplicity, assume that O is known. In practice, O would
be estimated via tensor factorization.
4
Of course, we could improve statistical efficiency by maximizing the likelihood of all of x, but this would lead to a nonconvex optimization problem.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

of the pseudoinverse estimator compared to the composite
likelihood estimator.

Note that z and µ are constrained to lie on simplexes ∆k−1
and ∆d−1 , respectively. To avoid constraints, we reparameterize z and µ using ze ∈ Rk−1 and µ
e ∈ Rd−1 :




ze
µ
e
.
z
=
µ=
1 − 1> ze
1 − 1> µ
e

In this representation, µ
e and ze are related as follows,

 


µ
e
ze
O¬d,¬k O¬d,k
=
Od,¬k
Od,k
1 − 1> µ
e
1 − 1> ze

µ
e = (O¬d,¬k − O¬d,k 1> ) ze + O¬d,k .
{z
}
|
e
,O

pi
e † (µ
b̃ −
The pseudoinverse estimator is defined as b̃
z = O
O¬d,k ), and the composite likelihood estimator is given by
cl
b̃
z = arg maxze Ê[`(x; ze)], where `(x; ze) = log(µ[x]) is
the log-likelihood function.

First, we compute the asymptotic variances of the two estimators.

Lemma 3 (Asymptotic variances). The asymptotic varipi
ances of the pseudoinverse estimator b̃
z and composite
cl
likelihood estimator b̃
z are:
e † (D
e −µ
e †> ,
Σpi = O
eµ
e> )O

−1
e > (D
e −1 + de−1 11> )O
e
Σcl = O
,

e , diag(e
where D
µ) and de , 1 − 1> µ
e.

Next, let us compare the relative efficiencies of the two es1
timators: epi , k−1
tr(Σcl (Σpi )−1 ). From the Cramér-Rao
bound (van der Vaart, 1998), we know that Σcl  Σpi . This
implies that the relative efficiency, epi , lies between 0 and 1,
and when epi = 1, the pseudoinverse estimator is said to be
(asymptotically) efficient. To gain intuition, let us explore
two special cases:
e is invertible). When
Lemma 4 (Relative efficiency when O
e
O is invertible, the asymptotic variances of the pseudoinverse and composite likelihood estimators are equal, Σcl =
Σpi , and the relative efficiency is 1.

kθ − θ̂k2

Abusing notation slightly, think of MV as just a flat multinomial over dm outcomes and ZS as a multinomial over k m
m
m
outcomes, where the two are related by O ∈ Rd ×k . We
will not need to access the internal tensor structure of MV
and ZS , so to simplify the notation, let m = 1 and define
µ = MV ∈ Rd , z = ZS ∈ Rk , and O = O ∈ Rd×k . The
hidden marginals z and observed marginals µ are related
via µ = Oz.

101

100

10−1
Pseudoinverse
Composite likelihood
10−2 0
10

10−1

10−2



10−3

10−4

10−5

Figure 4. Comparison of parameter estimation error (kθ̂ − θk2 )
versus error in moments () for a hidden Markov model with k =
c123
2 hidden and d = 5 observed values. Empirical moments M
were generated by adding Gaussian noise, N (0, I), to expected
moments M123 . Results are averaged over 400 trials.

Lemma 5 (Relative efficiency with uniform observed
marginals). Let the observed marginals µ be uniform: µ =
1
d 1. The efficiency of the pseudoinverse estimator is:


1
k1U k2
1
1
−
, (3)
epi = 1 −
k − 1 1 + k1U k2
d − k1U k2
eO
e † 1, the projection of 1 onto the column
where 1U , O
e Note that 0 ≤ k1U k2 ≤ k − 1.
space of O.
2

When k1U k2 = 0, the pseudoinverse estimator is efficient:
epi = 1. When k1U k2 > 0 and d > k, the pseudoinverse
estimator is strictly inefficient. In particular, if k1U k22 =
k − 1, and we get:


1
1
pi
1−
.
(4)
e =1−
k
1+d−k
Based on Equation 3 and Equation 4, we see that the pseudoinverse gets progressively worse compared to the composite likelihood as the gap between k and d increases for
the special case wherein the observed moments are uniformly distributed. For instance, when k = 2 and d → ∞,
the efficency of the pseudolikelihood estimator is half that
of the composite likelihood estimator. Empirically, we observe that the composite likelihood estimator also leads to
more accurate estimates in general non-asymptotic regimes
(see Figure 4).

5. Recovering parameters
We have thus far shown how to recover the conditional moments O(v|i) = P(xv | hi ) for each exclusive view xv of
each hidden variable hi , as well as the hidden marginals
ZS = P(hS ) for each bidependent subset of hidden variables S. Now all that remains to be done is to recover the
parameters.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods
h1,3
xb1,3

h2,3
xb2,3

xa1,3

xa2,3

h1,2
xb1,2
xa1,2

xa3,3

h2,2
xb2,2
xa2,2

h1,1
xb1,1
xa1,1

Algorithm 3 G ET PARAMETERS

h3,3
xb3,3

h3,2
xb3,2
xa3,2

h2,1
xb2,1
xa2,1

h3,1
xb3,1
xa3,1

Figure 5. Example: undirected grid model where each hidden
variable has two conditionally independent observations. This
model has high treewidth, but we can estimate it efficiently using pseudolikelihood.

Since our graphical model is in canonical form (Assumption 3), all cliques C ∈ G either consist of hidden variables hC or are of the form {xv , hi }. The key observation
is that the clique marginals are actually sufficient statistics of the model pθ . How we turn these clique marginals
{P(xC , hC )}C∈G into parameters θ depends on the exact
model parametrization.
For directed models, the parameters are simply the local
conditional tables pθ (a | Pa(a)) for each clique C = {a} ∪
Pa(a). These conditional distributions can be obtained by
simply normalizing ZC for each assignment of Pa(a).
For undirected log-linear models, the canonical parameters
θ cannot be obtained locally, but we can construct a global
convex optimization problem to solve for θ. Suppose we
were able to observe h. Then we could optimize the supervised likelihood, which is concave:
Lsup (θ) , E(x,h)∼pθ∗ [log pθ (x, h)]
!
X
>
=θ
E[φ(xC , hC )] − A(θ).

(5)

C∈G

Of course we don’t have supervised data, but we do have
the marginals P(xC , hC ), from which we can easily compute the expected features:
X
µC , E[φ(xC , hC )] =
P(xC , hC )φ(xC , hC ). (6)
xC ,hC

Therefore, we can optimize the supervised likelihood objective without actually having any supervised data! In the
finite data regime, the method of moments yields the estimate µ̂mom
which approximates the true µC . In supervised
C
learning, we obtain a different estimate µ̂sup
C of µC based
on an empirical average over data points. In the limit of
infinite data, both estimators converge to µC .

Input: Conditional moments O(v|i) = P(xv | hi ) and hidden marginals ZS = P(hS ).
Output: Parameters θ.
if G is directed then
Normalize P(a, Pa(a)) for a ∈ x ∪ h.
else if G is undirected with low treewidth then
Compute features µC for C ∈ G (Equation 6).
Optimize full likelihood (Equation 5).
else if G is undirected with high treewidth then
Compute features µ{a}∪N (a) for a ∈ h (Equation 8).
Optimize pseudolikelihood (Equation 7).
end if
Remark If we have exclusive views for only a subset of
the cliques, we can still obtain the expected features µC for
those cliques and use posterior regularization (Graça et al.,
2008), measurements (Liang et al., 2009), or generalized
expectation criteria (Mann & McCallum, 2008) to encourage Epθ [φ(xC , hC )] to match µC . The resulting objective
functions would be non-convex, but we expect local optima
to be less of an issue.
5.1. Pseudolikelihood
While we now have a complete algorithm for estimating
directed and undirected models, optimizing the full likelihood (Equation 5) can still be computationally intractable
for undirected models with high treewidth due to the intractability of the log-partition function A(θ). One can
employ various variational approximations of A(θ) (Wainwright & Jordan, 2008), but these generally lead to inconsistent estimates of θ. We thus turn to an older idea of
pseudolikelihood (Besag, 1975). The pseudolikelihood objective is a sum over the log-probability of each variable a
given its neighbors N (a):
"
#
X
Lpseudo (θ) , E(x,h)∼pθ∗
log pθ (a | N (a)) . (7)
a∈x∪h

In the fully-supervised setting, it is well-known that pseudolikelihood provides consistent estimates which are computationally efficient but less statistically efficiency.5
P
Let φa,N (a) (a, N (a)) = C3a φC (xC , hC ) denote the sum
over cliques C that contain a; note that φa,N (a) only depends on a and its neighbors N (a). We can write each
conditional log-likelihood from Equation 7 as:
pθ (a | N (a)) = exp(θ> φa,N (a) (a, N (a)) − Aa (θ; N (a))),
where

the

conditional

log-partition

function

5
Coincidentally, this is the same high-level motivation for using method of moments in the first place.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

P
Aa (θ; N (a)) = log α∈[k] exp(θ> φa,N (a) (α, N (a)))
involves marginalizing only over the single variable a.
If we knew the marginals for each neighborhood,
µa,N (a) , E[φa,N (a) (a, N (a))],

(8)

then we would be able to optimize the pseudolikelihood
objective again without having access to any labeled data.
Unfortunately, {a} ∪ N (a) does not always have exclusive views. For example, consider a = h1 and N (a) =
{h2 , h3 , h4 } in Figure 3(b).

However, we can decompose {a} ∪ N (a) as follows: conditioning on a partitions N (a) into independent subsets; let B(a) be the collection of these subsets, which we will call sub-neighborhoods. For example, B(h1 ) = {{h2 }, {h3 }, {h4 }} in Figure 3(b) and
B(h2,2 ) = {{h1,2 , h2,3 , h3,2 , h2,1 }} contains a single subneighborhood in Figure 5.
A key observation is that for each sub-neighborhood B ∈
B(a), each {a} ∪ B is bidependent: conditioning on a does
not introduce new independencies within B by construction of B(a), and conditioning on any b ∈ B does not either
since every other b0 ∈ B\{b} is connected to a. Assuming G is bottlenecked, by Lemma 2 we have that {a} ∪ B
has exclusive views. Hence, we can recover P(a, B) for
each a and B ∈ B(a). Based on conditional independence of the sub-neighborhoods
B given a, we have that
Q
P(a, N (a)) = P(a) B∈B(a) P(B | a). This allows us to
compute the expected features µa,N (a) and use them in the
optimization of the pseudolikelihood objective.

Note that our pseudolikelihood-based approach does depend exponentially on the size of the sub-neighborhoods,
which could be exceed the largest clique size. Therefore,
each node essentially should have low degree or locally exhibit a lot of conditional independence. On the positive
side, we can handle graphical models with high treewidth;
neither sample nor computational complexity necessarily
depends on the treewidth. For example, an n×n grid model
has a treewidth of n, but the degree is at most 4.

6. Discussion
For latent-variable models, there has been tension between
local optimization of likelihood, which is broadly applicable but offers no global theoretical guarantees, and the
spectral method of moments, which provides consistent estimators but are limited to models with special structure.
The purpose of this work is to show that the two methods
can be used synergistically to produce consistent estimates
for a broader class of directed and undirected models.
Our approach provides consistent estimates for a family of
models in which each hidden variable is a bottleneck—that

is, it has three conditionally independent observations. This
bottleneck property of Anandkumar et al. (2013) has been
exploited in many other contexts, including latent Dirichlet
allocation (Anandkumar et al., 2012b), mixture of spherical
Gaussians (Hsu & Kakade, 2013), probabilistic grammars
(Hsu et al., 2012), noisy-or Bayesian networks (Halpern
& Sontag, 2013), mixture of linear regressions (Chaganty
& Liang, 2013), and others. Each of these methods can
be viewed as “preprocessing” the given model into a form
that exposes the bottleneck or tensor factorization structure.
The model parameters correspond directly to the solution
of the factorization.
In contrast, the bottlenecks in our graphical models are
given by assumption, but the conditional distribution of
the observations given the bottleneck can be quite complex. Our work can therefore be viewed as “postprocessing”, where the conditional moments recovered from tensor
factorization are used to further obtain the hidden marginals
and eventually the parameters. Along the way, we developed the notion of exclusive views and bidependent sets,
which characterize conditions under which the conditional
moments can reveal the dependency structure between hidden variables. We also made use of custom likelihood functions which were constructed to be easy to optimize.
Another prominent line of work in the method of moments
community has focused on recovering observable operator representations (Jaeger, 2000; Hsu et al., 2009; Bailly
et al., 2010; Balle & Mohri, 2012). These methods allow
prediction of new observations, but do not recover the actual parameters of the model, making them difficult to use
in conjunction with likelihood-based models. Song et al.
(2011) proposed an algorithm to learn observable operator representations for latent tree graphical models, like
the one in Figure 3(b), assuming the graph is bottlenecked.
Their approach is similar to our first step of learning conditional moments, but they only consider trees. Parikh et al.
(2012) extended this approach to general graphical models
which are bottlenecked using a latent junction tree representation. Consequently, the size of the observable representations is exponential in the treewidth. In contrast, our
algorithm only constructs moments of the order of size of
the cliques (and sub-neighborhoods for pseudolikelihood),
which can be much smaller.
An interesting direction is to examine the necessity of the
bottleneck property. Certainly, three views is in general
needed to ensure identifiability (Kruskal, 1977), but requiring each hidden variable to be a bottleneck is stronger
than what we would like. We hope that by judiciously
leveraging likelihood-based methods in conjunction with
the method of moments, we can generate new hybrid techniques for estimating even richer classes of latent-variable
models.

Estimating Latent-Variable Graphical Models using Moments and Likelihoods

References
Anandkumar, A., Chaudhuri, K., Hsu, D., Kakade, S. M.,
Song, L., and Zhang, T. Spectral methods for learning
multivariate latent tree structure. In Advances in Neural
Information Processing Systems (NIPS), 2011.
Anandkumar, A., Hsu, D., and Kakade, S. M. A method of
moments for mixture models and hidden Markov models. In Conference on Learning Theory (COLT), 2012a.
Anandkumar, A., Liu, Y., Hsu, D., Foster, D. P., and
Kakade, S. M. A spectral algorithm for latent dirichlet
allocation. In Advances in Neural Information Processing Systems (NIPS), pp. 917–925, 2012b.
Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., and Telgarsky, M. Tensor decompositions for learning latent
variable models. Technical report, ArXiv, 2013.
Bailly, R., Habrard, A., and Denis, F. A spectral approach
for probabilistic grammatical inference on trees. In Algorithmic Learning Theory, pp. 74–88. Springer, 2010.
Balle, B. and Mohri, M. Spectral learning of general
weighted automata via constrained matrix completion.
In Advances in Neural Information Processing Systems
(NIPS), pp. 2159–2167, 2012.
Besag, J. The analysis of non-lattice data. The Statistician,
24:179–195, 1975.
Blei, D., Ng, A., and Jordan, M. I. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–
1022, 2003.
Chaganty, A. and Liang, P. Spectral experts for estimating
mixtures of linear regressions. In International Conference on Machine Learning (ICML), 2013.
Ghahramani, Z. and Beal, M. J. Variational inference for
Bayesian mixtures of factor analysers. In Advances in
Neural Information Processing Systems, 1999.
Graça, J., Ganchev, K., and Taskar, B. Expectation maximization and posterior constraints. In Advances in Neural Information Processing Systems (NIPS), 2008.
Haghighi, A. and Klein, D. Prototype-driven learning for
sequence models. In North American Association for
Computational Linguistics (NAACL), 2006.
Halpern, Y. and Sontag, D. Unsupervised learning of noisyor Bayesian networks. In Uncertainty in Artificial Intelligence (UAI), pp. 272–281, 2013.
Hsu, D. and Kakade, S. M. Learning mixtures of spherical
Gaussians: Moment methods and spectral decompositions. In Innovations in Theoretical Computer Science
(ITCS), 2013.

Hsu, D., Kakade, S. M., and Zhang, T. A spectral algorithm
for learning hidden Markov models. In Conference on
Learning Theory (COLT), 2009.
Hsu, D., Kakade, S. M., and Liang, P. Identifiability and
unmixing of latent parse trees. In Advances in Neural
Information Processing Systems (NIPS), 2012.
Jaakkola, T. S and Jordan, M. I. Variational probabilistic
inference and the QMR-DT network. Journal of Artificial Intelligence Research, 10:291–322, 1999.
Jaeger, H. Observable operator models for discrete stochastic time series. Neural Computation, 2000.
Koller, D. and Friedman, N. Probabilistic graphical models: principles and techniques. MIT press, 2009.
Kruskal, J. B. Three-way arrays: Rank and uniqueness of
trilinear decompositions, with application to arithmetic
complexity and statistics. Linear Algebra and Applications, 18:95–138, 1977.
Liang, P., Jordan, M. I., and Klein, D. Learning from measurements in exponential families. In International Conference on Machine Learning (ICML), 2009.
Lindsay, B. Composite likelihood methods. Contemporary
Mathematics, 80:221–239, 1988.
Mann, G. and McCallum, A. Generalized expectation criteria for semi-supervised learning of conditional random
fields. In Human Language Technology and Association
for Computational Linguistics (HLT/ACL), pp. 870–878,
2008.
Mossel, E. and Roch, S. Learning nonsingular phylogenies
and hidden markov models. In Theory of computing, pp.
366–375. ACM, 2005.
Parikh, A., Song, L., Ishteva, M., Teodoru, G., and Xing,
E. A spectral algorithm for latent junction trees. In Uncertainty in Artificial Intelligence (UAI), 2012.
Quattoni, A., Collins, M., and Darrell, T. Conditional random fields for object recognition. In Advances in Neural
Information Processing Systems (NIPS), 2004.
Song, Le, Xing, E. P, and Parikh, A. P. A spectral algorithm for latent tree graphical models. In International
Conference on Machine Learning (ICML), 2011.
van der Vaart, A. W. Asymptotic statistics. Cambridge
University Press, 1998.
Wainwright, M. and Jordan, M. I. Graphical models, exponential families, and variational inference. Foundations
and Trends in Machine Learning, 1:1–307, 2008.

