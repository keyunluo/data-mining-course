Generalization error bounds for learning to rank:
Does the length of document lists matter?
Ambuj Tewari
University of Michigan, Ann Arbor

TEWARIA @ UMICH . EDU

Sougata Chaudhuri
University of Michigan, Ann Arbor

SOUGATA @ UMICH . EDU

Abstract
We consider the generalization ability of algorithms for learning to rank at a query level, a
problem also called subset ranking. Existing
generalization error bounds necessarily degrade
as the size of the document list associated with a
query increases. We show that such a degradation is not intrinsic to the problem. For several
loss functions, including the cross-entropy loss
used in the well known ListNet method, there is
no degradation in generalization ability as document lists become longer. We also provide novel
generalization error bounds under `1 regularization and faster convergence rates if the loss function is smooth.

1. Introduction
Learning to rank at the query level has emerged as an exciting research area at the intersection of information retrieval
and machine learning. Training data in learning to rank
consists of queries along with associated documents, where
documents are represented as feature vectors. For each
query, the documents are labeled with human relevance
judgements. The goal at training time is to learn a ranking
function that can, for a future query, rank its associated documents in order of their relevance to the query. The performance of ranking functions on test sets is evaluated using a
variety of performance measures such as NDCG (Järvelin
& Kekäläinen, 2002), ERR (Chapelle et al., 2009) or Average Precision (Yue et al., 2007).
The performance measures used for testing ranking methods cannot be directly optimized during training time as
they lead to discontinuous optimization problems. As a result, researchers often minimize surrogate loss functions
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

that are easier to optimize. For example, one might consider smoothed versions of, or convex upper bounds on,
the target performance measure. However, as soon as one
optimizes a surrogate loss, one has to deal with two questions (Chapelle et al., 2011). First, does minimizing the
surrogate on finite training data imply small expected surrogate loss on infinite unseen data? Second, does small
expected surrogate loss on infinite unseen data imply small
target loss on infinite unseen data? The first issue is one of
generalization error bounds for empirical risk minimization (ERM) algorithms that minimize surrogate loss on
training data. The second issue is one of calibration: does
consistency in the surrogate loss imply consistency in the
target loss?
This paper deals with the former issue, viz. that of generalization error bounds for surrogate loss minimization. In
pioneering works, Lan et al. (2008; 2009) gave generalization error bounds for learning to rank algorithms. However,
while the former paper was restricted to analysis of pairwise approach to learning to rank, the later paper was limited to results on just three surrogates: ListMLE, ListNet
and RankCosine. To the best of our knowledge, the most
generally applicable bound on the generalization error of
query-level learning to rank algorithms has been obtained
by Chapelle & Wu (2010).
The bound of Chapelle & Wu (2010), while generally applicable, does have an explicit dependence on the length of
the document list associated with a query. Our investigations begin with this simple question: is an explicit dependence on the length of document lists unavoidable in generalization error bounds for query-level learning to rank algorithms? We focus on the prevalent technique in literature
where learning to rank algorithms learn linear scoring functions and obtain ranking by sorting scores in descending
order. Our first contribution (Theorem 3) is to show that dimension of linear scoring functions that are permutation invariant (a necessary condition for being valid scoring functions for learning to rank) has no dependence on the length
of document lists. Our second contribution (Theorems 5,

Generalization error bounds for learning to rank
Table 1. A comparison of three bounds given in this paper for Lipschitz loss functions. Criteria for comparison: algorithm bound applies
to (OGD = Online Gradient Descent, [R]ERM = [Regularized] Empirical Risk Minimization), whether it applies to general (possibly
non-convex) losses, and whether the constants involved are tight.

Bound
Theorem 5
Theorem 6
Theorem 9

Applies to
OGD
RERM
ERM

Handles Nonconvex Loss
No
No
Yes

6, 9) is to show that as long as one uses the “right” norm in
defining the Lipschitz constant of the surrogate loss, we can
derive generalization error bounds that have no explicit dependence on the length of document lists. The reason that
the second contribution involves three bounds is that they
all have different strengths and scopes of application (See
Table 1 for a comparison). Our final contribution is to provide novel generalization error bounds for learning to rank
in two previously unexplored settings: almost dimension
independent bounds when using high dimensional features
with `1 regularization (Theorem 12) and “optimistic” rates
(that can be as fast as O(1/n)) when the loss function is
smooth (Theorem 17). We also apply our results on popular convex and non-convex surrogates. All omitted proofs
can be found in the appendix (see supplementary material).

2. Preliminaries
In learning to rank (also called subset ranking to distinguish
it from other related problems, e.g., bipartite ranking), a
training example is of the form ((q, d1 , . . . , dm ), y). Here
q is a search query and d1 , . . . , dm are m documents with
varying degrees of relevance to the query. Human labelers
provide the relevance vector y 2 Rm where the entries in
y contain the relevance labels for the m individual documents. Typically, y has integer-valued entries in the range
{0, . . . , Ymax } where Ymax is often less than 5. For our
theoretical analysis, we get rid of some of these details by
assuming that some feature map exists to map a query
document pair (q, d) to Rd . As a result, the training example ((q, d1 , . . . , dm ), y) gets converted into (X, y) where
X = [ (q, d1 ), . . . , (q, dm )]> is an m ⇥ d matrix with
the m query-document feature vector as rows. With this
abstraction, we have an input space X ✓ Rm⇥d and a label
space Y ✓ Rm .
A
training
set
consists
of
iid
examples
(X (1) , y (1) ), . . . , (X (n) , y (n) ) drawn from some underlying distribution D. To rank the documents in an
instance X 2 X , often a score vector s 2 Rm is computed.
A ranking of the documents can then be obtained from
s by sorting its entries in decreasing order. A common
choice for the scoring function is to make it linear in the
input X and consider the following class of vector-valued

“Constant” hidden in O(·) notation
Smallest
Small
Hides several logarithmic factors

functions:
Flin = {X 7! Xw : X 2 Rm⇥d , w 2 Rd }.

(1)

Depending upon the regularization, we also consider the
following two subclasses of Flin :
F2 := {X 7! Xw : X 2 Rm⇥d , w 2 Rd , kwk2  W2 },
F1 := {X 7! Xw : X 2 Rm⇥d , w 2 Rd , kwk1  W1 }.
In the input space X , it is natural for the rows of X to have
a bound on the appropriate dual norm. Accordingly, whenever we use F2 , the input space is set to X = {X 2 Rm⇥d :
8j 2 [m], kXj k2  RX } where Xj denotes jth row of X
and [m] := {1, . . . , m}. Similarly, when we use F1 , we set
X = {X 2 Rm⇥d : 8j 2 [m], kXj k1  R̄X }. These
are natural counterparts to the following function classes
studied in binary classification and regression:
G2 := {x 7! hx, wi : kxk2  RX , w 2 Rd , kwk2  W2 },

G1 := {x 7! hx, wi : kxk1  R̄X , w 2 Rd , kwk1  W1 }.

A key ingredient in the basic setup of the learning to rank
problem is a loss function : Rm ⇥ Y ! R+ where R+
denotes the set of non-negative real numbers. Given a class
F of vector-valued functions, a loss yields a natural loss
class: namely the class of real-valued functions that one
gets by composing with functions in F:
F := {(X, y) 7! (f (X), y) : X 2 Rm⇥d , f 2 F}.
For vector valued scores, the Lipschitz constant of depends on the norm ||| · ||| that we decide to use in the score
space (||| · |||? is dual of ||| · |||):
8y 2 Y, s, s0 2 Rm , | (s1 , y)

(s2 , y)|  G |||s1 s2 |||.

If is differentiable, this is equivalent to: 8y 2 Y, s 2
Rm , |||rs (s, y)|||?  G . Similarly, the smoothness
constant H of defined as: 8y 2 Y, s, s0 2 Rm ,
|||rs (s1 , y)

rs (s2 , y)|||?  H |||s1

s2 |||.

also depends on the norm used in the score space. If is
twice differentiable, the above inequality is equivalent to
8y 2 Y, s 2 Rm , |||r2s (s, y)|||op  H

Generalization error bounds for learning to rank

where ||| · |||op is the operator norm induced by the pair
v|||?
||| · |||, ||| · |||? and defined as |||M |||op := supv6=0 |||M
|||v||| .
Define the expected loss of w under the distribution D
L (w) := E(X,y)⇠D [ (Xw, y)] and its empirical loss on
Pn
the sample as L̂ (w) := n1 i=1 (X (i) w, y (i) ). The minimizer of L (w) (resp. L̂ (w)) over some function class
(parameterized by w) will be denoted by w? (resp. ŵ). We
b [·]. To
may refer to expectations w.r.t. the sample using E
reduce notational clutter, we often refer to (X, y) jointly by
Z and X ⇥ Y by Z.PFor vectors, hu, vi denotes the standard inner product i ui vi and for matrices
P U, V of the
same shape, hU, V i means Tr(U > V ) = ij Uij Vij . The
set of m! permutation ⇡ of degree m is denoted by Sm . A
vector of ones is denoted by 1.

3. Application to Specific Losses
To whet the reader’s appetite for the technical presentation that follows, we will consider two loss functions,
one convex and one non-convex, to illustrate the concrete improvements offered by our new generalization
bounds. A generalization bound is of the form: L (ŵ) 
L (w? )+“complexity term”. It should be noted that w?
is not available to the learning algorithm as it needs knowledge of underlying distribution of the data. The complexity
p
term of Chapelle & Wu (2010) is O(GCW W2 RX m/n).
The constant GCW is the Lipschitz constant of the surrogate
(viewed as a function of the score vector s)
w.r.t. `2 norm.
p Our bounds will instead be of the form
O(G W2 RX 1/n), where G is the Lipschitz constant
of w.r.t. `1 norm. Note that our bounds are free of
any explicit
m dependence. Also, by definition, G 
p
GCW m but the former can be much smaller as the two
examples below illustrate. In benchmark datasets (Liu
et al., 2007), m can easily be in the 100-1000 range.
3.1. Application to ListNet
The ListNet ranking method (Cao et al., 2007) uses
a convex surrogate, that is defined in the following
m
way1 . Define
Pm m maps from R to R as: Pj (v) =
exp(vj )/ i=1 exp(vi ) for j 2 [m]. Then, we have, for
s 2 Rm and y 2 Rm ,
LN (s, y)

=

m
X

Pj (y) log Pj (s).

j=1

An easy calculation shows that the Lipschitz (as well as
smoothness) constant of LN is m independent.
Proposition 1. The Lipschitz (resp. smoothness) constant
1
The ListNet paper actually defines a family of losses based on
probability models for top k documents. We use k = 1 in our definition since that is the version implemented in their experimental
results.

of LN w.r.t. k · k1 satisfies G
for any m 1.

LN

 2 (resp. H

LN

 2)

Since the bounds above are independent of m, so the generalization bounds resulting from their use in Theorem 9
and Theorem 17 will also be independent of m (up to logarithmic factors). We are not aware of prior generalization
bounds for ListNet that do not scale with m. In particular,
the results of Lan et al. (2009) have an m! dependence since
they consider the top-m version of ListNet. However, even
if the top-1 variant above is considered, their proof technique will result in at least a linear dependence on m and
does not result in as tight a bound as we get from our general results. It is also easy to see that the Lipschitz constant
GCW
of ListNet loss w.r.t. `2 norm is also 2 and hence the
LN
p
bound of Chapelle & Wu (2010) necessarily has a m dependence in it. Moreover, generalization error bounds for
ListNet exploitingpits smoothness will interpolate between
the pessimistic 1/ n and optimistic 1/n rates. These have
never been provided before.
3.2. Application to Smoothed DCG@1
This example is from the work of Chapelle & Wu (2010).
Smoothed DCG@1, a non-convex surrogate, is defined as:
SD (s, y) = D(1)

m
X

exp(si / )
,
G(yi ) P
j exp(sj / )
i=1

where D(i) = 1/ log2 (1 + i) is the “discount” function
and G(i) = 2i 1 is the “gain” function. The amount
of smoothing is controlled by the parameter
> 0 and
the smoothed version approaches DCG@1 as
! 0
(DCG stands for Discounted Cumulative Gain (Järvelin &
Kekäläinen, 2002)).
Proposition 2. The Lipschitz constant of SD w.r.t. k · k1
satisfies G SD  2D(1)G(Ymax )/ for any m 1. Here
Ymax is maximum possible relevance score of a document
(usually less than 5).
As in the ListNet loss case we previously considered, the
generalization bound resulting from Theorem 9 will be independent of m. This is intuitively satisfying: DCG@1,
whose smoothing we are considering, only depends on the
document that is put in the top position by the score vector s (and not on the entire sorted order of s). Our generalization bound does not deteriorate as the total list size
m grows. In contrast, the bound
p of Chapelle & Wu (2010)
will necessarily deteriorate as m since the constant GCW
SD
is the same as G SD . Moreover, it should be noted that even
in the original SmoothedDCG paper, is present in the denominator of GCW
, so our results are directly comparable.
SD
Also note that this example can easily be extended to consider DCG@k for case when document list length m
k
(a very common scenario in practice).

Generalization error bounds for learning to rank

3.3. Application to RankSVM
RankSVM (Joachims, 2002) is another well established
ranking method, which minimizes a convex surrogate based
on pairwise comparisons of documents. A number of studies have shown that ListNet has better empirical performance than RankSVM. One possible reason for the better
performance of ListNet over RankSVM is that the Lipschitz constant of RankSVM surrogate w.r.t k · k1 doe scale
with document list size as O(m2 ). Due to lack of space,
we give the details in the supplement.

an m independent complexity term in the generalization
bound. The reader might wonder whether the dimension
reduction from m2 d to d in going from Ffull to Flin is arbitrary. To dispel this doubt, we prove the lower dimensional
class Flin is the only sensible choice of linear scoring functions in the learning to rank setting. This is because scoring
functions should satisfy a permutation invariance property.
That is, if we apply a permutation ⇡ 2 Sm to the rows of X
to get a matrix ⇡X then the scores should also simply get
permuted by ⇡. That is, we should only consider scoring
functions in the following class:

4. Does The Length of Document Lists
Matter?

Fperminv = {f : 8⇡ 2 Sm , 8X 2 Rm⇥d , ⇡f (X) = f (⇡X)}.

Our work is directly motivated by a very interesting generalization bound for learning to rank due to Chapelle & Wu
(2010, Theorem 1). They considered a Lipschitz continuous loss with Lipschitz constant GCW w.r.t. the `2 norm.
They show that, with probability at least 1
,
r

m
n

8w 2 F2 , L (w)  L̂ (w) + 3 GCW W2 RX
r
8 log(1/ )
+
.
n
p
The dominant term on the right is O(GCW W2 RX m/n).
In the next three sections, p
we will derive improved bounds
p
of the form Õ(G W2 RX 1/n) where G  GCW m
but can be much smaller. Before we do that, let us examine
the dimensionality reduction in linear scoring function that
is caused by a natural permutation invariance requirement.
4.1. Permutation invariance removes m dependence in
dimensionality of linear scoring functions
As stated in Section 2, a ranking is obtained by sorting a
score vector obtained via a linear scoring function f . Consider the space of linear scoring function that consists of
all linear maps f that map Rm⇥d to Rm :
n

o

Ffull := X 7! [hX, W1 i , . . . , hX, Wm i]> : Wi 2 Rm⇥d .

These linear maps are fully parameterized by matrices
W1 , . . . , Wm . Thus, a full parameterization of the linear
scoring function is of dimension m2 d. Note that the popularly used class of linear scoring functions Flin defined in
Eq. 1 is actually a low d-dimensional subspace of the full
m2 d dimensional space of all linear maps. It is important
to note that the dimension of Flin is independent of m.

In learning theory, one of the factors influencing the generalization error bound is the richness of the class of hypothesis functions. Since the linear function class Flin has
dimension independent of m, we intuitively expect that,
at least under some conditions, algorithms that minimize
ranking losses using linear scoring functions should have

The permutation invariance requirement, in turn, forces a
reduction from dimension m2 d to just 2d (which has no
dependence on m).
Theorem 3. The intersection of the function classes Ffull
and Fperminv is the 2d-dimensional class:
0
Flin
= {X 7! Xw + (1> Xv)1 : w, v 2 Rd }.

(2)

Note that the extra degree of freedom provided by the v
parameter in Eq. 2 is useless for ranking purposes since
adding a constant vector (i.e., a multiple of 1) to a score
vector has no effect on the sorted order. This is why we
said that Flin is the only sensible choice of linear scoring
functions.

5. Online to Batch Conversion
In this section, we build some intuition as to why it is natural to use k · k1 in defining the Lipschitz constant of the
loss . To this end, consider the following well known online gradient descent (OGD) regret guarantee. Recall that
OGD refers to the simple online algorithm that makes the
update wi+1
wi ⌘rwi fi (wi ) at time i. If we run OGD
to generate wi ’s, we have, for all kwk2  W2 :
n
X
i=1

fi (wi )

n
X
i=1

fi (w) 

W22
+ ⌘G2 n
2⌘

where G is a bound on the maximum `2 -norm of
the gradients rwi fi (wi ) and fi ’s have to be convex.
If (X (1) , y (1) ), . . . , (X (n) , y (n) ) are iid then by setting
fi (w) = (X (i) w, y (i) ), 1  i  n we can do an “online to batch conversion”. That is, we optimize over ⌘, take
expectations and use Jensen’s inequality to get the following excess risk bound:
r
2
8kwk2  W2 , E [L (ŵOGD )] L (w)  W2 G
n
Pn
1
where ŵOGD = n i=1 wi and G has to satisfy (noting
that s = X (i) wi )
G

krwi fi (wi )k2 = k(X (i) )> rs (X (i) wi , y (i) )k2

Generalization error bounds for learning to rank

where we use the chain rule to express rw in terms of rs .
Finally, we can upper bound
k(X (i) )> rs (X (i) wi , y (i) )k2

 k(X (i) )> k1!2 · krs (X (i) wi , y (i) )k1
 RX krs (X (i) wi , y (i) )k1

as RX
maxm
j=1 kXj k2 and because of the following
lemma.
Lemma 4. For any 1  p  1,
kXkp!q

kXvkq
= sup
v6=0 kvkp

m

kX > k1!p = kXkq!1 = max kXj kp ,
j=1

where q is the dual exponent of p (i.e.,

1
q

+

1
p

= 1).

Thus, we have shown the following result.
Theorem 5. Let be convex and have Lipschitz constant
G w.r.t. k · k1 . Suppose we run online gradient descent
(i)
(i)
(with appropriate step size
Pn ⌘) on fi (w) = (X w, y )
1
and return ŵOGD = T i=1 wi . Then we have,
r
2
8kwk2  W2 , E [L (ŵOGD )] L (w)  G W2 RX
.
n
The above excess risk bound has no explicit m dependence.
This is encouraging but there are two deficiencies of this
approach based on online regret bounds. First, the result
applies to the output of a specific algorithm that may not
be the method of choice for practitioners. For example,
the above argument does not yield uniform convergence
bounds that could lead to excess risk bounds for ERM
(or regularized versions of it). Second, there is no way
to generalize the result to Lipschitz, but non-convex loss
functions. It may noted here that the original motivation
for Chapelle & Wu (2010) to prove their generalization
bound was to consider the non-convex loss used in their
SmoothRank method. We will address these issues in the
next two sections.

6. Stochastic Convex Optimization
We first define the regularized empirical risk minimizer:
ŵ = argmin
kwk2 W2

2

kwk22 + L̂ (w).

(3)

We now state the main result of this section.
Theorem 6. Let the loss function be convex and have
Lipschitz constant Gp w.r.t. k·k1 . Then, for an appropriate
choice of = O(1/ n), we have
r !
8
2
?
E [L (ŵ )]  L (w ) + 2 G RX W2
+
.
n
n

This result applies to a batch algorithm (regularized ERM)
but unfortunately requires the regularization parameter
to be set in a particular way. Also, it does not apply to
non-convex losses and does not yield uniform convergence
bounds. In the next section, we will address these deficiencies. However, we will incur some extra logarithmic factors
that are absent in the clean bound above.

7. Bounds for Non-convex Losses
The above discussion suggests that we have a possibility
of deriving tighter, possibly m-independent, generalization
error bounds by assuming that is Lipschitz continuous
w.r.t. k · k1 . The standard approach in binary classification is to appeal to the Ledoux-Talagrand contraction principle for establishing Rademacher complexity (Bartlett &
Mendelson, 2003). It gets rid of the loss function and incurs a factor equal to the Lipschitz constant of the loss in
the Rademacher complexity bound. Since the loss function
takes scalar argument, the Lipschitz constant is defined for
only one norm, i.e., the absolute value norm. It is not immediately clear how such an approach would work when
the loss takes vector valued arguments and is Lipschitz
w.r.t. k · k1 . We are not aware of an appropriate extension
of the Ledoux-Talagrand contraction principle. Note that
Lipschitz continuity w.r.t. the Euclidean norm k · k2 does
not pose a significant challenge since Slepian’s lemma can
be applied to get rid of the loss. Several authors have already exploited Slepian’s lemma in this context (Bartlett &
Mendelson, 2003; Chapelle & Wu, 2010). We take a route
involving covering numbers and define the data-dependent
(pseudo-)metric:
(1:n)

dZ
1

n

(w, w0 ) := max
i=1

(X (i) w, y (i) )

(X (i) w0 , y (i) )

Let N1 (✏,
F, Z (1:n) ) be the covering number at scale
✏ of the composite class
F=
F1 or
F2 w.r.t. the
above metric. Also define
N1 (✏,

F, n) := max N1 (✏,
Z (1:n)

F, Z (1:n) ).

With these definitions in place, we can state our first result
on covering numbers.
Proposition 7. Let the loss be Lipschitz w.r.t. k · k1
with constant G . Then following covering number bound
holds:
&
'
2
G2 W22 RX
log2 N1 (✏,
F2 , n) 
log2 (2mn + 1).
✏2
Proof. Note that
n

(X (i) w, y (i) )
(X (i) w0 , y (i) )
D
E D
E
n
m
(i)
(i)
 G · max max Xj , w
Xj , w 0 .
max
i=1

i=1 j=1

Generalization error bounds for learning to rank

Corollary 8. Let be Lipschitz w.r.t. k · k1 and uniformly bounded3 by B for w 2 F2 . Then the empirical
Rademacher complexities of the class
F2 is bounded as
r
log2 (3mn)
bn (
R
F2 )  10G W2 RX
n
p
6B n
p
⇥ log
.

This immediately implies that if we have a cover of the
class G2 (Sec.2) at scale ✏/G w.r.t. the metric
n

m

max max
i=1 j=1

D

(i)

Xj , w

E

D

(i)

Xj , w 0

E

(1:n)

then it is also a cover of F2 w.r.t. dZ
, at scale ✏. Now
1
comes a simple, but crucial observation: from the point of
view of the scalar valued function class G2 , the vectors
(i)
(Xj )i=1:n
j=1:m constitute a data set of size mn. Therefore,
N1 (✏,

F2 , n)  N1 (✏/G , G2 , mn).

5G W2 RX

Proof. This follows by simply plugging in estimates from
Proposition 7 into (6) and choosing ↵ optimally.

(4)

Control on the Rademacher complexity immediately leads
to uniform convergence bounds and generalization error
bounds for ERM. The informal Õ notation hides factors
logarithmic in m, n, B, G , RX , W1 . Note that all hidden
factors are small and computable from the results above.

Now we appeal to the following bound due to Zhang (2002,
Corollary 3) (and plug the result into (4)):
'

log2 N1 (✏/G , G2 , mn) 

&

Covering number N2 (✏,

F, Z (1:n) ) uses pseudo-metric:

dZ
2

(1:n)

2
G2 W22 RX
✏2

n
X
1⇣
(X (i) w, y (i) )
n
i=1

(w, w0 ) :=

log2 (2mn+1)

(X (i) w0 , y (i) )

⌘2

Theorem 9. Suppose is Lipschitz w.r.t. k · k1 with constant G and is uniformly bounded by B as w varies over
F2 . With probability at least 1
,
8w 2 F2 , L (w)  L̂ (w)
!1/2

It is well known that a control on N2 (✏,
F, Z (1:n) )
provides control on the empirical Rademacher complexity
and that N2 covering numbers are smaller than N1 ones.
For us, it will be convenient to use a more refined version2
due to Mendelson (2002). Let H be a class of functions,
with H : Z 7! R, uniformly bounded by B. Then, we have
following bound on empirical Rademacher complexity
b n (H)
R
0

 inf @4↵ + 10
↵>0

 inf

↵>0

4↵ + 10

Z
Z

suph2H
↵

B
↵

r

q

b [h2 ]
E

r

1
log2 N2 (✏, H, Z (1:n) ) A
d✏
n
!

log2 N2 (✏, H, Z (1:n) )
d✏ .
n

(5)
(6)

b n (H) is the empirical Rademacher complexity of
Here R
the class H defined as
"
#
n
X
1
b n (H) := E
R
sup
i h(Zi ) ,
1:n
h2H n i=1
where 1:n = ( 1 , . . . , n ) are iid Rademacher (symmetric
Bernoulli) random variables.
2

We use a further refinement due to Srebro and Sridharan
available at http://ttic.uchicago.edu/˜karthik/
dudley.pdf

log2 (3mn)

+ Õ G W2 RX

r

1
+B
n

r

log(1/ )
n

!

and therefore with probability at least 1 2 ,
!
r
r
1
log(1/ )
?
L (ŵ)  L (w )+Õ G W2 RX
+B
.
n
n
where ŵ is an empirical risk minimizer over F2 .
Proof. Follows from standard bounds using Rademacher
complexity. See, for example, Bartlett & Mendelson
(2003).
As we said before, ignoring logarithmic factors, the bound
for F2 is an improvement over the bound of Chapelle &
Wu (2010).

8. Extensions
We extend the generalization bounds above to two settings:
a) high dimensional features and b) smooth losses.
8.1. High-dimensional features
In learning to rank situations involving high dimensional
features, it may not be appropriate to use the class F2 of
`2 bounded predictors. Instead, we would like to consider
the class F1 of `1 bounded predictors. In this case, it is
3
A uniform bound on the loss easily follows under the
(very reasonable) assumption that 8y, 9sy s.t. (sy , y) = 0.
Then (Xw, y)  G kXw
sy k1  G (W2 RX +
maxy2Y ksy k1 )  G (2W2 RX ).

Generalization error bounds for learning to rank

natural to measure size of the input matrix X in terms of
a bound R̄X on the maximum `1 norm of each of its row.
The following analogue of Proposition 7 can be shown.
Proposition 10. Let the loss be Lipschitz w.r.t. k·k1 with
constant G . Then the following covering number bound
holds:
&
'
2
(2 + log d)
288 G2 W12 R̄X
log2 N1 (✏,
F1 , n) 
✏2
✓ ⇠
⇡
◆
8G W1 R̄X
⇥ log2 2
mn + 1 .
✏
Using the above result to control the Rademacher complexity of
F1 gives the following bound.

Corollary 11. Let be Lipschitz w.r.t. k·k1 and uniformly
bounded by B for w 2 F1 . Then the empirical Rademacher
complexities of the class
F1 is bounded as
bn (
R

p

F1 )  120 2G W1 R̄X
⇥ log2

40

r

8.3. Online regret bounds under smoothness
Let us go back to OGD guarantee, this time presented in a
slightly more refined version. If we run OGD with learning
rate ⌘ then, for all kwk2  W2 :
n
X
i=1

i=1

fi (w) 

n
X
W22
+⌘
kgi k22
2⌘
i=1

which bounds the magnitude of the gradient of f at a point
in terms of the value of the function itself at that point. This
log(d) log2 (24mnG W1 R̄X )
means that kgi k22  4Hfi (wi ) which, when plugged into
n
the OGD guarantee, gives:
B+24mnG W1 R̄X

As in the previous section, control of Rademacher complexity immediately yields uniform convergence and ERM
generalization error bounds.
Theorem 12. Suppose is Lipschitz w.r.t. k · k1 with
constant G and is uniformly bounded by B as w varies
over F1 . With probability at least 1
,

+ Õ G W1 R̄X

n
X

where gi = rwi fi (wi ) (if fi is not differentiable at wi
then we can set gi to be an arbitrary subgradient of fi at
wi ). Now assume that all fi ’s are non-negative functions
and are smooth w.r.t. k · k2 with constant H. Lemma 3.1 of
Srebro et al. (2010) tells us that any non-negative, smooth
function f (w) enjoy an important self-bounding property
for the gradient:
p
krw fi (w)k2  4Hfi (w)

p
p
.
2G W1 R̄X log(d) log2 (24mnG W1 R̄X )

8w 2 F1 , L (w)  L̂ (w)
r

fi (wi )

log d
+B
n

and therefore with probability at least 1

r

log(1/ )
n

!

n
X

fi (wi )

i=1

n
X
i=1

n

fi (w) 

X
W22
+ 4⌘H
fi (wi )
2⌘
i=1

Again, setting fi (w) = (X (i) w, y (i) ), 1  t  n, and using the online to batch conversion technique, we can arrive
at the bound: for all kwk2  W2 :
E [L (ŵ)] 

L (w)
W22
+
(1 4⌘H) 2⌘(1 4⌘H)n

At this stage, we can fix w = w? , the optimal `2 -norm
bounded predictor and get optimal ⌘ as:
⌘=

2 ,

4HW2 + 2

p

W2
4H 2 W22

+ 2HL (w? )n

.

(7)

! After plugging this value of ⌘ in the bound above and some
algebra (see Section H), we get the upper bound
L (ŵ)  L (w )+Õ G W1 R̄X
r
2HW22 L (w? ) 8HW22
?
E [L (ŵ)]  L (w ) + 2
+
.
where ŵ is an empirical risk minimizer over F1 .
n
n
(8)
p
Such a rate interpolates between a 1/ n rate in the “pesAs can be easily seen from Theorem. 12, the generalization
simistic” case (L (w? ) > 0) and the 1/n rate in the “opbound is almost independent of the dimension of the docutimistic” case (L (w? ) = 0) (this terminology is due to
ment feature vectors. We are not aware of existence of such
Panchenko (2002)).
a result in learning to rank literature.
Now, assuming to be twice differentiable, we need H
such that
8.2. Smooth losses
?

r

log d
+B
n

r

log(1/ )
n

We will again use online regret bounds to explain why we
should expect “optimistic” rates for smooth losses before
giving more general results for smooth but possibly nonconvex losses.

H

kr2w (X (i) w, y (i) )k2!2 = kX > r2s (X (i) w, y (i) )Xk2!2

where we used the chain rule to express r2w in terms of
r2s . Note that, for OGD, we need smoothness in w w.r.t.

Generalization error bounds for learning to rank

k · k2 which is why the matrix norm above is the operator
norm corresponding to the pair k·k2 , k·k2 . In fact, when we
say “operator norm” without mentioning the pair of norms
involved, it is this norm that is usually meant. It is well
known that this norm is equal to the largest singular value
of the matrix. But, just as before, we can bound this in
terms of the smoothness constant of w.r.t. k · k1 (see
Section I in the appendix):
k(X (i) )> r2s (X (i) w, y (i) )X (i) k2!2

2
 RX
kr2s (X (i) w, y (i) )k1!1 .

where we used Lemma 4 once again. This result using online regret bounds is great for building intuition but suffers
from the two defects we mentioned at the end of Section 5.
In the smoothness case, it additionally suffers from a more
serious defect: the correct choice of the learning rate ⌘ requires knowledge of L (w? ) which is seldom available.
8.4. Generalization error bounds under smoothness
Once again, to prove a general result for possibly nonconvex smooth losses, we will adopt an approach based on
covering numbers. To begin, we will need a useful lemma
from Srebro et al. (2010, Lemma A.1 in the Supplementary
Material). Note that, for functions over real valued predictions, we do not need to talk about the norm when dealing
with smoothness since essentially the only norm available
is the absolute value.
Lemma 13. For any h-smooth non-negative function f :
R ! R+ and any t, r 2 R we have
(f (t)

f (r))2  6h(f (t) + f (r))(t

r)2 .

We first provide an extension of this lemma to the vector
case.
Lemma 14. If : Rm ! R+ is a non-negative function
with smoothness constant H w.r.t. a norm ||| · ||| then for
any s1 , s2 2 Rm we have
( (s1 )

(s2 ))2  6H · ( (s1 ) + (s2 )) · |||s1

s2 |||2 .

Control of covering numbers easily gives a control on the
Rademacher complexity of the random subclass F ,2 (r).

Corollary 16. Let be smooth w.r.t. k · k1 with constant
H and uniformly bounded by B for w 2 F2 . Then the
empirical Rademacher complexity of the class F ,2 (r) is
bounded as
p
p
b n (F ,2 (r))  4 rC log 3 B
R
C
q
p
H log2 (3mn)
.
where C = 5 3W2 RX
n
With the above corollary in place we can now prove our
second key result.
Theorem 17. Suppose is smooth w.r.t. k · k1 with constant H and is uniformly bounded by B over F2 . With
probability at least 1
,
!
r
L (w)D0
D0
8w 2 F2 , L (w)  L̂ (w) + Õ
+
n
n
2
where D0 = B log(1/ ) + W22 RX
H . Moreover, with
probability at least 1 2 ,
!
r
L (w? )D0
D0
?
L (ŵ)  L (w ) + Õ
+
n
n

where ŵ, w? are minimizers of L̂ (w) and L (w) respectively (over w 2 F2 ).

9. Conclusion
We showed that it is not necessary for generalization error
bounds for query-level learning to rank algorithms to deteriorate with increasing length of document lists associated
with queries. The key idea behind our improved bounds
was defining Lipschitz constants w.r.t. `1 norm instead of
the “standard” `2 norm. As a result, we were able to derive
much tighter guarantees for popular loss functions such as
ListNet and Smoothed DCG@1 than previously available.

Our generalization analysis of learning to rank algorithms
paves the way for further interesting work. One possibilUsing the basic idea behind local Rademacher complexity
ity is to use these bounds to design active learning algoanalysis, we define the following loss class:
rithms for learning to rank with formal label complexity
F ,2 (r) := {(X, y) 7! (Xw, y) : kwk2  W2 , L̂ (w)  r}. guarantees. Another interesting possibility is to consider
other problems, such as multi-label learning, where funcNote that this is a random subclass of functions since
tions with vector-valued outputs are learned by optimizing
L̂ (w) is a random variable.
a joint function of those outputs.
Proposition 15. Let be smooth w.r.t. k · k1 with constant
(1:n)
H . The covering numbers of F ,2 (r) in the dZ
metric
2
Acknowledgement
defined above are bounded as follows:
We gratefully acknowledge the support of NSF under grant
⇠
⇡
2
12H W22 RX
r
(1:n)
log2 N2 (✏, F ,2 (r), Z
)
log2 (2mn+1). IIS-1319810. Thanks to Prateek Jain for discussions that
✏2
led us to Theorem 3.

Generalization error bounds for learning to rank

References
Bartlett, Peter L. and Mendelson, Shahar. Rademacher and
Gaussian complexities: Risk bounds and structural results. The Journal of Machine Learning Research, 3:
463–482, 2003.
Bousquet, Olivier. Concentration inequalities and empirical processes theory applied to the analysis of learning
algorithms. PhD thesis, Ecole Polytechnique, 2002.
Cao, Zhe, Qin, Tao, Liu, Tie-Yan, Tsai, Ming-Feng, and Li,
Hang. Learning to rank: from pairwise approach to listwise approach. In Proceedings of the 24th International
Conference on Machine Learning, pp. 129–136, 2007.
Chapelle, Olivier and Wu, Mingrui. Gradient descent optimization of smoothed information retrieval metrics. Information retrieval, 13(3):216–235, 2010.
Chapelle, Olivier, Metlzer, Donald, Zhang, Ya, and
Grinspan, Pierre. Expected reciprocal rank for graded
relevance. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management, pp. 621–
630. ACM, 2009.
Chapelle, Olivier, Chang, Yi, and Liu, Tie-Yan. Future directions in learning to rank. In Proceedings of the Yahoo! Learning to Rank Challenge June 25, 2010, Haifa,
Israel, Journal of Machine Learning Research Workshop
and Conference Proceedings, pp. 91–100, 2011.
Järvelin, Kalervo and Kekäläinen, Jaana. Cumulated gainbased evaluation of IR techniques. ACM Transactions
on Information Systems, 20(4):422–446, 2002.
Joachims, Thorsten. Optimizing search engines using
clickthrough data. In Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 133–142. ACM, 2002.
Lan, Yanyan, Liu, Tie-Yan, Qin, Tao, Ma, Zhiming, and Li,
Hang. Query-level stability and generalization in learning to rank. In Proceedings of the 25th International
Conference on Machine Learning, pp. 512–519. ACM,
2008.
Lan, Yanyan, Liu, Tie-Yan, Ma, Zhiming, and Li, Hang.
Generalization analysis of listwise learning-to-rank algorithms. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 577–584,
2009.
Liu, Tie-yan, Xu, Jun, Qin, Tao, Xiong, Wenying, and Li,
Hang. LETOR: Benchmark dataset for research on learning to rank for information retrieval. In Proceedings of
SIGIR 2007 Workshop on Learning to Rank for Information Retrieval, pp. 3–10, 2007.

Mendelson, Shahar. Rademacher averages and phase transitions in Glivenko-Cantelli classes. IEEE Transactions
on Information Theory, 48(1):251–263, 2002.
Panchenko, Dmitriy. Some extensions of an inequality of
Vapnik and Chervonenkis. Electronic Communications
in Probability, 7:55–65, 2002.
Shalev-Shwartz, Shai, Shamir, Ohad, Srebro, Nathan, and
Sridharan, Karthik. Stochastic convex optimization. In
Proceedings of the 22nd Annual Conference on Learning
Theory, 2009.
Srebro, Nathan, Sridharan, Karthik, and Tewari, Ambuj.
Smoothness, low noise, and fast rates. In Advances in
Neural Information Processing Systems 23, pp. 2199–
2207, 2010.
Yue, Yisong, Finley, Thomas, Radlinski, Filip, and
Joachims, Thorsten. A support vector method for optimizing average precision. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, pp. 271–278,
2007.
Zhang, Tong. Covering number bounds of certain regularized linear function classes. The Journal of Machine
Learning Research, 2:527–550, 2002.

