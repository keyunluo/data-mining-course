Exponential Family Matrix Completion under Structural Constraints

Suriya Gunasekar
Pradeep Ravikumar
Joydeep Ghosh
The University of Texas at Austin, Texas, USA

Abstract
We consider the matrix completion problem of
recovering a structured matrix from noisy and
partial measurements. Recent works have proposed tractable estimators with strong statistical
guarantees for the case where the underlying matrix is low‚Äìrank, and the measurements consist
of a subset, either of the exact individual entries,
or of the entries perturbed by additive Gaussian
noise, which is thus implicitly suited for thin‚Äì
tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data‚Äìtypes, such as
skewed‚Äìcontinuous, count, binary, etc., (b) for
heterogeneous noise models (beyond Gaussian),
which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low‚Äìrank, such as block‚Äìsparsity,
or a superposition structure of low‚Äìrank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for
generalized matrix completion by considering a
matrix completion setting wherein the matrix entries are sampled from any member of the rich
family of exponential family distributions; and
impose general structural constraints on the underlying matrix, as captured by a general regularizer R(.). We propose a simple convex regularized M ‚Äìestimator for the generalized framework, and provide a unified and novel statistical
analysis for this general class of estimators. We
finally corroborate our theoretical results on simulated datasets.

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

SURIYA @ UTEXAS . EDU
PRADEEPR @ CS . UTEXAS . EDU
GHOSH @ ECE . UTEXAS . EDU

1. Introduction
In the general problem of matrix completion, we seek to
recover a structured matrix from noisy and partial measurements. This problem class encompasses a wide range
of practically important applications such as recommendation systems, recovering gene‚Äìprotein interactions, and
analyzing document collections in language processing,
among others. In recent years, leveraging developments in
sparse estimation and compressed sensing, there has been
a surge of work on computationally tractable estimators
with strong statistical guarantees, specifically for the setting where a subset of entries of a low‚Äìrank matrix are
observed either deterministically, or perturbed by additive
noise that is Gaussian (Candes & Plan, 2010), or more generally sub‚ÄìGaussian (Keshavan et al., 2010b; Negahban &
Wainwright, 2012). While such a Gaussian noise model is
amenable to the subtle statistical analyses required for the
ill‚Äìposed problem of matrix completion, it is not always
practically suitable for all data settings encountered in matrix completion problems. For instance, such a Gaussian
error model might not be appropriate in recommender systems based on movie ratings that are either binary (likes or
dislikes), or range over the integers one through five. The
first question we ask in this paper is whether we can generalize the statistical estimators for matrix completion as
well as their analyses to general noise models? Note that a
noise model captures the uncertainty underlying the matrix
measurements, and is thus an important component of the
problem specification given any application; and it is thus
vital for broad applicability of the class of matrix completion estimators to extend to general noise models.
Though this might seem like a narrow technical, although
important question, it is related to a broader issue. A
Gaussian observation model implicitly assumes the matrix values are continuous‚Äìvalued (and that they are thin‚Äì
tail‚Äìdistributed). But in modern applications, matrix data
span the gamut of heterogeneous data‚Äìtypes, for instance,
skewed‚Äìcontinuous, categorical‚Äìdiscrete including binary,
count‚Äìvalued, among others. This, thus gives rise to the
second question of whether we can generalize the stan-

Generalized Matrix Completion

dard matrix completion estimators and statistical analyses,
suited for thin‚Äìtailed continuous data, to more heterogeneous data‚Äìtypes? Note that there has been some recent
work for the specific case of binary data by Davenport et al.
(2012), but generalizations to other data‚Äìtypes and distributions is largely unexplored.
The recent line of work on matrix completion, moreover,
enforces the constraint that the underlying matrix be either
exactly or approximately low‚Äìrank. Aside from the low‚Äì
rank constraints, further assumptions to eliminate overly
‚Äúspiky‚Äù matrices are required for well‚Äìposed recovery under partial measurements (Candes & Recht, 2009). Early
work provided generalization error bounds for various low‚Äì
rank matrix completion algorithms, including algorithms
based on nuclear norm minimization (Candes & Recht,
2009; Candes & Tao, 2010; Candes & Plan, 2010; Recht,
2011), max‚Äìmargin matrix factorization (Srebro et al.,
2004), spectral algorithms (Keshavan et al., 2010a;b), and
alternating minimization (Jain et al., 2013). These work
made stringent matrix incoherence assumptions to avoid
‚Äúspiky‚Äù matrices. These assumptions have been made less
stringent in more recent results (Negahban & Wainwright,
2012), which moreover extend the guarantees to approximately low‚Äìrank matrices. Such (approximate) low‚Äìrank
structure is one instance of general structural constraints
which are now understood to be necessary for consistent statistical estimation under high‚Äìdimensional settings
(with very large number of parameters and very few observations). Note that the high‚Äìdimensional matrix completion problem is particularly ill‚Äìposed, since the measurements are typically both very local (e.g. individual matrix
entries), and partial (e.g. covering a decaying fraction of
entries of the entire matrix). However, the specific (approximately) low‚Äìrank structural constraint imposed in the past
work on matrix completion does not capture the rich variety of other qualitatively different structural constraints
such as row‚Äìsparseness, column‚Äìsparseness, or a superposition structure of low‚Äìrank plus elementwise sparseness,
among others. For instance, in the classical introductory
survey on matrix completion (Laurent, 2009), the authors
discuss structural constraints of a contraction matrix, and a
Euclidean distance matrix. Thus, the third question we ask
in this paper is whether we can generalize the recent line of
work on low‚Äìrank matrix completion to the more general
structurally constrained case.
In this paper, we answer all of the three questions above
in the affirmative, and provide a vastly unified framework
for generalized matrix completion. We address the first two
questions by considering a general matrix completion setting wherein observed matrix entries are sampled from any
member of a rich family of natural exponential family distributions. Note that this family of distributions encompass a wide variety of popular distributions including Gaus-

sian, Poisson, binomial, negative‚Äìbinomial, Bernoulli, etc.
Moreover, the choice of the exponential family distribution can be made depending on the form of the data. For
instance, thin‚Äìtailed continuous data is typically modeled
using the Gaussian distribution; count‚Äìdata is modeled
through an appropriate distribution over integers (Poisson,
binomial, etc.), binary data through Bernoulli, categorical‚Äì
discrete through multinomial, etc. We address the last
question by considering general structural constraints upon
the underlying matrix, as captured by a general regularization function R(.). Our general matrix completion setting
thus captures heterogeneous noise‚Äìchannels, for heterogeneous data‚Äìtypes, and heterogeneous structural constraints.
In a key contribution, we propose a simple regularized
convex M ‚Äìestimator for recovering the structurally constrained underlying matrix in this general setting; and
moreover provide a unified and novel statistical analysis
for our general matrix completion problem. Following a
standard approach (Negahban, 2012), we (a) first showed
that the negative log‚Äìlikelihood of the subset of observed
entries satisfies a form of Restricted Strong Convexity
(RSC) (Definition 4); and (b) under this RSC condition,
our proposed M ‚Äìestimator satisfies strong statistical guarantees. We note that proving these individual components
for our general matrix completion problem under general
structural constraints required a fairly delicate and novel
analysis, particularly the first component of showing the
RSC condition, which we believe would be of independent
interest. A key corollary of our general framework is matrix completion under sub‚ÄìGaussian samples and low‚Äìrank
constraints, where we show that our theorem recovers results comparable to the existing literature (Candes & Plan,
2010; Keshavan et al., 2010b; Negahban & Wainwright,
2012). Finally, we corroborate our theoretical findings via
simulated experiments.
1.1. Notations and Preliminaries
In this subsection we describe the notations and definitions
frequently used throughout the paper. Matrices are denoted
by capital letters, X, Œò, M , etc. For a matrix M , Mj and
M (i) are the j th column and ith row of M respectively,
and Mij denotes the (i, j)th entry of M . The transpose,
trace, and rank of a matrix M are denoted by M ‚Ä† , tr(M ),
and rk(M ), respectively. The inner product
Pbetween two
matrices is given by hX, Y i = tr(X ‚Ä† Y ) = (i,j) Xij Yij .
For a matrix M ‚àà Rm√ón of rank r, with singular values œÉ1 ‚â• œÉ2 ‚â• . . . œÉr , commonly
Pused matrix norms include the nuclear norm kM k‚àó = i œÉi , theP
spectral norm
kM k2 = œÉ1 , the Frobenius norm kM kF = i œÉi2 , and the
maximum norm kM kmax = max(i,j) Mij .
Given any matrix norm k ¬∑ k, the dual norm, k ¬∑ k‚àó is given
by kXk‚àó = supkY k‚â§1 hX, Y i.

Generalized Matrix Completion

Definition 1 (Natural Exponential Family). A distribution
of a random variable X, in a normed vector space is said to
belong to the natural exponential family, if its probability
density function, characterized by the parameter Œò in the
dual vector space, is given by: 

P (X|Œò) = h(X) exp hX, Œòi ‚àí G(Œò) ,
R
where G(Œò) = log X h(X)ehX,Œòi dX, called the log‚Äì
partition function, is strictly convex, and analytic.
Definition 2 (Bregman Divergence). Let œÜ : dom(œÜ) ‚Üí R
be a strictly convex function differentiable in the relative
interior of dom(œÜ). The Bregman divergence (associated
with œÜ) between x ‚àà dom(œÜ) and y ‚àà ri(dom(œÜ)) is defined as:
BœÜ (x, y) = œÜ(x) ‚àí œÜ(y) ‚àí h‚àáœÜ(y), x ‚àí yi.
Definition 3 (Subspace compatibility constants). Given a
matrix norm R(.), we define the following maximum and
minimum subspace compatibility constants of R(.) w.r.t
the subspace M:
R(Œò)
R(Œò)
, Œ®min (R) = inf
Œ®(M; R) = sup
.
kŒòk
kŒòk
Œò6
=
{0}
F
F
Œò‚ààM\{0}
Thus, ‚àÄŒò ‚àà M
Œ®min (R)kŒòkF ‚â§ R(Œò) ‚â§ Œ®(M, R)kŒòkF .

bution in (1); Y
it can be seen that:


		
P (X|Œò‚àó ) =
h(Xij ) exp Xij Œò‚àóij ‚àí G(Œò‚àóij )
ij

= h(X) exp {hX, Œò‚àó i ‚àí G(Œò‚àó )} ,
(2)
m√ón
where we overload
the
notation
to
denote
G
:
R
‚Üí
R
P
as G(Œò) Q
=
G(Œò
),
and
the
base
measure
h(X)
as
ij
ij
h(X) = ij h(xij ).
Uniformly Sampled Observations: In a ‚Äúfully observed‚Äù
setting, we would observe all the entries of the observation
matrix X ‚àà Rm√ón . However, we consider a partially observed setting, where we observe entries over a subset of
indices ‚Ñ¶ ‚äÇ [m] √ó [n]. We assume a uniform sampling
model, so that
‚àÄ (i, j) ‚àà ‚Ñ¶, i ‚àº uniform([m]), j ‚àº uniform([n]). (3)
Given, ‚Ñ¶, wedefine the following matrix P‚Ñ¶ (X):
Xij if (i, j) ‚àà ‚Ñ¶
P‚Ñ¶ (X)ij =
0
otherwise.
The matrix completion task can then be stated as the estimation of Œò‚àó from the partially observed matrix P‚Ñ¶ (X),
where X ‚àº P (X|Œò‚àó ). As noted earlier, this problem is ill‚Äì
posed in general. However, as we will show, under structural constraints imposed on the parameter matrix Œò‚àó , we
are able to design an M ‚Äìestimator with a near optimal deviation from Œò‚àó .
2.1. Applications

Definition 4 (Restricted Strong Convexity). A loss function L is said to satisfy Restricted Strong Convexity with
respected to a subspace S, if for some Œ∫L > 0,
L(Œò + ‚àÜ) ‚àí L(Œò) ‚àí h‚àáL(Œò), ‚àÜi ‚â• Œ∫L k‚àÜk2F , ‚àÄ‚àÜ ‚àà S.

Definition 5 (Sub‚ÄìGaussian Distributions). A random
variable, X, is said to have a sub‚ÄìGaussian distribution
with parameter b, if ‚àÄ s > 0, the distribution satisfies
2 2
E[esX ] ‚â§ es b /2 . Further, if X is sub‚ÄìGaussian with parameter b, then E[X] = 0 and V ar(X) ‚â§ b2 (refer Vershynin (2010)).

2. Exponential Family Matrix Completion
Denote the underlying target matrix by Œò‚àó ‚àà Rm√ón . We
then assume that individual entries Œò‚àóij are observed indirectly via a noisy channel: specifically, via a sample
drawn from the corresponding member of natural exponential family (see Definition 1): 
	
P (Xij |Œò‚àóij ) = h(Xij ) exp Xij Œò‚àóij ‚àí G(Œò‚àóij ) , (1)
where G : R ‚Üí R, is a strictly convex, and analytic function called the log‚Äìpartition function.
Consider the random matrix X ‚àà Rm√ón , where each entry
Xij is drawn independently from the corresponding distri-

Gaussian (fixed œÉ 2 ) is typically used to model continuous
data, x ‚àà R, such as measurements with additive errors,
affinity datasets. Here, G(Œ∏) = 12 œÉ 2 Œ∏2 .
Bernoulli is a popular distribution of choice to model binary data, x ‚àà {0, 1}, with G(Œ∏) = log (1 + eŒ∏ ). Some
examples of data suitable for Bernoulli model include social networks, gene protein interactions, etc.
Binomial (fixed N ) is used to model number of successes
in N trials. Here, x ‚àà {0, 1, 2, . . . , N }, and G(Œ∏) =
N log (1 + eŒ∏ ). Some applications include predicting success/failure rate, survey outcomes, etc.
Poisson is used to model count data x ‚àà {0, 1, 2, . . .}, such
as arrival times, events per unit time, click‚Äìthroughs among
others. Here, G(Œ∏) = eŒ∏ .
Exponential is often used to model positive valued continuous data x ‚àà R+ , specially inter arrival times between
events. Here, G(Œ∏) = ‚àí log (‚àíŒ∏).
2.2. Log‚Äìlikelihood
Denote the gradient map:
‚àÇG(Œò)
.
‚àÇŒ∏ij
It can then be verified that the mean and variance of the
distribution P (X|Œò‚àó ) are given by E[X] = g(Œò‚àó ), and
g(Œò) , ‚àáG(Œò) ‚àà Rm√ón , where g(Œò)ij =

Generalized Matrix Completion

Var(X) = ‚àá2 G(Œò‚àó ), respectively. The Fenchel conjugate
of the log partition function G, is denoted by: F (X) ,
supŒò hX, Œòi ‚àí G(Œò).
A useful consequence of the exponential family is that the
negative log‚Äìlikelihood is convex in the natural parameters
Œò‚àó , and moreover has a bijection with a large class of Bregman divergences (Definition 2). The following relationship
was first noted by (Forster & Warmuth, 2002), and later
established by (Banerjee et al., 2005) [Theorem 4]:
‚àí log P (X|Œò) ‚àù BF (X, g(Œò)), ‚àÄX ‚àà dom(F). (4)
2.3. Discussion and directions for future work
We consider the standard matrix‚Äìcompletion setting where
the distribution of the observation matrix X in (2) corresponds to its entries Xij being drawn independently from
the other entries. Further, the probability of observing a
specific entry Xij , under uniform sampling is independent
of the noise channel or the distribution P (Xij |Œò‚àóij ). However, in some applications, it might be beneficial to have
a sampling scheme involving dependencies; for instance,
when a user gives a movie a bad rating, we might want to
vary the sampling scheme to sample an entirely different
region of the matrix. It would be interesting to extend the
analysis of this paper to such a dependent sampling setting.
The form of the observation random matrix distribution in
(2), given the individual observations in (1), can be seen
to have connotations of multi‚Äìtask learning: here recovering each individual matrix entry Œò‚àóij together with its corresponding noise model forms a single task, and all these
tasks can be performed jointly given the shared structural
constraint on Œò‚àó . It would be interesting to generalize this
to form a more general statistical framework for partial
multi-task learning.
We use the general class of exponential family distributions as the underlying probabilistic model capturing the
measurement uncertainties. The richness of the class of exponential family distributions has been used in other settings to provide general statistical frameworks. Kakade
et al. (2010) provide a generalization of compressed sensing problem to general exponential family distributions.
Note however that results from compressed sensing cannot be immediately extended to matrix completion case,
since the sampling operator P‚Ñ¶ does not satisfy the typical assumptions (restricted isometry or restricted eigenvalues) made in such settings; see (Candes & Recht, 2009)
for additional discussion. There have been extensions
of classical probabilistic PCA (Tipping & Bishop, 1999)
from Gaussian noise models to exponential family distributions Collins et al. (2001); Mohamed et al. (2008); Gordon
(2002). There have also been recent extensions of probabilistic graphical model classes, beyond Gaussian and Ising
models, to multivariate extensions of exponential family

distributions (Yang et al., 2012; 2013).
More complicated probabilistic models have also been proposed in the context of collaborative filtering (Mnih &
Salakhutdinov, 2007; Salakhutdinov & Mnih, 2008), but
these typically involve non‚Äìconvex optimization, and it is
difficult to extend the rigorous statistical analyses of the
form in this paper (and in the matrix completion literature)
to these models.

3. Main Result and Consequences
As noted in the introduction, we consider the matrix completion setting with general structural constraints on the
underlying target matrix Œò‚àó . To formalize the notion of
such structural constraints, we follow (Negahban, 2012),
and assume that Œò‚àó satisfies Œò‚àó ‚àà M ‚äÜ M ‚äÇ Rm√ón , for
some subspace M ‚äÜ M, which contains parameter matrices that are structured similar to the target (the corresponding structural constraints such as low rankness, low rankness+sparsity etc); we also allow the flexibility of working with a superset M of the model subspace that is potentially easier to analyze. Moreover, we use their definition of a decomposable norm regularization function,
R(.) : Rm√ón ‚Üí R+ , which suitably captures these structural constraints:
A 1. (Decomposable Norm Regularizer) We assume
that R(.) is a matrix norm, and is decomposable over
‚ä•
‚ä•
(M, M ), i.e. if X ‚àà M, Y ‚àà M , then,
R(X + Y ) = R(X) + R(Y ).
We provide some examples of such decomposable regularizers and structural constraint subspaces, and refer to (Negahban, 2012) for more examples and discussion.
Example 1. Low‚Äìrank: This is a common structure assumed in numerous matrix estimation problems, particularly those in collaborative filtering, PCA, spectral clustering, etc. The corresponding structural constraint subspaces
‚ä•
(M, M ) in this case correspond to a linear span of specific one‚Äìrank matrices; we discuss these in further detail
in Section 3.2, where we derive a corollary of our general
theorem to the specific case of recovery guarantees for low‚Äì
rank constrained matrix
P completion. The nuclear norm
R(Œò) = kŒòk‚àó = k œÉk , has been shown to be decomposable with respect to these constraint subspaces (Fazel
et al., 2001).
Example 2. Block sparsity: Another important structural
constraint for a matrix is block‚Äìsparsity, where each row is
either all zeros or mostly non‚Äìzero, and the number of non‚Äì
zero rows is small. The structural constraint subspaces in
this case correspond to a linear span of specific Frobenius‚Äì
norm‚Äìone matrices that are non‚Äìzero in a small subset of
the rows (dependent on Œò‚àó ); it has been shown that `1 /`q

Generalized Matrix Completion

(q > 1) norms (Negahban & Wainwright, 2008; Obozinski
et al., 2011) are decomposable with respect to such structural constraint subspaces. Recalling that Œò(i) is the ith
row of Œò, the `1 /`q norm is defined as:
m
m h X
n
1/q i
X
X
kŒòk1,q =
kŒò(i) kq =
.
|Œòij |q
i=1

i=1

j=1

Example 3. Low‚Äìrank plus sparse: This structure is often used to model low‚Äìrank matrices which are corrupted
by a sparse outlier noise matrix. The structural constraint
subspaces corresponding to these consist of the linear span
of weighted sum of specific rank‚Äìone matrices and sparse
matrices with non‚Äìzero components on specified positions;
and appropriate regularization function decomposable with
respect to such structural constraints is the infimum convolution of the weighted nuclear
P norm with weighted elementwise `1 norm, kM k1,1 = ij |Mij | (Candes et al., 2011;
Yang & Ravikumar, 2013):
R(Œò) = inf{Œª1 kSk1,1 + Œª2 kLk‚àó : Œò = S + L}.

to satisfy A 1. For a suitable
Œª > 0,
i
mn h X
b
Œò = argmin
‚àí log P (Xij |Œòij ) + ŒªR(Œò)
‚àó
kŒòkmax ‚â§ ‚àöŒ±mn |‚Ñ¶| ij‚àà‚Ñ¶
i
mn h X
G(Œòij ) ‚àí Xij Œòij + ŒªR(Œò).
= argmin
‚àó
|‚Ñ¶|
kŒòkmax ‚â§ ‚àöŒ±
ij‚àà‚Ñ¶

mn

(6)
The above optimization problem is a convex program, and
can be solved by any off‚Äìthe shelf convex solvers.
3.2. Main Results
Without loss of generality, suppose that m ‚â§ n. Let
R‚àó (.) = supR(X)‚â§1 hX, .i be the dual norm of the regularizer R(.). Further, let Œ®(M) and Œ®min be the maximum and minimum subspace compatibility constants of R
w.r.t the model subspace M (Definition 3)‚àó . We next define
the following quantity: ‚àö
h mn  X
i
Œ∫R (n, |‚Ñ¶|) := E
R‚àó
ij ei e‚àój ,
|‚Ñ¶|
ij‚àà‚Ñ¶

The second assumption we make is on the curvature of the
log‚Äìpartition function. This is required to establish a form
of RSC (Definition 4) for the loss function.
A 2. The second derivative of the log‚Äìpartition function
G : R ‚Üí R has atmost an exponential decay, i.e,
‚àá2 G(u) ‚â• e‚àíŒ∑|u| , ‚àÄ u ‚àà R, for some Œ∑ > 0
It can be verified that commonly used members of natural
exponential family obey this assumption.
Finally, we make an assumption to avoid ‚Äúspiky‚Äù target
matrices. As Candes & Recht (2009) show with numerous examples, low‚Äìrank and presumably other such structural constraints as above, by themselves are not sufficient
for accurate recovery, in part due to the infeasibility of recovering overly ‚Äúspiky‚Äù matrices with very few large entries. Early work (Candes & Plan, 2010; Keshavan et al.,
2010a;b), assumed stringent matrix incoherence conditions
to preclude such matrices, while more recent work (Davenport et al., 2012; Negahban & Wainwright, 2012), relax
these assumptions to restricting the spikiness ratio, defined
as follows:
‚àö
mnkŒòkmax
.
(5)
Œ±sp (Œò) =
kŒòkF
A 3. There exists a known Œ±‚àó > 0, such that
Œ±sp (Œò‚àó ) ‚àó
Œ±‚àó
kŒò‚àó kmax = ‚àö
kŒò kF ‚â§ ‚àö
.
mn
mn
3.1. M ‚Äìestimator for Generalized Matrix Completion
We propose a regularized M ‚Äìestimate as our candidate pab The norm regularizer R(.) used is a conrameter matrix Œò.
vex surrogate for the structural constraints, and is assumed

where the expectation is over the random sampling index
set ‚Ñ¶, and over the Rademacher sequence {ij : ‚àÄ(i, j) ‚àà
‚Ñ¶}; here {ei ‚àà Rm }, {ej ‚àà Rn } are the standard basis.
This quantity Œ∫R (n, |‚Ñ¶|) captures the interaction between
the sampling scheme and the structural constraint as captured by the regularizer (specifically its dual R‚àó ). Note
that it depends only on n (n ‚â• m), and on the size |‚Ñ¶| of
‚Ñ¶.
b be the estimate from (6) with Œª ‚â•
Theorem 1. Let Œò
2
mn ‚àó
‚àó
|‚Ñ¶| R (P‚Ñ¶ (X ‚àí g(Œò )). Under the assumptions A1-3, if
|‚Ñ¶| = ‚Ñ¶(Œ®2 (M)n log n)‚Ä† , then given a constant c0 , ‚àÉ
constants C, C1 , C2 , and K1 , such that, with probability
> 1 ‚àí C1 e‚àíC2 Œ®min n log n :


‚àó 2
b
kŒò‚àíŒò
kF ‚â§ C max{Œ±‚àó2 , 1}Œ®2 (M) max
‚àó

provided Œ∫L := e

‚àö
‚àí 2Œ∑Œ±
mn



K1 ‚àí

64
c0

q

Œª2 c20 n log n
,
Œ∫2L
|‚Ñ¶|

|‚Ñ¶|Œ∫2R (n,|‚Ñ¶|)
n log n



,

> 0.

In the above theorem, Œ∑ and Œ±‚àó ‚â• Œ±sp (Œò‚àó )kŒò‚àó kF are constants from Assumptions 2 and 3, respectively. Our proof
uses elements from Negahban (2012), as well as Negahban & Wainwright (2012) where they analyze the case of
low‚Äìrank structure and additive noise, and establish a form
of restricted strong convexity (RSC) for squared loss over
subset of matrix entries (closely relates to the special case,
when the exponential family distribution assumed in (2) is
Gaussian). However, showing such an RSC condition for
our general loss function over a subset of structured matrix
entries involved some delicate arguments. Further, we provide a much simpler proof that moreover only required a
low‚Äìspikiness condition rather than a multiplicative spik‚àó

We suppress the dependence of Œ® and Œ®min on R in our
notation to avoid notational clutter
‚Ä†
f (n) = ‚Ñ¶(g(n)) if f (n) > kg(n) ‚àÄn > nÃÇ.

Generalized Matrix Completion

iness and structural constraint. Moreover, we are able to
provide an RSC condition broadly for general structures,
and the negative log‚Äìlikelihood loss associated with general exponential family distribution.
3.3. Corollary
An important special case of the problem is when the parameter matrix Œò‚àó , is assumed to be of a low‚Äìrank r  m.
The most commonly used convex regularizer to enforce
low‚Äìrank is the nuclear norm. The intuition behind the
low‚Äìrank assumption on the parameter matrix is as follows:
the parameter Œò‚àóij , can be thought of as the true measure of
affinity between the entities corresponding to row i and column j, respectively; and the data Xij , is a sample from a
noisy channel parametrized by Œòij . It is hypothesized that
{Œò‚àóij }, are obtained from a weighted interaction of a small
Pr
number of latent variables as, Œò‚àóij = k=1 œÉk uik vjk .
Let {uk ‚àà Rm } and {vk ‚àà Rn }, k ‚àà [r] be the left and
right singular vectors, respectively of Œò‚àó . Let the column
and row span of Œò‚àó be U ‚àó , col(Œò‚àó ) = span{ui } and
V ‚àó , row(Œò‚àó ) = span{vj }, respectively. Define:
M := {Œò : row(Œò) ‚äÜ V ‚àó , col(Œò) ‚äÜ U ‚àó }, and
(7)
‚ä•
M := {Œò : row(Œò) ‚äÜ V ‚àó‚ä• , col(Œò) ‚äÜ U ‚àó‚ä• }.
It can be verified that, M =
6 M, however, M ‚äÇ M.
Corollary 1. Let Œò‚àó be a low‚Äìrank matrix of rank atmost
r  m. If further, ‚àÄ(i, j), (Xij ‚àí g(Œò‚àóij )) is sub‚ÄìGaussian
(Definition 5) with parameter b, and |‚Ñ¶| >
q ‚Ñ¶(rn log n),
‚àö
log n
Œª
then using R(.) = k.k‚àó and 2 := C mnb n |‚Ñ¶|
in (6),
0

w.p. > 1 ‚àí C10 e‚àíC2 log n ,


‚àó2
2
1 b
rn log n
‚àó 2
0 max{Œ± , 1}b
kŒò ‚àí Œò kF ‚â§ C
,
mn
Œ∫02
|‚Ñ¶|
L
‚àó

where Œ∫0L = K10 e

‚àö
‚àí 2Œ∑Œ±
mn

.

Remark 1: Note that the above results hold for the minb of the convex program in (6), optimized for any
imizer Œò
‚àó
‚àó
Œ± ‚â• Œ±sp (Œò‚àó )kŒò‚àó kF ; in particular it holds
‚àö with Œ± =
‚àó
‚àó
‚àó
Œ±sp (Œò )kŒò kF , where 1 ‚â§ Œ±sp (Œò ) ‚â§ mn. While in
practice Œ±‚àó is chosen through cross‚Äìvalidation, the theoretical bound in Corollary 1 can be tightened to the following
(if kŒòkF ‚â• 1):


b ‚àí Œò‚àó k2
Œ±2 (Œò‚àó )b2 rn log n
1 kŒò
0 sp
F
‚â§
C
. (8)
mn kŒò‚àó k2F
Œ∫02
|‚Ñ¶|
L
Similar bound can be obtained for Theorem 1.
Remark 2: The parameter b2 is a measure of noise per
entry in the system; ‚àÄij, Var(Xij ‚àí g(Œò‚àóij )) ‚â§ b2 .

4.1. Proof of Theorem 1
The proof of our main theorem involves two key steps:
‚Ä¢ We first show that, under assumptions A1-3, RSC of
the form in Definition 4 holds for the loss function
in (6) over a large subset of the solution space.
‚Ä¢ When the RSC condition holds, the result follows
from a few simple calculations; we handle the case
where RSC does not hold separately.
b =Œò
b ‚àí Œò‚àó . We state two results of interest.
Let ‚àÜ
Lemma 1. We define the following subset:
V = {Œò ‚àà Rm√ón : R(ŒòM‚ä• ) ‚â§ 3R(ŒòM )},
where recall that ŒòM is the projection of Œò onto the
b is the minimizer of (6), and Œª ‚â•
subspace M. If Œò
2
mn ‚àó
‚àó
‚àó
b
b
|‚Ñ¶| R (P‚Ñ¶ (X ‚àí g(Œò )), then ‚àÜ = Œò ‚àí Œò ‚àà V.
The proof follows from Lemma 1 of Negahban (2012).
b be the minimizer of (6). If Œª ‚â•
Lemma 2. Let Œò
2
mn ‚àó
‚àó
|‚Ñ¶| R (P‚Ñ¶ (X ‚àí g(Œò )), then:
mn X
b ij , Œò‚àóij ) ‚â§ 3ŒªŒ®(M) kŒò‚àó ‚àí Œòk
b F
BG (Œò
|‚Ñ¶|
2
(i,j)‚àà‚Ñ¶

The proof is provided in Appendix A.2.
‚àö

max
Recall the notation Œ±sp (‚àÜ) = mnk‚àÜk
. We now conk‚àÜkF
sider two cases, depending on whether the following condition holds for some constant c0 >
s0:

b ‚â§
Œ±sp (‚àÜ)

In this section, we provide key steps in the proofs of the
main results (Sec. 3.2-3.3).

(9)

Case 1: Suppose condition
in (9) does not hold; so that
q
|‚Ñ¶|
1
b
Œ±sp (‚àÜ) > c Œ®(M) n log n . From the constraints of
0
b max ‚â§
the optimization problem (6), we have that k‚àÜk
‚àö
‚àó
‚àó
b
kŒòkmax + kŒò kmax ‚â§ (2Œ± / mn). Thus,
‚àö

b F =
k‚àÜk

b max
mnk‚àÜk
‚â§ 2c0 Œ±‚àó
b
Œ±sp (‚àÜ)

s

Œ®2 (M)n log n
.
|‚Ñ¶|

(10)

Case 2: Suppose condition in (9) does hold. Then, the
following theorem shows that an RSC condition of the form
in Definition 4 holds.
Theorem 2 (Restricted Strong
Convexity). If for a given
q
|‚Ñ¶|
1
b
c0 , Œ±sp (‚àÜ) ‚â§
. then, under the asc0 Œ®(M)

n log n

sumptions and notations in Theorem 1, w.p.
C1 e‚àíC2 Œ®min n log n :
mn X
b ij , Œò‚àóij ) ‚â• Œ∫L k‚àÜk
b 2F ,
BG (Œò
|‚Ñ¶|

> 1 ‚àí

ij‚àà‚Ñ¶

‚àó

4. Proof

|‚Ñ¶|
.
n log n

1
c0 Œ®(M)

where Œ∫L = e

‚àö
‚àí 2Œ∑Œ±
mn


K1 ‚àí

64
c0

q

|‚Ñ¶|Œ∫2R (n,|‚Ñ¶|)
n log n


, K1 > 0.

As noted earlier, such an RSC result for the special case of
squared loss under low‚Äìrank constraints was shown in Negahban & Wainwright (2012). However, proving the RSC

Generalized Matrix Completion

condition for our general setting required a different and
more involved proof technique. We prove this theorem in
Section 4.3.
Remaining steps q
of the proof of Theorem 1: Thus, if
|‚Ñ¶|
1
b ‚â§
Œ±sp (‚àÜ)
n log n , and Œ∫L > 0, from Theorem 2
c0 Œ®(M)
and Lemma 2, w.h.p.:

From the equivalence of sub-Gaussian definitions in
Lemma 5.5 of Vershynin (2010), kXij ‚àí g(Œò‚àóij )kœÜ ‚â§ c0 b,
‚àö
‚àÄij. Since, Y (ij) has a single‚àöelement, mn(Xij ‚àí
g(Œò‚àóij )), we have, kY (ij) kœÜ ‚â§ c0 mnb. Further,
T

E[Y (ij) Y (ij) ] = E[mn(Xij ‚àí g(Œò‚àóij ))2 ej e‚àój ]
(a)

b 2F
Œ∫L k‚àÜk

= mnE(ij‚àà‚Ñ¶) [EX [(Xij ‚àí g(Œò‚àóij ))2 ]ej e‚àój ]

mn X
b ij , Œò‚àóij ) ‚â§ 3ŒªŒ®(M) k‚àÜk
b F (11)
‚â§
B G (Œò
|‚Ñ¶| ij‚àà‚Ñ¶
2

From (10) and (11), under assumptions of Theorem 1, we
have that for an appropriate constant C, with probability
higher than 1 ‚àí C1 e‚àíC2 Œ®min n log n ,
 2 2

b 2 ‚â§ C max{Œ±‚àó2 , 1}Œ®2 (M) max Œª , c0 n log n .
k‚àÜk
F
Œ∫2L
|‚Ñ¶|

(b)

1
(c)
‚â§ mnb2 E(ij‚àà‚Ñ¶) [ej e‚àój ] = mnb2 In√ón ,
n

(13)

where (a) follows from Fubini‚Äôs Theorem, (b) follows as (Xij ‚àí g(Œò‚àóij )) is sub‚ÄìGaussian, and (c)
follows from the uniform sampling model.
Simi(ij) (ij)T
2
2
larly, E[Y
Y
] = mnb Im√óm . Define œÉij :=
T

T

max{E[Y (ij) Y (ij) ], E[Y (ij) Y (ij) ]}
P
2
2
In Lemma A.3, using œÉij
‚â§ nb2 , œÉ 2 :=
ij‚àà‚Ñ¶ œÉij =
‚àö
2
n|‚Ñ¶|b , M = c0 mnb ‚â§ c0 nb, and t = |‚Ñ¶|Œ¥, we have:

4.2. Proof of Corollary 1
‚ä•

From the definition of M in (7), we have M =
span{ui x‚Ä† , yvj ‚Ä† : x ‚àà Rn , y ‚àà Rm }. Let PU ‚àó ‚àà Rm√óm
and PV ‚àó ‚àà Rn√ón , be the projection matrices onto the column and row spaces (U ‚àó , V ‚àó ) of Œò‚àó , respectively. We
have, ‚àÄX ‚àà Rm√ón , XM = PU ‚àó X + XPV ‚àó ‚àí PU ‚àó XPV ‚àó .
Also, rk(PU ‚àó ) = rk(PV ‚àó ) = rk(Œò‚àó ) = r. Thus, ‚àÄŒ¶ ‚àà M,
rk(Œ¶) ‚â§ 2r; and hence,
Œ®(M) =

‚àö
kŒ¶k‚àó
‚â§ 2r. Further, Œ®min ‚â• 1.
Œ¶‚ààM\{0} kŒ¶kF
sup

Next, we use the following proposition by Negahban &
Wainwright (2012), to bound Œ∫R (n, |‚Ñ¶|) in Theorem 1.
Lemma 3 (Lemma 6 of Negahban & Wainwright (2012)).
If ‚Ñ¶ ‚äÇ [m] √ó [n] is sampled using uniform sampling
and |‚Ñ¶| > n log n, then for a Rademacher sequence
{ij , ‚àÄ(i, j) ‚àà ‚Ñ¶},
s
h 1 X‚àö
i
n log n
k
mnij ei e‚àój k2 ‚â§ 10
.
E
|‚Ñ¶|
|‚Ñ¶|
ij‚àà‚Ñ¶

Thus,
q for large enough c0 > 640, using Œ∫R (n, |‚Ñ¶|) =
log n
10 n |‚Ñ¶|
in Theorem 2, for some K10 > 0 we have:

o
 1 X

n Œ¥2 |‚Ñ¶|

Œ¥|‚Ñ¶|
P 
Y (ij) 2 ‚â• Œ¥ ‚â§ n2 max e‚àí 4nb2 , e‚àí 2c0 nb .
|‚Ñ¶|
ij‚àà‚Ñ¶

q
log n
Further, for all C, using Œ¥ = Cb n |‚Ñ¶|
, there exists a
large enough C1 , s.t. if |‚Ñ¶| > C1 n log n, then,
P

s
!
‚àö
0
mn
n log n
‚àó
kP‚Ñ¶ (X ‚àí g(Œò ))k2 ‚â• Cb
‚â§ e‚àíC1 log n .
|‚Ñ¶|
|‚Ñ¶|
(14)
‚àó

‚àö
‚àí 2Œ∑Œ±

Using Œ®min ‚â• 1, Œ∫L = K10 e mn (from (12)), and Œª2 :=
q
‚àö
log n
C mnb n |‚Ñ¶|
in Theorem 1 leads to the corollary as
q
‚àö
log n
‚àó
w.h.p. Œª2 = C mnb n |‚Ñ¶|
‚â• mn
|‚Ñ¶| kP‚Ñ¶ (X ‚àí g(Œò ))k2 .
4.3. Proof of Theorem 2
This proof uses symmetrization arguments and contractions (Ledoux & Talagrand (1991) Ch.4&6). We observe
that, ‚àÄ (ij) ‚àà ‚Ñ¶, ‚àÉvij ‚àà [0, 1], s.t.
b ij , Œò‚àóij ) = G(Œò
b ij ) ‚àí G(Œò‚àóij ) ‚àí g(Œò‚àóij )(Œò
b ij ‚àí Œò‚àóij )
BG (Œò
(a)

Œ∫0L = e

‚àó
‚àö
‚àí 2Œ∑Œ±
mn



‚àó
640
‚àö
‚àí 2Œ∑Œ±
K1 ‚àí
‚â• K10 e mn .
c0

(15)

(12)

Finally, to prove the corollary, we derive a bound on
kP‚Ñ¶ (X ‚àí g(Œò‚àó ))k2 using the Ahlswede‚ÄìWinter Ma2
trix bound (Appendix
A.3). Let œÜ(x) = ex ‚àí 1;
‚àö
and let Y (ij) ,
mn(Xij ‚àí g(Œò‚àóij ))ei e‚àój , such that,
‚àö
P (ij)
mn
1
‚àó
Y
k2 .
|‚Ñ¶| kP‚Ñ¶ (X ‚àí g(Œò ))k2 = k |‚Ñ¶|
ij‚àà‚Ñ¶

2Œ∑Œ±‚àó

b ij )‚àÜ
b 2ij ‚â• e‚àí ‚àömn ‚àÜ
b 2ij .
= ‚àá2 G((1 ‚àí vij )Œò‚àóij + vij Œò

b ij | ‚â§ kŒò‚àó kmax +
where (a) holds as |(1 ‚àí vij )Œò‚àóij + vij Œò
‚àó
2Œ±
2
‚àíŒ∑|u|
b max ‚â§ ‚àö , and ‚àá G(u) ‚â• e
kŒòk
(A2).
mn

Lemma 4. Under Theorem 2, consider the subset,
1
E = ‚àÜ ‚àà V : Œ±sp (‚àÜ) ‚â§
c0 Œ®(M)
n

s

o
|‚Ñ¶|
, k‚àÜkF = 1 .
n log n

Generalized Matrix Completion
3.5

2.2

0.5

n=50
n=100
n=150
n=200

3

n=50
n=100
n=150
n=200

0.45

2.5

0.4

2

0.35

n=50
n=100
n=150
n=200

2
1.8

1.5

RMSE

MAE

RMSE

1.6

0.3

1.4
1.2

1

0.25

0.5

0.2

1

0
0

0.5

1

1.5

2

2.5

3

3.5

4

0

0.8

0.5

1

1.5

2

2.5

3

3.5

4

0

|‚Ñ¶|/rn log n

|‚Ñ¶|/rn log n

0.5

1

1.5

2

2.5

3

3.5

4

|‚Ñ¶|/rn log n

b plotted against ‚Äúnormalized‚Äù
Figure 1. Appropriate error metric between observation matrix X, and the MLE estimate from (6) X,
sample size, when X is generated from (a) Gaussian, (b) Bernoulli, and (c) binomial distributions

w.p. > 1 ‚àí C1 e‚àíC2 Œ®min n log n , ‚àÄ ‚àÜ ‚àà E:

r = 2 log n. The observation matrices, X, are then sampled from the different members of exponential family distributions parameterized by Œò‚àó . For each n, we uniformly
sample a subset ‚Ñ¶ entries of the observation matrix X, and
b from (6).
estimate Œò

s

|‚Ñ¶|Œ∫2R (n, |‚Ñ¶|)
n log n
s
n log n
.
+ k10 R(‚àÜ)
|‚Ñ¶|



 mn X 2
 16R(‚àÜ)
‚àÜij ‚àí 1 ‚â§

|‚Ñ¶| ij‚àà‚Ñ¶
c0 Œ®(M)

Evaluation:
For each member of the exponential family of distributions considered, we can measure the performance of our

The proof is provided in Appendix A.1.

M ‚Äìestimator in parameter space as

‚àÜ
From the assumptions in Thm. 2 and Prop. 1, k‚àÜk
b F ‚àà E.
b ‚àà V, R(‚àÜ)
b = R(‚àÜ
b ) + R(‚àÜ
b ‚ä•) ‚â§
Further, as ‚àÜ
M
M
b F . Using Lemma 4, and (15),
b ) ‚â§ 4Œ®(M)k‚àÜk
4R(‚àÜ
M
and choosing |‚Ñ¶| = cŒ®2 (M)n
log n, for large enough c,
q
b

2

log n
we have K1 := 1 ‚àí 4k10 Œ® (M)n
> 0; thus using
|‚Ñ¶|
q


2Œ∑Œ±‚àó
2
|‚Ñ¶|Œ∫R (n,|‚Ñ¶|)
‚àí‚àö
Œ∫L := e mn K1 ‚àí 64
, w.h.p.,
c0
n log n

mn X
b ij , Œò‚àó ) ‚â• Œ∫L k‚àÜk
b 2.
BG (Œò
ij
F
|‚Ñ¶|

(16)

ij‚àà‚Ñ¶

5. Experiments
We provide simulated experiments to corroborate our theoretical guarantees, focusing specifically on Corollary 1,
where we consider the special case where the underlying
parameter matrix is low‚Äìrank, but the underlying noise
model for the matrix elements could be any of the general class of exponential family distributions. We look at
three well known members of exponential family suitable
for different data‚Äìtypes, namely Gaussian, Bernoulli, and
binomial, which are popular choices for modeling continuous valued, binary, and count valued data, respectively.
5.1. Experimental Setup
We create low‚Äìrank ground truth parameter matrices, Œò‚àó ‚àà
Rm√ón of sizes n ‚àà {50, 100, 150, 200} (for simplicity
we considered square matrices, m = n); we set the rank to

‚àó 2
b
kŒò‚àíŒò
kF
kŒò‚àó k2F

, or in obserb X),
vation space using an appropriate error metric err(X,
b
where X is the maximum likelihood estimate of the recovb = argmax P (X|Œò)
b (we use RMSE,
ered distribution, X
X
MAE in our plots). From our corollary, we require |‚Ñ¶| =
O(rn log n) samples for consistent recovery, so we plot
the error metric against the the ‚Äúnormalized‚Äù sample size,
|‚Ñ¶|
rn log n . For reasons of space, we only provide results for
the error metric in observations space plotted against the
the ‚Äúnormalized‚Äù sample size. The remainder of the results
are provided in Appendix B. It can be seen from the plots
that the error decays with increasing sample size, corroborating our consistency results; indeed |‚Ñ¶| > 1.5rn log n
samples suffice for the errors to decay to a very small value.
Further, the aligning of the curves (for different n) given
the ‚Äúnormalized‚Äù sample size corroborates the convergence
rates as well.

Acknowledgement
The research was funded by NSF Grants IIS-0713142 and
IIS-1017614. Pradeep Ravikumar acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS1149803, IIS-1320894, DMS-1264033.

Generalized Matrix Completion

References
Banerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J.
Clustering with bregman divergences. JMLR, 6:1705‚Äì
1749, 2005.
Candes, E. J. and Plan, Y. Matrix completion with noise.
Proceedings of the IEEE, 2010.
Candes, E. J. and Recht, B. Exact matrix completion
via convex optimization. Foundations of Computational
mathematics, 2009.
Candes, E. J. and Tao, T. The power of convex relaxation:
near-optimal matrix completion. IEEE Transactions on
Information Theory, 2010.
Candes, E. J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of the ACM (JACM),
58(3):11, 2011.
Collins, M., Dasgupta, S., and Schapire, R. E. A generalization of principal components analysis to the exponential family. In NIPS, pp. 617‚Äì624, 2001.
Davenport, M. A., Plan, Y., Berg, E., and Wootters, M. 1bit matrix completion. arXiv preprint arXiv:1209.3672,
2012.

Ledoux, M. and Talagrand, M.
Probability in Banach Spaces: isoperimetry and processes, volume 23.
Springer, 1991.
Mnih, A. and Salakhutdinov, R. Probabilistic matrix factorization. In NIPS, pp. 1257‚Äì1264, 2007.
Mohamed, S., Ghahramani, Z., and Heller, K. A. Bayesian
exponential family pca. In NIPS, pp. 1089‚Äì1096, 2008.
Negahban, S. Structured Estimation in High-Dimensions.
PhD thesis, EECS Department, University of California,
Berkeley, May 2012.
Negahban, S. and Wainwright, M. J. Joint support recovery
under high-dimensional scaling: Benefits and perils of
l1,-regularization. NIPS, 21:1161‚Äì1168, 2008.
Negahban, S. and Wainwright, M. J. Restricted strong convexity and weighted matrix completion: Optimal bounds
with noise. The Journal of Machine Learning Research,
98888:1665‚Äì1697, 2012.
Obozinski, G., Wainwright, M. J., and Jordan, M. I. Support union recovery in high-dimensional multivariate regression. The Annals of Statistics, 39(1):1‚Äì47, 2011.
Recht, B. A simpler approach to matrix completion. JMLR,
12:3413‚Äì3430, 2011.

Fazel, M., Hindi, H, and Boyd, S. P. A rank minimization heuristic with application to minimum order system approximation. In American Control Conference,
pp. 4734‚Äì4739, 2001.

Salakhutdinov, R. and Mnih, A. Bayesian probabilistic matrix factorization using markov chain monte carlo. In
ICML, pp. 880‚Äì887. ACM, 2008.

Forster, J. and Warmuth, M. Relative expected instantaneous loss bounds. Journal of Computer and System
Sciences, 2002.

Srebro, N., Rennie, J., and Jaakkola, T. S. Maximummargin matrix factorization. In NIPS, pp. 1329‚Äì1336,
2004.

Gordon, G. J. GeneralizedÀÜ 2 linearÀÜ 2 models. In NIPS,
pp. 577‚Äì584, 2002.

Tipping, M. E. and Bishop, C. M. Probabilistic principal
component analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), pp. 611‚Äì622,
1999.

Jain, P., Netrapalli, P., and Sanghavi, S. Low-rank matrix
completion using alternating minimization. In STOC,
2013.
Kakade, S. M., Shamir, O., Sridharan, K., and Tewari,
A. Learning exponential families in high-dimensions:
Strong convexity and sparsity. In AISTATS, JMLR Workshop and Conference Proceedings, pp. 381‚Äì388, 2010.
Keshavan, R. H., Montanari, A., and Oh, S. Matrix completion from a few entries. IEEE Transactions on Information Theory, 2010a.
Keshavan, R. H., Montanari, A., and Oh, S. Matrix completion from noisy entries. JMLR, 2010b.
Laurent, M. Matrix completion problems. Encyclopedia of
Optimization, 3:221‚Äì229, 2009.

Vershynin, R. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027,
2010.
Yang, E. and Ravikumar, P. Dirty statistical models. In
NIPS, 2013.
Yang, E., Allen, G., Liu, Z., and Ravikumar, P. Graphical
models via generalized linear models. In NIPS, 2012.
Yang, E., Ravikumar, P., Allen, G. I., and Liu, Z. Conditional random fields via univariate exponential families.
In Advances in Neural Information Processing Systems,
pp. 683‚Äì691, 2013.

