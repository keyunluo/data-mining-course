Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons

Yuxin Chen
Department of Statistics, Stanford University, Stanford, CA 94305, USA

YXCHEN @ STANFORD . EDU

Changho Suh
CHSUH @ KAIST. AC . KR
Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Korea

Abstract
This paper explores the preference-based top-K
rank aggregation problem. Suppose that a collection of items is repeatedly compared in pairs, and
one wishes to recover a consistent ordering that
emphasizes the top-K ranked items, based on
partially revealed preferences. We focus on the
Bradley-Terry-Luce model that postulates a set
of latent preference scores underlying all items,
where the odds of paired comparisons depend
only on the relative scores of the items involved.
We characterize the minimax limits on identifiability of top-K ranked items, in the presence
of random and non-adaptive sampling. Our results highlight a separation measure that quantifies the gap of preference scores between the
K th and (K + 1)th ranked items. The minimum
sample complexity required for reliable top-K
ranking scales inversely with the separation measure. To approach this minimax limit, we propose a nearly linear-time ranking scheme, called
Spectral MLE, that returns the indices of the topK items in accordance to a careful score estimate. In a nutshell, Spectral MLE starts with an
initial score estimate with minimal squared loss
(obtained via a spectral method), and then successively refines each component with the assistance of coordinate-wise MLEs. Encouragingly,
Spectral MLE allows perfect top-K item identification under minimal sample complexity. The
practical applicability of Spectral MLE is further
corroborated by numerical experiments.

1. Introduction and Motivation
The task of rank aggregation is encountered in a wide spectrum of contexts like social choice (Caplin & Nalebuff,
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

1991; Soufiani et al., 2014b), web search and information
retrieval (Dwork et al., 2001), crowd sourcing (Chen et al.,
2013), recommendation systems (Baltrunas et al., 2010), to
name just a few. Given partial preference information over
a collection of items, the aim is to identify a consistent ordering that best respects the revealed preference. In the
high-dimensional regime, one is often faced with two challenges: 1) the number of items to be ranked is ever growing,
which makes it increasingly harder to recover a consistent
total ordering over all items; 2) the observed data is highly
incomplete and inconsistent: only a small number of noisy
pairwise / listwise preferences can be acquired.
In an effort to address such challenges, this paper explores a
popular pairwise preference-based model, which postulates
the existence of a ground-truth ranking. Specifically, consider a parametric model involving n items, each assigned
a preference score that determines its rank. Concrete examples of preference scores include the overall rating of an
athlete, the academic performance and competitiveness of
a university, the dining quality of a restaurant, etc. Each
item is then repeatedly compared against a few others in
pairs, yielding a set of noisy binary comparisons generated
based on the relative preference scores. In many situations,
the number of repeated comparisons essentially reflects the
signal-to-noise ratio (SNR) / quality of the information revealed for each pair of items. The goal is then to develop a
â€œdenoisingâ€ procedure that recovers the ground-truth ranking with minimal sample complexity.
There has been a proliferation of ranking schemes that suggest partial solutions. While the ranking that we are seeking is better treated as a function of the preference parameters, most of the aforementioned schemes adopt the natural
â€œplug-inâ€ procedure, that is, start by inferring the preference scores, and then return a ranking in accordance to the
parametric estimates. The most popular paradigm is arguably the maximum likelihood estimation (MLE) (Ford,
1957), where the main appeal of MLE is its inherent convexity under several comparison models, e.g. the BradleyTerry-Luce (BTL) model (Bradley & Terry, 1952; Luce,
1959). Encouragingly, MLE often achieves low `2 estimation loss while retaining efficient finite-sample complexity.
Another prominent alternative concerns a family of spectral

Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons

ranking algorithms (e.g. PageRank (Brin & Page, 1998)).
A provably efficient choice within this family is Rank Centrality (Negahban et al., 2012), which produces an estimate
with nearly minimax mean squared error (MSE). While
both MLE and Rank Centrality allow intriguing guarantees
towards finding faithful parametric estimates, the squared
loss metric considered therein does not necessarily imply
optimality of the ranking accuracy. In fact, there is no
shortage of high-dimensional situations that admit parametric estimates with low squared loss while precluding
reliable ranking. Furthermore, many realistic scenarios emphasize only a few items that receive the highest ranks. Unfortunately, the above MSE results fall short of ensuring
recovery of the top-ranked items.
In this work, we consider accurate identification of top-K
ranked items under the popular BTL pairwise comparison
model, assuming that the item pairs we can compare are selected in a random and non-adaptive fashion (termed passive ranking). In particular, we aim to explore the following questions: (a) what is the minimum number of repeated
comparisons necessary for reliable ranking? (b) how is
the ranking accuracy affected by the underlying preference
score distributions? We will address these two questions
from both statistical and algorithmic perspectives.
1.1. Main Contributions
This paper investigates minimax optimal procedures for
top-K rank aggregation. Our contributions are two-fold.
To begin with, we characterize the fundamental three-way
tradeoff between the number of repeated comparisons, the
sparsity of the comparison graph, and the preference score
distribution, from a minimax perspective. In particular, we
emphasize a separation measure that quantifies the gap of
preference scores between the K th and (K + 1)th ranked
items. Our results demonstrate that the minimal sample
complexity or quality of paired evaluation (reflected by
the number of repeated comparisons per an observed pair)
scales inversely with the separation measure.
Secondly, we propose a nearly linear-time two-stage algorithm, called Spectral MLE, which allows perfect top-K
identification as soon as the sample complexity exceeds
the minimax limits (modulo some constant). Specifically,
Spectral MLE starts by obtaining careful initial scores that
are faithful in the `2 sense (e.g. via a spectral method), and
then iteratively sharpens the pointwise estimates by comparing the preceding estimates with coordinate-wise MLE.
This algorithm is designed primarily in an attempt to seek a
score estimate with minimal pointwise loss. Furthermore,
numerical experiments demonstrate that Spectral MLE outperforms Rank Centrality by achieving higher ranking accuracy and lower `âˆž estimation error.

1.2. Prior Art
There are two distinct families of observation models
that receive considerable interest: (1) value-based model,
where the observation on each item is drawn only from
the distribution underlying this individual; (2) preferencebased model, where one observes the relative order among
a few items instead of revealing their individual values.
Best-K identification in the value-based model with adaptive sampling (termed active ranking) is closely related to
the multi-armed bandit problem, where the fundamental
identification complexity has been characterized (Gabillon
et al., 2011; Bubeck et al., 2013; Jamieson et al., 2014).
The value-based and preference-based models have also
been compared in terms of minimax error rates in estimating the latent quantities (Shah et al., 2014).
In the realm of pairwise preference settings, many active
ranking schemes (Busa-Fekete & HuÌˆllermeier, 2014) have
been proposed in an attempt to optimize the explorationexploitation tradeoff. For instance, in the noise-free case,
Jamieson et al. (Jamieson & Nowak, 2011) considered perfect total ranking and characterized the query complexity
gain of adaptive sampling relative to random queries, provided that the items under study admit a low-dimensional
Euclidean embedding. Furthermore, the works (Ailon,
2012; Jamieson & Nowak, 2011; Braverman & Mossel,
2008; Wauthier et al., 2013) explored the query complexity in the presence of noise, but focused on â€œapproximately
correctâ€ total rankingsâ€”a solution with loss at most a factor (1 + ) from optimalâ€”rather than accurate ordering.
Another path-based approach has been proposed to accommodate accurate top-K queries from noisy pairwise data
(Eriksson, 2013), where the observation error is assumed
to be i.i.d. instead of being item-dependent. Motivated by
the success of value-based racing algorithms, (Busa-Fekete
et al., 2013; Busa-Fekete & HuÌˆllermeier, 2014) came up
with a generalized racing algorithm that often led to efficient sample complexity. In contrast, our paper concentrates on top-K identification in a passive setting, assuming that partial preferences are collected in a noisy, random,
and non-adaptive manner. This was previously out of reach.
Apart from Rank Centrality and MLE, the most relevant
work is (Rajkumar & Agarwal, 2014). For a variety of rank
aggregation methods, they developed intriguing sufficient
statistical hypotheses that guarantee the convergence to an
optimal ranking, which in turn leads to sample complexity
bounds for Rank Centrality and MLE. Nevertheless, they
focused on perfect total ordering instead of top-K selection, and their results fall short of a rigorous justification
as to whether or not the derived sample complexity bounds
are statistically optimal.
Finally, there are many related yet different problem settings considered in the prior literature. For instance, the
work (Ammar & Shah, 2012) approached top-K ranking
using a maximum entropy principle, assuming the existence of a distribution Âµ over all possible permutations.

Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons

Recent work (Soufiani et al., 2013; 2014a) investigated
consistent rank breaking under more generalized models
involving full rankings. A family of distance measures
on rankings has been studied and justified based on an
axiomatic approach (Farnoud & Milenkovic, 2014). Another line of works considered the popular distance-based
Mallows model (Lu & Boutilier, 2011; Busa-Fekete et al.,
2014; Awasthi et al., 2014). An online ranking setting has
also been studied (Harrington, 2003; Farnoud et al., 2014).
The minimax recovery limits under general pairwise measurements have been determined by (Chen et al., 2015a).
1.3. Notation
Let [n] represent {1, 2, Â· Â· Â· , n}. A graph G is said to be
an ErdoÌ‹sâ€“ReÌnyi random graph, denoted by Gn,pobs , if each
pair (i, j) is connected by an edge independently with probability pobs . Besdies, we use deg (i) to represent the degree
of vertex i in G.

Comparison Model and Assumptions. Suppose that we
observe a few pairwise evaluations over n items. To pursue
a statistical understanding towards the ranking limits, we
assume that the pairwise comparison outcomes are generated according to the BTL model (Bradley & Terry, 1952;
Luce, 1959), a long-standing model that has been applied
in numerous applications (Agresti, 2014; Hunter, 2004).
â€¢ Preference Scores. The BTL model hypothesizes on
the existence of some hidden preference vector w =
[wi ]1â‰¤iâ‰¤n , where wi represents the underlying preference score / weight of item i. The outcome of each
paired comparison depends only on the scores of the
items involved. Unless otherwise specified, we will assume without loss of generality that
w1 â‰¥ w2 â‰¥ Â· Â· Â· â‰¥ wn > 0.

(1)

â€¢ Comparison Graph. Denote by G = ([n] , E) the comparison graph such that items i and j are compared if
and only if (i, j) belongs to the edge set E. We will
mostly assume that G is drawn from the ErdoÌ‹sâ€“ReÌnyi
model G âˆ¼ Gn,pobs for some observation factor pobs .
â€¢ (Repeated) Pairwise Comparisons. For each (i, j) âˆˆ E,
we observe L paired comparisons between items i and
j. The outcome of the lth comparison between them,
(l)
denoted by yi,j , is generated as per the BTL model:
(
i
1, with probability wiw+w
,
(l)
j
yi,j =
(2)
0, else,
(l)

where yi,j = 1 indicates a win by i over j. We adopt the
(l)
yi,j â€™s

are jointly independent

yi,j :=

1 X (l)
yi,j .
L
l=1

â€¢ Signal to Noise Ratio (SNR) / Quality of Comparisons.
The overall faithfulness of the acquired evaluation between items i and j is captured by the sufficient statistic
yi,j . Its SNR can be captured by
SNR : = E2 [yi,j ]/Var [yi,j ]  L.

(3)

As a result, the number L of repeated comparisons measures the SNR or the quality of comparisons over any
observed pair of items.
â€¢ Dynamic Range of Preference Scores. It is assumed
throughout that the dynamic range of the preference
scores is fixed irrespective of n, namely,
1â‰¤iâ‰¤n

(4)

for some positive constants wmin and wmax bounded
away from 0, which amounts to the most challenging
regime (Negahban et al., 2012). In fact, the case in which
max
the range w
wmin grows with n can be readily translated
into the above fixed-range regime by first separating out
those items with vanishing scores (e.g. via a simple voting method like Borda count (Ammar & Shah, 2011)).
Performance metric. Given these pairwise observations,
one wishes to see whether or not the top-K ranked items
are identifiable. To this end, we consider the probability of
error Pe in isolating the set of top-K ranked items, i.e.
n
o
Pe (Ïˆ) := P Ïˆ (y) 6= [K] ,
(5)
where Ïˆ is any ranking scheme that returns a set of K indices. Here, [K] denotes the (unordered) set of the first
K indices. We aim to characterize the fundamental admissible region of (L, pobs ) where reliable top-K ranking is
feasible, i.e. Pe can be vanishingly small as n grows.

3. Minimax Ranking Limits
We explore the fundamental ranking limits from a minimax
perspective, which centers on the design of robust ranking
schemes that guard against the worst case in probability
of error. The most challenging component of top-K rank
aggregation hinges upon distinguishing the two items near
the decision boundary, i.e. the K th and (K + 1)th ranked
items. Due to the random nature of the acquired finitebit comparisons, the information concerning their relative
preference could be obliterated by noise, unless their latent
preference scores are sufficiently separated. In light of this,
we single out a preference separation measure as follows

(l)

convention that yj,i = 1 âˆ’ yi,j . It is assumed throughout that conditional on G,

L

y i := {yi,j | j : (i, j) âˆˆ E} ;

wi âˆˆ [wmin , wmax ] ,

2. Problem Setup

(l)

across all l and i > j. For ease of presentation, we introduce the collection of sufficient statistics as

âˆ†K :=

wK âˆ’ wK+1
.
wmax

(6)

Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons

As will be seen, this measure plays a crucial role in determining information integrity for top-K identification.
To model non-adaptive sampling and partial observation,
we employ the ErdoÌ‹sâ€“ReÌnyi model G âˆ¼ Gn,pobs . As already noted by (Ford, 1957), if the comparison graph G is
not connected, then there is absolutely no basis to determine relative preferences between two disconnected components. Therefore, a reasonable necessary condition that
one would expect is the connectivity of G, which requires
pobs > log n / n.

(7)

All results in this paper will operate under this assumption.
A main finding of this paper is an order-wise tight sufficient
condition for top-K identifiability, as stated below.
Theorem 1 (Identifiability). Suppose that G âˆ¼ Gn,pobs
with pobs â‰¥ c0 log n/n. Assume that L = O (poly (n))
âˆ’2
max
and w
,
wmin = O (1). With probability exceeding 1 âˆ’ c1 n
the set of top-K ranked items can be identified
exactly
by

an algorithm that runs in time O |E| log2 n , provided that
L â‰¥

c2 log n
.
npobs âˆ†2K

(8)

Here, c0 , c1 , c2 > 0 are some universal constants.
Remark 1. We assume throughout that the input fed
to each ranking algorithm is the sufficient statistic
(l)
{yi,j | (i, j) âˆˆ E} rather than the entire collection of yi,j ,
otherwise the complexity is at least O (L Â· |E|).
Theorem 1 characterizes an identifiable region within
which exact identification of top-K items is plausible by
nearly linear-time algorithms. The algorithm we propose,
as detailed in Section 4, attempts recovery by computing
a score estimate whose errors can be uniformly controlled
across all entries. Afterwards, the algorithm reports the K
items that receive the highest estimated scores.
Encouragingly, the above identifiable region is minimax
optimal. Consider a given separation condition âˆ†K , and
suppose that nature behaves in an adversarial manner by
choosing the worst-case scores w compatible with âˆ†K .
This imposes a minimax lower bound on the quality of
comparisons necessary for reliable ranking, as given below.

Theorem 2 (Minimax Lower Bounds). Fix  âˆˆ 0, 12 ,
and let G âˆ¼ Gn,pobs . If
L â‰¤ c

(1 âˆ’ ) log n âˆ’ 2
npobs âˆ†2K

(9)

holds for some constant1 c > 0, then for any ranking
scheme Ïˆ, there exists a preference vector w with separation âˆ†K such that Pe (Ïˆ) â‰¥ .
1

4
4
More precisely, c = wmin
/(2wmax
).

Theorem 2 taken collectively with Theorem 1 determines
the scaling of the fundamental ranking boundary on L.
Since the sample size sharply concentrates around n2 pobs L
in our model, this implies that the required sample complexity for top-K ranking scales inversely with the preference separation at a quadratic rate. Put another way, Theorem 2 justifies the need for a minimum separation criterion
that applies to any ranking scheme:
p
log n / (npobs L).
(10)
âˆ†K &
Somewhat unexpectedly, there is no computational barrier
away from this statistical limit. Several other remarks of
Theorems 1-2 are in order.
â€¢ `2 Loss vs. `âˆž Loss. A dominant fraction of prior methods focus on the mean squared error in estimating the
latent scores w. It was established by (Negahban et al.,
2012) that the minimax `2 regret is squeezed between
s
1
log n
E [kwÌ‚ âˆ’ wk]
âˆš
. inf sup
.
,
wÌ‚ w
kwk
npobs L
npobs L
where the infimum is over all score estimators wÌ‚. This
limit is almost identical to the minimax separation criterion (10) we derive for top-K identification. In fact,
if the pointwise error of wÌ‚ is uniformly bounded by
p
log n/(npobs L), then wÌ‚ necessarily achieves the minimax `2 error. Moreover, the pointwise error bound
presents a fundamental bottleneck for top-K rankingâ€”it
will be impossible to differentiate the K th and (K + 1)th
ranked items unless their score separation exceeds the
aggregate error of the corresponding score estimates for
these two items. Based on this observation, our algorithm is mainly designed to control the elementwise estimation error. As will be seen in Section 4, the resulting
estimation error will be uniformly spread over all entries,
which is optimal in both `2 and `âˆž sense.
â€¢ From Coarse to Detailed Ranking. The identifiable
region we present depends only on the preference separation between items K and K + 1. This arises since we
only intend to identify the group of top-K items without
specifying fine details within this group (termed coarse
ranking). In fact, our results readily uncover the minimax separation requirements for the case where one further expects fine ordering among these K items. Specifically, this task is feasible (in the minimax sense) iff
p
log n/(npobs L), 1 â‰¤ i â‰¤ K.
(11)
âˆ†i &
â€¢ High SNR Requirement for Total Ordering. In many
situations, the separation criterion (11) immediately
demonstrates the hardness (or even impossibility) of recovering the ordering over all items. In fact, to figure
out the total order, one expects sufficient score separation between all pairs of consecutive items, namely,
p
âˆ†i &
log n / (npobs L), âˆ€i (1 â‰¤ i < n).

Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons

Since the âˆ†i â€™s are defined in a normalized way (6), they
need to satisfy
nâˆ’1
X

âˆ†i =

i=1

w1 âˆ’ wn
â‰¤ 1.
wmax

As can be easily verified, the preceding two conditions
would be incompatible unless
L & n log n / pobs ,
which imposes a fairly stringent SNR requirement. For
instance, under a sparse graph where pobs  logn n , the
number of repeated comparisons (and hence the SNR)
needs to be at least Î˜(n2 ), regardless of the method employed. Such a high SNR requirement could be increasingly more difficult to guarantee as n grows.
â€¢ Passive Ranking vs. Active Ranking. In our passive ranking model, the sample complexity requirement
n2 pobs L for reliable top-K identification is given by
n2 pobs L & n log n / âˆ†2K .
In comparison, when adaptive sampling is employed
for the preference-based model, the most recent upper
bound on the sample complexity (e.g. Theorem 1 of
(Busa-Fekete et al., 2013)) is about the order of
Xnâˆ’1
i=1

âˆ†âˆ’2
i log n.

In the challenging regime where a dominant fraction of
consecutive pairs are minimally separated (e.g. âˆ†1 =
Â· Â· Â· = âˆ†nâˆ’1 ), the above results seem to suggest that active ranking may not outperform passive ranking. For the
other extreme case where only a single pair is minimally
separated (e.g. âˆ†1  âˆ†i (i â‰¥ 2)), active ranking is
more desirable as it will adaptively acquire more paired
evaluation over the minimally separated items.

4. Ranking Scheme: Spectral Method Meets
MLE
This section presents a nearly linear-time algorithm that attempts recovery of the top-K ranked items. The algorithm
proceeds in two stages: (1) an appropriate initialization that
concentrates around the ground truth in an `2 sense, which
can be obtained via a spectral ranking method; (2) a sequence of iterative updates sharpening the estimates in a
pointwise manner, which consists in computing coordinatewise MLE solutions. The two stages operate upon different
sets of samples, while no further sample splitting is needed
within each stage. The combination of these two stages will
be referred to as Spectral MLE.
Before continuing to describe the details of our algorithm,
we introduce a few notations that will be used throughout.

â€¢ L (w; y i ): the likelihood function of a latent preference vector w, given the part of comparisons y i that
have bearing on item i.
â€¢ w\i : for any preference vector w, let w\i represent
[w1 , Â· Â· Â· , wiâˆ’1 , wi+1 , Â· Â· Â· , wn ] excluding wi .

â€¢ L Ï„, w\i ; y i : with
 a slight abuse of notation, denote
by L Ï„, w\i ; y i the likelihood of the preference vector [w1 , Â· Â· Â· , wiâˆ’1 , Ï„, wi+1 , Â· Â· Â· , wn ].
4.1. Algorithm: Spectral MLE
It has been established that the spectral ranking method,
particularly Rank Centrality, is able to discover a preference vector wÌ‚ that incurs minimal `2 loss. To enable reliable ranking, however, it is more desirable to obtain an estimate that is faithful in an elementwise sense. Fortunately,
the solution returned by the spectral method will serve as
an ideal initial guess to seed our algorithm. The two components of the proposed Spectral MLE are described below.
1. Initialization via Spectral Ranking. We generate an
initialization w(0) via Rank Centrality. In words, Rank
Centrality proceeds by constructing a Markov chain
based on the pairwise observations, and then returning its stationary distribution by computing the leading
eigenvector of the associated probability transition matrix. Under the ErdoÌ‹sâ€“ReÌnyi model, the estimate w(0)
is reasonably faithful in terms of the mean squared loss
(Negahban et al., 2012), that is, with high probability,
p
kw(0) âˆ’ wk/ kwk .
log n/(npobs L).
2. Successive Refinement via Coordinate-wise MLE.
Note that the state-of-the-art finite-sample analyses for
MLE (e.g. (Negahban et al., 2012)) involve only the `2
accuracy of the global MLE when the locations of all
samples are i.i.d. (rather than the graph-based model
considered herein). Instead of seeking a global MLE
solution, we propose to carefully utilize the coordinatewise MLE. Specifically, we cyclically iterate through
each component, one at a time, maximizing the loglikelihood function with respect to that component. In
contrast to the coordinate-descent method for solving
the global MLE, we replace the preceding estimate with
the new coordinate-wise MLE only when they are far
apart. Theorem 4 (to be stated in Section 4.2) guarantees the contraction of the pointwise error for each cycle,
which leads to a geometric convergence rate.
The algorithm then returns the indices of top-K items in accordance to the score estimate. A formal and more detailed
description of the procedure is summarized in Algorithm 1.
Remark 2. We split E into E init and E iter for analytical
convenience. Empirically, if we keep E init = E iter = E

Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons

Algorithm 1 Spectral MLE.
Input: The average comparison outcome yi,j for all (i, j) âˆˆ E; the score range [wmin , wmax ].
Partition E randomly into two sets E init and E iter each containing
components of y i obtained over E init (resp. E iter ).

1
2

|E| edges. Denote by y init
(resp. y iter
i
i ) the

Initialize w(0) to be the estimate computed by Rank Centrality on y init
(1 â‰¤ i â‰¤ n).
i
Successive Refinement: for t = 0 : T do
1) Compute the coordinate-wise MLE
wimle â† arg

max

Ï„ âˆˆ[wmin ,wmax ]



(t)
.
L Ï„, w\i ; y iter
i

(12)

2) For each 1 â‰¤ i â‰¤ n, set
(t+1)
wi

â†



(

(t) 
wimle , if wimle âˆ’ wi  > Î¾t ,
(t)

wi ,

(13)

else.

Output the indices of the K largest components of w(T ) .

and reuse all samples, then it will outperform the graphsplitting procedure (but not by much). Thus, we recommend
the sample-reuse procedure for practical use.
Remark 3. Spectral MLE is inspired by recent advances in
solving non-convex programs by means of iterative methods (Keshavan et al., 2010; Candes et al., 2015; Chen &
Candes, 2015b; Jain et al., 2013; Netrapalli et al., 2013;
Balakrishnan et al., 2014). A key message conveyed from
these works is: once we arrive at an appropriate initialization (often via a spectral method), the iterative estimates
will often be rapidly attracted towards the global optimum.
Remark 4. While our analysis is restricted to Gn,pobs ,
Spectral MLE can be applied to general graphs. We caution, however, that spectral ranking is not guaranteed to
achieve minimal `2 loss, particularly for those graphs with
small spectral gaps. Therefore, Spectral MLE is not necessarily minimax optimal under general graph patterns.
Notably, the successive refinement stage is based on the observation that we are able to characterize the confidence intervals of the coordinate-wise MLEs at each iteration. Such
confidence intervals allow us to detect outlier components
that incur large pointwise loss. Since the initial guess is optimal in an overall `2 sense, a large fraction of its entries are
already faithful compared to the ground truth. As a result,
it suffices to disentangle the set of â€œsparseâ€ outliers.
One appealing feature of Spectral MLE is its low computational complexity. Recall that the initialization step
by Rank Centrality
can be solved for  accuracy within

O |E| log 1 time instances by means of a power method.
In addition, for each component i, the coordinate-wise likelihood function involves a sum of deg (i) terms. Since finding the coordinate-wise MLE (12) can be cast as an onedimensional convex program, one can get  accuracy via
a bisection method within O deg (i) Â· log 1 time. Therefore, each iteration cycle of the successive refinement
stage

can be accomplished in time O |E| Â· log 1 .

The following theorem establishes the ranking accuracy of
Spectral MLE under the BTL model.
Theorem 3. Let c0 , c1 , c2 , c3 > 0 be some universal constants. Suppose that L = O(poly(n)), the comparison
graph G âˆ¼ Gn,pobs with pobs > c0 log n/n, and assume
that the separation measure (6) satisfies
p
(14)
âˆ†K > c1 log n / (npobs L).
Then with probability exceeding 1 âˆ’ 1/n2 , Spectral MLE
perfectly identifies the set of top-K ranked items, provided
that the parameters obey T â‰¥ c2 log n and


1
Î¾t := c3 Î¾min + t (Î¾max âˆ’ Î¾min ) ,
(15)
2
q
q
n
log n
where Î¾min := nplog
and
Î¾
:=
max
pobs L .
obs L
Theorem 3 basically implies that the proposed algorithm
succeeds in separating out the high-ranking objects with
high probability, as long as the preference score satisfies
the separation condition
p
log n / (npobs L).
âˆ†K &
Additionally, Theorem 3 asserts that the number of iteration cycles required in the second stage scales at most logarithmically, revealing that Spectral MLE achieves the desired ranking precision with nearly linear time complexity.
4.2. Successive Refinement: Convergence and
Contraction of `âˆž Error
In the sequel, we would like to provide some interpretation as to why we expect the pointwise error of the score
estimates to be controllable. The argument is heuristic in
nature, since we will assume for simplicity that each iteration employs a fresh set of samples y independent from the
present estimate w(t) .

Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons

Denote by `âˆ— (Ï„ ) the true log-likelihood function

1
` (Ï„ ) := log L Ï„, w\i ; y i .
L
âˆ—

satisfies
(16)

One can easily verify that its expectation around wi can be
controlled through a locally strongly-concave function, due
to the existence of a second-order lower bound
2

|Ew [`âˆ— (wi ) âˆ’ `âˆ— (Ï„ )]| & |Ï„ âˆ’ wi | npobs .

(17)

This measures the penalty when Ï„ deviates from the ground
truth. Note, however, that we donâ€™t have direct access to
`âˆ— (Â·) since it relies on the latent scores w. To obtain a
computable surrogate, we replace w with the present estimate w(t) , resulting in the plug-in likelihood function


1
(t)
`Ë†i (Ï„ ) := log L Ï„, w\i ; y i .
L
Fortunately, the surrogate loss incurred by employing `Ë†i (Ï„ )
is locally stable in the sense that

h
i


Ew `Ë†i (Ï„ ) âˆ’ `Ë†i (wi ) âˆ’ (`âˆ— (Ï„ ) âˆ’ `âˆ— (wi )) 
. npobs |Ï„ âˆ’ wi |

kwÌ‚ âˆ’ wk
.
kwk

(18)

As a result, any candidate Ï„ 6= wi will be viewed as less
likely than and hence distinguishable from the ground truth
Ë† i ) > `(Ï„
Ë† )) in the mean sense, provided that its
wi (i.e. `(w
deviation penalty (17) dominates the surrogate loss (18),
namely,
|Ï„ âˆ’ wi | & kwÌ‚ âˆ’ wk / kwk.
Thus, if the aforementioned likelihood functions concentrate around their means, then our procedure should be able
to converge to a solution whose pointwise error is as low as
the normalized `2 error of the initial guess.
Encouragingly, the `âˆž estimation error not only converges,
but converges at a geometric rate as well. This rapid convergence property does not rely on the â€œfresh-sampleâ€ assumption imposed in the above heuristic argument, as formally stated in the following theorem.
Theorem 4. Suppose that G âˆ¼ Gn,pobs with pobs >
c0 log n/n for some large constant c0 , and there exists a
n
score vector wÌ‚ub âˆˆ [wmin , wmax ] independent of G satisfying
 ub

wÌ‚i âˆ’ wi  â‰¤ Î¾wmax , 1 â‰¤ i â‰¤ n;
(19)
kwÌ‚ub âˆ’ wk â‰¤ Î´ kwk .

(20)

Then with probability at least 1 âˆ’ c1 nâˆ’4 for some constant
c1 > 0, the coordinate-wise MLE

wimle := arg
max
L Ï„, wÌ‚\i ; y i
(21)
Ï„ âˆˆ[wmin ,wmax ]


5


wi âˆ’ wimle  < 20 (6 + log L/log n) wmax Â· max Î´ + Î¾ log n ,
4
wmin
npobs
s
)
(
log n
Î¾ log n
,
(22)
max Î´ +
npobs
npobs L
simultaneously
wÌ‚ âˆˆ [wmin , wmax ]n obeying
 for all scores

|wÌ‚i âˆ’ wi | â‰¤ wÌ‚iub âˆ’ wi , 1 â‰¤ i â‰¤ n.
q
n
In the regime where L = O (poly (n)) and Î´  nplog
,
obs L
Theorem 4 asserts that given an appropriate initialization,
the coordinate-wise MLE procedure is guaranteed to drag
down the elementwise estimation error at a rate
 (t)

w âˆ’ w


 (t+1)
log n 

w
w(t) âˆ’ w .
âˆ’w âˆž .
+
âˆž
kwk
npobs
The same collection of samples can be reused across all
iterations at the successive refinement stage, provided that
we can identify in each cycle another slightly looser estimate that is independent from the samples. From Theorem
4, the pointwise estimation error will converge to
p


w âˆ’ w(t+1)  .
log n/(npobs L),
âˆž
which is minimally apart from the ground truth.
4.3. Discussion
Choice of Initialization. Careful readers will remark
that the success of Spectral MLE can be guaranteed by a
broader selection of initialization procedures beyond Rank
Centrality. Indeed, Theorem 4 and subsequent analyses
lead to the following assertion: as long as the initialization method is able to produce an initial estimate w(0) that
is reasonably faithful in the `2 sense
p
(23)
kw(0) âˆ’ wk/kwk . log n/(npobs L),
then Spectral MLE will converge to a pointwise optimal
preference w(T ) obeying
p
kw(T ) âˆ’ wkâˆž . log n / (npobs L).
Initialization via Global MLE. One would naturally wonder whether we can employ the global MLE (computed
over y init ) to seed the iterative refinement stage (applied
over y iter ). In fact, the state-of-the-art analysis (with a different but order-wise equivalent model) (Negahban et al.,
2012) asserts that the global MLE satisfies the desired `2
property (23) for at least two cases: (a) complete graphs,
i.e. pobs = 1, and (b) ErdoÌ‹sâ€“Rnyi graphs with (almost)
no repeated comparisons, i.e. L = 1. In these two cases,
the proposed algorithm achieves minimal `âˆž errors if we
initialize it via the global MLE.

Spectral MLE:

0.5

pobs = 0.2

Rank Centrality: pobs =0.5
Spectral MLE:

pobs = 0.5

Rank Centrality: pobs =0.8
Spectral MLE:

pobs = 0.8

0.3

0.1
10

20

L: number of repeated comparisons

30

Rank Centrality: L = 5
Spectral MLE: L = 5
Rank Centrality: L = 20
Spectral MLE: L = 20
Rank Centrality: L = 50
Spectral MLE: L = 50

0.5

0.3

0.75

empirical success rate

Rank Centrality: pobs =0.2

infinity norm of estimation errors

infinity norm of estimation errors

Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons

Rank Centrality: n = 100
Spectral MLE: n = 100
Rank Centrality: n = 200
Spectral MLE: n = 200

0.5

0.1
0.5

1

pobs: graph sparsity

0.1

0.2

0.3

0.4

0.5

âˆ†K: score separation

Figure 1. (Left) Empirical `âˆž loss v.s. L; (Middle) `âˆž loss v.s. pobs ; (Right) Rate of success in top-K identification (n = 100, 200).

Nevertheless, whether the global MLE achieves minimal `2
loss for other configurations (L, pobs ) has not been established. The analytical bottleneck seems to stem from an
underlying bias-variance tradeoff when accounting for two
successive randomness mechanisms: the random graph G
and the repeated comparisons generated over G. In gen(l)
eral, yi,j â€™s are not jointly independent unless we condition
on G. In contrast, the above two special cases amount to
two extreme situations: (a) the randomness of G goes away
when pobs = 1; (b) the condition L = 1 avoids repeated
sampling. Nevertheless, these two cases alone (as well as
the model in Theorem 4 of (Negahban et al., 2012)) are not
sufficient in characterizing the complete tradeoff between
graph sparsity and the quality of the acquired comparisons.

the number n of items varies. We generate the latent scores
randomly over [0.5, 1], except that a separation âˆ†K is imposed between items K and K + 1. The results are shown
in Fig. 1(c) for the case where pobs = 0.2, and L = 5. As
can be seen, Spectral MLE achieves higher ranking accuracy compared to Rank Centrality for all these situations.
Interestingly, the benefit of Spectral MLE relative to Rank
Centrality is more apparent in the regime where the score
separation is small. In addition, it seems that Rank Centrality is capable of achieving good ranking accuracy in the
randomized model we simulate, and we leave the theoretical analysis for future work.

4.4. Numerical Experiments

This paper investigates rank aggregation from pairwise data
that emphasizes the top-K items. We developed a nearly
linear-time algorithm that performs as well as the best
model aware paradigm, from a minimax perspective.

A series of synthetic experiments is conducted to demonstrate the practical applicability of Spectral MLE. The important implementation parameters in our approach is the
choice of c2 and c3 given in Theorem 3, which specify T
and Î¾t . In all numerical simulations performed here, we
pick c2 = 5 and c3 = 1, and do not split samples. We focus on the case where n = 100, where each reported result
is calculated by averaging over 200 Monte Carlo trials.
We first examine the `âˆž error of the score estimates. The
latent scores are generated uniformly over [0.5, 1]. For each
(pobs , L), the paired comparisons are randomly generated
as per the BTL model, and we perform score inference by
means of both Rank Centrality and Spectral MLE. Fig. 1(a)
(resp. Fig. 1(b)) illustrates the empirical tradeoff between
the pointwise score estimation accuracy and the number L
of repeated comparisons (resp. graph sparsity pobs ). It can
be seen from these plots that the proposed Spectral MLE
outperforms Rank Centrality uniformly over all configurations, corroborating our theoretical results. Interestingly,
the performance gain is the most significant under sparse
graphs in the presence of low-resolution comparisons (i.e.
when pobs and L are small).
Next, we study the success rate of top-K identification as

5. Conclusion

This paper comes with some limitations in developing tight
sample complexity bounds under general graphs. The performances of Spectral MLE under other sampling models are worth investigating (Osting et al., 2015). In addition, it remains to characterize both statistical and computational limits for other choice models (e.g. the Plackett-Luce
model (Hajek et al., 2014)). It would also be interesting to
consider the case where the paired comparisons are drawn
from a mixture of BTL models (e.g. (Oh & Shah, 2014)),
as well as the collaborative ranking setting where one aggregates the item preferences from a pool of different users
in order to infer rankings for each individual user (e.g. (Lu
& Negahban, 2014; Park et al., 2015))

Acknowledgements
C. Suh was partly supported by the ICT R&D program of
MSIP/IITP [B0101-15-1272]. Y. Chen would like to thank
Yuejie Chi for helpful discussion.

Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons

References
Agresti, A. Categorical data analysis. John Wiley & Sons,
2014.

Chen, X., Bennett, P. N., Collins-Thompson, K., and
Horvitz, E. Pairwise ranking aggregation in a crowdsourced setting. In WSDM, pp. 193â€“202, 2013.

Ailon, N. Active learning ranking from pairwise preferences with almost optimal query complexity. Journal of
Machine Learning Research, 13:137â€“164, 2012.

Chen, Y., Suh, C., and Goldsmith, A. J. Information recovery from pairwise measurements: A Shannon-theoretic
approach. arXiv preprint arXiv:1504.01369, 2015a.

Ammar, A. and Shah, D. Ranking: Compare, donâ€™t score.
In Allerton Conference, pp. 776â€“783. IEEE, 2011.

Chen, Yuxin and Candes, Emmanuel J. Solving random
quadratic systems of equations is nearly as easy as solving linear systems. arXiv preprint arxiv:1505.05114,
2015b.

Ammar, A. and Shah, D. Efficient rank aggregation using
partial data. In SIGMETRICS, volume 40, pp. 355â€“366.
ACM, 2012.
Awasthi, P., Blum, A., Sheffet, O., and Vijayaraghavan, A.
Learning mixtures of ranking models. In Neural Information Processing Systems, pp. 2609â€“2617, 2014.
Balakrishnan, S., Wainwright, M. J., and Yu, B. Statistical
guarantees for the EM algorithm: From population to
sample-based analysis. arXiv:1408.2156, 2014.
Baltrunas, L., Makcinskas, T., and Ricci, F. Group recommendations with rank aggregation and collaborative
filtering. In ACM conference on Recommender systems,
pp. 119â€“126. ACM, 2010.
Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, pp. 324â€“345, 1952.
Braverman, M. and Mossel, E. Noisy sorting without resampling. In ACM-SIAM SODA, pp. 268â€“276, 2008.
Brin, S. and Page, L. The anatomy of a large-scale hypertextual web search engine. Computer networks and
ISDN systems, 30(1):107â€“117, 1998.
Bubeck, S., Wang, T., and Viswanathan, N. Multiple identifications in multi-armed bandits. ICML, 2013.
Busa-Fekete, R. and HuÌˆllermeier, E.
A survey of
preference-based online learning with bandit algorithms.
In Algorithmic Learning Theory, pp. 18â€“39, 2014.
Busa-Fekete, R., SzoÌˆreÌnyi, B., Weng, P., Cheng, W., and
HuÌˆllermeier, E. Top-k selection based on adaptive sampling of noisy preferences. In ICML, 2013.

Dwork, C., Kumar, R., Naor, M., and Sivakumar, D. Rank
aggregation methods for the web. In Proceedings of the
Tenth International World Wide Web Conference, 2001.
Eriksson, B. Learning to top-k search using pairwise comparisons. In AISTATS, pp. 265â€“273, 2013.
Farnoud, F. and Milenkovic, O. An axiomatic approach to
constructing distances for rank comparison and aggregation. IEEE Trans on Inf Theory, 60(10):6417â€“6439,
2014.
Farnoud, F., Yaakobi, E., and Bruck, J. Approximate sorting of data streams with limited storage. In Computing
and Combinatorics, pp. 465â€“476. Springer, 2014.
Ford, L. R. Solution of a ranking problem from binary
comparisons. American Mathematical Monthly, 1957.
Gabillon, V., Ghavamzadeh, M., Lazaric, A., and Bubeck,
S. Multi-bandit best arm identification. In Neural Information Processing Systems, pp. 2222â€“2230, 2011.
Hajek, B., Oh, S., and Xu, J. Minimax-optimal inference
from partial rankings. In NIPS, pp. 1475â€“1483, 2014.
Harrington, E. F. Online ranking/collaborative filtering using the perceptron algorithm. In ICML, volume 20, pp.
250â€“257, 2003.
Hunter, D. R. MM algorithms for generalized BradleyTerry models. Annals of Statistics, pp. 384â€“406, 2004.
Jain, P., Netrapalli, P., and Sanghavi, S. Low-rank matrix
completion using alternating minimization. In Symposium on Theory of computing, pp. 665â€“674. ACM, 2013.

Busa-Fekete, R., HuÌˆllermeier, E., and SzoÌˆreÌnyi, B.
Preference-based rank elicitation using statistical models: The case of Mallows. In ICML, pp. 1071â€“1079,
2014.

Jamieson, K., Malloy, M., Nowak, R., and Bubeck, S.
lilâ€™UCB: An optimal exploration algorithm for multiarmed bandits. Conference on Learning Theory, 2014.

Candes, Emmanuel, Li, Xiaodong, and Soltanolkotabi,
Mahdi. Phase retrieval via Wirtinger flow: Theory and
algorithms. to appear, IEEE Trans. on Inf. Theory, 2015.

Jamieson, K. G. and Nowak, R. Active ranking using pairwise comparisons. In Advances in Neural Information
Processing Systems, pp. 2240â€“2248, 2011.

Caplin, A. and Nalebuff, B. Aggregation and social choice:
a mean voter theorem. Econometrica, pp. 1â€“23, 1991.

Keshavan, R. H., Montanari, A., and Oh, S. Matrix completion from a few entries. 56(6):2980â€“2998, June 2010.

Spectral MLE: Top-K Rank Aggregation from Pairwise Comparisons

Lu, T. and Boutilier, C. Learning Mallows models with
pairwise preferences. In ICML, pp. 145â€“152, 2011.
Lu, Y. and Negahban, S. N. Individualized rank aggregation using nuclear norm regularization. arXiv preprint
arXiv:1410.0860, 2014.
Luce, R. D. Individual choice behavior: A theoretical analysis. Wiley, 1959.
Negahban, S., Oh, S., and Shah, D. Rank centrality: Ranking from pair-wise comparisons. 2012. URL http:
//arxiv.org/abs/1209.1688.
Netrapalli, P., Jain, P., and Sanghavi, S. Phase retrieval
using alternating minimization. In Advances in Neural
Information Processing Systems, pp. 2796â€“2804, 2013.
Oh, S. and Shah, D. Learning mixed multinomial logit
model from ordinal data. In Neural Information Processing Systems, pp. 595â€“603, 2014.
Osting, B., Xiong, J., Xu, Q., and Yao, Y. Analysis of
crowdsourced sampling strategies for hodgerank with
sparse random graphs. arXiv:1503.00164, 2015.
Park, D., Neeman, J., Zhang, J., Sanghavi, S., and Dhillon,
I. S. Preference completion: Large-scale collaborative
ranking from pairwise comparisons. 2015.
Rajkumar, A. and Agarwal, S. A statistical convergence
perspective of algorithms for rank aggregation from pairwise data. In ICML, pp. 118â€“126, 2014.
Shah, N. B., Balakrishnan, S., Bradley, J., Parekh, A., Ramchandran, K., and Wainwright, M. When is it better to
compare than to score? arXiv:1406.6618, 2014.
Soufiani, H. A., Chen, W. Z., Parkes, D. C., and Xia, L.
Generalized method-of-moments for rank aggregation.
In Neural Information Processing Systems, 2013.
Soufiani, H. A., Parkes, D., and Xia, L. Computing parametric ranking models via rank-breaking. In International Conference on Machine Learning (ICML), 2014a.
Soufiani, H. A., Parkes, D. C., and Xia, L. A statistical
decision-theoretic framework for social choice. In Neural Information Processing Systems, 2014b.
Wauthier, F., Jordan, M., and Jojic, N. Efficient ranking from pairwise comparisons. In ICML, pp. 109â€“117,
2013.

