Ranking from Stochastic Pairwise Preferences:
Recovering Condorcet Winners and Tournament Solution Sets at the Top
Arun Rajkumar
Suprovat Ghoshal
Indian Institute of Science
Lek-Heng Lim
University of Chicago

ARUN R @ CSA . IISC . ERNET. IN
SUPROVAT. GHOSHAL @ CSA . IISC . ERNET. IN

LEKHENG @ GALTON . UCHICAGO . EDU

Shivani Agarwal
Indian Institute of Science

Abstract
We consider the problem of ranking n items from
stochastically sampled pairwise preferences. It
was shown recently that when the underlying
pairwise preferences are acyclic, several algorithms including the Rank Centrality algorithm,
the Matrix Borda algorithm, and the SVMRankAggregation algorithm succeed in recovering a ranking that minimizes a global pairwise disagreement error (Rajkumar and Agarwal,
2014). In this paper, we consider settings where
pairwise preferences can contain cycles. In such
settings, one may still like to be able to recover
‘good’ items at the top of the ranking. For example, if a Condorcet winner exists that beats every other item, it is natural to ask that this be
ranked at the top. More generally, several tournament solution concepts such as the top cycle,
Copeland set, Markov set and others have been
proposed in the social choice literature for choosing a set of winners in the presence of cycles.
We show that existing algorithms can fail to perform well in terms of ranking Condorcet winners
and various natural tournament solution sets at
the top. We then give alternative ranking algorithms that provably rank Condorcet winners, top
cycles, and other tournament solution sets of interest at the top. In all cases, we give finite sample complexity bounds for our algorithms to recover such winners. As a by-product of our analysis, we also obtain an improved sample complexity bound for the Rank Centrality algorithm
to recover an optimal ranking under a BradleyTerry-Luce (BTL) condition, which answers an
open question of Rajkumar and Agarwal (2014).
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

SHIVANI @ CSA . IISC . ERNET. IN

1. Introduction
There has been much interest in recent years in designing algorithms for aggregating pairwise preferences to rank
a set of items (Fürnkranz & Hüllermeier, 2010; Ailon,
2011; Lu & Boutilier, 2011; Jiang et al., 2011; Jamieson &
Nowak, 2011; Negahban et al., 2012; Osting et al., 2013;
Wauthier et al., 2013; Busa-Fekete et al., 2014a;b; Rajkumar & Agarwal, 2014). Indeed, the need for aggregating
pairwise preferences arises in many domains where fully
ordered preferences may be hard to obtain, e.g. in customer
surveys, recommender systems, sports team rankings etc.
In many such settings, particularly those related to applications such as customer surveys and recommender systems,
it is common to assume that the pairwise comparisons follow some (unknown) underlying statistical model. An important question that arises then is, as we observe more and
more data from the underlying statistical model, do the algorithms used converge to a ‘good’ ranking solution?
Recently, Rajkumar & Agarwal (2014) showed that when
the underlying pairwise preferences are acyclic, several algorithms including the Rank Centrality (RC), Matrix Borda
(MB) and SVM-RankAggregation (SVM-RA) algorithms
can converge to an optimal ranking solution in terms of
minimizing a pairwise disagreement error. However, in
practice, pairwise preferences often contain cycles. In such
settings, minimizing the pairwise disagreement error is typically NP-hard, but one may still want to converge to a
ranking that places ‘good’ items at the top.
In this paper, we take ‘good’ items to refer to Condorcet
winners (which beat all other items) when they exist, and
more generally, to tournament solution sets such as top cycles and Copeland and Markov sets, which have been used
to define ‘winners’ in the computational social choice literature (Moulin, 1986; De Donder et al., 2000; Laslier, 1997;
Brandt et al., 2015). We show that when preferences contain cycles, the RC, MB and SVM-RA algorithms can fail
to rank such ‘good’ items at the top; we then propose three

Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top
Table 1. Ranking behavior of various algorithms for ranking from stochastic pairwise preferences under various conditions on the underlying statistical model. Here P BTL is the class of preferences following a Bradley–Terry–Luce model, P LN the class of preferences
following a certain ‘low-noise’ model, P DAG the class of acyclic preferences, and P CW the class of preferences that admit a Condorcet
winner; it is known that P BTL ⊂ P LN ⊂ P DAG ⊂ P CW (see Section 2.3 and Figures 1 and 2). The first three algorithms were analyzed
with respect to pairwise disagreement in (Rajkumar & Agarwal, 2014). Results established in this paper are highlighted in gray.

Algorithm

Minimizes
pairwise disagreement?
P BTL P LN
P DAG

Ranks
Condorcet winner
at top (in P CW )?

Ranks
top cycle
at top (in P)?

Ranks
Copeland set
at top (in P)?

Ranks
Markov set
at top (in P)?

Rank Centrality
Matrix Borda
SVM-RankAggregation

X
X
X

×
X
X

×
×
X

×
×
×

×
×
×

×
×
×

×
×
×

Matrix Copeland
Unweighted Markov
Parametrized Markov

X
×
X

X
×
×

X
×
×

X
X
X

X
X
X

X
×
×

×
X
X

TC(k)

Pn

PnCW
PnDAG
PnLN
PnBTL


= P ∈ Pn

= P ∈ Pn

= P ∈ Pn

= P ∈ Pn

= P ∈ Pn

: | TC(P)| = k

	

	
TC(1)
: P has a Condorcet winner = Pn
	
: GP = ([n], EP ) is a DAG
	
Pn
Pn
: i P j =⇒
k=1 pki >
k=1 pkj
	
wj
: ∃w ∈ Rn
+ s.t. pij = wi +wj ∀i 6= j

Figure 2. Relationships between various conditions on P.

Figure 1. Definitions of various conditions on P.

new algorithms, namely the Matrix Copeland, Unweighted
Markov, and Parametrized Markov algorithms, that provably rank such good items at the top (see Table 1 for a summary). In all cases, we provide explicit sample complexity
bounds for these algorithms to recover (with high probability) a ranking that places the desired elements at the top. As
a by-product of our analysis of the Parametrized Markov algorithm, we also obtain a tighter sample complexity bound
for the RC algorithm, thereby answering an open question
of Rajkumar & Agarwal (2014).

2. Preliminaries and Background
2.1. Problem Setup
Let [n] = {1, . . . , n} denote the set of n items to be ranked,


and [n]
= {(i, j) ∈ [n] × [n] : i < j} the set of n2
2
pairs that can be compared. We assume that there is an
underlying (unknown) probability distribution µ ∈ ∆(n)
2
such that each time a comparison is to be made, a pair
(i, j) is selected with probability µij , and that for each
i < j, there is an (unknown) parameter pij ∈ [0, 1] such
that when items i and j are compared, item j ‘beats’ (or is
preferred to) item i with probability pij . For each i < j,
define pji = 1 − pij ; also define pii = 0 for every i. Denote by P = [pij ] the resulting (unknown) pairwise preference matrix. Given a finite sample of pairwisecomparisons
m
S = ((i1 , j1 , y1 ), . . . , (im , jm , ym )) ∈ [n]
2 × {0, 1}
drawn according to (µ, P) as above (where pairs (ik , yk )

are drawn iid according to µ, and given (ik , jk ), yk ∼
Bernoulli(pik ,jk )), the goal is to construct a ranking or permutation σ ∈ Sn that ranks ‘good’ elements at the top.
Most algorithms we consider operate on an empirical pairb constructed from S as follows:
wise comparison matrix P
 (1)

if i < j and mij > 0
mij /mij

pbij = 1 − m(1)
/m
if i > j and mji > 0 (1)
ji
ji


0
otherwise,
Pm
where mij = k=1 1(ik = i, jk = j) ;
Pm
(1)
mij = k=1 1(ik = i, jk = j, yk = 1) .
Binary Relation and Tournament Induced by P. We
will assume throughout that the matrix P does not involve
ties, i.e., pij 6= 21 for all i, j, and will find it convenient to
define the binary relation P on [n] as follows:
i P j ⇐⇒ pij < 12
for all i, j ∈ [n] .
We will also define the edge set associated with P as
EP = {(i, j) ∈ [n] × [n] : i P j} ,

and the induced graph GP = [n], EP . Note that under
the assumption pij 6= 12 for all i, j, the graph GP is always
a complete directed graph, i.e., a tournament.
2.2. Measuring Ranking Performance
We will consider several notions of ‘goodness’ of a ranking
with respect to the underlying pairwise preferences P.

Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top

Pairwise Disagreement. Define the pairwise disagreement (PD) error of a permutation σ ∈ Sn w.r.t. P as
X
erPD
1(pij < 21 ) · 1(σ(i) > σ(j)) .
P [σ] =
i6=j

The pairwise disagreement error measures the number of
pairs on which σ disagrees with P, and is a natural measure
of overall ranking performance; a variant of this was studied in (Rajkumar & Agarwal, 2014). In general, however,
even if P is known directly, identifying an optimal ranking w.r.t. PD error is computationally hard (it corresponds
to the NP-hard minimum feedback arc set problem). Thus
one can hope to recover an optimal ranking w.r.t. PD error only under certain restrictive conditions on P; indeed,
all conditions studied in (Rajkumar & Agarwal, 2014) assumed GP is acyclic.
In this paper, we are interested in algorithms that can ensure
‘good’ elements are ranked at the top even when the graph
GP contains cycles; we formalize the notion of ‘good’ below in terms of Condorcet winners, top cycles, and other
tournament solution sets (see also Figure 3).
Condorcet Winners. An element i ∈ [n] is said to be
a Condorcet winner w.r.t. P if it beats all other elements
under P, i.e., if i P j ∀j 6= i. A Condorcet winner does
not always exist, but when it does, it is unique, and it is then
natural to want this element to be ranked at the top. When
a Condorcet winner exists, we will denote it by CW(P).
Top Cycles. More generally, one can ask that elements of
the top cycle of P be ranked at the top. The top cycle of P,
denoted TC(P), is defined as the smallest set of elements
W ⊆ [n] such that every element in W beats every element
not in W under P, i.e. such that i P j ∀i ∈ W, j ∈
/ W.
A Condorcet winner corresponds to a top cycle of size 1,
i.e. when | TC(P)| = 1, we have TC(P) = {CW(P)}.
The top cycle is also referred to as the Smith set in voting
theory literature (Smith, 1973).
Copeland and Markov Sets. The top cycle is one form
of tournament solution set, which selects a set of elements
considered to be ‘winners’ in a tournament (Laslier, 1997;
Brandt et al., 2015). One can also consider other notions of
tournament solution sets associated with P (or more specifically, associated with the tournament GP induced by P),
and ask that elements of the desired tournament solution
set be ranked at the top. Two such tournament solution sets
that we will be interested in are the Copeland set and the
Markov set, both of which are ‘score-based’ solution sets
that select all elements maximizing a certain ‘score’. The
Copeland set of P, denoted CO(P), selects elements with
maximal out-degree in the (unweighted) tournament
GP :
P
CO(P) = argmaxi∈[n] d(i), where d(i) = j 1(i P j).
The Markov set of P, denoted MA(P), selects elements
with highest stationary probability under a certain Markov
chain on the tournament GP : MA(P) = argmaxi∈[n] πi ,

Figure 3. Left: A tournament on n = 4 nodes in which there are
cycles but node 1 is a Condorcet winner. Middle: A tournament
on n = 4 nodes in which there is no Condorcet winner. Here the
set of nodes {1, 2, 3} forms a top cycle of size 3; in this case this
set also corresponds to the Copeland and Markov sets. Right: A
tournament on n = 5 nodes in which the top cycle {1, 2, 3, 4} is
a strict superset of the Copeland set {1, 3} and Markov set {1}.

where π is the stationary probability vector of the Markov
e defined as peij = 1 for all (i, j) : j P i and
chain P
n
P
peii = 1 − j6=i peij for all i. It is known that the Copeland
and Markov sets are refinements of the top cycle, i.e. that
CO(P) ⊆ TC(P) and MA(P) ⊆ TC(P), and that these
containments can be strict (see Figure 3 for illustrations).
2.3. Conditions on the Pairwise Preference Matrix P
As noted above, we are interested in understanding the
ranking behavior of various algorithms under various conditions on the pairwise preference matrix P. Let Pn denote
the set of all such matrices on n items not involving ties:

Pn =
P ∈ [0, 1]n×n : pij = 1 − pji ∀i 6= j ;
	
pij 6= 12 ∀i, j ; pii = 0 ∀i .
TC(k)

to be the set of preference
For each k ∈ [n], define Pn
matrices P in Pn that have a top cycle of size k.1 Clearly,
TC(1)
is the set of preference matrices in Pn that have a
Pn
Condorcet winner; we will also denote this by PnCW . Next,
define PnDAG to be the set of preference matrices P in Pn for
which the directed graph GP induced by the binary relation
P is acyclic,2 and define PnLN to be the set of matrices
in Pn satisfying the ‘low-noise’ (LN) condition (Rajkumar
& Agarwal, 2014). Finally, define PnBTL to be the set of
preference matrices in Pn that follow a Bradley-Terry-Luce
(BTL) model. Formal definitions of all these sets are given
TC(1)
TC(n)
in Figure 1. It can be verified that Pn
, . . . , Pn
form a partition of Pn , and that
PnBTL ⊂ PnLN ⊂ PnDAG ⊂ PnCW = PnTC(1) ,
with each containment above being strict (see Figure 2).
When n is clear from context, we will write P, P BTL , etc.

3. Related Work and Existing Results
As noted above, there has been much work in recent years
on developing algorithms for ranking a set of items from
1

Note that under a strict tournament (where there are no ties),
TC(2)
there cannot be a top cycle of size 2, i.e. Pn
= ∅.
2
The DAG condition is equivalent to the ‘generalized lownoise’ (GLN) condition studied in (Rajkumar & Agarwal, 2014).

Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top

pairwise preferences; below we discuss three specific algorithms that form the backdrop to our work. Before doing so, we point out that our work differs from that of
(Brandt & Fischer, 2007), where any ranking algorithm
that given P ∈ Pn produces a score vector f ∈ Rn is
considered to induce a corresponding tournament solution
TS(P) = argmaxi∈[n] fi ; in particular, Brandt & Fischer
study properties of the tournament solution induced in this
manner by PageRank scores. Our work also differs from
(Braverman & Mossel, 2009; Wauthier et al., 2013), where
pairs are sampled only once and the conditions on P are
significantly stronger than those assumed here.
Three representative ranking algorithms studied in recent
years that are most related to our work are the Rank Centrality (RC) algorithm (Negahban et al., 2012), the Matrix Borda (MB) algorithm (Rajkumar & Agarwal, 2014;
Borda, 1781), and the SVM-RankAggregation (SVM-RA)
algorithm (Rajkumar & Agarwal, 2014). RC produces a
ranking by sorting the stationary probabilities of a Markov
chain constructed from pairwise data. MB ranks items
based on the average probability of an item being preferred over the rest of the items. SVM-RA constructs a binary classification dataset from the pairwise data and ranks
items based on the maximum margin hyperplane for this
dataset. Detailed descriptions of these algorithms are given
in the appendix; below we summarize results of (Rajkumar
& Agarwal, 2014) which establish convergence of these algorithms to an optimal permutation w.r.t. PD error within
P BTL , P LN , and P DAG , respectively (see Table 2 for notation).3
Theorem 1 (Rank Centrality minimizes PD error in P BTL ).
Let µmin > 0. Let P ∈ PnBTL . Let π be the stationary
probability vector of the Markov chain P defined as pij =
P
pij
j6=i pij ∀i, and let rmin =
n ∀i 6= j and pii = 1 −
mini,j:πi 6=πj |πi − πj |. Let δ ∈ (0, 1]. If
m ≥ max


 9216 n 1  1
3  16n2 
− 1 ln
, Bµ ,
2
2
2
rmin µmin pmin pmin
δ

then with probability at least 1−δ (over S), the permutation
b satisfies
σ
bRC produced by running the RC algorithm on P
PD
σ
bRC ∈ argminσ∈Sn erP [σ] .
LN
Theorem 2 (Matrix Borda minimizes PD error
Pn in P ).
1
LN
∗
Let µmin > 0. Let P ∈ Pn . Let fi = n k=1 pki ∀i,
and let rmin = mini,j:fi∗ 6=fj∗ |fi∗ − fj∗ |. Let δ ∈ (0, 1]. If

m ≥ max



1152
2
rmin
µ2min

ln

 4n2 
δ


, Bµ ,

then with probability at least 1−δ (over S), the permutation
b satisfies
σ
bMB produced by running the MB algorithm on P
PD
σ
bMB ∈ argminσ∈Sn erP [σ] .
3

We note that the form of the PD error considered in (Rajkumar & Agarwal, 2014) is slightly different from that considered
here, but within P DAG both have the same sets of minimizers.

Table 2. Useful quantities associated with µ and P.
µmin
mini<j µij


Bµ
3 µ12
+
3 ln µ12
+3
2
2
min

pmin
γmin
γTC

min

mini6=j pij


mini6=j pij − 12 


pij − 1 
mini∈TC(P),j ∈TC(P)
/
2

Theorem 3 (SVM-RA minimizes PD error in P DAG ). Let
µmin > 0.P
Let P ∈ PnDAG . P
Let α ∈ Rn be such that i P
n
n
4
α
j =⇒ P k=1 αk pki >
k=1 αk pjk , and let rmin =
n
| k=1 αk (Pki −Pkj )|
. Let δ ∈ (0, 1]. If
mini6=j
kαk2
m ≥ max



2048 n
α
(rmin
)2 µ2min

ln

 16n3 
δ

,

128
2
γmin
µ2min

ln

 8n2 
δ


, Bµ ,

then with probability at least 1−δ (over S), the permutation
b satisfies
σ
bSVM-RA produced by running SVM-RA on P
σ
bSVM-RA ∈ argminσ∈Sn erPD
P [σ] .
We will make use of the following result of (Rajkumar &
b around P:
Agarwal, 2014) on concentration of entries of P
Lemma 4. Let µmin > 0 and
 P ∈ Pn . Fix any i 6= j. Let
√
ln( 4δ ), Bµ , then with
0 <  < 4 2. If m ≥ max 2128
µ2
min

probability at least 1 − δ (over S), |pij − pbij | <  .

4. Performance of RC, MB and SVM-RA
Algorithms Under Cyclic Preferences
It is easy to see that for P ∈ PnCW ,
σ
b ∈ argminσ∈Sn erPD
b−1 (1) = CW(P) .
P [σ] =⇒ σ
Therefore it follows from Theorems 1–3 that (when given
a sufficiently large sample, with high probability) the RC
algorithm ranks the Condorcet winner at the top for P ∈
P BTL ; the MB algorithm does so for P ∈ P LN ; and the
SVM-RA algorithm does so for P ∈ P DAG . However, as
the following examples show, outside P DAG , these algorithms can fail to rank the Condorcet winner, top cycle, or
Copeland or Markov sets at the top (in fact RC and MB can
fail to do so even in P LN \ P BTL and P DAG \ P LN , respectively; see Examples 6–7 in the supplementary material).
Example 1 (For P ∈ P CW \ P DAG , RC, MB and SVM-RA
algorithms can fail to rank the Condorcet winner/Copeland
set/Markov set at the top). Let n = 7, and consider
 0

0.49 0.49 0.49 0.49 0.49 0.49
0.51

0.51

P = 0.51
0.51
0.51
0.51

0
0.9
0.9
0.9
0.9
0.9

0.1
0
0.6
0.6
0.4
0.4

0.1
0.4
0
0.6
0.4
0.6

0.1
0.4
0.4
0
0.6
0.4

0.1
0.6
0.6
0.4
0
0.6

0.1
0.6 

0.4  .
0.6 
0.4
0

4
Such an α always exists by equivalence of the DAG and GLN
conditions (Rajkumar & Agarwal, 2014).

Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top

The tournament induced by P is shown in Figure 4 (left).
It can be verified that P ∈ P CW \ P DAG , with CW(P) = 1
and correspondingly CO(P) = MA(P) = {1}. However
the permutations σRC , σMB and σSVM-RA produced by running RC, MB and SVM-RA on the above preference matrix
P do not rank item 1 at the top; instead, it can be verified
−1
−1
−1
that σRC
(1) = σMB
(1) = 2 and σSVM-RA
(1) = 7.
Example 2 (For P ∈
/ P CW , RC, MB and SVM-RA algorithms can fail to rank the top cycle/Copeland set/Markov
set at the top). Let n = 6, and consider


P=

0
 0.7
 0.3
 0.6
0.55
0.55

0.3
0
0.7
0.6
0.55
0.55

0.7
0.3
0
0.6
0.55
0.55

0.4
0.4
0.4
0
0.9
0.9

0.45
0.45
0.45
0.1
0
0.4

0.45
0.45

0.45
0.1  .
0.6
0

The tournament induced by P is shown in Figure 4 (middle). It can be verified that P ∈ P TC(3) , with TC(P) =
{1, 2, 3} = CO(P) = MA(P). However the permutations
σRC , σMB and σSVM-RA produced by running RC, MB and
SVM-RA on the above matrix P do not rank these tournament solution sets at the top; indeed, it can be verified that
σRC = σMB = (4 1 2 3 6 5) and σSVM-RA = (4 2 6 1 3 5).
The above examples lead us to consider alternative ranking algorithms that rank Condorcet winners, top cycles, and
Copeland/Markov sets at the top even when the underlying
preference matrix P induces a cyclic tournament. Proofs
of all results can be found in the supplementary material.

5. Recovering Condorcet Winners, Top Cycles
and Copeland Sets at the Top
Let us start by considering the Copeland set. Recall that
this is defined as the set of items with maximal out-degree
in the underlying preference-induced tournament GP . A
natural approach to ranking the Copeland set at the top
would therefore be to order items by their observed outb as this emdegrees in the empirical comparison matrix P;
pirical matrix approaches the true preference matrix P, one
would expect items in the Copeland set to appear at the top.
The corresponding algorithm is shown in Algorithm 1; we
term this the Matrix Copeland (MC) algorithm due to its
similarity to the Copeland voting rule (Copeland, 1951).
The following result shows this algorithm indeed ranks the
Copeland set at the top for any preference matrix and also
minimizes the pairwise disagreement error when the preference matrix is acyclic:
Theorem 5 (Matrix Copeland ranks Copeland set at top
for a general P and minimizes PD error if P ∈ P DAG ). Let
µmin > 0. Let P ∈ Pn and let δ ∈ (0, 1]. If
m ≥ max




 4n2 
512
, Bµ ,
ln
2
2
γmin µmin
δ

then with probability at least 1−δ (over S), the permutation
b satisfies
σ
bMC produced by running the MC algorithm on P

Algorithm 1 Matrix Copeland (MC) Algorithm
Input: Pairwise comparison matrix P ∈ [0, 1]n×n
satisfying the following conditions:
(i) for all i 6= j: pij + pji = 1 or pij = pji = 0
(ii) for every i: pii = 0
Pn
• For i = 1 to n: f i = n1 j=1 1(pji > 12 )
Output: Permutation σ MC ∈ argsort(f )
σ
bMC (i) < σ
bMC (j) for all i ∈ CO(P), j ∈
/ CO(P) .
Moreover, if P ∈ P DAG , then
σ
bMC ∈ argminσ∈Sn erPD
P [σ]
In fact, as the following result shows, the MC algorithm
also recovers the top cycle (and therefore the Condorcet
winner whenever it exists) at the top:
Theorem 6 (Matrix Copeland ranks top cycle at top). Let
TC(k)
for some k ∈ [n − 1]. Let
µmin > 0. Let P ∈ Pn
δ ∈ (0, 1]. If
m ≥ max



512
2
µ2min
γTC

ln

 4n2 
δ


, Bµ ,

then with probability at least 1−δ (over S), the permutation
b satisfies
σ
bMC produced by running the MC algorithm on P
σ
bMC (i) < σ
bMC (j) for all i ∈ TC(P), j ∈
/ TC(P) .
While the MC algorithm is good for recovering the
Copeland set and the top cycle or the Condorcet winner
when it exists, as the following example illustrates, it does
not necessarily rank the Markov set at the top:
Example 3 (Matrix Copeland can fail to rank Markov set
at top). Let n = 8, and consider
 0
0.51
0.4
0.6
0.6 0.6
0.4
0.4 
0.49

 0.6
 0.4
P=
 0.4
 0.4
0.6
0.6

0
0.51
0.51
0.4
0.4
0.51
0.51

0.49
0
0.4
0.6
0.4
0.4
0.6

0.49
0.6
0
0.4
0.6
0.6
0.6

0.6
0.4
0.6
0
0.6
0.4
0.4

0.6
0.6
0.4
0.4
0
0.6
0.6

0.49
0.6
0.4
0.6
0.4
0
0.6

0.49
0.4 

0.4 
0.6  .
0.4 
0.4
0

The tournament induced by P is shown in Figure 4 (right).
It can be verified that MA(P) = {5}. However the permutation σMC produced by running the MC algorithm on the
above matrix P does not rank item 5 at the top; indeed, it
can be verified that σMC = (2 4 6 5 1 7 3 8).

6. Recovering Condorcet Winners, Top Cycles
and Markov Sets at the Top
The Matrix Copeland algorithm considered above can be
viewed as running the Matrix Borda algorithm on a transformed input matrix H where hij = 1(pij > 12 ) ∀i 6= j,
which thresholds the preferences pij at 12 to ‘amplify’ them
to 0 or 1, and has the effect of emphasizing more preferred

Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top

Figure 4. Tournaments induced by the preferences matrices in Examples 1–3. An edge from one group of nodes to another indicates
that the graph contains edges from all nodes in the first group to all those in the second group. Left: Here n = 7 and TC(P) =
CO(P) = MA(P) = {1}. Middle: Here n = 6 and TC(P) = CO(P) = MA(P) = {1, 2, 3}. Right: Here n = 8 and
TC(P) = {1, 2, 3, 4, 5, 6, 7, 8}, CO(P) = {2, 4, 6}, and MA(P) = {5}.

elements more strongly. As we saw above, this yielded desirable properties in terms of ranking Condorcet winners,
top cycles and Copeland sets (though not Markov sets) at
the top, while also maintaining the PD optimality properties of the MB algorithm in P LN (in fact, MC recovers a
PD-optimal ranking for all P in the larger set P DAG ).
In order to recover Markov sets at the top, one might consider running the Rank Centrality algorithm, which ranks
items according to the stationary vector associated with a
certain Markov chain associated with the input matrix, on
a similarly transformed matrix. Below we start by considering in Section 6.1 the Unweighted Markov (UM) ranking
algorithm, which does exactly this: it runs the RC algorithm on the same transformed matrix H described above.
As we will see below, this algorithm indeed ranks Condorcet winners, top cycles and Markov sets (though not
Copeland sets) at the top. Unfortunately, however, as we
will see, the UM algorithm fails to maintain the PD optimality properties of the RC algorithm in P BTL .
In Section 6.2, we propose instead the Parametrized
Markov (PM) algorithm, which runs the RC algorithm on
c
a matrix H obtained by applying a soft transformation
parametrized by a real number c ≥ 1 to the input matrix
P. This also amplifies the probabilities in the input matrix,
but not to the extremes of 0 and 1 (the hard transform H
above corresponds to c = ∞). We will see that for suitable
choices of c, the resulting PM algorithm can be made to
both rank Condorcet winners, top cycles and Markov sets
at the top, and maintain the PD optimality properties of the
RC algorithm in P BTL . As a by-product of our analysis,
we also obtain an improved sample complexity bound for
the RC algorithm to recover a PD-optimal ranking, thus answering an open question of Rajkumar & Agarwal (2014).
6.1. Unweighted Markov Algorithm
The UM algorithm is shown in Algorithm 2. We first show
this algorithm indeed recovers the Markov set at the top:
Theorem 7 (Unweighted Markov ranks Markov set at top).
Let µmin > 0. Let P ∈ Pn . Let δ ∈ (0, 1]. If
m ≥ max



512
2
γmin
µ2min

ln

 4n2 
δ


, Bµ ,

then with probability at least 1−δ (over S), the permutation

Algorithm 2 Unweighted Markov (UM) Algorithm
Input: Pairwise comparison matrix P ∈ [0, 1]n×n
satisfying the following conditions:
(i) for all i 6= j: pij + pji = 1 or pij = pji = 0
(ii) for every i: pii = 0
• Define H ∈ [0, 1]n×n as
(
1(pij > 12 ) if i 6= j
hij =
0
otherwise.
• Run Rank Centrality algorithm on input matrix H;
obtain stationary probability vector π
Output: Permutation σ UM ∈ argsort(π)
b satisfies
σ
bUM produced by running the UM algorithm on P
σ
bUM (i) < σ
bUM (j) for all i ∈ MA(P), j ∈
/ MA(P) .
In fact, as with the MC algorithm, the UM algorithm also
recovers the top cycle (and therefore the Condorcet winner
whenever it exists) at the top:
Theorem 8 (Unweighted Markov ranks top cycle at top).
TC(k)
for some k ∈ [n − 1]. Let
Let µmin > 0. Let P ∈ Pn
δ ∈ (0, 1]. If
m ≥ max



512
2
γmin
µ2min

ln

 4n2 
δ


, Bµ ,

then with probability at least 1−δ (over S), the permutation
b satisfies
σ
bUM produced by running the UM algorithm on P
σ
bUM (i) < σ
bUM (j) for all i ∈ TC(P), j ∈
/ TC(P) .
Again, as with the MC algorithm, the following example
shows that while UM is good for recovering the Markov set
and the top cycle or the Condorcet winner when it exists, it
does not necessarily rank the Copeland set at the top:
Example 4 (Unweighted Markov can fail to rank Copeland
set at top). Let n = 8, and consider again the matrix P
considered in Example 3. It can be verified that CO(P) =
{2, 4, 6}. However the permutation σUM produced by running the UM algorithm on P does not rank this set at the
top; indeed, it can be verified that σUM = (5 2 4 6 7 3 8 1).
Unfortunately, however, unlike the MC algorithm, the UM
algorithm fails to maintain the PD optimality properties of
the RC algorithm in P BTL :

Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top

Example 5 (Unweighted Markov can fail to minimize PD
error in P BTL ). Let n = 3, and consider


0
0.4
0.25
0
0.33 .
P = 0.6
0.75

0.67

0

It can be verified that P ∈ P BTL (consider the score vector
w = (3, 2, 1)> ), and that the PD error is uniquely minimized by σ = (1 2 3). However the permutation σUM produced by running the UM algorithm on P is σUM = (1 3 2),
and therefore this does not minimize the PD error.
Next we consider an alternative algorithm that will achieve
the best properties of both the UM and RC algorithms.
6.2. Parametrized Markov Algorithm
We now consider the Parametrized Markov (PM) ranking
algorithm shown in Algorithm 3, which effectively applies
c
the RC algorithm to a matrix H obtained by applying a
‘soft’ transform gc : [0, 1]→[0, 1], parametrized by c ≥ 1,
to the entries of the input matrix P:
pc
.
pc + (1 − p)c

1
0.8

gc (p)

gc (p) =

0.6

See Figure 5 for an illustrac=1
c=2
tion. With c = 1, one rec=5
c = 50
covers the RC algorithm; with
c=∞
c = ∞, one recovers the UM
p
algorithm. The PM algorithm Figure 5. The function gc .
can therefore be viewed as
interpolating between these two extremes. As we show
below, with a suitable choice of c, the PM algorithm gives
the best of both worlds: as with RC, it minimizes the
PD error in P BTL ; and as with UM, it ranks Condorcet
winners, top cycles, and Markov sets at the top.
0.4
0.2

0

0

0.2

0.4

0.6

0.8

1

6.2.1. PM A LGORITHM M INIMIZES PD E RROR IN P BTL
We first show that for any choice of 1 ≤ c < ∞, the PM
algorithm minimizes the PD error in P BTL :
Theorem 9 (Parametrized Markov minimizes PD error in
P BTL ). Let µmin > 0. Let P ∈ PnBTL . Let rmin =
mini6=j,k6=i,j |pik − pjk |. Let δ ∈ (0, 1]. Let 1 ≤ c < ∞. If
m ≥ max




 4n2 
512
,
B
,
ln
µ
2
2
min(rmin
, γmin
)µ2min
δ

then with probability at least 1−δ (over S), the permutation
b satisfies
σ
bPM produced by running the PM algorithm on P
σ
bPM ∈ argminσ∈Sn erPD
P [σ] .
The proof of Theorem 9 in fact establishes PD-optimality
of the PM algorithm (and therefore the RC algorithm,
which is a special case with c = 1) for a slightly larger
set of preference matrices than P BTL , namely, for P satisfying the restricted low-noise (RLN) property, defined as

Algorithm 3 Parametrized Markov (PM) Algorithm
Input: Pairwise comparison matrix P ∈ [0, 1]n×n
satisfying the following conditions:
(i) for all i 6= j: pij + pji = 1 or pij = pji = 0
(ii) for every i: pii = 0
Parameter: c ≥ 1
c

• Define H ∈ [0, 1]n×n as
(
gc (pij ) if i 6= j
c
hij =
0
otherwise.
c

• Run Rank Centrality algorithm on input matrix H ;
obtain stationary probability vector π c
Output: Permutation σ PM ∈ argsort(π c )
n
o
PnRLN = P ∈ P n : ∀ i 6= j 6= k : i P j =⇒ pkj < pki .

It is not hard to show that PnBTL ⊂ PnRLN ⊂ PnLN . The
proof of Theorem 9 then follows from the following three
observations: (a) for P ∈ PnRLN , running the RC algorithm
on P yields a PD-optimal permutation; (b) for large enough
sample size, if P is in PnRLN , then with high probability
b and (c) the RLN property is preserved by the gc
so is P;
transform (see supplementary material for details).
Remark (Improved sample complexity bound for RC algorithm). As noted above, the sample complexity bound in
Theorem 9 holds for any 1 ≤ c < ∞, and so in particular
it holds for the RC algorithm. Comparing this with the previous bound of (Rajkumar & Agarwal, 2014) in Theorem 1,
we see that the bound in Theorem 9 is in general considerably tighter; in particular, it removes an extraneous factor
of n present in the earlier bound. This answers in the affirmative an open question of (Rajkumar & Agarwal, 2014).
6.2.2. F OR S UITABLE c, PM A LGORITHM R ECOVERS
M ARKOV S ETS AND T OP C YCLES AT THE T OP
We now show that for sufficiently large (but finite) c, which
can be chosen based on the observed empirical comparison
b the PM algorithm also recovers Markov sets and
matrix P,
top cycles (and therefore Condorcet winners) at the top:
Theorem 10 (Parametrized Markov ranks Markov set at
b =
top). Let µmin > 0. Let P ∈ Pn . Let β2 (P)
b is obtained by running
π
bmax − maxk∈MA(
bk where π
b π
/
P)
p
b
b
UM on P. Let δ ∈ (0, 1]. Let α
bmin = mini j ji . If
P

c≥

b
ln(4n/β2 (P))
ln(b
αmin )

p
bij

and

m ≥ max



512
2
γmin
µ2min

ln

 4n2 
δ


, Bµ ,

then with probability at least 1−δ (over S), the permutation
b satisfies
σ
bPM produced by running the PM algorithm on P
σ
bPM (i) < σ
bPM (j) for all i ∈ MA(P), j ∈
/ MA(P) .

Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top
PD error minimization in P ∈ P DAG \P LN

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
2

3

10

4

10

5

10

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
2

6

10

Optimal recovery fraction

Optimal recovery fraction

RC
MB
SVM-RA
MC
UM
PM

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
2

10

3

4

10

5

10

4

10

Condorcet recovery in P CW \ P DAG

5

10

10

1

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

6

2

10

10

Sample size (m)

1

RC
MB
SVM-RA
MC
UM
PM

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
3

4

10

10

5

10

Sample size (m)

6

10

RC
MB
SVM-RA
MC
UM
PM

0.9

3

10

Sample size (m)
Copeland set recovery in P \ P CW

1

0.8

3

10

10

Sample size (m)
Top cycle recovery in P \ P CW
0.9

RC
MB
SVM-RA
MC
UM
PM

4

10

5

10

10

6

10

Sample size (m)
Markov set recovery in P \ P CW
Optimal recovery fraction

0.8

1

Optimal recovery fraction

RC
MB
SVM-RA
MC
UM
PM

0.9

Optimal recovery fraction

Optimal recovery fraction

PD error minimization in P BTL
1

1

RC
MB
SVM-RA
MC
UM
PM

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
3

10

4

10

5

10

6

10

Sample size (m)

Figure 6. Experimental results. See text for details.

Theorem 11 (Parametrized Markov ranks top cycle at top).
TC(k)
for some k ∈ [n − 1]. Let
Let µmin > 0. Let P ∈ Pn
p
bji
ln(4n)
α
bmin = miniP j pbij . Let δ ∈ (0, 1]. If c ≥ (n+1)
ln(b
αmin )
 512
 4n2 

and
m ≥ max

2
γmin
µ2min

ln

δ

, Bµ ,

then with probability at least 1−δ (over S), the permutation
b satisfies
σ
bPM produced by running the PM algorithm on P
σ
bPM (i) < σ
bPM (j) for all i ∈ TC(P), j ∈
/ TC(P) .
The proofs of Theorems 10–11 make use of the Cho-Meyer
perturbation bound for Markov chains (Cho & Meyer,
2001) to bound the difference between the stationary vector
π c and π. (see supplementary material for details).

7. Experiments
We conducted experiments to compare the performance
of different algorithms, including existing algorithms (RC,
MB, SVM-RA) and the proposed algorithms (MC, UM,
PM), both in terms of minimizing PD error and recovering
‘good’ items at the top (the parameter c for PM algorithm
was chosen as the maximum of the values prescribed by
Theorems 10 and 11). In all our experiments, we generated 100 training samples S (as described in Section 2.1)
from an underlying preference matrix P of interest (using
a uniform distribution µ on pairs) each of a number of sizes
m, ran the various algorithms on the samples, and for each
algorithm, counted the fraction of times (out of 100) that
the returned permutation satisfied a desired property, such
as PD error minimization or top cycle recovery at the top.
PD error minimization. For this we used two acyclic
BTL
preference matrices: a matrix P ∈ P10
generated from
a score vector w drawn uniformly from [0, 1]10 , and the
matrix P ∈ (P5DAG \ P5LN ) described in (Rajkumar & Agarwal, 2014). The results are shown in the first two plots in
Figure 6. As expected, for the first P, all algorithms except

UM recover an optimal ranking w.r.t. PD error, while for
the second P, only SVM-RA and MC do so.
Condorcet winner and top cycle recovery. For this we
used two preference matrices with cycles: the matrix P ∈
P7CW \ P7DAG described in Example 1 (see also Figure 4,
TC(3)
left), and the matrix P ∈ P6
⊂ (P6 \ P6CW ) described
in Example 2 (see also Figure 4, middle). The results are
shown in the 3rd and 4th plots in Figure 6. In this case, only
the new algorithms considered in this paper (MC, UM, PM)
recover the Condorcet winner and the top cycle at the top.
Copeland set recovery. For this we used the cyclic preference matrix P ∈ (P8 \ P8CW ) described in Example 3 (see
also Figure 4, right). The results are shown in the 5th plot in
Figure 6. As expected, only the MC algorithm successfully
recovers the Copeland set at the top.
Markov set recovery. For this we again used the cyclic
matrix P ∈ (P8 \ P8CW ) described in Example 3, for which
MA(P) 6= CO(P). The results are shown in the 6th plot
in Figure 6. As expected, in this case only the UM and PM
algorithms correctly recover the Markov set at the top.

8. Conclusion
In this paper, we investigated convergence properties of algorithms for ranking from pairwise preferences when preferences can contain cycles. When a globally optimal ranking is hard to compute, it is natural to ask that ‘good’ items,
such as Condorcet winners or suitable tournament solution
sets, be ranked at the top. We showed that several ranking
algorithms developed in recent years fail in this respect,
and designed new algorithms that rank Condorcet winners,
top cycles, Copeland and Markov sets at the top, while retaining or improving guarantees of previous algorithms for
acyclic preferences. Future work includes exploring alternative complexity parameters for our bounds and considering similar objectives in an active ranking setting.

Ranking from Stochastic Pairwise Preferences: Recovering Condorcet Winners and Tournament Solution Sets at the Top

Acknowledgements. Thanks to the anonymous reviewers for their helpful comments. AR is supported by a Microsoft Research India PhD Fellowship. The work of LHL
is supported by AFOSR FA9550-13-1-0133, NSF DMS1209136, and NSF DMS-1057064. SA thanks DST and
Indo-US Science & Technology Forum for their support.

References
Ailon, Nir. Active learning ranking from pairwise preferences with almost optimal query complexity. In Advances in Neural Information Processing Systems, 2011.

Jiang, Xiaoye, Lim, Lek-Heng, Yao, Yuan, and Ye, Yinyu.
Statistical ranking and combinatorial Hodge theory.
Mathematical Programming, 127(1):203–244, 2011.
Laslier, J.-F. Tournament Solutions and Majority Voting.
Springer-Verlag, 1997.
Lu, Tyler and Boutilier, Craig. Learning Mallows models
with pairwise preferences. In Proceedings of the 28th
International Conference on Machine Learning, 2011.
Moulin, Hervé. Choosing from a tournament.
Choice and Welfare, 3(4):271–291, 1986.

Social

Borda, Jean-Charles de. Mémoire sur les élections au
scrutin. Histoire de l’Académie Royale des Sciences,
1781.

Negahban, Sahand, Oh, Sewoong, and Shah, Devavrat.
Iterative ranking from pair-wise comparisons. In Advances in Neural Information Processing Systems, 2012.

Brandt, Felix and Fischer, Felix. PageRank as a weak tournament solution. In Proceedings of the 3rd International
Workshop on Internet and Network Economics, 2007.

Osting, Braxton, Brune, Christoph, and Osher, Stanley. Enhanced statistical rankings via targeted data collection.
In Proceedings of the 30th International Conference on
Machine Learning, 2013.

Brandt, Felix, Brill, Markus, and Harrenstein, Paul. Tournament solutions. In Handbook of Computational Social
Choice. Cambridge University Press, To appear, 2015.
Braverman, Mark and Mossel, Elchanan. Sorting from
noisy information. arXiv preprint arXiv:0910.1191,
2009.
Busa-Fekete, Róbert, Hüllermeier, Eyke, and Szörényi,
Balázs. Preference-based rank elicitation using statistical models: The case of Mallows. In Proceedings of
the 31st International Conference on Machine Learning,
2014a.
Busa-Fekete, Róbert, Szörényi, Balázs, and Hüllermeier,
Eyke. PAC rank elicitation through adaptive sampling
of stochastic pairwise preferences. In Proceedings of the
28th AAAI Conference on Artificial Intelligence, 2014b.
Cho, Grace E. and Meyer, Carl D. Comparison of perturbation bounds for the stationary distribution of a Markov
chain. Linear Algebra and its Applications, 335(1–3):
137–150, 2001.
Copeland, A. H. A ‘reasonable’ social welfare function. In
Seminar on Mathematics in Social Sciences, University
of Michigan, 1951.
De Donder, Philippe, Le Breton, Michel, and Truchon,
Michel. Choosing from a weighted tournament. Mathematical Social Sciences, 40(1):85–109, 2000.
Fürnkranz, Johannes and Hüllermeier, Eyke. Preference
Learning. Springer, 2010.
Jamieson, Kevin G. and Nowak, Rob. Active ranking using
pairwise comparisons. In Advances in Neural Information Processing Systems, 2011.

Rajkumar, Arun and Agarwal, Shivani. A statistical convergence perspective of algorithms for rank aggregation
from pairwise data. In Proceedings of the 31st International Conference on Machine Learning, 2014.
Seneta, E. Perturbation of the stationary distribution measured by ergodicity coefficients. Advances in Applied
Probability, pp. 228–230, 1988.
Smith, John H. Aggregation of preferences with variable
electorate. Econometrica: Journal of the Econometric
Society, pp. 1027–1041, 1973.
Wauthier, Fabian L., Jordan, Michael I., and Jojic, Nebojsa.
Efficient ranking from pairwise comparisons. In Proceedings of the 30th International Conference on Machine Learning, 2013.

