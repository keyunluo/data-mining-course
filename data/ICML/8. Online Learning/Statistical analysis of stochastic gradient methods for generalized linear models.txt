Statistical analysis of stochastic gradient methods for generalized linear models

Panos Toulis∗
PTOULIS @ FAS . HARVARD . EDU
Jason Rennie†
JRENNIE @ GOOGLE . COM
Edoardo M. Airoldi∗
EAIROLDI @ FAS . HARVARD . EDU
∗
Department of Statistics, Harvard University, 1 Oxford Street, Cambridge, MA 02138, USA.
†
Google, Inc., 1600 Amphitheatre Pkwy, Mountain View, CA 94043

Abstract
We study the statistical properties of stochastic
gradient descent (SGD) using explicit and implicit updates for fitting generalized linear models (GLMs). Initially, we develop a computationally efficient algorithm to implement implicit
SGD learning of GLMs. Next, we obtain exact formulas for the bias and variance of both
updates which leads to two important observations on their comparative statistical properties.
First, in small samples, the estimates from the
implicit procedure are more biased than the estimates from the explicit one, but their empirical variance is smaller and they are more robust to learning rate misspecification. Second,
the two procedures are statistically identical in
the limit: they are both unbiased, converge at the
same rate and have the same asymptotic variance.
Our set of experiments confirm our theory and
more broadly suggest that the implicit procedure
can be a competitive choice for fitting large-scale
models, especially when robustness is a concern.

tee consistency of the method even when the analytic form
of M (θ) is not known. Although initially applied in experimental design, the SA procedure was soon adapted for
statistical estimation. Sakrison (Sakrison, 1965) assumed
yn ∼ f (yn ; θ∗ ) i.e., that observations are drawn independently from a statistical model with unknown fixed parameter θ∗ . Sakrison’s recursive estimation procedure was defined using the update θn = θn−1 + an `0 (θn−1 ; yn ), where
`(θ; yn ) = log f (yn ; θ) is the log-likelihood of θ given
observation yn . Under certain regularity and monotonicity conditions, E (`0 (θ∗ ; yn )) = 0 and so according to SA
theory, the estimates θn converge to the real parameter θ∗
with possibly optimal asymptotic efficiency (Anbar, 1973;
Fabian, 1973). In recent years, Sakrison’s recursive estimation method has become known as stochastic gradient
descent (SGD). Further, note that the aforementioned SGD
update is explicit i.e., θn can be calculated immediately
from θn−1 and the data at the n-th iteration. For the rest
of this paper, we will refer to this procedure as “SGD with
explicit updates” or standard SGD for short. Despite the
theoretical guarantees, standard SGD is generally not robust to learning rate misspecification or input noise. Recursive procedures that aim to control the size of updates
have thus been proposed such as,

1. Introduction
Stochastic gradient descent (SGD) is a stochastic approximation (SA) method. Assume a random variable y ∈ R
as the outcome of interest controlled by a parameter θ ∈ R
with regression function M (θ) = E (y| θ), and consider the
problem of finding θ∗ such that M (θ∗ ) = E (y| θ∗ ) = 0.
The classical SA procedure (Robbins & Monro, 1951)
maintains an estimate θn−1 of θ∗ at each iteration n and
then obtains a sample yn (e.g. through an experiment) such
that E (yn ) = M (θn−1 ). The estimate is then updated to
θn = θn−1 − an yn . The scalar an > 0 is the learning rate
and should decay to zero but not too fast in order to guaranProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

θn = arg min {D(θ, θn−1 ) − an `(θ; yn )}
θ

(1)

in which `(·) is the log-likelihood as before and D(·, ·) is
some distance function. Minimizing (1) yields updates of
the form θn = g(θn ; θn−1 , yn , an ), that are called implicit
since the future estimate θn appears in both sides of the
equation. We will refer to procedure (1) as “SGD with implicit updates”1 or implicit SGD for short.
Historically, the duo of explicit-implicit updates originate
from the numerical methods invented by Euler (ca. 1770)
for approximating solutions of ordinary differential equations (Hoffman & Frankel, 2001). However, the normal1

Thus, we still regard procedure (1) to be a SGD procedure
because the gradient is calculated at one data point yn at a time
and, hence, it is stochastic.

Statistical analysis of SGD methods for fitting GLMs

ized least mean squares (NLMS) filter (Nagumo & Noda,
1967) was, arguably, the first statistical model that used an
implicit update as in Equation (1) and was shown to be
consistent and robust to excess input noise (Slock, 1993).2
Since then, several online learning models have been using implicit updates in various forms. For example, mirrordescent and projected subgradient algorithms (Nemirovski,
1983; Beck & Teboulle, 2003) and variants such as FOBOS
(Duchi & Singer, 2009) include updates that can be written
in the form of Equation (1). The regret of such algorithms
using implicit updates and D(·, ·) a Bregman divergence
has been shown to be comparable to standard SGD bounds
(Kivinen & Warmuth, 1995; Kivinen et al., 2006; Kulis &
Bartlett, 2010) and their robustness has been proven useful
in a wide range of modern machine learning problems (Nemirovski et al., 2009; Kulis & Bartlett, 2010; Schuurmans
& Caelli, 2007).
However, the statistical properties of SGD methods, either implicit or standard, remain not well-understood. In
this paper, we perform a statistical analysis of the implicit
method vis-à-vis with a standard SGD counterpart in the
family of generalized linear models (GLMs). Our main
contributions are the following:
(i) We adapt the classical SA procedure (Robbins &
Monro, 1951) and the proof therein to formalize its
implicit counterpart and show that the method is consistent in quadratic mean (Theorem 2.1). Next, we
focus on the problem of online estimation of GLMs
and provide a computationally efficient algorithm for
applying implicit updates (Algorithm 1).
(ii) We derive formulas for the asymptotic bias of the implicit and standard SGD procedures (Theorem 4.1).
We show that the implicit procedure converges slower
(in general) but asymptotically at the same rate as the
standard one. Furthermore, we derive exact formulas
for the asymptotic variance of both procedures (Theorem 4.2) and, thus, show that they have the same
asymptotic efficiency.
(iii) We show that the implicit method is unconditionally
stable under any specification of the learning rate,
whereas standard SGD can deviate arbitrarily when
the learning rate is misspecified (Lemma 4.2).
For clarity of exposition, we omit most proofs from the current document and make them all available online in the
unpublished, full version of this paper. 3
2
In the NLMS algorithm (Nagumo & Noda, 1967), the multivariate update has the form θ n = θ n−1 + (a + b||xn ||2 )−1 (yn −
x|n θn−1 )xn , a, b > 0 which can be written in the form of (1) for
which D(·, ·) is the usual L2 norm and the log-likelihood is that
of a linear normal model.
3
The full version of the paper together with the accom-

2. Implicit stochastic approximation
We first introduce a general definition of the implicit SA
procedure by adapting the original work of (Robbins &
Monro, 1951). Assume a function M : R → R for which
we wish to estimate the zero M (θ∗ ) = 0. Starting from
some θ0 ∈ R, we update our estimates at iteration n according to observed data yn ∈ R, a learning rate an ∈ R+
and the following rule:
θn = θn−1 − an yn

(2)

Equation (2) defines the implicit stochastic approximation
procedure under the following assumptions:
P
P
Assumption 2.1. an > 0, a2n < ∞, an = ∞.
Assumption 2.2. The random variable yn has a distribution that depends on θn such that E (yn | θn ) = M (θn ).
Furthermore, it is bounded such that P (|yn | < C) = 1 for
some constant C > 0.
Assumption 2.3. The function M (x) is non-decreasing
and differentiable. Furthermore, M 0 (θ∗ ) > 0.
Note that only Assumption (2.2) differentiates between
the standard SA procedure and the implicit one. Rather
counter-intuitively, the observation yn is considered a sample from the distribution of the future update θn .4 Clearly,
one may need to know the form of that distribution in order to perform the update. The following theorem establishes that the implicit SA procedure converges in quadratic
mean, just like the standard SA counterpart.5 .
Theorem 2.1. Suppose that assumptions (2.1)-(2.3) hold.
Then, the implicit SA procedure (2) converges in quadratic
mean i.e.,
2

E (θn − θ∗ ) → 0 as n → ∞

(3)

The proof is an adaptation of the original proof from (Robbins & Monro, 1951) and is given in the full version of this
paper.
panying source code and documentation can be found at the
following location: http://www.people.fas.harvard.edu/∼ptoulis/
harvard-homepage/implicit-sgd.html.
4
Originally, the SA procedure was created for experimental
design. Assume that y is a random response to a drug at a dose
level θ. Assume θ∗ to be the dose level in which the response
will have no effects i.e., E (y| θ∗ ) = 0. Then the SA procedure
describes a succession of dose levels θn , and thus a series of experiments with outcomes yn , that converges to θ∗ . This is not
possible with the implicit procedure since yn does not depend on
θn−1 but on θn , i.e. the future dose level!
5
Note that the convergence of implicit update procedures for
linear classifiers has already been shown in the literature (CesaBianchi, 2006). However, Theorem 2.1 formalizes the implicit
procedure under the general stochastic approximation framework
which is broader in scope than linear classifiers.

Statistical analysis of SGD methods for fitting GLMs

3. Preliminaries

(b) Var (y| x) = ψh0 (θ ∗ | x) = b00 (θ ∗ | x)

We now focus on the family of GLMs (Nelder & Wedderburn, 1972). Let y ∈ R denote the outcome of interest,
θ ∗ ∈ Rp be the vector of unknown model parameters and
x ∈ Rp denote a vector of features. In a GLM, we assume that the outcome y follows some distribution in the
exponential family as follows


ηy − b(η)
c(y, ψ) where η = θ ∗ | x (4)
y|x ∼ exp
ψ

(c) ∇`(θ; y, x) =

The quantity η is the linear predictor, the scalar ψ > 0 is
the dispersion parameter as it affects the variance of the
outcome, and b(·), c(·, ·) are appropriate real-valued functions. Equation (4) is known as the canonical form because
the linear predictor appears as a coefficient of y in the density function. Furthermore, the expected value of the outcome is given by the link function g(·) of the model i.e.,
g (E (y| x)) = θ ∗ | x = η

(5)

The inverse of the link function h = g −1 is the transfer
function of the GLM model so that E (y| x) = h(θ ∗ | x).
We will assume GLMs in the canonical form with a monotone link function which we will refer to as canonical
GLMs. This family is very broad and widely-applicable as
it contains models such as the linear normal model, logistic
regression, Poisson regression and so on. To illustrate our
notation, in logistic regression we assume P (y = 1|x) =
py (1 − p)1−y where p is a function of x. This can be written in the form of Equation (4) with η = log(p/(1 − p)),
b(η) = log(1 + eη ), ψ = 1 and c(y, ψ) = 1. We
know that E (y| x) = p = exp(θ ∗ | x)(1 + exp(θ ∗ | x))−1
and so the link function g(·) is the logit function g(u) =
log(u/(1 − u)) and the transfer function is the logistic i.e.,
h(u) = eu (1+eu )−1 . Table 1 summarizes the link/transfer
functions for the three aforementioned models. The results
model
Normal
Logistic
Poisson

g(u)
u
u
log( 1−u
)
log(u)

h(u)
u
eu (1 + eu )−1
eu

Table 1. Three well-known canonical GLMs.

in the proposition that follows will be useful for the rest
of our analysis. As these are standard results in the theory
of GLMs (Nelder & Wedderburn, 1972), we just give short
proofs in the full version of this paper.
Proposition 3.1. Let θ ∗ ∈ Rp be the true parameter vector
of a canonical GLM model with outcome y ∈ R, and x ∈
Rp be some feature vector. Then,
(a) E (y| x) = h(θ ∗ | x) = b0 (θ ∗ | x)

1
ψ

(y − h(θ | x)) x

(d) I(θ) = −E (∇∇`(θ; y, x)) =

1
ψE

(h0 (θ | x)xx| )

where θ ∈ Rp is an arbitrary vector.
3.1. Online learning of GLMs using SGD
In this paper, we assume the task of learning online the unknown parameter vector θ ∗ of a GLM model (4). At every
iteration indexed by n = 1, 2, · · · a new feature vector, denoted by xn , is sampled independently from a fixed and
known distribution. Given xn , the outcome yn is sampled
according to a canonical GLM as in (4). Upon observing
(yn , xn ), we update our estimate of θ ∗ from θ n−1 to θ n .
The initial estimate θ 0 has been set a priori to a reasonable value. We now proceed to define the implicit and the
standard SGD procedures for learning GLMs. The exact
requirements for the learning rate an will be set in Assumptions (4.1).
Definition 3.1. The standard SGD learning procedure for
a canonical GLM is defined as:

θ n = θ n−1 + an yn − h(θ |n−1 xn ) xn
(6)
After n steps of procedure (6), the vector θ sgd
n is the standard SGD estimator of θ ∗ .
Definition 3.2. The implicit SGD learning procedure for a
canonical GLM is defined as:
θ n = θ n−1 + an (yn − h(θ |n xn )) xn

(7)

After n steps of procedure (7), the vector θ im
n is the implicit
SGD estimator of θ ∗ .
Discussion. First, note that we omit the term of (1/ψ) of
the log-likelihood gradient (see Proposition (3.1)-(c)) since
it can be factored into an . Second, we clarify that Definitions 6 and 7 correspond to “baseline” definitions of the
two learning procedures. Especially for standard SGD,
there has been significant work in improving the performance of the procedure. These methods include averaging of the updates to speed up convergence (Polyak & Juditsky, 1992), approximating second-order information as
in SGD-QN (Bordes et al., 2009), using adaptive learning
rates as in AdaGrad (Duchi et al., 2011; Schaul et al., 2012)
or variance reduction methods (Johnson & Zhang, 2013;
Roux et al., 2012). So far, the implicit procedure has received disproportionately less attention, however it is reasonable to expect that similar methods could be employed
there as well. In fact, our subsequent analysis suggests that
the implicit procedure, being less sensitive to learning rate
specification, is likely to be more amenable to performance
improvements.

Statistical analysis of SGD methods for fitting GLMs

Algorithm 1 Implicit learning of canonical GLMs
1: for all n ∈ {1, 2, · · · } do

2:
rn ← an yn − h(θ |n−1 xn )
3:
Bn ← [0, rn ]
4:
if rn ≤ 0 then
5:
Bn ← [rn , 0]
6:
end if 

7:
ξn = an yn − h θ |n−1 xn + ||xn ||2 ξn , ξn ∈ Bn
8:
θ n ← θ n−1 + ξn xn
9: end for

3.2. Efficient implicit updates for canonical GLMs
The implicit equation (7) cannot be solved in general because the form of h(·) can be arbitrary. Furthermore, in a
multi-dimensional setting, this would require the solution
of multiple simultaneous equations. However, it has already been noted that line search methods can be employed
to implement general implicit updates (Kivinen et al., 2006;
Kulis & Bartlett, 2010). Here, we show that the special
structure of the log-likelihood gradient of the GLMs (see
Proposition 3.1-(c)) can be exploited in order to efficiently
compute the implicit updates. Algorithm 1 reduces equation (7) to a one-dimensional implicit equation which can
be solved efficiently, since narrow search bounds can be derived by using the monotone property of the transfer function in canonical GLMs.
Lemma 3.1. Algorithm 1 computes estimates θ n that are
identical to the estimates of the implicit procedure (7).
Proof. We first show that θ n = θ n−1 + ξn xn is the correct
update for the implicit procedure, where ξn is computed in
Step 7 of Algorithm 1. We multiply with xn on both sides
of (7) to get,

θ |n xn = θ |n−1 xn + an yn − h(θ |n−1 xn ) x|n xn
and we apply h(·) on both sides to further obtain,
h(θ |n xn ) = h θ |n−1 xn + an (yn − h(θ |n−1 xn ))||xn ||2



Setting ξn = an (yn − h(θ |n xn )), we can rewrite the above
equation as

h(θ |n xn ) = h θ |n−1 xn + ξn ||xn ||2
(8)
It also holds that h(θ |n xn ) = yn − ξn /an and so Equation
(8) now becomes,

Next we show why the bounds Bn in Algorithm 1 are correct. Let m(u) = an yn − h(θ |n−1 xn + u||xn ||2 ) and
let l(u) = u be the straight line. We wish to find the
fixed point ξn such that m(ξn ) = l(ξn ). Since m(u) is
monotone decreasing and l(u) is monotone increasing and
both functions are continuous in R, the intersection point
is unique. The sign of ξn depends on where m(ξ
 n ) crosses
the y-axis i.e., m(0) = an yn − h(θ |n−1 xn ) ≡ rn . If
rn > 0 then ξn > 0. Furthermore, since l(u) is increasing,
l(rn ) > l(ξn ) ⇒ ξn < rn , and thus [0, rn ] is a search interval for ξn . Similarly, if rn < 0 then ξn < 0 and ξn > rn .
Note that more restrictive bounds might be available. For
example, if rn > 0 we know that ξn has to be smaller
than the point u0 where m(u) crosses the x-axis, i.e.
m(u0 ) = 0. Through standard algebra we can obtain that
u0 = (g(yn ) − θ |n−1 xn )/||xn ||2 . In this case, a better
bound for ξn is [0, min(u0 , rn )], while a similar argument
works also if rn < 0. Significant improvements are expected in models where g(u) = o(u) such as the Poisson
regression model. In this case, instead of searching in an interval [0, rn ], the algorithm could search in [0, log rn ].

4. Statistical analysis
For an estimate θ n of θ ∗ , let µn = E (θ n ) and Vn =
Var (θ n ). Denote the bias of θ n with bn = µn − θ ∗ .
We will use a superscript to denote the specific procedure
im
under study, thus µsgd
n and µn denote the means of the estimates from the standard SGD and the implicit procedure
respectively, and so on. Also, let z n (θ; x) = h(θ | xn )xn ,
r(θ) = E (z n (θ; x)| θ) and let Dr (·) be the
of
PJacobian
∞
r(·). If a nonnegative series {γn } satisfies i=1 ai γi <
∞, we will call it an -convergent. We also write C(an ) for
an arbitrary an -convergent series. Last, throughout this paper the notation || · || for a vector or a matrix argument
denotes the L2 norm.
Assumption 4.1. (a) Let an >P0 be a decreasing
seP
quence of numbers such that an = ∞, a2n < ∞.
Furthermore, an−1 /an = 1 + (1/α)an + O(a2n ), for
some α > 0.
(b) For sufficiently large n, in the neighborhood of θ ∗ ,
make the approximation
r(θ n ) = r(θ ∗ ) + Dr (θ ∗ )(θ n − θ ∗ ) + n
and assume convergence θ n → θ ∗ such that the series

yn − ξn /an = h(θ |n−1 xn + ξn ||xn ||2 )
Solving for ξn we finally get the one-dimensional implicit
equation,
ξn = an (yn − h(θ |n−1 xn + ξn ||xn ||2 ))

By the definition of ξn and the implicit procedure (7), we
have that θ n = θ n−1 + ξn xn .

(9)

(i) ||n || and (1/an )||Cov (θ n , n ) ||
(ii) ||Var (z n (θ n ; x) − z n (θ ∗ ; x)) ||
(iii) ||Var (z n (θ n ; x)) − Var (z n (θ ∗ ; x)) ||
are all an -convergent.

Statistical analysis of SGD methods for fitting GLMs

The first part of the assumption is essential for convergence
of the stochastic approximation procedure. One schedule that satisfies such assumptions is an = α(β + n)−1 ,
α, β > 0. Note also that Assumption 4.1 does not cover
rates of the form (β + n)−c for c < 1; this will be part of
future work. Part (b) of our Assumption 4.1 puts weak constraints on convergence to θ ∗ . For example, if an ∝ n−1 ,
then the series of Assumption 4.1 (i)-(iii) are allowed to be
of the form n− for any  > 0. Approximation assumptions like Assumption 4.1 are generally necessary in order
to derive exact asymptotic variance formulas. For example,
the classical derivation of the variance of the MLE relies
on a second-order approximation (Fisher, 1922). Classical
results on the variance and/or normality of the SA estimators also rely on linearly-bounded derivatives (Sacks, 1958;
Fabian, 1968). In a similar statistical analysis of the standard SGD iterates, Murata (1998) relies on a complete second order approximation of the loss function (see (Murata,
1998), Equation 2.4). In machine learning, the assumptions
are generally weaker since the goal is only to bound the
regret of the online procedure. However, assumptions on
global bounds on the loss-gradients (Zinkevich, 2003) or
the distance ||θ n − θ ∗ || (Kulis & Bartlett, 2010) are common.
Lemma 4.1. Let the sequence an satisfy part (a) of Assumption 4.1 and consider the following matrix recursions,
X n = (I − an B n )X n−1 + an (C + D n )

(10)

−1

(11)

Y n = (I + an B n )

[Y n−1 + an (C + D n )]

such that,

asymptotic unbiasedness of the SGD estimators through
the following theorem. The complete proof is given in the
full version of the paper.
Theorem 4.1. Under Assumption 4.1, the asymptotic bias
of the standard SGD estimator satisfies,
sgd
∗
bsgd
n = (I − an ψI(θ )) bn−1 + C(an )

The asymptotic bias of the implicit SGD estimator satisfies,
h
i
im
∗ −1
bim
=
(I
+
a
ψI(θ
))
b
+
C(a
)
(18)
n
n
n
n−1
∗
Both methods are asymptotically unbiased i.e., µsgd
n →θ
im
∗
and µn → θ .

Note that Theorem 4.1 implies that the standard SGD
procedure converges faster than the implicit one, since
||(I − an ψI(θ ∗ ))|| < ||(I + an ψI(θ ∗ ))−1 || for sufficiently large n. However, the rates become equal in the
limit.
4.2. Asymptotic variance
Taking variances on both sides of updates (6) and (7), and
using Assumption 4.1 and Corollary 4.1, we obtain the exact formula for the asymptotic variances of the SGD estimators through the following theorem. The complete proof
is also given in the full version of the paper.
Theorem 4.2. Under Assumption (4.1) and if the matrix
(2ψI(θ ∗ ) − I/α) is positive-definite, the asymptotic variance of the standard SGD estimator satisfies,
−1

(1/an )Vnsgd → αψ 2 (2αψI(θ ∗ ) − I)
O(a2n ),

and
(a) B n → B > 0, ||B n − B n−1 || =
P∞
(b)
i=1 ai ||D i || < ∞ i.e., ||Dn || is an -convergent.

(12)

Corollary 4.1. Consider the following matrix recursions,
X n = (I − an B n )X n−1 + a2n (C + D n )


Y n = (I + an B n )−1 Y n−1 + a2n (C + D n )

(13)
(14)

where an , B n , B, C, D n satisfy the assumptions of
Lemma 4.1. Then, X n → 0 and Y n → 0. Furthermore, if
the matrix (B − I/α) is positive-definite,
(1/an )X n → (B − I/α)−1 C
(1/an )Y n → (B − I/α)

−1

C

I(θ ∗ )

(19)

The asymptotic variance of the implicit SGD estimator satisfies,

Then, both recursions approximate the matrix B −1 C i.e.,
||X n − B −1 C|| → 0 and ||Y n − B −1 C|| → 0

(17)

(15)
(16)

4.1. Asymptotic bias
Taking expectations on both sides of updates (6) and (7),
and using Assumption 4.1 and Lemma 4.1, we obtain the

−1

(1/an )Vnim → αψ 2 (2αψI(θ ∗ ) − I)

I(θ ∗ )

(20)

Therefore, both estimators have the same asymptotic efficiency.
Similar forms to the asymptotic variance of Theorem 4.2
have been discovered before. For example, assume a onedimensional normal model (p = 1) where xn = 1, an =
α/n and yn |xn ∼ N (θ∗ , σ 2 ). In our notation, ψ = σ 2 ,
h(u) = u and I(θ∗ ) = 1/σ 2 . Thus, the asymptotic variance in (19) becomes nVnsgd → α2 σ 2 /(2α − 1). This form
of asymptotic variance, which by Theorem 4.2 holds for
the implicit procedure as well, was first proved by Sacks
(1958) and has since been rediscovered multiple times. 6
6
Sacks (1958) proved normality of θnsgd with variance
(1/n)α2 σ 2 (2aM 0 (θ∗ ) − 1)−1 , under certain conditions on the
regression function M (θ) = E (yn | θ), but without requiring a
normal distributional assumption of yn . See also (Nemirovski
et al., 2009), page 1578, for similar variance results but for general strongly-convex objective functions.

Statistical analysis of SGD methods for fitting GLMs

We can also confirm that the asymptotic variance in
Theorem 4.2 is larger than the variance of the maximum likelihood
estimator (MLE) defined as θ mle
=
n
Pn
arg maxθ i=1 `(θ; yi , xi ). Standard theory suggests that
the MLE
is asymptotically optimal as an estimator and
√
has variance I(θ ∗√
)−1 for large n.√Let c =
that nθ mle
n
αψ > 0, then the variance of nθ sgd
and of nθ im
n
n is
2
∗
−1
∗
given by c (2cI(θ ) − I) I(θ ) ≥ I(θ ∗ )−1 for any
c > 0,7 thus showing that both estimators (standard SGD
and implicit) are not optimal. However, we can still utilize the variance formula to derive optimal learning rates.
Note that the eigenvalues of the variance of both estimators are given by c2 λi /(2cλi − 1) where λi are the eigenvalues of I(θ ∗ ). This formula could then be used to
pick optimal learning rates according to an appropriate criterion. For example, if we would like to minimize the
trace of the SGD
P asymptotic variance then we should pick
ĉ = arg minc i c2 λi /(2cλi −1) and α̂ = ĉ/ψ. Of course,
the λi ’s are unknown in general and thus we would need
to estimate them from data. Using our theory in order to
develop optimal learning rates, especially for the implicit
procedure, will be the focus of future work.
4.3. Stability
We simplify the bias recursions (17) and (18) by ignoring
the remainder terms and by considering a simpler form as
follows:
sgd
n
∗
bsgd
n = (I − an ψI(θ ))bn−1 = P 1 b0
∗ −1 im
bn−1 = Qn1 b0
bim
n = (I + an ψI(θ ))

(21)

Qn
Qn
where P n1 = i=1 (I − ai ψI(θ ∗ )), Qn1 = i=1 (I +
ai ψI(θ ∗ ))−1 , and b0 denotes the initial bias from a common starting point θ 0 . Thus, the simplified form actually
describes the effect of the starting point θ 0 on the estimates
θ n after n iterations. Also, let eig(A) be the set of eigenvalues of a matrix A, and let λ(p) = max{eig(I(θ ∗ ))},
λ(1) = min{eig(I(θ ∗ ))} be the maximum and minimum
eigenvalues of the Fisher information matrix respectively.
Note that, P n1 → 0 and Qn1 → 0 (based on the proof of
Lemma 4.1) and thus both methods are asymptotically stable i.e., both will converge, in theory, to the true parameter
vector regardless of the starting point. However, we are
interested in deviations of the standard and implicit SGD
methods as captured here by the empirical bias. Based on
the simplified bias equations (21), this information can be
summarized by the eigenvalues of the matrices P n1 and Qn1
through the following lemma.
Lemma 4.2. If an = α/n and αψλ(p) > 1, then the max7
To clarify, by A ≥ B we mean that A−B is a nonnegativedefinite matrix.

imum possible eigenvalue of a matrix P n1 is given by
q
max max{eig(P n1 )} = Θ(2αψλ(p) / αψλ(p) )
(22)
n>0

For the implicit method,
max max{eig(Qn1 )} = O(1)
n>0

(23)

Lemma 4.2 shows that in the standard SGD procedure, the
effect from the initial conditions can be amplified in an exponentially large way before fading out, if the learning rate
is misspecified (i.e., if α > 1/ψλ(p) ). This sensitivity of
the standard SGD procedure is well-known and requires
problem-specific considerations to be avoided in practice.
However, it is less well-known that the effects of the initial
conditions monotonically decrease in the implicit method
as shown in Equation (23). Rather remarkably, this robustness property of the implicit method holds under arbitrary
misspecifications of the learning rate.

5. Experiments
We illustrate the different aspects of our theory on three
separate sets of experiments. In Section 5.1, we work on
a simple bivariate Poisson regression model and verify the
variance asymptotics in Theorem 4.2, both analytically and
in simulation. In Section 5.2, we compare convergence and
stability of the standard and implicit SGD procedures on a
larger Normal model. Last, in Section 5.3 we implement
an implicit learning algorithm for SVM and compare with
a standard SGD algorithm on a typical classification task.
5.1. Bivariate Poisson model
We first illustrate on a bivariate Poisson model which is
simple enough to derive the relevant formulas analytically.
We assume binary features such that, for any iteration n,
xn is either (0, 0)| , (1, 0)| or (0, 1)| with probabilities 0.6,
0.2 and 0.2 respectively. We set θ ∗ = (θ1 , θ2 )| for some
∗|
θ1 , θ2 , and assume yn ∼ Poisson(eθ xn ). In our GLM
notation, p = 2, ψ = 1 and h(u) = eu . By Proposition 3.1
it easily follows that,
 θ

e1 0
∗
0 ∗|
|
I(θ ) = E (h (θ xn )xn xn ) = 0.2
0 eθ 2
We set an = 10/3n which implies α = 10/3 using
Assumption 4.1. For notational convenience, let c =
(2α)0.2 = 4/3. Setting θ1 = log 2 and θ2 = log 4, the
asymptotic variance Σ = α(2αI(θ ∗ ) − I)−1 I(θ ∗ ) in
Theorem 4.2 is equal to,
! 

eθ1
0
c
0.8
0
θ1 −1
ce
=
(24)
Σ=
θ2
e
0 0.62
2
0
θ
ce

2 −1

Statistical analysis of SGD methods for fitting GLMs
∗
im
∗
Table 2. Sample quantiles of ||θ sgd
20000 − θ || and ||θ 20000 − θ ||.
Values that are larger than 1e3 are marked with “*”.

QUANTILES
METHOD

SGD
I MPLICIT

25%
0.01
0.00

50%
1.3
0.01

75%
435.8
0.02

85%
*
0.02

95%
*
0.03

100%
*
0.04

im
Next, we obtain 100 independent samples of θ sgd
N and θ N
for N = 20000 iterations through the procedures defined
in (6) and (7), and compute their empirical variances. We
observe that the implicit estimates are particularly stable
and have an empirical variance that satisfies,


0.86 −0.06
im
d
(1/aN )Var(θ N ) =
−0.06 0.64

and is close to the theoretical value calculated in (24). In
contrast, the standard SGD estimates are quite unstable and
their L2 distance to the true values θ ∗ are orders of magnitude larger than the implicit ones (see Table 2 for sample
quantiles). By Lemma 4.2, such deviations are expected
for standard SGD because the largest eigenvalue of I(θ ∗ )
is λ(2) = 0.8 satisfying αψλ(2) = 8/3 > 1. Note however, that it is fairly straightforward to stabilize the standard
SGD procedure in this problem, for example by modifying
the learning rate to an = min{0.15, 10/3n}. In general,
when the optimization problem is well-understood, it is
easy to determine the learning rate schedule that avoids outof-band explicit updates; in practice, we are working with
problems that are not so well-understood and determining
the correct learning rate schedule may take substantial effort, especially in multi-dimensional settings. The implicit
method eliminates this overhead: a wide range of learning
rate schedules leads to convergence on all problems.
5.2. Multivariate Normal model
In our next experiment, we wish to validate our theory
through a toy problem of normal linear regression following (Xu, 2011). We assume θ ∗ = (1, 1, · · · , 1)| ∈ R20 to
be the ground-truth (i.e., p = 20 parameters). At each iteration n, the feature vector xn is sampled i.i.d. from a multivariate normal xn ∼ Np (0, V x ) for a fixed matrix V x .8
The outcome yn is then sampled from a normal as yn |xn ∼
N (θ ∗ | xn , 1). For each procedure, i.e., standard and implicit SGD, we collect iterates θ n for n = 1, 2, · · · N . We
also repeat the procedure M times so that we finally have
im
M samples of θ sgd
n and θ n , similar to Section 5.1.
Figure 1 shows results for a maximum N = 1000 iterations and M = 2000 samples. In the top-left subfigure, we
8

The covariance matrix is designed to have eigenvalues
almost-uniformly in the interval [0.2, 1.0] and one larger at the
order of 0.1p.

Figure 1. Standard SGD (dark shade, “x”) and implicit SGD (light
shade, “*”) procedures on normal linear regression. The figure
shows for each procedure, the (i) 2.5%/97.5% quantiles of logbias over iterations (top-left) (ii) 2.5%/97.5% quantiles of log-bias
over learning-rate scaling (bottom-left), (iii) L2 norm of empirical minus theoretical variance over iterations (top-right), and (iv)
trace of empirical variance over iterations (bottom-right).

plot the log-norm of the bias over N iterations, where for
each method we plot two lines corresponding to the 2.5%
(lower line) and 97.5% (upper line) over all M samples.
We observe that the implicit method is slightly slower to
converge but eventually obtains a similar rate of convergence to standard SGD, as predicted by Theorem 4.1. In
the bottom-left figure, we plot the log-norm of the bias
achieved by θ N , over M samples for each method and for
different learning rates (x-axis). In particular, we scale the
baseline learning-rate up to being 3x the optimal value as
calculated for the standard SGD. We observe that the bias
of the standard SGD method is significantly affected by this
scaling whereas the implicit method remains robust. In particular, the maximum observed bias of the implicit method
remains constant whereas the minimum bias is actually improving when scaling the learning rate.
In the top-right figure, we plot for each method the L2 norm
of the empirical variance (computed over M = 2000 samples) subtracted from the one predicted by Theorem 4.2,
and thus observe that both variances are converging to the
theoretical one. Finally, the bottom-right figure shows the
trace of the variances of the iterates θ n for every method.
This plot shows that the implicit method exhibits smaller
empirical variance of the iterates, thus achieving an interesting trade-off: it gives up bias at the early stages
of the iteration (see top-left) in order to compensate for
more robustness (bottom-left) and smaller empirical vari-

Statistical analysis of SGD methods for fitting GLMs
Table 3. Test errors of standard and implicit SGD methods on the
RCV1 dataset benchmark. Training times are roughly comparable. Best scores, for a particular loss and regularization, are
bolded.
REGULARIZATION

1 E -5

1 E -7

SGD

4.65%

3.57%

4.85%

I MPLICIT

4.68%

3.6%

3.46%

LOSS

H INGE
L OG

(λ)

1 E -12

SGD

5.23%

3.87%

5.42%

I MPLICIT

4.28%

3.69%

4.01%

ance (bottom-right). Asymptotically and assuming convergence, both methods provide identical estimators in terms
of bias (top-left) and variance (top-right) as predicted by
Theorems 4.1 and 4.2.
5.3. Additional experiments on SVM model
We are also interested to test the performance of the implicit procedure outside the family of GLMs. For that purpose, we implement an implicit online learning procedure
for a SVM model and compare it to a standard SGD method
on the RCV1 benchmark.9 Some results using variations
on the loss functions and the regularization parameter are
shown in Table 3. A complete understanding of these results is still missing, however we do observe that the implicit method fares well compared to standard SGD and, at
the same time, remains remarkably robust to misspecification. For example, note that in all experiments the standard
SGD method degrades in performance for small or large
regularization (in these experiments, the regularization parameter λ also affects the learning rate such that, larger λ
means larger learning rates). However, the implicit method
maintains a more stable performance accross experiments
and, interestingly, it achieves best performance under minimal regularization using the hinge loss.

6. Conclusion
We study the statistical properties of explicit and implicit
updates for fitting GLM models using SGD. In this model
family, we derive a computationally efficient algorithm to
perform the implicit updates. Furthermore, we derive formulas for the asymptotic bias and variance of both updates
and show the fundamental bias/variance trade-off achieved
by the implicit method. In small samples, the implicit estimates are more biased than the explicit ones but exhibit
smaller empirical variance and are substantially more robust to misspecification. In the limit, both methods are statistically equivalent: they are both unbiased at the same
9

We used Bottou’s SVM SGD implementation available at
http://leon.bottou.org/projects/sgd. Our implicit SVM is available
at the first author’s website.

convergence rate and enjoy the same statistical efficiency.
Our theoretical results thus suggest that the implicit method
could safely be the method of choice in estimating largescale GLMs, especially when robustness is a concern. Our
experiments confirm our theory and, more broadly, suggest that the implicit method can be a competitive method
in large-scale machine learning tasks, requiring less tuning
of learning-rate or regularization parameters. Future work
will focus on the implicit method towards optimal learning
rate schedules and a more detailed characterization of its
robustness properties.

Acknowledgments
This work was sponsored, in part, by the 2012 Google PhD
Fellowship in Statistics awarded to Panos Toulis, and grants
from the NSF, NIH, ARO, ONR and the Sloan Foundation
to Edoardo M. Airoldi.

References
Anbar, Dan. On optimal estimation methods using stochastic approximation procedures. The Annals of Statistics, pp. 1175–
1184, 1973.
Beck, Amir and Teboulle, Marc. Mirror descent and nonlinear
projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167–175, 2003.
Bordes, Antoine, Bottou, Léon, and Gallinari, Patrick. Sgd-qn:
Careful quasi-newton stochastic gradient descent. The Journal
of Machine Learning Research, 10:1737–1754, 2009.
Cesa-Bianchi, Nicolo. Prediction, learning, and games. Cambridge University Press, 2006.
Duchi, John and Singer, Yoram. Efficient online and batch learning using forward backward splitting. The Journal of Machine
Learning Research, 10:2899–2934, 2009.
Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 999999:
2121–2159, 2011.
Fabian, Vaclav. On asymptotic normality in stochastic approximation. The Annals of Mathematical Statistics, pp. 1327–1332,
1968.
Fabian, Vaclav. Asymptotically efficient stochastic approximation; the rm case. The Annals of Statistics, pp. 486–495, 1973.
Fisher, Ronald A. On the mathematical foundations of theoretical statistics. Philosophical Transactions of the Royal Society
of London. Series A, Containing Papers of a Mathematical or
Physical Character, 222:309–368, 1922.
Hoffman, Joe D and Frankel, Steven. Numerical methods for engineers and scientists. CRC press, 2001.
Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient
descent using predictive variance reduction. In Advances in
Neural Information Processing Systems, pp. 315–323, 2013.

Statistical analysis of SGD methods for fitting GLMs
Kivinen, Jyrki and Warmuth, Manfred K. Additive versus exponentiated gradient updates for linear prediction. In Proceedings of the twenty-seventh annual ACM symposium on Theory
of computing, pp. 209–218. ACM, 1995.
Kivinen, Jyrki, Warmuth, Manfred K, and Hassibi, Babak. The pnorm generalization of the lms algorithm for adaptive filtering.
Signal Processing, IEEE Transactions on, 54(5):1782–1793,
2006.
Kulis, Brian and Bartlett, Peter L. Implicit online learning. In
Proceedings of the 27th International Conference on Machine
Learning (ICML-10), pp. 575–582, 2010.
Murata, Noboru. A statistical study of on-line learning. Online
Learning and Neural Networks. Cambridge University Press,
Cambridge, UK, 1998.
Nagumo, Jin-Ichi and Noda, Atsuhiko. A learning method for
system identification. Automatic Control, IEEE Transactions
on, 12(3):282–287, 1967.
Nelder, J.A. and Wedderburn, R.W.M. Generalized linear models.
Journal of the Royal Statistical Society. Series A (General), pp.
370–384, 1972.
Nemirovski, Arkadi, Juditsky, Anatoli, Lan, Guanghui, and
Shapiro, Alexander. Robust stochastic approximation approach
to stochastic programming. SIAM Journal on Optimization, 19
(4):1574–1609, 2009.
Nemirovski, Yudin, DB. Problem complexity and method efficiency in optimization. Wiley (Chichester and New York),
1983.
Polyak, Boris T and Juditsky, Anatoli B. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and
Optimization, 30(4):838–855, 1992.
Robbins, Herbert and Monro, Sutton. A stochastic approximation
method. The Annals of Mathematical Statistics, pp. 400–407,
1951.
Roux, Nicolas Le, Schmidt, Mark, and Bach, Francis. A stochastic gradient method with an exponential convergence rate for
finite training sets. arXiv preprint arXiv:1202.6258, 2012.
Sacks, Jerome. Asymptotic distribution of stochastic approximation procedures. The Annals of Mathematical Statistics, 29(2):
373–405, 1958.
Sakrison, David J. Efficient recursive estimation; application to
estimating the parameters of a covariance function. International Journal of Engineering Science, 3(4):461–483, 1965.
Schaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky
learning rates. arXiv preprint arXiv:1206.1106, 2012.
Schuurmans, Li Cheng SVN Vishwanathan Dale and Caelli,
Shaojun Wang Terry. Implicit online learning with kernels. In
Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference, volume 19, pp. 249. MIT
Press, 2007.
Slock, Dirk TM. On the convergence behavior of the lms and the
normalized lms algorithms. Signal Processing, IEEE Transactions on, 41(9):2811–2825, 1993.

Xu, Wei.
Towards optimal one pass large scale learning
with averaged stochastic gradient descent. arXiv preprint
arXiv:1107.2490, 2011.
Zinkevich, Martin. Online convex programming and generalized
infinitesimal gradient ascent. 2003.

