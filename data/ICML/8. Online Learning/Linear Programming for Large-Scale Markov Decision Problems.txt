Linear Programming for Large-Scale Markov Decision Problems

Yasin Abbasi-Yadkori
Queensland University of Technology, Brisbane, QLD, Australia 4000
Peter L. Bartlett
University of California, Berkeley, CA 94720
and Queensland University of Technology, Brisbane, QLD, Australia 4000
Alan Malek
University of California, Berkeley, CA 94720

Abstract
We consider the problem of controlling a Markov
decision process (MDP) with a large state space,
so as to minimize average cost. Since it is intractable to compete with the optimal policy for
large scale problems, we pursue the more modest
goal of competing with a low-dimensional family of policies. We use the dual linear programming formulation of the MDP average cost problem, in which the variable is a stationary distribution over state-action pairs, and we consider a
neighborhood of a low-dimensional subset of the
set of stationary distributions (defined in terms
of state-action features) as the comparison class.
We propose a technique based on stochastic convex optimization and give bounds that show that
the performance of our algorithm approaches the
best achievable by any policy in the comparison
class. Most importantly, this result depends on
the size of the comparison class, but not on the
size of the state space. Preliminary experiments
show the effectiveness of the proposed algorithm
in a queuing application.

1. Introduction
We study the average loss Markov decision process problem. The problem is well-understood when the state and
action spaces are small (Bertsekas, 2007). Dynamic programming (DP) algorithms, such as value iteration (Bellman, 1957) and policy iteration (Howard, 1960), are stanProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

YASIN . ABBASIYADKORI @ QUT. EDU . AU

BARTLETT @ EECS . BERKELEY. EDU

MALEK @ EECS . BERKELEY. EDU

dard techniques to compute the optimal policy. In large
state space problems, exact DP is not feasible as the computational complexity scales quadratically with the number
of states.
A popular approach to large-scale problems is to restrict
the search to the linear span of a small number of features.
The objective is to compete with the best solution within
this comparison class. Two popular methods are Approximate Dynamic Programming (ADP) and Approximate Linear Programming (ALP). This paper focuses on ALP. For
a survey on theoretical results for ADP see (Bertsekas and
Tsitsiklis, 1996; Sutton and Barto, 1998), (Bertsekas, 2007,
Vol. 2, Chapter 6), and more recent papers (Sutton et al.,
2009b;a; Maei et al., 2009; 2010).
Our aim is to develop methods that find policies with performance guaranteed to be close to the best in the comparison class but with computational complexity that does not
grow with the size of the state space. All prior work on
ALP either scales badly or requires access to samples from
a distribution that depends on the optimal policy.
This paper proposes a new algorithm to solve the Approximate Linear Programming problem that is computationally
efficient and does not require knowledge of the optimal policy. In particular, we introduce new proof techniques and
tools for average cost MDP problems and use these techniques to derive a reduction to stochastic convex optimization with accompanying error bounds.
1.1. Notation
Let X and A be positive integers. Let X = {1, 2, . . . , X}
and A = {1, 2, . . . , A} be state and action spaces, respectively. Let ∆S denote probability distributions over set S.
A policy π is a map from the state space to ∆A : π : X →
∆A . We use π(a|x) to denote the probability of choosing
action a in state x under policy π. A transition probability

Linear Programming for Large-Scale Markov Decision Problems

kernel (or transition kernel) P : X × A → ∆X maps from
the direct product of the state and action spaces to ∆X . Let
P π denote the probability transition kernel under policy π.
A loss function is a bounded real-valued function over state
and action spaces, ` : X × A → [0, 1]. Let Mi,: and M:,j
denote ith rowP
and jth column of matrix M respectively.
Let kvk1,c =
i ci |vi | and kvk∞,c = maxi ci |vi | for a
positive vector c. We use 1 and 0 to denote vectors with all
elements equal to one and zero, respectively. We use ∧ and
∨ to denote the minimum and the maximum, respectively.
For vectors v and w, v ≤ w means element-wise inequality,
i.e. vi ≤ wi for all i.
1.2. Linear Programming Approach to Markov
Decision Problems
Under certain assumptions, there exist a scalar λ∗ and a
vector h∗ ∈ RX that satisfy the Bellman optimality equations for average loss problems,
"
#
X
0
λ∗ + h∗ (x) = min `(x, a) +
P(x,a),x0 h∗ (x ) .
a∈A

x0 ∈X

The scalar λ∗ is called the optimal average loss, while the
vector h∗ is called a differential value function. The action
that minimizes the right-hand side of the above equation is
the optimal action in state x and is denoted by a∗ (x). The
optimal policy is defined by π∗ (a∗ (x)|x) = 1. Given `
and P , the objective of the planner is to compute the optimal action in all states, or equivalently, to find the optimal
policy.
The MDP problem can also be stated in the LP formulation (Manne, 1960),
max λ ,

The objective function, µ> `, is the average loss under stationary distribution µ. The first two constraints ensure
that µ is a probability distribution over state-action space,
while the last constraint ensures that µ is a stationary distribution. Given a P
solution µ, we can obtain a policy via
π(a|x) = µ(x, a)/ a0 ∈A µ(x, a0 ).
1.3. Approximations for Large State Spaces
The LP formulations (1) and (2) are not practical for large
scale problems as the number of variables and constraints
grows linearly with the number of states. Schweitzer
and Seidmann (1985) propose approximate linear programming (ALP) formulations. These methods were later improved by de Farias and Van Roy (2003a;b); Hauskrecht
and Kveton (2003); Guestrin et al. (2004); Petrik and Zilberstein (2009); Desai et al. (2012). As noted by Desai
et al. (2012), the prior work on ALP either requires access
to samples from a distribution that depends on the optimal
policy or assumes the ability to solve an LP with as many
constraints as states. (See Appendix A for a more detailed
discussion.) Our objective is to design algorithms for very
large MDPs that do not require knowledge of the optimal
policy.
In contrast to the aforementioned methods, which solve the
primal ALPs (with value functions as variables), we work
with the dual form (2) (with stationary distributions as variables). Analogous to ALPs, we control the complexity by
limiting our search to a linear subspace defined by a small
number of features. Let d be the number of features and Φ
be a (XA) × d matrix with features as column vectors. By
adding the constraint µ = Φθ, we get
min θ> Φ> ` ,
θ

(1)

λ,h

s.t.

θ> Φ> 1 = 1, Φθ ≥ 0, θ> Φ> (P − B) = 0 .

s.t. B(λ1 + h) ≤ ` + P h ,
where B ∈ {0, 1}XA×X is a binary matrix such that the ith
column has A ones in rows 1+(i−1)A to iA. Let vπ be the
stationary distribution under policy π and let µπ (x, a) =
vπ (x)π(a|x). We can write
X
X
π∗ = argmin
vπ (x)
π(a|x)`(x, a)
π

= argmin
π

x∈X

X

If a stationary distribution µ0 is known, it can be added to
the linear span to get the ALP
min(µ0 + Φθ)> ` ,
θ

s.t.

(µ0 + Φθ)> 1 = 1, µ0 + Φθ ≥ 0,
(µ0 + Φθ)> (P − B) = 0 .

a∈A

Although µ0 + Φθ might not be a stationary distribution, it
still defines a policy1

µπ (x, a)`(x, a)

(x,a)∈X ×A

= argmin µ>
π` .

[µ0 (x, a) + Φ(x,a),: θ]+
,
0
a0 [µ0 (x, a ) + Φ(x,a0 ),: θ]+

π

πθ (a|x) = P

In fact, the dual of LP (1) has the form of
min µ> ` ,

µ∈RXA

s.t.

(3)

µ> 1 = 1, µ ≥ 0, µ> (P − B) = 0 .

(2)

(4)

We denote the stationary distribution of this policy µθ ,
which is only equal to µ0 + Φθ if θ is in the feasible set.
1

We use the notation [v]− = v ∧ 0 and [v]+ = v ∨ 0.

Linear Programming for Large-Scale Markov Decision Problems

1.4. Problem definition
With the above notation, we can now be explicit about the
problem we are solving.
Definition 1 (Efficient Large-Scale Dual ALP). For an
MDP specified by ` and P , a feature matrix Φ and a stationary distribution µ0 , the efficient large-scale dual ALP
problem is to produce parameters θb such

	
µ>
` ≤ min µ>
(5)
θ ` : θ feasible for (3) + O()
θb
in time polynomial in d and 1/. The model of computation
allows access to arbitrary entries of Φ, `, P , µ0 , P > Φ, and
1> Φ in unit time.
Importantly, the computational complexity cannot scale
with X and we do not assume any knowledge of the optimal policy. In fact, as we shall see, we solve a harder
problem, which we define as follows.
Definition 2 (Expanded Efficient Large-Scale Dual ALP).
Let V : <d → <+ be some “violation function” that represents how far µ0 + Φθ is from a valid stationary distribution, satisfying V (θ) = 0 if θ is a feasible point for
the ALP (3). The expanded efficient large-scale dual ALP
problem is to produce parameters θb such that


1
d
>
V
(θ)
:
θ
∈
<
+ O(),
(6)
`
≤
min
µ
`
+
µ>
θ
θb

in time polynomial in d and 1/, under the same model of
computation as in Definition 1.
Note that the expanded problem is strictly more general as
guarantee (6) implies guarantee (5). Also, many feature
vectors Φ may not admit any feasible points. In this case,
the dual ALP problem is trivial, but the expanded problem
is still meaningful.
Having access to arbitrary entries of the quantities in Definition 1 arises naturally in many situations. In many cases,
entries of P > Φ are easy to compute. For example, suppose
that for any state x0 there is a small number of state-action
pairs (x, a) such that P (x0 |x, a) > 0. Consider Tetris; although the number of board configurations is large, each
state has a small number of possible neighbors. Dynamics specified by graphical models with small connectivity
also satisfy this constraint. Computing entries of P > Φ is
also feasible given reasonable features. If a feature ϕi is
a stationary distribution, then P > ϕi = B > ϕi . Otherwise,
it is our prerogative to design sparse feature vectors, hence
making the multiplication easy. We shall see an example of
this setting later.

(standard) assumption that any policy converges quickly
to its stationary distribution. Our algorithm take as input a constant S and an error tolerance , and has access to the various quantities listed in Definition 1. Define
Θ = {θ : θ> Φ> 1 = 1 − µ>
0 1, kθk ≤ S}. If no stationary
distribution is known, we can simply choose µ0 = 0. The
algorithm is based on stochastic convex optimization. We
prove that for any δ ∈ (0, 1), after O(1/4 ) steps of gradient descent, the algorithm finds a vector θb ∈ Θ such that,
with probability at least 1 − δ,


1
>
>
k[µ0 + Φθ]− k1
µθb ` ≤µθ ` + O




1
>


(P − B) (µ0 + Φθ) 1 + O( log(1/δ))
+O

holds for all θ ∈ Θ; i.e., we solve the expanded problem
for V (θ) bounded by a constant times the L1 error of the
violation. The second and third terms are zero for feasible
points (points in the intersection of the feasible set of LP (2)
and the span of the features). For points outside the feasible
set, these terms measure the extent of constraint violations
for the vector µ0 + Φθ, which indicates how well stationary
distributions can be represented by the chosen features.

2. A Reduction to Stochastic Convex
Optimization
In this section, we describe our algorithm as a reduction
from Markov decision problems to stochastic convex optimization. The main idea is to convert the ALP (3) into
an unconstrained optimization over Θ by adding a function of the constraint violations to the objective, then run
stochastic gradient descent with unbiased estimated of the
gradient.
For a positive constant H, form the following convex cost
function by adding a multiple of the total constraint violations to the objective of the LP (3):
c(θ) = `> (µ0 + Φθ) + H k[µ0 + Φθ]− k1


+ H (P − B)> (µ0 + Φθ)
1

= `> (µ0 + Φθ) + H k[µ0 + Φθ]− k1


+ H (P − B)> Φθ1
X

[µ0 (x, a) + Φ(x,a),: θ]− 
= `> (µ0 + Φθ) + H
(x,a)

X

(P − B)>

+H
:,x0 Φθ .
x0

(7)
1.5. Our Contributions
In this paper, we introduce an algorithm that solves the
expanded efficient large-scale dual ALP problem under a

We justify using this surrogate loss as follows. Supb ≤
pose we find a near optimal vector θb such that c(θ)
minθ∈Θ c(θ) + O(). We will prove

Linear Programming for Large-Scale Markov Decision Problems







b
b −
1. that [µ0 + Φθ]
 are
 and (P − B)> (µ0 + Φθ)
1

1

small and µ0 + Φθb is close to µθb (by Lemma 2 in
Section 2.1), and
b ≤ minθ∈Θ c(θ) + O().
2. that `> (µ0 + Φθ)
As we will show, these two facts imply that with high probability, for any θ ∈ Θ,


1
>
µ>
`
≤
µ
`
+
O
k[µ
+
Φθ]
k
0
−
θ
1
θb




1
(P − B)> (µ0 + Φθ) + O() ,
+O
1

which is to say that minimization of c(θ) solves the extended efficient large-scale ALP problem.
Unfortunately, calculating the gradients of c(θ) is O(XA).
Instead, we construct unbiased estimators and use stochastic gradient descent. Let T be the number of iterations of
our algorithm. Let q1 and q2 be distributions over the stateaction and state space, respectively (we will later discuss
how to choose them). Let ((xt , at ))t=1...T be i.i.d. samples from q1 and (x0t )t=1...T be i.i.d. samples from q2 . At
round t, the algorithm estimates subgradient ∇c(θ) by
gt (θ) = `> Φ − H

+H

Φ(xt ,at ),:
I{µ (x ,a )+Φ(xt ,at ),: θ<0}
q1 (xt , at ) 0 t t
(8)

(P − B)>
:,x0 Φ
t

q2 (x0t )

s((P − B)>
:,x0t Φθ).

This estimate is fed to the projected subgradient method,
which in turn generates a vector θt . After T rounds, we
average vectors (θt )t=1...T and obtain the final solution
PT
b
θbT =
t=1 θt /T . Vector µ0 + ΦθT defines a policy,
which in turn defines a stationary distribution µθbT .2 The
algorithm is shown in Figure 1.
2.1. Analysis
In this section, we state and prove our main result, Theorem 1. We begin with a discussion of the assumptions
we make then follow with the main theorem. We break
the proof into two main ingredients. First, we demonstrate
that a good approximation to the surrogate loss gives a feature vector that is almost a stationary distribution; this is
Lemma 2. Second, we justify the use of unbiased gradients
2

Recall that µθ is the stationary distribution of policy
[µ0 (x, a) + Φ(x,a),: θ]+
.
0
0
a0 [µ0 (x, a ) + Φ(x,a ),: θ]+

πθ (a|x) = P

With an abuse of notation, we use µθ to denote policy πθ as well.

Input: Constant S > 0, number of rounds T , constant
H.
Let ΠΘ be the Euclidean projection onto Θ.
Initialize θ1 = 0.
for t := 1, 2, . . . , T do
Sample (xt , at ) ∼ q1 and x0t ∼ q2 .
Compute subgradient estimate gt (8).
Update θt+1 = ΠΘ (θt − ηt gt ).
end for P
T
θbT = T1 t=1 θt .
Return policy πθbT .
Figure 1. The Stochastic Subgradient Method for Markov Decision Processes

in Theorem 3 and Lemma 5. The section concludes with
the proof of Theorem 1.
We make a mixing assumption on the MDP so that any
policy quickly converges to its stationary distribution.
Assumption A1 (Fast Mixing)
Let M π be a X ×
π
(XA) matrix that encodes policy π, M(i,(i−1)A+1)
-(i,iA) =
π(·|xi ). Other entries of this matrix are zero. For any
policy π, there exists a constant τ (π) > 0 such that
for all distributions d and d0 over the state-action space,
kdP M π − d0 P M π k1 ≤ e−1/τ (π) kd − d0 k1 .
Further, we assume columns of the feature matrix Φ are
positive and sum to one. Define


Φ(x,a),: 
C1 = max
,
(x,a)∈X ×A q1 (x, a)


(P − B)>

:,x Φ
C2 = max
.
x∈X
q2 (x)
These constants appear in our performance bounds. So we
would like to choose distributions q1 and q2 such that C1
and C2 are small. For example, if there is C 0 > 0 such
that for any (x, a) and i, Φ(x,a),i ≤ C 0 /(XA) and each
column of P has only N non-zero elements, then we can
simply choose q1 and q2 to be uniform distributions. Then
it is easy to see that




(P − B)>

Φ(x,a),: 
:,x Φ
0
≤C ,
≤ C 0 (N + A) .
q1 (x, a)
q2 (x)
As another example, if Φ:,i are exponential distributions
and feature values at neighboring states are close to each
other, then we can choose q1 and
 q2 to be appropriate
Φ(x,a),:  /q1 (x, a) and
exponential
distributions
so
that


>
(P − B):,x Φ /q2 (x) are always bounded. Another example is when there exists a constant C 00 > 0 such

Linear Programming for Large-Scale Markov Decision Problems

 >   > 
that,3 for any x, P:,x
Φ / B:,x Φ < C 00 and we have
access to an efficient
algorithm
that computes
Z1 =


P 
P

 > 

can sample
x B:,x Φ and
(x,a) Φ(x,a),:  and Z2 =
 > 
Φ /Z2 .
from q1 (x, a) = Φ(x,a),:  /Z1 and q2 (x) = B:,x
In what follows, we assume that appropriate distributions
q1 and q2 are known.
We now state the main theorem.
Theorem 1. Consider an expanded efficient large-scale
dual ALP problem, with violation function V = O(V1 +
V2 ), defined by
V1 (θ) = k[µ0 + Φθ]− k1


V2 (θ) = (P − B)> (µ0 + Φθ)1 .
Assume τ := sup{τ (µθ ) : θ ∈ Θ} < ∞ is finite. Suppose we apply the stochastic subgradient method (shown
in Figure 1) to the problem. Let  ∈ (0, 1). Let T = 1/4
be the number of rounds and H = 1/ be the constraints
multiplier in the subgradient estimate (8). Let θbT be the
output of the stochastic subgradient method after
√ T rounds
and let√ the learning rate be ηt = S/(G0 T ), where
G0 = d + H(C1 + C2 ). Then, for any δ ∈ (0, 1), with
probability at least 1 − δ,




1
>
(V
(θ)
+
V
(θ))
+
O()
,
µ>
`
≤
min
µ
`
+
O
1
2
θ
θbT
θ∈Θ

(9)
where the constants hidden in the big-O notation are polynomials in S, d, C1 , C2 , and log(1/δ).
The functions V1 and V2 are bounded by small constants
for any set of normalized features: for any θ ∈ Θ,
V1 (θ) ≤ kµ0 k1 + kΦθk1
X
√

Φ(x,a),: θ ≤ 1 + S d ,
≤1+
(x,a)

where the last step follows from the fact that columns of Φ
are probability distributions. Further,
X
 X >

>
P:,x

B:,x0 (µ0 + Φθ)
V2 (θ) ≤
0 (µ0 + Φθ) +
x0

≤

x0

X

>
P:,x
0

|µ0 + Φθ| +

x0

X

>
B:,x
0 |µ0 + Φθ|

x0
>

= 21 |µ0 + Φθ|
≤ 21> (|µ0 | + |Φθ|)
√
≤ 2(1 + S d) .
Thus V1 and V2 can be small given a carefully designed set
of features.
3

This condition requires that columns of Φ are close to their
one step look-ahead.

p
The optimal choice for  is  = V1 (θ∗ ) + V2 (θ∗ ), where
θ∗ is the minimizer of the RHS
p of (9). Thus, the optimized
error bound scales like O( V1 (θ∗ ) + V2 (θ∗ )). Unfortunately, θ∗ is not known in advance. To partially alleviate
the problem, once we obtain θbT , q
we can estimate V1 (θbT )
and V2 (θbT ) and use input  =
V1 (θbT ) + V2 (θbT ) in a
second run of the algorithm.
The next lemma, providing the first ingredient of the proof,
shows how the amount of constraint violation of a vector θ
shifts the resulting stationary distribution µθ .
Lemma 2. Let u ∈ RXA be a vector. Assume
X


u(x, a) = 1, k[u]− k1 ≤ 0 , u> (P − B)1 ≤ 00 .
x,a

The vector [u]+ / k[u]+ k1 defines a policy, which in turn
defines a stationary distribution µu . We have that
kµu − uk1 ≤ (τ (µu ) log(1/(20 + 00 )) + 2)(20 + 00 ) .
Proof. Define h = [u]+ / k[u]+ k1 . We first show that h is
almost a stationary distribution, in the sense that
 >

h (P − B) ≤ 20 + 00 .
(10)
1
To see this, notice that the first
 assumption
 is equivalent to
k[u]+ k1 − k[u]− k1 = 1, so h> (P − B)1 is equal to


 [u]>

+


 k[u]+ k (P − B)
1
1


(u − [u]− )> (P − B)
1
=
1 + k[u]− k1





≤ u> (P − B)1 + [u]>
− (P − B) 1


≤ 00 + k[u]− k (P − B)> 
1

1

≤ 00 + 20 ,
because the linear maps defined by P and B have operator
norms (corresponding to the 1-norm) bounded by 1. Next,
notice that
kh − uk1 ≤ kh − [u]+ k1 + k[u]+ − uk1
= k[u]− k1 + k[u]− k1 ≤ 20 .
Next we bound kµh − hk1 . Let ν0 = h be the initial stateaction distribution. We will show that as we run policy h
(equivalently, policy µh ), the state-action distribution converges to µh and this vector is close to h. From (10), we
have ν0> P = h> B + v0 , where v0 is such that kv0 k1 ≤
20 + 00 . Let M h be the X × (XA) matrix that encodes
h
policy h, via M(i,(i−1)A+1)
-(i,iA) = h(·|x = i). Other
entries of this matrix are zero. Define the state-action distribution after running policy h for one step as
ν1> := h> P M h = (h> B + v0 )M h
= h> BM h + v0 M h = h> + v0 M h .

Linear Programming for Large-Scale Markov Decision Problems

Let
v1 = v0 M h P = v0 P h and notice that kv1 k1 =
 h>
P v0>  ≤ kv0 k ≤ 20 + 00 . Thus,
1
1
ν2> = ν1> P M h = h> + (v0 + v1 )M h .
By repeating this argument for k rounds, we get that
νk> = h> + (v0 + v1 + · · · + vk−1 )M h .
Since
the operator norm of M h is no more than 1,

(v0 + v1 + · · · + vk−1 )M h  ≤ Pk−1 kvi k ≤ k(20 +
i=0
1
1
00 ). Thus, kνk − hk1 ≤ k(20 + 00 ). Now, since νk is
the state-action distribution after k rounds of policy µh ,
by the mixing assumption, kνk − µh k1 ≤ 2e−k/τ (h) . By
the choice of k = τ (h) log(1/(20 + 00 )), we get that
kµh − hk1 ≤ (τ (h) log(1/(20 + 00 )) + 2)(20 + 00 ).

have that for any δ ∈ (0, 1), with probability at least 1 − δ,
SG0
(12)
c(θbT ) − min c(θ) ≤ √
θ∈Θ
T
s



1 + 4S 2 T
1
S2T
+
2
log
+
d
log
1
+
.
T2
δ
d
With both ingredients in place, we can prove our main result.
Proof of Theorem 1. Let bT be the RHS of (12). Lemma 5
implies that with high probability for any θ ∈ Θ,
`> (µ0 + ΦθbT ) + H V1 (θbT ) + H V2 (θbT ) ≤ `> (µ0 + Φθ)
+ H V1 (θ) + H V2 (θ) + bT .

(13)

From (13), we get that
The second ingredient is the validity of using estimates of
the subgradients. We assume access to estimates of the subgradient of a convex cost function. Error bounds can be
obtained from results in the stochastic convex optimization
literature; the following theorem, a high-probability version of Lemma 3.1 of Flaxman et al. (2005) for stochastic
convex optimization, is sufficient. The proof can be found
in Appendix B.
Theorem 3. Let Z be a positive constant and Z be a
bounded convex subset of Rd such that for any z ∈ Z,
kzk ≤ Z. Let (ft )t=1,2,...,T be a sequence of real-valued
convex cost functions defined over Z. Let z1 , z2 , . . . , zT ∈
Z be defined by z1 = 0 and zt+1 = ΠZ (zt − ηft0 ), where
ΠZ is the Euclidean projection onto Z, η > 0 is a learning
rate, and f10 , . . . , fT0 are unbiased subgradient estimates
such that E [ft0 |zt ] = ∇f (zt ) √
and kft0 k ≤ F for some
F > 0. Then, for η = Z/(F T ), for any δ ∈ (0, 1),
with probability at least 1 − δ,
T
T
X
X
√
ft (zt ) − min
ft (z) ≤ ZF T
z∈Z

t=1

s
+

(11)

t=1



1
Z 2T
2
(1 + 4Z T ) 2 log + d log 1 +
.
δ
d



√
1 
2(1 + S d) + H V1 (θ) + H V2 (θ) + bT
V1 (θbT ) ≤
H
def
= 0 ,
(14)


√
1
2(1 + S d) + H V1 (θ) + H V2 (θ) + bT
V2 (θbT ) ≤
H
def 00
= .
(15)
Inequalities (14) and (15) and Lemma 2 give the following
bound:


 >

µθb ` − (µ0 + ΦθbT )> ` ≤
T

(τ (µθbT ) log(1/(20 + 00 )) + 2)(20 + 00 ) .

(16)

From (13) we also have
`> (µ0 + ΦθbT ) ≤ `> (µ0 + Φθ) +H V1 (θ) +H V2 (θ) +bT ,
which, together with (16) and Lemma 2, gives the final result:
µ>
` ≤ `> (µ0 + Φθ) + H V1 (θ) + H V2 (θ) + bT
θb
T

+ (τ (µθbT ) log(1/(20 + 00 )) + 2)(20 + 00 )
≤ µ>
θ ` + H V1 (θ) + H V2 (θ) + bT
+ (τ (µθbT ) log(1/(20 + 00 )) + 2)(20 + 00 )

Remark 4. Let BT denote the RHS of (11). If all cost functions are equal to f , then by convexity of f and
PTan application of Jensen’s inequality, we obtain that f ( t=1 zt /T )−
minz∈Z f (z) ≤ BT /T .
As the next lemma shows, Theorem 3 can be applied in our
problem to optimize the cost function c. The proof can be
found in Appendix B.
Lemma 5. Under the same conditions as in Theorem 1, we

+ (τ (µθ ) log(1/(2V1 (θ) + V2 (θ))))
× (2V1 (θ) + V2 (θ)) .
√
Recall that bT = O(H/ T ). Because H = 1/ and T =
1/4 , we get that with high probability,
for any θ ∈ Θ,

1
>
µ>
`
≤
µ
`
+
O
(V
(θ)
+
V
(θ))
+
O().
1
2
θ

θb
T

Linear Programming for Large-Scale Markov Decision Problems

Let’s compare Theorem 1 with results of de Farias and Van
Roy (2006). Their approach is to relate the original MDP
to a perturbed version4 and then analyze the corresponding
ALP. (See Appendix A for more details.) Let Ψ be a feature
matrix that is used to estimate value functions. Recall that
λ∗ is the average loss of the optimal policy and λw is the
average loss of the greedy policy with respect to value function Ψw. Let h∗γ be the differential value function when the
restart probability in the perturbed MDP is 1 − γ. For vector v and positive vector u, define the weighted maximum
norm kvk∞,u = maxx u(x) |v(x)|. de Farias and Van Roy
(2006) prove that for appropriate constants C, C 0 > 0 and
weight vector u,


C
min h∗γ − Ψw∞,u + C 0 (1 − γ) .
1−γ w
(17)
This bound has similarities to bound (9): tightness of both
bounds depends on the quality of feature vectors in representing the relevant quantities (stationary distributions in
(9) and value functions in (17)). Once again, we emphasize that the algorithm proposed by de Farias and Van Roy
(2006) is computationally expensive and requires access to
a distribution that depends on optimal policy.
λw∗ − λ∗ ≤

Remark 6. In our algorithm, we estimate the subgradient
by sampling constraints of the LP. A natural question to ask
is if we can first sample constraints then exactly solve the
resulting LP. Analysis for such an algorithm is presented in
Appendix C. However the analysis requires stronger conditions on the choice of feature vectors.

3. Experiments
In this section, we apply our algorithm to the fourdimensional discrete-time queueing network illustrated in
Figure 3. This network has a relatively long history; see,
e.g. (Rybko and Stolyar, 1992) and more recently (de Farias
and Van Roy, 2003a) (c.f. Section 6.2). There are four
queues, µ1 , . . . , µ4 , each with state 0, . . . , B. Since the
cardinality of the state space is X = (1 + B)4 , even a modest B results in huge state spaces. For time t, let Xt ∈ X
be the state and let si,t ∈ {0, 1}, i = 1, 2, 3, 4 denote the
actions. The value si,t = 1 indicates that queue i is being served. Server 1 only serves queue 1 or 4, server 2
only serves queue 2 or 3, and neither server can idle. Thus,
s1,t + s4,t = 1 and s2,t + s3,t = 1. The dynamics are defined by the rate parameters a1 , a3 , d1 , d2 , d3 , d4 ∈ (0, 1)
as follows. At each time t, the following random variables are sampled independently: A1,t ∼ Bernoulli(a1 ),
A3,t ∼ Bernoulli(a3 ), and Di,t ∼ Bernoulli(di si,t ) for
i = 1, 2, 3, 4. Using e1 , . . . , e4 to denote the standard basis
4

In a perturbed MDP, the state process restarts with a certain
probability to a restart distribution. Such perturbed MDPs are
closely related to discounted MDPs.

vectors, the dynamics are:
0
Xt+1
=Xt + A1,t e1 + A3,t e3

+ D1,t (e2 − e1 ) − D2,t e2
+ D3,t (e4 − e3 ) − D4,t e4 ,
0
and Xt+1 = max(0, min(B, Xt+1
)) (i.e. all four states
are thresholded from below by 0 and above by B). The
loss function is the total queue size: `(Xt ) = ||Xt ||1 . We
compared our method against two common heuristics. In
the first, denoted LONGER, each server operates on the
queue that is longer with ties broken uniformly at random
(e.g. if queue 1 and 4 had the same size, they are equally
likely to be served). In the second, denoted LBFS (last
buffer first served), the downstream queues always have
priority (server 1 will serve queue 4 unless it has length
0, and server 2 will serve queue 2 unless it has length 0).
These heuristics are common and have been used an benchmarks for queueing networks (e.g. (de Farias and Van Roy,
2003a)).

We used a1 = a3 = .08, d1 = d2 = .12, and d3 = d4 =
.28, and buffer sizes B1 = B4 = 38, B2 = B3 = 25
as the parameters of the network. The asymmetric size
was chosen because server 1 is the bottleneck and tends to
have longer queues. The first two features are the stationary
distributions corresponding to the two heuristics LONGER
and LBFS. We also included two types of features that do
not correspond to stationary distribution. For every interval
(0, 5], (6, 10], . . . , (45, 50] and action A, we added a feature ψ with ϕ(x, a) = 1 if `(x, a) is in the interval and
a = A. To define the second type, consider the three intervals I1 = [0, 10], I2 = [11, 20], and I3 = [21, 25]. For
every 4-tuple of intervals (J1 , J2 , J3 , J4 ) ∈ {I1 , I2 , I3 }4
and action A, we created a feature ψ with ψ(x, a) = 1 only
if xi ∈ Ji and a = A. Every feature was normalized to
sum to 1. In total, we had 372 features which is about a
104 reduction in dimension from the original problem.
To obtain a lower variance estimate of our gradient, we
sampled gt (θ) 1000 times and averaged (which is equivalent to sampling 1000 i.i.d. constraints from both q1 and
q2 ). Rather than the fixed learning rate η considered in Section 2, our learning rate began at 10−4 and halved every
2000 iterations. The results of the simulations are plotted in Figure 3, where θbt denotes the running average of
θt . The left plot is of the LP objective, `> (µ0 + Φθbt ).
The
 middle plot
 is of the sum of theconstraint violations,




b
[µ0 + Φθt ]−  + (P − B)> Φθbt  . Thus, c(θbt ) is a
1
1
scaled sum of the first two plots. Finally, the right plot
is of the average losses, `> µθbt and the two horizontal lines
correspond to the loss of the two heuristics, LONGER and
LBFS. The right plot demonstrates that, as predicted by
our theory, minimizing the surrogate loss c(θ) does lead
to lower average losses.

Linear Programming for Large-Scale Markov Decision Problems

a1
d4

µ1

d1

µ2

d2

µ4

d2

µ3

a3

server1

server2

Figure 2. The 4D queueing network. Customers arrive at queue µ1 or µ3 then are referred to queue µ2 or µ4 , respectively. Server 1 can
either process queue 1 or 4, and server 2 can only process queue 2 or 3.
loss of running average

total constraint violation of running average

0

42

average loss of the running average policy

10

52
50

41

48
40

46
−1

39

10

44
42

38

40
37

36

38
−2

0

2000

4000

6000

8000

10

0

2000

4000

6000

8000

36

0

2000

4000

6000

8000

Figure 3. The left plot is of the linear objective of the running average, i.e. `> Φθbt . The center plot is the sum of the two constraint
violations of θbt , and the right plot is `> µ̃θbt (the average loss of the derived policy). The two horizontal lines correspond to the loss of
the two heuristics, LONGER and LBFS.

All previous algorithms (including (de Farias and Van Roy,
2003a)) work with value functions, while our algorithm
works with stationary distributions. Due to this difference,
we cannot use the same feature vectors to make a direct
comparison. The solution that we find in this different approximating set is comparable to the solution of de Farias
and Van Roy (2003a).

5. Acknowledgements
We gratefully acknowledge the support of the NSF through
grant CCF-1115788 and of the ARC through an Australian Research Council Australian Laureate Fellowship
(FL110100281).

References
4. Conclusions
In this paper, we defined and solved the extended largescale efficient ALP problem. We proved that, under certain
assumptions about the dynamics, the stochastic subgradient
method produces a policy with average loss competitive to
all θ ∈ Θ, not not just all θ producing a stationary distribution. We demonstrated this algorithm on the RybkoStoylar four-dimensional queueing network and recovered
a policy better than two common heuristics and comparable to previous results on ALPs (de Farias and Van Roy,
2003a). A future direction is to find other interesting regularity conditions under which we can handle large-scale
MDP problems. We also plan to conduct more experiments
with challenging large-scale problems.

Y. Abbasi-Yadkori.
Online Learning for Linearly
Parametrized Control Problems. PhD thesis, University
of Alberta, 2012.
R. Bellman. Dynamic Programming. Princeton University
Press, 1957.
D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 2007.
D. P. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena scientific optimization and computation
series. Athena Scientific, 1996.
G. Calafiore and M. C. Campi. Uncertain convex programs:
randomized solutions and confidence levels. Mathematical Programming, 102(1):25–46, 2005.

Linear Programming for Large-Scale Markov Decision Problems

M. C. Campi and S. Garatti. The exact feasibility of randomized solutions of uncertain convex programs. SIAM
Journal on Optimization, 19(3):1211–1230, 2008.
D. P. de Farias and B. Van Roy. The linear programming
approach to approximate dynamic programming. Operations Research, 51, 2003a.
D. P. de Farias and B. Van Roy. Approximate linear programming for average-cost dynamic programming. In
NIPS, 2003b.
D. P. de Farias and B. Van Roy. On constraint sampling
in the linear programming approach to approximate dynamic programming. Mathematics of Operations Research, 29, 2004.
D. P. de Farias and B. Van Roy. A cost-shaping linear program for average-cost approximate dynamic programming with performance guarantees. Mathematics of Operations Research, 31, 2006.
V. H. de la Peña, T. L. Lai, and Q-M. Shao. Self-normalized
processes: Limit theory and Statistical Applications.
Springer, 2009.
V. V. Desai, V. F. Farias, and C. C. Moallemi. Approximate
dynamic programming via a smoothed linear program.
Operations Research, 60(3):655–674, 2012.
A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online
convex optimization in the bandit setting: gradient descent without a gradient. In SODA, 2005.
C. Guestrin, M. Hauskrecht, and B. Kveton. Solving factored mdps with continuous and discrete variables. In
UAI, 2004.
M. Hauskrecht and B. Kveton. Linear program approximations to factored continuous-state markov decision processes. In NIPS, 2003.
R. A. Howard. Dynamic Programming and Markov Processes. MIT, 1960.
H. R. Maei, Cs. Szepesvári, S. Bhatnagar, D. Precup,
D. Silver, and R. S. Sutton. Convergent temporaldifference learning with arbitrary smooth function approximation. In NIPS, 2009.
H. R. Maei, Cs. Szepesvári, S. Bhatnagar, and R. S. Sutton.
Toward off-policy learning control with function approximation. In ICML, 2010.
A. S. Manne. Linear programming and sequential decisions. Management Science, 6(3):259–267, 1960.
M. Petrik and S. Zilberstein. Constraint relaxation in approximate linear programs. In ICML, 2009.

A. N. Rybko and A. L. Stolyar. Ergodicity of stochastic processes describing the operation of open queueing
networks. Problemy Peredachi Informatsii, 28(3):3–26,
1992.
P. Schweitzer and A. Seidmann. Generalized polynomial
approximations in Markovian decision processes. Journal of Mathematical Analysis and Applications, 110:
568–582, 1985.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An
Introduction. Bradford Book. MIT Press, 1998.
R. S. Sutton, H. R. Maei, D. Precup, S. Bhatnagar, D. Silver, Cs. Szepesvári, and E. Wiewiora. Fast gradientdescent methods for temporal-difference learning with
linear function approximation. In ICML, 2009a.
R. S. Sutton, Cs. Szepesvári, and H. R. Maei. A convergent
O(n) algorithm for off-policy temporal-difference learning with linear function approximation. In NIPS, 2009b.
V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16
(2):264–280, 1971.
M. H. Veatch. Approximate linear programming for average cost mdps. Mathematics of Operations Research, 38
(3), 2013.
T. Wang, D. Lizotte, M. Bowling, and D. Schuurmans.
Dual representations for dynamic programming. Journal of Machine Learning Research, pages 1–29, 2008.
M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In ICML, 2003.

