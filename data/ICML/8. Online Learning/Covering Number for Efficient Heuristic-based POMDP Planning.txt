Covering Number for Efficient Heuristic-Based POMDP Planning

Zongzhang Zhang
ZHANGZZ @ COMP. NUS . EDU . SG
David Hsu
DYHSU @ COMP. NUS . EDU . SG
Wee Sun Lee
LEEWS @ COMP. NUS . EDU . SG
Department of Computer Science, National University of Singapore, Singapore 117417, Singapore

Abstract
The difficulty of POMDP planning depends on
the size of the search space involved. Heuristics are often used to reduce the search space
size and improve computational efficiency; however, there are few theoretical bounds on their
effectiveness. In this paper, we use the covering number to characterize the size of the search
space reachable under heuristics and connect the
complexity of POMDP planning to the effectiveness of heuristics. With insights from the theoretical analysis, we have developed a practical
POMDP algorithm, Packing-Guided Value Iteration (PGVI). Empirically, PGVI is competitive
with the state-of-the-art point-based POMDP algorithms on 65 small benchmark problems and
outperforms them on 4 larger problems.

1. Introduction
Partially observable Markov decision processes (POMDPs)
provide a rich mathematical model for planning under uncertainty (Kaelbling et al., 1998). However, POMDPs are
computationally intractable to solve exactly (Madani et al.,
1999). In the past decade, enormous progress has been
made in computing approximate POMDP solutions (Pineau
et al., 2003; Smith & Simmons, 2005; Kurniawati et al.,
2008; Ross et al., 2008; Bonet & Geffner, 2009; Silver &
Veness, 2010; Zhang & Chen, 2012; GrzesÃÅ et al., 2013;
Shani et al., 2013).
On the theoretical front, the covering number of the reachable space has been proposed to quantify the complexity of POMDP planning (Hsu et al., 2007), particularly,
for point-based methods (Smith & Simmons, 2005; Pineau
et al., 2006; Shani et al., 2007; Kurniawati et al., 2008;
Shani et al., 2013). Intuitively, the covering number is the
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

minimum number of fixed-size balls required to cover the
search space so that all points in the search space lie within
some ball. Both theoretical and empirical results support
the covering number as a promising complexity measure
for POMDP planning and learning (Hsu et al., 2007; Zhang
et al., 2012).
In practice, many well-known POMDP planning algorithms use the lower and upper bounds of the optimal value
function (Hauskrecht, 2000; Smith & Simmons, 2005; Kurniawati et al., 2008), or just the upper bound as heuristics (Hauskrecht, 2000; Bonet & Geffner, 2009) in guiding the search towards the optimally reachable space. Although these algorithms have made impressive progress in
computing approximate solutions by using heuristics and
have been successfully applied in several practical domains
(Hsiao et al., 2007; Pineau et al., 2006; Hsu et al., 2008;
Kurniawati et al., 2011; GrzesÃÅ et al., 2013), few existing
works have analyzed the relationship between the quality
of the heuristics and the complexity of POMDP planning.
In this paper, we fill this gap by connecting the size of
search spaces reachable under heuristic, as measured by the
covering number, to the complexity of POMDP planning.
We consider two cases, when only an upper bound heuristic
is available and when both upper and lower bound heuristics are available. We show that an -optimal solution can
be computed in time polynomial in the covering number for
the both cases. This suggests one avenue of handling practical problems: use domain knowledge to find good upper
and lower bounds that effectively reduce the covering number of the reachable space under the heuristics.
One key idea behind our theoretical analysis is to build a
separate packing of sampled beliefs at each level of the
search tree to control the number of beliefs at level d,
so that it does not grow exponentially in d. Packing is
closely related to covering and is used to create a covering in the proofs. We exploit this proof idea in building a
practical point-based algorithm, Packing-Guided Value Iteration (PGVI). In addition to providing theoretical guarantees, packing helps PGVI to identify interesting parts of
the space that is sparsely packed and to sample new beliefs

Covering Number for Efficient Heuristic-based POMDP Planning

and perform point-based backup there. Compared with
state-of-the-art point-based POMDP algorithms, PGVI is
very efficient on 65 small benchmark problems and 4 larger
robotic problems.

2. Preliminaries
A POMDP models an agent taking a sequence of actions in
a partially observable stochastic environment to maximize
its total reward (Kaelbling et al., 1998). A discrete and
discounted POMDP model can be formally defined by a
tuple (S, A, Z, T, ‚Ñ¶, R, Œ≥). In the tuple, S, A and Z are the
finite and discrete state space, action space, and observation
space, respectively. At each time step, the agent takes some
action a ‚àà A and moves from a start state s to an end state
s0 . The end state s0 is given by a state-transition function
T (s, a, s0 ) : S √ó A √ó S ‚Üí [0, 1], where T (s, a, s0 ) =
P r(s0 |s, a). The agent then makes an observation to gather
information on its current state. The outcome of observing
z ‚àà Z is given by an observation function ‚Ñ¶(a, s0 , z) :
A √ó S √ó Z ‚Üí [0, 1], where ‚Ñ¶(a, s0 , z) = P r(z|a, s0 ). The
reward function R(s, a) : S √ó A ‚Üí R gives the agent
a real-value reward after it takes action a in state s. We
let Rmax = maxs‚ààS,a‚ààA |R(s, a)|. The last element Œ≥ ‚àà
(0, 1) is the discount
P‚àû factor. Thus, the expected total reward
is given by E[ t=0 Œ≥ t R(st , at )], where st and at are the
agent‚Äôs state and action at time t.
A belief state (or belief ) b is a discrete probability distribution over the state space, whose element b(s) gives
the probability that the system‚Äôs state is s. A belief state
space B is comprised of all possible beliefs. A search
space B is a subset of B and can be represented as an
AND/OR belief tree TB rooted at the initial belief b0 . In
the tree, nodes and edges correspond to beliefs and actionobservation pairs, respectively. Suppose that a child node
b0 , denoted by œÑ (b, a, z), is connected to its parent b by an
0
edge (a, z). We can compute bP
using the Bayesian formula
1
0
0 0
b (s ) = P r(z|b,a) ‚Ñ¶(a, s , z) s‚ààS T (s, a, s0 )b(s), where
P
P
0
0
P r(z|b, a) =
s0 ‚ààS ‚Ñ¶(a, s , z)
s‚ààS T (s, a, s )b(s)
(Kaelbling et al., 1998).
A POMDP solution is a policy œÄ that specifies the action
œÄ(b) for every belief b. Our goal is to find an optimal policy œÄ ‚àó that maximizes the expected total reward. A policy œÄ
induces a value function V œÄ (b) that specifies the expected
total reward of executing œÄ starting from any belief b. The
optimal value function V ‚àó (b) is the value function asso‚àó
ciated
with the optimal
Q‚àó (b, a) as
P
Ppolicy œÄ . We define
‚àó
s‚ààS R(s, a)b(s) +
z‚ààZ P r(z|b, a)V (œÑ (b, a, z)), and
therefore, have V ‚àó (b) = maxa‚ààA Q‚àó (b, a). We denote V L
and V U as the lower and upper bounds of V ‚àó , respectively.
Both V L and V U are assumed to be uniformly improvable
(Smith, 2007) in this paper, meaning that applying a pointbased update brings them everywhere closer to V ‚àó . We

also assume that we are provided with heuristics, f and g,
that can provide initial values VfL to the lower bound and
VgU to the upper bound. Similarly, we use QL , QU , QL
f
‚àó
and QU
g for the corresponding bounds of Q . In practice,
the bounds for Q can be constructed from the bounds for V
by one step lookahead (Hauskrecht, 2000).
We describe the mathematical definition of the covering
number of a set of points as follows:
Definition 1. (Hsu et al., 2007) Given a metric space X,
a Œ¥-cover of a set B ‚äÜ X is a set of points C ‚äÜ X such
that for every point b ‚àà B, there is a point c ‚àà C with
||b ‚àí c|| ‚â§ Œ¥. The Œ¥-covering number of B, denoted by
CB (Œ¥), is the size of the smallest Œ¥-cover of B.
Intuitively, the covering number is equal to the minimum
number of balls of radius Œ¥ needed to cover the set B. In
this paper, we measure the distance between belief points
in an L1 metric space B: for b1 , b2 ‚àà B, ||b1 ‚àí b2 || =
P
s‚ààS |b1 ‚àíb2 |. We refer a belief b‚Äôs Œ¥-region as a subspace
in X that satisfies ||b0 ‚àí b|| ‚â§ Œ¥ for all b0 in the region.
We use the covering number to measure the size of belief
spaces. The set of beliefs that are reachable from the initial
belief b0 under arbitrary sequences of actions and observations is denoted by R(b0 ). The optimally reachable belief
space R‚àó (b0 ) refers to the set of beliefs that are reachable
from b0 under some optimal policy. We denote C(Œ¥) as the
Œ¥-covering number of R(b0 ), C ‚àó (Œ¥) as the Œ¥-covering number of R‚àó (b0 ), and TR as the tree rooted at b0 consisting of
beliefs in the reachable belief space R(b0 ).

3. Related Work
Kakade and his colleagues applied the notion of the covering number into reinforcement learning (Kakade et al.,
2003). Later, Hsu et al. (2007) extended use of covering
number from MDPs to POMDP planning. Recent research
work provided empirical evidence that estimated covering
number of R(b0 ) was better than the state space size in predicting the difficulty of POMDP planning and learning on
small benchmark problems (Zhang et al., 2012).
In (Hsu et al., 2007), the key point of connecting the covering number to POMDP planning complexity is the following: for any two beliefs, if their distance is small, then their
optimal values are also similar. Thus, when the value of a
belief b is accurate enough, it can be used to estimate the
value of beliefs that are close to b with only small error. By
stopping the search when it is near to a region that has been
searched before, the covering number can be exploited to
bound the width of the search tree. Together with the idea
of bounding the depth of the search tree by the discount factor, it was shown that an approximately optimal POMDP
solution can be computed in time at most quadratic poly-

Covering Number for Efficient Heuristic-based POMDP Planning

nomial in both C(Œ¥) and C ‚àó (Œ¥).
In the past decade, point-based value-iteration algorithms
have made impressive progress in computing approximate
solutions to large POMDPs (Shani et al., 2013). Their
success is mainly due to the efficient sampling strategies.
Point-Based Value Iteration (PBVI) (Pineau et al., 2003)
prefers to sample beliefs that are far away from those sampled beliefs. Heuristic Search Value Iteration (HSVI2)
(Smith & Simmons, 2005), SARSOP (Kurniawati et al.,
2008) and GapMin (Poupart et al., 2011) sample beliefs by
using both the lower and upper bound heuristics. The lower
bound is usually obtained by using the blind policy and the
upper bound is often initialized by the QMDP or Fast Informed Bound (FIB) method (Hauskrecht, 2000). These
algorithms use the action-selection strategy that chooses
the action with the highest upper bound. Compared with
HSVI2 and SARSOP, GapMin strives to compute tight
bounds by performing a prioritized breadth-first search,
propagating upper bound improvements, and computing
exact interpolations by Linear Programming (LP) (Poupart
et al., 2011). However, the idea of using the insight from
the covering number to control the width of the search tree
has not been exploited in the three state-of-the-art algorithms. The performance guarantee for HSVI2, provided in
(Smith, 2007), is O(h ¬∑ |A|h |Z|h ), where h is the height of
the search tree. It is essentially the time required to search
the whole depth-bounded search tree.

4. Complexity of Heuristic-Based POMDP
Planning
We examine the complexity of POMDP planning when various heuristics are used. We show that these heuristics can
be used to define various search spaces and that approximately optimal POMDP solutions can be found in time
polynomial in the covering number of these search spaces.
We start with insights from practical POMDP algorithms
such as HSVI2 and SARSOP. These algorithms are action
optimistic ‚Äì they select actions with the highest QU during the search process. Since V ‚àó (b) ‚â§ maxa‚ààA QU (b, a)
and QU is uniformly improvable, we know that action optimistic algorithms have zero probability of visiting beliefs
‚àó
under the action branch a that satisfies QU
g (b, a) < V (b).
This allows us to define the search space reachable under
action optimistic algorithms with heuristic g, RU
g (b0 ), as
the set of beliefs b that can be reached from b0 by taking
‚àó
action branches a that satisfy QU
g (b, a) ‚â• V (b).
The size of RU
g (b0 ) gives a reasonable indication of the
complexity of solving the POMDP exactly. However, we
would like to approximate in two ways: by fixing the depth
of the search tree and by interpolating the values of nearby
beliefs to take advantage of the Lipschitz property. We

define CgU (Œ¥) as the Œ¥-covering number of RU
g (b0 ). Interestingly, approximately optimal POMDP solutions can be
found in time polynomial in CgU (Œ¥). As CgU (Œ¥) depends on
QU
g , the covering number of the space reachable under action optimistic algorithms is a reasonable measure of the
quality of the heuristic. Note also that RU
g (b0 ) becomes
R‚àó (b0 ) if we use Q‚àó as our heuristic, hence the covering
number converges to the covering number of R‚àó (b0 ) as the
quality of the heuristic improves.
Theorem 1. Given any constant  > 0 and any initial belief b0 ‚àà B. Let CgU (Œ¥) be the Œ¥-covering num‚àó
ber of RU
g (b0 ). Then, an approximation V (b0 ) of V (b0 ),
‚àó
with error |V (b0 ) ‚àí V (b0 )| ‚â§ , can be found in time
(1‚àíŒ≥)2 
O(h ¬∑ CgU (Œ¥/2)2 ), where h = logŒ≥ (1‚àíŒ≥)
2Rmax , Œ¥ = 2Œ≥Rmax and
QU
g (b, a) is used for the initial upper bound.
Before proving Theorem 1, we state two lemmas from (Hsu
et al., 2007). The first lemma states that the optimal value
function V ‚àó satisfies the following Lipschitz condition:
Lemma 1. (Hsu et al., 2007) For any two belief points b
max
Œ¥.
and b0 , if ||b ‚àí b0 || ‚â§ Œ¥, then |V ‚àó (b) ‚àí V ‚àó (b0 )| ‚â§ R1‚àíŒ≥
The second one is related to the packing number, a notion
closely related to the covering number:
Definition 2. Given a metric space X, a Œ¥-packing of a set
B ‚äÜ X is a set of points P in B such that for any two points
p1 , p2 ‚àà P , ||p1 ‚àí p2 || > Œ¥. The Œ¥-packing number of a set
B, denoted PB (Œ¥), is the size of the largest Œ¥-packing of B.
For any set B, the following relationship holds between its
packing number and covering number.
Lemma 2. (Hsu et al., 2007) CB (Œ¥) ‚â§ PB (Œ¥) ‚â§ CB (Œ¥/2).
Proof. (of Theorem 1) To prove the result, we give an algorithm that computes the required approximation. It performs a depth-first search on a depth-bounded belief tree
and uses approximate memorization to avoid unnecessarily
computing the values of very similar beliefs. Intuitively, to
achieve a polynomial time algorithm, we bound the height
of the tree by using the discount factor and bound the width
of the tree by exploiting the covering number.
We perform the depth-first search recursively on TR that
has root b0 and height h, while maintaining a Œ¥-packing
at every level of TR . By convention, the root node is at
level 0 and the leaf nodes are at level h. Each leaf node
bl is initialized with estimated value V (bl ) = 0. At an
internal node b, we first check if b is within a distance Œ¥
of a point b0 in the current packing at level i. If it is we
set the estimate V (b) = V (b0 ), abort the recursion at b,
and backtrack. Otherwise, we add b to the packing, sort
the actions using QU
g (b, a) and explore the actions according to the sorted order, with the larger values searched earlier. The estimate V (b) is initialized to the value returned

Covering Number for Efficient Heuristic-based POMDP Planning

by searching the first action and updated each time searching a new action, which returns an improved value, until
(1‚àíŒ≥ h‚àíi )

.
QU
g (b, a) < V (b) + 2Œ≥ i +
2
h‚àíi

We now prove that |V ‚àó (b) ‚àí V (b)| ‚â§ 2Œ≥ i + (1‚àíŒ≥2 ) after the update operations are completed at any b at level
i of TR . Following (Hsu et al., 2007), we prove that
(1‚àíŒ≥ h‚àíi )
max
Œ¥ + Œ≥ h‚àíi R1‚àíŒ≥
. This
|V (b) ‚àí V ‚àó (b)| ‚â§ Œ≥Rmax
(1‚àíŒ≥)2
gives |V ‚àó (b) ‚àí V (b)| ‚â§
logŒ≥ (1‚àíŒ≥)
2Rmax

and Œ¥ =


2Œ≥ i
2
(1‚àíŒ≥) 
2Œ≥Rmax .

+

(1‚àíŒ≥ h‚àíi )
2

by setting h =

Let i = |V (b) ‚àí V ‚àó (b)| be the approximation error for a
node b at level i of TR , if search is not aborted at b. Let 0i
be the error for b, if the search aborts at b and sets V (b) =
V (b0 ) for some b0 in the packing at level i. We have
0i

=

|V ‚àó (b) ‚àí V (b0 )|

‚â§ |V ‚àó (b) ‚àí V ‚àó (b0 )| + |V ‚àó (b0 ) ‚àí V (b0 )|
Rmax
Œ¥ + i ,
‚â§
1‚àíŒ≥
where the last inequality uses Lemma 1 and the definition of i . At the leaves, we set the estimated value to 0,
hence h ‚â§ Rmax /(1 ‚àí Œ≥). The children of b, which are
at level i ‚àí 1, have error at most 0i‚àí1 . We do a proof by
induction. Assume that, at level i + 1, |V (b) ‚àí V ‚àó (b)| ‚â§
Œ≥Rmax (1‚àíŒ≥ h‚àíi‚àí1 )
max
Œ¥ + Œ≥ h‚àíi‚àí1 R1‚àíŒ≥
. For every action a that
(1‚àíŒ≥)2
we search at level i, we have
|Q(b, a) ‚àí Q‚àó (b, a)|


Œ≥Rmax (1 ‚àí Œ≥ h‚àíi‚àí1 )
h‚àíi‚àí1 Rmax
‚â§ Œ≥
Œ¥ + (Œ¥ + Œ≥
)
(1 ‚àí Œ≥)2
1‚àíŒ≥
=
=

Œ≥Rmax (1 ‚àí Œ≥ h‚àíi )
Rmax
Œ¥ + Œ≥ h‚àíi
(1 ‚àí Œ≥)2
1‚àíŒ≥

(1 ‚àí Œ≥ h‚àíi )
+
2Œ≥ i
2

when we set h = logŒ≥

(1‚àíŒ≥)
2Rmax

and Œ¥ =

(1‚àíŒ≥)2 
2Œ≥Rmax .

Each action is backed up in sorted order until QU
g (b, a) <
h‚àíi

V (b) + 2Œ≥ i + (1‚àíŒ≥2 ) , where the right hand side is
the current upper bound on V ‚àó (b). Because the remain0
U
ing actions a0 have QU
g (b, a ) ‚â§ Qg (b, a), we know for
h‚àíi

sure that V ‚àó (b) ‚â§ V (b) + 2Œ≥ i + (1‚àíŒ≥2 ) . At the
same time, any action a that has been searched estabh‚àíi
lishes that V ‚àó (b) ‚â• Q(b, a) ‚àí 2Œ≥ i ‚àí (1‚àíŒ≥2 ) . As V (b)
is the value of the largest Q(b, a) found so far, we have
h‚àíi
V ‚àó (b) ‚â• V (b) ‚àí 2Œ≥ i ‚àí (1‚àíŒ≥2 ) . Taken together, this
implies |V ‚àó (b) ‚àí V (b)| ‚â§


2Œ≥ i

+

(1‚àíŒ≥ h‚àíi )
.
2

Finally, we calculate the running time of the algorithm. We
h‚àíi
first note that V ‚àó (b) ‚â§ V (b) + 2Œ≥ i + (1‚àíŒ≥2 ) , hence we

‚àó
will never search an action for which QU
g (b, a) < V (b).
This implies that the number of elements in the packing at each level is bounded by the packing number of
U
RU
g (b0 ). For each node b in the packing of Rg (b0 ) is
expanded and it takes O(|A| log |A|) time to determine
the search order of action branches. Then, it calculates
the beliefs and the corresponding values for all its (expanded) children and performs a point-based update at b
to compute V (b). It takes O(|S|2 ) time to calculate the
belief at a child node. After that, we perform a nearest
neighbour search in O(PgU (Œ¥)|S|) time to check whether
the child node lies within a distance Œ¥ of any point in
the packing of RU
g (b0 ) at that level. Since b has at most
|A||Z| expanded children, the expansion operation takes
O(|A| log |A| + |A||Z|(|S|2 + |S|PgU (Œ¥))) time. The pointbased update then computes V (b) as an average of its children‚Äôs values, weighted by the probabilities specified by the
observation function, and takes only O(|A||Z|) time. Since
there are h packing of size PgU (Œ¥) each and by Lemma 2,
PgU (Œ¥) ‚â§ CgU (Œ¥/2), the total running time of our algo
rithm is O h ¬∑ CgU (Œ¥/2)(|A| log |A| + |A||Z|(|S|2 + (|S| +

1)CgU (Œ¥/2))) . Assume that |S|, |A|, and |Z| are constant
to focus on the dependency on the covering number. So,
we get the final result.

In Theorem 1, only an initial upper bound is utilized. Algorithms such as HSVI2 and SARSOP utilize an initial
lower bound as well (Smith & Simmons, 2005; Kurniawati
et al., 2008). The use of a good initial lower bound may cut
down the size of the search space substantially. We define
a search space limited by lower bound VfL (b) and upper
bound VgU (b) as follows: Rg,U
f,L (b0 ) is the space reachable
from b0 under all action-observation sequences satisfying

L
‚àó
U
QU
g (b, a) ‚â• V (b) and Vg (b) ‚àí Vf (b) > Œ≥ db , where db
is the depth (or level) of b in the belief tree TR .
Theorem 2. Given any constant  > 0 and any initial
g,U
belief b0 ‚àà B. Let Cf,L
(Œ¥) be the Œ¥-covering number
of Rg,U
(b
).
Then,
an
approximation
V (b0 ) of V ‚àó (b0 ),
f,L 0
‚àó
with error |V (b0 ) ‚àí V (b0 )| ‚â§ 2, can be found in time
(1‚àíŒ≥)2 
g,U
O(h ¬∑ Cf,L
(Œ¥/2)2 ), where h = logŒ≥ (1‚àíŒ≥)
2Rmax , Œ¥ = 2Œ≥Rmax ,
VfL (b) is used as an initial lower bound and VgU (b) is used
as an initial upper bound.
Proof. We prove this theorem by using a modified algorithm in the proof of Theorem 1. We perform the depth-first
search recursively on a belief tree TR that has root b0 and
height h, while maintaining a Œ¥-packing of Rg,U
f,L (b0 ) at evU
L
ery level. If b is not in Rg,U
(b
),
namely
V
(b)‚àíV
0
0 (b) ‚â§
f,L 0

Œ≥ db

V L (b)+V U (b)

, we set V (b) = f 2 g , abort the recursion at b,
and backtrack. Else, if b is within a distance Œ¥ of a b0 in

Covering Number for Efficient Heuristic-based POMDP Planning

the current packing at level i, we set V (b) = V (b0 ), abort
the recursion at b, and backtrack. Otherwise, we add b to
the packing, sort the actions using QU
g (b, a) and explore
the actions according to the sorted order, with the larger
values searched earlier. The estimate V (b) is initialized to
the value returned by searching the first action and updated
each time searching a new action, which returns an im(1‚àíŒ≥ h‚àíi‚àí1 )
3
.
proved value, until QU
g (b, a) < V (b) + 2Œ≥ i +
2

Algorithm 1 œÄ = PGVI(, Œ¥).

We now calculate the values for h and Œ¥ required to achieve
the given approximation bound 2 at b0 . Let i = |V ‚àó (b) ‚àí
V (b)| denote the approximation error for a node b at level
i of TR , if the recursive search continues in the children of
b. Let 0i denote the error for b, if the search aborts at b and
sets V (b) = V (b0 ) for some b0 in the packing at level i.
max
Œ¥ + i . As in Theorem 1, we explore an
Hence, 0i ‚â§ R1‚àíŒ≥
action only if its initial upper bound is at least as large as
the best upper bound among the searched actions. By the
same argument as in Theorem 1‚Äôs proof, after completing
the update operations at b at level i, we have


h 
i

0
0
,

+

‚â§
Œ≥
i ‚â§ Œ≥ max
i+1
i+1
Œ≥ i+1
Œ≥ i+1

Algorithm 2 EXPLORE(b, db , , Œ¥).
1: if excess(b, db , ) ‚â§ 0 then
2:
insert b into finished(db );
3:
return ;
4: end if
5: a‚àó = arg maxa‚ààA QU (b, a);
6: z ‚àó = arg maxz‚ààZ UF [P r(z|b, a‚àó ) ¬∑ excess(œÑ (b, a‚àó , z),
db + 1, ) ¬∑ dis(œÑ (b, a‚àó , z), packing(db + 1), Œ¥)];
7: if z ‚àó == NULL or excess(œÑ (b, a‚àó , z ‚àó ), db + 1, ) ‚â§ 0
then
8:
insert b into finished(db );
9: else
10:
p0min = arg minp‚ààpacking(db +1) ||œÑ (b, a‚àó , z ‚àó ) ‚àí p||;
11:
if ||œÑ (b, a‚àó , z ‚àó ) ‚àí p0min || > Œ¥ then
12:
insert œÑ (b, a‚àó , z ‚àó ) into packing(db + 1);
13:
end if
2

14:
if ||œÑ (b, a‚àó , z ‚àó ) ‚àí p0min || > (1‚àíŒ≥)
2Œ≥Rmax then
‚àó ‚àó
15:
EXPLORE(œÑ (b, a , z ), db + 1, , Œ¥);
16:
else if p0min ‚àà
/ finished(db + 1) then
17:
EXPLORE(p0min , db + 1, , Œ¥);
18:
else
19:
insert œÑ (b, a‚àó , z ‚àó ) into finished(db + 1);
20:
end if
21: end if
22: Perform a point-based update of lower and upper
bounds at belief b;

and thus we can write the recurrence
h 
R
i
max
i ‚â§ Œ≥ i+1 +
Œ¥ + i+1 .
Œ≥
1‚àíŒ≥
Rmax
Rmax
L
1‚àíŒ≥ and Vf (b) ‚â• ‚àí 1‚àíŒ≥
U
L
V (b)‚àíV (b)
max
maxb‚ààB g 2 f
‚â§ R1‚àíŒ≥
.

Clearly, we can set VgU (b) ‚â§
for all b ‚àà B. So h ‚â§

Expanding the recurrence, we get
k

‚â§


Œ≥Rmax (1 ‚àí Œ≥ (h‚àí1)‚àík )
Œ≥ h‚àík Rmax
+
Œ¥+
k
2
Œ≥
(1 ‚àí Œ≥)
1‚àíŒ≥

=

3
(1 ‚àí Œ≥ (h‚àí1)‚àík )
+
,
2Œ≥ k
2

which holds for all 0 ‚â§ k ‚â§ h ‚àí 2, by setting Œ¥ =
(1‚àíŒ≥)
2Rmax . The final equality
(1‚àíŒ≥ h‚àíi‚àí1 )
to instead of 2Œ≥ i
2

and h = logŒ≥

(1‚àíŒ≥)2 
2Œ≥Rmax

Initialize the bounds V L and V U ;
packing = ‚àÖ, finished = ‚àÖ;
while V U (b0 ) ‚àí V L (b0 ) > 2 do
EXPLORE(b = b0 , db = 0, , Œ¥);
end while
return the action corresponding to V L ;

5. Packing-Guided Value Iteration

explains why we
h‚àíi

3
use 2Œ≥
+ (1‚àíŒ≥2 ) that
i +
was used in the proof of Theorem 1. So, we find that the
error 0 at the root b0 is given by |V ‚àó (b0 ) ‚àí V (b0 )| ‚â§ 3
2 +
(1‚àíŒ≥ h‚àí1 )
2

1:
2:
3:
4:
5:
6:

‚â§ 2.

Using the same analysis method in the proof of Theorem
 1, the running time of the algorithm here is
g,U
O h ¬∑ Cf,L
(Œ¥/2)(|A| log |A| + |A||Z|(|S|2 + (|S| +

g,U
1)Cf,L
(Œ¥/2))) . Thus, we get the final result.
The theorem suggests one avenue of fighting the curses of
dimensionality and history (Pineau et al., 2003; Silver &
Veness, 2010): use domain knowledge to find good lower
g,U
and upper bounds, VfL and VgU , so that Cf,L
(Œ¥/2) is small.

The algorithms in the proofs of Theorems 1 and 2 are
designed to prove performance bounds, hence use depthfirst search as the search strategy. Practical algorithms such
as HSVI2 and SARSOP do a trial-based search, where the
current bounds are used to select a path from b0 to one leaf
belief. The advantage of this is that the algorithm can be
greedy with respect to the current best bounds, and this
appears to be useful in practice. However, these algorithms
do not really utilize the other insight of the analysis ‚Äì that
packing can be helpful in getting good performance.
In this section, we describe PGVI. It gets power by using
the idea of building a separate packing of sampled beliefs at
each level of the search tree in the proofs of our theorems.
Such an idea can alleviate the curse of history in both theoretical and practical aspects. First, it controls the number

Covering Number for Efficient Heuristic-based POMDP Planning

of sampled beliefs at each level of search tree so that it does
not grow exponentially in the depth of the tree (subject to
g,U
Cf,L
(Œ¥) being manageably small). Second, it can be used
to spread the sampling areas in the search tree reduced by
heuristics. PGVI achieves this by preferring to sample beliefs at level i which are far away from the beliefs in the
packing of sampled beliefs at level i and that no belief in
its Œ¥-region has performed point-based updates recently.
5.1. PGVI Overview
PGVI is outlined in Algorithms 1 and 2. In these algorithms, we used the packing container to store a set
of Œ¥-packing and the finished container to store finished belief nodes at each level of the search tree in
PGVI. Since PGVI is an extension of SARSOP, they
have common points in selecting actions (Line 5), recursively invoking the EXPLORE function (Lines 15 and
17), and performing point-based updates (Line 22), as
shown in Algorithm 2. We now emphasize some key
differences between SARSOP and PGVI (see Algorithm
2). The first difference is the definition of finished belief nodes. Let excess(b, db , ) = V U (b) ‚àí V L (b) ‚àí
h‚àídb ‚àí1
)
3
‚àí (1‚àíŒ≥ 2
, where h = logŒ≥ (1‚àíŒ≥)
2Rmax . A be2Œ≥ db
lief node b in the packing of sampled beliefs at level
db , packing(db ), is finished if excess(b, db , ) ‚â§ 0. Let
pmin (b) = arg minp‚ààpacking(db ) ||b ‚àí p||. A node b that is
not in the packing(db ) is finished if excess(b, db , ) ‚â§ 0, or
2

||b ‚àí pmin (b)|| ‚â§ (1‚àíŒ≥)
2Œ≥Rmax and pmin (b) is finished. The second difference is the observation selection strategy in Line
6. We define Z UF = {z ‚àà Z|œÑ (b, a‚àó , z) ‚àà
/ finished(db +
1)}. PGVI calculates the distance between œÑ (b, a‚àó , z) and
the Œ¥-packing of sampled beliefs at level db + 1, denoted
dis(œÑ (b, a‚àó , z), packing(db + 1), Œ¥), to use it to spread the
sampling in Rg,U
f,L (b0 ). The new observation selection plays
a key role in PGVI‚Äôs efficient performance. The third difference is the criterion of forward exploration in Lines 7 ‚àº
21. Line 7 is used to check whether all successors of b are
finished, where b is a belief in the packing(db ). If yes, Line
8 is executed to change b‚Äôs status into finished. The belief
p0min in Line 10 is the belief in the packing(db + 1) of sampled beliefs with minimal distance to the belief œÑ (b, a‚àó , z ‚àó ).
Algorithm 2 inserts œÑ (b, a‚àó , z ‚àó ) into the packing at level
db + 1 only if the distance is greater than Œ¥ (see Lines 11 ‚àº
13). If the distance between œÑ (b, a‚àó , z ‚àó ) and p0min is greater
2

than (1‚àíŒ≥)
2Œ≥Rmax , it recursively invokes the EXPLORE function at œÑ (b, a‚àó , z ‚àó ). Otherwise, it checks whether p0min is
finished. If yes, it changes the status of œÑ (b, a‚àó , z ‚àó ) into
finished. If not, it recursively invokes the EXPLORE function at p0min . Overall, Lines 14 ‚àº 20 are used to control the
width of the tree and the frequency of point-based updates
to ensure PGVI‚Äôs polynomial time performance, as stated
later. If PGVI encounters a belief b at level db that is close

to some belief b0 in the packing at level db , it only explores
forward from b0 and terminates the forward search from b.
5.2. Packing-Guided Search
Besides using the set of Œ¥-packing to control the width of
the search tree, PGVI also uses it to guide search based
on the two following principles. First, for beliefs b with at
least Œ¥ distance to the Œ¥-packing of sampled beliefs at level
db of the tree, it prefers to sample beliefs that are far away
from beliefs in packing(db ). This is implemented by setting
dis(b, packing(db ), Œ¥) = minp‚ààpacking(db ) ||b ‚àí p|| when
minp‚ààpacking(db ) ||b ‚àí p|| > Œ¥. Second, for beliefs with at
most Œ¥ distance to the corresponding Œ¥-packing, it biases
sampling beliefs b that pmin (b) has not performed a pointbased update recently. We record the time index of the
last update at pmin (b) and denote it as N (pmin (b)). Let N
be the total number of point-based updates that have been
performed by PGVI. We set dis(b, packing(db ), Œ¥) = œâŒ¥,
where œâ = (N + 1 ‚àí N (pmin (b)))/(N + 1), when
minp‚ààpacking(db ) ||b ‚àí p|| ‚â§ Œ¥. Together, we define
dis(œÑ (b, a‚àó , z), packing(db + 1), Œ¥)

œâŒ¥ if minp‚ààpacking(db +1) ||œÑ (b, a‚àó , z) ‚àí p|| ‚â§ Œ¥
=
minp‚ààpacking(db +1) ||œÑ (b, a‚àó , z) ‚àí p|| otherwise
in Line 6 of Algorithm 2. The dis(b, packing(db ), Œ¥) is
always greater than 0 in our setting. The key innovation
here is to use the packing container to spread the sampling
areas by modifying the observation selection strategy in
HSVI2 and SARSOP to enable much better exploration.
5.3. Convergence
Theorem 3 shows that PGVI(, Œ¥) terminates after performg,U
ing at most h2 Cf,L
(Œ¥/2)|A||Z| point-based updates. Its
proof is a combination of the proof in HSVI2 style algorithm (Smith, 2007) and the proof in Theorem 2.
Theorem 3. Given any constant  > 0 and any initial belief b0 ‚àà B, PGVI(, Œ¥) guarantees V ‚àó (b0 ) ‚àí V œÄ (b0 ) ‚â§
g,U
2 after at most h2 Cf,L
(Œ¥/2)|A||Z| point-based updates,
2

(1‚àíŒ≥) 
L
U
where h = logŒ≥ (1‚àíŒ≥)
2Rmax , Œ¥ = 2Œ≥Rmax , Vf (b) and Vg (b)
are used as initial lower and upper bounds respectively.

Proof. In the proof of Theorem 2, we have shown that if a
point-based update is performed at b when all its children
are finished, then b will also be finished. This shows the
correctness of Lines 7 and 8 in the code.
Now we argue that each trial will switch one unfinished
belief node into finished belief node if b0 is still unfinished. From Algorithm 2 we can see a belief b is inserted into the finished(db ) at the end of each trial. In Line
2 it inserts b in the packing(db ) into finished nodes only

Covering Number for Efficient Heuristic-based POMDP Planning

when excess(b, db , ) ‚â§ 0. In Line 8 it inserts b in the
packing(db ) into finished nodes only when all of its children become finished. In Line 19 it inserts b not in the
packing(db ) into finished nodes only when pmin (b) is finished. So, all nodes b inserted into the finished(db ) satisfy
the condition of finished nodes.
PGVI only searches in Rg,U
f,L (b0 ). From Lines 1 ‚àº 3 and 7
‚àº 8 we can see that it does not search and perform pointbased updates on nodes that satisfy excess(b, db , ) ‚â§ 0.
Since maxa‚ààA QU (b, a) ‚â• V ‚àó (b), from Line 5 we can see
that PGVI does not search towards action branches a that
satisfy QU (b, a) < V ‚àó (b).
Finally, Algorithm 2 only searches the children of belief nodes b in the packing(db ). Thus, there are at
g,U
most Cf,L
(Œ¥/2)|A||Z| unfinished beliefs at each level of
the search tree in PGVI. Totally, there are at most h ¬∑
g,U
Cf,L
(Œ¥/2)|A||Z| unfinished beliefs. Each trial performs at
most h point-based updates at beliefs. So, PGVI converges
g,U
after performing at most h2 ¬∑ Cf,L
(Œ¥/2)|A||Z| updates.
This theorem implies that PGVI converges with a number
of updates that is quadratic polynomial rather than exponential in the planning horizon h, given there is no h‚Äôs
g,U
exponential term hidden in Cf,L
(Œ¥).

6. Experiments
In this section, we compare PGVI with some existing pointbased algorithms in their performance on 65 out of the
68 small benchmark problems from Cassandra‚Äôs POMDP
website1 and 4 larger robotic problems (Ross et al., 2008;
Hsu et al., 2008; Kurniawati et al., 2008; 2011). We
discarded 3 of the 68 problems (1d.noisy, 4√ó4.95 and
bulkhead.A) due to parsing issues. Our experimental
platform is a CPU at 2.40GHz, with 3GB memory. We used
the APPL-0.95 software package2 to implement the PGVI
algorithm, but did not use the MOMDP representation
(Ong et al., 2010). We used Œ±-vectors as lower bounds and
sawtooth representations as upper bounds (Smith, 2007).
Although the convergence proof of PGVI suggests using
2

Œ¥ = (1‚àíŒ≥)
2Œ≥Rmax , the value is not useful for achieving the
best performance in practice because Œ¥ often becomes very
small for problems with large Rmax and Œ≥. We set Œ¥ =
(tmax ‚àí t)Œ¥0 /tmax , where Œ¥0 = 0.5, tmax represents the
upper bound of running time, and t represents the elapsed
time in running PGVI, to make PGVI do the best in the
available time. Given that the value of Œ¥ changes with time,
we use the simpler value of excess(b, db , ) = V U (b) ‚àí
V L (b) ‚àí /Œ≥ db to terminate trials. Theorem 3 still holds
when using the simple one. In PGVI and SARSOP,  is set
to 0.5 √ó [V U (b0 ) ‚àí V L (b0 )] in the beginning of each trial.
1
2

http://www.pomdp.org
http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/

6.1. 65 Small Benchmark Problems
We compared PGVI with HSVI2, SARSOP, and GapMin
on the suite of 65 benchmark problems:
‚Ä¢ PGVI found a near optimal solution (gap smaller
than one unit at the third significant digit) in less than
1,000 seconds for 35 problems. In comparison, HSVI
found 33, SARSOP found 32, and GapMin found 46
(Poupart et al., 2011).
‚Ä¢ Among the 33 problems in Tables 1 and 2 of (Poupart
et al., 2011) with 1,000 seconds limit, PGVI achieved
the highest lower bound on 31 problems, while
GapMin (LP) only on 1 problem. PGVI achieved the
smallest gap on 12 problems, while GapMin (LP) on
19 problems. PGVI achieved smaller gap than HSVI2
on 27 problems and than SARSOP on all 33 problems.
‚Ä¢ Among the 8 problems in Table 3 of (Poupart et al.,
2011) with 50,000 seconds limit, PGVI achieved the
highest lower bound on 6 of the problems and the
smallest gap on 2 of the problems (cit, pentagon).
To summarize, the performance of PGVI is comparable to
HSVI2, SARSOP, and GapMin on these small problems.
For the 35 problems on which PGVI performed well, we
recorded the number of Œ±-vectors (|Œì|), the number of
beliefs expanded (|B s |), the estimated Œ¥-packing number
g,U
of B s (PÃÇf,L
(Œ¥)), with Œ¥ = 10‚àí6 , and the corresponding
computation time (T ime). The linear correlation coefg,U
ficient between PÃÇf,L
(Œ¥) and T ime is as high as 0.987,
while the correlation coefficient between |Œì| and T ime
g,U
is only ‚àí0.045. This suggests that PÃÇf,L
(Œ¥) is a good
indicator of the running time of PGVI.
6.2. Larger Benchmark Problems
We now report PGVI and SARSOP‚Äôs experimental results
on 4 more challenging robotic tasks: FieldVisionRockSample[5,5] (Ross et al., 2008), Tracking (Hsu et al., 2008),
Homecare (Kurniawati et al., 2008) and 3D Navigation
(Kurniawati et al., 2011) (see Table 1). For lack of space,
we present only the comparison with SARSOP in details.
While GapMin variants have good performance on small
benchmarks, the software package3 provided by the authors
is unable to scale up on these larger problems. In general,
SARSOP outperforms HSVI2 (Kurniawati et al., 2008).
For each problem, we ran SARSOP and recorded the gap
it achieved and other performance measurements when
10,000 seconds reached. Then, for all test problems except
3D Navigation, we recorded the time that PGVI needed to
achieve the same gap and other performance measurements
at that time point. For the 3D Navigation problem we chose
a time point to better distinguish PGVI from SARSOP.
3

https://cs.uwaterloo.ca/ ppoupart/

Covering Number for Efficient Heuristic-based POMDP Planning
Table 1. Performance comparison.
Algorithm
Reward
Gap V L (b0 ) V U (b0 )
FieldVisionRockSample[5,5] (|S| = 801, |A| = 5, |Z| = 32)
SARSOP
23.31¬± 0.14
0.47
23.27
23.74
PGVI
23.32¬± 0.14
0.47
23.29
23.76
Tracking (|S| = 9, 248, |A| = 9, |Z| = 1, 264)
SARSOP
14.79¬± 0.14
0.69
14.41
15.10
PGVI
14.76¬± 0.15
0.69
14.43
15.12
Homecare (|S| = 5, 408, |A| = 9, |Z| = 928)
SARSOP
16.97¬± 0.14
3.43
16.55
19.98
PGVI
17.06¬± 0.14
3.43
16.56
19.99
3D Navigation (|S| = 16, 969, |A| = 5, |Z| = 14)
SARSOP
‚àí63¬±
0 754,977
‚àí100 754,877
PGVI
202,665¬±1,859 744,045
16,339 760,383

In Table 1, Column 2 lists the estimated expected total rewards for the computed policies and the 95% confidence
intervals. Each pair of reward and confidence interval was
received over 100,000 simulation runs respectively. Each
simulation was terminated after 100 steps. Columns 3‚àº10
list the gap between V L (b0 ) and V U (b0 ), lower bound
g,U
V L (b0 ), upper bound V U (b0 ), |Œì|, |B s |, PÃÇf,L
(Œ¥) with Œ¥ =
‚àí1
10 , the lower and upper bounds initialization time T0 ,
and the total computation time (including T0 ), respectively.
PGVI found the same gaps (see Column 3) but required
significantly fewer Œ±-vectors, expanded beliefs and computation time than SARSOP (see Columns 6, 7 and 10).
g,U
Compared with SARSOP, PGVI had higher PÃÇf,L
(Œ¥) / |B s |
empirically (see Columns 7 and 8), which captured well the
fact that PGVI prefers to sample beliefs far away from the
sampled set. Since SARSOP and PGVI used the blind policy and FIB methods to initialize bounds, their initialization
time T0 were the same as each other on each problem (see
Column 9). On these larger POMDP problems, PGVI substantially outperformed SARSOP by 3.80 ‚àº 5.78 times in
terms of T ime (see Column 10), and by 3.80 ‚àº 6.80 times
in terms of T ime ‚àí T0 (see Columns 9 and 10).
Figure 1 compares the evolution of the bound gap over time
between PGVI and SARSOP. We have two observations
from it. First, PGVI achieved a smaller gap than SARSOP
over time on each problem. This implies that PGVI is
more efficient. Second, PGVI was risky in consuming
more space in order to store the packing set. As shown on
the 3D Navigation task, PGVI ran out of memory around
6,000 seconds. Here, we leave the space reduction of the
packing set stored in PGVI as a future topic.

7. Conclusion
In this paper, we presented two theoretical results that respectively connect the complexity of approximate POMDP
planning to covering numbers of the search spaces reduced
by the upper bound, and both the lower and upper bounds
of the optimal value function. We designed the novel

|Œì|

|B s |

g,U
PÃÇf,L
(Œ¥)

T0 (s)

T ime (s)

24,289
14,207

10,187
6,473

6,204
5,107

0.2
0.2

9,764
2,570

24,174
7,734

2,240
1,601

1,762
1,365

893
893

9,998
2,294

27,134
7,583

5,242
3,452

2,624
2,338

410
410

9,987
1,819

2
289

38,541
15,687

31,717
13,085

12
12

9,934
1,719

FieldVisionRockSample[5,5]

Tracking
2

0.6

SARSOP
PGVI

1.5

0.5

1

0.4

0.5

0.3
0

5000

10000

0
0

5000

Homecare
5

5

8

10000

3D Navigation

x 10

4.5
7.5

4
3.5

7

3
2.5
0

5000

10000

6.5
0

5000

10000

Figure 1. Evolution of the gap between upper and lower bounds
(y-axis) over running time (x-axis) in PGVI and SARSOP.

PGVI algorithm by using the idea of building a separate
packing at each level of the search tree. The set of packing
is used to alleviate the curse of history by controlling
the width of the search tree and spreading the sampling
areas in the search space reachable under heuristics. Theoretically, PGVI guarantees to find an -optimal solution
g,U
after performing at most h2 Cf,L
(Œ¥/2)|A||Z| point-based
updates. Empirically, PGVI outperformed SARSOP by
3.80 ‚àº 6.80 times on 4 challenging robotic problems; it
also showed a very efficient performance compared with
other state-of-the-art point-based algorithms on 65 small
benchmark problems from Cassandra‚Äôs POMDP website.

Acknowledgements
Z. Zhang is supported in part by MoE ARF grant
MOE2010-T2-2-071. D. Hsu is supported in part by the
National Research Foundation Singapore through the Singapore MIT Alliance for Research and Technology‚Äôs IRG
research program (Subaward Agreement No. 41). W.S. Lee
is supported in part by US Air Force Research Laboratory
under agreement FA2386-12-1-4031.

Covering Number for Efficient Heuristic-based POMDP Planning

References
Bonet, B. and Geffner, H. Solving POMDPs: RTDP-Bel
vs. point-based algorithms. In IJCAI, pp. 1641‚Äì1646,
2009.
GrzesÃÅ, M., Poupart, P., and Hoey, J. Isomorph-free branch
and bound search for finite state controllers. In IJCAI,
pp. 2282‚Äì2290, 2013.
Hauskrecht, M. Value-function approximations for partially observable Markov decision processes. In Journal
of Artificial Intelligence Research, volume 13, pp. 33‚Äì
94, 2000.
Hsiao, K., Kaelbling, L.P., and Lozano-Perez, T. Grasping
POMDPs. In ICRA, pp. 4685‚Äì4692, 2007.
Hsu, D., Lee, W.S., and Rong, N. What makes some
POMDP problems easy to approximate. In NIPS, pp.
689‚Äì696, 2007.
Hsu, D., Lee, W.S., and Rong, N. A point-based POMDP
planner for target tracking. In ICRA, pp. 2644‚Äì2650,
2008.
Kaelbling, L.P., Littman, M.L., and Cassandra, A.R. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1-2):99‚Äì134, 1998.
Kakade, S., Kearns, M., and Langford, J. Exploration in
metric state spaces. In ICML, pp. 306‚Äì312, 2003.
Kurniawati, H., Hsu, D., and Lee, W.S. SARSOP: Efficient point-based POMDP planning by approximating
optimally reachable belief spaces. In RSS, 2008.
Kurniawati, H., Du, Y.Z., Hsu, D., and Lee, W.S. Motion planning under uncertainty for robotic tasks with
long time horizons. International Journal of Robotics
Research, 30(3):308‚Äì323, 2011.
Madani, O., Hanks, S., and Condon, A. On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems. In AAAI,
pp. 541‚Äì548, 1999.
Ong, S.C., Png, S.W., Hsu, D., and Lee, W.S. Planning
under uncertainty for robotic tasks with mixed observability. International Journal of Robotics Research, 29
(8):1053‚Äì1068, 2010.
Pineau, J., Gordon, G., and Thrun, S. Point-based value
iteration: An anytime algorithm for POMDPs. In IJCAI,
pp. 1025‚Äì1032, 2003.
Pineau, J., Gordon, G.J., and Thrun, S. Anytime pointbased approximations for large POMDPs. Journal of
Aritificial Intelligence Research, 27:335‚Äì380, 2006.

Poupart, P., Kim, K.E., and Kim, D. Closing the gap: Improved bounds on optimal POMDP solutions. In ICAPS,
pp. 194‚Äì201, 2011.
Ross, S., Pineau, J., Paquet, S., and Chaib-Draa, B. Online
planning algorithms for POMDPs. Journal of Aritificial
Intelligence Research, 32:663‚Äì704, 2008.
Shani, G., Brafman, R.I., and Shimony, S.E. Forward
search value iteration for POMDPs. In IJCAI, pp. 2619‚Äì
2624, 2007.
Shani, G., Pineau, J., and Kaplow, R. A survey of pointbased POMDP solvers. Autonomous Agents and MultiAgent Systems, 27(1):1‚Äì51, 2013.
Silver, D. and Veness, J. Monte-Carlo planning in large
POMDPs. In NIPS, pp. 2164‚Äì2172, 2010.
Smith, T. Probabilistic planning for robotic exploration.
PhD thesis, Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, 2007.
Smith, T. and Simmons, R. Point-based POMDP algorithms: Improved analysis and implementation. In UAI,
pp. 542‚Äì547, 2005.
Zhang, Z. and Chen, X. FHHOP: A factored hybrid heuristic online planning algorithm for large POMDPs. In UAI,
pp. 934‚Äì943, 2012.
Zhang, Z., Littman, M.L., and Chen, X. Covering number as a complexity measure for POMDP planning and
learning. In AAAI, pp. 1853‚Äì1859, 2012.

