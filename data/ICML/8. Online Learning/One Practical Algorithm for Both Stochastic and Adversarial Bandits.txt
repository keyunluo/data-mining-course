One Practical Algorithm for Both Stochastic and Adversarial Bandits

Yevgeny Seldin
Queensland University of Technology, Brisbane, Australia
Aleksandrs Slivkins
Microsoft Research, New York NY, USA

Abstract
We present an algorithm for multiarmed bandits
that achieves almost optimal performance in both
stochastic and adversarial regimes without prior
knowledge about the nature of the environment.
Our algorithm is based on augmentation of the
EXP3 algorithm with a new control lever in the
form of exploration parameters that are tailored
individually for each arm. The algorithm simultaneously applies the “old” control lever, the
learning rate, to control the regret in the adversarial regime and the new control lever to detect
and exploit gaps between the arm losses. This
secures problem-dependent “logarithmic” regret
when gaps are present without compromising on
the worst-case performance guarantee in the adversarial regime. We show that the algorithm can
exploit both the usual expected gaps between the
arm losses in the stochastic regime and deterministic gaps between the arm losses in the adversarial regime. The algorithm retains “logarithmic” regret guarantee in the stochastic regime
even when some observations are contaminated
by an adversary, as long as on average the contamination does not reduce the gap by more than
a half. Our results for the stochastic regime are
supported by experimental validation.

YEVGENY. SELDIN @ GMAIL . COM

SLIVKINS @ MICROSOFT. COM

gorithms for adversarial bandits are unable to exploit the
simpler regime of stochastic bandits. The recent attempt of
Bubeck & Slivkins (2012) to bring them together did not
make it in the full sense of unification, since the algorithm
of Bubeck and Slivkins relies on the knowledge of time
horizon and makes a one-time irreversible switch between
stochastic and adversarial operation modes if the beginning
of the game is estimated to exhibit adversarial behavior.
We present an algorithm that treats both stochastic and adversarial multiarmed bandit problems without distinguishing between them. Our algorithm “just runs”, as most other
bandit algorithms, without knowledge of time horizon and
without making any hard statements about the nature of the
environment. We show that if the environment happens to
be adversarial the performance of the algorithm is just a
factor of 2 worse than the performance of the EXP3 algorithm (with the best constants, as described in Bubeck &
Cesa-Bianchi (2012)) and if the environment happens to be
stochastic the performance of our algorithm is comparable
to the performance of UCB1 of Auer et al. (2002a). Thus,
we cover the full range and achieve almost optimal performance at the extreme points.

Stochastic multiarmed bandits (Thompson, 1933; Robbins,
1952; Lai & Robbins, 1985; Auer et al., 2002a) and adversarial multiarmed bandits (Auer et al., 1995; 2002b) have
co-existed in parallel for almost two decades by now, in
the sense that no algorithm for stochastic multiarmed bandits is applicable to adversarial multiarmed bandits and al-

Furthermore, we show that the new algorithm can exploit
both the usual expected gaps between the arm losses in
the stochastic regime and deterministic gaps between the
arm losses in the adversarial regime. We also show that
the algorithm retains “logarithmic” regret guarantee in the
stochastic regime even when some observations are adversarially contaminated, as long as on average the contamination does not reduce the gap by more than a half. To
the best of our knowledge, no other algorithm has been yet
shown to be able to exploit gaps in the adversarial or adversarially contaminated stochastic regimes. The contaminated stochastic regime is a very practical model, since in
many real-life situations we are dealing with stochastic environments with occasional disturbances.

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

Since the introduction of Thompson’s sampling (Thompson, 1933) (which was analyzed only after 80 years (Kaufmann et al., 2012; Agrawal & Goyal, 2013)) a variety of al-

1. Introduction

One Practical Algorithm for Both Stochastic and Adversarial Bandits

gorithms were invented for the stochastic multiarmed bandit problem. The most powerful for today are KL-UCB
(Cappé et al., 2013), EwS (Maillard, 2011), and the aforementioned Thompson’s sampling. It is easy to show that
any deterministic algorithm can potentially suffer linear regret in the adversarial regime (see the supplementary material for a proof). Although nothing is known about the performance of randomized algorithms for stochastic bandits
in the adversarial regimes, empirically they are extremely
sensitive to deviations from the stochastic assumption.
In the adversarial world the most powerful algorithm for
today is INF (Audibert & Bubeck, 2009; Bubeck & CesaBianchi, 2012). Nevertheless, the EXP3 algorithm of Auer
et al. (2002b) still retains an important place, mainly due to
its simplicity and wide applicability, which covers combinatorial bandits, partial monitoring games, and many other
adversarial problems. Since any stochastic problem can be
seen as an instance of an adversarial problem, both INF and
EXP3 have the worst-case “root-t” regret guarantee in the
stochastic regime, but it is not known whether they can do
better. Empirically in the stochastic regime EXP3 is inferior
to all other known algorithms for this setting, including the
simplest UCB1 algorithm.
It is interesting to take a brief look into the development
of EXP3. The algorithm was first suggested in Auer et al.
(1995) and its parametrization and analysis were improved
in Auer et al. (2002b). The EXP3 of Auer et. al. was designed for the multiarmed bandit game with rewards and
its playing strategy is based on mixing Gibbs distribution
(also known as “exponential weights”) with a uniform exploration distribution in proportion to the learning rate. The
uniform exploration leaves no hope for achieving “logarithmic” regret in the stochastic regime simultaneously with
the “root-t” regret in√
the adversarial regime, since each arm
is played at least Ω( t) times in t rounds of the game. By
changing the learning rate Cesa-Bianchi & Fischer (1998)
managed to derive a different parametrization of the algorithm that was shown to achieve “logarithmic” regret in
the stochastic regime, but it had no regret guarantees in
the adversarial regime. Stoltz (2005) has observed that in
the game with losses the “root-t” regret guarantee in the
adversarial regime can be achieved without mixing in the
uniform distribution (and even lead to better constants).1
However, mixing in any distribution that element-wise does
not exceed the learning rate does not break the worst-case
performance of the algorithm in the game with losses. We
exploit this emerged freedom in order to derive a modification of the EXP3 algorithm that achieves almost optimal
regret in both adversarial and stochastic regimes without
prior knowledge about the nature of the environment.
1

Rewards can be transformed into losses by taking ` = 1 − r.

2. Problem Setting
We study the multiarmed bandit (MAB) game with losses.
In each round t of the game the algorithm chooses one action At among K possible actions, a.k.a. arms, and obt
serves the corresponding loss `A
t . The losses of other arms
are not observed. There is a large number of loss generation
models, four of which are considered below. In this work
we restrict ourselves to loss sequences {`at }t,a that are generated independently of the algorithm’s actions. Under this
assumption we can assume that the loss sequences are written down before the game starts (but not revealed to the
algorithm). We also make a standard assumption that the
losses are bounded in the [0, 1] interval.
The performance of the algorithm is quantified by regret,
defined as the difference between the expected loss of the
algorithm up to round t and the expected loss of the best
arm up to round t:
( " t
#)
t
X
X
 As 
a
R(t) =
E `s − min E
`s
.
a

s=1

s=1

The expectation is taken over the possible randomness of
the algorithm and loss generation model. The goal of the
algorithm is to minimize the regret.
We consider two standard loss generation models, the adversarial regime and the stochastic regime and two intermediate regimes, the contaminated stochastic regime and
the adversarial regime with a gap.
Adversarial regime. In this regime the loss sequences are
generated by an unrestricted adversary (who is oblivious to
the algorithm’s actions). This is the most general setting
and the other three regimes can be seen as special
P cases 0of

t
a
the adversarial regime. An arm a ∈ arg mina0
s=1 `s
is known as a best arm in hindsight for the first t rounds.
Stochastic regime. In this regime the losses `at are sampled independently from an unknown distribution that depends on a, but not on t. We use µ(a) = E [`at ] to denote the expected loss of arm a. Arm a is called a best
arm if µ(a) = mina0 {µ(a0 )} and suboptimal otherwise;
let a∗ denote some best arm. For each arm a, define the
gap ∆(a) = µ(a) − µ(a∗ ). Let ∆ = mina:∆(a)>0 {∆(a)}
denote the minimal gap.
Letting Nt (a) be the number of times arm a was played up
to (and including) round t, the regret can be rewritten as
X
R(t) =
E [Nt (a)] ∆(a).
(1)
a

Contaminated stochastic regime. In this regime the adversary picks some round-arm pairs (t, a) (“locations”) before the game starts and assigns the loss values there in an

One Practical Algorithm for Both Stochastic and Adversarial Bandits

arbitrary way. The remaining losses are generated according to the stochastic regime.
We call a contaminated stochastic regime moderately contaminated after τ rounds if for all t ≥ τ the total number of
contaminated locations of each suboptimal arm up to time
t is at most t∆(a)/4 and the number of contaminated locations of each best arm is at most t∆/4. By this definition,
for all t ≥ τ on average (over stochasticity of the loss sequences) the adversary can reduce the gap of every arm by
at most a half.
Adversarial regime with a gap. An adversarial regime
is named by us an adversarial regime with a gap if there
exists a round τ and an arm a∗τ that persists to be the best
arm in hindsight for all rounds t ≥ τ . We name such arm
a consistently best arm after round τ . If no such arm exists
then a∗τ is undefined. Note that if a∗τ is defined for some τ
then a∗τ 0 is defined for all τ 0 > τ .
Pt
We use λt (a) = s=1 `as to denote the cumulative loss of
arm a. Whenever a∗τ is defined we define a deterministic
gap of arm a on round τ as:


1
∗
(λt (a) − λt (aτ )) .
∆(τ, a) = min
t≥τ
t

ing rate ηt and the exploration parameters ξt (a). The
EXP3 with losses (as described in Bubeck & Cesa-Bianchi
(2012)) is a special case of the EXP3++ with ηt = 2βt and
ξt (a) = 0.
The crucial innovation in EXP3++ is the introduction of exploration parameters ξt (a), which are tuned individually
for each arm depending on the past observations. In the
sequel we show that tuning only the learning rate ηt suffices to control the regret of EXP3++ in the adversarial
regime, irrespective of the choice of the exploration parameters ξt (a). Then we show that tuning only the exploration parameters ξt (a) suffices to control the regret of
EXP3++ in the stochastic regime irrespective of the choice
of ηt , as long as ηt ≥ βt . Applying the two control
levers simultaneously we obtain an algorithm that achieves
the optimal “root-t” regret in the adversarial regime (up
to logarithmic factors) and almost optimal “logarithmic”
regret in the stochastic regime (though with a suboptimal
power in the logarithm). Then show that the new control
lever is even more powerful and allows to detect and exploit the gap in even more challenging situations, including
moderately contaminated stochastic regime and adversarial
regime with a gap.

If a∗τ is undefined, ∆(τ, a) is defined as zero.

Adversarial Regime

Notation. We use 1{E} to denote the indicator function of
event E and 1at = 1{At =a} to denote the indicator function
of the event that arm a was played on round t.

First, we show tuning ηt is sufficient to control the regret
of EXP3++ in the adversarial regime.

3. Main Results
Our main results include a new algorithm, which we name
EXP3++, and its analysis in the four regimes defined in the
previous section. The EXP3++ algorithm, provided in Algorithm 1 box, is a generalization of the EXP3 algorithm
with losses.
Algorithm 1 Algorithm EXP3++.
Remark: See text for definition of ηt and ξt (a).
∀a: L̃0 (a) = 0.
for t = 1,q
2, ... do
βt =

1
2

ln K
tK .

 1
	
∀a: εt (a) = min 2K
, βt , ξt (a) .
 P −η L̃ (a0 )
t−1 (a)
t t−1
∀a: ρt (a) = e−ηt L̃P
.
a0 e
0
∀a: ρ̃t (a) = (1 − a0 εt (a )) ρt (a) + εt (a).
Draw action At according to ρ̃t and play it.
t
Observe and suffer the loss `A
t .
At
`
∀a : `˜a = t 1a .
t

ρ̃t (a)

t

∀a : L̃t (a) = L̃t−1 (a) + `˜at .
end for
The EXP3++ algorithm has two control levers: the learn-

Theorem 1. For ηt = βt and any ξt (a) ≥ 0 the regret of
EXP3++ for any t satisfies:
√
R(t) ≤ 4 Kt ln K.
Note that the regret bound in Theorem 1 is just a factor
of 2 worse than the regret of EXP3 with losses (Bubeck &
Cesa-Bianchi, 2012).
Stochastic Regime
Now we show that for any ηt ≥ βt tuning the exploration
parameters ξt (a) suffices to control the regret of the algorithm in the stochastic regime. By choosing ηt = βt we
obtain algorithms that have both the optimal “root-t” regret
scaling in the adversarial regime and “logarithmic” regret
scaling in the stochastic regime.
We consider a number of different ways of tuning the
exploration parameters ξt (a), which lead to different
parametrizations of EXP3++. We start with an idealistic
assumption that the gap is known, just to give an idea of
what is the best result we can hope for.
Theorem 2. Assume that the gaps ∆(a) are known. For
any choice of ηt ≥ βt and any c ≥ 18, the regret of
2
)
EXP3++ with ξt (a) = c ln(t∆(a)
in the stochastic regime
t∆(a)2

One Practical Algorithm for Both Stochastic and Adversarial Bandits

satisfies:
R(t) ≤

X
a


O

2

ln(t)
∆(a)


+

X
a


Õ

K
∆(a)3


.

The constants in this theorem are small and are provided
explicitly in the analysis. We also show that c can be made
almost as small as 2.
Next we show that using the empirical gap as an estimate
of the true gap



1
0
ˆ
L̃t (a) − min
L̃t (a )
(2)
∆t (a) = min 1,
a0
t
we can also achieve polylogarithmic regret guarantee. We
call this algorithm EXP3++AVG .
Theorem 3. Let c ≥ 18 and ηt ≥ βt . Let t∗ be the
2
ln(t∗ )4
and let
minimal integer that satisfies t∗ ≥ 4c K
ln(K)
n l
mo
2
t∗ (a) = max t∗ , e1/∆(a)
. The regret of EXP3++
c(ln t)2
ˆ
t∆t−1 (a)2

AVG

with ξt (a) =
(termed EXP3++ ) in the
stochastic regime satisfies:
X  ln(t)3  X
R(t) ≤
O
+
∆(a)t∗ (a).
∆(a)
a
a
Although the additive constants t∗ (a) in this theorem are
very large, in the experimental section we show that a minor modification of this algorithm performs comparably to
UCB1 in the stochastic regime (and has the adversarial regret guarantee in addition).
In the following theorem we show that if we assume a
known time horizon T , then we can eliminate the additive
2
term e1/∆(a) in the regret bound. The algorithm in Theorem 4 replaces the empirical gap estimate (2) in the definition of ξt (a) with a lower confidence bound on the gap
and slightly adjusts other terms. We name this algorithm
EXP3++LCBT .
Theorem 4. Consider the stochastic regime with a known
time horizon T . The EXP3++LCBT algorithm with any ηt ≥
βt and appropriately defined ξt (a) achieves regret R(T ) ≤
O(log3 T )
.
∆3
The precise definition of EXP3++LCBT and the proof of Theorem 4 are provided in the supplementary material. It
seems that simultaneous elimination of the assumption on
the known time horizon and the exponentially large additive term is a very challenging problem and we defer it for
future work.

Theorem 5. Under n
the parametrization
l
mo given in Theorem
2
3, for t∗ (a) = max t∗ , e4/∆(a)
, where t∗ is defined
as before, the regret of EXP3++AVG in the stochastic regime
that is moderately contaminated after τ rounds satisfies:
X  ln(t)3  X
R(t) ≤
O
+
max {t∗ (a), τ } .
∆(a)
a
a
The price that is paid for moderate contamination after τ
rounds is the scaling of ∆(a) by a factor of 1/2 and the
additive factor of τ . (The scaling
of∆ affects the definition

3
of t∗ and the constant in O ln(t)
∆(a) .) As before, the regret
guarantee of Theorem 5 comes in addition to the guarantee
of Theorem 1.
Adversarial Regime with a Gap
Finally, we show that EXP3++AVG can also take advantage
of deterministic gap in the adversarial regime.
Theorem 6. Under the parametrization given in Theorem
3, the regret of EXP3++AVG in the adversarial regime satisfies:
R(t) ≤



n
o
X
2
ln(t)3
.
min max t∗ , τ, e1/(∆(τ,a)) + O
τ
∆(τ, a)
a
We remind the reader that in the absence of consistently
best arm ∆(τ, a) is defined as zero and the regret bound
is vacuous (but the regret bound of Theorem 1 still holds).
We also note that ∆(τ, a) is a non-decreasing function of
τ . Therefore, there is a trade-off: increasing τ increases
∆(τ, a), but loses the regret guarantee on the rounds before τ (for simplicity, we assume that we have no guarantees before τ ). Theorem 6 allows to pick τ that minimizes
this trade-off. An important implication of the theorem is
that if the deterministic gap is growing with time the regret
guarantee improves too.

4. Proofs
We prove the theorems from the previous section in the order they were presented.
The Adversarial Regime

Contaminated Stochastic Regime

The proof of Theorem 1 relies on the following lemma,
which is an intermediate step in the analysis of EXP3 by
Bubeck (2010) (see also Bubeck & Cesa-Bianchi (2012)).

Next we show that EXP3++AVG can sustain moderate contamination in the stochastic regime without a significant
deterioration in performance.

Lemma 7. For any K sequences of non-negative numbers X1a , X2a , . . . indexed by a ∈ {1, . . . , K} and any
non-increasing positive sequence η1 , η2 , . . . , for ρt (a) =

One Practical Algorithm for Both Stochastic and Adversarial Bandits
Pt−1

a
s=1 Xs
Pt−1 a
−ηt s=1 Xs

exp(−ηt

)

(assuming for t = 1 the sum in the
)
exponent is zero) we have:
P

PT P
T
a
a
t=1
a ρt (a)Xt − mina
t=1 Xt
P

h0

exp(

≤

T
1X

2

ηt

X

2

ρt (a) (Xta ) +

a

t=1

ln K
.
ηT

(3)

a

"
≤E

T
X
ηt
t=1

2

t=1

"
Et

##
X

2
ρt (a) (Xta )

ln K
.
ηT

+

a

a
˜a
Proof of Theorem 1. We associate
h Xit in (3) with `t in the
EXP3++ algorithm. We have Et `˜at = `at and since

ρt (a) =

1
1−

εt (a0 )

P

a0

(ρ̃t (a) − εt (a)) ≥ ρ̃t (a) − εt (a)

and `at ∈ [0, 1] we also have:
"
#
"
#
X
X
a
a
˜
Et
ρt (a)` ≥ Et
(ρ̃t (a) − εt (a)) `
t

t

a

a

Our proofs are based on the following form of Bernstein’s inequality, which is a minor improvement over
Cesa-Bianchi & Lugosi (2006, Lemma A.8) based on the
ideas from Boucheron et al. (2013, Theorem 2.10).
Theorem 9 (Bernstein’s inequality for martingales). Let
X1 , . . . , Xn be a martingale difference sequencePwith rei
spect to filtration F = (Fi )1≤i≤n and let Si = j=1 Xj
be the associated martingale. Assume that there exist positive numbers ν and c, such
that
 Xj i≤ c for all j with probh
Pn
2
ability 1 and i=1 E (Xi ) Fi−1 ≤ ν with probability
1. Then for all b > 0:


√
cb
≤ e−b .
P Sn > 2νb +
3
We are also using the following technical lemma, which is
proved in the supplementary material.
√

P∞
Lemma 10. For any c > 0: t=0 e−c t = O c22 .
The proof of Theorems 2 and 3 is based on the following
lemma.
∞
Lemma 11. Let {εt (a)}t=1 be non-increasing deterministic sequences, such that εt (a) ≤ εt (a) with probability
1Pand εt (a) ≤ εt (a∗ ) for all t and a. Define νt (a) =
t
1
a
s=1 εs (a) and define the event Et


t∆(a) − L̃t (a) − L̃t (a∗ )
≤

As well, we have:

Et


#
 2
X
ρt (a) `˜at
= Et 
ρt (a)

a

a

"
≤ Et
=

!2 


(1 −

p
1.25bt
2 (νt (a) + νt (a∗ )) bt +
.
3εt (a∗ )

E [Nt (a)] ≤ (t∗ − 1) +

ρt (a)
≤ 2K,
0 )) ρ (a) + ε (a)
ε
(a
t
t
a0 t

whereP the last inequality follows by the fact that
(1 − a0 εt (a0 )) ≥ 12 by the definition of εt (a). Substi-

t
X

e−bs +

s=t∗

#

P

(Eta )

Then for any positive sequence b1 , b2 , . . . and any t∗ ≥ 2
the number of times arm a is played by EXP3++ up to round
t is bounded as:

X ρt (a)
X ρt (a)
1at =
2
ρ̃t (a)
ρ̃t (a)
a
a

X
a

t
`A
t
1a
ρ̃t (a) t

X
ln K
ln K X
+
εt (a) ≤ 2K
ηt +
.
ηT
ηT
a
t=1

The Stochastic Regime

a

X

t=1
T

ηt +

t=1

h i X
t
−
εt (a).
≥ Et `A
t

"

T
X

The result of the theorem follows by the choice of ηt .

Corollary 8. Let X1a , X2a , . . . for a ∈ {1, . . . , K} be nonnegative random variables and let ηt and ρt as defined in
Lemma 7. Then:
" T
"
##
" T
#!
X
X
X
a
a
E
Et
ρt (a)Xt
− min E
Et [Xt ]
a

a

t=1

≤K

More precisely, we are using the following corollary, which
follows by allowing Xta -s to be random variables and taking expectations of the two sides of (3) and using the fact
that E [min[·]] ≤ min [E [·]]. We decompose expectations
of incremental sums into incremental sums of conditional
expectations and use Et [·] to denote expectations conditioned on realization of all random variables up to round
t.

t=1

tution of the above calculations into Corollary 8 yields:
" T
#
" T
#
X
X
At
a
R(t) = E
`t − min E
`t

+

t
X

t
X

εs (a)1{Eta }

s=t∗

e−ηs gs−1 (a) ,

s=t∗

where
s
gt (a) = t∆(a) −

2tbt



1
1
+
εt (a) εt (a∗ )


−

1.25bt
.
3εt (a∗ )

One Practical Algorithm for Both Stochastic and Adversarial Bandits

Proof. Note
ofo
the martingale difference sen that elements

∞
a
a∗
˜
˜
quence ∆(a) − `t − `t
are upper bounded by
1
εt (a∗ )

t=1

+ 1. Since εt (a∗ ) ≤ εt (a∗ ) ≤ 1/(2K) ≤ 1/4 we

can simplify the upper bound by using
Further note that

t
2 

X
a
a∗
˜
˜
Es ∆(a) − `s − `s

1
εt (a∗ )

+1 ≤

1.25
εt (a∗ ) .

s=1

≤

=

t
X


Es

s=1
t 
X

∗
`˜as − `˜as

2 

  
  
2
∗ 2
a
˜
Es `s
+ Es `˜as

s=1

 X

t 
t 
X
1
1
1
1
+
≤
+
≤
p̃s (a) p̃s (a∗ )
εs (a) εs (a∗ )
s=1
s=1

t 
X
1
1
+
= νt (a) + νt (a∗ )
≤
∗)
ε
(a)
ε
(a
s
s
s=1
of event
with probability 1. Let E denote thecomplement

E. Then by Bernstein’s inequality P Eta ≤ bt . The number of times arm a is played up to round t is bounded as:
E [Nt (a)] =
=

t
X
s=1
t
X

Proof of Theorem 2. The proof is based on Lemma 11. Let
bt = ln(t∆(a)2 ) and εt (a) = εt (a). For any c ≥ 18
and any t ≥ t∗ , where t∗ is the minimal integer for which
2
K ln(t∗ ∆(a)2 )2
, we have:
t∗ ≥ 4c ∆(a)
4 ln(K)
s


1
1
1.25bt
gt (a) = t∆(a) − 2tbt
+
−
∗
εt (a) εt (a )
3εt (a∗ )
s
tbt
1.25bt
≥ t∆(a) − 2
−
εt (a) 3εt (a)


2
1
1.25
= t∆(a) 1 − √ −
≥ t∆(a).
3c
2
c
(The choice of t∗ ensures that for all suboptimal actions a
we have εt (a) = ξt (a), which slightly simplifies
 1the cal	
, βt ,
culations. Also note that since εt (a∗ ) = min 2K
asymptotically 1/εt (a) term in gt (a) dominates 1/εt (a∗ )
term and with a bit more careful bounding c can be made
almost as small as 2.) By substitution of the lower bound
on gt (a) into Lemma 11 we have:
c ln(t)2
ln(t)
+
2
∆(a)
∆(a)2

q
t 
X
∆(a)
(s−1) ln(K)
K
+
e− 4

E [Nt (a)] ≤ t∗ +

s=1

P [As = a]
 a   a 

P As = aEs−1
P Es−1

s=1



  a 
a
+ P As = aEs−1
P Es−1
t
X
 a 
 a 

1{E a } + P Es−1
≤
P As = aEs−1
s−1
s=1
t
X
 a 

P As = aEs−1
1{E a } + e−bs−1 .
≤
s−1
s=1

For the terms of the sum above we have:
 a 

P At = aEt−1
1{E a } = ρ̃t (a)1{E a }
s−1
s−1
≤ (ρt (a) + εt (a)) 1{E a }
s−1
!
−ηt L̃t−1
e
(a)
1{E a }
= εt (a) + P
−ηt L̃t−1 (a0 )
s−1
a0 e


∗
≤ εt (a) + e−ηt (L̃t−1 (a)−L̃t−1 (a )) 1{E a }
s−1
≤ εt (a)1{E a } + e−ηt gt−1 (a) ,
s−1
Where in the last inequality we used the facts that event
Eta holds and that since εt (a) is a non-increasing sequence
t
νt (a) ≤ ε (a)
. Substitution of this result back into the comt
putation of E [Nt (a)] completes the proof.

c ln(t)2
ln(t)
≤
+
+O
2
∆(a)
∆(a)2



K
∆(a)2



+ t∗ ,

where we used Lemma 10 to bound
 thesum of the expoK
∗
.
nents. Note that t is of order Õ ∆(a)
4
Proof of Theorem 3. Note that since by our defiˆ
nition
1 the sequence εt (a) = εt =
n ∆t (a) ≤ 2 o
c ln(t)
1
min 2K , βt , t
satisfies the condition of
Lemma 11. Also note that for t large enough, so
2
2
ln(t)4
that t ≥ 4c K
, we have εt = c ln(t)
. Let bt = ln(t)
ln K
t
and let t∗ be large enough, so that for all t ≥ t∗ we
1
2
ln(t)4
have t ≥ 4c K
and t ≥ e ∆(a)2 . We are going
ln K
to bound the three terms
Pt in the bound on E [Nt (a)] in
Lemma 11. Bounding s=t∗ e−bs is easy. For bounding
Pt
a
a
s=t∗ εs (a)1{Es−1
} we note that when Et holds and
c ≥ 18 we have:




ˆ t (a) ≥ 1 L̃t (a) − min L̃t (a0 ) ≥ 1 L̃t (a) − L̃t (a∗ )
∆
a0
t
t
s
!
1
1
tbt
1.25bt
≥ gt (a) =
t∆(a) − 2
−
(4)
t
t
εt
3εt
!
1
2t
1.25t
=
t∆(a) − p
−
t
c ln(t) 3c ln t


2
1.25
≥ ∆(a) 1 − √ −
≥ 12 ∆(a),
3c
c

One Practical Algorithm for Both Stochastic and Adversarial Bandits

where in (4) we used the fact that Eta holds and in√the last
line we used the fact that for t ≥ t∗ we have ln t ≥
1/∆(a). Thus

Proof. Again, the only modification we need is a high∗
probability lower bound
 on L̃t (a) − L̃t (aτ ). We note that

4c2 (ln t)2
c(ln t)2
≤
εt 1{E a } ≤
s−1
ˆ t (a)2
t∆(a)2
t∆

that by definition for t ≥ τ we have (λt (a) − λt (a∗τ )) ≥
t∆(τ, a). Define the events Wta :



Pt

ln(t)3
∆(a)2



and s=t∗ εs 1{E a } = O
. Finally, for the last
s−1
term in Lemma 11 we have already shown as an intermeˆ t (a) that for
diate step in the calculation of the bound on ∆
1
t ≥ t∗ we have
g
(a)
≥
∆(a).
Therefore,
the last term
2
 t 

K
is of order O ∆(a)
. By taking all these calculations to2
gether we obtain the result of the theorem. Note that the
result holds for any ηt ≥ βt .

The Contaminated Stochastic Regime
Proof of Theorem 5. The key element of the previous proof
was a high-probability lower bound on L̃t (a) − L̃t (a∗ ).
We show that we can obtain a similar lower bound in the
contaminated setting too. Let 1×
t,a denote the indicator
function of contamination in “location” (t, a) (1×
t,a takes
value 1 if contamination occurred
and
0
otherwise).
Let

×
a
+
1
−
1
µ(a),
in
other
words,
if
either
`
mat = 1×
t,a
t,a t
a was contaminated on round t then mat is the adversarially assigned value of the loss of arm a on round
Pt t and
otherwise it is the expectedloss. Let Mt (a) = s=1 mas
then (Mt (a) − Mt (a∗ )) − L̃t (a) − L̃t (a∗ ) is a martingale. By definition of moderately contaminated after τ
rounds process, for t ≥ τ and any suboptimal action
a the total number of rounds up to t where either a itself or a∗ were contaminated is at most t∆(a)/2. Therefore, Mt (a) − Mt (a∗ ) ≥ (t − t∆(a)/2) ∆(a) − t∆/2 ≥
t∆(a)/2. Define event Bta :

p
t∆(a)
1.25bt
− L̃t (a) − L̃t (a∗ ) ≤ 2 νt bt +
, (Bta )
2
3εt


where
 νt =
Pt ε1t is defined in the proof of Theorem 3 and
a ≤ b .
.
Then
by
Bernstein’s
inequality
P
B
t
t
s=1 εs
The remainder of the proof is identical to the proof of Theorem 3 with ∆(a) replaced by ∆(a)/2.
The Adversarial Regime with a Gap
The proof of Theorem 6 is based on the following lemma,
which is an analogue of Theorems 3 and 5.
Lemma 12. Under the parametrization given in Theorem
3, the number of times a suboptimal arm a is played by
EXP3++AVG in an adversarial regime with a gap satisfies:


n
o
ln(t)3
∗
1/(∆(τ,a))2
E [Nt (a)] ≤ max t , τ, e
+O
.
∆(τ, a)2

(λt (a) − λt (a∗τ )) − L̃t (a) − L̃t (a∗τ ) is a martingale and



p
1.25bt
, (Wta )
t∆(τ, a)− L̃t (a) − L̃t (a∗τ ) ≤ 2 νt bt +
3εt
where εt and νt are as inthe proof of Theorem 5. By Bernstein’s inequality P Wta ≤ bt . The remainder of the proof
is identical to the proof of Theorem 3.
Proof of Theorem 6. Note that by definition ∆(τ, a) is a
non-decreasing sequence of τ . Since Lemma 12 is a deterministic result it holds for all τ simultaneously and we
are free to choose the one that minimizes the bound.

5. Empirical Evaluation: Stochastic Regime
We consider the stochastic multiarmed bandit problem with
Bernoulli rewards. For all the suboptimal arms the rewards
are Bernoulli with bias 0.5 and for the single best arm the
reward is Bernoulli with bias 0.5 + ∆. We run the experiments with K = 2, K = 10, and K = 100, and ∆ = 0.1
and ∆ = 0.01 (in total, six combinations of K and ∆). We
run each game for 107 rounds and make ten repetitions of
each experiment. The solid lines in the graphs in Figure 1
represent the mean performance over the experiments and
the dashed lines represent the mean plus one standard deviation (std) over the ten repetitions of the corresponding
experiment.
In the experiments EXP3++ is parametrized by ξt (a) =
ˆ t (a)2 )
ln(t∆
ˆ t (a) is the empirical estimate of ∆(a)
, where ∆
ˆ t (a)2
32t∆

defined in (2). In order to demonstrate that in the stochastic
regime the exploration parameters are in full control of the
performance we run the EXP3++ algorithm with two different learning rates. EXP3++EMP corresponds to ηt = βt
and EXP3++ACC corresponds to ηt = 1. Note that only the
EXP3++EMP has a performance guarantee in the adversarial
regime.
We compare EXP3++ algorithm with the EXP3 algorithm
(as described in Bubeck & Cesa-Bianchi (2012)), the UCB1
algorithm of Auer et al. (2002a), and Thompson’s sampling. Since it was demonstrated empirically in Seldin et al.
(2013) that in the above experiments the performance of
Thompson sampling is comparable or superior to the performance of EwS and KL-UCB, the latter two algorithms are
excluded from the comparison. For the EXP3++ and the
EXP3 algorithms we transform the rewards into losses via
`at = 1 − rta transformation, other algorithms operate directly on the rewards.

One Practical Algorithm for Both Stochastic and Adversarial Bandits
K = 2. ∆ = 0.1

K = 10. ∆ = 0.1

700

K = 100. ∆ = 0.1

4

5000

3.5

600

x 10

3

400
300
200

Cumulative Regret

Cumulative Regret

Cumulative Regret

4000
500

3000

2000

2.5
2
1.5
1

1000
100

0.5
2

4

6

8

t

0
0

10

2

6

8

t

(a) K = 2, ∆ = 0.1
K = 2. ∆ = 0.01

2

x 10

Cumulative Regret

1500
1000

8

10
6

x 10

(c) K = 100, ∆ = 0.1
K = 100. ∆ = 0.01

4

10

x 10

8

3
2.5
2
1.5
1

500

6
t

3.5

2000

4

6

K = 10. ∆ = 0.01

4

4

2500

0
0

0
0

10
x 10

(b) K = 10, ∆ = 0.1

3000

Cumulative Regret

4

6

x 10

Cumulative Regret

0
0

UCB
Thom
EXP3
EXP3++EMP

6

ACC

EXP3++

4

2

0.5
2

4

6
t

(d) K = 2, ∆ = 0.01

8

10

0
0

2

4

6
t

6

x 10

8

10

0
0

6

x 10

(e) K = 10, ∆ = 0.01

2

4

6
t

8

10
6

x 10

(f) K = 100, ∆ = 0.01

Figure 1. Comparison of UCB1, Thompson sampling (“Thom”), EXP3, and EXP3++ algorithms in the stochastic regime. The legend
in figure (f) corresponds to all the figures. EXP3++EMP is the Empirical EXP3++ algorithm and EXP3++ACC is an Accelerated Empirical
EXP3++, where we take ηt = 1. Solid lines correspond to means over 10 repetitions of the corresponding experiments and dashed lines
correspond to the means plus one standard deviation.

The results are presented in Figure 1. We see that in all the
experiments the performance of EXP3++EMP is almost identical to the performance of UCB1. However, unlike UCB1
and Thompson’s sampling, EXP3++EMP is secured against
the possibility that the game is controlled by an adversary.
In the supplementary material we show that any deterministic algorithm is vulnerable against an adversary.
The EXP3++ACC algorithm can be seen as a teaser for future
work. It performs better than EXP3++EMP , but it does not
have the adversarial regime performance guarantee. However, we do not exclude the possibility that by some more
sophisticated simultaneous control of ηt and εt (a)-s it may
be possible to design an algorithm that will have both better
performance in the stochastic regime and a regret guarantee in the adversarial regime. An example of such sophisticated control of the learning rate in the full information
games can be found in de Rooij et al. (2014).

6. Discussion
We presented a generalization of the EXP3 algorithm, the
EXP3++ algorithm, which augments the EXP3 algorithm
with a new control lever in the form exploration parameters εt (a) that are tuned individually for each arm. We have

shown that the new control lever is extremely useful in detecting and exploiting the gap in a wide range of regimes,
while the old control lever always keeps the worst-case performance of the algorithm under control. Due to the central role of the EXP3 algorithm in the adversarial analysis
that stretches far beyond the adversarial bandits and due
to the simplicity of our generalization we believe that our
result will lead to a multitude of new algorithms for other
problems that exploit the gaps without compromising on
the worst-case performance guarantees. There is also room
for further improvement of the presented technique that we
plan to pursue in future work.

Acknowledgments
The authors would like to thank Sébastien Bubeck
and Wouter Koolen for useful discussions and Csaba
Szepesvári for bringing up the reference to Cesa-Bianchi
& Fischer (1998). This research was supported by an Australian Research Council Australian Laureate Fellowship
(FL110100281).

One Practical Algorithm for Both Stochastic and Adversarial Bandits

References
Agrawal, Shipra and Goyal, Navin. Further optimal regret bounds
for Thompson sampling. In AISTATS, 2013.
Audibert, Jean-Yves and Bubeck, Sébastien. Minimax policies
for adversarial and stochastic bandits. In Proceedings of the
International Conference on Computational Learning Theory
(COLT), 2009.
Auer, Peter, Cesa-Bianchi, Nicolò, Freund, Yoav, and Schapire,
Robert E. Gambling in a rigged casino: The adversarial multiarmed bandit problem. In Proceedings of the Annual Symposium on Foundations of Computer Science, 1995.
Auer, Peter, Cesa-Bianchi, Nicolò, and Fischer, Paul. Finite-time
analysis of the multiarmed bandit problem. Machine Learning,
47, 2002a.
Auer, Peter, Cesa-Bianchi, Nicolò, Freund, Yoav, and Schapire,
Robert E. The nonstochastic multiarmed bandit problem. SIAM
Journal of Computing, 32(1), 2002b.
Boucheron, Stéphane, Lugosi, Gábor, and Massart, Pascal. Concentration Inequalities A Nonasymptotic Theory of Independence. Oxford University Press, 2013.
Bubeck, Sébastien. Bandits Games and Clustering Foundations.
PhD thesis, Université Lille, 2010.
Bubeck, Sébastien and Cesa-Bianchi, Nicolò. Regret analysis
of stochastic and nonstochastic multi-armed bandit problems.
Foundations and Trends in Machine Learning, 5, 2012.
Bubeck, Sébastien and Slivkins, Aleksandrs. The best of both
worlds: stochastic and adversarial bandits. In Proceedings of
the International Conference on Computational Learning Theory (COLT), 2012.
Cappé, Olivier, Garivier, Aurélien, Maillard, Odalric-Ambrym,
Munos, Rémi, and Stoltz, Gilles. Kullback-Leibler upper confidence bounds for optimal sequential allocation. Annals of
Statistics, 41(3), 2013.
Cesa-Bianchi, Nicolò and Fischer, Paul. Finite-time regret bounds
for the multiarmed bandit problem. In Proceedings of the International Conference on Machine Learning (ICML), 1998.
Cesa-Bianchi, Nicolò and Lugosi, Gábor. Prediction, Learning,
and Games. Cambridge University Press, 2006.
de Rooij, Steven, van Erven, Tim, Grünwald, Peter D., and
Koolen, Wouter M. Follow the leader if you can, hedge if you
must. Journal of Machine Learning Research, 2014.
Kaufmann, Emilie, Korda, Nathaniel, and Munos, Rémi. Thompson sampling: An optimal finite time analysis. In Proceedings
of the International Conference on Algorithmic Learning Theory (ALT), 2012.
Lai, Tze Leung and Robbins, Herbert. Asymptotically efficient
adaptive allocation rules. Advances in Applied Mathematics,
6, 1985.
Maillard, Odalric-Ambrym. Apprentissage Séquentiel: Bandits,
Statistique et Renforcement. PhD thesis, INRIA Lille, 2011.
Robbins, Herbert. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 1952.

Seldin, Yevgeny, Szepesvári, Csaba, Auer, Peter, and AbbasiYadkori, Yasin. Evaluation and analysis of the performance
of the EXP3 algorithm in stochastic environments. In JMLR
Workshop and Conference Proceedings, volume 24 (EWRL),
2013.
Stoltz, Gilles. Incomplete Information and Internal Regret in Prediction of Individual Sequences. PhD thesis, Université ParisSud, 2005.
Thompson, William R. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples.
Biometrika, 25, 1933.

