Online Time Series Prediction with Missing Data

Oren Anava
Technion, Haifa, Israel

OANAVA @ TX . TECHNION . AC . IL

Elad Hazan
Princeton University, NY, USA

EHAZAN @ CS . PRINCETON . EDU

Assaf Zeevi
Columbia University, NY, USA

ASSAF @ GSB . COLUMBIA . EDU

Abstract
We consider the problem of time series prediction in the presence of missing data. We cast the
problem as an online learning problem in which
the goal of the learner is to minimize prediction
error. We then devise an efficient algorithm for
the problem, which is based on autoregressive
model, and does not assume any structure on the
missing data nor on the mechanism that generates the time series. We show that our algorithm‚Äôs
performance asymptotically approaches the performance of the best AR predictor in hindsight,
and corroborate the theoretic results with an empirical study on synthetic and real-world data.

1. Introduction
A time series is a sequence of signal observations, typically
measured at uniform time intervals. Perhaps one of the
most well-studied models for time series analysis and prediction is the autoregressive (AR) model. Roughly speaking, the AR model is based on the assumption that each
observation can be represented as a (noisy) linear combination of some previous observations. This model has been
successfully used in many real-world applications such as
DNA microarray data analysis, stock market prediction,
and noise cancelation, to name but a few.
Recently there has been growing interest in the problem
of time series prediction in the presence of missing data,
mainly in the ‚Äúproper learning‚Äù setting, in which an underlying model is assumed and the goal is to recover its
parameters. Most of the current work relies on statistical
assumptions on the error terms, such as independence and
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

Gaussian distribution. These assumptions allow the use of
Maximum Likelihood (ML) techniques to recover consistent (and sometimes optimal) estimators for the model parameters. However, these assumptions are many times not
met in practice, causing the resulting estimators to be no
longer consistent. Occasionally, additional assumptions on
the structure of the missing data are added, and the statistical modeling becomes even more distant from the data.
In this paper we argue that assumptions on the model generating the time series and the structure of its missing data
can be relaxed in a substantial manner while still supporting
the development of efficient methods to solve the problem.
Our main contribution is a novel online learning approach
for time series prediction with missing data, that allows the
observations (along with the missing data) to be arbitrarily
or even adversarially generated. The goal of this paper is
to show that the new approach is theoretically more robust,
and is thus capable of coping with a wider range of time
series and missing data structures.
1.1. Informal Statement of Our Results
We cast the problem of AR prediction in the presence of
missing data as an online learning problem. A major modeling challenge arises as AR prediction is not well defined
when some of the data is missing. To overcome this issue,
we define a new family of AR predictors; each such predictor makes use of its own past predictions to ‚Äúfill in‚Äù the
missing data, and then provides an AR prediction using the
completed data. To be slightly more formal, a predictor in
this family has the following recursive form:

REC

XÃÉt (Œ±) =

p
X

Œ±k Xt‚àík 1{Xt‚àík is revealed}

k=1
p
X

+

k=1

REC
Œ±k XÃÉt‚àík
(Œ±)1{Xt‚àík is not revealed},

Online Time Series Prediction with Missing Data

where Xt is the signal measured at time point t, and Œ± ‚àà
Rp is the vector of AR coefficients. Now, let `t (Xt , XÃÉt )
denote the loss suffered by predicting XÃÉt at time point t,
and RT be the corresponding regret term. Then, our main
theorem is the following:
Theorem 3.1. Our algorithm generates an online sequence
of predictions {XÃÉt }Tt=1 , for which it holds that:
RT =

T
X


`t Xt , XÃÉt 1{Xt is revealed}

t=1

‚àí

T
X

‚àö

`t Xt , XÃÉtREC (Œ±‚àó ) 1{Xt is revealed} = O( F ),

t=1

where F denotes the number of time points in which our
algorithm
received feedback, and Œ±‚àó is the minimizer of
PT
REC
t=1 `t (Xt , XÃÉt (Œ±))1{Xt is revealed}.
This result is somewhat surprising, as even the problem of
finding the best AR predictor in hindsight is non-convex
due to the recursive structure of the predictors we consider.
The key observation that enables an efficient solution in
this scheme relies on non-proper learning: it is possible
to learn coefficients in a much larger class and compete
against the best AR predictor. The complexity of this class
is in fact exponential in the parameters of our problem, yet
we prove that the learning of the new coefficients can be
done efficiently due to some special characteristics. This
idea was successfully applied also in the work of (Hazan
et al., 2015), who considered the problem of low-rank classification with missing data.
1.2. Related Work
Several different approaches for time series prediction in
the presence of missing data exist; we overview the major
ones. Perhaps the earliest approach, originated in the control theory community, goes back to the work of (Kalman,
1960). In this work, the concept of state-space modeling
is presented to deal with streams of noisy input data. Although the original work is not aimed at coping with missing data, it initialized a solid line of works that use statespace modeling to impute missing data (Shumway & Stoffer, 1982; Sinopoli et al., 2004; Liu & Goldsmith, 2004;
Wang et al., 2005). We refer the reader to (Durbin & Koopman, 2012) for a complete overview of time series analysis
using state-space models.
Another increasingly common approach builds upon the
concept of multiple imputation (Honaker & King, 2010).
Roughly speaking, multiple imputation aims at inferring
relevant information from the observed data (using known
statistical methods), and use it to impute multiple values
for each missing data point. The resulted multiple data sets
are now treated as completed, which allows the use of stan-

dard statistical methods for the analysis task. Results from
different data sets are then combined using various simple
procedures.
In the statistical literature, missing data are usually imputed
using maximum likelihood estimators corresponding to a
specific underlying model. Very often, these estimators are
not efficiently computed, which motivates the use of Expectation Maximization (EM) algorithms. This approach
was proposed by (Dempster et al., 1977), and is currently
the most popular for dealing with missing data in time series. Essentially, the EM algorithm avoids the separate
treatment of each of the exponentially many missing data
patterns by using the following two-step procedure: in the
E-step, missing observations are filled in with their conditional expectations given the observed data and the current estimate of the model parameters; and in the M-step,
a new estimate of the model parameters is computed from
the current version of the completed data. We note that the
vast majority of the state-space modeling literature relies
on EM techniques as well (for instance, see (Shumway &
Stoffer, 1982; Sinopoli et al., 2004)).
One particular time series model that has received a great
attention in the statistical literature is the AR model. In the
context of missing data, there are many works that assume
this underlying model, and differ in the assumptions on the
missing data patterns: in (Dunsmuir & Robinson, 1981),
a stochastic mechanism is assumed to generate the missing data; (Ding et al., 2010) consider a scarce pattern of
the observed signal; and (Choong et al., 2009) rely on local
similarity structures in the data. The existence of distributional assumptions on the time series or on the patterns of
the missing data is in common to all of these works.
To date, we are not aware of an approach that allows the
signal (along with its missing data) to be generated arbitrary, let alone adversarially. The only approach that allows
for adversarially generated signals we are aware of is the recent result of (Anava et al., 2013), which does not account
for missing data. Our work can be seen as an extension of
the latter to the missing data setting.

2. Preliminaries and Model
A time series is a sequence of signal observations, measured at successive time points (usually spaced at uniform
intervals). Let Xt denote the signal measured at time t. The
traditional AR model, parameterized by lag p and coefficient vector Œ± ‚àà Rp , assumes that each observation complies with the formula

Xt =

p
X
k=1

Œ±k Xt‚àík + t ,

Online Time Series Prediction with Missing Data

where {t }t‚ààZ is assumed to be white noise. In words, the
model assumes that Xt is a noisy linear combination of the
previous p observations. Sometimes, an additional additive
term Œ±0 is included to indicate drift, but we will ignore
this for simplicity. Notice that this does not increase the
complexity of the problem, since we can simply raise the
dimension of the vector Œ± by one and assign the value 1 to
the corresponding observation.
The motivation to use AR(p) models for signal prediction
goes back to Wold‚Äôs decomposition theorem. According
to this theorem, a stationary signal {Xt }t‚ààZ can be represented as an MA(‚àû) process. That is,
Xt =

‚àû
X

Œ≤i t‚àíi + t ,

i=1

P‚àû
where i=1 Œ≤i2 < ‚àû, and {t }t‚ààZ have zero-mean and
equal variance. If, in addition, {Xt }Tt=1 is assumed to be
invertible, we can represent it as an AR(‚àû) process. That
is,
‚àû
X
Xt =
Œ±i Xt‚àíi + t ,
i=1

{Œ±i }‚àû
i=1

where
are uniquely defined. This representation,
accompanied with the natural assumption that Œ±i decays
fast enough as a function of i, motivates the use of AR(p)
models for the task of signal prediction.
2.1. The Online Setting for AR Prediction
After motivating the use of AR models, arises the question
of misspecification: what happens if we employ a model
that does not comply with our data? Standard statistical
methods (e.g., maximum likelihood) for estimating the AR
coefficients are based on the assumption that the observations come from an AR model, and thus are not suitable
when the model is misspecified. This drives the use of online learning based techniques to circumvent this issue.
Online learning is a well established learning paradigm
which has both theoretical and practical appeals. The goal
in this paradigm is to make a sequential prediction, where
the data, rather than being generated stochastically, is assumed to be chosen by an adversary that has full knowledge
of our learning algorithm (see for instance (Cesa-Bianchi &
Lugosi, 2006)). Specifically, the following setting is usually assumed in the context of time series prediction: at
time point t, we need to make a prediction XÃÉt , after which
the true value of the signal Xt is revealed, and we suffer a
loss denoted by `t (Xt , XÃÉt ). Usually, `t is assumed to be
convex with Lipschitz gradients.
When considering an AR(p) prediction, we must define in
advance the decision set K ‚äÇ Rp , which stands for the class
of AR coefficients against which we want to compete. We

henceforth let K = [‚àí1, 1]p . Our prediction at time point t
then takes the form:
AR

t

XÃÉt (Œ± ) =

p
X

Œ±kt Xt‚àík ,

(1)

k=1

where Œ±t ‚àà K is generated by our online algorithm. Here
comes the punch of the online setting: our goal is to design
an algorithm that generates a sequence {Œ±t }Tt=1 which is
almost as good as the best (in hindsight) AR coefficients in
K. More formally, we define the regret to be

RT =

T
X

T
X


`t Xt , XÃÉtAR (Œ±t ) ‚àí min
`t Xt , XÃÉtAR (Œ±) ,
Œ±‚ààK

t=1

t=1

and wish to design efficient algorithms, whose regret grows
sublinearly in T . Thus, even if the model is misspecified
(meaning the best AR coefficients in K have unsatisfactory predictive power), minimizing the regret term is still
meaningful. Remains the question of how can we compete
against the best AR coefficients in the presence of missing
data? The latter is the main question we try to answer in
this work.
2.2. Problem Definition
Throughout this work we consider the following setting
(which accounts for missing data): at time point t, we need
to make a prediction XÃÉt , after which feedback in the form
of the real signal Xt is not necessarily revealed, and we
suffer loss denoted by `t (Xt , XÃÉt )1{Xt is revealed}. That
is, we suffer loss only if we receive feedback. Here also, `t
is assumed to be convex with Lipschitz gradients.
The problem arising in this setting is two-fold: first, we
cannot provide an AR prediction at time t (even given
the vector Œ±) if some of the required past observations are
missing. Second, the best AR predictor in hindsight is not
well-defined. To solve this problem, we define a family of
recursive predictors, each of the form:
XÃÉtREC (Œ±) =

p
X

Œ±k Xt‚àík 1{Xt‚àík is revealed}

k=1

+

p
X

REC
Œ±k XÃÉt‚àík
(Œ±)1{Xt‚àík is not revealed},

k=1

(2)
where Œ± ‚àà K = [‚àí1, 1]p . Essentially, a predictor in this
family uses its own (updated) estimations as a proxy for
the actual signal, and then provides an AR(p) prediction
as if there is no missing data. The problem at hand then
translates into minimizing the corresponding regret term.

Online Time Series Prediction with Missing Data

2.3. Our Assumptions
Throughout the remainder of this work we assume that the
following hold:
(1) Xt ‚àà [‚àí1, 1] for all t. Here, the constant 1 can be replaced by any other constant C < ‚àû (which is known
in advanced), but in order to simplify the writing we
assume that C = 1.
(2) For all t there exist at least p successive time points in
t ‚àí d, . . . , t ‚àí 1 for which we received feedback. This
assumption makes sure that each prediction XÃÉtREC (Œ±)
does not ‚Äúlook back‚Äù more than d time points. This
assumption can be completely removed, as discussed
in Section 3.4.

where missing observations are encoded using {‚àó}. The
notation 1{Xt } will be used as the indicator of the event
{Xt is revealed}. Finally, denote
o
n
d
BÃÉd = w ‚àà R2 ‚àí1 : kwk22 ‚â§ 2d .
We say that the vector b = (b1 , . . . , bd ) ‚àà {0, 1}d is the
binary representation of a number n if n is represented as
Pd
k‚àí1
. We denote by b(n) the unique binary repk=1 bk 2
resentation of n. For a given number n and its binary representation b(n) = (b1 , . . . , bd ), we define m(n) to be the
maximal index k such that bk = 1. That is,

	
m(n) = max k : 2k‚àí1 < n .

Note we do not assume an underlying AR(p) model that
generates the signal, nor a statistical model according to
which the missing observations are omitted from us.

The definition below links between a structure of missing
observations and a binary vector b. This, in turn, will be
d
used to link between a vector w ‚àà R2 ‚àí1 and a structure of
missing observations.

3. Our Approach

Definition 1. Let b = (b1 , . . . , bd ) ‚àà {0, 1}d and set m =
max{k | bk = 1}. We say that b is a semi-valid path with
sv
respect to X(t‚àíd:t‚àí1) (and denote b ‚àº X(t‚àíd:t‚àí1) ), if bm =
1{Xt‚àím } = 1 and bi ‚â• 1{Xt‚àíi } for i < m. If in addition
(b1 , . . . , bm ) does not contain p successive zeros we say
that b is a valid path with respect to X(t‚àíd:t‚àí1) (and denote
v
b ‚àº X(t‚àíd:t‚àí1) ).

We briefly outline our approach to the problem at hand.
Basically, observe that the prediction at time point t can be
written in the following form:
XÃÉtREC (Œ±) =

d
X

pk (Œ±)Xt‚àík 1{Xt‚àík },

k=1

where pk (Œ±) is a polynomial in Œ± that is determined by the
structure of missing data. Each polynomial pk potentially
contains up to 2k‚àí1 terms of the form Œ±i1 ¬∑ . . . ¬∑ Œ±ij , such
Pj
that m=1 im = k. This means that the prediction at time
point t constitutes of up to 2d terms of the form Œ±i1 ¬∑. . .¬∑Œ±ij ,
Pj
such that for each of them it holds that m=1 im ‚â§ d.
Notice that each such term is less or equal to 1 for Œ±‚àó that
is the best recursive AR(p) predictor in hindsight, since we
consider K = [‚àí1, 1]p .
The latter observation allows the use of non-proper learning
techniques in this setting. Essentially, the idea is to learn a
d
vector w ‚àà R2 such that each entry in w corresponds to a
product of the form Œ±i1 ¬∑. . .¬∑Œ±ij , while ignoring the restrictions imposed on w by Œ±. Obtaining a regret bound w.r.t.
to best w would imply a regret bound w.r.t. the best Œ±, yet
an efficiency question would remain since the dimension
of w is 2d . In Section 3.3 we prove that the inner products
in the space induced by this relaxation can be computed
efficiently, which overall gives an efficient algorithm with
provable regret bound.
3.1. Notation and Definitions
We denote by [n] the set {1, . . . , n}, and use X(t‚àíd:t‚àí1) ‚àà
{R ‚à™ {‚àó}}d to denote the vector of values Xt‚àíd , . . . , Xt‚àí1 ,

Basically, each structure of missing data corresponds to
many valid paths; each of these corresponds to coefficient
of a revealed signal in Equation (2). To see that, note that
XÃÉtREC (Œ±) =

d
X

pk (Œ±|X(t‚àíd:t‚àí1) )Xt‚àík 1{Xt‚àík },

k=1

where pk (Œ±|X(t‚àíd:t‚àí1) ) is a polynomial in Œ± that is determined by the structure of missing data in X(t‚àíd:t‚àí1) .
Definition 2. For a given vector X(t‚àíd:t‚àí1) , we define the
d
function Œ¶(X(t‚àíd:t‚àí1) ) ‚àà R2 ‚àí1 as follows:


Œ¶(X(t‚àíd:t‚àí1) ) n =



Xt‚àím(n)
0

sv

if b(n) ‚àº X(t‚àíd:t‚àí1) ,
otherwise.



Here, Œ¶(X(t‚àíd:t‚àí1) ) n denotes the n-th coordinate of the
vector Œ¶(X(t‚àíd:t‚àí1) ). Similarly, we define an auxiliary
function Œ¶p for valid paths:

v
 p

Xt‚àím(n) if b(n) ‚àº X(t‚àíd:t‚àí1) ,
Œ¶ (X(t‚àíd:t‚àí1) ) n =
0
otherwise.
From now on, we use the notation XÃÉt (w) for predictions of
the form w> Œ¶(X(t‚àíd:t‚àí1) ), and XÃÉtp (w) for predictions of
the form w> Œ¶p (X(t‚àíd:t‚àí1) ).

Online Time Series Prediction with Missing Data

Algorithm 1 L AZY OGD (on `2 -ball with radius D)
1: Input: learning rate Œ∑t .
2: Set a1 = 0.
3: for t = 1 to T do
4:
Play at and incur lossPft (at )
t
Œ∑
‚àáf (a )
Pt i i
5:
Set at+1 = ‚àí max{1,tŒ∑t ki=1
i=1 ‚àáfi (ai )k}
D
6: end for

Algorithm 2
1: Input: learning rate Œ∑t .
2: Set w1 = 0.
3: for t = 1 to T do
t
4:
Play wt and incur ft (w
) = `t (Xt , XÃÉt (wt ))1{Xt }
Pt
œÑ
Œ∑
t
œÑP
=1 ‚àáfœÑ (w )1{XœÑ }
5:
Set wt+1 = ‚àí max{1,Œ∑
t
‚àíd/2 }
œÑ
tk
œÑ =1 ‚àáfœÑ (w )k¬∑2
6: end for

3.2. Algorithm and Analysis

Proof. Algorithm 2 is simply L AZY OGD on the decision
set BÃÉd . Thus, by applying it we get that

We take a step back to present an online algorithm: L AZY
OGD (Algorithm 1), which is aimed at minimizing regret
in the general online learning framework. L AZY OGD
is a special instance of the FTRL algorithm when K is
an `2 -ball with radius D. Let {ft }Tt=1 be convex loss
functions for which maxa,t {k‚àáft (a)k} ‚â§ G and dePT
note a‚àó = arg minkak‚â§D t=1 ft (a). Then, the regret of
L AZY OGD is bounded as follows:
RLazy
T

=

T
X

ft (at ) ‚àí min

kak‚â§D

t=1

T
X

T
X
t=1

‚àí min

t=1

T

min

Our algorithm (Algorithm 2) is an adaptation of L AZY
OGD to BÃÉd , but notice that in its current form it is inefficient (since the dimension of BÃÉd is exponential in d).
This form is easier to analyze and is thus stated here; an
efficient version of Algorithm 2 is presented in Section
3.3. In the sequel, we denote D = maxw‚ààBÃÉd {kwk} and
G = maxw,t {k‚àá`t (Xt , XÃÉt (w))k}. From the definition of
BÃÉd it follows directly that D = 2d/2 . The value of G depends on the loss functions considered; for instance, if we
consider the squared loss then G = 2d/2 .

w‚ààBÃÉd t=1
(a)

‚â§ min
(b)


`t (Xt , XÃÉt (wt ) 1{Xt }

t=1

v
u T
uX
REC
`t (Xt , XÃÉt (Œ±))1{Xt } ‚â§ 3GDt
1{Xt },

if we choose Œ∑t =

t=1

‚àöPt

G

D

œÑ =1

1{XœÑ }

.

min

kŒ±k‚àû ‚â§1


`t Xt , XÃÉtp (w) 1{Xt }

T
X


`t Xt , XÃÉtREC (Œ±) 1{Xt },

(4)

t=1

where XÃÉtp (w) is of the form w> Œ¶p (X(t‚àíd:t‚àí1) ).
The two key inequalities above are explained as follows.
To prove (a), note that from the construction of Œ¶ and Œ¶p it
follows that for any t, Œ¶p (X(t‚àíd:t‚àí1) ) can be written as

 
 p

Œ¶(X(t‚àíd:t‚àí1) ) n if n ‚àà N ,
Œ¶ (X(t‚àíd:t‚àí1) ) n =
0
otherwise,
 d

where N is the set of all numbers n ‚àà 2 ‚àí 1 for which
v
there exists X ‚àà {R ‚à™ {‚àó}}d such that b(n) ‚àº X. In
particular, if we denote
w‚àó = arg min

Theorem 3.1. Algorithm 2 generates an online sequence
{wt }Tt=1 for which it holds that:

t=1

T
X

w‚ààBÃÉd t=1

The following is our main theorem:

Œ±‚ààK

T
X

`t Xt , XÃÉt (w) 1{Xt }

‚â§

A complete analysis can be found in (Hazan, 2011; ShalevShwartz, 2012).

‚àí min

t=1

t=1

(3)

t=1

T
X

v
u T
uX

1{Xt }.
`t Xt , XÃÉt (w) 1{Xt } ‚â§ 3GDt

Now, we can write

ka‚àó k2 X
+
Œ∑t k‚àáft (at )k2
Œ∑T
t=1
v
u T
uX
‚àö
k‚àáft (at )k2 ‚â§ 3GD T .
‚â§ 3Dt

T
X

T
X

w‚ààBd

ft (a)

‚â§

RT =


`t Xt , XÃÉt (wt ) 1{Xt }

T
X

w‚ààBÃÉd t=1


`t Xt , XÃÉtp (w) 1{Xt }

then w.l.o.g. we can assume that wn‚àó = 0 for all n 6‚àà N .
Now, note that for w‚àó it holds that XÃÉtp (w‚àó ) = XÃÉt (w‚àó ) for
all t, which implies that
T
X

T
X


`t Xt , XÃÉt (w‚àó ) 1{Xt } =
`t Xt , XÃÉtp (w‚àó ) 1{Xt }.

t=1

t=1

PT


Finally, since minw‚ààBd t=1 `t Xt , XÃÉt (w) 1{Xt } ‚â§

PT
‚àó
t=1 `t Xt , XÃÉt (w ) 1{Xt }, the claim holds.

Online Time Series Prediction with Missing Data

Now, to prove (b), let us first denote
Œ±‚àó = arg min

kŒ±k‚àû ‚â§1

T
X


`t Xt , XÃÉt (Œ±) 1{Xt }.

t=1

Next, for a given vector b ‚àà {0, 1}d we define I(b) ‚àà [d]d
as follows:

i ‚àí max {j|j < i and bj = 1} if bi = 1,
[I(b)]i =
0
otherwise,
where max {j : j < i and bj = 1} = 0 if no such j exists. In words, I(b) is a vector holding the distance between
ones in b to the nearest one to their left1 .
Now, consider the following construction of w:
( Q
d

i=1

wn =

‚àó
Œ±[I(b(n))]

0

v

i

if b(n) ‚àº X(t‚àíd:t‚àí1)
otherwise

Our claim is that for the construction above, it holds that
XÃÉtp (w) = XÃÉt (Œ±‚àó ). Let us look first at XÃÉt (Œ±‚àó ). By definition, it can be recursed until it ‚Äúencounters‚Äù p successive
revealed observations. Thus, it has the following structure:
XÃÉt (Œ±‚àó ) =

d
X

pk (Œ±‚àó |X(t‚àíd:t‚àí1) )Xt‚àík 1{Xt },

k=1
‚àó

where pk (Œ± |X(t‚àíd:t‚àí1) ) is a polynomial consisting of
sums of products of Œ±1‚àó , . . . , Œ±p‚àó . In fact, the polynomial
pk (Œ±‚àó |X(t‚àíd:t‚àí1) ) is a sum of elements, each of them is
Qd
‚àó
of the form i=1 Œ±[I(b)]
, where b is a valid path w.r.t.
i
X(t‚àíd:t‚àí1) and in addition k = max{i : bi = 1}. This
property follows directly from Definition 1 in Section 3.1.
Thus, we can write
XÃÉ t (Œ±‚àó ) =

d
X

pk (Œ±‚àó |X(t‚àíd:t‚àí1) )Xt‚àík 1{Xt‚àík }

k=1

=

d X Y
d
X

=

‚àó
Œ±[I(b)]
Xt‚àík 1{k = max{i : bi = 1}}
i

=

2

s=1

œÑ =1

Errs ErrœÑ K(s,œÑ )

3.3. Efficient Implementation of Algorithm 2
Theorem 3.1 ignores computational considerations. Next,
We show that XÃÉt (wt ) can in fact be generated efficiently.
For t = 1 we have that w1 = 0 and thus XÃÉ1 (w1 ) = 0.
Next, assume that {XÃÉœÑ (wœÑ )}tœÑ =1 are efficiently generated
and prove for XÃÉt+1 (wt+1 ). Now, notice that if we denote
‚àá`œÑ (wœÑ )1{Xt } = ErrœÑ Œ¶(X(œÑ ‚àíd:œÑ ‚àí1) ), the above implies
that ErrœÑ is known for all œÑ ‚â§ t. Thus,
XÃÉt+1 (wt+1 ) = Œ¶(X(t‚àíd+1:t) )> wt+1
Pt
‚àíŒ∑ œÑ =1 Œ¶(X(t‚àíd+1:t) )> ‚àá`œÑ (wœÑ )
=
Pt
max{1, Œ∑k œÑ =1 ‚àá`œÑ (wœÑ )k2‚àíd/2 }
Pt
‚àíŒ∑ œÑ =1 ErrœÑ K(t + 1, œÑ )

,
=
q
Pt Pt
‚àíd
max 1, Œ∑ 2
s=1
œÑ =1 Errs ErrœÑ K(s, œÑ )
where K(s, œÑ ) = Œ¶(X(s‚àíd:s‚àí1) )> Œ¶(X(œÑ ‚àíd:œÑ ‚àí1) ). The
above is efficient to generate if and only if K(s, œÑ ) is
efficient to compute for all s and œÑ . This computation
can be done in O(d) computations despite the fact that
d
Œ¶(X(s‚àíd:s‚àí1) ) ‚àà R2 ‚àí1 .
To see that, we first needto define an auxiliary function
 c
as follows: c(s, œÑ ) k = c(X(s‚àíd:s‚àí1) , X(œÑ ‚àíd:œÑ ‚àí1) ) k =
Pk‚àí1
i=1 (1 ‚àí 1{Xs‚àíi })(1 ‚àí 1{XœÑ ‚àíi }) . Essentially, c(s, œÑ )
counts the number of relatively common missing observations in X(s‚àíd:s‚àí1) and X(œÑ ‚àíd:œÑ ‚àí1) . Now, notice that

=

d
2X
‚àí1

 


Œ¶(X(s‚àíd:s‚àí1) ) n Œ¶(X(œÑ ‚àíd:œÑ ‚àí1) ) n

n=1
v
‚àó
Œ±[I(b(n))]
Xt‚àím(n) 1{b(n) ‚àº
i

X(t‚àíd:t‚àí1) }

n=1 i=1
d
2X
‚àí1

max 1,Œ∑

6: end for

K(s, œÑ ) = Œ¶(X(s‚àíd:s‚àí1) )> Œ¶(X(œÑ ‚àíd:œÑ ‚àí1) )

k=1 b‚ààXtv i=1
d
2X
‚àí1 Y
d

Algorithm 3 Efficient Implementation of Algorithm 2
1: Input: learning rate Œ∑.
2: Set w1 = 0.
3: for t = 1 to T do

4:
Predict XÃÉt and incur `t (Xt , XÃÉt 1{Xt }
5:
Set prediction:
Pt
œÑ =1 ErrœÑ K(t+1,œÑ )
o
n
‚àö‚àíŒ∑‚àíd P
XÃÉt+1 =
Pt
t

=

d
2X
‚àí1

Xs‚àím(n) XœÑ ‚àím(n) 1{b(n) ‚àà Xssv }1{b(n) ‚àà XœÑsv }

n=1



wn Œ¶p (X(t‚àíd:t‚àí1) ) n

n=1
> p

= w Œ¶ (X(t‚àíd:t‚àí1) ) = XÃÉtp (w),
where Xtv is the set of all valid paths w.r.t. X(t‚àíd:t‚àí1) .
This establishes the claim in (b). Now, plugging (4) into
(3) gives the stated result.

=

d
X

2[c(s,œÑ )]k Xs‚àík XœÑ ‚àík 1{Xs‚àík }1{XœÑ ‚àík },

k=1

where Xtsv is the set of all valid paths w.r.t. X(t‚àíd:t‚àí1) ,
and the last equality follows from Definition 1.
1
For instance, it holds that I((0, 1, 0, 1)) = (0, 2, 0, 2) and
I((0, 0, 1, 1)) = (0, 0, 3, 1).

Online Time Series Prediction with Missing Data

3.4. Some Extensions
We briefly discuss two issues: removal of assumption (2)
and replacement of BÃÉd with a smaller ball.
As mentioned before, assumption (2) makes sure that each
prediction XÃÉtREC (Œ±) considers at most d past observations.
However, our algorithm is still applicable if this assumption
does not hold. The theoretic guarantee then gets a different interpretation: we are almost as good as the best recursive AR(p) predictor, but now the recursion is limited to at
most d past observations. Essentially, this is equivalent to
defining a family of recursive AR predictors with bounded
memory, and compete against predictors in this family.
As for the second issue, recall that the dimension of the
decision set BÃÉd is exponential in d. This affects us only
in the regret bound, as we proved earlier that computations
can be done efficiently in our setting.
To mitigate the regret
n
o
d
bound effect, we define BÃÇd = w ‚àà R2 ‚àí1 : kwk22 ‚â§ d
and state the following corollary:
Corollary 3.2. Algorithm 2 generates an online sequence
{wt }Tt=1 for which it holds that:
RT =

T
X


`t (Xt , XÃÉt (wt ) 1{Xt }

t=1

v
u T
T
uX
X
REC
1{Xt },
`t (Xt , XÃÉt (Œ±))1{Xt } ‚â§ 3GDt
‚àí min
Œ±‚ààK

t=1

t=1


where K =

Œ± ‚àà Rp : Œ±i ‚â§



‚àö1
2

i 

.

The proof follows by a simple calculation,
‚àö and is thus omitted here. Note that in the above D = d and G is again
determined by the selection of the loss functions. This case
captures natural scenarios, in which the effect of past observations decays as they are more distant.

4. Illustrative Examples
The following experiments demonstrate the effectiveness
of the proposed algorithm under various synthetic settings.
Due to lack of space, we defer the experimental results on
real-world data to the supplementary material.
4.1. Baselines
Most of the works on time series with missing observations
consider what we call the offline setting: given a time series
that contains missing observations, compute the model parameters (in our case, the AR coefficients) and\or impute
the missing data. Our online setting can be seen as a sequential offline setting, in which at time t we are given the
time series values up to time t ‚àí 1 and our task is to predict

Algorithm 4 OGD IMPUTE
1: Input: learning rate Œ∑
2: Initialize Œ±1 = 0, and set Xt = 0 for t ‚â§ 0
3: for t = 1 to T do
Pp
4:
Predict XÃÉtAR (Œ±t ) = i=1 Œ±it Xt‚àíi

5:
Observe loss `t Xt , XÃÉtAR (Œ±t ) 1{Xt }
6:
If 1{Xt } = 0, then set Xt = XÃÉtAR (Œ±t ) 

7:
Set Œ±t+1 = Œ†K Œ±t ‚àí Œ∑‚àá`t Xt , XÃÉtAR (Œ±t ) 1{Xt }
8: end for

the signal at time t. In light of this, we adapt the offline
baselines presented below to the online setting. We note
that this adaptation does not weaken the offline baselines
in any way, and we use it only for comparison purposes.
Yule-Walker estimator. We use the well-known YuleWalker estimator, and adapt it to our setting as follows: at
first, we initialize some AR coefficients. At time t, at our
disposal is the data seen up to time t ‚àí 1, where missing
observations are filled by corresponding past predictions
of the algorithm. Then, the AR coefficients are computed
by solving the Yule-Walker equations, and a prediction is
made accordingly.
Expectation Maximization (EM). Our EM baseline is
based on the algorithm originally proposed by (Shumway
& Stoffer, 1982). Roughly speaking, the algorithm assumes an underlying state-apace model and employs the
Kalman filter to estimate its parameters. The estimation is
done by maximizing the log-likelihood using iterative EM
steps, and the resulting Kalman smoothed estimator is used
to complete the missing observations.
ARLS IMPUTE. This algorithm was proposed by (Choong
et al., 2009) to cope originally with missing observations
in DNA microarray data. The algorithm is based on the
following iterative method: at the first iteration, initialize
all the missing observations to 0. Then, in every iteration
compute LS estimator for the AR coefficients and update
the value of the missing observations by maximizing the
log-likelihood.
OGD IMPUTE. We propose another algorithm for the
problem at hand, denoted Algorithm 4. Basically, the algorithm applies the standard Online Gradient Descent algorithm to learn the AR coefficients, while filling missing
observations with their past predictions. Whereas this algorithm is very fast and simple to implement, its downside
is the lack of theoretic guarantee.
4.2. Generating the Synthetic Data
To compare the performance of the proposed algorithms
we design several different settings (presented below). In
order to ensure the stability of the results we average them

Online Time Series Prediction with Missing Data
MSE vs. time for 10% missing data (sanity check)

‚àí0.6

10

‚àí0.7

MSE

‚àí0.5

10

MSE vs. time for 10% missing data (heteroscedasticity)
‚àí0.5

10

Yule Walker
ARLSimpute
EM (Kalman Filter)
OGDimpute
Our Algorithm

‚àí0.7

10

10

‚àí0.8

10

‚àí0.8

10

‚àí0.7

Yule Walker
ARLSimpute
EM (Kalman Filter)
OGDimpute
Our Algorithm

‚àí0.6

10

MSE

‚àí0.3

10

MSE

MSE vs. time for 10% missing data (AR mixture)

Yule Walker
ARLSimpute
EM (Kalman Filter)
OGDimpute
Our Algorithm

10

‚àí0.9

10

‚àí0.9

10

‚àí0.9

10

0

500

1000
Time

1500

2000

0

MSE@2000 vs. percentage of missing data (sanity check)

1000
Time

1500

2000

0

MSE@2000 vs. percentage of missing data (AR mixture)

0.25

0.3

0.2
0.15

0.25

0.3

0.2
0.15

0.25
0.2

0.1

0.1

0.05

0.05

0

10%
20%
Percentage of missing data

2000

0.15

0.1

0%

1500

0%

Yule Walker
ARLSimpute
EM (Kalman Filter)
OGDimpute
Our Algorithm

0.35

0.05
0

1000
Time

0.4
Yule Walker
ARLSimpute
EM (Kalman Filter)
OGDimpute
Our Algorithm

0.35

MSE@2000

0.3

500

MSE@2000 vs. percentage of missing data (heteroscedasticity)

0.4
Yule Walker
ARLSimpute
EM (Kalman Filter)
OGDimpute
Our Algorithm

MSE@2000

0.4
0.35

MSE@2000

500

0

10%
20%
Percentage of missing data

0%

10%
20%
Percentage of missing data

Figure 1. Experimental results for synthetic data.
Setting 1. sanity check

Setting 2. AR mixture

Setting 3. heteroscedasticity

0%

10%

20%

0%

10%

20%

0%

10%

20%

Yule-Walker

0.1156

0.1270

0.1427

0.1343

0.1476

0.1496

0.0986

0.1023

0.1059

ARLS IMPUTE

0.1058

0.1160

0.1336

0.1344

0.1424

0.1417

0.1063

0.1150

0.1276

EM (Kalman filter)

0.1065

0.1170

0.1301

0.1101

0.1189

0.1244

0.1042

0.1078

0.1205

OGD IMPUTE

0.1071

0.1175

0.1305

0.1396

0.1530

0.1568

0.0945

0.0984

0.1028

Our algorithm

0.1085

0.1212

0.1447

0.1072*

0.1168*

0.1260

0.0937

0.0979

0.1018

Table 1. Experimental results for synthetic data.

over 50 runs. For our algorithm, we used d = 3p in all
considered settings. In our tables, we mark with bold font
the best results, and add an asterisk to indicate significance
level of 0.05.
Setting 1 (sanity check). We generate a time series using
the coefficient vector Œ± = [0.6, ‚àí0.5, 0.4, ‚àí0.4, 0.3] and
i.i.d. noise terms that are distributed N (0, 0.32 ). We then
omit some of the data points in a random manner (that is,
each data point is omitted with a certain probability).
Setting 2 (AR mixture). Our motivation in this setting is to
examine the functionality of the different algorithms when
faced with changing environments. Thus, we consider a
predefined set of AR coefficients, and generate time series
by alternating between them in a random manner. We add
an additive noise which is distributed U ni[‚àí0.5, 0.5]. Here
also, the missing data is omitted in a random manner.
Setting 3 (heteroscedasticity). Here we test the robustness
of the different algorithms to unequally distributed noise
terms. Thus, we generate a time series using the coefficient vector Œ± = [0.11, ‚àí0.5], and noise terms that are
distributed normally, with expectation that is the value of
the previous noise term and variance 0.32 . This implies
that consecutive noise terms are positively correlated. The
missing data points are chosen randomly here as well.

As evident in Figure 1 and Table 1, our online algorithm
outperforms the other algorithms when the time series exhibits some complicated structure. In the case where the
error terms are Gaussian and the time series complies with
the AR model (sanity check), we can see that all algorithms
perform roughly the same, as can be expected by the theoretical guarantees. We point out that whereas the offline
algorithms (especially the EM based and ARLS IMPUTE)
require rather large computational power, the two online
algorithms are fast and quite simple to implement.

5. Discussion and Conclusion
In this work we studied the problem of time series prediction using the AR model in the presence of missing data.
We considered a setting in which the signal, along with the
missing data, are allowed to be arbitrary. We then defined
the notion of learning in this setting with respect to the best
(in hindsight) AR predictor, and showed that we can be almost as good as this predictor.
It remains for future work to study whether the dependence
on the parameter d in the regret bound can be improved. It
would also be interesting to study whether our approach
could be extended to more complex time series models,
such as ARCH, ARMA, and ARIMA.

Online Time Series Prediction with Missing Data

Acknowledgments
The research leading to these results has received funding
from the European Unions Seventh Framework Programme
(FP7/2007-2013) under grant agreement n‚ó¶ 336078 ‚Äì ERCSUBLRN. We would also like to acknowledge the work of
(Hazan et al., 2015), which incited the non-proper learning
technique in this work.

References
Anava, Oren, Hazan, Elad, Mannor, Shie, and Shamir,
Ohad. Online learning for time series prediction. arXiv
preprint arXiv:1302.6927, 2013.
Cesa-Bianchi, N. and Lugosi, G. Prediction, learning, and
games. Cambridge University Press, 2006.
Choong, Miew Keen, Charbit, Maurice, and Yan, Hong.
Autoregressive-model-based missing value estimation
for dna microarray time series data. Information Technology in Biomedicine, IEEE Transactions on, 13(1):
131‚Äì137, 2009.
Dempster, Arthur P, Laird, Nan M, and Rubin, Donald B.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the royal statistical society. Series
B (methodological), pp. 1‚Äì38, 1977.
Ding, Jie, Han, Lili, and Chen, Xiaoming. Time series ar
modeling with missing observations based on the polynomial transformation. Mathematical and Computer
Modelling, 51(5):527‚Äì536, 2010.
Dunsmuir, William and Robinson, PM. Estimation of time
series models in the presence of missing data. Journal of
the American Statistical Association, 76(375):560‚Äì568,
1981.
Durbin, James and Koopman, Siem Jan. Time series analysis by state space methods. Number 38. Oxford University Press, 2012.
Hazan, Elad. The convex optimization approach to regret
minimization. Optimization for machine learning, pp.
287, 2011.
Hazan, Elad, Livni, Roi, and Mansour, Yishay. Classification with low rank and missing data. CoRR,
abs/1501.03273, 2015.
Honaker, James and King, Gary. What to do about missing values in time-series cross-section data. American
Journal of Political Science, 54(2):561‚Äì581, 2010.
Kalman, Rudolph Emil. A new approach to linear filtering
and prediction problems. Journal of Fluids Engineering,
82(1):35‚Äì45, 1960.

Liu, Xiangheng and Goldsmith, Andrea. Kalman filtering
with partial observation losses. In Decision and Control,
2004. CDC. 43rd IEEE Conference on, volume 4, pp.
4180‚Äì4186. IEEE, 2004.
Shalev-Shwartz, Shai. Online learning and online convex optimization. Foundations and Trends in Machine
Learning, 4(2):107‚Äì194, 2012.
Shumway, Robert H and Stoffer, David S. An approach
to time series smoothing and forecasting using the em
algorithm. Journal of time series analysis, 3(4):253‚Äì264,
1982.
Sinopoli, Bruno, Schenato, Luca, Franceschetti, Massimo, Poolla, Kameshwar, Jordan, Michael I, and Sastry,
Shankar S. Kalman filtering with intermittent observations. Automatic Control, IEEE Transactions on, 49(9):
1453‚Äì1464, 2004.
Wang, Zidong, Yang, Fuwen, Ho, Daniel WC, and Liu,
Xiaohui. Robust finite-horizon filtering for stochastic
systems with missing measurements. Signal Processing
Letters, IEEE, 12(6):437‚Äì440, 2005.

