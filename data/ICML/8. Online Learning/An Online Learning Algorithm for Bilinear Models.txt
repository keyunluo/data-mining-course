An Online Learning Algorithm for Bilinear Models

Yuanbin Wu
Shiliang Sun
Shanghai Key Laboratory of Multidimensional Information Processing
Department of Computer Science and Technology, East China Normal University

Abstract
We investigate the bilinear model, which is a matrix form linear model with the rank 1 constraint.
A new online learning algorithm is proposed to
train the model parameters. Our algorithm runs
in the manner of online mirror descent, and gradients are computed by the power iteration. To
analyze it, we give a new second order approximation of the squared spectral norm, which helps
us to get a regret bound. Experiments on two sequential labelling tasks give positive results.

1. Introduction
In supervised classification, linear models are important
and fundamental. Features are packed into a vector, and
a weight in the same vector space is used to vote the importance of different features. However, in some applications of computer vision (Pirsiavash et al., 2009), natural
language processing (Lei et al., 2014) and recommender
systems (Rendle, 2010), matrices are more natural and informative than vectors to express features They can help to
explore latent structures of the input space (e.g., semantic
relations among features), which can potentially improve
the classification performance. In this work, as a specific
case, we will study the bilinear model, which is a matrix
form linear model with the rank 1 constraint.
The rank constraint brings difficulties both on designing
and analyzing learning algorithms. We introduce a simple and fast online algorithm for the bilinear model which
tries to overcome those difficulties. First, models with
low rank constraints usually need singular value decomposition (SVD). The full SVD is computationally unaffordable for large scale matrices. In the case of our bilinear
model, we will rely on the power iteration to compute the
leading singular vectors. By the carefully selected initial
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

YBWU @ CS . ECNU . EDU . CN
SLSUN @ CS . ECNU . EDU . CN

value and normalization factor, we get an efficient update
of singular vectors. Second, since the rank constraint is
non-convex, the framework of online convex optimization
(Shalev-Shwartz, 2012) is not directly applicable for analyzing the learning problem. We give a second order approximation of the squared spectral norm (Proposition 3).
It serves as a complement of the strong smoothness result on the squared Schatten norm (Ball et al., 1994; Duchi
et al., 2010; Kakade et al., 2012). Equipped with this result,
we derive a regret bound of the algorithm.
We conduct experiments on two sequential labelling tasks:
word segmentation and text chunking. The results show
that the prior knowledge expressed by the matrix form feature and the new online learning algorithm can help to build
efficient and competitive models.

2. Related Work
Bilinear models have been applied in computer vision
(Tenenbaum & Freeman, 2000; Pirsiavash et al., 2009).
Major motivations of these works are that it is more natural
to represent images by matrices, and the bilinear formulation can help to reduce the number of parameters and the
risk of overfitting. In natural language processing, although
the matrix feature representation is not as obvious as the intensity matrix of an image, the bilinear models could also
have clear physical interpretations. For example, a tensor
model has been recently proposed by Lei et al. (2014) for
dependency parsing. Different from their work, we give a
solid formulation and analysis of the learning problem.
There are many works on low rank approximations in collaborative filtering (Srebro et al., 2005; Rennie & Srebro, 2005; Wang et al., 2013). It is popular to use the
trace norm as a convex surrogate of rank constraints. In
this work, we deal with a special case of hard rank constraints (rank = 1). The analysis of our learning algorithm will show the relation to the trace norm regularization. Shalev-Shwartz et al. (2011) considered the general
low rank constrained optimization problem with convex
objectives. Compared with that work, we investigate the

An Online Learning Algorithm for Bilinear Models

dual problem which might be more efficient in the case of
rank = 1: instead of computing the leading singular vectors of a “big” gradient matrix, we incrementally compute
singular vectors for a sequence of matrices, and each step
only involves sparse matrix operations.
Another closely related topic is online mirror descent
(Duchi et al., 2010; Kakade et al., 2012; Shalev-Shwartz,
2012). By using different strongly convex functions, it unifies many existing online learning algorithms. In fact, our
algorithm (Eq. 8) runs in the manner of mirror descent.
However, due to the non-convexity of rank constraints,
tools from the online mirror descent framework are not
readily applicable to analyzing our algorithm. To proceed,
we turn to view the proposed method as a dual coordinate
ascent approach (Shalev-Shwartz & Singer, 2006; ShalevShwartz & Kakade, 2008; Shalev-Shwartz & Zhang, 2013).
It increases the dual objective incrementally, and the loss
could be bounded by the weak duality.

the rank 1 constraint, and a bilinear model:
h(x) = arg max α⊺ Φ(x, y)β,

(1)

y∈Y

where α ∈ Rm , β ∈ Rn . The model parameter W = αβ ⊺
is a rank 1 matrix.
The following section contains a concrete example of the
bilinear formulation for sequential labelling, which is a
baseline of many natural language processing tasks. We
show that the rank 1 constraint appears naturally by prior
knowledge, and helps to reduce the number of parameters.
3.3. An Example
For an input sentence x, the sequential labelling task outputs a label sequence y = y1 y2 . . . y|y| ∈ Y , where Y contains all possible such sequences. Let S be the label set,
where yi ∈ S. For simplicity, assume S = {B, I, O} 1 .
We first review the vector form linear model:
h(x) = arg max w⊺ Φ̂(x, y).

3. The Model

(2)

y∈Y

3.1. Notations
For a matrix A ∈ Rm×n , let σ(A) = [σ1 (A), . . . , σl (A)]⊺
be A’s singular values, where σ1 (A) ≥ · · · ≥ σl (A),
l = min{m, n}. Denote ∥A∥F as the Frobenius norm,
∥A∥2 = σ1 as the spectral norm, ∥A∥s(p) = ∥σ(A)∥p as
∑k
the Schatten p-norm, and ∥A∥k(k) =
i=1 σi as the Ky
Fan k-norm. The inner product ⟨A, B⟩ = Tr(A⊺ B). A⊗B
is the Kronecher product. Let F be a real-valued function,
and its Fenchel conjugate is denoted by F ∗ .
3.2. The Bilinear Model

With the first order Markov assumption, let Φ̂(x, y) =
∑|y|
i=1 Φ̂(x, yi , yi−1 ), and h(x) is computed by the standard
Viterbi algorithm.
In natural language processing, Φ̂(x, yi , yi−1 ) are usually
sparse vectors in a high dimensional vector space. They
could be instantiated by a set of feature templates. For example, a template could be “whether ith word of x is v”. It
will expand to a vector, which is indexed by all possible assignments of (yi , yi−1 ) 2 . Here, if position i of x is indeed
v and yi = B, yi−1 = O, the template will expand to

We consider the matrix form linear classifier h : X 7→ Y :
h(x) = arg max Tr(W ⊺ Φ(x, y)),
y∈Y

where X is the input space, Y is the class label set, Φ :
X × Y 7→ Rm×n is the matrix-valued feature function, and
W ∈ Rm×n is the model parameter. When n = 1, we get
the vector form linear model.
In practice, instead of using free W , we may be interested
in models with additional constraints. On the one hand, in
some applications, we have prior knowledge about semantic relations among features, which can help to improve the
classification performances. We would like to encode such
information both in Φ(x, y) and W . On the other hand, by
imposing different matrix constraints on W , we can tailor
parameters to meet the structure of the input space, which
may result in more efficient and compact models.
In this paper, we will explore a specific constraint on W :

[

BB

BI

BO

IB

II

IO

OB

OI

0

0

1

0

0

0

0

0

OO

]
0 .

(3)

Formally, a feature template is a function φ̂ : X × S × S 7→
2
{0, 1}|S| ,
φ̂u,v = P (x) · I (yi = u, yi−1 = v) ,

(4)

where (u, v) is an index of vector φ̂(x, yi , yi−1 ), P (x) is a
boolean function, and I is the indicator function. Given K
templates, Φ̂(x, yi , yi−1 ) is a blocked feature vector:
Φ̂(x, yi , yi−1 ) = [φ̂1 (x, yi , yi−1 )⊺ , . . . , φ̂K (x, yi , yi−1 )⊺ ]⊺ .
Next, we show how to use a blocked diagonal matrix to
represent features, and then exploit a rank 1 constraint to
1

“B”: begin, “I”: inside, “O”: outside.
There are |V | such templates, where V is the vocabulary.
Hence, for each x, we have a sparse (only |x| ≈ 101 active entries) high dimension (|V ||S|2 ≈ 104 ) feature vector. For bigram
features (whether v1 , v2 appears), the dimension will be in order
of 107 .
2

An Online Learning Algorithm for Bilinear Models

get a bilinear model. A simple observation on the feature
template (4) is that φ̂u,v can be decomposed:
φ̂u,v = P (x)I(yi = u) · P (x)I(yi−1 = v).

(5)

Accordingly, we have two separated “views” on a single
feature: one from the current label, the other from the
previous label. It implies that the corresponding weight
could also be decomposed. Furthermore, for a template
φ̂(x, yi , yi−1 ), different instantiations of (yi , yi−1 ) can
share weights if their “views” overlap, so the total number
of parameters is reduced. The following are the details.
Define the matrix feature template φ(x, yi , yi−1 ) =
ζ1 (x, yi ) ⊗ ζ2 (x, yi−1 ), where ζ1 (x, ·), ζ2 (x, ·) : X × S 7→
{0, 1}|S| have elements
ζ1u (x, yi ) = P (x) · I(yi = u)
ζ2v (x, yi−1 ) = P (x) · I(yi−1 = v).
The new feature template will expand to a matrix, rather
than a vector. For example, now (3) is
B

I

O



∑n
Denoting Φ(x, y) = i=1 Φ(x, yi , yi−1 ), we have the matrix form linear model
(6)

Until now, we haven’t changed the linear model. Indeed,
(6) is equal to (2) if W is unconstrained. But it is interesting to investigate W with additional structures. For example, driven by the feature template decomposition (5), it
is natural to question whether we can also decompose the
weight matrix W . In other words, whether it is possible
to assign weights on ζ1 (x, ·), ζ2 (x, ·), rather than φ(x, ·, ·).
These questions lead to a W with rank = 1 constraint, and
we get a bilinear model as (1).
To clarify the decomposition of W , let’s expand the two
discriminant functions (1) and (2) on a single template:
∑
αu · βv · P (x) · I(yi = u) · I(yi−1 = v),
(u,v)∈S×S

∑

(u,v)∈S×S

wu,v P (x) · I(yi = u, yi−1 = v).

Thus, wu,v = αu · βv . Since α, β ∈ R|S|K , the number of
parameters is 2|S|K, which is less than |S|2 K in the case
of the original linear model (assume |S| > 2).
The low rank constraint on W provides a mechanism to
capture special structures of sequential labelling problems
(e.g., the decomposition of feature templates). It is also a
sparsity requirement on the model parameters, which is not
obvious when linear models are otherwise used.

4. Online Learning of the Bilinear Model
4.1. The Algorithm

W =αβ ∈Ω1

Φ(x, yi , yi−1 )≜ diag(φ1 (x, yi , yi−1 ), . . . , φK (x, yi , yi−1 )).

y∈Y

1
1
α(0) = ∥1∥
; β (0) = ∥1∥
; R: number of iterations
for r = 0 to R do
α(r+1) = SVMsolver(α(r) , β (r) )
β (r+1) = SVMsolver(β (r) , α(r+1) )
end for
return α(R) , β (R)

min
⊺

Define the matrix-valued feature function as

h(x)= arg max Tr(W ⊺ Φ(x, y)).

1:
2:
3:
4:
5:
6:

Let {(xj , y j )}N
j=1 be a training set. Consider an SVM with
the bilinear formulation,

 
B
I
O
B 1
B 0 0 1
]
[
0 0 1 .
I 0 0 0= I 0
O 0
O 0 0 0
φ(x, yi , yi−1 ) ζ1 (x, yi ) ζ2⊺ (x, yi−1 )


Algorithm 1 Blockwise Coordinate Descent

N
∑
1
∥W ∥2F + C
L(W ; xj , y j ),
2
j=1

(7)

where L(W ; xj , y j ) = [1 − ⟨W, ∆Φj ⟩]+ , ∆Φj ≜
Φ(xj , y j ) − Φ(xj , ȳ j ), and ȳ j = h(xj ). Let Ωk = {W ∈
R|S|K×|S|K , rank(W ) ≤ k}, and P(W ) be the primal
problem with optimal value p∗ .
Different from the usual linear SVM, (7) is not convex, but
a biconvex problem (Gorski et al., 2007). To solve it, the
straightforward method is blockwise coordinate descent.
When β is fixed, (7) is a linear SVM with parameter α,
and vise versa. The blockwise coordinate descent works
by solving the two SVMs alternately (Algorithm 1).
Blockwise coordinate descent has been widely used for
bilinear problems (Gorski et al., 2007; Pirsiavash et al.,
2009). It suffers the common local optimum problem. Assume that, in an extreme case, (α(1) , β (0) ) has been a local
optimum and no further update on β is needed. Then the
algorithm only solves a linear model. We develop a new
algorithm which solve α, β simultaneously. It incrementally increases the dual function of (7). The idea is builds
on (Shalev-Shwartz & Singer, 2006) and (Shalev-Shwartz
& Kakade, 2008), but now the problem is non-convex and
more work is needed to compute the gradient and bound
the regret. Furthermore, solving from the dual space also
provides some insights on the learning problem.
Let Fk (W ) =

1
2
2 ∥W ∥F

with domain Ωk , and Θt =

An Online Learning Algorithm for Bilinear Models

∑t

ηj ∆Φj . The dual function of (7) is


N
N
∑
∑
1
D(η)=
ηj − max ⟨W,
ηj ∆Φj ⟩ − ∥W ∥2F 
W ∈Ω1
2
j=1
j=1
j=1

=

N
∑

ηj − F1∗ (ΘN ), where ηj ∈ [0, C].

j=1

∑t−1

F1∗ (Θt−1 ).

Denote Dt (η1 , . . . , ηt−1 ) = j=1 ηj −
D(η) = DN +1 (η1 , . . . , ηN ).
Proposition 1. F1∗ (Θ) = 12 ∥Θ∥22 = 12 ∥Θ∥2s(∞) .
Proof. Let

∑
i

Then

σi ui vi⊺ be the SVD of Θ.

1
F1∗ (Θ)= max ⟨W, Θ⟩ − ∥W ∥2F
W ∈Ω1
2
1
1
= − min ∥Θ − W ∥2F + ∥Θ∥2F
W ∈Ω1 2
2
1
1
= − ∥Θ − σ1 u1 v1⊺ ∥2F + ∥Θ∥2F
2
2
1 2
1
2
= σ1 = ∥Θ∥2 .
2
2
The third line is by Eckart-Young-Mirsky theorem.

The initial value and normalization are two important components of the power iteration. The former relates to convergence speed, and the latter affects the numerical stability. In the case of Θt , if the feature matrices Φ are sparse
(e.g., the sequential labelling example), instead of choosing
a random initial value and normalizing to the unit ball, we
can have better strategies.
For the initial value, recalling that Θt = Θt−1 + C∆Φt ,
one would expect that αt is close to αt−1 if C∆Φt is not a
“big” change. In fact, Wedin sin theorem (Demmel, 1997)
states that sin θ is bounded by ∥C∆Φt ∥, where θ ∈ [0, π/2]
is the angle between αt−1 and αt . Thus, αt−1 , βt−1 could
be good initial values.

Similar to the online mirror descent, at round t (1 ≤ t ≤
T ), our online learning algorithm runs as follows 3 :
⊺
• uses Wt−1 = αt−1 βt−1
to predict xt , ȳ t = h(xt );

• sets the dual variable ηt as
{
0 ȳ t = y t
ηt =
;
C ȳ t ̸= y t

Θ⊺ Θα(τ ) (with normalization α(τ +1) /∥α(τ +1) ∥) will converge to u1 . Similarly, β (τ +1) = ΘΘ⊺ β (τ ) will converge
to v1 . If β (0) is set to Θα(0) , we can also compute u1 , v1 at
the same time: α(τ +1) = Θβ (τ ) , β (τ +1) = Θ⊺ α(τ +1) . The
(Θ)
convergence speed is determined by σσ12 (Θ)
and α(0) , β (0)
(see Golub & Van Loan (1996), Theorem 8.2.1).

For normalization, when αt , βt are dense vectors in a high
dimensional space, normalization α(τ +1) /∥α(τ +1) ∥ will be
time-consuming. Note that only directions of singular vectors are important, and if the feature Φ is sparse, we can
use an alternative normalization to speedup. At round t, assume ∥αt−1 ∥ = ∥βt−1 ∥ = 1. Let’s consider the first multiplication of the power iteration with initial value βt−1 :
(1)
αt = (Θt−1 + C∆Φt )βt−1 = σ1 αt−1 + C∆Φt βt−1 ,
(1)
where σ1 = σ1 (Θt−1 ). We normalize αt by dividing σ1 :
(1)

(8)

• updates Wt : Wt = ∇F1∗ (Θt ) = αt βt⊺ .
Note that we need to compute the gradient of F1∗ . In general, when p = 1 or ∞, the Schatten p-norm is not differentiable if there are singular values with multiplicity greater
than 1. The following proposition from Watson (1992)
will help to compute Wt (see Theorem 2 and Example 1
therein).
∑
Proposition 2. Let Θ have SVD i σi ui vi⊺ . If σ1 ̸= σ2 ,
then F1∗ is differentiable at Θ, and ∇F1∗ (Θ) = σ1 u1 v1⊺ .
4.2. The Power Iteration
Updates of αt , βt need u1 , v1 of Θt . We use the power iteration to compute them. Roughly, for a matrix Θ and an initial value α(0) , if σ1 (Θ) ̸= σ2 (Θ), the sequence α(τ +1) =
3
Rigorously, dual variables are ηt,y , where y is any possible
output sequence of xt . Here we set ηt ≜ ηt,ȳt . Also in (8), we
set ηt,y = 0 for y ̸= ȳ t , y t .

ᾱt

= αt−1 +

1
C∆Φt βt−1 .
σ1
|
{z
}
∆α(1)
(1)

After the second multiplication, βt
= (Θt−1 +
(1)
C∆Φt )⊺ ᾱt = σ1 βt−1 + C(∆Φt )⊺ αt−1 + Θ⊺t ∆α(1) .
(1)
Again, we normalize βt by σ1 :
)
1 (
(1)
β̄t = βt−1 +
C(∆Φt )⊺ αt−1 + Θ⊺t ∆α(1) .
σ
|1
{z
}
∆β (1)

We can write the general update equations:
)
1 (
∆α(τ ) =
C(∆Φt )βt−1 + Θt ∆β (τ −1) ,
σ1
(τ )
ᾱt = αt−1 + ∆α(τ ) ,
)
1 (
C(∆Φt )⊺ αt−1 + Θ⊺t ∆α(τ ) ,
∆β (τ ) =
σ1
(τ )
β̄t = βt−1 + ∆β (τ ) .

(9)
(10)
(11)
(12)

In (9-12), we only update ∆α, ∆β. If ∆Φ is a sparse matrix, ∆α, ∆β are also sparse. Thus, instead of visiting
every entry of αt−1 , βt−1 , we update entries in ∆α, ∆β,

An Online Learning Algorithm for Bilinear Models

Algorithm 2 Online Learning of the Bilinear Model
Training set:{xj , y j }N
j=1
number of iterations: T , model parameter: C, number of power
iterations: R
1
1
1: α = ∥1∥
, β = ∥1∥
2: for t = 0 to T do
3:
for j = 0 to N do
4:
ȳ j = arg maxy∈Y α⊺ Φ(xj , y)β
5:
if ȳ j ̸= y j then
6:
Θt = Θt−1 + C∆Φt
7:
∆α(0) = ∆β (0) = 0 //Power Iteration
8:
for τ = 0 to R do
9:
ᾱ(τ ) = α + ∆α(τ ) , by Eq. (9)
β̄ (τ ) = β + ∆β (τ ) , by Eq. (12)
10:
11:
end for
(R)
(R)
12:
α = ∥ᾱ
, β = ∥β̄β̄ (R) ∥ //Wt = ∇F1∗ (Θt )
ᾱ(R) ∥
13:
end if
14:
end for
15: end for
16: return α, β

which is more efficient. When the power method converges
after R iterations (in experiments, R = 4 is enough to con(R)
(R)
verge), we set αt = αt , βt = βt .
In a word, with the carefully selected initial value and normalization method, the power iteration is an efficient procedure, which only manipulates sparse matrices. The algorithm is summarized in Algorithm 2 4 .
4.3. Extensions
We give two extensions for the bilinear model and the online learning algorithm.
First, instead of using the bilinear model alone, we can easily incorporate linear models for the 0 order features
h(x) = arg max w⊺ Φ̂(x, y) + α⊺ Φ(x, y)β,
y∈Y

∑

where Φ̂(x, y) = i φ̂(x, yi ) is the 0 order feature vector.
The learning algorithm could also be modified correspondingly. The dual objective D(η) now becomes:
N
∑
j=1

ηj − F1∗ (ΘN ) − max(⟨w,
w

N
∑

1
ηj ∆Φ̂j ⟩ − ∥w∥2 ).
2
j=1

For α, β, Algorithm 2 is unchanged, and for w, we have
wt = wt−1 + C∆Φ̂(xt , y t ).
The second extension is about averaging parameters. In
previous works on online convex optimization, averaging
is a simple method for online batch conversion, and the averaged parameter usually performs better. However, due to
the non-convexity of the bilinear formulation, the averaged
4

More implementation details are in the supplementary.

W may not be in Ω1 (the sum of rank 1 matrices may not
have rank 1). Heuristically, instead of directly averaging
W , we can average α and β individually.

5. Analyses
With loss function Lt at round t (the hinge loss here), the
regret of an online game against a given strategy U is
RN (U ) =

N
N
1 ∑
1 ∑
Lt (Wt ) −
Lt (U ).
N t=1
N t=1

To analyze the regret, we will generally follow the analysis of dual coordinate ascent algorithms. Before starting,
it is worth pointing out that, instead of analyzing the dual
problem, the online mirror descent framework has provided
uniform regret and generalization results for various online learning algorithms (Kakade et al., 2012). However,
our bilinear model does not belong to this family. In fact,
F1 (W ) = 21 ∥W ∥2F is no longer a convex function under
the rank constraint, and F1∗∗ = 21 ∥W ∥22 ̸= F1 .
Denote ∆t = Dt+1 (η1 , . . . , ηt ) ∑
− Dt (η1 , . . . , ηt−1 ). By
N
weak duality, D(η1 , . . . , ηN ) = t=1 ∆t ≤ p∗ . We will
show that, with ηt in (8), ∆t is bounded from below. Our
discussions will focus on the case ηt = C (when ηt = 0,
no update on Wt−1 ). Expand ∆t as
1
1
∆t =C − ∥Θt−1 + C∆Φt ∥22 + ∥Θt−1 ∥22 .
2
2
Proposition 3. Let Θ ∈ Rm×n , l = min(m, n), F1∗ (Θ) =
1
2
2 ∥Θ∥2 . If σ1 (Θ) ̸= σ2 (Θ) > 0, the following second
order approximation holds in a neighborhood of Θ:
F1∗ (Θ + E) ≤ F1∗ (Θ) + ⟨∇F1∗ (Θ), E⟩ + ∥E∥2F

2l
,
1 − σ̂σ̂12

where [σ̂1 , . . . , σ̂l ] = σ(Θ̂), and Θ̂ = Θ + θE, θ ∈ (0, 1).
The proof is given in Section 7.1. To compare with online
mirror descent algorithms, let’s consider the widely used
result about the strong smoothness of the squared Schatten
norm (Ball et al., 1994; Kakade et al., 2012). Namely, for
p ∈ [2, ∞], p1 + 1q = 1,
∥E∥2s(q)
1
1
∥Θ + E∥2s(p) ≤ ∥Θ∥2s(p) + ⟨∇∥Θ∥s(p) , E⟩ +
.
2
2
2(q − 1)
When p = ∞ (the case of F1∗ ), the bound is trivial. Kakade
et al. (2012) approximated this case by using a finite but
sufficiently large p; however, the naive computation of gradients becomes expensive (roughly, it needs the full SVD
of Θ). Proposition 3 provides a new (local) bound with
respect to the Frobenius norm, which helps to derive the
forthcoming regret. .
Let σ(Θt ) = [σ1t , . . . , σlt ], Θ̂t = Θt−1 + θt C∆Φt with
θt ∈ (0, 1) and σ(Θ̂t ) = [σ̂1t , . . . , σ̂lt ].

An Online Learning Algorithm for Bilinear Models

Proposition 4 (Regret). Assume for all Θ = Θt−1 , E =
C∆Φt , Proposition 3 holds. Then
N
1
2lC ∑ ∥∆Φt ∥2F
2
RN (U ) ≤
∥U ∥F +
.
2CN
N t=1 1 − σ̂2tt
σ̂

6. Experiments
We present experiments on two sequential labelling tasks
(word segmentation and chunking) for which the formulation in Section 3.3 is used.

1

Proof. Note that ⟨∇F1∗ (Θt−1 ), ∆Φt ⟩ ≤ 0, we have ∆t ≥
2lC 2 ∥∆Φt ∥2F
CL(Wt−1 ; xt , y t ) −
by Proposition 3. Sumσ̂ t
1−

2
t
σ̂1

ming over t and by weak duality, the proof is complete.
σt

We can see that σ2t controls not only the convergence speed
1
of the power iteration, but also the regret of the online
learning algorithm: the larger gap, the tighter regret bound.
Thus, if rank 1 approximation is reasonable for our problem, which means σ1 dominates other singular values, the
proposed algorithm is expected to be efficient.
σ̂ t

Next, we continue to give a concrete bound of σ̂2t . We
1
will make a separable assumption on samples. Define that
a matrix
γ with respect to a norm ∥ · ∥ if
[ W has margin
]
W
j
minj ⟨ ∥W
∥ , ∆Φ ⟩ ≥ γ.

Proposition 5. Assume that supj,W ∥∆Φj ∥2 ≤ M1 ,
supj,W ∥∆Φj ∥k(2) ≤ M2 . If M1 > M22 and ∃W̃ has margin γ w.r.t. ∥ · ∥s(1) , where γ ∈ ( M22 , M1 ), then
σ̂2t
M2 − γ
≤
.
t
σ̂1
γ
Proposition
The proof is given in Section 7.2. Combining
√
4 and 5 and noting that ∥∆Φt ∥F ≤ l∥∆Φt ∥2 , we have
the following corollary.
Corollary 6. Assume the conditions in Propositions 4 and
5 hold, the regret is bounded by
RN (U ) ≤

γ
1
∥U ∥2F + 2Cl2 M12
.
2CN
2γ − M2

Let’s have a closer look at the two conditions of Proposition 5. First, a sufficient condition for M1 > M22 is that
there exits δ > 0, such that for every j, W , σ1 (∆Φj ) −
σ2 (∆Φj ) > δ. In other words, σ1 is “uniformly” greater
than σ2 on the input space. Second, it is clear that W̃ exists if and only if the following trace norm minimization
problem has a solution ∥W̃ ∥s(1) < M22 .
1
min . ∥W ∥2s(1) s.t. ⟨W, ∆Φj ⟩ ≥ 1, ∀j.
2
Previous works on low rank constrained problems usually
use the trace norm regularization as an approximation of
rank constraints. Similarly, Corollary 6 says that if the
problem is well-formed in the trace norm regularization situation, our algorithm which deals with the hard rank constraint will have a small regret bound.

6.1. Chinese Word Segmentation (CWS)
Given an input sentence in Chinese (a sequence of characters), CWS systems will output a sequence of words by
grouping its characters. The data set is from the second
SIGHAN Backoff (Emerson, 2005).
We use standard features (Sun et al., 2009; Sun, 2010).
Given the current position i, the templates are words at
i−2, i−1, i, i+1, i+2 and bigrams of them. All of them are
1st order features, and we don’t average the model parameters. The performance is measured by the F1-value. We use
“BIES” to encode segmentation results, “BIE”represent beginning, inside, and end of a word, and “S” is a word with
only on character. Thus, feature numbers of the bilinear
model (107 ) are only 50% of the linear model.
The algorithms for comparison are 5 : “bol” which is
the proposed method with T = 20, C = 1, R = 4,
“bcd” which is the blockwise coordinate descent with
SVM solver in Shalev-Shwartz & Singer (2006) (C = 1).
We also compare state-of-the-art online linear model “sp”
which is the structured perceptron with T = 20, learning
rate C = 1 (note that “sp” can be seen as a solver of the
structured SVM (Freund & Schapire, 1999)).
Figure 1 describes the performances with different training data sizes 6 . In general, our method is the best on pku,
cityu, and as. Especially, when the training set is small,
the advantage of “bol” is more obvious. It suggests that,
compared with “sp”, the prior knowledge on feature functions will be helpful if we are lack of training data. At the
same time, compared with “bol”, the new learning algorithm could prevent the training process being attracted by
a solution near the 0-order model which is less expressive.
In Table 1, we further compare the performances of online learning algorithms with models learned by conditional random fields (CRF) 7 . The “crf2” is one of the most
powerful batch learner for sequential labelling, and “crf1”
enforce the sparsity on CRF models. We also lists the training time on data set as (other data sets are similar). It shows
that “bol” is competitive to “crf2” on pku and msr, and outperforms “crf1” on pku, msr and cityu. But the CRFs need
more time for training (Figure 2).
5
T and C are from Sun (2010), and the value R is selected on
a dev set (10% of the pku training set).
6
We define that one method is better than another if it is better
on 8 points (out of 10) at least.
7
http://crfpp.googlecode.com/

An Online Learning Algorithm for Bilinear Models
pku

0.3

0.30

msr

0.4

0.040
bol
crf2
crf1

0.3

0.2

0.035

0.25

0.2

0.1

bol
sp
bcd

0.030

0.1
0.0
89.7 92.0 92.7 93.2 93.5 93.8 94.0 94.1 94.4 94.4 0.0
91.5 93.3 94.5 95.1 95.7 95.8 96.1 96.2 96.4 96.5
−0.1
−0.1
−0.2

−0.2

−0.3
0.1

−0.3
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

cityu

0.4

0.025

0.15
0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0.020

1.0

0.015

as

0.3

0.3

0.20

0.10
0.010

0.2

0.2

0.05

0.1

0.005

0.1
0.0
0.0
88.5 91.1 92.6 93.3 93.8 93.9 93.9 94.0 94.1 94.2
87.5 89.6 90.7 91.5 92.1 92.5 92.7 93.5 93.8 94.0 −0.1
−0.1

0.00
0

20

40

60

80

0.000
100
0

20

40

60

80

100

−0.2

−0.2

bol
−0.3
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

−0.3
0.1

0.2

0.3

bcd
0.4

0.5

sp
0.6

0.7

0.8

0.9

1.0

Figure 1. Performances on word segmentation. x-axis is the proportion of training set. The horizontal line y = 0 is the average F1
value among algorithms, and y-axis is the offset against average.

Figure 2. Convergence speeds on the pku data set. x-axis is the
number of iterations, y-axis is the label error rate. “bol” converges
much faster than CRFs, and it is slightly slower than “sp”.
Chunking

0.3
0.2

Models
bol
bcd
sp
crf2
crf1

pku
94.6
94.4
94.3
93.3
92.5

msr
96.5
96.3
96.7
96.5
96.1

F1
cityu
94.0
93.9
94.1
94.2
93.5

as
94.3
94.2
94.2
94.6
95.0

Training Time
×103 s
2.4
2.4
1.5
9.8
8.3

0.1
0.0
90.2 91.4 92.2 92.7 92.8 93.0 93.2 93.3 93.4 93.6
−0.1
−0.2
−0.3
bol
−0.4
0.1

Table 1. Comparison with batch learners. “crf2” is the CRF with
L2 regularization, and “crf1” is the CRF with L1 regularization.

For the power iteration, we examined its convergence by
sampling at different rounds on the dev set. The conclusion
is that a small R (≤ 4) is sufficient to converge. Thus, for
this particular problem, the singular vectors of Θt are actually close to those of Θt−1 , and the power iteration is efficient with the initial value we have chosen. Note that, even
if the singular vectors change severely, our initial value still
helps to avoid manipulations on dense vectors, which may
also speed up the iteration.
6.2. Text Chunking
The task of text chunking divides a sentence in syntactically correlated parts of words (e.g., noun phrase, verb
phrase). We conduct experiments on the CoNLL Sharedtask 2000 (Sang & Buchholz, 2000). Different from the
word segmentation task, in order to show the extendability
of our model and algorithm, we include both the 1 order
and the 0 order features, and average the parameters. The
tag set size is 23 (e.g., B-NP, I-NP, B-VP, I-VP). The results
(Figure 3) are similar to the task of word segmentation in

0.2

0.3

bcd
0.4

0.5

sp
0.6

0.7

0.8

0.9

1.0

Figure 3. Performances on chunking. x-axis represents the proportion of training set. y-axis is the F1-value.

general, which means that the two extensions to the basic
bilinear model are effective.

7. Proofs
7.1. The Proof of Proposition 3
The following expression of a Hessian matrix is fundamental for our analysis. ((Overton & Womersley, 1995), more
general results are given in (Lewis & Sendov, 2001))
Let A(a) be an n × n symmetric matrix-valued function,
a ∈ Rm . Assume A(a) is twice continuously differentiable, with the second derivative satisfying a Lipschitz
condition on a. The eigenvalues of A(a) are λ1 (a) ≥ · · · ≥
λn (a).
Proposition 7. Let A(a) have eigen decomposition at â
A(â) = Q̂Diag(λ1 (â), . . . , λn (â))Q̂⊺ ,

where Q̂ = [qˆ1 , qˆ2 , . . . , qˆn ], Q̂⊺ Q̂ = I. If λ(â) are distinct,

An Online Learning Algorithm for Bilinear Models

then the second derivative of λd (a) (1 ≤ d ≤ n) at â is
⊺ ∂A(â)
⊺ ∂A(â)
∂ax q̂s q̂d ∂au q̂s

∑ q̂d
∂ 2 λd (â)
∂ 2 A(â)
= q̂d⊺
q̂d + 2
∂ax ∂au
∂ax ∂au
s̸=d

λd − λs

Proof. Assume w.l.o.g. l = m. The Taylor expansion
(with the Lagrange remainder) of F1∗ at Θ is
.

Given a matrix B ∈ √
Rm×n , B can be seen as a function
of vec(B). σd (B) = λd (B ⊺ B). We have the following
corollary of Proposition 7.
Corollary 8. Assume that B ∈ Rm×n , b = vec(B), and B
has singular value decomposition P ΣQ⊺ , where

F1∗ (Θ + E) = F1∗ (Θ) + ⟨∇F1∗ (Θ), E⟩ + vec(E)⊺ H(Θ̂)vec(E),

where H is the Hessian matrix. The aim is to bound the
remainder. By the chain rule and Corollary 8 with d = 1,
d2 F1∗ dσ1
dσ1
=
⊗
+ ∥Θ∥2 C + D.
dΘ2
dΘ
dΘ
The convexity of ∥Θ∥2 implies that D ⪰ 0. Since D is also
symmetric, we have ∥D∥2 ≤ Tr(D). It is easy to show that
Tr(D)= l +

P = [p1 , p2 , . . . , pm ], Q = [q1 , q2 , . . . , qn ]
Σ= Diag(σ1 , . . . , σl ), l = min(m, n).

l
∑
σ2 + σ2
1
2
σ
s=2 1

−
(

s
σs2

≤ l + (l − 1) 1 +

If σ(B) are distinct, then the Hessian matrix of σd w.r.t. b
is C + σ1d D, where C ⪯ 0 and D has entry Dxy,uv :

≤l+

l
∑
σ1 + σs

)

s=2

σ1
σ2

2
−1

σ1 − σs
.

By Theorem 2 of Watson (1992),
(
)

∑
+ qsy pxd σd )(qdv pus σd + qsv pud σs ) vec(E)⊺ H(Θ̂)vec(E)≤ ∥E∥2F ∥ dσ1 ⊗ dσ1 ∥2 + ∥D∥2 
y v

qd qd I(x = u) +
,
dΘ
dΘ
Θ̂
σd2 − σs2
s̸=d
2l
2
≤ ∥E∥F
.
1 − σ̂σ̂21
where xy, uv are indices of b, and py is the y-th entry of p.
(qdy pxs σs

Proof. Denote A = B ⊺ B, A = (aij ), B = (bxy ). By the
chain rule
dσd
dσd dλd (A)
1 dλd (A)
=
=
,
db
dλd (A) db
2σd db
1 dλd (A) dλd (A) 1 1 d2 λd (A)
d2 σd
=− 3
⊗
+
.
2
2
db
4σd db
db
σd |2 db
{z
}
|
{z
}
D

C

Note that aij =

∑

i = y, j
i ̸= y, j
i = y, j
i ̸= y, j

̸= y
=y
.
=y
̸= y

We have
qd⊺

Proof. We first give a lower bound on σ̂1t . By von Neumann’s inequality (Bhatia, 1997), for any W, Θ,
⟨W, Θ⟩ ≤ ⟨σ(W ), σ(Θ)⟩ ≤ ∥W ∥s(1) ∥Θ∥s(∞) .
Let W = W̃ , Θ = Θ̂t . We have
σ̂1t ≥

t−1
∑
1
C
⟨W̃ , Θ̂t ⟩ =
⟨W̃ ,
∆Φj + θt ∆Φt ⟩
∥W̃ ∥s(1)
∥W̃ ∥s(1)
j=1

≥ (t − 1 + θt )Cγ.

k bki bkj ,


bxj


(
)

∂A
bxi
=
 2bxy
∂bxy ij 

0

7.2. The Proof of Proposition 5

∑
∂A
bxi (qdy qsi + qdi qsy ) = qdy pxs σs + qsy pxd σd .
qs =
∂bxy
i

Similarly,

Then,
1+

(t − 1 + θt )CM2
M2
σ̂2t ∥Θ̂t ∥k(2)
=
≤
≤
.
σ̂1t
σ̂1t
σ̂1t
γ

Rearranging the equation leads to the result. We remark
that γ > M22 is necessary for a non-trivial bound; on the
other hand, W̃ has margin γ implies that
1
⟨W̃ , ∆Φj ⟩ ≤ ∥∆Φj ∥2 ≤ M1 .
γ≤
∥W̃ ∥s(1)

8. Conclusion
∂A
qd⊺
qd = 2qdy qdv I(x = u).
∂bxy ∂buv

Using Proposition 7 completes the proof.
For a matrix A, let vec(A) = [a⊺1 , a⊺2 , . . . , a⊺n ]⊺ , where ai
are columns of A. The following is the proof of Proposition
3.

We presented a bilinear model with matrix features. A simple online algorithm was derived and analyzed. Empirical
results on sequential labelling tasks showed that the proposed method is competitive and efficient. In future work,
it is interesting to explore models with rank ≤ k (if k > 1,
we need an efficient algorithm to compute ∇Fk∗ , which is
roughly the first k singular vectors). And it is meaningful
to bound σσ12 under other conditions.

An Online Learning Algorithm for Bilinear Models

Acknowledgments
The authors wish to thank the reviewers for their helpful
comments and suggestions. This research is supported by
NSFC (61402175, 61370175), IIPL (IIPL-2014-008), and
STCSM (14DZ2260800).

References
Ball, Keith, Carlen, Eric A., and Lieb, Elliott H. Sharp
uniform convexity and smoothness inequalities for trace
norms. Inventiones mathematicae, 115:463–482, 1994.
Bhatia, Rajendra. Matrix Analysis. Springer New York,
1997.

Rendle, Steffen. Factorization machines. In ICDM, pp.
995–1000, 2010.
Rennie, Jason D. M. and Srebro, Nathan. Fast maximum
margin matrix factorization for collaborative prediction.
In ICML, pp. 713–719, 2005.
Sang, Erik F. Tjong Kim and Buchholz, Sabine. Introduction to the CoNLL-2000 shared task: Chunking. In Proc.
of CoNLL and LLL, 2000.
Shalev-Shwartz, Shai. Online learning and online convex optimization. Foundations and Trends in Machine
Learning, 4(2):107–194, 2012.

Demmel, James W. Applied Numerical Linear Algebra.
Society for Industrial and Applied Mathematics, 1997.

Shalev-Shwartz, Shai and Kakade, Sham M. Mind the duality gap: Logarithmic regret algorithms for online optimization. In NIPS, pp. 1457–1464, 2008.

Duchi, John C., Shalev-Shwartz, Shai, Singer, Yoram, and
Tewari, Ambuj. Composite objective mirror descent. In
COLT, pp. 14–26, 2010.

Shalev-Shwartz, Shai and Singer, Yoram. Online learning
meets optimization in the dual. In COLT, pp. 423–437,
2006.

Emerson, Thomas. The second international Chinese word
segmentation bakeoff. In the Second SIGHAN Workshop
on Chinese Language Processing, pp. 123 – 133, 2005.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual
coordinate ascent methods for regularized loss. JMLR,
14:567–599, 2013.

Freund, Yoav and Schapire, Robert E. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296, 1999.

Shalev-Shwartz, Shai, Gonen, Alon, and Shamir, Ohad.
Large-scale convex minimization with a low-rank constraint. In ICML, pp. 329–336, 2011.

Golub, Gene H. and Van Loan, Charles F. Matrix Computations (3rd Ed.). Johns Hopkins University Press, 1996.

Srebro, Nathan, Rennie, Jason D. M., and Jaakola,
Tommi S. Maximum-margin matrix factorization. In
NIPS, pp. 1329–1336, 2005.

Gorski, Jochen, Pfeuffer, Frank, and Klamroth, Kathrin.
Biconvex sets and optimization with biconvex functions:
a survey and extensions. Mathematical Methods of Operations Research, 66:373–407, 2007.

Sun, Weiwei. Word-based and character-based word segmentation models: Comparison and combination. In
Coling: Posters, pp. 1211–1219, 2010.

Kakade, Sham M., Shalev-Shwartz, Shai, and Tewari, Ambuj. Regularization techniques for learning with matrices. JMLR, 13:1865–1890, 2012.
Lei, Tao, Xin, Yu, Zhang, Yuan, Barzilay, Regina, and
Jaakkola, Tommi. Low-rank tensors for scoring dependency structures. In ACL, pp. 1381–1391, 2014.
Lewis, Adrian S. and Sendov, Hristo S. Twice differentiable spectral functions. SIAM Journal on Matrix Analysis and Applications, 23:368–386, 2001.
Overton, Michael L. and Womersley, Robert S. Second
derivatives for optimizing eigenvalues of symmetric matrices. SIAM Journal on Matrix Analysis and Applications, 16:697–718, 1995.
Pirsiavash, Hamed, Ramanan, Deva, and Fowlkes, Charless. Bilinear classifiers for visual recognition. In NIPS,
2009.

Sun, Xu, Zhang, Yaozhong, Matsuzaki, Takuya, Tsuruoka,
Yoshimasa, and Tsujii, Jun’ichi. A discriminative latent
variable Chinese segmenter with hybrid word/character
information. In NAACL, pp. 56–64, 2009.
Tenenbaum, Joshua B. and Freeman, William T. Separating
style and content with bilinear models. Neural Computation, 12:1247–1283, 2000.
Wang, Jialei, Hoi, Steven C.H., Zhao, Peilin, and Liu, ZhiYong. Online multi-task collaborative filtering for onthe-fly recommender systems. In Proceedings of the 7th
ACM Conference on Recommender Systems, pp. 237–
244, 2013.
Watson, G.A. Characterization of the subdifferential of
some matrix norms. Linear Algebra and its Applications,
170:33 – 45, 1992.

