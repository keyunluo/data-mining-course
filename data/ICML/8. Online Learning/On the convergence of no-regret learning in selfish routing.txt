On the convergence of no-regret learning in selfish routing

Walid Krichene
University of California, 652 Sutardja Dai Hall, Berkeley, CA 94720 USA
Benjamin DrigheÌ€s
Ecole Polytechnique, Route de Saclay, 91120 Palaiseau, France

BENJAMIN . DRIGHES @ POLYTECHNIQUE . EDU

Alexandre Bayen
University of California, 642 Sutardja Dai Hall, Berkeley, CA 94720 USA

Abstract
We study the repeated, non-atomic routing game,
in which selfish players make a sequence of routing decisions. We consider a model in which
players use regret-minimizing algorithms as the
learning mechanism, and study the resulting dynamics. We are concerned in particular with
the convergence to the set of Nash equilibria of
the routing game. No-regret learning algorithms
are known to guarantee convergence of a subsequence of population strategies. We are concerned with convergence of the actual sequence.
We show that convergence holds for a large class
of online learning algorithms, inspired from the
continuous-time replicator dynamics. In particular, the discounted Hedge algorithm is proved to
belong to this class, which guarantees its convergence.

1. Introduction
Routing games are important in modeling and understanding the interaction of non-cooperative players who share
resources, such as roads in a road network and links in
a communication network. They have been studied extensively, including the seminal work of Beckmann et al.
(1955) and Dafermos & Sparrow (1969). In a one-shot scenario, selfish players choose the routes that minimize their
individual travel time. One solution concept to the game is
the Nash equilibrium, also called Wardrop equilibrium in
the traffic literature (1952). In some classes of games, Nash
equilibria can be hard to compute and have been questioned
as a realistic equilibrium concept, for example by PapadimProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

WALID @ EECS . BERKELEY. EDU

BAYEN @ BERKELEY. EDU

itriou (1994). By contrast, for one-shot non-atomic routing
games, Nash equilibria are known to be easy to compute
as they can be expressed as the solution to a convex optimization problem, using a convex potential function, due
to Rosenthal (1973). This is an argument in favor of the
one-shot routing game model. However, most realistic scenarios do not correspond to a one-shot game, but rather a
repeated game, in which players make a sequence of routing decisions and may adapt their strategies given the outcome on previous days. Therefore studying the repeated
routing game is important to understand how players can
arrive at the equilibrium. Arguably, a good learning model
for the population of players should be distributed and easy
to implement by individual players. A natural framework
is that of online learning.
No-regret learning is of particular interest, given its generality and ease of implementation, and the fact that it only
requires the current losses to be revealed. The Hedge algorithm is one example of no-regret learning, introduced for
Machine Learning by Freund & Schapire (1999), a generalization of the weighted majority algorithm of Littlestone &
Warmuth (1989). Cesa-Bianchi & Lugosi (2006) give convergence results, together with convergence rates, for noregret algorithms. These results hold for a broad class of
games. However, they guarantee convergence of the timeaveraged strategies, and not the actual sequence of strategies.
Other learning processes have been studied for repeated
routing games, such as fictitious play by Monderer & Shapley (1996), adaptive sampling by Fischer et al. (2010) or
continuous-time replicator dynamics by Fischer & VoÌˆcking
(2004), which is also of particular interest in evolutionary
game theory, see for example Weibull (1997). For some
classes of continuous-time dynamics, convergence of the
actual sequence is guaranteed. For example, Sandholm
proved convergence in continuous-time potential games for
a class of evolutionary dynamics which satisfy a positive

On the convergence of no-regret learning in selfish routing

correlation condition (2001). Blum et al. proved in (2006)
that under no-regret learning, the resulting sequence of
population strategies converges, on a 1 âˆ’  fraction of days,
to the set of -approximate Nash equilibria, and they gave
explicit convergence rates that depend on the Lipschitz constant of the latency functions.
We are concerned with convergence of the actual sequence
of strategies (as opposed to a subsequence). Our approach
combines ideas from evolutionary dynamics and no-regret
learning. In Sections 2 and 3, we present the model and
summarize existing results. In Section 4, we prove that
convergence holds for a class of algorithms, which have
sublinear discounted regret, and which can be viewed as a
generalization of replicator dynamics.

2. The model
2.1. Non-atomic routing game
We consider a set X of players. A routing game is a noncooperative game played on a directed graph G = (V, E)
representing a network, and in which a pure strategy corresponds to a directed path on the graph. To formalize the
notion of non-atomicity, we endow X with a structure of
measurable space (X , M, m), where M is a Ïƒ-algebra of
measurable sets, and m is a measure. The set of players
is said to be non-atomic if each single player x âˆˆ X is
negligible for m.
We consider a setting similar to (Wardrop, 1952), in which
the set of players is partitioned in populations or commodities X = tK
k=1 Xk , where each Xk is measurable and has
positive finite measure. Formally, the model is defined by
the tuple (E; K; (Xk )kâˆˆ[K] ; (Pk )kâˆˆ[K] ; (ce )eâˆˆE ), where E
denotes the finite set of edges, K is the number of commodities, [K] denotes the set {1, . . . , K}, and for all k,
Pk âŠ† P (E) is a set of paths (pure strategies) available to
players in Xk . For each k, all paths in Pk have a common
source sk âˆˆ V and a common destination tk âˆˆ V. We will
denote P the disjoint union P = tK
k=1 Pk . The positive
measure of Xk is denoted Fk = m(Xk ) and called the total
flow of Xk . For each edge e, ce (Â·) : R+ â†’ R+ is an edge
latency function satisfying the following assumption:
Assumption 1. The latency functions ce are assumed to be
continuous, non-decreasing, and locally Lipschitz.
At a microscopic scale, the joint action of all players in X
can be represented by an action profile A : x âˆˆ X 7â†’ A(x)
which maps a player x âˆˆ Xk to a path A(x) âˆˆ Pk . This
function is assumed to be M-measurable, and defines, for
each population Xk , a path distribution Âµk = (Âµkp )pâˆˆPk ,
R
where Âµkp = F1k Xk 1{A(x)=p} dm(x) is the fraction of
players utilizing path p âˆˆ Pk . We have ÂµkP
âˆˆ âˆ†Pk , the
Pk
Pk
simplex on Pk , that is, âˆ† = {u âˆˆ R+ :
pâˆˆPk up =

v0

v1
v5
v4
v6

v2

v3

Figure 1. Example of a network with K = 2 populations. Population 1 travels from v0 to v1 , and population 2 travels from v2 to
v3 .

1}. The pure strategy profile can be summarized at a
macroscopic scale using the product of distributions Âµ =
(Âµ1 , Â· Â· Â· , ÂµK ) âˆˆ âˆ†P1 Ã— Â· Â· Â· Ã— âˆ†PK . The product of simplexes will be denoted âˆ†.
The distribution Âµ P
determinesPthe edge flow or load,
K
k
defined as Ï†e =
This can
k=1 Fk
pâˆˆPk :eâˆˆp Âµp .
be written compactly as Ï†e = (M Âµ)e where M =
[M 1 | . . . |M K ] âˆˆ REÃ—P is a weighted incidence matrix:
(
Fk if e âˆˆ p
k
âˆ€e âˆˆ E, âˆ€k âˆˆ [K], âˆ€p âˆˆ Pk Me,p =
0
otherwise
For each edge e, the edge load determines the edge latency,
given by ce (Ï†e ). Finally, the loss of a player who chooses
path p is simply the sum of edge latencies along the path,
P
eâˆˆp ce (Ï†e ). This latency is entirely determined by the
distribution Âµ, so we define a path latency
P function (or loss
function) `p : Âµ âˆˆ âˆ† 7â†’ `kp (Âµ) = eâˆˆp ce ((M Âµ)e ). Finally, we write `k (Âµ) to denote the vector of path latencies
(`kp (Âµ))pâˆˆPk , and `(Âµ) = (`1 (Âµ), . . . , `K (Âµ)).
2.2. Nash equilibria and the Rosenthal potential
function
Given this setting, we now define Nash equilibria of the
routing game.
Definition 2.1 (Nash equilibrium).
A distribution Âµ âˆˆ âˆ† is a Nash equilibrium if for every
population k, whenever Âµkp > 0 for some path p âˆˆ Pk ,
then `kp0 (Âµ) â‰¥ `kp (Âµ) for all p0 âˆˆ Pk . We will denote by
N âŠ‚ âˆ† the set of Nash equilibria.
The definition implies that, for a commodity k, all paths
with non-zero mass have equal latencies and paths with
zero mass have larger latencies.
There is a natural potential function that allows one to formulate the problem of computing the set N of Nash equilibria as the solution of a convex optimization problem.
Consider the function
X Z (M Âµ)e
V (Âµ) =
ce (u)du
(1)
eâˆˆE

0

On the convergence of no-regret learning in selfish routing

The gradient of V is the vector of path latencies:
âˆ€k, âˆ€p âˆˆ Pk ,

âˆ‚V
(Âµ) = Fk `kp (Âµ)
âˆ‚Âµkp

3.1. The online learning framework
(2)

Theorem 1. (Rosenthal, 1973) N is the set of minimizers
of V in âˆ†. It is a non-empty convex compact set. We denote
VN the value of V on N .
A proof can be found for example in (Roughgarden, 2007).
As a result of Theorem 1, computing the Nash equilibria
of the routing game can be done efficiently by minimizing the potential. However, the idea of minimizing a potential function cannot be directly applied to designing a
distributed learning algorithm, as it would a priori require
coordination between players.
2.3. Restricted Nash equilibria
In the analysis, we use a weaker notion of equilibrium, introduced by Fischer & VoÌˆcking in (2004).
Definition 2.2 (Restricted Nash equilibrium). A product
distribution Âµ is a restricted Nash equilibrium if all paths
with non-zero mass have equal latencies for each commodity i.e. for all k and all p, p0 âˆˆ Pk such that Âµkp , Âµkp0 > 0,
`kp (Âµ) = `kp0 (Âµ). We will denote RN the set of restricted
Nash equilibria.
Such an equilibrium is restricted in the sense that it would
be a Nash equilibrium of the routing game if we restricted
the set of paths to its support (Fischer & VoÌˆcking, 2004).
Remark 1. Restricted Nash equilibria are also minimizers of the potential function V if we restrict the feasible
set to distributions with the same support. As the number
of supports is finite, the set V (RN ) of potential values of
restricted Nash equilibria is also finite.

3. No-regret learning in the repeated routing
game
Algorithm 1 Online learning setting
Input: For every player x âˆˆ Xk , a learning algorithm (hxÏ„ )Ï„ and
initial distribution Ï€ (0) (x) âˆˆ âˆ†Pk .
1: for each time step Ï„ do
2:
Every player x independently draws a path A(Ï„ ) (x) âˆ¼
Ï€ (Ï„ ) (x).
3:
For all k, the vector of path losses `k (Âµ(Ï„ ) ) is revealed to
players in Xk . Players incur losses corresponding to their
path choice.
(Ï„ +1)
4:
Every
(x) =
 player updates her strategy: Ï€
hxÏ„ (`k (Âµ(t)))tâ‰¤Ï„ , Ï€ (Ï„ ) (x) .

We now define the online learning setting. Assume players
make decisions repeatedly, and index iterations by Ï„ âˆˆ N.
For each commodity k, every player x âˆˆ Xk maintains a
mixed strategy Ï€ (Ï„ ) (x) âˆˆ âˆ†Pk , which reflects her preferences on paths, and randomly draws a path A(Ï„ ) (x) âˆ¼
Ï€ (Ï„ ) (x). Similarly to the one-shot case, the path profile
A(Ï„ ) defines a distribution Âµ(Ï„ ) âˆˆ âˆ†.
To formalize the probabilistic setting, let (â„¦, F, P) be a
probability space. We suppose that for x âˆˆ Xk , A(Ï„ ) (x)
is a random variable with values in Pk such that the mapping (x, Ï‰) 7â†’ A(Ï„ ) (x)(Ï‰) is M âŠ— F-measurable for all
(Ï„ )
Ï„ . We have for all x âˆˆ Xk and p âˆˆ Pk : Ï€p (x) =
(Ï„ )
P[A(Ï„ ) (x) = p]. In this setting, the distribution Âµk
is
k (Ï„ )
a random variable, as we recall that âˆ€p âˆˆ Pk , Âµ p =
R
1
(Ï„ )
(x) is random. In parFk Xk 1{A(Ï„ ) (x)=p} dm(x), and A
R
(Ï„ )
1
k (Ï„ )
ticular, E[Âµ p ] = Fk Xk Ï€p (x)dm(x).
Since players are non-cooperative, we consider that players randomize independently. Under this assumption, the
distribution Âµ(Ï„ ) is almost surely equal to its expectation.
Here, non-atomicity is essential.
Proposition 1. In the non-atomic routing game, if players
(Ï„ )
randomize independently, then for all Ï„ , Âµk is a random
variable with zero variance.
This follows from Fubiniâ€™s theorem. As a result, one can
think of the distribution Âµ as a deterministic variable, although individual players are randomizing.
Definition 3.1 (Online algorithm for routing). An online
algorithm (or update rule) for the routing game, applied
by a player x âˆˆ Xk , is a deterministic sequence of
functions (hÏ„ )Ï„ âˆˆN such that at iteration Ï„ , hÏ„ maps the
history of losses (`k (Âµ(t)))tâ‰¤Ï„ and the current strategy
(Ï„ +1)
Ï€ (Ï„ ) (x) to the strategy on the
(x) =
 next iteration, Ï€
k
(Ï„ )
hÏ„ (` (Âµ(t)))tâ‰¤Ï„ , Ï€ (x) .
This online learning framework is summarized in Algorithm 1. Here, we assume that, at the end of day Ï„ , a player
x âˆˆ Xk observes all the path latencies for her commodity,
i.e. (`kp (Âµ(Ï„ ) ))pâˆˆPk . This can be achieved for example by
having a central authority publicly report the path latencies
at the end of a given day. We note however that the information model could be further restricted such that every player
only observes the latency on his/her own path. One appropriate framework to study this problem is that of multiarmed bandit learning, see for example Auer et al. (2002),
GyoÌˆrgy et al. (2007), Dani et al. (2008), and Bubeck &
Cesa-Bianchi (2012). However, we do not currently consider this extension.

On the convergence of no-regret learning in selfish routing

3.2. Discounted regret
The regret is a natural measure of performance of a learning
algorithm (Cesa-Bianchi & Lugosi, 2006). In particular, we
are interested in online learning algorithms with sublinear
discounted regret. More precisely, we assume that losses
are discounted over time, by a decreasing sequence of factors (Î³Ï„ )Ï„ âˆˆN . So at iteration Ï„ , a player who chooses path
p incurs a loss Î³Ï„ `kp (Âµ(Ï„ ) ). The sequence (Î³Ï„ )Ï„ is assumed
to be universal: the discounting is identical across players.
This can be justified from an economic perspective if one
thinks of discounting as reflecting interest rates.
Assumption 2. (Î³Ï„ )Ï„ is a positive, decreasing, nonsummable sequence.
The idea of discounted regret is common in the online
learning literature, and is studied for example by CesaBianchi & Lugosi in (2006). It is worth noting, however,
that the sequence is usually assumed to be increasing. In
our case, discounting the losses by a decreasing sequence
can be motivated by the assumption that players value future time less than current time. Given the sequence of discount factors, the discounted regret is defined as follows:
Definition 3.2 (Discounted regret). Consider a player x âˆˆ
Xk . Given a sequence of strategies (Ï€ (Ï„ ) (x))Ï„ and a sequence of distributions (Âµ(Ï„ ) )Ï„ , the discounted regret of x
up to time T is:
R(T ) (x) = L(T ) (x) âˆ’ min Lpk

(T )

pâˆˆPk

(3)

(T )

are, respectively, the expected
where L(T ) (x) and Lpk
discounted cumulative loss incurred by x, and the discounted cumulative loss on path p âˆˆ Pk :
L(T ) (x) =
(T )
Lpk

=

X

Î³Ï„

X

Ï€p(Ï„ ) (x)`kp (Âµ(Ï„ ) )

Ï„ â‰¤T

pâˆˆPk

X

Î³Ï„ `kp (Âµ(Ï„ ) )

Ï„ â‰¤T

Definition 3.3 (Sublinear discounted regret). An online
learning algorithm for routing (hÏ„ )Ï„ is said to have sublinear discounted regret if whenever a player x applies the algorithm, for all initial strategies Ï€ (0) (x) and all sequences
(Âµ(Ï„ ) ), lim supT â†’âˆž P 1 Î³Ï„ R(T ) (x) â‰¤ 0.

Definition 3.4 (Hedge algorithm). Consider a player x âˆˆ
Xk . A Hedge algorithm with learning rates (Î³Ï„ )Ï„ is an
online algorithm (hÏ„ )Ï„ which satisfies the following update
equation:

(Ï„ ) ) 
`k
p (Âµ
(Ï„ +1)
(Ï„ ) âˆ’Î³Ï„
Ï
(4)
Ï€
âˆ Ï€p e
pâˆˆPk


=

Ï€p(0) e

3.3. Discounted Hedge algorithm
We now give one example of online learning algorithm
with sublinear discounted regret.


(5)
pâˆˆPk

Next, we give a bound on the discounted regret of the
Hedge algorithm, a generalization of Lemma 5.1 in CesaBianchi & Lugosi (2006).
Proposition 2. If (Î³Ï„ )Ï„ is a sequence satisfying Assumption 2, the Hedge algorithm with learning rates (Î³Ï„ )Ï„ , applied by a player x âˆˆ Xk , has sublinear regret. More precisely, if Ï is a uniform upper bound on the sequence of
losses, then
(0)

R(T ) (x) â‰¤ âˆ’Ï log Ï€min (x) + Ï

X Î³2
Ï„
.
8

Ï„ â‰¤T
(0)

(0)

where Ï€min = minp Ï€p

P
(0) up
k
Proof. Let Î¾ : u âˆˆ RP
7â†’ log( pâˆˆPk Ï€p e Ï ). By
+
equation (5), we have for all Ï„ :
Î¾(L k

(Ï„ âˆ’1)

(Ï„ )

) âˆ’ Î¾(L k
)
ï£«
ï£¶
(Ï„ âˆ’1)
Lk
`k (Âµ(Ï„ ) )
P
(0) âˆ’ p Ï
âˆ’Î³Ï„ p Ï
e
ï£¬ pâˆˆPk Ï€p e
ï£·
= log ï£­
ï£¸
k (Ï„ âˆ’1)
L
P
âˆ’ p Ï
pâˆˆPk Ï€p (0)e
ï£«
ï£¶
(Ï„ ) )
`k
X
p (Âµ
= log ï£­
Ï€p(Ï„ ) eâˆ’Î³Ï„ Ï ï£¸
pâˆˆPk

â‰¤ âˆ’Î³Ï„

X

Ï€p(Ï„ )

pâˆˆPk

`kp (Âµ(Ï„ ) ) Î³Ï„2
+
Ï
8

The last inequality follows from Hoeffdingâ€™s lemma, since
0 â‰¤ `kp (Âµ(Ï„ ) )/Ï â‰¤ 1.
Summing over Ï„ , we have:

Ï„ â‰¤T

An algorithm with sublinear discounted regret performs
asymptotically as well as the best constant strategy in hindsight.

L k (Ï„ )
âˆ’ pÏ

Î¾(Lpk

(T )

)â‰¤âˆ’

L(T ) (x) X Î³Ï„2
+
Ï
8
Ï„ â‰¤T

(T )

(0)

As log is increasing, Î¾(Lpk ) â‰¥ log(Ï€p )+Lpk
all p âˆˆ Pk . Rearranging, we have:
L(T ) (x) âˆ’ Lpk

(T )

â‰¤ âˆ’Ï log Ï€p(0) + Ï

(Ï„ )

X Î³2
Ï„
8

Ï„ â‰¤T

/Ï for

On the convergence of no-regret learning in selfish routing

we conclude by maximizing both sides with respect to
p âˆˆ Pk . This bound proves that
regret is
Pthe discounted

PT
T
sublinear since Ï„ =1 Î³Ï„2 = o
Î³
whenever
(Î³Ï„ )Ï„
Ï„ Ï„
satisfies Assumption 2.
Given the previous Proposition, discounting losses can be
interpreted, in the case of the Hedge algorithm, as using a
decreasing sequence of learning rates (Î³Ï„ )Ï„ .

Proof of Proposition 3. Let Âµ? âˆˆ N be a Nash equilibrium, i.e. Âµ? âˆˆ arg minÂµâˆˆâˆ† V (see Theorem 1). Then
by convexity of V and Equation (2),
E
D
V (Âµ(Ï„ ) ) âˆ’ VN â‰¤ âˆ‡V (Âµ(Ï„ ) ), Âµ(Ï„ ) âˆ’ Âµ?
â‰¤

K
X

E
D
(Ï„ )
Fk `k (Âµ(Ï„ ) ), Âµk âˆ’ Âµ? k

k=1

3.4. Population regret
We define the discounted regret for population Xk by integrating the individual regrets of players:
Z
1
k (T )
R(T ) (x)dm(x)
R
=
Fk Xk
If we define the average cumulative loss of populaR
(T )
tion Xk to be Lk
= F1k Xk L(T ) (x)dm(x) =
D
E
P
(T )
k (Ï„ ) k
, ` (Âµ(Ï„ ) ) , then we also have Rk
=
Ï„ â‰¤T Î³Ï„ Âµ
(T )

(T )

Lk
âˆ’ minpâˆˆPk Lpk . As a consequence of this definition, if all players in Xk apply algorithms with sublinear
discounted regret, the population-wide regret is also sub(T )
â‰¤ 0.
linear, that is, lim supT â†’âˆž P 1 Î³Ï„ Rk
Ï„ â‰¤T

4. Convergence to Nash equilibria
4.1. Convergence on almost all days
We give a first convergence result. For Âµ âˆˆ âˆ†, let
d(Âµ, N ) = inf Î½âˆˆN kÂµ âˆ’ Î½k where k Â· k is the Euclidean
distance on RP . We say that a sequence (Âµ(Ï„ ) )Ï„ converges
to the set N if d(Âµ(Ï„ ) , N ) â†’ 0.
Proposition 3 (Statistical convergence to Nash equilibria).
Consider a routing game with population dynamics such
that for all k, the population regret Rk is sublinear, and
let (Âµ(Ï„ ) )Ï„ be the sequence of path distributions. Then
there exists a subsequence (Âµ(Ï„ ) )Ï„ âˆˆT which converges to
N , defined
P on a subset T âŠ‚ N of density one, that is,
Î³Ï„
limT â†’âˆž PÏ„ âˆˆT :Ï„ â‰¤T Î³Ï„ = 1.
Ï„ âˆˆN:Ï„ â‰¤T

In other words, the strategies converge on almost all days
if the population regret is sublinear. This is a limit case in
Theorem 5.1 in (Blum et al., 2006). We present a different proof which uses the Rosenthal potential function, and
which holds even if the latency functions are not Lipschitz
continuous. We first need the following technical Lemma.
Lemma 1. Let (Î³Ï„ )Ï„ âˆˆN be a non-summable sequence of
positive weights. If a real sequence (u(Ï„ ) )Ï„ âˆˆN converges
absolutely to u in the sense of CesaÌ€ro means w.r.t. (Î³Ï„ )Ï„ ,
P

Î³Ï„ |u(Ï„ ) âˆ’u|

P
that is limT â†’âˆž Ï„ â‰¤T
= 0, then there exists a
Ï„ â‰¤T Î³Ï„
subset of indexes T of density one such that the subsequence (u(Ï„ ) )Ï„ âˆˆT converges to u.

here we use hÂ·, Â·i to denote the inner product on RP . Then,
taking the weighted sum up to time T ,
Î³Ï„ (V (Âµ(Ï„ ) ) âˆ’ VN )
P
Ï„ â‰¤T Î³Ï„
D
E D
E
P
(Ï„ )
(T )
Î³Ï„ Âµk , `k (Âµ(Ï„ ) ) âˆ’ Âµ? k , L k
K
X Ï„ â‰¤T
P
â‰¤
Fk
Î³Ï„

P

Ï„ â‰¤T

k=1

â‰¤

K
X
k=1

R
Fk P

k (T )

Ï„ â‰¤T

Ï„ â‰¤T

Î³Ï„

where
the E
last inequality follows from the fact that
D
(T )
?
k (T )
Âµ ,L
â‰¥ minp Lpk . Since, for all Ï„ , V (Âµ(Ï„ ) ) âˆ’
VN â‰¥ 0, and for all k, lim supT â†’âˆž

P 1

Ï„ â‰¤T

Î³Ï„

Rk

(T )

â‰¤ 0

by assumption, we have (V (Âµ(Ï„ ) ))Ï„ converges absolutely
to VN in the sense of CesaÌ€ro means w.r.t. (Î³Ï„ )Ï„ . Thus by
Lemma 1, there exists a dense subset of indexes T such
that (V (Âµ(Ï„ ) ))Ï„ âˆˆT converges to VN , and by continuity of
V and compactness of âˆ†, the subsequence (Âµ(Ï„ ) )Ï„ âˆˆT converges to N .

In order to show strong convergence for a class of online
algorithms with sublinear discounted regret, we first study
the continuous-time replicator dynamics, which can be motivated as a continuous-time limit of the Hedge algorithm,
as discussed next.
4.2. Continuous-time dynamics
We consider the discounted Hedge algorithm with a vanishing sequence of learning rates (Î³Ï„ ), acting on the sequence
of population strategies (Âµ(Ï„ ) )Ï„ âˆˆN .
Let us imagine an underlying continuous time T âˆˆ R+ ,
and set Âµ(TÏ„ ) = Âµ(Ï„ ) , where TÏ„ is the time at which the
Ï„ -th update happens. Now choosing the update times to be

On the convergence of no-regret learning in selfish routing
p1

2

2

p0

1.5
1

(Ï„ )

: Nash equilibrium

p0

1.5
1

0.5

Âµ1
10

20

30

40

50

Ï„

2

Âµ1

0.5

: uniform

p2

0

10

20

Âµ2

40

50

(0)

: uniform

p3

1

(0)

: uniform

p2

p4

path p3 = (v2 , v4 , v5 , v3 )
path p4 = (v2 , v4 , v6 , v3 )
path p5 = (v2 , v3 )

2.5
2

1.5

30
Ï„

p4

path p3 = (v2 , v4 , v5 , v3 )
path p4 = (v2 , v4 , v6 , v3 )
path p5 = (v2 , v3 )

2.5

(0)

`2p (Âµ(Ï„ ) )

0

`2p (Âµ(Ï„ ) )

lim Âµ1

Ï„ â†’âˆž

path p0 = (v0 , v4 , v5 , v1 )
path p1 = (v0 , v4 , v6 , v1 )
path p2 = (v0 , v1 )

2.5

`1p (Âµ(Ï„ ) )

2.5

`1p (Âµ(Ï„ ) )

p1

path p0 = (v0 , v4 , v5 , v1 )
path p1 = (v0 , v4 , v6 , v1 )
path p2 = (v0 , v1 )

Âµ2

(0)

: uniform

p3

1.5
1

0.5

0.5
0

10

20

30

40

50

Ï„

0

p5

10

20

30

40

50

Ï„

p5

lim Âµ2

Ï„ â†’âˆž

(Ï„ )

: Nash equilibrium

Figure 2. Example of a routing game payed on the example network of Figure 1. Latency functions are taken to be quadratic increasing,
(Ï„ )
and generated randomly. The population strategies (Âµk )Ï„ obey the Hedge algorithm. The figures show the trajectories in the simplex
Pk
k
(Ï„ )
âˆ† , and the resulting path latencies (`p (Âµ )) for population X1 (top) and X2 (bottom). With a constant learning rate Î³ = 0.7,
(Ï„ )

(Âµk )Ï„ does not converge (left). With a harmonic sequence of learning rates, Î³Ï„ =
equilibria.

TÏ„ =

PÏ„

t=1

(Ï„ +1)
k

=

(Ï„ )
Âµkp P

eâˆ’Î³Ï„ `p (Âµ

p0 âˆˆPk

Âµkp0

(Ï„ )

(Âµk

(Ï„ )

)Ï„ converges to the set of Nash

Ëš = {Âµ âˆˆ âˆ† : âˆ€p âˆˆ P, Âµp > 0} is the relative
Here, âˆ†
Ëš guarantees that Âµ(t) remains
interior of âˆ†. Starting in âˆ†
Ëš
in âˆ† for all t. In this derivation, the discount factors Î³Ï„
are interpreted as discrete time steps. The dynamics described by this ODE, called the replicator dynamics (Fischer & VoÌˆcking, 2004), has been studied extensively. One
can observe in particular that the set RN of restricted Nash
equilibria (Definition 2.2) is exactly the set of stationary
points for the ODE.

Î³t , we can write, âˆ€p âˆˆ Pk

Âµkp (TÏ„ +1 ) = Âµkp

1
,
1+Ï„ /10

)/Ï

(Ï„ ) âˆ’Î³Ï„ `k0 (Âµ(Ï„ ) )/Ï
p

e

1 âˆ’ Î³Ï„ `kp (Âµ(Ï„ ) )/Ï + o(Î³Ï„ )
P
(Ï„ )
1 âˆ’ Î³Ï„ p0 âˆˆPk Âµkp0 `kp0 (Âµ(Ï„ ) )/Ï + o(Î³Ï„ )
"
#

 (Ï„ ) k (Ï„ ) 
(Ï„ )
k
(Âµ
)
Âµ
,
`
(Âµ
)
âˆ’
`
p
= Âµkp (TÏ„ ) 1 + Î³Ï„
+ o(Î³Ï„ ) 4.3. Replicator updates
Ï
By discretizing the replicator dynamics, we obtain a multiThus,
plicative update rule we call REP for Replicator, which has
desirable properties which we prove next.
k
k
Âµp (TÏ„ + Î³Ï„ ) âˆ’ Âµp (TÏ„ )
=
Definition 4.1 (REP algorithm). The replicator (REP) alÎ³

 kÏ„

gorithm with rates (Î³Ï„ )Ï„ , Î³Ï„ â‰¤ 1, applied by x âˆˆ Xk , is an
Âµ (TÏ„ ), `k (Âµ(TÏ„ )) âˆ’ `kp (Âµ(TÏ„ ))
k
online algorithm for routing given by the following update
Âµp (TÏ„ )
+ o(1)
Ï
equation
= Âµkp

(Ï„ )

taking the limit of the above equation as Î³Ï„ â†’ 0, we obtain
the following ODE
(
Ëš
Âµ(0) âˆˆ âˆ†
(6)
dÂµ(t)
dt = G(Âµ(t), `(Âµ(t)))
where âˆ€k and âˆ€p âˆˆ Pk
Gkp (Âµ, `)

=

Âµkp


 k k
Âµ , ` âˆ’ `kp
Ï

(7)

Ï€p(Ï„ +1) âˆ’ Ï€p(Ï„ ) = Î³Ï„ Gkp (Ï€ (Ï„ ) , `(Âµ(Ï„ ) ))

(8)

We note that summing this update equation over p âˆˆ Pk
P
(Ï„ +1)
(Ï„ )
yields pâˆˆPk (Ï€p
âˆ’ Ï€p ) = 0, thus Ï€ remains in âˆ†Pk
as long as Î³Ï„ â‰¤ 1. We now show that the REP update rule
guarantees a sublinear discounted regret. To see this, we
need the following regret bound on multiplicative-weights
updates with signed losses.
Lemma 2. Consider an online learning setting with signed

On the convergence of no-regret learning in selfish routing
(Ï„ )

losses sp âˆˆ [âˆ’1, 1] and discount factors Î³Ï„ â‰¤ 1/2 satisfying Assumption 2. Then the discounted multiplicativeweights algorithm defined by


)
Ï€ (Ï„ +1) âˆ Ï€p(Ï„ ) (1 âˆ’ Î³Ï„ s(Ï„
)
(9)
p
p

guarantees that for any p,
E

X D
X
(0)
)
Î³Ï„
Ï€ (Ï„ ) , s(Ï„ ) âˆ’ s(Ï„
â‰¤ âˆ’ log Ï€min +
Î³Ï„2
p
Ï„ â‰¤T

Ï„ â‰¤T

This Lemma is a straightforward extension of Theorem 2.1
in (Arora et al., 2012) to the discounted case.
Proposition 4. If (Î³Ï„ )Ï„ is a sequence of discount factors
satisfying Assumption 2 and such that Î³Ï„ â‰¤ 1/2 for all
Ï„ , the (REP) update rule with rates (Î³Ï„ )Ï„ has sublinear
discounted regret.



(Ï„ )
Proof. Let rp = Ï€ (Ï„ ) , `k (Âµ(Ï„ ) ) âˆ’`kp (Âµ(Ï„ ) ) âˆˆ [âˆ’Ï, Ï] be
the instantaneous regret of the player. Then the REP update
can be viewed as a multiplicative-weights algorithm with
(Ï„ )
(Ï„ )
update rule (9), signed losses sp = âˆ’rpk /Ï âˆˆ [âˆ’1, 1],
D
E
(Ï„ )
and discount factors Î³Ï„ . Observing that rk , Ï€ (Ï„ ) = 0,
we have by Lemma 2:

Condition (11) corresponds to the first hypothesis of Proposition 4.1 in (BenaÄ±Ìˆm, 1999), which we will use in the proof
of the main convergence theorem. It bounds the cumulative perturbation over a given time interval T . Intuitively,
this condition will ensure that the trajectories of a discrete
AREP algorithm are asymptotically close to the trajectories
of the continuous-time replicator dynamics.
Note that the REP update rule is an AREP algorithm with
zero perturbation. By allowing perturbations, we extend
the class of algorithms for which we can show convergence.
In particular, we show that the discounted Hedge algorithm
is in this class.
Proposition 5. The Hedge algorithm with learning rates
(Î³Ï„ )Ï„ satisfying Assumption 2 is an AREP algorithm.
Proof. Let (Ï€ (Ï„ ) )Ï„ âˆˆN be the sequence of player strategies,
and (Âµ(Ï„ ) )Ï„ be any sequence of population distributions.
By definition of the Hedge algorithm,
Ï€p(Ï„ +1)

Ï„ â‰¤T

which shows lim supT â†’âˆž P

1

Ï„ â‰¤T

Î³Ï„

R(T ) (x) â‰¤ 0.

Ï€p(Ï„ +1) âˆ’ Ï€p(Ï„ ) = Î³Ï„ (Gkp (Ï€ (Ï„ ) , `(Âµ(Ï„ ) )) + Up(Ï„ +1) ) (10)
where (U (Ï„ ) )Ï„ â‰¥1 is a bounded sequence of stochastic perturbations with values in RPk , and which satisfies the following condition: for all T > 0,
 Ï„

2
X


(Ï„ +1) 
lim
max
Î³
U
(11)

=0
Ï„
Ï„2
Ï„1 â†’âˆž
Ï„ =Ï„

P
Ï„2 :

Ï„ =Ï„1

Î³Ï„ <T

1

X

/

(Ï„ )
Ï€p0 eâˆ’Î³Ï„

`k0 (Âµ(Ï„ ) )
p
Ï

k
(Ï„ )
Ëœk (Ï„ )
(Ï„ )
`kp (Âµ(Ï„ ) ) âˆ’ `Ëœk (Ï„ )
Ï€p h âˆ’Î³Ï„ `p (Âµ Ï)âˆ’`
e
+ Î³Ï„
Î³Ï„
Ï
i
k (Ï„ )
k (Ï„ )
Ëœ
Â¯
`
âˆ’
`
âˆ’ 1 + Ï€p(Ï„ )
Ï
X (Ï„ ) âˆ’Î³ `k (Âµ(Ï„ ) )/Ï
Ï
k
(Ï„
)
Ï€p0 e Ï„ p0
`Ëœ
= âˆ’ log
Î³Ï„
p0 âˆˆPk
D
E
k
(Ï„
)
(Ï„
)
k
`Â¯
= Ï€ , ` (Âµ(Ï„ ) )

Up(Ï„ +1) =

Letting Î¸(x) = ex âˆ’ x âˆ’ 1, we have for all p âˆˆ Pk :
!
(Ï„ )
(Ï„ )
Ëœk(Ï„ )
Ï€
`
(Âµ
)
âˆ’
`
p
p
Up(Ï„ +1) =
Î¸ âˆ’Î³Ï„
Î³Ï„
Ï
+

4.4. Approximate Replicator algorithms
Definition 4.2 (AREP algorithm). An online algorithm for
routing, applied by x âˆˆ Xk , is said to be an approximate
replicator algorithm (AREP) if its update equation can be
written as

Ï

which we can write in the form of equation (10), with

Ï„ â‰¤T

Rearranging and taking the maximum over p âˆˆ Pk , we
obtain the following bound on the discounted regret
X
(0)
R(T ) (x) â‰¤ âˆ’Ï log Ï€min + Ï
Î³Ï„2

(Ï„ ) )
`k
p (Âµ

p0 âˆˆPk

X
1 X
(Ï„ )
Î³Ï„ rk p â‰¤ âˆ’ log Ï€min (0) +
Î³Ï„2
Ï
Ï„ â‰¤T

=

Ï€p(Ï„ ) eâˆ’Î³Ï„

(Ï„ )
Ï€p  Ëœk(Ï„ ) Â¯k(Ï„ ) 
`
âˆ’`
Ï

The first term is a O(Î³Ï„ ) as Î¸(x) âˆ¼0 x2 /2. To bound the
second term, we have by concavity of the logarithm
X (Ï„ )
`Ëœk(Ï„ ) â‰¤
Ï€ 0 `k0 (Âµ(Ï„ ) ) = `Â¯k(Ï„ )
p

p

p0 âˆˆPk

And by Hoeffdingâ€™s lemma,
(Ï„ ) )

log

X
p0 âˆˆPk

Ï€p0 e

` 0 (Âµ
âˆ’Î³Ï„ p Ï

â‰¤ âˆ’Î³Ï„

X
p0 âˆˆPk

(Ï„ ) `p0 (Âµ

Ï€p0

Ï

(Ï„ )

)

+

Î³Ï„2
8

Rearranging, we have 0 â‰¤ `Â¯k(Ï„ ) âˆ’ `Ëœk(Ï„ ) â‰¤ ÏÎ³8Ï„ ,
PÏ„
(Ï„ +1)
therefore Up
= O(Î³Ï„ ), and k Ï„2=Ï„1 Î³Ï„ U (Ï„ +1) k =
PÏ„2
O( Ï„ =Ï„1 Î³t2 ). Finally, since Î³Ï„ decrease to 0 by Assumption 2, condition (11) is verified.

On the convergence of no-regret learning in selfish routing

4.5. Strong convergence to Nash equilibria
k (Ï„ )

Theorem 2. If for all k, the population strategies (Âµ
)Ï„
satisfy an AREP algorithm with sublinear regret, then the
sequence (Âµ(Ï„ ) ) converges to the set of Nash equilibria.
Proof. The proof proceeds in two steps: first, we use results from (BenaÄ±Ìˆm, 1999) to prove that the sequence of
potentials V (Âµ(Ï„ ) ) converges. Then, using convergence on
most days given in Proposition 3, we conclude that V (Âµ(Ï„ ) )
converges necessarily to the minimum VN , which proves
that (Âµ(Ï„ ) ) converges to N by continuity of V on the compact âˆ†. First, we recall the definition of Lyapunov function.
Definition 4.3 (Lyapunov function). Let Î“ âŠ‚ âˆ† be a compact invariant set for the replicator ODE (6). A continuous
non-negative function V : âˆ† â†’ R+ is a Lyapunov function
d
V (Âµ(t)) = hâˆ‡V (Âµ(t)), G(Âµ(t), `(Âµ(t)))i < 0
for Î“ if dt
for all Âµ(t) âˆˆ
/ Î“.
Lemma 3 (Convergence of potentials under AREP algorithms). Let Î“ be a compact invariant set for the replicator ODE (6), V a Lyapunov function for Î“, and assume
V (Î“) has empty interior. Assume that the sequence of distributions (Âµ(Ï„ ) )Ï„ âˆˆN obeys an AREP update rule. Then the
sequence of potentials (V (Âµ(Ï„ ) ))Ï„ converges.

Corollary 1. If (Î³Ï„ )Ï„ is a sequence bounded by 1/2 satisfying Assumption 2, and (ÂµÏ„ )Ï„ obeys the REP update rule
with rates (Î³Ï„ ), then (ÂµÏ„ )Ï„ converges to the set of Nash
equilibria.
Proof. Under these assumptions, the REP algorithm has
sublinear discounted regret by Proposition 4. It is also an
AREP algorithm (with zero perturbations) so we can apply
Theorem 2.
Corollary 2. If (Î³Ï„ )Ï„ is a sequence satisfying Assumption 2, and (ÂµÏ„ )Ï„ obeys the discounted Hedge algorithm
with rates (Î³Ï„ ), then (ÂµÏ„ )Ï„ converges to the set of Nash
equilibria.
Proof. By Proposition 2 and Proposition 5, the Hedge algorithm with rates Î³Ï„ is an AREP algorithm with sublinear
discounted regret, and we can apply Theorem 2.
Figure 2 shows an example of discounted Hedge algorithm with a decreasing, non-summable sequence of learning rates. The resulting strategies converge to the set of
Nash equilibria.

5. Conclusion

In order to obtain strong convergence guarantees of online learning algorithms applied to routing games, we conThis follows from Theorem 5.7 and Proposition 4.1
sider a model in which losses are discounted. We studin (BenaÄ±Ìˆm, 1999). Here, condition (11) is essential.
ied a continuous-time limit of the Hedge algorithm. This
Next, we show that the Rosenthal potential function V is
motivated the introduction of a class of no-regret learning
a Lyapunov function for the invariant set RN of restricted
algorithms, called AREP, which can be viewed as approxiNash equilibria. From equation (2) and the definition of G,
mations of the replicator dynamics. Using results from the




theory of stochastic approximation, we showed that under
âˆ‡V Âµ(t) , G(Âµ(t), `(Âµ(t)))
D
E
 this class, (Âµ(Ï„ ) ) is guaranteed to converge to the set of
X k
X
`p (Âµ(t))Âµkp (t) Âµk (t), `k (Âµ(t)) âˆ’ `kp (Âµ(t))
=
Fk
Nash equilibria.
k

pâˆˆPk

ï£¶2

ï£®ï£«
=

X
k

Fk ï£°ï£­

X

pâˆˆPk

Âµkp (t)`kp (Âµ(t))ï£¸

ï£¹
âˆ’

X

Âµkp (t)`kp (Âµ(t))2 ï£»

pâˆˆP

which is less than or equal to 0 by Jensenâ€™s inequality, with
equality if and only if Âµ âˆˆ RN . Therefore V is a Lyapunov
function for RN . And since V (RN ) is a finite set by Remark 1, it has empty interior relatively to R, and we can
apply Lemma 3, and conclude that the sequence of potentials (V (Âµ(Ï„ ) ))Ï„ âˆˆN converges. It remains to show that its
limit is VN .
Since the AREP algorithm is assumed to have sublinear discounted regret, we can apply Proposition 3: there
exists a dense subsequence (Âµ(Ï„ ) )Ï„ âˆˆT which converges
to N . The corresponding subsequence of potentials
(V (Âµ(Ï„ ) ))Ï„ âˆˆT converges to VN by continuity of V , and by
uniqueness of the limit, we must have limÏ„ V (Âµ(Ï„ ) ) = VN .
This concludes the proof.

These results assume a universal sequence (Î³Ï„ )Ï„ of discounts; thus a natural question is whether convergence still
holds if this assumption is relaxed. Another open question is whether the learning algorithm is robust to observation noise: if latency observations are noisy, with bounded
noise, can one guarantee convergence if the bound is small
enough?

On the convergence of no-regret learning in selfish routing

References
Arora, Sanjeev, Hazan, Elad, and Kale, Satyen. The multiplicative weights update method: a meta-algorithm and
applications. Theory of Computing, 8(1):121â€“164, 2012.

Littlestone, Nick and Warmuth, Manfred K. The weighted
majority algorithm. In Foundations of Computer Science, 1989., 30th Annual Symposium on, pp. 256â€“261.
IEEE, 1989.

Auer, Peter, Cesa-Bianchi, NicoloÌ€, and Fischer, Paul.
Finite-time analysis of the multiarmed bandit problem.
Mach. Learn., 47(2-3):235â€“256, May 2002.

Monderer, Dov and Shapley, Lloyd S. Fictitious play property for games with identical interests. journal of economic theory, 68(1):258â€“265, 1996.

Beckmann, Martin J, McGuire, Charles B, and Winsten,
Christopher B. Studies in the economics of transportation. 1955.

Papadimitriou, Christos H. On the complexity of the parity
argument and other inefficient proofs of existence. Journal of Computer and system Sciences, 48(3):498â€“532,
1994.

BenaÄ±Ìˆm, Michel. Dynamics of stochastic approximation
algorithms. In SeÌminaire de probabiliteÌs XXXIII, pp. 1â€“
68. Springer, 1999.
Blum, Avrim, Even-Dar, Eyal, and Ligett, Katrina. Routing without regret: on convergence to nash equilibria of
regret-minimizing algorithms in routing games. In Proceedings of the twenty-fifth annual ACM symposium on
Principles of distributed computing, PODC â€™06, pp. 45â€“
52, New York, NY, USA, 2006. ACM.
Bubeck, SeÌbastien and Cesa-Bianchi, NicoloÌ€. Regret analysis of stochastic and nonstochastic multi-armed bandit
problems. Foundations and Trends in Machine Learning, 5(1):1â€“122, 2012.
Cesa-Bianchi, NicoloÌ€ and Lugosi, GaÌbor. Prediction,
learning, and games. Cambridge University Press, 2006.
Dafermos, Stella C and Sparrow, Frederick T. The traffic
assignment problem for a general network. Journal of
Research of the National Bureau of Standards, Series B,
73(2):91â€“118, 1969.
Dani, Varsha, Hayes, Thomas, and Kakade, Sham. The
price of bandit information for online optimization. In
Advances in Neural Information Processing Systems 20,
pp. 345â€“352, Cambridge, MA, 2008. MIT Press.
Fischer, Simon and VoÌˆcking, Berthold. On the evolution of
selfish routing. In Algorithmsâ€“ESA 2004, pp. 323â€“334.
Springer, 2004.
Fischer, Simon, RaÌˆcke, Harald, and VoÌˆcking, Berthold.
Fast convergence to wardrop equilibria by adaptive sampling methods. SIAM Journal on Computing, 39(8):
3700â€“3735, 2010.
Freund, Yoav and Schapire, Robert E. Adaptive game playing using multiplicative weights. Games and Economic
Behavior, 29(1):79â€“103, 1999.
GyoÌˆrgy, AndraÌs, Linder, TamaÌs, Lugosi, GaÌbor, and OttucsaÌk, GyoÌˆrgy. The on-line shortest path problem under
partial monitoring. Journal of Machine Learning Research, 8:2369â€“2403, December 2007.

Rosenthal, Robert W. A class of games possessing purestrategy nash equilibria. International Journal of Game
Theory, 2(1):65â€“67, 1973.
Roughgarden, T. Routing games. In Algorithmic game
theory, chapter 18, pp. 461â€“486. Cambridge University
Press, 2007.
Sandholm, William H. Potential games with continuous
player sets. Journal of Economic Theory, 97(1):81â€“108,
2001.
Wardrop, John Glen. Some theoretical aspects of road traffic research. In ICE Proceedings: Engineering Divisions, volume 1, pp. 325â€“362. Thomas Telford, 1952.
Weibull, JoÌˆrgen W. Evolutionary game theory. MIT press,
1997.

