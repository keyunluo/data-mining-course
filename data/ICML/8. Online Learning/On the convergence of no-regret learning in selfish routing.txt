On the convergence of no-regret learning in selfish routing

Walid Krichene
University of California, 652 Sutardja Dai Hall, Berkeley, CA 94720 USA
Benjamin Drighès
Ecole Polytechnique, Route de Saclay, 91120 Palaiseau, France

BENJAMIN . DRIGHES @ POLYTECHNIQUE . EDU

Alexandre Bayen
University of California, 642 Sutardja Dai Hall, Berkeley, CA 94720 USA

Abstract
We study the repeated, non-atomic routing game,
in which selfish players make a sequence of routing decisions. We consider a model in which
players use regret-minimizing algorithms as the
learning mechanism, and study the resulting dynamics. We are concerned in particular with
the convergence to the set of Nash equilibria of
the routing game. No-regret learning algorithms
are known to guarantee convergence of a subsequence of population strategies. We are concerned with convergence of the actual sequence.
We show that convergence holds for a large class
of online learning algorithms, inspired from the
continuous-time replicator dynamics. In particular, the discounted Hedge algorithm is proved to
belong to this class, which guarantees its convergence.

1. Introduction
Routing games are important in modeling and understanding the interaction of non-cooperative players who share
resources, such as roads in a road network and links in
a communication network. They have been studied extensively, including the seminal work of Beckmann et al.
(1955) and Dafermos & Sparrow (1969). In a one-shot scenario, selfish players choose the routes that minimize their
individual travel time. One solution concept to the game is
the Nash equilibrium, also called Wardrop equilibrium in
the traffic literature (1952). In some classes of games, Nash
equilibria can be hard to compute and have been questioned
as a realistic equilibrium concept, for example by PapadimProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

WALID @ EECS . BERKELEY. EDU

BAYEN @ BERKELEY. EDU

itriou (1994). By contrast, for one-shot non-atomic routing
games, Nash equilibria are known to be easy to compute
as they can be expressed as the solution to a convex optimization problem, using a convex potential function, due
to Rosenthal (1973). This is an argument in favor of the
one-shot routing game model. However, most realistic scenarios do not correspond to a one-shot game, but rather a
repeated game, in which players make a sequence of routing decisions and may adapt their strategies given the outcome on previous days. Therefore studying the repeated
routing game is important to understand how players can
arrive at the equilibrium. Arguably, a good learning model
for the population of players should be distributed and easy
to implement by individual players. A natural framework
is that of online learning.
No-regret learning is of particular interest, given its generality and ease of implementation, and the fact that it only
requires the current losses to be revealed. The Hedge algorithm is one example of no-regret learning, introduced for
Machine Learning by Freund & Schapire (1999), a generalization of the weighted majority algorithm of Littlestone &
Warmuth (1989). Cesa-Bianchi & Lugosi (2006) give convergence results, together with convergence rates, for noregret algorithms. These results hold for a broad class of
games. However, they guarantee convergence of the timeaveraged strategies, and not the actual sequence of strategies.
Other learning processes have been studied for repeated
routing games, such as fictitious play by Monderer & Shapley (1996), adaptive sampling by Fischer et al. (2010) or
continuous-time replicator dynamics by Fischer & Vöcking
(2004), which is also of particular interest in evolutionary
game theory, see for example Weibull (1997). For some
classes of continuous-time dynamics, convergence of the
actual sequence is guaranteed. For example, Sandholm
proved convergence in continuous-time potential games for
a class of evolutionary dynamics which satisfy a positive

On the convergence of no-regret learning in selfish routing

correlation condition (2001). Blum et al. proved in (2006)
that under no-regret learning, the resulting sequence of
population strategies converges, on a 1 −  fraction of days,
to the set of -approximate Nash equilibria, and they gave
explicit convergence rates that depend on the Lipschitz constant of the latency functions.
We are concerned with convergence of the actual sequence
of strategies (as opposed to a subsequence). Our approach
combines ideas from evolutionary dynamics and no-regret
learning. In Sections 2 and 3, we present the model and
summarize existing results. In Section 4, we prove that
convergence holds for a class of algorithms, which have
sublinear discounted regret, and which can be viewed as a
generalization of replicator dynamics.

2. The model
2.1. Non-atomic routing game
We consider a set X of players. A routing game is a noncooperative game played on a directed graph G = (V, E)
representing a network, and in which a pure strategy corresponds to a directed path on the graph. To formalize the
notion of non-atomicity, we endow X with a structure of
measurable space (X , M, m), where M is a σ-algebra of
measurable sets, and m is a measure. The set of players
is said to be non-atomic if each single player x ∈ X is
negligible for m.
We consider a setting similar to (Wardrop, 1952), in which
the set of players is partitioned in populations or commodities X = tK
k=1 Xk , where each Xk is measurable and has
positive finite measure. Formally, the model is defined by
the tuple (E; K; (Xk )k∈[K] ; (Pk )k∈[K] ; (ce )e∈E ), where E
denotes the finite set of edges, K is the number of commodities, [K] denotes the set {1, . . . , K}, and for all k,
Pk ⊆ P (E) is a set of paths (pure strategies) available to
players in Xk . For each k, all paths in Pk have a common
source sk ∈ V and a common destination tk ∈ V. We will
denote P the disjoint union P = tK
k=1 Pk . The positive
measure of Xk is denoted Fk = m(Xk ) and called the total
flow of Xk . For each edge e, ce (·) : R+ → R+ is an edge
latency function satisfying the following assumption:
Assumption 1. The latency functions ce are assumed to be
continuous, non-decreasing, and locally Lipschitz.
At a microscopic scale, the joint action of all players in X
can be represented by an action profile A : x ∈ X 7→ A(x)
which maps a player x ∈ Xk to a path A(x) ∈ Pk . This
function is assumed to be M-measurable, and defines, for
each population Xk , a path distribution µk = (µkp )p∈Pk ,
R
where µkp = F1k Xk 1{A(x)=p} dm(x) is the fraction of
players utilizing path p ∈ Pk . We have µkP
∈ ∆Pk , the
Pk
Pk
simplex on Pk , that is, ∆ = {u ∈ R+ :
p∈Pk up =

v0

v1
v5
v4
v6

v2

v3

Figure 1. Example of a network with K = 2 populations. Population 1 travels from v0 to v1 , and population 2 travels from v2 to
v3 .

1}. The pure strategy profile can be summarized at a
macroscopic scale using the product of distributions µ =
(µ1 , · · · , µK ) ∈ ∆P1 × · · · × ∆PK . The product of simplexes will be denoted ∆.
The distribution µ P
determinesPthe edge flow or load,
K
k
defined as φe =
This can
k=1 Fk
p∈Pk :e∈p µp .
be written compactly as φe = (M µ)e where M =
[M 1 | . . . |M K ] ∈ RE×P is a weighted incidence matrix:
(
Fk if e ∈ p
k
∀e ∈ E, ∀k ∈ [K], ∀p ∈ Pk Me,p =
0
otherwise
For each edge e, the edge load determines the edge latency,
given by ce (φe ). Finally, the loss of a player who chooses
path p is simply the sum of edge latencies along the path,
P
e∈p ce (φe ). This latency is entirely determined by the
distribution µ, so we define a path latency
P function (or loss
function) `p : µ ∈ ∆ 7→ `kp (µ) = e∈p ce ((M µ)e ). Finally, we write `k (µ) to denote the vector of path latencies
(`kp (µ))p∈Pk , and `(µ) = (`1 (µ), . . . , `K (µ)).
2.2. Nash equilibria and the Rosenthal potential
function
Given this setting, we now define Nash equilibria of the
routing game.
Definition 2.1 (Nash equilibrium).
A distribution µ ∈ ∆ is a Nash equilibrium if for every
population k, whenever µkp > 0 for some path p ∈ Pk ,
then `kp0 (µ) ≥ `kp (µ) for all p0 ∈ Pk . We will denote by
N ⊂ ∆ the set of Nash equilibria.
The definition implies that, for a commodity k, all paths
with non-zero mass have equal latencies and paths with
zero mass have larger latencies.
There is a natural potential function that allows one to formulate the problem of computing the set N of Nash equilibria as the solution of a convex optimization problem.
Consider the function
X Z (M µ)e
V (µ) =
ce (u)du
(1)
e∈E

0

On the convergence of no-regret learning in selfish routing

The gradient of V is the vector of path latencies:
∀k, ∀p ∈ Pk ,

∂V
(µ) = Fk `kp (µ)
∂µkp

3.1. The online learning framework
(2)

Theorem 1. (Rosenthal, 1973) N is the set of minimizers
of V in ∆. It is a non-empty convex compact set. We denote
VN the value of V on N .
A proof can be found for example in (Roughgarden, 2007).
As a result of Theorem 1, computing the Nash equilibria
of the routing game can be done efficiently by minimizing the potential. However, the idea of minimizing a potential function cannot be directly applied to designing a
distributed learning algorithm, as it would a priori require
coordination between players.
2.3. Restricted Nash equilibria
In the analysis, we use a weaker notion of equilibrium, introduced by Fischer & Vöcking in (2004).
Definition 2.2 (Restricted Nash equilibrium). A product
distribution µ is a restricted Nash equilibrium if all paths
with non-zero mass have equal latencies for each commodity i.e. for all k and all p, p0 ∈ Pk such that µkp , µkp0 > 0,
`kp (µ) = `kp0 (µ). We will denote RN the set of restricted
Nash equilibria.
Such an equilibrium is restricted in the sense that it would
be a Nash equilibrium of the routing game if we restricted
the set of paths to its support (Fischer & Vöcking, 2004).
Remark 1. Restricted Nash equilibria are also minimizers of the potential function V if we restrict the feasible
set to distributions with the same support. As the number
of supports is finite, the set V (RN ) of potential values of
restricted Nash equilibria is also finite.

3. No-regret learning in the repeated routing
game
Algorithm 1 Online learning setting
Input: For every player x ∈ Xk , a learning algorithm (hxτ )τ and
initial distribution π (0) (x) ∈ ∆Pk .
1: for each time step τ do
2:
Every player x independently draws a path A(τ ) (x) ∼
π (τ ) (x).
3:
For all k, the vector of path losses `k (µ(τ ) ) is revealed to
players in Xk . Players incur losses corresponding to their
path choice.
(τ +1)
4:
Every
(x) =
 player updates her strategy: π
hxτ (`k (µ(t)))t≤τ , π (τ ) (x) .

We now define the online learning setting. Assume players
make decisions repeatedly, and index iterations by τ ∈ N.
For each commodity k, every player x ∈ Xk maintains a
mixed strategy π (τ ) (x) ∈ ∆Pk , which reflects her preferences on paths, and randomly draws a path A(τ ) (x) ∼
π (τ ) (x). Similarly to the one-shot case, the path profile
A(τ ) defines a distribution µ(τ ) ∈ ∆.
To formalize the probabilistic setting, let (Ω, F, P) be a
probability space. We suppose that for x ∈ Xk , A(τ ) (x)
is a random variable with values in Pk such that the mapping (x, ω) 7→ A(τ ) (x)(ω) is M ⊗ F-measurable for all
(τ )
τ . We have for all x ∈ Xk and p ∈ Pk : πp (x) =
(τ )
P[A(τ ) (x) = p]. In this setting, the distribution µk
is
k (τ )
a random variable, as we recall that ∀p ∈ Pk , µ p =
R
1
(τ )
(x) is random. In parFk Xk 1{A(τ ) (x)=p} dm(x), and A
R
(τ )
1
k (τ )
ticular, E[µ p ] = Fk Xk πp (x)dm(x).
Since players are non-cooperative, we consider that players randomize independently. Under this assumption, the
distribution µ(τ ) is almost surely equal to its expectation.
Here, non-atomicity is essential.
Proposition 1. In the non-atomic routing game, if players
(τ )
randomize independently, then for all τ , µk is a random
variable with zero variance.
This follows from Fubini’s theorem. As a result, one can
think of the distribution µ as a deterministic variable, although individual players are randomizing.
Definition 3.1 (Online algorithm for routing). An online
algorithm (or update rule) for the routing game, applied
by a player x ∈ Xk , is a deterministic sequence of
functions (hτ )τ ∈N such that at iteration τ , hτ maps the
history of losses (`k (µ(t)))t≤τ and the current strategy
(τ +1)
π (τ ) (x) to the strategy on the
(x) =
 next iteration, π
k
(τ )
hτ (` (µ(t)))t≤τ , π (x) .
This online learning framework is summarized in Algorithm 1. Here, we assume that, at the end of day τ , a player
x ∈ Xk observes all the path latencies for her commodity,
i.e. (`kp (µ(τ ) ))p∈Pk . This can be achieved for example by
having a central authority publicly report the path latencies
at the end of a given day. We note however that the information model could be further restricted such that every player
only observes the latency on his/her own path. One appropriate framework to study this problem is that of multiarmed bandit learning, see for example Auer et al. (2002),
György et al. (2007), Dani et al. (2008), and Bubeck &
Cesa-Bianchi (2012). However, we do not currently consider this extension.

On the convergence of no-regret learning in selfish routing

3.2. Discounted regret
The regret is a natural measure of performance of a learning
algorithm (Cesa-Bianchi & Lugosi, 2006). In particular, we
are interested in online learning algorithms with sublinear
discounted regret. More precisely, we assume that losses
are discounted over time, by a decreasing sequence of factors (γτ )τ ∈N . So at iteration τ , a player who chooses path
p incurs a loss γτ `kp (µ(τ ) ). The sequence (γτ )τ is assumed
to be universal: the discounting is identical across players.
This can be justified from an economic perspective if one
thinks of discounting as reflecting interest rates.
Assumption 2. (γτ )τ is a positive, decreasing, nonsummable sequence.
The idea of discounted regret is common in the online
learning literature, and is studied for example by CesaBianchi & Lugosi in (2006). It is worth noting, however,
that the sequence is usually assumed to be increasing. In
our case, discounting the losses by a decreasing sequence
can be motivated by the assumption that players value future time less than current time. Given the sequence of discount factors, the discounted regret is defined as follows:
Definition 3.2 (Discounted regret). Consider a player x ∈
Xk . Given a sequence of strategies (π (τ ) (x))τ and a sequence of distributions (µ(τ ) )τ , the discounted regret of x
up to time T is:
R(T ) (x) = L(T ) (x) − min Lpk

(T )

p∈Pk

(3)

(T )

are, respectively, the expected
where L(T ) (x) and Lpk
discounted cumulative loss incurred by x, and the discounted cumulative loss on path p ∈ Pk :
L(T ) (x) =
(T )
Lpk

=

X

γτ

X

πp(τ ) (x)`kp (µ(τ ) )

τ ≤T

p∈Pk

X

γτ `kp (µ(τ ) )

τ ≤T

Definition 3.3 (Sublinear discounted regret). An online
learning algorithm for routing (hτ )τ is said to have sublinear discounted regret if whenever a player x applies the algorithm, for all initial strategies π (0) (x) and all sequences
(µ(τ ) ), lim supT →∞ P 1 γτ R(T ) (x) ≤ 0.

Definition 3.4 (Hedge algorithm). Consider a player x ∈
Xk . A Hedge algorithm with learning rates (γτ )τ is an
online algorithm (hτ )τ which satisfies the following update
equation:

(τ ) ) 
`k
p (µ
(τ +1)
(τ ) −γτ
ρ
(4)
π
∝ πp e
p∈Pk


=

πp(0) e

3.3. Discounted Hedge algorithm
We now give one example of online learning algorithm
with sublinear discounted regret.


(5)
p∈Pk

Next, we give a bound on the discounted regret of the
Hedge algorithm, a generalization of Lemma 5.1 in CesaBianchi & Lugosi (2006).
Proposition 2. If (γτ )τ is a sequence satisfying Assumption 2, the Hedge algorithm with learning rates (γτ )τ , applied by a player x ∈ Xk , has sublinear regret. More precisely, if ρ is a uniform upper bound on the sequence of
losses, then
(0)

R(T ) (x) ≤ −ρ log πmin (x) + ρ

X γ2
τ
.
8

τ ≤T
(0)

(0)

where πmin = minp πp

P
(0) up
k
Proof. Let ξ : u ∈ RP
7→ log( p∈Pk πp e ρ ). By
+
equation (5), we have for all τ :
ξ(L k

(τ −1)

(τ )

) − ξ(L k
)


(τ −1)
Lk
`k (µ(τ ) )
P
(0) − p ρ
−γτ p ρ
e
 p∈Pk πp e

= log 

k (τ −1)
L
P
− p ρ
p∈Pk πp (0)e


(τ ) )
`k
X
p (µ
= log 
πp(τ ) e−γτ ρ 
p∈Pk

≤ −γτ

X

πp(τ )

p∈Pk

`kp (µ(τ ) ) γτ2
+
ρ
8

The last inequality follows from Hoeffding’s lemma, since
0 ≤ `kp (µ(τ ) )/ρ ≤ 1.
Summing over τ , we have:

τ ≤T

An algorithm with sublinear discounted regret performs
asymptotically as well as the best constant strategy in hindsight.

L k (τ )
− pρ

ξ(Lpk

(T )

)≤−

L(T ) (x) X γτ2
+
ρ
8
τ ≤T

(T )

(0)

As log is increasing, ξ(Lpk ) ≥ log(πp )+Lpk
all p ∈ Pk . Rearranging, we have:
L(T ) (x) − Lpk

(T )

≤ −ρ log πp(0) + ρ

(τ )

X γ2
τ
8

τ ≤T

/ρ for

On the convergence of no-regret learning in selfish routing

we conclude by maximizing both sides with respect to
p ∈ Pk . This bound proves that
regret is
Pthe discounted

PT
T
sublinear since τ =1 γτ2 = o
γ
whenever
(γτ )τ
τ τ
satisfies Assumption 2.
Given the previous Proposition, discounting losses can be
interpreted, in the case of the Hedge algorithm, as using a
decreasing sequence of learning rates (γτ )τ .

Proof of Proposition 3. Let µ? ∈ N be a Nash equilibrium, i.e. µ? ∈ arg minµ∈∆ V (see Theorem 1). Then
by convexity of V and Equation (2),
E
D
V (µ(τ ) ) − VN ≤ ∇V (µ(τ ) ), µ(τ ) − µ?
≤

K
X

E
D
(τ )
Fk `k (µ(τ ) ), µk − µ? k

k=1

3.4. Population regret
We define the discounted regret for population Xk by integrating the individual regrets of players:
Z
1
k (T )
R(T ) (x)dm(x)
R
=
Fk Xk
If we define the average cumulative loss of populaR
(T )
tion Xk to be Lk
= F1k Xk L(T ) (x)dm(x) =
D
E
P
(T )
k (τ ) k
, ` (µ(τ ) ) , then we also have Rk
=
τ ≤T γτ µ
(T )

(T )

Lk
− minp∈Pk Lpk . As a consequence of this definition, if all players in Xk apply algorithms with sublinear
discounted regret, the population-wide regret is also sub(T )
≤ 0.
linear, that is, lim supT →∞ P 1 γτ Rk
τ ≤T

4. Convergence to Nash equilibria
4.1. Convergence on almost all days
We give a first convergence result. For µ ∈ ∆, let
d(µ, N ) = inf ν∈N kµ − νk where k · k is the Euclidean
distance on RP . We say that a sequence (µ(τ ) )τ converges
to the set N if d(µ(τ ) , N ) → 0.
Proposition 3 (Statistical convergence to Nash equilibria).
Consider a routing game with population dynamics such
that for all k, the population regret Rk is sublinear, and
let (µ(τ ) )τ be the sequence of path distributions. Then
there exists a subsequence (µ(τ ) )τ ∈T which converges to
N , defined
P on a subset T ⊂ N of density one, that is,
γτ
limT →∞ Pτ ∈T :τ ≤T γτ = 1.
τ ∈N:τ ≤T

In other words, the strategies converge on almost all days
if the population regret is sublinear. This is a limit case in
Theorem 5.1 in (Blum et al., 2006). We present a different proof which uses the Rosenthal potential function, and
which holds even if the latency functions are not Lipschitz
continuous. We first need the following technical Lemma.
Lemma 1. Let (γτ )τ ∈N be a non-summable sequence of
positive weights. If a real sequence (u(τ ) )τ ∈N converges
absolutely to u in the sense of Cesàro means w.r.t. (γτ )τ ,
P

γτ |u(τ ) −u|

P
that is limT →∞ τ ≤T
= 0, then there exists a
τ ≤T γτ
subset of indexes T of density one such that the subsequence (u(τ ) )τ ∈T converges to u.

here we use h·, ·i to denote the inner product on RP . Then,
taking the weighted sum up to time T ,
γτ (V (µ(τ ) ) − VN )
P
τ ≤T γτ
D
E D
E
P
(τ )
(T )
γτ µk , `k (µ(τ ) ) − µ? k , L k
K
X τ ≤T
P
≤
Fk
γτ

P

τ ≤T

k=1

≤

K
X
k=1

R
Fk P

k (T )

τ ≤T

τ ≤T

γτ

where
the E
last inequality follows from the fact that
D
(T )
?
k (T )
µ ,L
≥ minp Lpk . Since, for all τ , V (µ(τ ) ) −
VN ≥ 0, and for all k, lim supT →∞

P 1

τ ≤T

γτ

Rk

(T )

≤ 0

by assumption, we have (V (µ(τ ) ))τ converges absolutely
to VN in the sense of Cesàro means w.r.t. (γτ )τ . Thus by
Lemma 1, there exists a dense subset of indexes T such
that (V (µ(τ ) ))τ ∈T converges to VN , and by continuity of
V and compactness of ∆, the subsequence (µ(τ ) )τ ∈T converges to N .

In order to show strong convergence for a class of online
algorithms with sublinear discounted regret, we first study
the continuous-time replicator dynamics, which can be motivated as a continuous-time limit of the Hedge algorithm,
as discussed next.
4.2. Continuous-time dynamics
We consider the discounted Hedge algorithm with a vanishing sequence of learning rates (γτ ), acting on the sequence
of population strategies (µ(τ ) )τ ∈N .
Let us imagine an underlying continuous time T ∈ R+ ,
and set µ(Tτ ) = µ(τ ) , where Tτ is the time at which the
τ -th update happens. Now choosing the update times to be

On the convergence of no-regret learning in selfish routing
p1

2

2

p0

1.5
1

(τ )

: Nash equilibrium

p0

1.5
1

0.5

µ1
10

20

30

40

50

τ

2

µ1

0.5

: uniform

p2

0

10

20

µ2

40

50

(0)

: uniform

p3

1

(0)

: uniform

p2

p4

path p3 = (v2 , v4 , v5 , v3 )
path p4 = (v2 , v4 , v6 , v3 )
path p5 = (v2 , v3 )

2.5
2

1.5

30
τ

p4

path p3 = (v2 , v4 , v5 , v3 )
path p4 = (v2 , v4 , v6 , v3 )
path p5 = (v2 , v3 )

2.5

(0)

`2p (µ(τ ) )

0

`2p (µ(τ ) )

lim µ1

τ →∞

path p0 = (v0 , v4 , v5 , v1 )
path p1 = (v0 , v4 , v6 , v1 )
path p2 = (v0 , v1 )

2.5

`1p (µ(τ ) )

2.5

`1p (µ(τ ) )

p1

path p0 = (v0 , v4 , v5 , v1 )
path p1 = (v0 , v4 , v6 , v1 )
path p2 = (v0 , v1 )

µ2

(0)

: uniform

p3

1.5
1

0.5

0.5
0

10

20

30

40

50

τ

0

p5

10

20

30

40

50

τ

p5

lim µ2

τ →∞

(τ )

: Nash equilibrium

Figure 2. Example of a routing game payed on the example network of Figure 1. Latency functions are taken to be quadratic increasing,
(τ )
and generated randomly. The population strategies (µk )τ obey the Hedge algorithm. The figures show the trajectories in the simplex
Pk
k
(τ )
∆ , and the resulting path latencies (`p (µ )) for population X1 (top) and X2 (bottom). With a constant learning rate γ = 0.7,
(τ )

(µk )τ does not converge (left). With a harmonic sequence of learning rates, γτ =
equilibria.

Tτ =

Pτ

t=1

(τ +1)
k

=

(τ )
µkp P

e−γτ `p (µ

p0 ∈Pk

µkp0

(τ )

(µk

(τ )

)τ converges to the set of Nash

˚ = {µ ∈ ∆ : ∀p ∈ P, µp > 0} is the relative
Here, ∆
˚ guarantees that µ(t) remains
interior of ∆. Starting in ∆
˚
in ∆ for all t. In this derivation, the discount factors γτ
are interpreted as discrete time steps. The dynamics described by this ODE, called the replicator dynamics (Fischer & Vöcking, 2004), has been studied extensively. One
can observe in particular that the set RN of restricted Nash
equilibria (Definition 2.2) is exactly the set of stationary
points for the ODE.

γt , we can write, ∀p ∈ Pk

µkp (Tτ +1 ) = µkp

1
,
1+τ /10

)/ρ

(τ ) −γτ `k0 (µ(τ ) )/ρ
p

e

1 − γτ `kp (µ(τ ) )/ρ + o(γτ )
P
(τ )
1 − γτ p0 ∈Pk µkp0 `kp0 (µ(τ ) )/ρ + o(γτ )
"
#

 (τ ) k (τ ) 
(τ )
k
(µ
)
µ
,
`
(µ
)
−
`
p
= µkp (Tτ ) 1 + γτ
+ o(γτ ) 4.3. Replicator updates
ρ
By discretizing the replicator dynamics, we obtain a multiThus,
plicative update rule we call REP for Replicator, which has
desirable properties which we prove next.
k
k
µp (Tτ + γτ ) − µp (Tτ )
=
Definition 4.1 (REP algorithm). The replicator (REP) alγ

 kτ

gorithm with rates (γτ )τ , γτ ≤ 1, applied by x ∈ Xk , is an
µ (Tτ ), `k (µ(Tτ )) − `kp (µ(Tτ ))
k
online algorithm for routing given by the following update
µp (Tτ )
+ o(1)
ρ
equation
= µkp

(τ )

taking the limit of the above equation as γτ → 0, we obtain
the following ODE
(
˚
µ(0) ∈ ∆
(6)
dµ(t)
dt = G(µ(t), `(µ(t)))
where ∀k and ∀p ∈ Pk
Gkp (µ, `)

=

µkp


 k k
µ , ` − `kp
ρ

(7)

πp(τ +1) − πp(τ ) = γτ Gkp (π (τ ) , `(µ(τ ) ))

(8)

We note that summing this update equation over p ∈ Pk
P
(τ +1)
(τ )
yields p∈Pk (πp
− πp ) = 0, thus π remains in ∆Pk
as long as γτ ≤ 1. We now show that the REP update rule
guarantees a sublinear discounted regret. To see this, we
need the following regret bound on multiplicative-weights
updates with signed losses.
Lemma 2. Consider an online learning setting with signed

On the convergence of no-regret learning in selfish routing
(τ )

losses sp ∈ [−1, 1] and discount factors γτ ≤ 1/2 satisfying Assumption 2. Then the discounted multiplicativeweights algorithm defined by


)
π (τ +1) ∝ πp(τ ) (1 − γτ s(τ
)
(9)
p
p

guarantees that for any p,
E

X D
X
(0)
)
γτ
π (τ ) , s(τ ) − s(τ
≤ − log πmin +
γτ2
p
τ ≤T

τ ≤T

This Lemma is a straightforward extension of Theorem 2.1
in (Arora et al., 2012) to the discounted case.
Proposition 4. If (γτ )τ is a sequence of discount factors
satisfying Assumption 2 and such that γτ ≤ 1/2 for all
τ , the (REP) update rule with rates (γτ )τ has sublinear
discounted regret.



(τ )
Proof. Let rp = π (τ ) , `k (µ(τ ) ) −`kp (µ(τ ) ) ∈ [−ρ, ρ] be
the instantaneous regret of the player. Then the REP update
can be viewed as a multiplicative-weights algorithm with
(τ )
(τ )
update rule (9), signed losses sp = −rpk /ρ ∈ [−1, 1],
D
E
(τ )
and discount factors γτ . Observing that rk , π (τ ) = 0,
we have by Lemma 2:

Condition (11) corresponds to the first hypothesis of Proposition 4.1 in (Benaı̈m, 1999), which we will use in the proof
of the main convergence theorem. It bounds the cumulative perturbation over a given time interval T . Intuitively,
this condition will ensure that the trajectories of a discrete
AREP algorithm are asymptotically close to the trajectories
of the continuous-time replicator dynamics.
Note that the REP update rule is an AREP algorithm with
zero perturbation. By allowing perturbations, we extend
the class of algorithms for which we can show convergence.
In particular, we show that the discounted Hedge algorithm
is in this class.
Proposition 5. The Hedge algorithm with learning rates
(γτ )τ satisfying Assumption 2 is an AREP algorithm.
Proof. Let (π (τ ) )τ ∈N be the sequence of player strategies,
and (µ(τ ) )τ be any sequence of population distributions.
By definition of the Hedge algorithm,
πp(τ +1)

τ ≤T

which shows lim supT →∞ P

1

τ ≤T

γτ

R(T ) (x) ≤ 0.

πp(τ +1) − πp(τ ) = γτ (Gkp (π (τ ) , `(µ(τ ) )) + Up(τ +1) ) (10)
where (U (τ ) )τ ≥1 is a bounded sequence of stochastic perturbations with values in RPk , and which satisfies the following condition: for all T > 0,
 τ

2
X


(τ +1) 
lim
max
γ
U
(11)

=0
τ
τ2
τ1 →∞
τ =τ

P
τ2 :

τ =τ1

γτ <T

1

X

/

(τ )
πp0 e−γτ

`k0 (µ(τ ) )
p
ρ

k
(τ )
˜k (τ )
(τ )
`kp (µ(τ ) ) − `˜k (τ )
πp h −γτ `p (µ ρ)−`
e
+ γτ
γτ
ρ
i
k (τ )
k (τ )
˜
¯
`
−
`
− 1 + πp(τ )
ρ
X (τ ) −γ `k (µ(τ ) )/ρ
ρ
k
(τ
)
πp0 e τ p0
`˜
= − log
γτ
p0 ∈Pk
D
E
k
(τ
)
(τ
)
k
`¯
= π , ` (µ(τ ) )

Up(τ +1) =

Letting θ(x) = ex − x − 1, we have for all p ∈ Pk :
!
(τ )
(τ )
˜k(τ )
π
`
(µ
)
−
`
p
p
Up(τ +1) =
θ −γτ
γτ
ρ
+

4.4. Approximate Replicator algorithms
Definition 4.2 (AREP algorithm). An online algorithm for
routing, applied by x ∈ Xk , is said to be an approximate
replicator algorithm (AREP) if its update equation can be
written as

ρ

which we can write in the form of equation (10), with

τ ≤T

Rearranging and taking the maximum over p ∈ Pk , we
obtain the following bound on the discounted regret
X
(0)
R(T ) (x) ≤ −ρ log πmin + ρ
γτ2

(τ ) )
`k
p (µ

p0 ∈Pk

X
1 X
(τ )
γτ rk p ≤ − log πmin (0) +
γτ2
ρ
τ ≤T

=

πp(τ ) e−γτ

(τ )
πp  ˜k(τ ) ¯k(τ ) 
`
−`
ρ

The first term is a O(γτ ) as θ(x) ∼0 x2 /2. To bound the
second term, we have by concavity of the logarithm
X (τ )
`˜k(τ ) ≤
π 0 `k0 (µ(τ ) ) = `¯k(τ )
p

p

p0 ∈Pk

And by Hoeffding’s lemma,
(τ ) )

log

X
p0 ∈Pk

πp0 e

` 0 (µ
−γτ p ρ

≤ −γτ

X
p0 ∈Pk

(τ ) `p0 (µ

πp0

ρ

(τ )

)

+

γτ2
8

Rearranging, we have 0 ≤ `¯k(τ ) − `˜k(τ ) ≤ ργ8τ ,
Pτ
(τ +1)
therefore Up
= O(γτ ), and k τ2=τ1 γτ U (τ +1) k =
Pτ2
O( τ =τ1 γt2 ). Finally, since γτ decrease to 0 by Assumption 2, condition (11) is verified.

On the convergence of no-regret learning in selfish routing

4.5. Strong convergence to Nash equilibria
k (τ )

Theorem 2. If for all k, the population strategies (µ
)τ
satisfy an AREP algorithm with sublinear regret, then the
sequence (µ(τ ) ) converges to the set of Nash equilibria.
Proof. The proof proceeds in two steps: first, we use results from (Benaı̈m, 1999) to prove that the sequence of
potentials V (µ(τ ) ) converges. Then, using convergence on
most days given in Proposition 3, we conclude that V (µ(τ ) )
converges necessarily to the minimum VN , which proves
that (µ(τ ) ) converges to N by continuity of V on the compact ∆. First, we recall the definition of Lyapunov function.
Definition 4.3 (Lyapunov function). Let Γ ⊂ ∆ be a compact invariant set for the replicator ODE (6). A continuous
non-negative function V : ∆ → R+ is a Lyapunov function
d
V (µ(t)) = h∇V (µ(t)), G(µ(t), `(µ(t)))i < 0
for Γ if dt
for all µ(t) ∈
/ Γ.
Lemma 3 (Convergence of potentials under AREP algorithms). Let Γ be a compact invariant set for the replicator ODE (6), V a Lyapunov function for Γ, and assume
V (Γ) has empty interior. Assume that the sequence of distributions (µ(τ ) )τ ∈N obeys an AREP update rule. Then the
sequence of potentials (V (µ(τ ) ))τ converges.

Corollary 1. If (γτ )τ is a sequence bounded by 1/2 satisfying Assumption 2, and (µτ )τ obeys the REP update rule
with rates (γτ ), then (µτ )τ converges to the set of Nash
equilibria.
Proof. Under these assumptions, the REP algorithm has
sublinear discounted regret by Proposition 4. It is also an
AREP algorithm (with zero perturbations) so we can apply
Theorem 2.
Corollary 2. If (γτ )τ is a sequence satisfying Assumption 2, and (µτ )τ obeys the discounted Hedge algorithm
with rates (γτ ), then (µτ )τ converges to the set of Nash
equilibria.
Proof. By Proposition 2 and Proposition 5, the Hedge algorithm with rates γτ is an AREP algorithm with sublinear
discounted regret, and we can apply Theorem 2.
Figure 2 shows an example of discounted Hedge algorithm with a decreasing, non-summable sequence of learning rates. The resulting strategies converge to the set of
Nash equilibria.

5. Conclusion

In order to obtain strong convergence guarantees of online learning algorithms applied to routing games, we conThis follows from Theorem 5.7 and Proposition 4.1
sider a model in which losses are discounted. We studin (Benaı̈m, 1999). Here, condition (11) is essential.
ied a continuous-time limit of the Hedge algorithm. This
Next, we show that the Rosenthal potential function V is
motivated the introduction of a class of no-regret learning
a Lyapunov function for the invariant set RN of restricted
algorithms, called AREP, which can be viewed as approxiNash equilibria. From equation (2) and the definition of G,
mations of the replicator dynamics. Using results from the




theory of stochastic approximation, we showed that under
∇V µ(t) , G(µ(t), `(µ(t)))
D
E
 this class, (µ(τ ) ) is guaranteed to converge to the set of
X k
X
`p (µ(t))µkp (t) µk (t), `k (µ(t)) − `kp (µ(t))
=
Fk
Nash equilibria.
k

p∈Pk

2


=

X
k

Fk 

X

p∈Pk

µkp (t)`kp (µ(t))


−

X

µkp (t)`kp (µ(t))2 

p∈P

which is less than or equal to 0 by Jensen’s inequality, with
equality if and only if µ ∈ RN . Therefore V is a Lyapunov
function for RN . And since V (RN ) is a finite set by Remark 1, it has empty interior relatively to R, and we can
apply Lemma 3, and conclude that the sequence of potentials (V (µ(τ ) ))τ ∈N converges. It remains to show that its
limit is VN .
Since the AREP algorithm is assumed to have sublinear discounted regret, we can apply Proposition 3: there
exists a dense subsequence (µ(τ ) )τ ∈T which converges
to N . The corresponding subsequence of potentials
(V (µ(τ ) ))τ ∈T converges to VN by continuity of V , and by
uniqueness of the limit, we must have limτ V (µ(τ ) ) = VN .
This concludes the proof.

These results assume a universal sequence (γτ )τ of discounts; thus a natural question is whether convergence still
holds if this assumption is relaxed. Another open question is whether the learning algorithm is robust to observation noise: if latency observations are noisy, with bounded
noise, can one guarantee convergence if the bound is small
enough?

On the convergence of no-regret learning in selfish routing

References
Arora, Sanjeev, Hazan, Elad, and Kale, Satyen. The multiplicative weights update method: a meta-algorithm and
applications. Theory of Computing, 8(1):121–164, 2012.

Littlestone, Nick and Warmuth, Manfred K. The weighted
majority algorithm. In Foundations of Computer Science, 1989., 30th Annual Symposium on, pp. 256–261.
IEEE, 1989.

Auer, Peter, Cesa-Bianchi, Nicolò, and Fischer, Paul.
Finite-time analysis of the multiarmed bandit problem.
Mach. Learn., 47(2-3):235–256, May 2002.

Monderer, Dov and Shapley, Lloyd S. Fictitious play property for games with identical interests. journal of economic theory, 68(1):258–265, 1996.

Beckmann, Martin J, McGuire, Charles B, and Winsten,
Christopher B. Studies in the economics of transportation. 1955.

Papadimitriou, Christos H. On the complexity of the parity
argument and other inefficient proofs of existence. Journal of Computer and system Sciences, 48(3):498–532,
1994.

Benaı̈m, Michel. Dynamics of stochastic approximation
algorithms. In Séminaire de probabilités XXXIII, pp. 1–
68. Springer, 1999.
Blum, Avrim, Even-Dar, Eyal, and Ligett, Katrina. Routing without regret: on convergence to nash equilibria of
regret-minimizing algorithms in routing games. In Proceedings of the twenty-fifth annual ACM symposium on
Principles of distributed computing, PODC ’06, pp. 45–
52, New York, NY, USA, 2006. ACM.
Bubeck, Sébastien and Cesa-Bianchi, Nicolò. Regret analysis of stochastic and nonstochastic multi-armed bandit
problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.
Cesa-Bianchi, Nicolò and Lugosi, Gábor. Prediction,
learning, and games. Cambridge University Press, 2006.
Dafermos, Stella C and Sparrow, Frederick T. The traffic
assignment problem for a general network. Journal of
Research of the National Bureau of Standards, Series B,
73(2):91–118, 1969.
Dani, Varsha, Hayes, Thomas, and Kakade, Sham. The
price of bandit information for online optimization. In
Advances in Neural Information Processing Systems 20,
pp. 345–352, Cambridge, MA, 2008. MIT Press.
Fischer, Simon and Vöcking, Berthold. On the evolution of
selfish routing. In Algorithms–ESA 2004, pp. 323–334.
Springer, 2004.
Fischer, Simon, Räcke, Harald, and Vöcking, Berthold.
Fast convergence to wardrop equilibria by adaptive sampling methods. SIAM Journal on Computing, 39(8):
3700–3735, 2010.
Freund, Yoav and Schapire, Robert E. Adaptive game playing using multiplicative weights. Games and Economic
Behavior, 29(1):79–103, 1999.
György, András, Linder, Tamás, Lugosi, Gábor, and Ottucsák, György. The on-line shortest path problem under
partial monitoring. Journal of Machine Learning Research, 8:2369–2403, December 2007.

Rosenthal, Robert W. A class of games possessing purestrategy nash equilibria. International Journal of Game
Theory, 2(1):65–67, 1973.
Roughgarden, T. Routing games. In Algorithmic game
theory, chapter 18, pp. 461–486. Cambridge University
Press, 2007.
Sandholm, William H. Potential games with continuous
player sets. Journal of Economic Theory, 97(1):81–108,
2001.
Wardrop, John Glen. Some theoretical aspects of road traffic research. In ICE Proceedings: Engineering Divisions, volume 1, pp. 325–362. Thomas Telford, 1952.
Weibull, Jörgen W. Evolutionary game theory. MIT press,
1997.

