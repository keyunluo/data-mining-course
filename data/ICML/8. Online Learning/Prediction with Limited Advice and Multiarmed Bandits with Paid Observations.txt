Prediction with Limited Advice and Multiarmed Bandits with Paid
Observations
Yevgeny Seldin
Queensland University of Technology and UC Berkeley

YEVGENY. SELDIN @ GMAIL . COM

Peter Bartlett
UC Berkeley and Queensland University of Technology

BARTLETT @ EECS . BERKELEY. EDU

Koby Crammer
The Technion

KOBY @ EE . TECHNION . AC . IL

Yasin Abbasi-Yadkori
Queensland University of Technology and UC Berkeley

Abstract
We study two problems of online learning under restricted information access. In the first
problem, prediction with limited advice, we consider a game of prediction with expert advice,
where on each round of the game we query
the advice of a subset of M out of N experts.
q We present
 an algorithm that achieves

N
O
regret on T rounds of this
M T ln N
game. The second problem, the multiarmed bandit with paid observations, is a variant of the adversarial N -armed bandit game, where on round
t of the game we can observe the reward of
any number of arms, but each observation has
a cost
 c. We present an algorithm
that achieves
√
1/3 2/3
O (cN ln N ) T
+ T ln N regret on T
rounds of this game in the worst case. Furthermore, we present a number of refinements that
treat arm- and time-dependent observation costs
and achieve lower regret under benign conditions. We present lower bounds that show that,
apart from the logarithmic factors, the worst-case
regret bounds cannot be improved.

1. Introduction
We study two problems of online learning under restricted
information access. The first problem is a variation of the
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

YASIN . ABBASI @ GMAIL . COM

game of prediction with expert advice (see, for example,
(Cesa-Bianchi & Lugosi, 2006)), which we call prediction
with limited advice. In this game, the player has access
to a set of N experts, but on each round of the game is allowed to query the advice of only M of the N experts. This
game corresponds, for example, to a situation where each
expert is a computationally-expensive function and there
is a constraint on the response time. Because of the constraint, it may be possible to compute only a subset of M
of the N functions. We
q provide an
 algorithm for this setN
ting that achieves O
M T ln N regret on T rounds and
a matching lower bound (up to logarithmic factors).

We note that there is a tight connection between prediction
with limited advice and multiarmed bandits. In particular,
if we ask for the advice of just one expert on every round
(meaning that M = 1), the problem of prediction with limited advice becomes equivalent to an N -armed bandit problem (we can treat each expert as an arm). Furthermore, if
M > 1 and we restrict the algorithm to follow the advice of
one of the M experts (rather than playing some function of
the advice) the problem is equivalent to an N -armed bandit where we play one arm and are allowed to observe the
reward of M − 1 additional arms. As M grows from 1 to
N the game of prediction with limited advice interpolates
between a limited-feedback game and a full-information
game.Our regretbound provides an interpolation between
√
T ln N regret bound for full-information games
the O

√
and the O
N T regret bound for bandit games (CesaBianchi & Lugosi, 2006; Audibert & Bubeck, 2010).
The second question studied in this work considers a different type of restriction on information acquisition. We

Prediction with Limited Advice and Multiarmed Bandits with Paid Observations

define a variation of the adversarial N -armed bandit game,
which we call the multiarmed bandit with paid observations. On each round of this game, the player pulls one
arm and suffers the loss of that arm, but that loss is not
necessarily observed. The player has the option to request
to observe the loss of any number of arms, but the cost of
each observation is c and it is added to the loss of the player.
As a motivational example, we can think about a problem
of signing annual contracts with service providers. For instance, imagine a medical insurance company that every
year signs an annual contract with a hospital for some set
of medical services. The insurance company can choose to
order a follow-up survey of service quality in any number
of hospitals from an independent inspection body, but each
inspection will be associated with inspection cost c. The
goal is, of course, to maximize service quality and minimize the cost of inspections. We derive an algorithm for
this setting that ensures that the regret (that is, the observation costs plus the excessloss over the loss of the best fixed

√
1/3
arm in hindsight) is O (cN ln N ) T 2/3 + T ln N .
Note that we achieve a smooth transition between prediction with expert advice (which corresponds to zero observations cost, c = 0, since when the cost is zero we can observe all arms for free) and the harder game with c > 0. For
c > 0 sublinear regret is achieved by gradual decrease of
exploration (the number of observations made), eventually
getting into a regime where no observations are made on
some rounds. We also provide a matching lower bound (up
to logarithmic factors). Furthermore, we present a refined
algorithm that handles arm- and time-dependent observations costs and reduces the regret under benign conditions.
1.1. Related Work
Our work is not the first attempt to investigate what
happens between full-information and limited-feedback
games. Mannor & Shamir (2011) provided an alternative
approach. Specifically, they considered an N -armed bandit game, where at each round there is a graph and the actions correspond to the nodes of this graph. When playing
a node in the graph the player observes the reward of the
node played and the rewards of all adjacent nodes in the
graph. The work of Mannor and Shamir was further simplified, improved, and generalized by Alon et al. (2013). The
main difference between this line of work and our work is
that we allow complete freedom in the choice of observations to make (in prediction with limited advice the only
restriction is the number of observations and in multiarmed
bandits with paid observations there are no restrictions at
all).
Other related work is that of Avner et al. (2012) on “decoupling exploration and exploitation”. Avner et. al. studied a
multiarmed bandit game, where on each round the player is

allowed to play one arm and to observe the reward of one
arm, but not necessarily the same arm that was played. Although coming from a different motivation, our work (especially Theorem 3) can be seen as a generalization of the
work of Avner et. al. along two dimensions. First, we allow any number of observations on every round (including
none) rather than making exactly one observation and, second, we take the cost of observations into account. Our
work can also be seen as a generalization of label-efficient
prediction (Cesa-Bianchi et al., 2005) and label-efficient
bandits (Ottucsák & György, 2006; Audibert & Bubeck,
2010). In label-efficient prediction the player can choose to
observe the loss of all arms or nothing and in label-efficient
bandits the player can choose to observe the loss of the arm
played or nothing and learning is done under a constraint on
the total number of observations that can be made throughout the game. In our formulation the observed arm(s) does
not have to be the one played and there may be any number of observations per round, which allows to improve the
exploration strategy.
The effect of an information acquisition cost appears
implicitly in locally non-observable partial monitoring
games (Bartók et al., 2011; Foster & Rakhlin, 2012).
Roughly speaking, local non-observability means that two
desirable actions differ in their loss but are identical in their
feedback, so that it is necessary to play a third action with
higher loss in order to obtain information. Similarly to the
results in locally non-observable partial monitoring games,
our regret bound for the multiarmed bandit with paid observations scales as T 2/3 . We note that casting multiarmed
bandit with paid observations game as a partial monitoring
game leads to an exponential increase of the size of the action set (since we have to consider all possible subsets of
actions for the observation requests) and, as a result, suboptimal regret bounds.
Zolghadr et al. (2013) have recently introduced the online
probing game. In online probing the learner has to predict
labels of feature vectors when there is a cost for observing
entries of the feature vectors and for observing the labels
at the end of each prediction round. This game shares with
our work the spirit of an online game with restricted information access. We believe that it will be possible to make
mutual transfer of ideas in future work.

2. Main Results
In this section, we provide formal definitions of the games
and present our main results. We start with prediction with
limited advice in Section 2.1 and then present multiarmed
bandits with paid observations in Section 2.2. More illuminating proofs are provided in Section 3, whereas more
technical results are provided in the appendix.

Prediction with Limited Advice and Multiarmed Bandits with Paid Observations

2.1. Prediction with Limited Advice
The definition of this game is based on the setting of prediction with expert advice described in Cesa-Bianchi & Lugosi (2006). We denote the action space by X , the outcome
space by Y, and the loss function by ` : X × Y → [0, 1]
(for our analysis there is no need to assume that the loss is
convex in the first parameter). The number of experts is denoted by N and the experts are indexed by h ∈ {1, . . . , N }.
On each round t of the game, each expert h produces a
piece of advice ξth ∈ X , which is not necessarily observed
by the player. The player gets a budget 1 ≤ Mt ≤ N
and picks a subset Ot ⊆ {1, . . . , N } of Mt experts and
observes their advice. The player then plays an action
Xt ∈ X , the environment reveals an outcome yt ∈ Y, the
player suffers a loss Lt = `(Xt , yt ), and the experts suffer
losses `ht = `(ξth , yt ). The player observes the losses of all
experts in Ot , but gets no information on the losses of experts that are not in Ot . We study the problem in a slightly
restricted setting, where the player has to follow the advice
of one of the experts (rather than playing some function of
the experts advice).

Algorithm 1 Prediction with limited advice.
Remark: ηt is defined in Theorem 1.
Input: M1 , M2 , . . . , such that Mt ∈ {1, . . . , N }.
∀h: L̂0 (h) = 0.
for t = 1, 2, ... do
Let
qt (h) = P

e−ηt L̂t−1 (h)

h0

e−ηt L̂t−1 (h0 )

.

Draw one expert Ht according to qt . Get advice ξtHt .
Sample Mt − 1 additional experts uniformly without
replacement. Denote by Ot the set of sampled experts
(we have Ht ∈ Ot and the cardinality of Ot is Mt )
and let 1ht = 1{h∈Ot } .
Play Xt = ξtHt .
Observe outcome yt and suffer loss Lt = `(Xt , yt ).
∀h : Lht =

`ht
1ht .
t −1
qt (h) + (1 − qt (h)) M
N −1

∀h : L̂t (h) =

Prediction with Limited Advice Game

t
X

(1)

Lhs .

s=1

For t = 1, 2, . . . :

end for

1. The algorithm gets Mt and plays (Ht , Ot ), such that
Ot ⊆ {1, . . . , N } and |Ot | = Mt and Ht ∈ Ot .
2. The environment reveals `ht for h ∈ Ot and the algot
rithm suffers the loss `H
t .
We emphasize that in this game the number of observations
Mt is provided externally to the algorithm and it is assumed
that Mt ≥ 1. We evaluate the performance of algorithms
by their regret defined as
( T
)
" T
#
X
X
`ht .
RT = E
Lt − min
t=1

h

against an oblivious adversary satisfies:
T
N X ηt
ln N
RT ≤
.
+
2 t=1 Mt
ηT

In particular, for ηt =

q
N

N
Pln
t

v
u
u
RT ≤ 2tN

we have:

1
s=1 Ms

T
X
1
M
t
t=1

!
ln N .

t=1

Our first result is an anytime (i.e., independent of the time
horizon) algorithm for prediction with limited advice (see
Algorithm 1) and a corresponding bound on its regret in
Theorem 1. We note that for M = N the algorithm recovers the exponentially weighted average forecaster algorithm for prediction with expert advice (Cesa-Bianchi &
Lugosi, 2006) (with the restriction that we have to follow
the advice of one expert rather than a linear combination of
expert advice) and for M = 1 it recovers the EXP3 algorithm for multiarmed bandits (Auer et al., 2002a; Bubeck &
Cesa-Bianchi, 2012). The proof of the theorem is provided
in Section 3.
Theorem 1. For any non-increasing positive sequence
η1 ≥ η2 ≥ · · · > 0 the expected regret of Algorithm 1

If Mt = M is constant and ηt =
r
RT ≤ 2

q

M ln N
tN

then:

N
T ln N .
M

The “price” that we pay for observing the advice
q of M
N
rather than all N experts is the multiplicative M
term.
In Theorem 2 we provide a matching lower bound, showing that this price is inevitable without additional assumptions. For convenience, Theorem 2 is stated for N -armed
bandits, where we are allowed to make arbitrary observations. Theorem 2 holds for any algorithm that makes M T
observations throughout the game, no matter how they are
distributed. In particular, the number of observations can

Prediction with Limited Advice and Multiarmed Bandits with Paid Observations

depend on the past observations and the number of observations on some rounds can be zero. We note that M in
Theorem 2 does not have to be an integer, it is only the
product M T that is assumed to be integer. The proof of the
theorem is provided in the appendix.

Algorithm 2 Multiarmed Bandits with Paid Observations.
Remark: ηt is defined in Theorem 3.
∀h: L̂0 (h) = 0.
for t = 1, 2, ... do

Theorem 2. For the N -armed bandit game with M T ob3 N
served rewards and T ≥ 16
M,
r
N
inf sup RT ≥ 0.03
T,
M

∀h : qt (h) = P

Lhs .

s=1

end for
oblivious adversary satisfies:
RTc

≤

T
X
t=1

h=1

!
N p
X
ηt p
ln N
+ 2ηt
qt (h)ct (h) +
.
2
ηT
h=1

ct (h)+

1. The algorithm observes ct (1), . . . , ct (N ).
∈

`ht

for h ∈ Ot and the algo3. The environment reveals P
Ht
t
rithm suffers the loss `H
+
t
h∈Ot ct (h), where `t
is not necessarily observed.
We emphasize that in this game the number of observations
is chosen by the algorithm and it may be equal to zero. In
Algorithm 2 box we present an algorithm for this problem,
which is analyzed in Theorem 3 (the proof is proved in Section 3). We use
" T
#
" T
#
( T
)
X
X X
X
c
h
RT = E
Lt + E
ct (h) − min
`t
t=1 h∈Ot

t
X

∀h : L̂t (h) =

η t =  √P
N

For t = 1, 2, . . . :

t=1

.

In particular, if

Multiarmed Bandits with Paid Observations Game

h

e−ηt L̂t−1 (h0 )

For each h query the loss of h with probability pt (h).
Let 1ht = 1 if the loss of h was observed and 1ht = 0
otherwise.
`h
∀h : Lht = t 1ht .
pt (h)

2.2. Multiarmed Bandits with Paid Observations

2. The algorithm plays (Ht , Ot ), such that Ht
{1, . . . , N } and Ot ⊆ {1, . . . , N }.

h0

Draw action Ht according to qt and play it.
( s
)
ηt qt (h)
∀h : pt (h) = min 1,
.
2ct (h)

where the infimum is over all playing strategies and the
supremum is over all oblivious adversaries.

In order to stress the relation with the game of prediction
with limited advice, we use N to denote the number of arms
in the multiarmed bandit and h ∈ {1, . . . , N } to index the
arms. On every round of the game, the algorithm gets a
vector of non-negative costs ct (1), . . . , ct (N ) for observing the outcomes of the arms. The algorithm then plays
one arm, denoted by Ht , and can request to observe the rewards of any P
subset of arms Ot ⊆ {1, . . . , N }. The cost of
observations h∈Ot ct (h) is added to the regret of the algorithm. The set of observed arms Ot can be empty. Even
when Ot is not empty, Ht does not have to be in Ot (in
other words, even when we make observations we are not
obliged to observe the outcome of the arm that we played).
Formally, the game proceeds as follows.

e−ηt L̂t−1 (h)

t=1

to denote cost-sensitive regret.
Theorem 3. For any non-increasing positive sequence
η1 ≥ η2 ≥ · · · > 0 the regret of Algorithm 2 against an

Pt−1 PN

1
√

h=1
√s=1
2
3 ln N

qs (h)cs (h)

2/3
+

q

t
ln N

is a non-increasing sequence we have:
RTc
v
2/3
uN
T
−1 X
N p
uX
X
1/3 t
≤ (32 ln N )
cT (h) +
qt (h)ct (h)
√

t=1 h=1

h=1

+ 2 T ln N .

(2)

For
ηt = 
P

1
√PN

t
s=1 √

2
3

h=1

cs (h)

ln N

2/3
+

q

t
ln N

(which is always a non-increasing sequence) we have:
v

2/3
T u
N
u
X
X
√
1/3
t
RTc ≤ (32 ln N ) 
ct (h) + 2 T ln N .
t=1

h=1

(3)

Prediction with Limited Advice and Multiarmed Bandits with Paid Observations

Finally, if the cost of observations is uniform over the arms
and game rounds (ct (h) = c for all t and h) the first regret
bound simplifies to
!2/3
T
−1 X
N p
X
√
1/3
N+
qt (h)
RTc ≤ (32c ln N )
√
+ 2 T ln N

t=1 h=1

(4)

(in this case ηt is always a non-increasing sequence) and
the second regret bound simplifies to
√
1/3
RTc ≤ (32cN ln N ) T 2/3 + 2 T ln N .
(5)
The constant in the regret bounds satisfiesp(32)1/3 < 3.2.
PN
Note
qt (h)ct (h) ≤
h=1
qP that by Jensen’s inequality
N
h=1 ct (h), and so bound (2) (when it holds) is tighter
than (3) and bound (4) is tighter than (5). If some arm
h∗ dominates all other arms, asymptotically the distri∗
bution
qt converges
to a delta distribution
p
PT P
PT onph and
N
qt (h)ct (h) converges to
ct (h∗ )
t=1
h=1
t=1
PT p
and for the uniform costs t=1 qt (h) converges to T . In
such case bound (2) improves the dependence on the cost
of observations of suboptimal arms and (4) improves the
dependence on the number of arms (compared to (3) and
(5), respectively).
In general, the first term of the regret bounds in Theorem 3 is dominating, unless the cost of observations is very
small. When the cost of observations is very small the second term of the regret bounds dominates. For the uniform
cost setting
q in Eq. (5) the domination switchover occurs at

2
ln N
c = 9N
T . At the extreme of zero cost of observations
the algorithm and the regret bound match the prediction
with expert advice setting, where all arms are observed on
all rounds.

Another interesting observation is the decrease rate in the
number of observations made by the algorithm. For the
uniform costs setting
the number
of observations per round


(N ln N )1/3
decreases as Θ c2/3 t1/3 . When the cost of observations is relatively small compared to the time horizon, the
regime when the algorithm queries more than one observation per round has significant interest.
The last result of Theorem 3 is accompanied by a matching (up to logarithmic terms) lower bound. The proof of
Theorem 4 is provided in Section 3.
Theorem 4. In the N -armed bandit game with uniform
cost of observations c
n
√ o
1/3
inf sup RTc ≥ max 0.19 (cN ) T 2/3 , 0.03 T ,
where the infimum is over all playing strategies and the
supremum is over all oblivious adversaries.

We note that there is no contradiction between Theorem 4
and the data-dependent improvement achieved in Eq. (4),
since Theorem 4 considers the worst case and the improvement is achieved under benign conditions.

3. Proofs
The analysis of both our algorithms is based on the following lemma, which represents an intermediate step in the
analysis of EXP3 by Bubeck (2010).
Lemma 5. For any N sequences of random variables
Lh1 , Lh2 , . . . indexed by h ∈ {1, . . . , N }, such that Lht ≥ 0,
and any non-increasing
positive sequence η1 , η2 , . . . , for
P
h
exp(−ηt t−1
s=1 Ls )
P
P
(assuming for t = 1 the
qt (h) =
t−1
h0
s=1 Ls )
h0 exp(−ηt
sum in the exponent is zero) we have:
!
T X
T
X
X
h
h
qt (h)Lt − min
Lt
t=1

h

h

≤

t=1

T
X
ηt X
t=1

2

qt (h) Lht

2

+

h

ln N
. (6)
ηT

More precisely, we are using the following corollary, which
follows by taking expectations of the two sides of (6) and
using the fact that E [min [·]] ≤ min [E [·]]. We decompose
expectations of incremental sums into sums of conditional
expectations and use Et [·] to denote expectations conditioned on observations up to round t.
Corollary 6. Under the definitions of Lemma 5:
#!
" T
"
##
" T
X
X  
X
h
h
Et
Et Lt
E
qt (h)Lt
− min E
t=1

h

h

"
≤E

T
X
t=1

"
Et

t=1

2
ηt X
qt (h) Lht
2
h

##
+

ln N
.
ηT
(7)

We also use the following two technical lemmas. The
proofs of the lemmas are provided in the appendix.
Lemma 7. For any probability distribution q on
{1, . . . , N } and any m ∈ [1, N ]:
N
X
h=1

N
q(h)(N − 1)
≤ .
q(h)(N − m) + m − 1
m

(8)

Lemma 8. For any sequence of non-negative numbers
a1 , a2 , . . . , such that a1 > 0, and any power γ ∈ (0, 1)
we have:
!1−γ
T
T
X
X
at
1
P
γ ≤
at
.
t
1 − γ t=1
t=1
s=1 as

Prediction with Limited Advice and Multiarmed Bandits with Paid Observations

Lemma 8 is a generalization of Auer et al. (2002b, Lemma
3.5) from γ = 1/2 to arbitrary γ.
Now we are ready to present the proofs of the theorems.
3.1. Proof of Theorem 1
 h

P
h
Proof.
and
h qt (h)Lt ,
hP We study iEt Lt , Et
h 2
Et
for the case of our algorithm.
h qt (h) Lt
We have:
 
Et Lht = `ht .
(9)
And we have:
"
#
"
#
X
X
h
h h
Et
qt (h)Lt = Et
`t 1t = Et [Lt ] .
h

(10)

h


X
≤ Et 
qt (h) 
h

=

X
h

=

X

=

X

qt (h) 
qt (h)

h

h

"

Lt + E

≤

T X
N
X

#
1ht ct (h)

− min

t=1 h=1

T X
N 
X

h

T
X

!
`ht

t=1


ln N
ηt qt (h)
,
+ ct (h)pt (h) +
2pt (h)
ηT

(12)

where ηt has to be a non-increasing sequence.

`ht
1ht
t −1
qt (h) + (1 − qt (h)) M
N −1

t −1
qt (h)) M
N −1


min

1
t −1
qt (h) + (1 − qt (h)) M
N −1

t −1
qt (h) + (1 − qt (h)) M
N −1

pt

h
2 1t 

 h
2 Et 1t

N
X
ηt qt (h)
+ ct (h)pt (h)
2pt (h)

h=1

s.t. ∀h : 0 ≤ pt (h) ≤ 1,

h
2 1t 



1

Our first goal is to minimize the instantaneous contribuPN
t qt (h)
tions h=1 η2p
+ ct (h)pt (h). This leads to the followt (h)
ing optimization problem



2
`ht
qt (h) + (1 −

!2 

(13)

o
n q
t qt (h)
. We note
which is solved by p∗t (h) = min 1, η2c
(h)
t
t qt (h)
that if p∗t (h) = 1 it means that η2c
≥ 1, which in turn
t (h)
q
p
p
1
means that ct (h) = ct (h) ct (h) ≤
2 ηt qt (h)ct (h).
∗
By substituting pt into the minimization problem (13) we
obtain:

N
X
ηt qt (h)
+ ct (h)pt (h)
2p∗t (h)

1
t −1
qt (h) + (1 − qt (h)) M
N −1

h=1

N
X

qt (h)(N − 1)
qt (h)(N − Mt ) + Mt − 1

N
≤
,
Mt

#

t=1 h=1

h


X
= Et 
qt (h) 

=E

" T
X
t=1

We also have:
"
#
X

2
Et
qt (h) Lht

h

Proof. Similar to the previous proof,
hPit is easy toi ver h
N
h
h
ify that Et Lt
=
= `t , that Et
h=1 qt (h)Lt

P
PN
Et [Lt ], and that Et
h∈Oht ct (h) i =
h=1 pt (h)ct (h).

2
Furthermore, we have Et Lht
≤ pt1(h) and thus
hP
i

P
N
N
qt (h)
h 2
Et
≤
h=1 qt (h) Lt
h=1 pt (h) . By substituting
this into (7) and adding the cost of observations we obtain:
RTc

h


X
= Et 
qt (h)

3.2. Proof of Theorem 3

(11)

where the last inequality is by Lemma 7. By substituting
(9), (10), and (11) into (7) we obtain for all h:
" T
#
!
T
T
X
X
N X ηt
ln N
h
E
Lt − min
`t ≤
+
.
h
2
M
ηT
t
t=1
t=1
t=1
This proves the first inequality in Theorem 1. The second
inequality follows by the choice of ηt and Lemma 8 (for
γ = 1/2) and the last inequality follows from the identity
PT
1
T
t=1 Mt = M when Mt = M is constant.



ηt qt (h)
=
1
+ ct (h)
2
h=1
p
+ 1{p∗t (h)<1}
2ηt qt (h)ct (h)



{p∗
t (h)=1}

N

≤

Xp
ηt p
+ 2ηt
qt (h)ct (h).
2
h=1

By substituting this result back into (12) we obtain the first
claim of the theorem:
!
T
N p
X
X
p
η
ln N
t
RTc ≤
+ 2ηt
qt (h)ct (h) +
.
2
ηT
t=1
h=1
(14)
Now we have to tune the learning
rate ηt .
We
PN p
note that by Jensen’s inequality
q
(h)c
(h)
=
t
t
h=1

Prediction with Limited Advice and Multiarmed Bandits with Paid Observations

qP
q
N
ct (h)
h=1 qt (h)
h=1 ct (h). We set:
qt (h) ≤

bounded by:

PN

r
η t =  √P

N
h=1

ct (h)+

Pt−1 PN

1
√

h=1
√s=1
2
3 ln N

qs (h)cs (h)

,

2/3
+

q

p
2ηt

t=1

N p
X

≤

t=1

qt (h)ct (h)

h=1

T
X

4
3

ln N

1/3 PN

q
PN

h=1 ct (h) +

p

h=1

Pt−1 PN
s=1

qt (h)ct (h)
p

h=1

1/3
qs (h)cs (h)

p

1/3 X
T
qt (h)ct (h)
4
h=1
≤
ln N

1/3
p
Pt PN
3
t=1
qs (h)cs (h)
s=1
h=1
!2/3

1/3 X
N p
T X
9
qt (h)ct (h)
,
≤
ln N
2
t=1
PN

h=1

where in the last inequality
pused Lemma 8. Note that
PN we
if for some t we have h=1 qt (h)ct (h) = 0 the corresponding game round makes no contribution to both sides
of the inequalityP
and sop
the result holds irrespective of the
N
assumption that h=1 qt (h)ct (h) > 0 for all t. By substituting the selected ηt into (14) we obtain bound (2):
RTc
2/3
v
uN
N p
T
−1 X
u
X
X
1/3
cT (h) +
qt (h)ct (h)
≤ (32 ln N ) t
√

h=1

= RT + cM T ≥ 0.03

t=1 h=1

+ 2 T ln N .
Inequality (3) follows q
in a similar way after applying
PN
PN p
qt (h)ct (h) ≤
h=1
h=1 ct (h) in (14).
3.3. Proof of Theorem 4
Proof. By Theorem 2 we know that without taking the cost
of the queries into account the regret of any algorithm that
makes M T observations is lower bounded as:
r
N
inf sup RT ≥ 0.03
T.
M
Adding the cost of observations we have that the regret
of any algorithm that makes M T observations is lower

N
T + cM T
M

n
√ o
≥ max 0.19c1/3 N 1/3 T 2/3 , 0.03 T ,

t
ln N

qP
N
where
h=1 ct (h) should be seen as an upper bound
PN p
on
qt (h)ct (h). For a moment assume that
p
PN h=1
qt (h)ct (h) > 0 for all t. Then we have:
h=1
T
X

RTc

where the last inequality follows by the fact that the expression is minimized by M = 0.032/3 c−2/3 N 1/3 T −1/3 and
that M is upper bounded by N .

4. Discussion
We defined the games of prediction with limited advice and
multiarmed bandits with paid observations and provided
algorithms and matching (up to logarithmic factors) upper and lower bounds for the two games. Our algorithm
for multiarmed bandits with paid observations treats armand time-dependent observation costs and reduces the regret below the worst-case lower bound under benign conditions.
Our work opens multiple directions for future research.
The multiarmed bandits with paid observations game can
serve as a basic model for learning under restricted information access in more general reinforcement learning
problems. At the same time, prediction with limited advice game poses interesting questions about learning under
constraints on the resources, for example, whether it is possible to achieve sub-polynomial dependence on the number
of experts with a sub-polynomial amount of advice, under
some assumptions on the loss function and/or the experts
class. (Due to the lower bounds we know that without additional assumptions this is impossible.) It would also be interesting to extend both games to continuous domains and
to stochastic environments.

Acknowledgments
We would like to thank Claudio Gentile for pointing the
reference to Auer et al. (2002b, Lemma 3.5) and Wouter
Koolen for helpful discussions and comments on a preliminary draft. This research was supported by an Australian Research Council Australian Laureate Fellowship
(FL110100281). We gratefully acknowledge the support of
the NSF through grant CCF-1115788. This research was
funded in part by the Intel Collaborative Research Institute
for Computational Intelligence (ICRI-CI).

References
Alon, Noga, Cesa-Bianchi, Nicolò, Gentile, Claudio, and Mansour, Yishay. From bandits to experts: A tale of domination
and independence. In Advances in Neural Information Processing Systems (NIPS), 2013.
Audibert, Jean-Yves and Bubeck, Sébastien. Regret bounds and

Prediction with Limited Advice and Multiarmed Bandits with Paid Observations
minimax policies under partial monitoring. Journal of Machine
Learning Research, 11, 2010.
Auer, Peter, Cesa-Bianchi, Nicolò, Freund, Yoav, and Schapire,
Robert E. The nonstochastic multiarmed bandit problem. SIAM
Journal of Computing, 32(1), 2002a.
Auer, Peter, Cesa-Bianchi, Nicolò, and Gentile, Claudio. Adaptive and self-confident on-line learning algorithms. Journal of
Computer and System Sciences, 64, 2002b.
Avner, Orly, Mannor, Shie, and Shamir, Ohad. Decoupling exploration and exploitation in multi-armed bandits. In Proceedings
of the International Conference on Machine Learning (ICML),
2012.
Bartók, Gábor, Pál, Dávid, and Szepesvári, Csaba. Minimax regret of finite partial-monitoring games in stochastic environments. In Proceedings of the International Conference on
Computational Learning Theory (COLT), 2011.
Bubeck, Sébastien. Bandits Games and Clustering Foundations.
PhD thesis, Université Lille, 2010.
Bubeck, Sébastien and Cesa-Bianchi, Nicolò. Regret analysis
of stochastic and nonstochastic multi-armed bandit problems.
Foundations and Trends in Machine Learning, 5, 2012.
Cesa-Bianchi, Nicolò and Lugosi, Gábor. Prediction, Learning,
and Games. Cambridge University Press, 2006.
Cesa-Bianchi, Nicolò, Lugosi, Gábor, and Stoltz, Gilles. Minimizing regret with label efficient prediction. IEEE Transactions on Information Theory, 51, 2005.
Foster, Dean P. and Rakhlin, Alexander. No internal regret via
neighborhood watch. In Proceedings on the International
Conference on Artificial Intelligence and Statistics (AISTATS),
2012.
Mannor, Shie and Shamir, Ohad. From bandits to experts: On the
value of side-observations. In Advances in Neural Information
Processing Systems (NIPS), 2011.
Ottucsák, György and György, András.
The combination of the label efficient and the multi-armed bandit problem in adversarial setting.
Technical report,
http://citeseerx.ist.psu.edu/viewdoc/versions?doi=10.1.1.126.1228,
2006.
Zolghadr, Navid, Bartók, Gábor, Greiner, Rissell, György,
András, and Szepesvári, Csaba. Online learning with costly
features and labels. In Advances in Neural Information Processing Systems (NIPS), 2013.

