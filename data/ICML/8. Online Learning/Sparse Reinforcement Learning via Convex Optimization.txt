Sparse Reinforcement Learning via Convex Optimization

Zhiwei (Tony) Qin1
@WalmartLabs, 850 Cherry Ave, San Bruno, CA 94066

TQIN @ WALMARTLABS . COM

Weichang Li, Firdaus Janoos
WEICHANG . LI , FIRDAUS . JANOOS @ EXXONMOBIL . COM
ExxonMobil Corporate Strategic Research, 1545 Rt. 22 East, Annandale, NJ 08801

Abstract
We propose two new algorithms for the sparse reinforcement learning problem based on different
formulations. The first algorithm is an off-line
method based on the alternating direction method
of multipliers for solving a constrained formulation that explicitly controls the projected Bellman residual. The second algorithm is an online
stochastic approximation algorithm that employs
the regularized dual averaging technique, using
the Lagrangian formulation. The convergence of
both algorithms are established. We demonstrate
the performance of these algorithms through several classical examples.

1. Introduction
As an approximation technique for problems involving
Markov Decision Processes (MDP), reinforcement learning has been applied to such diverse fields as robotics (Bentivegna et al., 2002), helicopter control (Ng et al., 2006),
and operations research (Proper & Tadepalli, 2006), among
many others. The problem of approximating the value
function of an MDP is central in reinforcement learning,
where the model information is usually unavailable, and
even noisy samples of the true target function values are
not accessible. When the state space is large or infinitedimensional, such a task is often accomplished through the
linear approximation architecture, under which the hypothesis space is spanned by a set of K basis functions or ‘features’ (Tsitsiklis & Van Roy, 1996). The least-squares temporal difference (LSTD) (Bradtke & Barto, 1996) learning
and gradient-based TD learning (Sutton et al., 2009) have
been effective ways of learning a linear representation of
the value function by using realized trajectory samples. It
has become increasingly clear that learning a sparse representation of the value function can prevent over-fitting

and improve generalization performance, especially with
the presence of a large number of features or a limited number of samples (e.g., (Kolter & Ng, 2009)).
In this paper we approach the sparse reinforcement learning
problem with a new constrained formulation that explicitly
controls the projected Bellman residual (PBR) and a popular Lagrangian formulation based on l1 -regularization. We
propose tailored optimization algorithms for solving each
of these two sparse reinforcement learning problems, including: 1) an efficient off-line off-policy learning algorithm using the alternating direction method of multipliers
(ADMM) (Eckstein & Bertsekas, 1992) with a proximal
gradient step, and 2) a novel online stochastic approximation algorithm that combines elements from the regularized
dual-averaging method (Xiao, 2010) and the least mean
squares technique. Convergence results are established for
both algorithms. We demonstrate through experiment results that regularized PBR minimization might be preferable over regularized minimization of the fixed-point slack
for value function approximation.
The rest of this paper is organized as follows. Section 2 reviews reinforcement learning with approximate value and
policy iterations, and TD learning. The main contribution
of this paper is presented in Sections 3 and 4. Section 3 introduces the constrained and the Lagrangian formulations
for sparse value function approximation and their extensions to the Q-function. The two new off-policy learning algorithms and their convergence proof are presented
in Section 4. We discuss related existing work in Section
4.4 and present experiment results against LSTD and two
other sparse reinforcement learning algorithms in Section
5. Finally we conclude the paper in Section 6.

2. Background and Preliminaries
In a Markov Decision Process (MDP), the state-action pair
is denoted by (x, u) ∈ X ×U(x), where X and U(x) are the
1

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

The work was completed when the first author was a summer
intern with ExxonMobil Corporate Strategic Research in 2012
working towards a Ph.D. degree at Columbia University, New
York.

Sparse Reinforcement Learning

set of states and the set of compatible actions respectively.
The set of state indices is S. The immediate reward by applying u to x is r(x, u). We use the capital letters X and R
to denote the random variable versions of x and r. Hence
the random reward R(t) := R(X(t), π(X(t))). π denotes
a particular policy mapping from a state to a compatible action, i.e. π(x) ∈ U(x). The
value function, V π : X → R
P∞
π
is defined as V (x) = E [ t=0 γ t R(t) | X(0) = x], with
the discounting factor γ ∈ [0, 1). V π satisfies the Bellman’s equation: for x ∈ X ,
X
V π (x) = r(x, π(x)) + γ
P(x, π(x), y)V π (y), (1)
y∈X

where P(x, u, y) is the transition probability from state x
to state y by applying action u. The transition probabilities and the reward distribution are generally not known.
The optimal value function V ∗ (x) satisfies the Bellman’s
optimality equation:
X
V ∗ (x) = max r(x, u) + γ
P(x, u, y)V ∗ (y) (2)
u∈U (x)

y∈X
π

Define the Bellman operator T by the right-hand-side of
(1) so that T π (V π ) := V π , and similarly, the Bellman optimality operator T ∗ by the right-hand-side of (2) so that
T ∗ (V ∗ ) := V ∗ .
2.1. Value Function Approximation
For problems with a large state space, evaluating V (t) exactly can be infeasibly expensive. A natural approach is to
approximate V by Ṽ through some compact representation
with a parameter β ∈ RK . The most common approximation architecture is linear function approximation (Tsitsiklis & Van Roy, 1996), i.e.
Ṽ (t) (x) =

K
X

β k φk (x) = Φ(x)T β

(3)

k=1

φ(x) defines a K-dimensional feature vector for the state
x and is part of the design architecture.
2.2. Approximate Policy Iteration
We consider the following state-action value function
"∞
#
X
Qπ (x, u) := E
γ t R(t+1) |X (0) = x, U (0) = u ,
t=0

which offers the flexibility of specifying the initial action u.
In approximate Policy Iteration, the tasks of policy evaluation and policy improvement are separated. Given a particular policy π, we approximate the state-action value function Qπ (x, u) by using function approximation techniques.
An improved policy π + is then built upon Qπ (x, u), and
the iterations continue till convergence. Our approach for
learning the optimal policy falls into this class of methods.

2.3. Temporal Difference (TD) Learning
With the transition probabilities and the reward distribution unknown a priori, the value function V π with
regard to the underlying policy π has to be learned
through a set of n transition observations (or samples)
n
{(xt , ut , rt+1 , yt+1 )}t=1 . The most common approaches
include TD Learning (Sutton, 1988) and the Least Squares
TD (LSTD) (Bradtke & Barto, 1996). Defining the temporal difference associated with the transition from time t to
t + 1 by
δt+1 = rt+1 + γ Ṽ (t) (yt+1 ) − Ṽ (t) (xt ),

(4)

TD Learning with linear value function approximation (3)
takes the following form (Tsitsiklis & Van Roy, 1997):

>
δt+1 (β) = rt+1 + γφ0>
t+1 β − φt β
(5)
(t)
β t+1 = β t + αt δt+1 ∇β Ṽ (xt ).
where the short-hand notation φt := φ(xt ), φ0t+1 :=
φ(yt+1 ) denote the feature vectors.
The LSTD algorithm solves the linear system
Ã(n) β = b̃(n)
(6)
P
n
(n)
where
:= n1 t=1 (φt (φt − γφ0t+1 )> ), b̃(n) :=
Pn Ã
1
t=1 (rt+1 φt ). We will drop the superscripts for simn
pler notation. Loosely speaking, the overall idea here is
to sample the unknown state transition probabilities from
a sequence of state trajectories, and to construct the subspace within which the value function could be well approximated in linear form.
For compact notation, we replace the summation terms in
(6) by the matrix forms:
1 >
> 0
(7)
(Φ̃ Φ̃ − γ Φ̃ Φ̃ )
n
1 >
b̃ :=
(8)
(Φ̃ R̃).
n
>
0
where Φ̃ := φ1 , · · · , φn
∈ Rn×K , Φ̃ :=

>
φ02 , · · · , φ0n+1
∈
Rn×K ,
R̃
:=
>
n
r2 , · · · , rn+1
∈ R . In addition, we define
Ã :=

>

G̃ := n(Φ̃ Φ̃)−1 . We note that Ã b̃ and G̃ may be viewed
as sample averaged approximation to the following ensemble avaerage terms: A := E[φ(X)(φ(X) − γφ(Y ))> ],
b := E[Rφ(X)] and G := E[φ(X)φ(X)> ]−1 .

3. Sparse Reinforcement Learning
The linear value function approximation architecture poses
two conflicting requirements on the size of the feature
space: First, a compact representation of the value function, desired for both computational and storage consideration, calls for a small number of features; On the other

Sparse Reinforcement Learning

hand, a better approximation to the value function requires
a rich feature space which implies a relatively large number
of features. However, as we saw above, the linear system
for LSTD may not even be invertible with the number of
features larger than the number of samples. This motivates
feature selection, or more precisely, learning a sparse representation out of a large number of basis functions.

Note that (11) is exactly the basis pursuit denoising
(BPDN) problem. It finds the most sparse solution that satisfies the given tolerance on the PBR. The Euclidean projection onto the l2 ball can be computed in closed-form, facilitating the development of a fast optimization algorithm.

A common loss function, which both the GTD2/TDC
(Sutton et al., 2009) and LSTD methods minimize,
is based on the projected Bellman residual (PBR):
LPBR (β) := kṼ − ΠT π Ṽ k2S , where S is a diagonal matrix
whose diagonal entries represent the stationary distribution
of states in the input data, and Π = Φ(Φ> Φ)−1 Φ>
1
is the orthogonal projector
onto the feature space.
√
x> Sx. With linear value funcThe norm kxkS :=
tion approximation it can be shown that LPBR (β) =
E[δ(β)φ(X)]> E[φ(X)φ(X)> ]−1 E[δ(β)φ(X)]
=
ˆ
kAβ − bk2G . The corresponding empirical loss LPBR
can be obtained by replacing A, b, G with Ã, b̃, G̃:
ˆ (β) := kÃβ − b̃k2 , and the sample based feature
LPBR
nG̃

We consider the Lagrangian formulation of problem (9):

>

>

space projector is given by Π̂ = Φ̃(Φ̃ Φ̃)−1 Φ̃ .
We now discuss two different formulations of sparse TD
learning through the l1 -regularized empirical PBR minimization.
3.1. Constrained Formulations
We first propose a constrained formulation where we have
explicit control of the PBR:

	
min kβk1 | LPBR (β) ≤ 2 .
(9)
β

In the offline setting with finite samples, we solve the empirical version of the above problem
n
o
min kβk1 | kÃβ − b̃knG̃ ≤  .
(10)
β

When K > n and  = 0, it is a special case of finding the
sparsest solution that satisfies the projected Bellman fixedpoint equation and is also known as the basis pursuit problem.

3.2. Lagrangian Formulation

min
β

1
LPBR + ρkβk1 .
2

(12)

Existing works have focused on offline algorithms for solving the empirical version of this problem with LPBR replaced by L̂PBR . The empirical version can be treated as
a vanilla unconstrained Lasso problem, so that many existing solvers can be directly applied, including LARS (Efron
et al., 2004), FISTA (Beck & Teboulle, 2009), and FPC-BB
(Hale et al., 2008). However, the stochastic version (12) is
more challenging to solve, as also mentioned in (Liu et al.,
2012). In Section 4.2, we propose a novel online algorithm
for solving the stochastic optimization problem (12).
3.3. Extension to the Q-Function
It is straightforward to extend the above algorithms to
estimate the state-action value function (Q-function) and
integrate them into the policy iterations by following
(Lagoudakis & Parr, 2003). The feature vector φ(x) ∈
RK is augmented to φ(x, u) ∈ RK×|U | . The defini0
tions of Φ̃ and Φ̃ are modified accordingly to incorporate the additional argument u. The Bellman equaπ
tion,
is Qπ (x, u) = r(x, u) +
P which Q satisfies,
π
γ y∈X P(x, u, y)Q (y, π(y)), x ∈ X .

4. Learning Algorithms
Our approaches for solving the constrained formulations
(BPDN (11)) is based on ADMM, which has been applied
extensively in solving machine learning problems. For
the Lagrangian formulation (12), we develop a specialized
stochastic gradient descent method which is based on two
approximation sequences of different speed.

0

We define d̃ := Π̂R̃, C̃ := γ Π̂Φ̃ − Φ̃ as in (Geist & Scherrer, 2012) and reformulate (10) into
n
o
min kβk1 | kd̃ + C̃βk ≤  .
(11)
β

The empirical PBR loss term used in (11) is equivalent to
that used in (10) since
0

kC̃β + d̃k2 = kΠ̂(R̃ + γ Φ̃ β − Φ̃β)k2 = kb̃ − Ãβk2nG̃ .
1

When the Grammian matrix is not invertible, we can use its
Moore-Penrose pseudoinverse.

4.1. BPDN (Offline)
We first explain an inexact version of the ADMM (Yang &
Zhang, 2011), which was originally developed for solving
compressed sensing problems. We apply variable-splitting
to (11) and reformulate the problem into
n
o
min kβk1 + I(kαk ≤ ) | d̃ + C̃β = α .
(13)
β

The augmented Lagrangian of this constrained problem is
L(α, β, v) = kβk1 + I(kαk ≤ ) − v> (d̃ + C̃β − α) +
1
2
2µ kd̃ + C̃β − αk , where v is the Lagrange multipliers.

Sparse Reinforcement Learning

For solving for α, we minimize minα L(α, β, v), which
is equivalent to the following Euclidean projection problem
onto the L2 ball,


1
2
min
kα − ck | kαk ≤  ,
(14)
α
2
where c = d̃ + C̃β − µv. The solution to the above projection problem is given by

α=

c, ifkck ≤ 
c
, otherwise.
 kck

(15)

The subproblem w.r.t. β is
min
β

1
kC̃β + d̃ − α − µvk2 + µkβk1 .
2

(16)

Define f (β, α) := 12 kC̃β + d̃ − α − µvk2 , then
∇β f (β, α) = C̃> (C̃β + d̃ − α − µv), which is Lipschitz
continuous with a Lipschitz constant L = λmax (C̃> C̃),
where λmax denotes the largest eigenvalue. Problem (16)
is a Lasso problem itself, and we can employ any Lasso
solver as a subroutine. However, this step becomes very
expensive in this way. Instead, we take a proximal gradient step to solve this subproblem approximately, which is
much cheaper computationally. Specifically, given β t and
α, we solve
min
β

1
kβ − (β j − τ ∇β f (β j , α))k2 + τ µkβk1 ,
2

1: Given C̃, d̃, µ, τ, .
2: for j = 0, 1, · · · do
3:
c ← d̃ + C̃β j − µvj
4:
αj+1 ← (15)
5:
β j+1 ← Sτ µ (β j − τ ∇β f (β j , αj+1 ))
6:
vj+1 ← vj −
7: end for

1
µ (d̃

+ C̃β j+1 − αj+1 )

We describe our online algorithm for solving the unconstrained stochastic optimization problem (12). The
algorithm is inspired by the regularized dual averaging (RDA) algorithm (Xiao, 2010) with a specialized
stochastic approximation of the LPBR gradient. The
gradient of LP BR (β) is ∇LP BR (β) = E[(φ(X) −
γφ(Y ))φ(X)> ]E[φ(X)φ(X)> ]−1 E[δ(β)φ(X)]. Note
that the finite sample approximation LPˆBR used in BPDN
is a biased estimate of LP BR (β). Likewise, the direct finite sample approximation of the gradient is also biased
without double sampling the next state Y . To get around
this problem, we adopt the approach of (Sutton et al.,
2009) for GTD2 and use an auxiliary variable ω to approximate E[φ(X)φ(X)> ]−1 E[δ(β)φ(X)]. The gradient
˜ t (β) =
can now be estimated by sample approximation ∇f
0
∆t (φ>
ω
),
where
∆
=
γφ
−
φ
.
The
auxiliary
varit
t
t
t
t+1
able ω is updated by Least Mean Square (LMS) rule as in
(Sutton et al., 2009). Each iteration of our algorithm then
solves the following problem
c
¯ t , βi + ρkβk1 + √
kβk2
min h∇f
β
2 t
¯ t = 1 Pt ∇f
˜ t (β (s) ) is the time average of
where ∇f
s=1
t
the gradient approximations, and c is a positive constant.
Note that the coefficient
for kβk2 above corresponds to the
√
c t
scaling factor Lt = 2 as defined in (Xiao,
 √2010). The
(t+1)
¯t .
solution to this problem is β
= S √tρ − t ∇f
c

(17)

which has a closed-form solution β j+1 = Sτ µ (β j −
τ ∇β f (β j , α)), where Sη (·) := max(| · | − η, 0) is the
shrinkage operator (Tibshirani, 1996) with the shrinkage
parameter η, and the operation is component-wise. It can
be shown that ADMM with this proximal step converges if
τ < λ (1C̃> C̃) . The stopping criteria that we use in the
max
implementation is based on the primal and dual residuals
(Boyd et al., 2010). A major advantage of this algorithm is
that it is a first-order method, requiring only matrix-vector
multiplications. The iterations are thus very fast to compute
and simple to implement. We state the inexact ADMM algorithm in Algorithm 1.
Algorithm 1 ADMM-BPDN

4.2. L1 -regularized PBR minimization (Online)

c

We state the RDA algorithm with the customized gradient approximation in Algorithm 2. The computational and
storage complexity of each iteration is linear O(K).
Algorithm 2 RDA with LMS update
¯ 0 = 0.
1: Given ω 0 , γ, c, {ηt }, ∇f
2: for t = 1, · · · , Tmax do
3:
∆t ← γφ0t+1 − φt
˜ t ← ∆t (φ> ω t−1 )
4:
∇f
t
¯ t ← 1 ((t − 1)∇f¯t−1 + ∇f
˜ t)
5:
∇f
t

ωt ← ω
+ ηt (δt+1 (β t−1 ) − φ>
t ω t−1 )φt
√ t−1
t
¯ t)
7:
β t ← c Sρ (−∇f
8: end for

6:

4.3. Convergence Analysis
Both of the proposed algorithms enjoy guaranteed convergence properties. We establish the convergence results in
Theorems 4.1 and 4.2 below.
4.3.1. BPDN-ADMM (O FFLINE )
Theorem 4.1. For any positive τ < λ (1C̃> C̃) , the semax
quence {(αj , β j , vj )} generated by Algorithm 1 converges

Sparse Reinforcement Learning

to an optimal solution to problem (11) from any starting
point.

recursion

1
F (ḡt , ω t ) + Mt+1 ,
t+1


= ω t + ηt G(ḡt , ω t ) + Nt+1 .

ḡt+1 = ḡt +
The proof parallels the one given in (Yang & Zhang, 2011)
for compressed sensing and the one in (Ma et al., 2012)
for latent variable Gaussian graphical model selection. The
major steps are outlined in Appendix A the supplementary
file.
4.3.2. RDA-LMS (O NLINE )

ω t+1

(19)
(20)

We prove the convergence of this coupled recursion
through the following propositions and lemmas, which facilitate the application of Theorem 1.1 in (Borkar, 1997).

The original convergence results for RDA in (Xiao, 2010)
no longer applies due to the fact that we are only approximating the gradients through maintaining a quasistationary sequence of ω t . Our proof of convergence is
based on the analysis of a two-time-scale stochastic approximation (Borkar, 1997) similar to that in (Sutton et al.,
2009).

In all subsequent results, we make the following standard assumptions: (A1) The MDP is finite and stationary.
(A2) The transition samples {(φt , rt+1 , φ0t+1 )}
is an i.i.d. sequence with uniformly bounded second
moments. (A3) The features defined by φ are linearly
independent, and E[φφ> ]−1 exists. (A4) The iterates
{ḡt , ω t } are bounded, i.e. supt kḡt k < ∞, supt kω t k <
∞.

¯ t for
For notational conciseness, we use ḡt in place of ∇f
the time-averaged gradient. The iterations in Algorithm 2
can be written as

Proposition 4.1. The functions F and G are Lipschitz continuous.


1
t
ω)
ḡt + t+1
(γφ0t+1 − φt )(φ>
 ḡt+1 = t+1
 t t
ω
φ
ω t+1 = ω t + ηt δt+1 (β t ) − φ>
t
t
t

β t+1 = qSρ (−ḡt+1 ).
Note that a different scalar is used in the update for β t .
The original update
√ in Algorithm 2 corresponds to using a
scaling factor c t as suggested in (Xiao, 2010). For the
convergence analysis here, we choose a scaling factor qt
(q > 0) so that the update for β t does not explicitly depends on t. Note that the sequence {qt} is still non-negative
and non-decreasing, as required in (Xiao, 2010). The above
iteration can be further consolidated into

 ḡt+1 = ḡt +

1
(−ḡt
t+1

+ D̂t ω t )


 ω t+1 = ω t + ηt (b̂t − q Ât Sρ (−ḡt )) − φφ> ω t ,
(18)
where b̂t = φt rt+1 , Ât = φt (φt − γφ0t+1 )> , D̂t =
(γφ0t+1 − φt )φ>
t , so that b = E[b̂t ], A = E[Ât ] by our
definitions, and we define D := E[D̂t ]. We also define the
following functions and variables:
F (ḡt , ω t )
Mt+1
K

:= −ḡt + Dω t
:=

(D̂t − D)ω t

:= E[φφ> ]

G(ḡt , ω t )

:=

(b − qASρ (−ḡt )) − Kω t

Nt+1

:=

(b̂t − b) − q(Â − A)Sρ (−ḡt ) −
(φt φ>
t − K)ω t

Now, the iteration (18) is in the form of a coupled stochastic

Proof. Since F is linear in both ḡt and ω t , it is clearly Lipschitz continuous. For the Lipschitz continuity of G, it suffices to show that the shrinkage operator Sρ (·) is Lipschitz
continuous. But it is known that Sρ (·) is non-expansive
(Hale et al., 2008), and hence, it is Lipschitz continuous.
Proposition 4.2. For each ḡ ∈ RK , the o.d.e.
ω̇ t = G(ḡ, ω t )

(21)

has a unique global asymptotically stable equilibrium λ(ḡ)
such that the function λ(·) is Lipschitz continuous.
Proof. With ḡ fixed, G(ḡ, ω t ) = (b−qASρ (−ḡ))−Kω t ,
where the first term is a constant. Since the features
are linearly independent, K is symmetric positive definite.
Hence, the o.d.e. (21) has a unique global asymptotic stable equilibrium K−1 (b − qASρ (−ḡ)) = λ(ḡ). Now, since
Sρ (·) is non-expansive, the function K−1 ASρ is clearly
Lipschitz continuous. Hence, the claim holds.
Lemma 4.1. For any 0 < q <

1
σmax (DK−1 A) ,

ḡ˙ t = F (ḡt , λ(ḡt ))

the o.d.e.
(22)

has a unique global asymptotically stable equilibrium ḡ∗ .
Proof. We can rewrite the right-hand-side of the o.d.e. (22)
as
F (ḡt , λ(ḡt )) = H(ḡt ) − ḡt ,
where H(ḡt ) = DK−1 (b − qASρ (−ḡt )). The shrinkage
operator Sρ (·) is non-expansive w.r.t. any lp -norm, with
p ≥ 0 (Hale et al., 2008). Then, it is easy to see that for

Sparse Reinforcement Learning
1
any 0 < q < σmax (DK
−1 A) , H(·) is a contraction w.r.t.
the l2 -norm. By Theorem 3.1 in (Borkar & Soumyanatha,
1997), the o.d.e. (22) is globally asymptotically stable. The
equilibrium point ḡ∗ satisfies

• For each ḡ ∈ RK , the o.d.e. (21) has a unique global
asymptotically stable equilibrium λ(ḡ) such that the
function λ(·) is Lipschitz continuous.

F (ḡ∗ , λ(ḡ∗ )) = 0
ḡ∗ = H(ḡ∗ ).

⇔

Hence, ḡ∗ is a fixed-point of the operator H(·). Since H(·)
is a contraction w.r.t. the l2 -norm, the fixed-point is unique.
Lemma 4.2. The unique optimal solution to the PBRLasso problem
1
LPBR (β) + ρkβk1 .
2

(23)

is given by β ∗ = qSρ (−ḡ∗ ).
Proof. First, we observe that for any 0 < ξt < 1, the iterate
ḡt+1 = ḡt + ξt (−ḡt + H(ḡt )) has the same fixed-point ḡ∗
1
, the above
if it converges. In particular, with ξt = t+1
iterate is equivalent to
ḡt+1 =

t
1
ḡt +
H(ḡt ),
t+1
t+1

(24)

which is exactly the time-averaged gradient of the loss
function LPBR(·) evaluated at β 1 , · · · , β t . By construction,
the sequence {β t = qSρ (−ḡt )} is the same as the one
generated by the RDA algorithm (Xiao, 2010) applied to
the strictly convex optimization problem (23) using the true
gradients. By the convergence results in (Xiao, 2010), the
sequence {β t } converges to the unique optimal solution
of problem (23). Hence, qSρ (−ḡ∗ ) ≡ β ∗ , and the claim
holds.
Theorem 4.2. The sequence {β t , ω t } generated by Algorithm 2 converges to (β ∗ , ω ∗ ) w.p. 1.
Proof. In order to apply Theorem 1.1 in (Borkar, 1997),
we need to verify the following conditions:
• The functions F and G are Lipschitz continuous.
• The ξt and ηt satisfy
X
ξt = ∞,

X

t

X

ηt = ∞,

t

ξt2 < ∞,

X

t

ηt2 < ∞,

t

ξt = (o)(ηt ).
• The sequences {Mt } and {Nt } satisfy
X
X
ξt Mt < ∞,
ηt Nt < ∞.
t

t

• The o.d.e. (22) has a unique global asymptotically stable equilibrium ḡ∗ .

1
The step size of the recursion (19) ξt is equal to t+1
in our
ξt
case. Then, ηt → 0 as t → ∞, thus satisfying the above
conditions. It is also easy to see that both Mt+1 and Nt+1
are martingale differences with bounded second moments,
by (A1), (A2), (A4), and the non-expansiveness of Sρ (·).
The third condition can be ensured by applying
Ps martingale
∞
convergence
theorem
on
the
martingales
{
t=1 ξt Mt }s=1
Ps
∞
and { t=1 ηt Nt }s=1 . The conditions for Theorem 1.1 in
(Borkar, 1997) are all satisfied, given Propositions 4.1 and
4.2 and Lemma 4.1. An immediate consequence is that the
sequences {(ḡt , ω t )} generated by Algorithm 2 converge
to (ḡ∗ , ω ∗ ) w.p. 1, where ω ∗ = λ(ḡ∗ ). By Lemma 4.2, the
iterates {β t } converge to β ∗ w.p. 1.

4.4. Related Work
There have been some recent work on the development of
a sparse LSTD method. LARS-TD (Kolter & Ng, 2009;
Ghavamzadeh et al., 2011) applies the l1 -regularization to
the projector onto the space of representable linear functions and solves the problem through LARS. (Johns et al.,
2010) formulates the L1 regularized linear fixed-point
problem as a linear complementarity problem. (Painterwakefield & Parr, 2012) proposes a greedy algorithm based
on orthogonal matching pursuit. l1 -PBR (Geist & Scherrer,
2012) instead applies the l1 -regularization to the classical
PBR minimization, and (Mahadevan & Liu, 2012) develops an on-policy algorithm based on mirror-descent. Unlike the above algorithms, our BPDN algorithm solves a
constrained formulation of the sparse TD learning problem which explicitly controls the PBR. The closest work
to our method is probably (Geist et al., 2012) which constrains kÃβ − b̃k∞ . Our RDA-LMS algorithm, on the
other hand, is an off-policy online algorithm based on gradient TD (Sutton et al., 2009). The only other l1 -based
online sparse TD method that we are aware of is RO-TD
(Liu et al., 2012), which minimizes a different loss function, kÃβ − b̃k2 , based on the slack in (6) similar to DLSTD. Note that the PBR is a weighted squared-norm of
the fixed-point slack, being equivalent to kÃβ − b̃k22 only
when the feature bases are orthonormal (G = I), which is
not true in general. This difference appears to contribute
to the different convergence speeds that we have observed
below in Section 5.2.

Sparse Reinforcement Learning

5. Experiments
5.1. BPDN
We compared the proposed ADMM algorithm for BPDNbased sparse reinforcement learning with LSTD and DLSTD (Geist et al., 2012) on two test problems, which
are described in the next two sections. D-LSTD was formulated as a linear program and solved by the Mosek LP
solver. For both problems, we constructed features using
RBF basis functions with centers on grids of different densities and widths, five polynomial basis functions, as well
as irrelevant features (white noise) of different lengths. We
show results on fit errors (w.r.t. the true value functions)
for value function approximation over 20 independent runs
and simulated reward for the policy obtained from policy
iterations over 10 independent runs.

Figure 1. Chain Walk: Comparison on approximating the stateaction value function of the optimal policy. The number of irrelevant features is 500 and 1000 in that order.

5.1.1. C HAIN WALK
This is the classical 20-state test example from (Lagoudakis
& Parr, 2003). Two actions (left or right) are available to
control the current state. Visiting state 1 and 20 yields a reward of 1 and zero reward for anywhere else. Either action
has a success probability of 0.9. Failure of an action bring
the current state to the opposite direction.
For approximating the optimal value function, we collected
1000 training samples off-policy by following a random
policy for 100 episodes, 10 steps each. The samples were
collected in the same way for policy iterations, and we used
a fixed set of samples throughout the policy iterations. The
value function of any given policy can be computed exactly
for this problem, and the optimal policy is to go to the left if
the current state is from 1 to 10, and to the right otherwise.
Figures 1 and 2 show the comparison among BPDN, DLSTD, and LSTD on the approximation error (RMSE)
of the Q value function and the simulated reward of the
learned policies for two cases of irrelevant features. The
whiskers of the boxplots extend to the most extreme data
point which is no more than 1.5 times the interquartile
range from the boxes. BPDN and D-LSTD are significantly better than LSTD on both metrics in the presence
of irrelevant features. BPDN is competitive to D-LSTD on
simulated reward while doing tangibly better on value approximation. The average CPU times for BPDN on one
instance of value approximation are 11.4s for Nnoise = 500
and 15.7s for Nnoise = 1000, and those for D-LSTD are
6.6s and 31.7s respectively. It is clear that BPDN has better
scalability, being a first-order method.
5.1.2. I NVENTORY C ONTROL
This is a single-shop single-product inventory control problem. The inventory level at the end of the day is a continuous variable between 0 and 20. (Think of each unit as

Figure 2. Chain Walk: Comparison on simulated reward from
policy iterations. The number of irrelevant features is 500 and
1000 in that order.

a pallet of the products.) Orders of new items have to be
made in batches of five (i.e. five actions). Customer demand for each day follows the continuous uniform distribution [0, 24]. There is a one-time order overhead of $2 for
any positive orders. The product unit cost is $2, and the
selling price is $2.5 per unit. Any n unsold items incur an
inventory cost of $0.2n1.4 . Any unmet customer demand
incurs a penalty of $0.5 per unit. The goal is to learn an
optimal planning strategy to operate the store.
We collected two sets of off-policy training samples with
sizes 1000 and 5000 respectively. The samples were collected from different numbers of episodes of 10 steps each.
The policy used for value function approximation tests is
one that fills up the inventory as much as possible every
day. Since the state space is continuous, the value function
cannot be solved exactly. We used Monte Carlo rollouts to
compute the value of the test states and inital actions associated with the test policy.
Figures 3 present the comparison results on the fitting error of the Q value function and the simulated reward of
the learned policies. Similar to the Chain Walk example,
BPDN was able to learn a more accurate Q value function and a better policy than LSTD from the same number
of samples. We were unable to test D-LSTD on this domain because the large problem size led to excessive memory consumption by the Mosek LP solver, which reported
out-of-memory error. This again shows that our first-order

Sparse Reinforcement Learning

Figure 4. Left: Evolution of PBR on the Star example. Right: Qand V-value functions approximation (by running RDA-LMS) associated with (bottom) the optimal policy and (top) a suboptimal
policy for Chain.

Figure 3. Inventory Control: Comparison on state-action value
function approximation of an sub-optimal policy (top) and simulated reward from policy iterations (bottom). The number of irrelevant features is 1000. The number of training samples is 1000
for the left column and 5000 for right column.

method is more scalable and memory efficient.
5.2. RDA-LMS
We tested the RDA-LMS algorithm (Algorithm 2) on the
7-state Star MDP example (Sutton et al., 2009) and Chain
domain to empirically verify the convergence of the algorithm. For the Star example, we set ρ = 0 to verify the
convergence of the PBR to zero, and hence, the correctness of the algorithm. We plotted the evolution of the PBR
for RDA-LMS, RO-TD and GTD2 in Figure 4. RDA-LMS
was able to decrease the PBR much more rapidly than both
RO-TD and GTD2. Comparisons were also made against
RO-TD on Chain. We considered approximating the Qvalue functions associated with the optimal policy and a
sub-optimal policy. The transition samples were collected
from 100 random episodes, with 10 consecutive steps each,
totalling a collection of 1,000 samples. The algorithms
swept through the samples multiple times. The features
were constructed in the same way as in the previous section, except that there was no white noise features. Comparisons of the learned (solid lines) and true (dotted lines)
value functions were plotted in the right-hand-side of Figure 4, which shows that the sparse solutions obtained by
RDA-LMS approximate the true value functions faithfully.
Figure 5 provides empirical evidence for the convergence
of RDA-LMS with non-zero l1 -regularization. In addition,
RDA-LMS decreased the PBR at a much faster rate than
RO-TD.2 We surmise that this is because RDA-LMS min2
The approximation error of RO-TD hovers above 20 and is
not plotted.

Figure 5. Evolution of PBR of RDA-LMS and RO-TD (left) and
approximation error of RDA-LMS (right) for the Q-value function
associated with the optimal policy for Chain.

imizes the PBR directly while RO-TD minimizes the slack
in the fixed-point condition (6) as a proxy.

6. Conclusion
In this paper, we have proposed two optimization algorithms for solving two different formulations of the sparse
reinforcement learning problem: an ADMM-based off-line
algorithm for constrained sparse TD learning and a novel
online algorithm which combines elements from the RDA
and LMS methods for unconstrained sparse TD learning.
We provide convergence analysis for both algorithms, and
promising preliminary test results have been reported in
comparison to LSTD and two other sparse reinforcement
learning algorithms. Future work will be focused on applications of the proposed sparse TD learning algorithms on
real-world problems.

7. Acknowledgement
The authors would like to thank Niranjan Subrahmanya and
Wendy (Lu) Xu for valuable discussions and the anonymous reviewers for helpful feedback. Special thanks to Bo
Liu and Ji Liu for sharing the sample ROTD code.

Sparse Reinforcement Learning

References
Beck, A. and Teboulle, M. A fast iterative shrinkage-thresholding
algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.
Bentivegna, Darrin C, Ude, Ales, Atkeson, Christopher G, and
Cheng, Gordon. Humanoid robot learning and game playing
using pc-based vision. In Intelligent Robots and Systems, 2002.
IEEE/RSJ International Conference on, volume 3, pp. 2449–
2454. IEEE, 2002.
Borkar, V.S. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):291–294, 1997.
Borkar, V.S. and Soumyanatha, K. An analog scheme for fixed
point computation. i. theory. Circuits and Systems I: Fundamental Theory and Applications, IEEE Transactions on, 44(4):
351–355, 1997.
Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J. Distributed optimization and statistical learning via the alternating
direction method of multipliers. Machine Learning, 3(1):1–
123, 2010.

Liu, Bo, Mahadevan, Sridhar, and Liu, Ji. Regularized off-policy
td-learning. In Advances in Neural Information Processing
Systems 25, pp. 845–853, 2012.
Ma, S., Xue, L., and Zou, H. Alternating direction methods for
latent variable gaussian graphical model selection. Technical
report, University of Minnesota, 2012.
Mahadevan, Sridhar and Liu, Bo. Sparse q-learning with mirror
descent. In UAI 2012, 2012.
Ng, Andrew Y, Coates, Adam, Diel, Mark, Ganapathi, Varun,
Schulte, Jamie, Tse, Ben, Berger, Eric, and Liang, Eric. Autonomous inverted helicopter flight via reinforcement learning.
In Experimental Robotics IX, pp. 363–372. Springer, 2006.
Painter-wakefield, Christopher and Parr, Ronald. Greedy algorithms for sparse reinforcement learning. In Proceedings of the
29th International Conference on Machine Learning (ICML12), pp. 1391–1398, 2012.
Proper, S. and Tadepalli, P. Scaling model-based average-reward
reinforcement learning for product delivery. Machine Learning: ECML 2006, pp. 735–742, 2006.

Bradtke, S.J. and Barto, A.G. Linear least-squares algorithms for
temporal difference learning. Machine Learning, 22(1):33–57,
1996.

Sutton, R.S. Learning to predict by the methods of temporal differences. Machine learning, 3(1):9–44, 1988.

Eckstein, J. and Bertsekas, D.P. On the Douglas-Rachford splitting method and the proximal point algorithm for maximal
monotone operators. Mathematical Programming, 55(1):293–
318, 1992. ISSN 0025-5610.

Sutton, R.S., Maei, H.R., Precup, D., Bhatnagar, S., Silver, D.,
Szepesvári, C., and Wiewiora, E. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th Annual International
Conference on Machine Learning, pp. 993–1000. ACM, 2009.

Efron, Bradley, Hastie, Trevor, Johnstone, Iain, and Tibshirani,
Robert. Least angle regression. The Annals of statistics, 32(2):
407–499, 2004.

Tibshirani, R. Regression shrinkage and selection via the lasso.
Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267–288, 1996.

Geist, M. and Scherrer, B. l1-penalized projected bellman residual. Recent Advances in Reinforcement Learning, pp. 89–101,
2012.

Tsitsiklis, J.N. and Van Roy, B. Feature-based methods for large
scale dynamic programming. Machine Learning, 22(1):59–94,
1996.

Geist, M., Supélec, IMS, Metz, F., Scherrer, B., Nancy, F.,
Lazaric, A., and Ghavamzadeh, M. A dantzig selector approach to temporal difference learning. In Proceedings of the
29th Annual International Conference on Machine Learning,
2012.

Tsitsiklis, J.N. and Van Roy, B. An analysis of temporaldifference learning with function approximation. Automatic
Control, IEEE Transactions on, 42(5):674–690, 1997.

Ghavamzadeh, M., Lazaric, A., Munos, R., and Hoffman, M.
Finite-sample analysis of lasso-td. In Proceedings of the 28th
Annual International Conference on Machine Learning, 2011.
Hale, E.T., Yin, W., and Zhang, Y. Fixed-Point Continuation for
L1-Minimization: Methodology and Convergence. SIAM Journal on Optimization, 19:1107, 2008.
Johns, Jeffrey, Painter-Wakefield, Christopher, and Parr, Ronald.
Linear complementarity for regularized policy evaluation and
improvement. In Advances in Neural Information Processing
Systems, pp. 1009–1017, 2010.
Kolter, J.Z. and Ng, A.Y. Regularization and feature selection in
least-squares temporal difference learning. In Proceedings of
the 26th Annual International Conference on Machine Learning, pp. 521–528. ACM, 2009.
Lagoudakis, M.G. and Parr, R. Least-squares policy iteration. The
Journal of Machine Learning Research, 4:1107–1149, 2003.

Xiao, L. Dual averaging methods for regularized stochastic learning and online optimization. The Journal of Machine Learning
Research, 11:2543–2596, 2010.
Yang, J. and Zhang, Y. Alternating direction algorithms for l1problems in compressive sensing. SIAM Journal on Scientific
Computing, 33(1):250–278, 2011.

