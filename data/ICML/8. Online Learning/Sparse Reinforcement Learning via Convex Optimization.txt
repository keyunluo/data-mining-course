Sparse Reinforcement Learning via Convex Optimization

Zhiwei (Tony) Qin1
@WalmartLabs, 850 Cherry Ave, San Bruno, CA 94066

TQIN @ WALMARTLABS . COM

Weichang Li, Firdaus Janoos
WEICHANG . LI , FIRDAUS . JANOOS @ EXXONMOBIL . COM
ExxonMobil Corporate Strategic Research, 1545 Rt. 22 East, Annandale, NJ 08801

Abstract
We propose two new algorithms for the sparse reinforcement learning problem based on different
formulations. The first algorithm is an off-line
method based on the alternating direction method
of multipliers for solving a constrained formulation that explicitly controls the projected Bellman residual. The second algorithm is an online
stochastic approximation algorithm that employs
the regularized dual averaging technique, using
the Lagrangian formulation. The convergence of
both algorithms are established. We demonstrate
the performance of these algorithms through several classical examples.

1. Introduction
As an approximation technique for problems involving
Markov Decision Processes (MDP), reinforcement learning has been applied to such diverse fields as robotics (Bentivegna et al., 2002), helicopter control (Ng et al., 2006),
and operations research (Proper & Tadepalli, 2006), among
many others. The problem of approximating the value
function of an MDP is central in reinforcement learning,
where the model information is usually unavailable, and
even noisy samples of the true target function values are
not accessible. When the state space is large or infinitedimensional, such a task is often accomplished through the
linear approximation architecture, under which the hypothesis space is spanned by a set of K basis functions or â€˜featuresâ€™ (Tsitsiklis & Van Roy, 1996). The least-squares temporal difference (LSTD) (Bradtke & Barto, 1996) learning
and gradient-based TD learning (Sutton et al., 2009) have
been effective ways of learning a linear representation of
the value function by using realized trajectory samples. It
has become increasingly clear that learning a sparse representation of the value function can prevent over-fitting

and improve generalization performance, especially with
the presence of a large number of features or a limited number of samples (e.g., (Kolter & Ng, 2009)).
In this paper we approach the sparse reinforcement learning
problem with a new constrained formulation that explicitly
controls the projected Bellman residual (PBR) and a popular Lagrangian formulation based on l1 -regularization. We
propose tailored optimization algorithms for solving each
of these two sparse reinforcement learning problems, including: 1) an efficient off-line off-policy learning algorithm using the alternating direction method of multipliers
(ADMM) (Eckstein & Bertsekas, 1992) with a proximal
gradient step, and 2) a novel online stochastic approximation algorithm that combines elements from the regularized
dual-averaging method (Xiao, 2010) and the least mean
squares technique. Convergence results are established for
both algorithms. We demonstrate through experiment results that regularized PBR minimization might be preferable over regularized minimization of the fixed-point slack
for value function approximation.
The rest of this paper is organized as follows. Section 2 reviews reinforcement learning with approximate value and
policy iterations, and TD learning. The main contribution
of this paper is presented in Sections 3 and 4. Section 3 introduces the constrained and the Lagrangian formulations
for sparse value function approximation and their extensions to the Q-function. The two new off-policy learning algorithms and their convergence proof are presented
in Section 4. We discuss related existing work in Section
4.4 and present experiment results against LSTD and two
other sparse reinforcement learning algorithms in Section
5. Finally we conclude the paper in Section 6.

2. Background and Preliminaries
In a Markov Decision Process (MDP), the state-action pair
is denoted by (x, u) âˆˆ X Ã—U(x), where X and U(x) are the
1

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

The work was completed when the first author was a summer
intern with ExxonMobil Corporate Strategic Research in 2012
working towards a Ph.D. degree at Columbia University, New
York.

Sparse Reinforcement Learning

set of states and the set of compatible actions respectively.
The set of state indices is S. The immediate reward by applying u to x is r(x, u). We use the capital letters X and R
to denote the random variable versions of x and r. Hence
the random reward R(t) := R(X(t), Ï€(X(t))). Ï€ denotes
a particular policy mapping from a state to a compatible action, i.e. Ï€(x) âˆˆ U(x). The
value function, V Ï€ : X â†’ R
Pâˆ
Ï€
is defined as V (x) = E [ t=0 Î³ t R(t) | X(0) = x], with
the discounting factor Î³ âˆˆ [0, 1). V Ï€ satisfies the Bellmanâ€™s equation: for x âˆˆ X ,
X
V Ï€ (x) = r(x, Ï€(x)) + Î³
P(x, Ï€(x), y)V Ï€ (y), (1)
yâˆˆX

where P(x, u, y) is the transition probability from state x
to state y by applying action u. The transition probabilities and the reward distribution are generally not known.
The optimal value function V âˆ— (x) satisfies the Bellmanâ€™s
optimality equation:
X
V âˆ— (x) = max r(x, u) + Î³
P(x, u, y)V âˆ— (y) (2)
uâˆˆU (x)

yâˆˆX
Ï€

Define the Bellman operator T by the right-hand-side of
(1) so that T Ï€ (V Ï€ ) := V Ï€ , and similarly, the Bellman optimality operator T âˆ— by the right-hand-side of (2) so that
T âˆ— (V âˆ— ) := V âˆ— .
2.1. Value Function Approximation
For problems with a large state space, evaluating V (t) exactly can be infeasibly expensive. A natural approach is to
approximate V by VÌƒ through some compact representation
with a parameter Î² âˆˆ RK . The most common approximation architecture is linear function approximation (Tsitsiklis & Van Roy, 1996), i.e.
VÌƒ (t) (x) =

K
X

Î² k Ï†k (x) = Î¦(x)T Î²

(3)

k=1

Ï†(x) defines a K-dimensional feature vector for the state
x and is part of the design architecture.
2.2. Approximate Policy Iteration
We consider the following state-action value function
"âˆ
#
X
QÏ€ (x, u) := E
Î³ t R(t+1) |X (0) = x, U (0) = u ,
t=0

which offers the flexibility of specifying the initial action u.
In approximate Policy Iteration, the tasks of policy evaluation and policy improvement are separated. Given a particular policy Ï€, we approximate the state-action value function QÏ€ (x, u) by using function approximation techniques.
An improved policy Ï€ + is then built upon QÏ€ (x, u), and
the iterations continue till convergence. Our approach for
learning the optimal policy falls into this class of methods.

2.3. Temporal Difference (TD) Learning
With the transition probabilities and the reward distribution unknown a priori, the value function V Ï€ with
regard to the underlying policy Ï€ has to be learned
through a set of n transition observations (or samples)
n
{(xt , ut , rt+1 , yt+1 )}t=1 . The most common approaches
include TD Learning (Sutton, 1988) and the Least Squares
TD (LSTD) (Bradtke & Barto, 1996). Defining the temporal difference associated with the transition from time t to
t + 1 by
Î´t+1 = rt+1 + Î³ VÌƒ (t) (yt+1 ) âˆ’ VÌƒ (t) (xt ),

(4)

TD Learning with linear value function approximation (3)
takes the following form (Tsitsiklis & Van Roy, 1997):

>
Î´t+1 (Î²) = rt+1 + Î³Ï†0>
t+1 Î² âˆ’ Ï†t Î²
(5)
(t)
Î² t+1 = Î² t + Î±t Î´t+1 âˆ‡Î² VÌƒ (xt ).
where the short-hand notation Ï†t := Ï†(xt ), Ï†0t+1 :=
Ï†(yt+1 ) denote the feature vectors.
The LSTD algorithm solves the linear system
AÌƒ(n) Î² = bÌƒ(n)
(6)
P
n
(n)
where
:= n1 t=1 (Ï†t (Ï†t âˆ’ Î³Ï†0t+1 )> ), bÌƒ(n) :=
Pn AÌƒ
1
t=1 (rt+1 Ï†t ). We will drop the superscripts for simn
pler notation. Loosely speaking, the overall idea here is
to sample the unknown state transition probabilities from
a sequence of state trajectories, and to construct the subspace within which the value function could be well approximated in linear form.
For compact notation, we replace the summation terms in
(6) by the matrix forms:
1 >
> 0
(7)
(Î¦Ìƒ Î¦Ìƒ âˆ’ Î³ Î¦Ìƒ Î¦Ìƒ )
n
1 >
bÌƒ :=
(8)
(Î¦Ìƒ RÌƒ).
n
>
0
where Î¦Ìƒ := Ï†1 , Â· Â· Â· , Ï†n
âˆˆ RnÃ—K , Î¦Ìƒ :=

>
Ï†02 , Â· Â· Â· , Ï†0n+1
âˆˆ
RnÃ—K ,
RÌƒ
:=
>
n
r2 , Â· Â· Â· , rn+1
âˆˆ R . In addition, we define
AÌƒ :=

>

GÌƒ := n(Î¦Ìƒ Î¦Ìƒ)âˆ’1 . We note that AÌƒ bÌƒ and GÌƒ may be viewed
as sample averaged approximation to the following ensemble avaerage terms: A := E[Ï†(X)(Ï†(X) âˆ’ Î³Ï†(Y ))> ],
b := E[RÏ†(X)] and G := E[Ï†(X)Ï†(X)> ]âˆ’1 .

3. Sparse Reinforcement Learning
The linear value function approximation architecture poses
two conflicting requirements on the size of the feature
space: First, a compact representation of the value function, desired for both computational and storage consideration, calls for a small number of features; On the other

Sparse Reinforcement Learning

hand, a better approximation to the value function requires
a rich feature space which implies a relatively large number
of features. However, as we saw above, the linear system
for LSTD may not even be invertible with the number of
features larger than the number of samples. This motivates
feature selection, or more precisely, learning a sparse representation out of a large number of basis functions.

Note that (11) is exactly the basis pursuit denoising
(BPDN) problem. It finds the most sparse solution that satisfies the given tolerance on the PBR. The Euclidean projection onto the l2 ball can be computed in closed-form, facilitating the development of a fast optimization algorithm.

A common loss function, which both the GTD2/TDC
(Sutton et al., 2009) and LSTD methods minimize,
is based on the projected Bellman residual (PBR):
LPBR (Î²) := kVÌƒ âˆ’ Î T Ï€ VÌƒ k2S , where S is a diagonal matrix
whose diagonal entries represent the stationary distribution
of states in the input data, and Î  = Î¦(Î¦> Î¦)âˆ’1 Î¦>
1
is the orthogonal projector
onto the feature space.
âˆš
x> Sx. With linear value funcThe norm kxkS :=
tion approximation it can be shown that LPBR (Î²) =
E[Î´(Î²)Ï†(X)]> E[Ï†(X)Ï†(X)> ]âˆ’1 E[Î´(Î²)Ï†(X)]
=
Ë†
kAÎ² âˆ’ bk2G . The corresponding empirical loss LPBR
can be obtained by replacing A, b, G with AÌƒ, bÌƒ, GÌƒ:
Ë† (Î²) := kAÌƒÎ² âˆ’ bÌƒk2 , and the sample based feature
LPBR
nGÌƒ

We consider the Lagrangian formulation of problem (9):

>

>

space projector is given by Î Ì‚ = Î¦Ìƒ(Î¦Ìƒ Î¦Ìƒ)âˆ’1 Î¦Ìƒ .
We now discuss two different formulations of sparse TD
learning through the l1 -regularized empirical PBR minimization.
3.1. Constrained Formulations
We first propose a constrained formulation where we have
explicit control of the PBR:

	
min kÎ²k1 | LPBR (Î²) â‰¤ 2 .
(9)
Î²

In the offline setting with finite samples, we solve the empirical version of the above problem
n
o
min kÎ²k1 | kAÌƒÎ² âˆ’ bÌƒknGÌƒ â‰¤  .
(10)
Î²

When K > n and  = 0, it is a special case of finding the
sparsest solution that satisfies the projected Bellman fixedpoint equation and is also known as the basis pursuit problem.

3.2. Lagrangian Formulation

min
Î²

1
LPBR + ÏkÎ²k1 .
2

(12)

Existing works have focused on offline algorithms for solving the empirical version of this problem with LPBR replaced by LÌ‚PBR . The empirical version can be treated as
a vanilla unconstrained Lasso problem, so that many existing solvers can be directly applied, including LARS (Efron
et al., 2004), FISTA (Beck & Teboulle, 2009), and FPC-BB
(Hale et al., 2008). However, the stochastic version (12) is
more challenging to solve, as also mentioned in (Liu et al.,
2012). In Section 4.2, we propose a novel online algorithm
for solving the stochastic optimization problem (12).
3.3. Extension to the Q-Function
It is straightforward to extend the above algorithms to
estimate the state-action value function (Q-function) and
integrate them into the policy iterations by following
(Lagoudakis & Parr, 2003). The feature vector Ï†(x) âˆˆ
RK is augmented to Ï†(x, u) âˆˆ RKÃ—|U | . The defini0
tions of Î¦Ìƒ and Î¦Ìƒ are modified accordingly to incorporate the additional argument u. The Bellman equaÏ€
tion,
is QÏ€ (x, u) = r(x, u) +
P which Q satisfies,
Ï€
Î³ yâˆˆX P(x, u, y)Q (y, Ï€(y)), x âˆˆ X .

4. Learning Algorithms
Our approaches for solving the constrained formulations
(BPDN (11)) is based on ADMM, which has been applied
extensively in solving machine learning problems. For
the Lagrangian formulation (12), we develop a specialized
stochastic gradient descent method which is based on two
approximation sequences of different speed.

0

We define dÌƒ := Î Ì‚RÌƒ, CÌƒ := Î³ Î Ì‚Î¦Ìƒ âˆ’ Î¦Ìƒ as in (Geist & Scherrer, 2012) and reformulate (10) into
n
o
min kÎ²k1 | kdÌƒ + CÌƒÎ²k â‰¤  .
(11)
Î²

The empirical PBR loss term used in (11) is equivalent to
that used in (10) since
0

kCÌƒÎ² + dÌƒk2 = kÎ Ì‚(RÌƒ + Î³ Î¦Ìƒ Î² âˆ’ Î¦ÌƒÎ²)k2 = kbÌƒ âˆ’ AÌƒÎ²k2nGÌƒ .
1

When the Grammian matrix is not invertible, we can use its
Moore-Penrose pseudoinverse.

4.1. BPDN (Offline)
We first explain an inexact version of the ADMM (Yang &
Zhang, 2011), which was originally developed for solving
compressed sensing problems. We apply variable-splitting
to (11) and reformulate the problem into
n
o
min kÎ²k1 + I(kÎ±k â‰¤ ) | dÌƒ + CÌƒÎ² = Î± .
(13)
Î²

The augmented Lagrangian of this constrained problem is
L(Î±, Î², v) = kÎ²k1 + I(kÎ±k â‰¤ ) âˆ’ v> (dÌƒ + CÌƒÎ² âˆ’ Î±) +
1
2
2Âµ kdÌƒ + CÌƒÎ² âˆ’ Î±k , where v is the Lagrange multipliers.

Sparse Reinforcement Learning

For solving for Î±, we minimize minÎ± L(Î±, Î², v), which
is equivalent to the following Euclidean projection problem
onto the L2 ball,


1
2
min
kÎ± âˆ’ ck | kÎ±k â‰¤  ,
(14)
Î±
2
where c = dÌƒ + CÌƒÎ² âˆ’ Âµv. The solution to the above projection problem is given by

Î±=

c, ifkck â‰¤ 
c
, otherwise.
 kck

(15)

The subproblem w.r.t. Î² is
min
Î²

1
kCÌƒÎ² + dÌƒ âˆ’ Î± âˆ’ Âµvk2 + ÂµkÎ²k1 .
2

(16)

Define f (Î², Î±) := 12 kCÌƒÎ² + dÌƒ âˆ’ Î± âˆ’ Âµvk2 , then
âˆ‡Î² f (Î², Î±) = CÌƒ> (CÌƒÎ² + dÌƒ âˆ’ Î± âˆ’ Âµv), which is Lipschitz
continuous with a Lipschitz constant L = Î»max (CÌƒ> CÌƒ),
where Î»max denotes the largest eigenvalue. Problem (16)
is a Lasso problem itself, and we can employ any Lasso
solver as a subroutine. However, this step becomes very
expensive in this way. Instead, we take a proximal gradient step to solve this subproblem approximately, which is
much cheaper computationally. Specifically, given Î² t and
Î±, we solve
min
Î²

1
kÎ² âˆ’ (Î² j âˆ’ Ï„ âˆ‡Î² f (Î² j , Î±))k2 + Ï„ ÂµkÎ²k1 ,
2

1: Given CÌƒ, dÌƒ, Âµ, Ï„, .
2: for j = 0, 1, Â· Â· Â· do
3:
c â† dÌƒ + CÌƒÎ² j âˆ’ Âµvj
4:
Î±j+1 â† (15)
5:
Î² j+1 â† SÏ„ Âµ (Î² j âˆ’ Ï„ âˆ‡Î² f (Î² j , Î±j+1 ))
6:
vj+1 â† vj âˆ’
7: end for

1
Âµ (dÌƒ

+ CÌƒÎ² j+1 âˆ’ Î±j+1 )

We describe our online algorithm for solving the unconstrained stochastic optimization problem (12). The
algorithm is inspired by the regularized dual averaging (RDA) algorithm (Xiao, 2010) with a specialized
stochastic approximation of the LPBR gradient. The
gradient of LP BR (Î²) is âˆ‡LP BR (Î²) = E[(Ï†(X) âˆ’
Î³Ï†(Y ))Ï†(X)> ]E[Ï†(X)Ï†(X)> ]âˆ’1 E[Î´(Î²)Ï†(X)]. Note
that the finite sample approximation LPË†BR used in BPDN
is a biased estimate of LP BR (Î²). Likewise, the direct finite sample approximation of the gradient is also biased
without double sampling the next state Y . To get around
this problem, we adopt the approach of (Sutton et al.,
2009) for GTD2 and use an auxiliary variable Ï‰ to approximate E[Ï†(X)Ï†(X)> ]âˆ’1 E[Î´(Î²)Ï†(X)]. The gradient
Ëœ t (Î²) =
can now be estimated by sample approximation âˆ‡f
0
âˆ†t (Ï†>
Ï‰
),
where
âˆ†
=
Î³Ï†
âˆ’
Ï†
.
The
auxiliary
varit
t
t
t
t+1
able Ï‰ is updated by Least Mean Square (LMS) rule as in
(Sutton et al., 2009). Each iteration of our algorithm then
solves the following problem
c
Â¯ t , Î²i + ÏkÎ²k1 + âˆš
kÎ²k2
min hâˆ‡f
Î²
2 t
Â¯ t = 1 Pt âˆ‡f
Ëœ t (Î² (s) ) is the time average of
where âˆ‡f
s=1
t
the gradient approximations, and c is a positive constant.
Note that the coefficient
for kÎ²k2 above corresponds to the
âˆš
c t
scaling factor Lt = 2 as defined in (Xiao,
 âˆš2010). The
(t+1)
Â¯t .
solution to this problem is Î²
= S âˆštÏ âˆ’ t âˆ‡f
c

(17)

which has a closed-form solution Î² j+1 = SÏ„ Âµ (Î² j âˆ’
Ï„ âˆ‡Î² f (Î² j , Î±)), where SÎ· (Â·) := max(| Â· | âˆ’ Î·, 0) is the
shrinkage operator (Tibshirani, 1996) with the shrinkage
parameter Î·, and the operation is component-wise. It can
be shown that ADMM with this proximal step converges if
Ï„ < Î» (1CÌƒ> CÌƒ) . The stopping criteria that we use in the
max
implementation is based on the primal and dual residuals
(Boyd et al., 2010). A major advantage of this algorithm is
that it is a first-order method, requiring only matrix-vector
multiplications. The iterations are thus very fast to compute
and simple to implement. We state the inexact ADMM algorithm in Algorithm 1.
Algorithm 1 ADMM-BPDN

4.2. L1 -regularized PBR minimization (Online)

c

We state the RDA algorithm with the customized gradient approximation in Algorithm 2. The computational and
storage complexity of each iteration is linear O(K).
Algorithm 2 RDA with LMS update
Â¯ 0 = 0.
1: Given Ï‰ 0 , Î³, c, {Î·t }, âˆ‡f
2: for t = 1, Â· Â· Â· , Tmax do
3:
âˆ†t â† Î³Ï†0t+1 âˆ’ Ï†t
Ëœ t â† âˆ†t (Ï†> Ï‰ tâˆ’1 )
4:
âˆ‡f
t
Â¯ t â† 1 ((t âˆ’ 1)âˆ‡fÂ¯tâˆ’1 + âˆ‡f
Ëœ t)
5:
âˆ‡f
t

Ï‰t â† Ï‰
+ Î·t (Î´t+1 (Î² tâˆ’1 ) âˆ’ Ï†>
t Ï‰ tâˆ’1 )Ï†t
âˆš tâˆ’1
t
Â¯ t)
7:
Î² t â† c SÏ (âˆ’âˆ‡f
8: end for

6:

4.3. Convergence Analysis
Both of the proposed algorithms enjoy guaranteed convergence properties. We establish the convergence results in
Theorems 4.1 and 4.2 below.
4.3.1. BPDN-ADMM (O FFLINE )
Theorem 4.1. For any positive Ï„ < Î» (1CÌƒ> CÌƒ) , the semax
quence {(Î±j , Î² j , vj )} generated by Algorithm 1 converges

Sparse Reinforcement Learning

to an optimal solution to problem (11) from any starting
point.

recursion

1
F (gÌ„t , Ï‰ t ) + Mt+1 ,
t+1


= Ï‰ t + Î·t G(gÌ„t , Ï‰ t ) + Nt+1 .

gÌ„t+1 = gÌ„t +
The proof parallels the one given in (Yang & Zhang, 2011)
for compressed sensing and the one in (Ma et al., 2012)
for latent variable Gaussian graphical model selection. The
major steps are outlined in Appendix A the supplementary
file.
4.3.2. RDA-LMS (O NLINE )

Ï‰ t+1

(19)
(20)

We prove the convergence of this coupled recursion
through the following propositions and lemmas, which facilitate the application of Theorem 1.1 in (Borkar, 1997).

The original convergence results for RDA in (Xiao, 2010)
no longer applies due to the fact that we are only approximating the gradients through maintaining a quasistationary sequence of Ï‰ t . Our proof of convergence is
based on the analysis of a two-time-scale stochastic approximation (Borkar, 1997) similar to that in (Sutton et al.,
2009).

In all subsequent results, we make the following standard assumptions: (A1) The MDP is finite and stationary.
(A2) The transition samples {(Ï†t , rt+1 , Ï†0t+1 )}
is an i.i.d. sequence with uniformly bounded second
moments. (A3) The features defined by Ï† are linearly
independent, and E[Ï†Ï†> ]âˆ’1 exists. (A4) The iterates
{gÌ„t , Ï‰ t } are bounded, i.e. supt kgÌ„t k < âˆ, supt kÏ‰ t k <
âˆ.

Â¯ t for
For notational conciseness, we use gÌ„t in place of âˆ‡f
the time-averaged gradient. The iterations in Algorithm 2
can be written as

Proposition 4.1. The functions F and G are Lipschitz continuous.

ï£±
1
t
Ï‰)
gÌ„t + t+1
(Î³Ï†0t+1 âˆ’ Ï†t )(Ï†>
ï£² gÌ„t+1 = t+1
 t t
Ï‰
Ï†
Ï‰ t+1 = Ï‰ t + Î·t Î´t+1 (Î² t ) âˆ’ Ï†>
t
t
t
ï£³
Î² t+1 = qSÏ (âˆ’gÌ„t+1 ).
Note that a different scalar is used in the update for Î² t .
The original update
âˆš in Algorithm 2 corresponds to using a
scaling factor c t as suggested in (Xiao, 2010). For the
convergence analysis here, we choose a scaling factor qt
(q > 0) so that the update for Î² t does not explicitly depends on t. Note that the sequence {qt} is still non-negative
and non-decreasing, as required in (Xiao, 2010). The above
iteration can be further consolidated into
ï£±
ï£² gÌ„t+1 = gÌ„t +

1
(âˆ’gÌ„t
t+1

+ DÌ‚t Ï‰ t )


ï£³ Ï‰ t+1 = Ï‰ t + Î·t (bÌ‚t âˆ’ q AÌ‚t SÏ (âˆ’gÌ„t )) âˆ’ Ï†Ï†> Ï‰ t ,
(18)
where bÌ‚t = Ï†t rt+1 , AÌ‚t = Ï†t (Ï†t âˆ’ Î³Ï†0t+1 )> , DÌ‚t =
(Î³Ï†0t+1 âˆ’ Ï†t )Ï†>
t , so that b = E[bÌ‚t ], A = E[AÌ‚t ] by our
definitions, and we define D := E[DÌ‚t ]. We also define the
following functions and variables:
F (gÌ„t , Ï‰ t )
Mt+1
K

:= âˆ’gÌ„t + DÏ‰ t
:=

(DÌ‚t âˆ’ D)Ï‰ t

:= E[Ï†Ï†> ]

G(gÌ„t , Ï‰ t )

:=

(b âˆ’ qASÏ (âˆ’gÌ„t )) âˆ’ KÏ‰ t

Nt+1

:=

(bÌ‚t âˆ’ b) âˆ’ q(AÌ‚ âˆ’ A)SÏ (âˆ’gÌ„t ) âˆ’
(Ï†t Ï†>
t âˆ’ K)Ï‰ t

Now, the iteration (18) is in the form of a coupled stochastic

Proof. Since F is linear in both gÌ„t and Ï‰ t , it is clearly Lipschitz continuous. For the Lipschitz continuity of G, it suffices to show that the shrinkage operator SÏ (Â·) is Lipschitz
continuous. But it is known that SÏ (Â·) is non-expansive
(Hale et al., 2008), and hence, it is Lipschitz continuous.
Proposition 4.2. For each gÌ„ âˆˆ RK , the o.d.e.
Ï‰Ì‡ t = G(gÌ„, Ï‰ t )

(21)

has a unique global asymptotically stable equilibrium Î»(gÌ„)
such that the function Î»(Â·) is Lipschitz continuous.
Proof. With gÌ„ fixed, G(gÌ„, Ï‰ t ) = (bâˆ’qASÏ (âˆ’gÌ„))âˆ’KÏ‰ t ,
where the first term is a constant. Since the features
are linearly independent, K is symmetric positive definite.
Hence, the o.d.e. (21) has a unique global asymptotic stable equilibrium Kâˆ’1 (b âˆ’ qASÏ (âˆ’gÌ„)) = Î»(gÌ„). Now, since
SÏ (Â·) is non-expansive, the function Kâˆ’1 ASÏ is clearly
Lipschitz continuous. Hence, the claim holds.
Lemma 4.1. For any 0 < q <

1
Ïƒmax (DKâˆ’1 A) ,

gÌ„Ë™ t = F (gÌ„t , Î»(gÌ„t ))

the o.d.e.
(22)

has a unique global asymptotically stable equilibrium gÌ„âˆ— .
Proof. We can rewrite the right-hand-side of the o.d.e. (22)
as
F (gÌ„t , Î»(gÌ„t )) = H(gÌ„t ) âˆ’ gÌ„t ,
where H(gÌ„t ) = DKâˆ’1 (b âˆ’ qASÏ (âˆ’gÌ„t )). The shrinkage
operator SÏ (Â·) is non-expansive w.r.t. any lp -norm, with
p â‰¥ 0 (Hale et al., 2008). Then, it is easy to see that for

Sparse Reinforcement Learning
1
any 0 < q < Ïƒmax (DK
âˆ’1 A) , H(Â·) is a contraction w.r.t.
the l2 -norm. By Theorem 3.1 in (Borkar & Soumyanatha,
1997), the o.d.e. (22) is globally asymptotically stable. The
equilibrium point gÌ„âˆ— satisfies

â€¢ For each gÌ„ âˆˆ RK , the o.d.e. (21) has a unique global
asymptotically stable equilibrium Î»(gÌ„) such that the
function Î»(Â·) is Lipschitz continuous.

F (gÌ„âˆ— , Î»(gÌ„âˆ— )) = 0
gÌ„âˆ— = H(gÌ„âˆ— ).

â‡”

Hence, gÌ„âˆ— is a fixed-point of the operator H(Â·). Since H(Â·)
is a contraction w.r.t. the l2 -norm, the fixed-point is unique.
Lemma 4.2. The unique optimal solution to the PBRLasso problem
1
LPBR (Î²) + ÏkÎ²k1 .
2

(23)

is given by Î² âˆ— = qSÏ (âˆ’gÌ„âˆ— ).
Proof. First, we observe that for any 0 < Î¾t < 1, the iterate
gÌ„t+1 = gÌ„t + Î¾t (âˆ’gÌ„t + H(gÌ„t )) has the same fixed-point gÌ„âˆ—
1
, the above
if it converges. In particular, with Î¾t = t+1
iterate is equivalent to
gÌ„t+1 =

t
1
gÌ„t +
H(gÌ„t ),
t+1
t+1

(24)

which is exactly the time-averaged gradient of the loss
function LPBR(Â·) evaluated at Î² 1 , Â· Â· Â· , Î² t . By construction,
the sequence {Î² t = qSÏ (âˆ’gÌ„t )} is the same as the one
generated by the RDA algorithm (Xiao, 2010) applied to
the strictly convex optimization problem (23) using the true
gradients. By the convergence results in (Xiao, 2010), the
sequence {Î² t } converges to the unique optimal solution
of problem (23). Hence, qSÏ (âˆ’gÌ„âˆ— ) â‰¡ Î² âˆ— , and the claim
holds.
Theorem 4.2. The sequence {Î² t , Ï‰ t } generated by Algorithm 2 converges to (Î² âˆ— , Ï‰ âˆ— ) w.p. 1.
Proof. In order to apply Theorem 1.1 in (Borkar, 1997),
we need to verify the following conditions:
â€¢ The functions F and G are Lipschitz continuous.
â€¢ The Î¾t and Î·t satisfy
X
Î¾t = âˆ,

X

t

X

Î·t = âˆ,

t

Î¾t2 < âˆ,

X

t

Î·t2 < âˆ,

t

Î¾t = (o)(Î·t ).
â€¢ The sequences {Mt } and {Nt } satisfy
X
X
Î¾t Mt < âˆ,
Î·t Nt < âˆ.
t

t

â€¢ The o.d.e. (22) has a unique global asymptotically stable equilibrium gÌ„âˆ— .

1
The step size of the recursion (19) Î¾t is equal to t+1
in our
Î¾t
case. Then, Î·t â†’ 0 as t â†’ âˆ, thus satisfying the above
conditions. It is also easy to see that both Mt+1 and Nt+1
are martingale differences with bounded second moments,
by (A1), (A2), (A4), and the non-expansiveness of SÏ (Â·).
The third condition can be ensured by applying
Ps martingale
âˆ
convergence
theorem
on
the
martingales
{
t=1 Î¾t Mt }s=1
Ps
âˆ
and { t=1 Î·t Nt }s=1 . The conditions for Theorem 1.1 in
(Borkar, 1997) are all satisfied, given Propositions 4.1 and
4.2 and Lemma 4.1. An immediate consequence is that the
sequences {(gÌ„t , Ï‰ t )} generated by Algorithm 2 converge
to (gÌ„âˆ— , Ï‰ âˆ— ) w.p. 1, where Ï‰ âˆ— = Î»(gÌ„âˆ— ). By Lemma 4.2, the
iterates {Î² t } converge to Î² âˆ— w.p. 1.

4.4. Related Work
There have been some recent work on the development of
a sparse LSTD method. LARS-TD (Kolter & Ng, 2009;
Ghavamzadeh et al., 2011) applies the l1 -regularization to
the projector onto the space of representable linear functions and solves the problem through LARS. (Johns et al.,
2010) formulates the L1 regularized linear fixed-point
problem as a linear complementarity problem. (Painterwakefield & Parr, 2012) proposes a greedy algorithm based
on orthogonal matching pursuit. l1 -PBR (Geist & Scherrer,
2012) instead applies the l1 -regularization to the classical
PBR minimization, and (Mahadevan & Liu, 2012) develops an on-policy algorithm based on mirror-descent. Unlike the above algorithms, our BPDN algorithm solves a
constrained formulation of the sparse TD learning problem which explicitly controls the PBR. The closest work
to our method is probably (Geist et al., 2012) which constrains kAÌƒÎ² âˆ’ bÌƒkâˆ . Our RDA-LMS algorithm, on the
other hand, is an off-policy online algorithm based on gradient TD (Sutton et al., 2009). The only other l1 -based
online sparse TD method that we are aware of is RO-TD
(Liu et al., 2012), which minimizes a different loss function, kAÌƒÎ² âˆ’ bÌƒk2 , based on the slack in (6) similar to DLSTD. Note that the PBR is a weighted squared-norm of
the fixed-point slack, being equivalent to kAÌƒÎ² âˆ’ bÌƒk22 only
when the feature bases are orthonormal (G = I), which is
not true in general. This difference appears to contribute
to the different convergence speeds that we have observed
below in Section 5.2.

Sparse Reinforcement Learning

5. Experiments
5.1. BPDN
We compared the proposed ADMM algorithm for BPDNbased sparse reinforcement learning with LSTD and DLSTD (Geist et al., 2012) on two test problems, which
are described in the next two sections. D-LSTD was formulated as a linear program and solved by the Mosek LP
solver. For both problems, we constructed features using
RBF basis functions with centers on grids of different densities and widths, five polynomial basis functions, as well
as irrelevant features (white noise) of different lengths. We
show results on fit errors (w.r.t. the true value functions)
for value function approximation over 20 independent runs
and simulated reward for the policy obtained from policy
iterations over 10 independent runs.

Figure 1. Chain Walk: Comparison on approximating the stateaction value function of the optimal policy. The number of irrelevant features is 500 and 1000 in that order.

5.1.1. C HAIN WALK
This is the classical 20-state test example from (Lagoudakis
& Parr, 2003). Two actions (left or right) are available to
control the current state. Visiting state 1 and 20 yields a reward of 1 and zero reward for anywhere else. Either action
has a success probability of 0.9. Failure of an action bring
the current state to the opposite direction.
For approximating the optimal value function, we collected
1000 training samples off-policy by following a random
policy for 100 episodes, 10 steps each. The samples were
collected in the same way for policy iterations, and we used
a fixed set of samples throughout the policy iterations. The
value function of any given policy can be computed exactly
for this problem, and the optimal policy is to go to the left if
the current state is from 1 to 10, and to the right otherwise.
Figures 1 and 2 show the comparison among BPDN, DLSTD, and LSTD on the approximation error (RMSE)
of the Q value function and the simulated reward of the
learned policies for two cases of irrelevant features. The
whiskers of the boxplots extend to the most extreme data
point which is no more than 1.5 times the interquartile
range from the boxes. BPDN and D-LSTD are significantly better than LSTD on both metrics in the presence
of irrelevant features. BPDN is competitive to D-LSTD on
simulated reward while doing tangibly better on value approximation. The average CPU times for BPDN on one
instance of value approximation are 11.4s for Nnoise = 500
and 15.7s for Nnoise = 1000, and those for D-LSTD are
6.6s and 31.7s respectively. It is clear that BPDN has better
scalability, being a first-order method.
5.1.2. I NVENTORY C ONTROL
This is a single-shop single-product inventory control problem. The inventory level at the end of the day is a continuous variable between 0 and 20. (Think of each unit as

Figure 2. Chain Walk: Comparison on simulated reward from
policy iterations. The number of irrelevant features is 500 and
1000 in that order.

a pallet of the products.) Orders of new items have to be
made in batches of five (i.e. five actions). Customer demand for each day follows the continuous uniform distribution [0, 24]. There is a one-time order overhead of $2 for
any positive orders. The product unit cost is $2, and the
selling price is $2.5 per unit. Any n unsold items incur an
inventory cost of $0.2n1.4 . Any unmet customer demand
incurs a penalty of $0.5 per unit. The goal is to learn an
optimal planning strategy to operate the store.
We collected two sets of off-policy training samples with
sizes 1000 and 5000 respectively. The samples were collected from different numbers of episodes of 10 steps each.
The policy used for value function approximation tests is
one that fills up the inventory as much as possible every
day. Since the state space is continuous, the value function
cannot be solved exactly. We used Monte Carlo rollouts to
compute the value of the test states and inital actions associated with the test policy.
Figures 3 present the comparison results on the fitting error of the Q value function and the simulated reward of
the learned policies. Similar to the Chain Walk example,
BPDN was able to learn a more accurate Q value function and a better policy than LSTD from the same number
of samples. We were unable to test D-LSTD on this domain because the large problem size led to excessive memory consumption by the Mosek LP solver, which reported
out-of-memory error. This again shows that our first-order

Sparse Reinforcement Learning

Figure 4. Left: Evolution of PBR on the Star example. Right: Qand V-value functions approximation (by running RDA-LMS) associated with (bottom) the optimal policy and (top) a suboptimal
policy for Chain.

Figure 3. Inventory Control: Comparison on state-action value
function approximation of an sub-optimal policy (top) and simulated reward from policy iterations (bottom). The number of irrelevant features is 1000. The number of training samples is 1000
for the left column and 5000 for right column.

method is more scalable and memory efficient.
5.2. RDA-LMS
We tested the RDA-LMS algorithm (Algorithm 2) on the
7-state Star MDP example (Sutton et al., 2009) and Chain
domain to empirically verify the convergence of the algorithm. For the Star example, we set Ï = 0 to verify the
convergence of the PBR to zero, and hence, the correctness of the algorithm. We plotted the evolution of the PBR
for RDA-LMS, RO-TD and GTD2 in Figure 4. RDA-LMS
was able to decrease the PBR much more rapidly than both
RO-TD and GTD2. Comparisons were also made against
RO-TD on Chain. We considered approximating the Qvalue functions associated with the optimal policy and a
sub-optimal policy. The transition samples were collected
from 100 random episodes, with 10 consecutive steps each,
totalling a collection of 1,000 samples. The algorithms
swept through the samples multiple times. The features
were constructed in the same way as in the previous section, except that there was no white noise features. Comparisons of the learned (solid lines) and true (dotted lines)
value functions were plotted in the right-hand-side of Figure 4, which shows that the sparse solutions obtained by
RDA-LMS approximate the true value functions faithfully.
Figure 5 provides empirical evidence for the convergence
of RDA-LMS with non-zero l1 -regularization. In addition,
RDA-LMS decreased the PBR at a much faster rate than
RO-TD.2 We surmise that this is because RDA-LMS min2
The approximation error of RO-TD hovers above 20 and is
not plotted.

Figure 5. Evolution of PBR of RDA-LMS and RO-TD (left) and
approximation error of RDA-LMS (right) for the Q-value function
associated with the optimal policy for Chain.

imizes the PBR directly while RO-TD minimizes the slack
in the fixed-point condition (6) as a proxy.

6. Conclusion
In this paper, we have proposed two optimization algorithms for solving two different formulations of the sparse
reinforcement learning problem: an ADMM-based off-line
algorithm for constrained sparse TD learning and a novel
online algorithm which combines elements from the RDA
and LMS methods for unconstrained sparse TD learning.
We provide convergence analysis for both algorithms, and
promising preliminary test results have been reported in
comparison to LSTD and two other sparse reinforcement
learning algorithms. Future work will be focused on applications of the proposed sparse TD learning algorithms on
real-world problems.

7. Acknowledgement
The authors would like to thank Niranjan Subrahmanya and
Wendy (Lu) Xu for valuable discussions and the anonymous reviewers for helpful feedback. Special thanks to Bo
Liu and Ji Liu for sharing the sample ROTD code.

Sparse Reinforcement Learning

References
Beck, A. and Teboulle, M. A fast iterative shrinkage-thresholding
algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183â€“202, 2009.
Bentivegna, Darrin C, Ude, Ales, Atkeson, Christopher G, and
Cheng, Gordon. Humanoid robot learning and game playing
using pc-based vision. In Intelligent Robots and Systems, 2002.
IEEE/RSJ International Conference on, volume 3, pp. 2449â€“
2454. IEEE, 2002.
Borkar, V.S. Stochastic approximation with two time scales. Systems & Control Letters, 29(5):291â€“294, 1997.
Borkar, V.S. and Soumyanatha, K. An analog scheme for fixed
point computation. i. theory. Circuits and Systems I: Fundamental Theory and Applications, IEEE Transactions on, 44(4):
351â€“355, 1997.
Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J. Distributed optimization and statistical learning via the alternating
direction method of multipliers. Machine Learning, 3(1):1â€“
123, 2010.

Liu, Bo, Mahadevan, Sridhar, and Liu, Ji. Regularized off-policy
td-learning. In Advances in Neural Information Processing
Systems 25, pp. 845â€“853, 2012.
Ma, S., Xue, L., and Zou, H. Alternating direction methods for
latent variable gaussian graphical model selection. Technical
report, University of Minnesota, 2012.
Mahadevan, Sridhar and Liu, Bo. Sparse q-learning with mirror
descent. In UAI 2012, 2012.
Ng, Andrew Y, Coates, Adam, Diel, Mark, Ganapathi, Varun,
Schulte, Jamie, Tse, Ben, Berger, Eric, and Liang, Eric. Autonomous inverted helicopter flight via reinforcement learning.
In Experimental Robotics IX, pp. 363â€“372. Springer, 2006.
Painter-wakefield, Christopher and Parr, Ronald. Greedy algorithms for sparse reinforcement learning. In Proceedings of the
29th International Conference on Machine Learning (ICML12), pp. 1391â€“1398, 2012.
Proper, S. and Tadepalli, P. Scaling model-based average-reward
reinforcement learning for product delivery. Machine Learning: ECML 2006, pp. 735â€“742, 2006.

Bradtke, S.J. and Barto, A.G. Linear least-squares algorithms for
temporal difference learning. Machine Learning, 22(1):33â€“57,
1996.

Sutton, R.S. Learning to predict by the methods of temporal differences. Machine learning, 3(1):9â€“44, 1988.

Eckstein, J. and Bertsekas, D.P. On the Douglas-Rachford splitting method and the proximal point algorithm for maximal
monotone operators. Mathematical Programming, 55(1):293â€“
318, 1992. ISSN 0025-5610.

Sutton, R.S., Maei, H.R., Precup, D., Bhatnagar, S., Silver, D.,
SzepesvaÌri, C., and Wiewiora, E. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th Annual International
Conference on Machine Learning, pp. 993â€“1000. ACM, 2009.

Efron, Bradley, Hastie, Trevor, Johnstone, Iain, and Tibshirani,
Robert. Least angle regression. The Annals of statistics, 32(2):
407â€“499, 2004.

Tibshirani, R. Regression shrinkage and selection via the lasso.
Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267â€“288, 1996.

Geist, M. and Scherrer, B. l1-penalized projected bellman residual. Recent Advances in Reinforcement Learning, pp. 89â€“101,
2012.

Tsitsiklis, J.N. and Van Roy, B. Feature-based methods for large
scale dynamic programming. Machine Learning, 22(1):59â€“94,
1996.

Geist, M., SupeÌlec, IMS, Metz, F., Scherrer, B., Nancy, F.,
Lazaric, A., and Ghavamzadeh, M. A dantzig selector approach to temporal difference learning. In Proceedings of the
29th Annual International Conference on Machine Learning,
2012.

Tsitsiklis, J.N. and Van Roy, B. An analysis of temporaldifference learning with function approximation. Automatic
Control, IEEE Transactions on, 42(5):674â€“690, 1997.

Ghavamzadeh, M., Lazaric, A., Munos, R., and Hoffman, M.
Finite-sample analysis of lasso-td. In Proceedings of the 28th
Annual International Conference on Machine Learning, 2011.
Hale, E.T., Yin, W., and Zhang, Y. Fixed-Point Continuation for
L1-Minimization: Methodology and Convergence. SIAM Journal on Optimization, 19:1107, 2008.
Johns, Jeffrey, Painter-Wakefield, Christopher, and Parr, Ronald.
Linear complementarity for regularized policy evaluation and
improvement. In Advances in Neural Information Processing
Systems, pp. 1009â€“1017, 2010.
Kolter, J.Z. and Ng, A.Y. Regularization and feature selection in
least-squares temporal difference learning. In Proceedings of
the 26th Annual International Conference on Machine Learning, pp. 521â€“528. ACM, 2009.
Lagoudakis, M.G. and Parr, R. Least-squares policy iteration. The
Journal of Machine Learning Research, 4:1107â€“1149, 2003.

Xiao, L. Dual averaging methods for regularized stochastic learning and online optimization. The Journal of Machine Learning
Research, 11:2543â€“2596, 2010.
Yang, J. and Zhang, Y. Alternating direction algorithms for l1problems in compressive sensing. SIAM Journal on Scientific
Computing, 33(1):250â€“278, 2011.

