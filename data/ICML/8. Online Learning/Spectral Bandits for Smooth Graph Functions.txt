Spectral Bandits for Smooth Graph Functions
Michal Valko
MICHAL . VALKO @ INRIA . FR
INRIA Lille - Nord Europe, SequeL team, 40 avenue Halley 59650, Villeneuve d’Ascq, France
Rémi Munos
REMI . MUNOS @ INRIA . FR
INRIA Lille - Nord Europe, SequeL team, France; Microsoft Research New England, Cambridge, MA, USA
Branislav Kveton
BRANISLAV. KVETON @ TECHNICOLOR . COM
Technicolor Research Center, 735 Emerson St, Palo Alto, CA 94301, USA
Tomáš Kocák
TOMAS . KOCAK @ INRIA . FR
INRIA Lille - Nord Europe, SequeL team, 40 avenue Halley 59650, Villeneuve d’Ascq, France

Abstract
Smooth functions on graphs have wide applications in manifold and semi-supervised learning.
In this paper, we study a bandit problem where
the payoffs of arms are smooth on a graph. This
framework is suitable for solving online learning
problems that involve graphs, such as contentbased recommendation. In this problem, each
item we can recommend is a node and its expected rating is similar to its neighbors. The
goal is to recommend items that have high expected ratings. We aim for the algorithms where
the cumulative regret with respect to the optimal
policy would not scale poorly with the number
of nodes. In particular, we introduce the notion of an effective dimension, which is small in
real-world graphs, and propose two algorithms
for solving our problem that scale linearly and
sublinearly in this dimension. Our experiments
on real-world content recommendation problem
show that a good estimator of user preferences
for thousands of items can be learned from just
tens of nodes evaluations.

1. Introduction
A smooth graph function is a function on a graph that returns similar values on neighboring nodes. This concept
arises frequently in manifold and semi-supervised learning (Zhu, 2008), and reflects the fact that the outcomes on
the neighboring nodes tend to be similar. It is well-known
(Belkin et al., 2006; 2004) that a smooth graph function can
be expressed as a linear combination of the eigenvectors of
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the authors.

0.2

Eigenvector 1

0.2

0

−0.2
−1
0.2

0.2

0

0

1

Eigenvector 4

−0.2
−1
0.2

0

−0.2
−1

Eigenvector 2

0

0

1

Eigenvector 5

0

1

−0.2
−1

−0.2
−1
0.2

0

Eigenvector 7

Eigenvector 3

0

1

Eigenvector 6

0

0

1

−0.2
−1

Eigenvector 8

0

1

Eigenvector 9

0.2 1. Eigenvectors from
0.2
Figure
the Flixster data0.2corresponding to the
smallest few eigenvalues of the graph Laplacian projected onto
0
0
the first
principal component
of data. Colors 0indicate the values.
−0.2
−1

0

1

−0.2
−1

0

1

−0.2
−1

0

1

the graph Laplacian with smallest eigenvalues. Therefore,
the problem of learning such function can be cast as a regression problem on these eigenvectors. This is the first
work that brings this concept to bandits. In particular, we
study a bandit problem where the arms are the nodes of a
graph and the expected payoff of pulling an arm is a smooth
function on this graph.
We are motivated by a range of practical problems that involve graphs. One application is targeted advertisement in
social networks. Here, the graph is a social network and our
goal is to discover a part of the network that is interested in
a given product. Interests of people in a social network
tend to change smoothly (McPherson et al., 2001), because
friends tend to have similar preferences. Therefore, we take
advantage of this structure and formulate this problem as
learning a smooth preference function on a graph.
Another application of our work are recommender systems
(Jannach et al., 2010). In content-based recommendation
(Chau et al., 2011), the user is recommended items that are
similar to the items that the user rated highly in the past.
The assumption is that users prefer similar items similarly.
The similarity of the items can be measured for instance by
a nearest neighbor graph (Billsus et al., 2000), where each

Spectral Bandits

item is a node and its neighbors are the most similar items.
In this paper, we consider the following learning setting.
The graph is known in advance and its edges represent the
similarity of the nodes. At time t, we choose a node and
then observe its payoff. In targeted advertisement, this may
correspond to showing an ad and then observing whether
the person clicked on the ad. In content-based recommendation, this may correspond to recommending an item and
then observing the assigned rating. Based on the payoff,
we update our model of the world and then the game proceeds into time t + 1. Since the number of nodes N can be
huge, we are interested in the regime when t < N .
If the smooth graph function can be expressed as a linear
combination of k eigenvectors of the graph Laplacian, and
k is small and known, our learning problem can be solved
using ordinary linear bandits (Auer, 2002; Li et al., 2010).
In practice, k is problem specific and unknown. Moreover, the number of features k may approach the number
of nodes N . Therefore, proper regularization is necessary,
so that the regret of the learning algorithm does not scale
with N . We are interested in the setting where the regret is
independent of N and therefore this problem is non-trivial.

2. Setting
Let G be the given graph with the set of nodes V and denote
|V| = N the number of nodes. Let W be the N × N matrix of similarities wij (edge weights) and
P D is the N × N
diagonal matrix with the entries dii = j wij . The graph
N
Laplacian of G is defined as L = D − W. Let {λL
k , q k }k=1
be the eigenvalues and eigenvectors of L ordered such that
T
L
L
0 = λL
1 ≤ λ2 ≤ · · · ≤ λN . Equivalently, let L = QΛL Q
be the eigendecomposition of L, where Q is an N × N orthogonal matrix with eigenvectors in columns.
In our setting we assume that the reward function is a linear
combination of the eigenvectors. For any set of weights α
let fα : V → R be the function defined on nodes, linear in
the basis of the eigenvectors of L:
fα (v) =

N
X

αk (qk )v = hxv , αi,

k=1

where xv is the v-th row of Q, i.e., (xv )i = (qi )v . If the
weight coefficients of the true α∗ are such that the large
coefficients correspond to the eigenvectors with the small
eigenvalues and vice versa, then fα∗ would be a smooth
function on G (Belkin et al., 2006). Figure 1 displays first
few eigenvectors of the Laplacian constructed from data
we use in our experiments. In the extreme case, the true
α∗ may be of the form [α1∗ , α2∗ , . . . , αk∗ , 0, 0, 0]TN for some
k  N . Had we known k in such case, the known linear
bandits algorithm would work with the performance scaling with k instead of D = N . Unfortunately, first, we do

not know k and second, we do not want to assume such an
extreme case (i.e., αi∗ = 0 for i > k). Therefore, we opt
for the more plausible assumption that the coefficients with
the high indexes are small. Consequently, we deliver algorithms with the performance that scale with the smoothness
with respect to the graph.
The learning setting is the following. In each time step
t ≤ T , the recommender π chooses a node π(t) and obtains
a noisy reward such that:
rt = hxπ(t) , α∗ i + εt ,
where the noise εt is assumed to be R-sub-Gaussian for
any t. In our setting, we have xv ∈ RD and kxv k2 ≤ 1
for all xv . The goal of the recommender is to minimize the
cumulative regret with respect to the strategy that always
picks the best node w.r.t. α∗ . Let π(t) be the node picked
(referred to as pulling an arm) by an algorithm π at time t.
The cumulative (pseudo) regret of π is defined as:
RT = T max fα∗ (v) −
v

T
X

fα∗ (π(t))

t=1

We call this bandit setting spectral since it is built on the
spectral properties of a graph. Compared to the linear and
multi-arm bandits, the number of arms K is equal to the
number of nodes N and to the dimension of the basis D
(eigenvectors are of dimension N ). However, a regret that
scales with N or D that can be obtained using those settings
is not acceptable because the number of nodes can be large.
While we are mostly interested in the setting with K = N ,
our algorithms and analyses can be applied for any finite K.

3. Related Work
Most related setting to our work is that of the linear and
contextual linear bandits. Auer (2002) proposed
a SupLin√
Rel algorithm and showed that it obtains DT regret that
matches the lower bound by Dani et al. (2008). First practical and empirically successful algorithm was LinUCB
(Li et al., 2010). Later, Chu et al. (2011) analyzed SupLinUCB, which is a LinUCB
√ equivalent of SupLinRel,
to show that it also obtains DT regret. Abbasi-Yadkori
et al. (2011) proposed√the OFUL algorithm for linear bandits which obtains D T regret. Using their
√ analysis, it is
possible to show that LinUCB obtains D T
√ regret as well
(Remark 2). Whether LinUCB matches the DT the lower
bound for this setting is still an open problem.
Abernethy et al. (2008) and Bubeck et al. (2012) studied
a more difficult adversarial setting of linear bandits where
the reward function is time-dependent. It is an open problem if this approaches would work in our setting and have
an upper bound on the regret that scales better than with D.
Kleinberg et al. (2008); Slivkins (2009), and Bubeck et al.
(2011) use similarity information between the context of

Spectral Bandits

arms, assuming a Lipschitz or more general properties.
While such settings are indeed more general, the regret
bounds scale worse with the relevant dimensions. Srinivas
et al. (2010) and Valko et al. (2013) also perform maximization over the smooth functions that are either sampled
from a Gaussian process prior or have a small RKHS norm.
Their setting is also more general than ours since it already
generalizes linear bandits. However their regret bound in
the linear case also scales with D. Moreover, the regret of
these algorithms also depends on a quantity for which dataindependent bounds exists only for some kernels, while our
effective dimension is always computable given the graph.

In general, we assume there exists a diagonal matrix Λ with
the entries 0 < λ = λ1 ≤ λ2 ≤ · · · ≤ λN and a set of K
vectors x1 , . . . , xK ∈ RN such that kxi k2 ≤ 1 for all i.
For the spectral bandits, we have K = N . Moreover, since
Q is an orthonormal matrix, kxi k2 = 1. Finally, since
the first eigenvalue of a graph Laplacian is always zero,
λL
1 = 0, we use Λ = ΛL + λI, in order to have λ1 = λ.
Definition 1. Let the effective dimension d be the largest
d such that:

Another bandit graph setting called the gang of bandits was
studied by Cesa-Bianchi et al. (2013) where each node is a
linear bandit with its own weight vector which are assumed
to be smooth on the graph. Next, Caron et al. (2012) assume that they obtain the reward not only from the selected
node but also from all its neighbors. Yet another kind of
the partial observability model for bandits on graphs in the
adversarial setting is considered by Alon et al. (2013).

The effective dimension d is small when the coefficients λi
grow rapidly above T . This is the case when the dimension of the space D (and K) is much larger than T , such
as in graphs from social networks with very large number
of nodes N . In contrast, when the coefficients are all small
then d may be of the order of T , which would make the
regret bounds useless. Figure 2 shows how d behaves compared to D on the generated and the real Flixster network
graphs1 that we use for the experiments in Section 6.

(d − 1)λd ≤

4. Spectral Bandits

T
log(1 + T /λ)

Flixster graph: N=4546

Barabasi−Albert graph N=500
11

7

10

Given a vector of weights α, we define its Λ norm as:
v
uN
uX
√
(1)
λk αk2 = αT Λα
kαkΛ = t
k=1

We make use of this norm later in our algorithms. Intuitively, we would like to penalize the coefficients α that
correspond to the eigenvectors with the large eigenvalues,
in other words, to the less smooth basis functions on G.
4.1. Effective dimension
In order to present our algorithms and analyses, we introduce a notion of effective dimension d. We keep using capital D to denote the ambient dimension, which is equal to N
in the spectral setting. Intuitively, the effective dimension
is a proxy for the number of relevant dimensions. We first
provide a formal definition and then discuss its properties.

6

9

effective dimenstion

effective dimenstion

In our setting, the arms are orthogonal to each other.
Thinking that the reward observed for an arm does not provide any information for other arms would not be correct
because of the assumption that under another basis, the unknown parameter has a low norm. This provides additional
information across the arms through the estimation of the
parameter. We could think of our setting as an N –armed
bandit problem where N is larger than the time horizon
T and the mean reward µk for each arm k satisfies the
property that under a change of coordinates, the vector has
small weights, i.e., there exists a known orthogonal matrix
U such that α = Uµ has a low norm. As a consequence,
we can estimate α using penalization and then recover µ.

5

4

3

8
7
6
5
4
3

2

2

1

1
0

50

100

150

200

250

time T

300

350

400

450

500

500

1000

1500

2000

2500

3000

3500

4000

4500

time T

Figure 2. Effective dimension d in the regime T < N . The effective dimension for this data is much smaller than the ambient
dimension D = N , which is 500 and 4546 respectively.

The actual form of Definition 1 comes from Lemma 6 and
will become apparent in Section 5. The dependence of the
effective dimension on T comes from the fact, that d is related to the number of “non-negligible” dimensions characterizing the space where the solution to the penalized
least-squares may lie, since this solution is basically constrained to an ellipsoid defined by the inverse of the eigenvalues multiplied by T . Consequently, d is related to the
metric dimension of this ellipsoid. Therefore, when T goes
to infinity, the constraint is relaxed and all directions matter, thus the solution can be anywhere in a (bounded) space
of dimension N . On the contrary, for a smaller T , the ellipsoid possesses a smaller number of “non-negligible” dimensions. Notice that it is natural that this effective dimension depends on T as we consider the setting T < N . If we
wanted to avoid T in the definition of d, we could define it
as well in terms of N by replacing T by N in Definition 1,
but this would only loosen its value.
1
We set Λ to ΛL + λI with λ = 0.01, where ΛL is the graph
Laplacian of the respective graph.

Spectral Bandits

Algorithm 1 S PECTRAL UCB
Input:
N : the number of nodes, T : the number of pulls
{ΛL , Q} spectral basis of L
λ, δ : regularization and confidence parameters
R, C : upper bounds on the noise and kα∗ kΛ
Run:
Λ ← ΛL + λI
d ← max{d : (d − 1)λd ≤ T / log(1 + T /λ)}
for t = 1 to T do
Update the basis coefficients α̂:
Xt ← [x1 , . . . , xt−1 ]T
r ← [r1 , . . . , rt−1 ]T
Vt ← Xt XTt + Λ
α̂t ← p
Vt−1 XTt r
ct ← 2R d log(1 + t/λ) + 2 log(1/δ) + C
Choose the node vt(xvt -th row of Q): 
vt ← arg maxv fα̂ (v) + ct kxv kV−1
t
Observe the reward rt
end for

4.2. S PECTRAL UCB
The first algorithm we present is S PECTRAL UCB (Algorithm 1) which is based on LinUCB and uses the spectral
penalty (1). For clarity, we set xt = xvt = xπ(t) . Here we
consider regularized least-squares estimate α̂t of the form:
!
t
X
2
α̂t = arg min
[hxv , αi − rv ] + kαkΛ
α

v=1

A key part of the algorithm is to define the ct kxkV−1 cont
fidence widths for the prediction of the rewards. We take
advantage of our analysis (Section 5.2) to define ct based
on the effective dimension d which is specifically tailored
to our setting. By doing this we also avoid the computation
of the determinant (see Section 5). The following theorem characterizes the performance of S PECTRAL UCB and
bounds the regret as a function of effective dimension d.
Theorem 1. Let d be the effective dimension and λ be the
minimum eigenvalue of Λ. If kα∗ kΛ ≤ C and for all xv ,
hxv , α∗ i ∈ [−1, 1], then the cumulative regret of S PEC TRAL UCB is with probability at least 1 − δ bounded as:
h p
i
RT ≤ 8R d log(1 + T /λ) + 2 log(1/δ) + 4C + 4
p
× dT log(1 + T /λ)
Remark 1. The constant C needs to be such that kα∗ kΛ ≤
C. If we set C too small, the true α∗ will lie outside
of the region and far from α̂t , causing the algorithm to
underperform. Alternatively, C can be time dependent,
e.g., Ct = log T . In such case, we do not need to know an
upper bound on kα∗ kΛ in advance, but our regret bound
would only hold after some t, when Ct ≥ kα∗ kΛ .

Algorithm 2 S PECTRAL E LIMINATOR
Input:
N : the number of nodes, T : the number of pulls
{ΛL , Q} spectral basis of L
λ : regularization parameter
β, {tj }Jj parameters of the elimination and phases
A1 ← {x1 , . . . , xK }.
for j = 1 to J do
Vtj ← γΛL + λI
for t = tj to min(tj+1 − 1, T ) do
Play xt ∈ Aj with the largest width to observe rt :
xt ← arg maxx∈Aj kxkV−1
t
Vt+1 ← Vt + xt xtT
end for
Eliminate the arms that are not promising:
α̂t ← Vt−1 [xtj h, . . . , xt ][rtj , . . . , rt ]Ti
p ← maxx∈Aj hα̂t , xi − kxkV−1 β
t
o
n
Aj+1 ← x ∈ Aj , hα̂t , xi + kxkV −1 β ≥ p
t
end for

We provide the proof of Theorem 1 in Section 5 and examine the performance
√ of S PECTRAL UCB experimentally in
Section 6. The d T result of Theorem 1 is to be compared
with the classical linear bandits, where LinUCB is the algorithm
often used in practice (Li et al., 2010) achieving
√
D T cumulative regret. As mentioned above and demonstrated in Figure 2, in the T < N regime we can expect
d  D = N and obtain an improved performance.
4.3. S PECTRAL E LIMINATOR
It is known that the available upper bound for LinUCB or
OFUL is not optimal for the linear bandit setting with finite number of arms in terms of dimension D. On the other
hand, the√algorithms SupLinRel or SupLinUCB achieve the
optimal DT regret. In the following, we likewise provide an algorithm that also scales better with d and achieves
√
dT regret. The algorithm is called S PECTRAL E LIMINA TOR (Algorithm 2) and works in phases, eliminating the
arms that are not promising. The phases are defined by the
time indexes t1 = 1 ≤ t2 ≤ . . . and depend on some
parameter β. The algorithm is in a spirit similar to the
Improved UCB by Auer & Ortner (2010). The main idea
of S PECTRAL E LIMINATOR is to divide the time steps in
to sets in order to introduce independence and allow the
Azuma-Hoeffding inequality (Azuma, 1967) to be applied.
In the following theorem we characterize the performance
of S PECTRAL√
E LIMINATOR and show that the upper bound
on regret has d improvement over S PECTRAL UCB.
Theorem 2. Choose the phases starts as tj = 2j−1 . Assume all rewards are in [0, 1] and kα∗ kΛ ≤ C. For any
δ > 0, with probability at least 1 − δ, the cumulative regret

Spectral Bandits

of S PECTRAL
p E LIMINATOR algorithm run with parameter
β = 2R 14 log(2Klog2 T /δ) + C is bounded as:
4
RT ≤
2R
log 2

!s


2K log2 T
T
14 log
dT log 1+
+C
δ
λ

setting (Section 5.2). We also used this result to upperbound the regret of S PECTRAL E LIMINATOR (Section 5.4).
The proofs of the lemmas are in the appendix.

r

4.4. Scalability and computational complexity
There are three main computational issues to address in order to make S PECTRAL UCB scalable: the computation of
N UCBs, matrix inversion, and obtaining the eigenbasis
which serves as an input to the algorithm. First, to speed up
the computation of N UCBs in each time step, we use lazy
updates technique (Desautels et al., 2012) which maintains
a sorted queue of UCBs and in practice leads to substantial
speed gains. Second, to speed up matrix inversion we do
iterative matrix inversion (Zhang, 2005).
Finally, while the eigendecomposition of a general matrix
is computationally difficult, Laplacians are symmetric diagonally dominant (SDD). This enables us to use fast SDD
solvers as CMG by Koutis et al. (2011). Furthermore, using CMG we can find good approximations to the first J
eigenvectors in O(Jm log m) time, where m is the number
of edges in the graph (e.g. m = 10N in the Flixter experiment). CMG can easily work with N in millions. In general, we have J = N but from our experience, a smooth
reward function can be often approximated by dozens of
eigenvectors. In fact, J can be considered as an upper
bound on the number of eigenvectors we actually need.
Furthermore, by choosing small J we not only reduce the
complexity of eigendecomposition but also the complexity
of the least-square problem being solved in each iteration.
Choosing a small J can significantly reduce the computation but it is important to choose J large enough so that still
less than J eigenvectors are enough. This way, the problem that we solve is still relevant and our analysis applies.
In short, the problem cannot be solved trivially by choosing first k relevant eigenvectors because k is unknown.
Therefore, in practice we choose the largest J such that
our method is able to run. In Section 6.4, we demonstrate
that we can obtain good results with relatively small J.

5. Analysis
The analysis of S PECTRAL UCB (Section 5.3) has two
main ingredients. The first one is the derivation of the confidence ellipsoid for α̂, which is a straightforward update of
the analysis of OFUL (Abbasi-Yadkori et al., 2011) using
self-normalized martingale inequality (Section 5.1). The
second part is crucial to prove that the final regret bound
scales only with the effective dimension d and not with the
ambient dimension D. We achieve this by considering the
geometrical properties of the determinant which hold in our

5.1. Confidence ellipsoid
The first two lemmas are by Abbasi-Yadkori et al. (2011)
and we restate them for the convenience.
Lemma 1. Let Vt = Xt−1 XTt−1 + Λ and define ξ t =
Pt
s=1 εs xs . With probability at least 1 − δ, ∀t ≥ 1:


|Vt |1/2
2
2
kξ t kV−1 ≤2R log
t
δ|Λ|1/2
Lemma 2. We have:
t


X
|Vt |
min 1, kxs k2V−1 ≤ 2 log
s−1
|Λ|
s=1
The next lemma is a generalization of Theorem 2
by Abbasi-Yadkori et al. (2011) to the regularization
with Λ. The result of this lemma is also used for the confidence coefficient ct in Algorithm 1, which we upperbound
in Section 5.2 to avoid the computation of determinants.
Lemma 3. Let Vt = Xt−1 XTt−1 + Λ and kα∗ kΛ ≤ C.
For any x and t ≥ 1, with probability at least 1 − δ:
s
!


|Vt |1/2
T
∗
+C
|x (α̂t − α )| ≤ kxkV−1 R 2 log
t
δ|Λ|1/2
5.2. Effective dimension
In Section 5.1 we show that several quantities scale with
log(|VT |/|Λ|), which can be of the order of D. Therefore,
in this part we present the key ingredient of our analysis
based on the geometrical properties of determinants (Lemmas 4 and 5) to upperbound log(|VT |/|Λ|) by a term that
scales with d (Lemma 6). Not only this will allow us to
show that the regret scales with d, but it also helps us to
avoid the computation of the determinants in Algorithm 1.
Lemma 4. Let Λ = diag(λ1 , . . . , λN ) be any diagonal
matrix with strictly positive entries and for any vectors
(xt )1≤t≤T such that kxt k2 ≤ 1 for all 1 ≤ t ≤ T , we
PT
have that the determinant |VT | of VT = Λ + t=1 xt xtT
is maximized when all xt are aligned with the axes.
PT
Lemma 5. For any T , let VT = t=1 xt xtT + Λ. Then:
log

N

X
|VT |
ti 
≤ max
log 1 +
,
|Λ|
λi
i=1

where the maximum is taken overP
all possible positive real
N
numbers {t1 , . . . , tN }, such that i=1 ti = T .
Lemma 6. Let d be the effective dimension. Then:


|VT |
T
log
≤ 2d log 1 +
|Λ|
λ

Spectral Bandits

5.3. Cumulative regret of S PECTRAL UCB
Proof of Theorem 1. Let x∗ = arg maxxv xvT α∗ and let r̄t
denote the instantaneous regret at time t. With probability
at least 1 − δ, for all t:
r̄t = x∗T α∗ − xtT α∗
≤ xtT α̂t + ct kxt kV−1 − xtT α∗

(2)

t

T

T

≤ xt α̂t + ct kxt kV−1 − xt α̂t + ct kxt kV−1
t

t

(3)

Denote by Fj the σ-algebra generated by the rewards
r1 , . . . , rtj+1 −1 , i.e., received before and during the phase
j. We have the following three lemmas for any phase j. Let
us write Vj for Vtj and α̂j for α̂tj .
N
Lemma 7. For any fixed
p x ∈ R and any∗ δ > 0, we have
∗
that if β(α , δ) = 2R 14 log(2/δ) + kα kΛ , then at time
tj (beginning of phase j):


P |xT (α̂j − α∗ )| ≤ kxkV−1 β(α∗ , δ) ≥ 1 − δ
j

Lemma 8. For all x ∈ Aj , we have:

= 2ct kxt kV−1 .
t

The inequality (2) is by the algorithm design and reflects
the optimistic principle of S PECTRAL UCB. Specifically,
x∗T α∗ + ct kx∗ kV−1 ≤ xtT α̂t + ct kxt kV−1 , from which:
t

t

kxk2V−1
j

tj
X

kxs k2V−1

s−1

s=tj−1 +1

Lemma 9. For each phase j, we have:
tj
X

x∗T α∗ ≤ xtT α̂t + ct kxt kV−1 − ct kx∗ kV−1
t

1
≤
tj − tj−1

t

≤ xtT α̂t + ct kxt kV−1

s=tj−1



|Vj |
min 1, kxs k2V−1 ≤ log
s−1
|Λ|
+1

t

In (3) we applied Lemma 3: xtT α̂t ≤ xtT α∗ + ct kxt kV−1 .
t
Finally, by Lemmas 2 and 6 and the assumption r̄t ≤ 2:
v
u T
T
u X
X
r̄t2
r̄t ≤ tT
RT =
t=1

t=1

v
u T


u X
min 1, kxt k2V−1
≤ 2 max(1, cT )tT
t=1

≤ 2(cT + 1)

p

2T log(|VT |/|Λ|)

≤ 4(cT + 1)

p

dT log(1 + T /λ)

t

Combining all the lemmas above together we obtain:
Lemma 10. For each j, x, and δ, with probability at least
1 − δ:


min 1,|xT (α̂j − α∗ )|
s
s


1
tj − tj−1
∗
2d log 1 +
≤ β(α , δ)
tj − tj−1
λ
Now we are ready to upperbound the cumulative regret of
S PECTRAL E LIMINATOR.
Proof of Theorem 2. Let J = blog2 T c. We have:
RT =

Above, we used that ct ≤ cT because ct is nondecreasing.
By plugging cT , we get that with probability at least 1 − δ:
h p
i
RT ≤ 4 R 2 log(|VT |/|Λ|) + 2 log(1/δ) + C + 1
p
× dT log(1 + T /λ)
h p
i
≤ 8R d log(1 + T /λ) + 2 log(1/δ) + 4C + 4
p
× dT log(1 + T /λ)
Remark 2. Notice that if we set Λ = I in Algorithm 1,
we recover LinUCB. Since log(|VT |/|Λ|) can be upperbounded by
√ D log T (Abbasi-Yadkori et al., 2011), we obtain Õ(D T ) upper bound of regret of LinUCB as a corollary of Theorem 1.
5.4. Cumulative regret of S PECTRAL E LIMINATOR
The probability space induced by the rewards r1 , r2 , . . .
can be decomposed as a product of independent probability spaces induces by rewards in each phase [tj , tj+1 − 1].

n
X

hx∗ − xt , α∗ i =

≤

hx∗ − xt , α∗ i =

j=0 t=tj

t=1
J
X

J tj+1
X
X−1


(tj+1 − tj ) hx∗ − xt , α̂j i

j=0


+ (kx∗ kV−1 + kxt kV−1 )β ,
j

j

in an event Ω of probability 1 − KJδ, where we used
Lemma 7 in the last inequality. By definition of the action
subset Aj at phase j, under Ω, we have:
hx∗ − xt , α̂j i ≤ (kx∗ kV−1 + kxt kV−1 )β,
j

j

since x∗ ∈ Aj for all j ≤ J. By previous lemma:
s
J
X
1
∗
RT ≤ 2
(tj+1 − tj )β(α , δ)
tj − tj−1
j=0
s


tj − tj−1
× 2d log 1 +
λ
s


4
T
≤
β dT log 1 +
log 2
λ

Spectral Bandits
Erdos−Renyi N=500, param=5, d=1

Barabasi−Albert N=500, param=5, d=1

Lattice N=625, param=5, d=1

160

140

SpectralUCB
SpectralEliminator
LinUCB

120

15

SpectralUCB
SpectralEliminator
LinUCB

140

SpectralUCB
SpectralEliminator
LinUCB

120

80

60

cumulative regret

cumulative regret

cumulative regret

100
100

80

60

10

5

40
40
20

0

20

0

50

100

150

200

250

0

0

50

100

150

200

250

0

0

50

time T

time T

100

150

200

250

time T

Figure 3. Cumulative regret for random graphs models

Remark 3. If we set Λ = I in Algorithm 2 as in Remark 2,
we get a new algorithm, L INEAR E LIMINATOR, which is a
competitor to SupLinRel (Auer,
√ 2002) and as a corollary to
Theorem 2 also enjoys Õ( DT ) upper bound on the cumulative regret. On the other hand, compared to SupLinRel, L INEAR E LIMINATOR and its analysis are significantly
much simpler and elegant.

6. Experiments
We evaluated our algorithms and compared them to LinUCB. In all experiments we set δ to 0.001 and R to 0.01.
For S PECTRAL UCB and S PECTRAL E LIMINATOR we set
Λ to ΛL + λI with λ = 0.01. For LinUCB we regularized with λI with λ = 1. Our results are robust to
small perturbations of all learning parameters. We also performed experiments with SupLinRel, SupLinUCB, SupSpectralUCB2 , but due to the known reasons (Chu et al.,
2011) these algorithms are not efficient3 and they were always outperformed by S PECTRAL UCB and LinUCB.
6.1. Random graph models
To simulate realistic graph structures, we generated graphs
of N nodes using three models that are commonly used
in the social networks modeling. First, we considered the
widely known Erdős-Rényi (ER) model. We sampled the
edges in the ER model independently with probability 3%.
Second, we considered the Barabási-Albert (BA) model
(1999), with the degree parameter 3. BA models are commonly used for modeling real networks due to their preferential attachment property. Finally, we considered graphs
where the edge structure forms a regular lattice.
For all the graph models we assigned uniformly random
weights to their edges. Then, we randomly generated ksparse vector α∗ of N weights, k  N and defined the
true graph function as f = Qα∗ , where Q is the matrix
of eigenvectors from the eigendecomposition of the graph
Laplacian. We ran the algorithms in the desired T < N
regime, with N = 500 (N = 54 for the lattice), T = 250,
2

an equivalent of SupLinUCB with spectral regularization
3
at least for the sizes of N and T that we deal with

and k = 5. Figure 3 shows that the regret of LinUCB for
all three models has within first T steps still a linear trend
unlike S PECTRAL UCB that performs much better.
Unfortunately, even though the regret bound Spectral Eliminator is asymptotically better, it was outperformed by
S PECTRAL UCB. This is similar to the linear case when
LinUCB outperforms4 SupLinUCB in practice (Chu et al.,
2011) while it is open problem whether LinUCB can be
shown to have a better regret.
6.2. MovieLens experiments
In this experiment we took user preferences and the similarity graph over movies from the MovieLens dataset (Lam &
Herlocker, 2012), a dataset of 6k users who rated one million movies. We divide the dataset into two equally-sized
parts. The first dataset is used to build our model of users,
the rating that user i assigns to movie j. We factor the
user-item matrix using low-rank matrix factorization (Keshavan et al., 2009) as M ≈ UV0 , a standard approach to
collaborative filtering. The rating that the user i assigns to
movie j is estimated as r̂i,j = hui , vj i, where ui is the
i-th row of U and vj is the j-th row of V. The rating r̂i,j
is the payoff of pulling arm j when recommending to user
i. The second dataset is used to build our similarity graph
over movies. We factor the dataset in the same way as the
first dataset. The graph contains an edge between movies i
and i0 if the movie i0 is among 10 nearest neighbors of the
movie i in the latent space of items V. The weight on all
edges is one. Notice that if two items are close in the item
space, then their expected rating is expected to be similar.
However, the opposite is not true. If two items have a similar expected rating, they do not have to be close in the item
space. In other words, we take advantage of ratings but do
not hardwire the two similarly rated items to be similar.
In Figure 4, we sampled 50 users and evaluated the regret of
both algorithms for T = 100. Here S PECTRAL UCB suffers only about one fourth of regret over LinUCB, specifically 43.4611 vs. 133.0996 on average.
4
For example, these algorithms do not to use all the rewards
obtained for the estimation of α̂.

Spectral Bandits
Movielens: Cumulative regret for randomly sampled users. T = 100

Flixster: Cumulative regret for randomly sampled users. T = 100

50

0

150
100
50
0

7
216
135
7
588
211
2
148
7
38
253
2
183
3
212
6
711
186
3
115
9
135
3
192
989
322
132
161
7
156
165
4
337
142
0

100

SpectralUCB
LinUCB

200

136
0
137
0
236
2
162
569
239
1
52
245
8
74
159
5
969
253
3
196
0
258
9
246
0
1
814
224
9
878
967
994
202
9
184
3
162
0
168
248
8
645

150

cumulative regret

250

SpectralUCB
LinUCB

200

243
4
168
9
233
5
970
659
920
172
2
244
5
856
118
6
253
9
181
1
292
0
600
243
7
199
1
734
140
2
117
6
176
6
235
8
310
242
0
247
7
104
5
126
7
168
6
206
4
218
7
223
2
114
6
126
5
281
7
168
8
250
3
814
183
3
173
3
283
8
253
147
4
153
7
266
266
5
260
5
129
3
230
3
438
182
6
768

cumulative regret

250

Figure 4. MovieLens and Flixter: Cumulative regret for 50 randomly chosen users. Horizontal axis shows the user number.

4

150

10

computational time in seconds

cumulative regret

J = 20
J = 200
J = 2000

100

50

0

0

10

20

30

40

50

60

70

80

90

time T

100

functions (N = 2019). The total computational time also
includes the computational savings from lazy updates and
iterative matrix inversion. We see that with 10% of the
eigenvectors we can achieve similar performance as for the
full set for the fraction of the computational time.

3

10

2

10

1

10

0

10

J = 20

J=200

J=2000

Figure 5. Regret and computational time with reduced basis

6.3. Flixter Experiments
We also performed experiments on users preferences from
the movie recommendation website Flixter. The social network of the users was crawled by Jamali & Ester (2010)
and then clustered by Graclus (2013) to obtain a strongly
connected subgraph. We extracted a subset of users and
movies, where each movie has at least 30 ratings and each
user rated at least 30 movies. This resulted in a dataset of
4546 movies and 5202 users. As with MovieLens dataset
we completed the missing ratings by a low-rank matrix factorization and used it construct a 10-NN similarity graph.
Again in Figure 4, we sampled 50 users and evaluated the
regret of both algorithms for T = 100. On average, S PEC TRAL UCB suffers only about one third of regret over LinUCB, specifically 37.6499 vs. 99.8730 on average.

7. Conclusion
We presented spectral bandit setting inspired mostly by the
applications in the recommender systems and targeted advertisement in social networks. In this setting, we are asked
to repeatedly maximize an unknown graph function, assumed to be smooth on a given similarity graph. Traditional linear bandit algorithm can be applied but their regret
scales with the ambient dimension D, either linearly or as
a square root, which can be very large.
Therefore, we introduced two algorithms, S PECTRAL UCB
and S PECTRAL E LIMINATOR, for which the regret only
scales with effective dimension d which is typically much
smaller than D for real-world graphs. We demonstrated
that S PECTRAL UCB delivers desired benefit for the graphs
generated by Barabási–Albert, Erdős-Rényi, and regular
lattice models; and for the movie rating data from the
MovieLens and Flixster social networks. In future, we plan
to extend this work to a sparse setting when the smooth
function is assumed to be a linear combination of only finite number of eigenvectors.

6.4. Reduced basis
As discussed in Section 4.4, one can decrease the computational complexity and thus increase the scalability by only
extracting first J  N eigenvectors of the graph Laplacian. First, the computational complexity of such operation
is O(Jm log m), where m is the number of edges. Second,
the least-squares problem that we have to do in each time
step of the algorithm is only J dimensional.
In Figure 5 we plot the cumulative regret and the total computational time in seconds (log scale) for a single user from
the MovieLens dataset. We varied J as 20, 200, and 2000
which corresponds to about 1%, 10% and 100% of basis

8. Acknowledgements
We would like to thank Yiannis Koutis for his great help
with the efficient computation of eigenvectors. We thank
Andreas Krause for suggesting the lazy updates of UCBs.
We would also like to thank Giovanni Zappella, Claudio Gentile, and especially Alessandro Lazaric for helpful discussions. The research presented in this paper
was supported by French Ministry of Higher Education
and Research, by European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement
no 270327 (project CompLACS), and by Intel Corporation.

Spectral Bandits

References
Abbasi-Yadkori, Y, Pál, D, and Szepesvári, C. Improved
Algorithms for Linear Stochastic Bandits. In Neural Information Processing Systems. 2011.
Abernethy, J. D, Hazan, E, and Rakhlin, A. Competing
in the Dark: An Efficient Algorithm for Bandit Linear
Optimization. In Conference on Learning Theory, 2008.
Alon, N, Cesa-Bianchi, N, Gentile, C, and Mansour, Y.
From Bandits to Experts: A Tale of Domination and Independence. In NIPS, 2013.
Auer, P.
Using confidence bounds for exploitationexploration trade-offs. Journal of Machine Learning Research, 3:397–422, March 2002. ISSN 1532-4435.
Auer, P and Ortner, R. UCB Revisited: Improved Regret
Bounds for the Stochastic Multi-Armed Bandit Problem.
Periodica Mathematica Hungarica, 2010.
Azuma, K. Weighted sums of certain dependent random
variables. Tohoku Mathematical Journal, 19(3):357–
367, 1967.
Barabási, A.-L and Albert, R. Emergence of scaling in random networks. Science, 286:11, 1999.
Belkin, M, Matveeva, I, and Niyogi, P. Regularization and
Semi-Supervised Learning on Large Graphs. In Conference on Learning Theory, 2004.
Belkin, M, Niyogi, P, and Sindhwani, V. Manifold Regularization: A Geometric Framework for Learning from
Labeled and Unlabeled Examples. Journal of Machine
Learning Research, 7:2399–2434, 2006.
Billsus, D, Pazzani, M. J, and Chen, J. A learning agent for
wireless news access. In IUI, pp. 33–36, 2000.
Bubeck, S, Munos, R, Stoltz, G, and Szepesvari, C. Xarmed bandits. Journal of Machine Learning Research,
12:1587–1627, 2011.
Bubeck, S, Cesa-Bianchi, N, and Kakade, S. Towards minimax policies for online linear optimization with bandit
feedback. In COLT, 2012.
Caron, S, Kveton, B, Lelarge, M, and Bhagat, S. Leveraging Side Observations in Stochastic Bandits. In Uncertainty in Artificial Intelligence, pp. 142–151, 2012.
Cesa-Bianchi, N, Gentile, C, and Zappella, G. A Gang of
Bandits. In NIPS, 2013.
Chau, D. H, Kittur, A, Hong, J. I, and Faloutsos, C. Apolo:
making sense of large network data by combining rich
user interaction and machine learning. In CHI, 2011.
Chu, L, Li, L, Reyzin, L, and Schapire, R. Contextual Bandits with Linear Payoff Functions. In AISTATS, 2011.

Dani, V, Hayes, T. P, and Kakade, S. M. Stochastic Linear Optimization under Bandit Feedback. In The 21st
Annual Conference on Learning Theory, 2008.
Desautels, T, Krause, A, and Burdick, J. Parallelizing
Exploration-Exploitation Tradeoffs with Gaussian Process Bandit Optimization. In ICML, 2012.
Graclus. Graclus, 2013. URL www.cs.utexas.edu/users/
dml/Software/graclus.html.
Jamali, M and Ester, M. A matrix factorization technique
with trust propagation for recommendation in social networks. In Proceedings of the fourth ACM conference on
Recommender systems. ACM, 2010.
Jannach, D, Zanker, M, Felfernig, A, and Friedrich, G. Recommender Systems: An Introduction. Cambridge University Press, 2010.
Keshavan, R, Oh, S, and Montanari, A. Matrix Completion
from a Few Entries. In IEEE International Symposium
on Information Theory, pp. 324–328, 2009.
Kleinberg, R, Slivkins, A, and Upfal, E. Multi-armed bandit problems in metric spaces. In 40th ACM symposium
on Theory Of Computing, 2008.
Koutis, I, Miller, G. L, and Tolliver, D. Combinatorial preconditioners and multilevel solvers for problems in computer vision and image processing. Computer Vision and
Image Understanding, 115:1638–1646, 2011.
Lam, S and Herlocker, J.
MovieLens 1M Dataset.
http://www.grouplens.org/node/12, 2012.
Li, L, Chu, W, Langford, J, and Schapire, R. E. A
Contextual-Bandit Approach to Personalized News Article Recommendation. WWW 10, 2010.
McPherson, M, Smith-Lovin, L, and Cook, J. Birds of a
Feather: Homophily in Social Networks. Annual Review
of Sociology, 27:415–444, 2001.
Slivkins, A. Contextual Bandits with Similarity Information. Proceedings of the 24th annual Conference On
Learning Theory, pp. 1–27, 2009.
Srinivas, N, Krause, A, Kakade, S, and Seeger, M. Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. Proceedings of International Conference on Machine Learning, 2010.
Valko, M, Korda, N, Munos, R, Flaounas, I, and Cristianini, N. Finite-Time Analysis of Kernelised Contextual
Bandits. In Uncertainty in Artificial Intelligence, 2013.
Zhang, F. The Schur complement and its applications, volume 4. Springer, 2005.
Zhu, X. Semi-Supervised Learning Literature Survey.
Technical Report 1530, U. of Wisconsin-Madison, 2008.

