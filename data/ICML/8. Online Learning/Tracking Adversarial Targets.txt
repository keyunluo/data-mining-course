Tracking Adversarial Targets

Yasin Abbasi-Yadkori
Queensland University of Technology

YASIN . ABBASIYADKORI @ QUT. EDU . AU

Peter Bartlett
University of California, Berkeley and QUT

BARTLETT @ EECS . BERKELEY. EDU

Varun Kanade
University of California, Berkeley

VKANADE @ EECS . BERKELEY. EDU

Abstract
We study linear control problems with quadratic
losses and adversarially chosen tracking targets.
We present an efficient algorithm for this problem and show that, under standard conditions on
the linear system, its regret with respect to an optimal linear policy grows as O(log2 T ), where T
is the number of rounds of the game. We also
study a problem with adversarially chosen transition dynamics; we present an exponentiallyweighted average algorithm for this problem,
and
âˆš
we give regret bounds that grow as O( T ).

1. Introduction
Consider a robot that controls an electron microscope to
track a microorganism. Given the entire trajectory of the
microorganism and the dynamics of the system, the optimal control can be computed. The trajectory, however, is
not known in advance and the target might behave in an arbitrary fashion. In such situations, designing a controller
based on some prior knowledge about the target location
might be sub-optimal. It is important to take the behavior
of the target into account.
We consider problems with linear transition functions and
quadratic tracking losses. When the target trajectory
is known in advance, the problem is called the linear
quadratic (LQ) problem in the control community. The LQ
problem is one of the most studied problems in the control literature and is widely applied in practice (Lai and
Wei, 1982; 1987; Chen and Guo, 1987; Chen and Zhang,
1990; Fiechter, 1997; Lai and Ying, 2006; Campi and KuProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

mar, 1998; Bittanti and Campi, 2006; Abbasi-Yadkori and
SzepesvaÌri, 2011). By principles of dynamic programming
the optimal controller can be computed analytically. The
solution is obtained by computing value functions, starting
from the last round and recursing backward. This method
needs to know the entire target sequence from the beginning and its computational complexity scales linearly with
the length of the trajectory. It turns out that the optimal controller is linear in the state vector, and the value functions
are quadratic in state and action.
As we discussed earlier, the assumption that the entire trajectory is known in advance is not always realistic. But
what would tracking mean without a reference trajectory?
To make the problem well-defined, we fix a class of mappings from states to actions (also known as policies) as our
competitors. Our objective is to track the trajectory nearly
as well as the best policy in the comparison class in hindsight. The standard dynamic programming procedures are
not applicable when the entire sequence is not known in
advance. We show that we can still have an effective tracking algorithm even if the sequence is not known in advance.
The proposed algorithm is perhaps the first tracking method
that can deal with infinite and unknown sequences.
We study the adversarial version of the LQ problem where
an adversary designs the trajectory of the target and reveals
the target location only at the end of each round. Formally,
we study problems with transition dynamics and loss functions
xt+1 = Axt + Bat ,
`t (xt , at ) = (xt âˆ’ gt )> Q(xt âˆ’ gt ) + a>
t at ,
where xt âˆˆ Rn is the state at round t; at âˆˆ Rd is the action;
gt is the target location; `t : Rn Ã— Rd â†’ R is the loss
function; and A, B, Q are known matrices.1 The matrix Q
is symmetric and positive definite. The learner observes the
1

We can also use this formulation for loss functions of the

Tracking Adversarial Targets

sequence of states (xt )t . We make no assumptions on the
trajectory sequence (gt )t , apart from its boundedness. The
sequence might even be generated by an adversary. An LQ
problem is defined by a 4-tuple (A, B, Q, G), where G is
an upper bound on the norm of vectors gt .
Let T be a time horizon, xÏ€t be the state of the system if we
run policy Ï€ for t rounds, and Î  be a class of policies. Let
ÏT (g1 , . . . , gT ) =

T
X

`t (xt , at )âˆ’min
Ï€âˆˆÎ 

t=1

T
X

`t (xÏ€t , Ï€(xÏ€t )) .

t=1

The objective of the learner is to suffer low loss. The performance is measured by the regret defined by
RT (A, B, Q, G, x1 , Î ) =

sup

ÏT (g1 , . . . , gT ) .

g1 ,...,gT :kgt kâ‰¤G

where k Â· k denotes the 2-norm. In what follows, we use
def
RT = RT (A, B, Q, G, x1 , Î ). For simplicity, we assume
that x1 = 0. As the optimal policy for the classical problem
with a constant target is linear (strictly speaking, affine), a
reasonable choice for the class of competitors is the set of
linear (affine) policies.
The problem that we describe is an instance of Markov Decision Process (MDP) problems with fixed and known dynamics, and changing loss functions. Note that the loss
function depends on the target location, which can change
in an arbitrary fashion. Such MDP problems were previously studied by Even-Dar et al. (2009) and Neu et al.
(2010b;a). However, these papers provide results for finite MDP problems and are not applicable to problems
with large state/action spaces. Our key finding is that the
algorithm of Even-Dar et al. (2009) can be modified to
be applicable in adversarial LQ problems. Interestingly,
the resultant algorithm is identical with a Policy Iteration
method (see, for example, (Howard, 1960)) with changing
loss functions. Another interesting observation is that the
gain matrix is independent of target vectors (see Lemma 4).
This simplifies the design and analysis of our algorithm.
We prove that the regret of the algorithm is logarithmic in
the number of rounds of the game.
Finally, we also study a more adversarial problem:
xt+1 = At xt + Bt at ,
`t (xt ) = (xt âˆ’ gt )> Q(xt âˆ’ gt ) + a>
t at .
Here, the time-varying transition matrices At and Bt and
the target vector gt are chosen by an adversary. In this problem, we show that under a uniform stability assumption,
form `t (xt , at ) = (xt âˆ’ gt )> Q(xt âˆ’ gt ) + a>
t Rat for a positive
definite matrix R, by writing xt+1 = Axt +(BRâˆ’1/2 )R1/2 at =
e at and `t (xt , at ) = (xt âˆ’ gt )> Q(xt âˆ’ gt ) +
Axt + Be
> 1/2 1/2
at R R at = (xt âˆ’ gt )> Q(xt âˆ’ gt ) + e
a>
at .
t e

an exponentially-weighted average algorithm recentlyâˆš
proposed by Abbasi-Yadkori et al. (2013) enjoys an O( T )
regret bound with respect to the class of linear policies.

2. Notation
We use Ïƒmin (M ) and Ïƒmax (M ) to denote the minimum
and maximum eigenvalues of the positive semidefinite matrix M , respectively. We use k Â· k to denote the 2-norm of
matrices and vectors,pwhere the 2-norm of a matrix M is
defined by kM k = Ïƒmax (M > M ). We use M  0 to
denote that M is positive definite, while we use M  0 to
denote that it is positive semidefinite. We use Mij to denote
a block of matrix M . The indices and the dimensionality
of the block will be understood from the context. Similarly,
vi denotes a block of vector v.

3. Tracking Adversarial Targets
Even-Dar et al. (2009) study finite state MDP problems
with fixed and known transition functions and adversarial loss functions. Their algorithm (MDP-E) in its present
form is not applicable to our problem with a continuous
state space. Somewhat surprisingly, we can design a variant
of the MDP-E algorithm that is applicable to our tracking
problem with continuous state and action spaces.
The MDP-E algorithm, shown in Figure 1, maintains an
expert algorithm in each state; that is, it treats each state
as a separate problem of prediction with expert advice,
where each action corresponds to an expert. At round t, the
product of expert recommendations over the state space defines the policy, denoted by Ï€t . The algorithm takes action
at âˆ¼ Ï€t (xt ) and observes the loss function `t . It computes
the value function VÏ€t ,`t defined by the Bellman Optimality
Equation
âˆƒÎ», âˆ€x, a,

Î» + VÏ€t ,`t (x, a) = `t (x, a)
+ Ex0 âˆ¼m(.|x,a) [VÏ€t ,`t (x0 , Ï€t (x0 ))] ,

where m defines the state transition probabilities.2 Then,
the algorithm feeds the expert algorithm in state x with
Vt (x, .) = VÏ€t ,`t (x, .) as the loss function at time t. Thus,
the computational cost of the MDP-E algorithm per round
is O(W + |X |), where W is the cost of obtaining the value
function and X is the finite state space.
Applied to the LQ problem, the value functions are defined
2

In the Bellman Optimality Equation, scalar Î» is the average loss of policy Ï€t , and VÏ€t ,`t (x, a) is the relative goodness
of state-action pair (x, a) under policy Ï€t . Note that under certain assumptions, the average loss is independent of the initial
state, however, some states are more favorable as the policy incurs lower losses during the transient phase, starting from those
states.

Tracking Adversarial Targets

Initialize an expert algorithm in each state
for t := 1, 2, . . . do
Let Ï€t (xt ) be the prediction of the expert algorithm
in state xt
Take action at âˆ¼ Ï€t (xt )
Observe loss function `t
Compute Vt = VÏ€t ,`t
For all x, feed the expert algorithm in state x with
loss Vt (x, .)
end for

x1 = 0
âˆ€x, Ï€1 (x) = âˆ’Kâˆ— x (6)
for t := 1, 2, . . . do
Take action at = Ï€t (xt ) and suffer the loss
`t (xt , at )
Move to the state xt+1 = Axt + Bat
Compute the value function Vt = VÏ€t ,`t (2)
Pt
Let V 0 = s=1 Vs
Obtain the policy by solving âˆ‡a V 0 (x, a) = 0:
Ï€t+1 (x) = âˆ’Kt+1 x + ct+1
end for

Figure 1. The MDP-E Algorithm
Figure 2. FTL-MDP: The Follow the Leader Algorithm for
Markov Decision Processes

by
âˆƒÎ», âˆ€x, a,

Î» + Vt (x, a) = `t (x, a)

(1)

+ Vt (Ax + Ba, Ï€t (Ax + Ba)) ,
where we use the notation Vt = VÏ€t ,`t . The linear structure allows us to compute Vt implicitly for all states, thus
overcoming the difficulty of the infinite state space. As we
will show, a suitable expert algorithm for our problem is the
Follow The Leader (FTL) algorithm that we define next.
Consider an online quadratic optimization problem where
at round t the adversary chooses a quadratic loss function ft
that is defined over a convex set D âˆˆ Rk . Simultaneously,
the learner makes a prediction pt âˆˆ D, suffers loss ft (pt )
and observes the loss P
function ft . The regret ofPthe learner
T
T
is defined by BT = t=1 ft (pt ) âˆ’ minpâˆˆD t=1 ft (p).
The FTL algorithm makes the prediction
pt = argmin
pâˆˆD

tâˆ’1
X

fs (p) .

s=1

The FTL algorithm enjoys the following regret bound for
quadratic losses (Cesa-Bianchi and Lugosi, 2006, Theorem
3.1):
Theorem 1 (FTL Regret bound). Assume ft is convex,
maps to [0, C1 ], is Lipschitz with constant C2 , and is twice
differentiable everywhere with Hessian H  C3 I. Then
the regret of the Follow The Leader algorithm is bounded
4C C 2
by BT â‰¤ C13 2 (1 + log T ).
Figure 2 shows the FTL-MDP algorithm for the linear
quadratic tracking problem. It corresponds to the MDP-E
algorithm, with FTL as the expert algorithm for each state.
The algorithm starts at state x1 = 0 and the first policy is
chosen to be Ï€1 (x) = âˆ’Kâˆ— x, where Kâˆ— is a gain matrix
that will be defined later.3 The algorithm computes
Ptâˆ’1 the total loss in each state, shown by V 0 (x, .) = s=1 Vs (x, .).
3
Adopting a convention from feedback control, we represent
linear policies with a negative sign.

The FTL strategy chooses the greedy action in each state,
which is obtained by minimizing V 0 (x, .). As the next
lemma shows, value functions computed in the FTL-MDP
algorithm are always quadratic and thus, the function V 0
is always quadratic in state and action. This implies that
policies are linear in state.
Lemma 2. Consider the MDP-E algorithm applied to the
adversarial LQ problem (A, B, Q, G). Let the expert algorithm be the FTL strategy. Assume the first policy is chosen
to be an arbitrary linear policy, Ï€1 (x) = âˆ’K1 x+c1 . Then,
for appropriate matrices Pt and Lt , the value function at
time t has the form of
 
 

x
x
Vt (x, a) = x> a> Pt
+ L>
,
t
a
a
and the policy chosen by the algorithm at time t is Ï€t (x) =
Ptâˆ’1
Ptâˆ’1
âˆ’Kt x + ct , where Kt = ( s=1 Ps,22 )âˆ’1 s=1 Ps,21 and
Ptâˆ’1
P
tâˆ’1
ct = âˆ’( s=1 Ps,22 )âˆ’1 s=1 Ls,2 /2 and Ps,ij and Ls,i
are the ijth and ith blocks of matrix Ps and vector Ls ,
respectively. (Here the block structure naturally appears
as components corresponding to the state and action.)
The proof uses the following lemma that shows that the
value of a linear policy is quadratic.
Lemma 3. Consider the LQ problem (A, B, Q, G) with
fixed target gâˆ— . Let K be a matrix such that kA âˆ’ BKk <
1. The value function of policy Ï€(x) = âˆ’Kx + c has a
quadratic form
 
 

x
> x
>
>
a P
VÏ€,` (x, a) = x
+L
, (2)
a
a
where P = P (K) and L are solutions to equations
 >






A
I
Q 0
>
A B +
I âˆ’K P
P =
âˆ’K
0 I
B>
(3)

Tracking Adversarial Targets

Lemma 2 implies that the MDP-E algorithm can be efficiently implemented in the adversarial LQ problem.

and
L> = L> + 2 0

 
c> P



I
âˆ’K


A



B âˆ’ 2gâˆ—> Q 0 .
(4)

Further, the loss in the limiting state xÏ€âˆ is
Î» = gâˆ—> Qgâˆ— + c> P22 c + L>
2c.

(5)

The proof can be found in Appendix A.
Proof of Lemma 2. We prove the lemma by induction. By
Lemma 3, the value of the first policy, Ï€1 (x) = âˆ’K1 x+c1 ,
has the form of
 
 

x
x
V1 = VÏ€1 ,`1 (x, a) = x> a> P1
+ L>
,
1
a
a
for some matrices P1 and L1 . This establishes the base
case.
Next assume the value functions up to time t âˆ’ 1 are all
quadratic. Because we use a FTL strategy as our expert
algorithm in states, the policy in state x in round t can be
computed by
Ï€t (x) = argmin
a

tâˆ’1
X

Vs (x, .) .

s=1

Ptâˆ’1
We obtain the policy by setting âˆ‡a s=1 Vs (x, .) = 0.
Letting
 
 

x
> x
>
>
a Ps
Vs (x, a) = x
+ Ls
a
a
for s = 1 . . . (t âˆ’ 1), we get that
V 0 (x, a) =

tâˆ’1
X

Vs (x, a) = x>

s=1

 
 

x
x
e>
a> Pet
+L
,
t
a
a

Ptâˆ’1
Ptâˆ’1
e
where Pet =
s=1 Ps and Lt =
s=1 Ls . Taking the
gradient with respect to the second argument and setting to
zero, we get that,
0

e t,2 = 0
âˆ‡a V (x, a) = 2Pet,21 x + 2Pet,22 a + L
and thus,
1 âˆ’1 e
âˆ’1 e
Lt,2 .
a = âˆ’Pet,22
Pt,21 x âˆ’ Pet,22
2
Thus, the policy can be compactly written as
âˆ€x,

Ï€t (x) = âˆ’Kt x + ct ,

âˆ’1 e
âˆ’1 e
where Kt = Pet,22
Pt,21 and ct = âˆ’Pet,22
Lt,2 /2. Given this
linear policy, we get the quadratic value function Vt from
Equation (2).

Before stating the main result of this paper, we describe
certain assumptions and definitions from the control literature. (See, for example, (Bertsekas, 2001)).
Definition 1. A pair (A, B), where A is an n Ã— n matrix
and B is an n Ã— d matrix, is said to be controllable if the
n Ã— nd matrix [B AB . . . Anâˆ’1 B] has full rank. A pair
(A, C), where A is an n Ã— n matrix and C is an d Ã— n
matrix, is said to be observable if the pair (A> , C > ) is
controllable.
Roughly speaking, controllability implies that the state can
be moved arbitrarily by changing the actions, while observability implies that the state can be externally measured.
We assume that the system is controllable and observable.
These assumptions are standard in the literature, and will
allow a closed form expression for the optimal control law.
Assumption A1. (Controllability and observability) The
pair (A, B) is controllable and the pair (A, Q1/2 ) is observable.
Under this assumption, the gain matrix is stable, i.e. there
exists Ï âˆˆ (0, 1) such that kA âˆ’ BKâˆ— k â‰¤ Ï < 1, where
Kâˆ— = (I + B > SB)âˆ’1 B > SA

(6)

is the gain matrix (Bertsekas, 2001), and S is the solution
of the Riccati equation
S = Q + A> SA âˆ’ A> SB(I + B > SB)âˆ’1 B > SA .
Interestingly, as the next lemma shows, all gain matrices
are equal. The proof can be found in Appendix A.
Lemma 4. Consider the FTL-MDP algorithm. Let Pâˆ— =
P (Kâˆ— ) as defined by (3). If we choose K1 = Kâˆ— , then all
gain matrices are equal, Kâˆ— = K1 = K2 = K3 = . . . ,
and hence Pâˆ— = P1 = P2 = P3 = . . . .
Lemma 4 shows that gain matrices are independent of target vectors and can be computed by assuming that all target
vectors are zero. Given the fixed gain matrix, the system is
driven to a desired target position by changing the bias term
of the linear policy.
We represent the linear policy Ï€(x) = âˆ’Kx + c by the pair
Ï€ = (K, c). The class of (K 0 , C 0 )-bounded, Ï-stable linear
policies is defined by Î  = {Ï€ = (K, c) : kA âˆ’ BKk â‰¤
Ï, kKk â‰¤ K 0 , kck â‰¤ C 0 }.
Theorem 5. For a controllable, observable LQ
problem (A, B, Q, G), the regret of the FTL-MDP
algorithm with respect to the class of (K 0 , C 0 )bounded, Ï-stable linear policies is O(log2 T ),
where the hidden constants are polynomials in

Tracking Adversarial Targets

kAk , kBk , kQk , G, S, kPâˆ— k , 1/Î»min (Pâˆ— ), kKâˆ— k , K 0 , C 0 ,
and 1/(1 âˆ’ Ï).
The hidden constants are all small order polynomials. As
we will show in the next section, a careful asymptotic analysis gives us an asymptotic O(log T ) bound. It is an open
problem to show a finite-time O(log T ) regret bound with
polynomial constants.

4. Analysis

Proof. We have that

Let xÏ€âˆ = limtâ†’âˆ xÏ€t be the limiting state under (K 0 , C 0 )bounded and Ï-stable linear policy Ï€ = (K, c). In
Lemma 6 we show that this limit exists. Let Î»t (Ï€) =
`t (xÏ€âˆ , Ï€(xÏ€âˆ )) be the loss of policy Ï€ in state xÏ€âˆ . We
decompose the regret
RT =

=

T
X

First, we study the behavior of the state vector under any
bounded and stable policy. We show that the policy converges to its stationary state exponentially fast.
Lemma 6. The limiting state xÏ€âˆ = limtâ†’âˆ xÏ€t under a
(K 0 , C 0 )-bounded and Ï-stable linear policy Ï€ = (K, c)
exists and is equal to xÏ€âˆ = (I âˆ’ A + BK)âˆ’1 Bc. Further,
we have that kxÏ€t k â‰¤ kBk C 0 /(1 âˆ’ Ï) and


 Ï€

xt+1 âˆ’ xÏ€âˆ  â‰¤ Ït (I âˆ’ A + BK)âˆ’1 Bc .

`t (xt , at ) âˆ’

T
X

t=1

t=1

T
X

T
X

`t (xt , at ) âˆ’

âˆ’

T
X

Î»t (Ï€) +

t=1

Î±T =

T
X

Î»t (Ï€t ) +

T
X

T
X
t=1

t
X
(A âˆ’ BK)tâˆ’s Bc

Î»t (Ï€) âˆ’

t=1

T
X

`t (xÏ€t , Ï€)

t=1

`t (xt , at ) âˆ’

T
X

Î»t (Ï€t ) ,

t=1

Î»t (Ï€t ) âˆ’

T
X

tâˆ’1
X

(A âˆ’ BK)s Bc ,

s=0

Î»t (Ï€t )

t=1

T
X

where we used x1 = 0 in the last equality. Thus, as t goes
to infinity, xÏ€t â†’ (I âˆ’ A + BK)âˆ’1 Bc. This also implies
that kxÏ€t k â‰¤ kBk C 0 /(1 âˆ’ Ï). It is also easy to see that


âˆ
tâˆ’1
X

X
 Ï€



xt+1 âˆ’ xÏ€âˆ  =  (A âˆ’ BK)s Bc âˆ’
(A âˆ’ BK)s Bc


s=0
s=0


= (A âˆ’ BK)t (I âˆ’ A + BK)âˆ’1 Bc

t
â‰¤ kA âˆ’ BKk (I âˆ’ A + BK)âˆ’1 Bc


â‰¤ Ït (I âˆ’ A + BK)âˆ’1 Bc .

Î»t (Ï€) ,

t=1

t=1

Î³T =

= (A âˆ’ BK)t x1 +
=0+

t=1

Î²T =

= ...

s=1

Let
T
X

= (A âˆ’ BK)2 xÏ€tâˆ’1 + (A âˆ’ BK)Bc + Bc

`t (xÏ€t , Ï€)

t=1

t=1

xÏ€t+1 = (A âˆ’ BK)xÏ€t + Bc

Î»t (Ï€) âˆ’

T
X

`t (xÏ€t , Ï€) .

t=1

The terms Î±T and Î³T correspond to the difference between
the losses of the policies between their stationary and transient states. The term Î²T measures the regret with respect
to the optimal policy. The rest of this section is devoted to
providing bounds on these terms.
4.1. Bounding Î±T
To bound Î±T , we need to show that sum of terms
`t (xt , at ) âˆ’ Î»t (Ï€t ) is small. Let xÏ€âˆt = limsâ†’âˆ xÏ€s t .
We will show that this limit exists. Because Î»t (Ï€t ) =
`t (xÏ€âˆt , Ï€t (xÏ€âˆt )) and `t (xt , at ) = `t (xt , Ï€t (xt )), we need
to show that xÏ€âˆt is close to xt . This is done in a number
of steps. First, we obtain the limiting state xÏ€âˆt (Lemma 6
and the discussion after that). Then, we show that the chosen policy is slowly changing. Given these two results, we
bound kxt âˆ’ xÏ€âˆt k, which is then used to bound Î±T .

Note that even if Ï€ = (K, c) âˆˆ
/ Î , but (A âˆ’ BK) is stable,
the above argument is still valid and we get a similar result.
In particular,
xÏ€âˆt = (I âˆ’ A + BKâˆ— )âˆ’1 Bct .

(7)

Letting C be an upper bound on kct k for t â‰¤ T , with a
similar argument we can also show that
kBk C
.
(8)
1âˆ’Ï
In what follows, we use X to denote the upper bound on
def
the norm of state vector, X = kBkC
1âˆ’Ï . Next, we prove that
the chosen policy is slowly changing and the bias term in
policies is bounded by C, where
kxt k â‰¤

C = kDk GH 0 ,
âˆ’1
D = Pâˆ—,22
B > (I âˆ’ A + BKâˆ— )âˆ’> Q ,
q
kKâˆ— k kBk
2
2
H 00 = 1 + kKâˆ— k kBk /(1 âˆ’ Ï)2 +
,
1âˆ’Ï
q
2
2
H 0 = H 00 1 + kKâˆ— k kBk /(1 âˆ’ Ï)2 ,

Tracking Adversarial Targets

where Pâˆ— denotes the solution to Equation (3), corresponding to gain matrix Kâˆ— . The proof can be found in Appendix A.

Lemma 10. Let
  kBk C
Z1 = 2(C kKâˆ— k + G kQk) + 2(kQk + Kâˆ—2 )
,
1âˆ’Ï


log T
kBk C
Z2 = 4 kBk C
+
log(1/Ï)
1âˆ’Ï

Lemma 7. We have that
(i). kct k â‰¤ C ,

(ii). kct âˆ’ ctâˆ’1 k â‰¤

kDk G + 2C
.
tâˆ’1

Z3 = kBk (kDk G + 2C)(1 + log T )


log T
Ã— 1 + log T +
.
log(1/Ï)

Next, we show that the limiting state of policy Ï€t (i.e.
limsâ†’âˆ xÏ€s t ) is close to the state at time t. The proof can
be found in Appendix A.

Then we have that
Î±T â‰¤ Z1 (Z2 + Z3 )/(1 âˆ’ Ï) .

Lemma 8. If t > dlog(T âˆ’ 1)/ log(1/Ï)e, then

Proof. For policy Ï€ = (K, c), we have
kBk (kDk G + 2C) 1 + log(t âˆ’ 1)
kxt âˆ’ xÏ€âˆt k â‰¤
1âˆ’Ï
tâˆ’1

!
log(t âˆ’ 1)
1
+
log(1/Ï) t âˆ’ log(t âˆ’ 1)/ log(1/Ï)
+ Ïtâˆ’1

`t (x, Ï€) = x> (Q + K > K)x
âˆ’ 2(c> K + gt> Q)x + c> c + gt> Qgt .
For policy Ï€t = (Kt , ct ) = (Kâˆ— , ct ), define St = Q +
>
Kâˆ—> Kâˆ— and dt = 2(c>
t Kâˆ— + gt Q). We write

kBk C
.
1âˆ’Ï

Î±T =
Also, we have that
T
X
t=1

kxt âˆ’ xÏ€âˆt k â‰¤

T
X

T

 X
xÏ€âˆt > St xÏ€âˆt âˆ’ dt xÏ€âˆt
x>
S
x
âˆ’
d
x
âˆ’
t
t
t
t
t
t=1

t=1

=



1
log T
kBk C
4 kBk C
+
1âˆ’Ï
log(1/Ï)
1âˆ’Ï

Ã—

To show this, we argue as follows: first establish the simple recurrence f (t + 1) = Ïf (t) + Ï/t. This implies that
f (t) â†’ 0 as t â†’ âˆ. Now, define g(t + 1) = tf (t + 1).
Thus g(t + 1) = Ïg(t) + Ïf (t) + Ï. Since Ï < 1 and
limtâ†’âˆ f (t) = 0, limtâ†’âˆ g(t) exists and a simple calculation shows that this is Ï/(1 âˆ’ Ï). This in fact implies that
f (t) â†’ Ï/(t(1 âˆ’ Ï)) asymptotically, which in turn implies
that regret is O(log T ) asymptotically.
Now we are ready to bound Î±T .

t=1

 


 1/2   1/2 
Ã— St xt  + St xÏ€âˆt 

!!

Remark 9. This lemma shows an O(log2 T ) bound on
PT
Ï€t
As we will see, this leads to an
t=1 kxt âˆ’ xâˆ k.
2
O(log T ) regret bound. Let t = kxt âˆ’ xÏ€âˆt k. To get an
O(log T ) regret bound, we need to show that t = H1 /t for
a constant H1 . A careful
examination of proof of Lemma 8
Ptâˆ’1
reveals that t = H2 s=1 Ïs /(t âˆ’ s) + H3 for constants
Ptâˆ’1
H2 and H3 . Let f (t) = s=1 Ïs /(t âˆ’ s). We want that
f (t) = H4 /t for a constant H4 so that we get an O(log T )
regret bound.

T 

 
X
 1/2   1/2 Ï€t 
dt (xÏ€âˆt âˆ’ xt ) +
St xt  âˆ’ St xâˆ 

t=1

+ kBk (kDk G + 2C)(1 + log T )
log T
1 + log T +
log(1/Ï)

T
X

.

â‰¤

T
X

dt (xÏ€âˆt âˆ’ xt ) +


t=1 

 1/2   1/2 Ï€t 
Ã— St xt  + St xâˆ 

t=1

â‰¤

T 
X

T 

X
 1/2

St (xt âˆ’ xÏ€âˆt )



 
 
 1/2   1/2   1/2 
kdt k + St  St xt  + St xÏ€âˆt 

t=1

Ã— kxÏ€âˆt âˆ’ xt k
â‰¤ Z1

T
X

kxÏ€âˆt âˆ’ xt k

t=1

â‰¤ Z1 (Z2 + Z3 )/(1 âˆ’ Ï) .
where we used Lemma 8 in the last step.
4.2. Bounding Î²T
The term Î²T is bounded by showing a reduction to regret
minimization algorithms (in this case, the FTL algorithm).
To use the regret bound of the FTL algorithm (Theorem 1),
we need to show boundedness of the value functions.
Lemma 11. Let X 0 = kBk C 0 /(1 âˆ’ Ï), U =
max{kKâˆ— k , K 0 } max{X, X 0 } + max{C, C 0 }, V =
2
kPâˆ— k (X 0 + U )2 + 1âˆ’Ï
(G kQk + ÏC kPâˆ— k) (X 0 + U ), and

Tracking Adversarial Targets
2
F = 2 kPâˆ—,22 k U +kPâˆ—,21 k X 0 + 1âˆ’Ï
(G kQk + ÏC kPâˆ— k).
0
0
For any t, and any (K , C )-bounded, Ï-stable linear policy Ï€ = (K, c),

(i). kat k â‰¤ U ,

4.4. Putting Everything Together
Proof of Theorem 5. The regret bound follows from Lemmas 10, 12, and 13.

(ii). kâˆ’KxÏ€âˆ + ck â‰¤ U ,
(iii). kâˆ’Kâˆ— xÏ€âˆ + ct k â‰¤ U .

5. Adversarially Chosen Transition Matrices

For any action such that kak â‰¤ U ,

Our results can be extended to LQ problems with adversarial transition matrices,

(iv). Vt (xÏ€âˆ , a) â‰¤ V .
Further, Vt (xÏ€âˆ , .) is Lipschitz in its second argument with
constant F . Finally, the Hessians of the value functions are
positive definite and H(Vt (xÏ€âˆ , .))  2I.
The proof can be found in Appendix A.
Lemma 12. We have
Î²T â‰¤ 2V F 2 (1 + log T ) .
Proof. To bound Î²T , first note that
0

0

0

VÏ€,` (xÏ€âˆ , Ï€ 0 ) = `(xÏ€âˆ , Ï€ 0 ) âˆ’ Î»Ï€,` + VÏ€,` (xÏ€âˆ , Ï€)
0

= Î»Ï€0 ,` âˆ’ Î»Ï€,` + VÏ€,` (xÏ€âˆ , Ï€) .
Thus,
0

0

Î»Ï€,` âˆ’ Î»Ï€0 ,` = VÏ€,` (xÏ€âˆ , Ï€) âˆ’ VÏ€,` (xÏ€âˆ , Ï€ 0 ) .
Thus,
T
X
Î²T â‰¤
(VÏ€t ,`t (xÏ€âˆ , Ï€t ) âˆ’ VÏ€t ,`t (xÏ€âˆ , Ï€)) .
t=1

Now notice that VÏ€t ,`t (xÏ€âˆ , .) is the loss function that is fed
to the FTL strategy in state xÏ€âˆ . Lemma 11 shows that conditions of Theorem 1 are satisfied. Thus, we get the result
from the regret bound for the FTL algorithm (Theorem 1):
2

Î²T â‰¤ 2V F (1 + log T ) .

xt+1 = At xt + Bt at ,

(9)

>

`t (xt , at ) = (xt âˆ’ gt ) Q(xt âˆ’ gt ) +

a>
t at

.

Here, transition matrices At and Bt and the target vector
gt are chosen by an adversary. Once again, we measure the
regret with respect to the class of linear policies. Thus,
any policy Ï€ is identified by some pair (K, c) such that
Ï€(x) = âˆ’Kx + c.
The only no-regret algorithm for this setting is the result of Abbasi-Yadkori et al. (2013) who propose an
exponentially-weighted average algorithm and analyze it
under a mixing assumption. Similarly, we make the following assumption:
Assumption A2. (Uniform Stability) The choices of the
learner and the adversary are restricted to sets K Ã— C âŠ‚
RdÃ—n Ã— Rd and A Ã— B âŠ‚ RnÃ—n Ã— RnÃ—d , respectively.
There exists 0 < Ï < 1 such that for any A âˆˆ A and
B âˆˆ B, and any K âˆˆ K,
kA âˆ’ BKk < Ï .
Further, there exits K 0 , C 0 > 0 such that for any K âˆˆ K
and c âˆˆ C, kKk â‰¤ K 0 and kck â‰¤ C 0 .
The proposed algorithm for the LQ problem (9) is shown
in Figure 3. The algorithm maintains a distribution over
policies. The distribution has the form of
qt (Ï€) âˆ eâˆ’Î·

Ptâˆ’1

s=1

`s (xÏ€
s ,Ï€)

,

Î·>0.

(10)

The following theorem bounds the regret of this algorithm.
Theorem 14. Consider a uniformly stable system. The regret of the algorithm in Figurep3 with respect to a class of
policies |Î | is bounded by O( T log |Î | + log |Î |).

4.3. Bounding Î³T
Finally, we bound Î³T . The proof is similar to the proof of
Lemma 10 and can be found in Appendix A.
Z10

0

Lemma 13. Let
= (CK + G kQk) + (kQk +
K 02 ) kBk C 0 /(1 âˆ’ Ï). Then we have that
Î³T â‰¤

2Z10 kBk C 0
.
(1 âˆ’ Ï)2

Proof. We prove the theorem by showing that conditions
of Theorem 1 in (Abbasi-Yadkori et al., 2013) are satisfied.
Uniform mixing assumption: Let P (Ï€, A, B) be the transition probability matrix of policy Ï€ = (K, c) âˆˆ K Ã— C under
transition dynamics (A, B) âˆˆ A Ã— B. Let p1 and p01 be two
distributions over the state space and p2 = p1 P (Ï€, A, B)

Tracking Adversarial Targets

âˆš
q0 : the uniform distribution over Î , Î· = 1/ T
for t := 1, 2, . . . do
With probability Î²t = wÏ€tâˆ’1 ,tâˆ’1 /wÏ€tâˆ’1 ,tâˆ’2 choose
the previous policy, Ï€t = Ï€tâˆ’1 , while with probability 1 âˆ’ Î²t , choose Ï€t based on the distribution qt .
Learner takes the action at = âˆ’Kt xt + ct . Simultaneously, adversary chooses transition matrices At
and Bt and target vector gt .
Learner suffers loss `t (xt , at ) and observes
At , Bt , gt .
Update state: xt+1 = At xt + Bt at .
Update
the distribution qt (Ï€) âˆ wÏ€,t , where wÏ€,t =
Pt
Ï€
eâˆ’Î· s=1 E[`s (xs ,Ï€)]
end for
Figure 3. The Exponentially Weighted Algorithm for Linear
Quadratic Problems

and p02 = p01 P (Ï€, A, B). Let 1 â‰¤ k â‰¤ n be rank of M =
(Aâˆ’BK). Let M 0 be a kÃ—k matrix whose eigenvalues are
the nonzero eigenvalues of M . For x âˆˆ Rn , let xr (x) âˆˆ Rk
be a parameterization of the component of x that is on the
row space of M . Similarly, define xn (x) âˆˆ Rnâˆ’k that corresponds to the orthogonal component on the null space of
M . For u âˆˆ Rk and v âˆˆ Rnâˆ’k , let x(u, v) be a vector in
Rn such that xr (x(u,
R v)) = u and xn (x(u, v)) = v. Finally, let pr (u) = Rnâˆ’k p1 (x(u, v))dv. Using integration
by substitution, we get that
Z
kp2 âˆ’ p02 k1 =
|p2 (y) âˆ’ p02 (y)| dy
Rk
Z
=
|pr (u) âˆ’ p0r (u)| |det(M 0 )| du
Rk
Z
â‰¤ Ïk
|pr (u) âˆ’ p0r (u)| du
k
R
Z  Z

â‰¤ Ïk
p1 (x(u, v))dv

Rk  Rnâˆ’k

Z


âˆ’
p01 (x(u, v))dv du

nâˆ’k
R
Z Z
â‰¤Ï
|p1 (x(u, v)) âˆ’ p01 (x(u, v))| dvdu
Rk Rnâˆ’k
Z
=Ï
|p1 (x) âˆ’ p01 (x)| dx = Ï kp1 âˆ’ p01 k1 .
Rn

This shows that the uniform mixing assumption of AbbasiYadkori et al. (2013) is satisfied with the choice of mixing
time Ï„ = 1/ log(1/Ï).
Bounded losses: With an argument similar to the proof of
Lemma 6, we can show that the state is bounded. This,
together with the boundedness of sets K and C, give that
the action is bounded. Thus, all losses are bounded.

Results of Abbasi-Yadkori et al. (2013) also apply to the
simpler setting of Section 3. However, sampling from
the distribution (10) can be computationally expensive,
whereas the FTL-MDP algorithm is computationally efficient.

6. Conclusions and Future Work
We studied the problem of controlling linear systems with
adversarial quadratic tracking losses, competing with the
family of policies that compute actions as linear functions
of the state. We presented an algorithm whose regret, with
respect to such linear policies, is logarithmic in the number of rounds of the game. An interesting direction for future work is to consider more complex families of policies,
such as the class of linear policies with a limited number of
switches.
Existing tracking algorithms require the target sequence to
be known in advance. Also their computational complexity scales linearly with the length of the trajectory. The
main difficulty in the setting studied here, is the adversarial nature of target vectors, which is very different from the
classical setting. The key advance is to show how the idea
of Even-Dar et al. (2009) (instantiating expert algorithms
in all states) can be applied to the LQ problem, which has
a continuous and unbounded state space. This is done by
showing that the sequence of value functions and policies
will be quadratic and linear respectively, if we choose the
right expert algorithm (FTL). The compact representation
of value functions and policies allows an efficient implementation of the FTL algorithm.
We showed how a related approach can be applied to adversarially chosen changing linear dynamics. Unfortunately, this algorithms is computationally expensive. A
more challenging problem is to design efficient algorithms
for the case of adversarially chosen changing transition matrices. An interesting open problem is whether there is an
efficient no-regret algorithm, or whether a computational
lower bound can be established.
It might be possible to extend our results to LQ problems
with fixed, but unknown transition matrices of the form:
xt+1 = Axt + Bat + wt+1 ,
`t (xt , at ) = (xt âˆ’ gt )> Q(xt âˆ’ gt ) + a>
t at ,
where wt+1 is a sub-Gaussian noise and matrices A and
B are unknown. We expect this extension to be fairly
straighfoward using techniques from (Neu et al., 2012) and
(Abbasi-Yadkori and SzepesvaÌri, 2011). Our approach is
similar to Neu et al. (2012), with the difference that we use
FTL instead of FPL.

Tracking Adversarial Targets

References
Yasin Abbasi-Yadkori and Csaba SzepesvaÌri. Regret
bounds for the adaptive control of linear quadratic systems. In COLT, 2011.
Yasin Abbasi-Yadkori, Peter Bartlett, Varun Kanade,
Yevgeny Seldin, and Csaba SzepesvaÌri. Online learning
in Markov decision processes with adversarially chosen
transition probability distributions. In NIPS, 2013.
D. P. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 2nd edition, 2001.
S. Bittanti and M. C. Campi. Adaptive control of linear
time invariant systems: the â€œbet on the bestâ€ principle.
Communications in Information and Systems, 6(4):299â€“
320, 2006.
M. C. Campi and P. R. Kumar. Adaptive linear quadratic
Gaussian control: the cost-biased approach revisited.
SIAM Journal on Control and Optimization, 36(6):
1890â€“1907, 1998.
NicoloÌ€ Cesa-Bianchi and GaÌbor Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York,
NY, USA, 2006.
H. Chen and L. Guo. Optimal adaptive control and consistent parameter estimates for armax model with quadratic
cost. SIAM Journal on Control and Optimization, 25(4):
845â€“867, 1987.
H. Chen and J. Zhang. Identification and adaptive control for systems with unknown orders, delay, and coefficients. Automatic Control, IEEE Transactions on, 35(8):
866 â€“877, August 1990.
Eyal Even-Dar, Sham M. Kakade, and Yishay Mansour.
Online Markov decision processes. Mathematics of Operations Research, 34(3):726â€“736, 2009.
C. Fiechter. Pac adaptive control of linear systems. In
in Proceedings of the 10th Annual Conference on Computational Learning Theory, ACM, pages 72â€“80. Press,
1997.
R. A. Howard. Dynamic Programming and Markov Processes. MIT, 1960.
T. L. Lai and C. Z. Wei. Least squares estimates in stochastic regression models with applications to identification
and control of dynamic systems. The Annals of Statistics,
10(1):pp. 154â€“166, 1982.
T. L. Lai and C. Z. Wei. Asymptotically efficient selftuning regulators. SIAM Journal on Control and Optimization, 25:466â€“481, March 1987.

T. L. Lai and Z. Ying. Efficient recursive estimation and
adaptive control in stochastic regression and armax models. Statistica Sinica, 16:741â€“772, 2006.
Gergely Neu, AndraÌs GyoÌˆrgy, and AndraÌs Antos
Csaba SzepesvaÌri. Online Markov decision processes
under bandit feedback. In NIPS, 2010a.
Gergely Neu, AndraÌs GyoÌˆrgy, and Csaba SzepesvaÌri. The
online loop-free stochastic shortest path problem. In
COLT, 2010b.
Gergely Neu, AndraÌs GyoÌˆrgy, and Csaba SzepesvaÌri. The
adversarial stochastic shortest path problem with unknown transition probabilities. In AISTATS, 2012.

