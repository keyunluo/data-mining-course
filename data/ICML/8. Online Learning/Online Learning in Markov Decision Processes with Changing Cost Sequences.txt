Online Learning in Markov Decision Processes with Changing Cost Sequences
Travis Dick
TDICK @ UALBERTA . CA
András György
GYORGY @ UALBERTA . CA
Csaba Szepesvári
SZEPESVA @ UALBERTA . CA
Department of Computing Science, University of Alberta, Edmonton, AB, Canada T6G 2E8

Abstract
In this paper we consider online learning in finite Markov decision processes (MDPs) with
changing cost sequences under full and banditinformation. We propose to view this problem as an instance of online linear optimization. We propose two methods for this problem:
MD2 (mirror descent with approximate projections) and the continuous exponential weights algorithm with Dikin walks. We provide a rigorous
complexity analysis of these techniques, while
providing near-optimal regret-bounds (in particular, we take into account the computational costs
of performing approximate projections in MD2 ).
In the case of full-information feedback, our results complement existing ones. In the case of
bandit-information feedback we consider the online stochastic shortest path problem, a special
case of the above MDP problems, and manage to improve the existing results by removing
the previous restrictive assumption that the statevisitation probabilities are uniformly bounded
away from zero under all policies.

1. Introduction
We consider the problem of online learning in discretetime finite Markov decision processes (MDPs) with arbitrarily changing cost functions. It is assumed that a learner
moves in a finite state space X . Occupying a state xt at time
instant t, the learner takes an action at ∈ A(xt ), where
A(xt ) denotes the finite set of actions available at state
xt . Then the agent moves to some new random state xt+1 ,
where the distribution of xt+1 , given xt and at is determined by a Markov transition kernel P (·|xt , at ). Simultaneously, the agent receives some immediate cost �t (xt , at ),
where the cost function �t : U → R is assumed to be
bounded (in fact, we will assume that the costs lie in [0, 1])
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

and U = {(x, a) : x ∈ X , a ∈ A(x)}. The goal of the
learner, who is assumed to know P before the interaction
starts but not {�t }t , is to minimize its total cost. We assume here that the cost function �t can change in an arbitrary manner between time instants. The performance of
the learner is measured against the best stationary policy in
hindsight, giving rise to the expected regret:
� T
�
� T
�
�
�
RT = E
�t (xt , at ) − min E
�t (xπt , aπt ) .
t=1

π

t=1

Here, for a given stationary policy π (i.e., π is such that
π(x, ·) is a probability distribution over A(x) for any x ∈
X), (xπt , aπt ) denotes the state-action pair that policy π
would visit in time step t if this policy was used from t = 1
(we may assume that xπ1 = x1 ). Note that a sublinear
regret-growth, RT = o(T ) (T → ∞) means that the average cost collected by the learning agent approaches that
of the best (stationary) policy in hindsight. Naturally, a
smaller growth-rate is more desirable.
Motivated by the desire to design robust learning algorithms, this problem has been studied under various conditions by numerous authors (see, e.g., Even-Dar et al., 2009;
Yu et al., 2009; Neu et al., 2010; 2011; 2013) and the reader
can consult these papers for examples and extra motivation.
We consider two variants of the above model with respect to what observations are available to the learner. In
both models the learner can observe its actual state, xt . In
the full information feedback model, the learner can observe the full cost function �t at the end of time instant t,
while in the bandit feedback model the learner only observes the cost �t (xt , at ) it receives.
Treating the online MDP problem as a huge but standard
online learning problem, it is not hard to obtain algorithms
that enjoy good regret bounds but whose computational
complexity is huge. Therefore, earlier work in the literature concentrated on obtaining computationally efficient algorithms that also achieve near-optimal regret rates. These
results either concern the (stochastic) shortest path problem (SSP, an episodic MDP), or uniformly ergodic MDPs.
Several methods achieve near-optimal regret rates by running an independent expert algorithm for each x ∈ X , see
Even-Dar et al. (2005; 2009); Neu et al. (2013) for the full

Online Learning in MDPs with Changing Cost Sequences

information case and Neu et al. (2010; 2011; 2013) for the
bandit case. Yu et al. (2009) gave other low-complexity
methods with inferior performance guarantees.
The disadvantage √
of these methods is that, although they
achieve optimal O( T ) regret rate in terms of the time
horizon T , they often scale suboptimally in other problem
parameters, such as the mixing time in the uniformly ergodic case or the length of the paths in the SSP case. Furthermore, the optimal-order bounds in the literature for the
bandit setting require all states in X to be visited with positive probability under any deterministic policy, and the
inverse of this, potentially very small probability appears
in the regret bounds. In this paper we alleviate this problem and obtain optimal-order bounds that do not deteriorate
with the minimum visitation probability.
To achieve this, we treat the MDP problem as an online linear optimization problem and show that the resulting methods can be implemented efficiently (we note that
the same idea was applied successfully to the deterministic shortest path problem (György et al., 2007), where the
minimum visitation probability can also be zero). We rigorously analyze the regret and the computational complexity of our online linear optimization methods, which are
approximate versions of the mirror descent and the continuous exponential weights algorithms; we believe that
these results are also of independent interest. The mirror
descent algorithm (see, e.g., Beck & Teboulle 2003) has
a usually computationally expensive projection step which
we perform approximately using another mirror descent algorithm, while our results for the continuous exponential
weights algorithm are based on the Dikin-walk approximation of Narayanan & Rakhlin (2011).
Recently, Zimin & Neu (2013) have independently obtained similar reductions for the SSP case, using the mirror
descent algorithm, achieving essentially the same bounds.
However, they do not consider the implementation issues of
the projection step of the mirror descent algorithm (i.e., the
computational complexity of obtaining sufficiently good
approximate projections and the effect of this approximation in their final bound).
The rest of the paper is organized as follows. Section 2
introduces the classes of OMDPs we study and reduces
them to online linear optimization problems. Section 3 provides the analysis of the impact of approximation errors on
the mirror descent and the continuous exponential weights
algorithms. In Section 4 we obtain our algorithms for the
OMDP problems by applying the methods of Section 3 to
the online linear optimization problems from Section 2.

2. Online Markov Decision Processes as
Online Linear Optimization Problems
In this section we give a formal description of online
Markov decision processes (OMDPs) and show that two

classes of OMDPs can be reduced to online linear optimization. The idea behind the reduction, which goes
back to Manne (1960) (for a modern account, see Borkar
(2002)), is to represent policies by their stationary (occupation) measures over the set of state-action pairs. Under
this representation, the map from policies to their expected
cost turns out to be (approximately) linear. Further, when
the transition model of the environment is known, it is easy
to convert between policies and their stationary occupation
measures.
First, let us introduce some notation. Let ∆S denote the
set of probability measures over S.1 Note that for S finite,
we can also view ∆S as the unit simplex in R|S| : ∆S =
�|S|
{v ∈ [0, 1]|S| :
i=1 vi = 1}. The standard inner product
of Euclidean spaces will be denoted by �·, ·�. For p ≥ 1,
the p-norm of vector v is denoted by �v�p .
The structure of an online MDP is given by a finite state
space X , finite action spaces A(x), x ∈ X , with U =
{(x, a) : x ∈ X , a ∈ A(x)}, and probability
transition ker�
nel P : U × X → [0, 1] satisfying x∈X P (x|u) = 1 for
def

all u ∈ U where P (x|u) = P (u, x). The learner’s starting
state, x1 , is distributed according to some distribution µ0
over X . At each time instant t = 1, 2, . . ., based on its previous observation, state, and action sequences, the learner
chooses an action at ∈ A(xt ), possibly in a random manner. Extending this notion, we can think of a learning agent
as
�if it chose a (randomized) Markov policy πt : U → [0, 1],
a∈A(x) πt (x, a) = 1 so that at is chosen according to the
distribution πt (xt , ·). If πt = π independently of t, we say
that the agent’s strategy is stationary and we identify such a
control strategy with π. The set of such stationary Markov
policies will be denoted by Π.

2.1. Online Linear Optimization
In the following subsections we reduce two special
classes of OMDPs to online linear optimization, which we
briefly review now. Let K be a convex and compact subset of a Hilbert space V . In most cases, we take V = Rd
equipped with the standard inner product. In online linear
optimization, an adversary selects a sequence of loss vectors �1 , . . . , �T ∈ F ⊂ V and the learner’s goal is to choose
a sequence of vectors wt ∈ K so as to keep the regret
RT =

T
�
t=1

��t , wt � − min

w∈K

T
�
t=1

��t , w�

(1)

small. Naturally, the choice of wt should only depend on
the history of earlier choices and losses. As with OMDPs,
we say that there is full-information feedback if the learner
observes the entire vector �t and bandit-information feedback if only the actual loss ��t , wt � is observed. In the
1

We assume that S is equipped with the necessary σ-algebra,
and will not discuss similar trivial measurability issues in the paper.

Online Learning in MDPs with Changing Cost Sequences

semi-bandit case, the situation most related to our OMDP
problems, V = Rd and only those components of �t are observed for which the corresponding components of wt are
non-zero.
2.2. Loop-Free Stochastic Shortest Path (LF-SSP)
Problems
Here we assume that X has a layered structure, that is,
X can be partitioned into disjunct sets X1 , . . . , XL (L ≥ 1)
such that if P (x� |x, a) > 0 then x ∈ Xl and x� ∈ Xl+1
for some l = 1, . . . , L − 1, or x ∈ XL , x� ∈ X1 , and
P (x� |x, a) = µ0 (x� ) for any a ∈ A(x). This assumption
means that starting in X1 (µ0 is concentrated on X1 ), the
learner moves through X2 , X3 , . . . to reach XL , after which
the whole process returns to X1 and is restarted (we assume
without loss of generality that each x ∈ X is achievable
by following a suitable policy). The sequence of transitions from a state in X1 back to some other state in X1 is
an episode of the MDP, and in this case t will index the
episodes in the process. Since each episode starts from
the same distribution, the episodes are memoryless, and
any policy π introduces an “occupation
µπ over
�measure”
π
U , such that for any stage index l,
u∈Ul µ (u) = 1,
where Ul = {(x, a) :�x ∈ Xl , a ∈ A(x)}. Furthermore, for any x ∈ X1 , a∈A(x) µπ (x, a) = µ0 (x). With
this we can view K = {µπ : π ∈ Π} as a subset of
L
|Ul |
×L
= R|U | . Let d = |U|. Note that K
l=0 ∆Ul ⊂ ×l=0 R
is a convex polytope in Rd , since it can be described by a
set of linear constraints:
�
K = µ ∈ [0, 1]U :
�
�
�
µ(x� , a� ) =
µ(u)P (x� |u), x� ∈ X .
a� ∈A(x� )

u∈U

These constraints guarantee that the probability “flowing
into” each state is equal to the probability “flowing out”
of it. It is unnecessary to explicitly require the probability
assigned to each layer to sum to one, thanks to the assumption that the transition probability kernel P agrees with µ0
on the first layer.
Furthermore, with an immediate cost function � : U →
[0, 1], the expected total cost of policy π in an episode
can be written as ��, µπ �. Note that with this the problem of finding the stationary policy with the smallest per
episode expected cost can be written as the linear optimization problem of arg minµ∈K ��, µ�: Once the solution of this problem is found, a Markov policy πµ is extracted from
� the optimizing measure µ by πµ (x, a) =
µ(x, a)/ a∈A(x) µ(x, a). Then, by construction, µπµ =
µ.
The above description implies that all paths from the
starting layer X1 back to itself are of the same length. This
assumption is not restrictive, though, as any layered MDP
can be modified without loss of generality to satisfy this

assumption at the price of moderately increasing the state
space (see György et al., 2007). For convenience, for online learning with changing costs in LF-SSPs we redefine
the regret to be the regret of the first T episodes and use �t
to be the cost function effective in episode t. With this,
� T
�
T
�
�
πt
RT = E
��t , µ � − min
��t , µ� ,
(2)
µ∈K

t=1

t=1

where πt ∈ Π is the Markov policy used in the tth episode.
The problem of keeping the regret low is thus viewed as an
instance of online linear optimization over the convex set
K. Note that when πt is a deterministic function of the past
then the expectation can be removed.
2.3. Uniformly Ergodic MDPs
Without loss of generality, we assume that X =
{1, . . . , |X |}. Here, following previous works, we assume the so-called uniform mixing condition: There exists a number τ ≥ 0 such that under any policy π and any
pair of distributions µ and µ� over X , �(µ − µ� )P π �1 ≤
e−1/τ �µ − µ� �1 , where we use the convention of viewing distributions over X as row vectors of R|X | and P π ∈
R|X |×|X | is the�transition probability matrix underlying
π: (P π )x,x� = a∈A(x) π(x, a)P (x� |x, a). As Even-Dar
et al. (2009), we call the smallest τ satisfying this assumption the mixing time of the transition probability kernel P ,
and call a resulting MDP problem uniformly ergodic. This
assumption is not unrestrictive, but relaxing it would further complicate the paper and hence we leave this for future work. As for LF-SSPs, for a Markov policy π, let µπ
be its stationary distribution over U . Under the assumption of τ < ∞, µπ is uniquely determined. Introduce
K = {µπ : π ∈ Π} ⊂ ∆U . Again, K is a convex polytope in R|U | , since it can be described by a set of linear
constraints:
�
�
K = µ ∈ [0, 1]U :
µ(u) = 1,
�

a� ∈A(x� )

u∈U

�

�

µ(x , a ) =

�

u∈U

�
µ(u)P (x� |u), x� ∈ X .

Again, we will take d = |U| as the “dimension” of the
problem. In this case, we are concerned with finding a sequence of policies whose expected total cost up to time T
is not much larger than that of the best policy in hindsight.
Similarly to Neu et al. (2011; 2013), we can bound this
expected additional cost as shown in the following result:2
Lemma 1. Consider a uniformly ergodic OMDP with mixing time τ < ∞ and losses �t ∈ [0, 1]d . Then the regret
of an agent following policies π1 , . . . , πT through the trajectory (xt , at )t relative to a fixed policy π can be bounded
2
The proof of this result can be found in the appendix along
with several other proofs omitted from the main text.

Online Learning in MDPs with Changing Cost Sequences

as
E

�

T
�
t=1

≤

�

�t (xt , at ) − E

T
�

E [��t , µ

πt

t=1

for any k ≥ E [�µ

πt

�

T
�

�t (xπt , aπt )

t=1

�

πt−1

�], t = 2, . . . , T .

Since we can recover a policy from a stationary distribution (as in the LF-SSP case), it is enough to find a slowly
changing sequence µ1 , . . . , µT ∈ K such that the first term
of the bound is small. This is again an online linear optimization problem.
We have now mapped online learning in MDPs, under
both sets of assumptions, to online linear optimization,
which is a well-studied problem in online learning (CesaBianchi & Lugosi, 2006; Shalev-Shwartz, 2012). In the
next section, we discuss two general algorithms designed
to attack this problem and how they apply to our case.

3. Online Linear Optimization
In this section we consider the challenges of implementing two standard algorithms for online linear optimization: mirror descent (MD) and the continuous exponential weights algorithm (CEWA). When the feasible set K
is complicated, some operations from both algorithms have
no closed form and need to be approximated iteratively. We
analyze the impact of the approximation errors on the regret analysis for both methods. With an understanding of
how the regret scales with the approximation errors, we are
able to determine the necessary precision, which will lead
to bounds on the computational complexity of the approximate versions of these methods.
Recall that the goal of online linear optimization is to
choose a sequence �
of vectors wt ∈ K ⊂ V in �
order to keep
T
T
the regret RT =
��
,
w
�
−
min
t
t
w∈K
t=1
t=1 ��t , w�
small, no matter how the sequence of loss vectors �t ∈ F
is chosen.
3.1. Mirror Descent with Approximate Projections
Mirror descent is a well-known strategy for achieving
low regret in online linear optimization problems. It has
two parameters: a step size η > 0 and a Legendre function3
R : A → R, called a regularizer. We assume that A is
a superset of K. Starting from w1 ∈ K, MD makes a
sequence of predictions wt defined by
wt+1 = argmin η ��t , w� + DR (w, wt ),
w∈K

where DR (u, v) = R(u)−R(v)−�∇R(v), u − v� denotes
the Bregman divergence induced by R. As is well known,
3

w̃t+1 = argmin η ��t , w� + DR (w, wt )
w∈A

wt+1 = ΠR (w̃t+1 ),

π

− µ �] + T (τ + 1)k + 4τ + 4,

−µ

wt+1 can be obtained in the following two-step process:

R : A → R is Legendre if A �= ∅ is convex, R is strictly
convex on its interior A◦ where ∇R exists, and �∇R(w)� → ∞
as w approaches the boundary of A from inside A.

where ΠR (w̃) = argminw∈K DR (w, w̃) : A → K denotes the Bregman projection associated with R. Often
w̃t+1 can be expressed in closed form and computed in
constant time; then the main challenge of applying MD is
in computing the Bregman projection.
Unless the set K is very simple, there is no closed form
for the Bregman projection and we must use inexact iterative techniques. Hence, the next iterate wt+1 will be different from ΠR (w̃t+1 ). The following theorem analyzes
the regret of MD with c-approximate projections when
�wt+1 − ΠR (w̃t+1 )� ≤ c.4

Theorem 2. Let R be convex and K be a convex set such
that ∇R is λ-Lipschitz on K with respect to (wrt) �·�. Let
D = supu,v∈K �u − v�∗ be the diameter of K wrt the dual
norm of �·�. Then the regret of MD, with c-approximate
projections, step size η, and regularizer R satisfies
T
�
t=1

��t , wt −w� ≤

T
�
t=1

��t , wt − w̃t+1 �+

DR (w, w1 ) cλDT
+
,
η
η

for any w ∈ K and losses {�t }Tt=1 , where �t can depend
on {(ws , �s )}t−1
s=1 . When c = 0, the result remains true
even when λ and/or D are unbounded, in which case we
interpret cλD = 0.
When the regularizer R is σ-strongly convex wrt the
norm �·�, i.e., if R(w) ≥ R(w� ) + �∇R(w� ), w − w� � +
σ
� 2
�
∈ A, we can use the following
2 �w − w � for any w, w�
lemma to bound the sum t ��t , wt − w̃t+1 � from Theorem 2.

Lemma 3. Let R : A → R be a σ-strongly convex Legendre function wrt the norm �·�, η > 0, w ∈ A, � ∈ Rd ,
and define w̃ ∈ A to be the unconstrained MD update:
w̃ = argminu∈A η ��, u� + DR (u, w). Then ��, w − w̃� ≤
2
η
σ ���∗ , where �·�∗ denotes the dual norm of �·�.
3.1.1. MD2 : E FFICIENT O NLINE L INEAR
O PTIMIZATION FOR S UBSETS OF [β, 1]d .
In this section we present a particular implementation of
MD with approximate projections that is of interest for the
optimization problems presented in Section 2. When the
4

The results and their proofs in this and the next section
are folklore in the online learning literature. We have learned
the proof techniques mostly from Cesa-Bianchi & Lugosi (2006);
Rakhlin (2009); Shalev-Shwartz (2012), though some of our proof
steps might be different. The reader can consult György et al.
(2013) for the proofs of “general” online learning results whose
proofs are omitted.

Online Learning in MDPs with Changing Cost Sequences

constraint set K is a subset of the unit cube [0, 1]d ⊂ Rd ,
to obtain a regret bound that scales with the logarithm of
the dimension d, we use MD�
with the unnormalized negentropy regularizer R(w) =
u wu ln(wu ) − wu . Unfortunately, ∇R(w) = (ln(w1 ), . . . , ln(wd ))� is unbounded
when any component of w approaches zero. Thus R violates the condition of Theorem 2 that requires ∇R to be
Lipschitz continuous and makes it challenging to design efficient methods for computing c-approximate projections.
Therefore, for the rest of this section we assume that the elements of K have components that are uniformly bounded
away from zero by β > 0. In other words, we assume that
K ⊂ [β, 1]d ⊂ Rd .
In order to apply MD, we need to provide a method for
computing c-approximate projections onto the set K. We
propose to use a second instance of MD with the squared
2
2-norm regularizer R� (w) = 12 �w�2 . The motivation for
this choice of regularizer is that the induced Bregman divergence equal to the Euclidean distance and the associated Bregman projection is the Euclidean projection. Then
the inner instance of MD will also use approximate projections, but they can be calculated as the solutions to
quadratic programming problems, which can be solved efficiently by interior point methods. We call this algorithm
MD2 , since it has two instances of MD running. The regret
of MD2 can be bounded as follows:
Corollary 4. Let β > 0 and K ⊂ [β, 1]d . Let
w1 , . . . , wT ∈ K be the sequence of predictions made
by MD2 on K with losses �1 , . . . , �T ∈ [0, ∞)d , capproximate projections where accuracy is measured by
�·�1 , step size η > 0, and the unnormalized negentropy
regularizer R. Then, for any w ∈ K, we have
T
�
t=1

��t , wt − w� ≤

T
�
t=1

��t , wt − w̃t+1 �+

DR (w, w1 ) cT
+
η
βη

where w̃t+1 is defined√ component-wise by w̃t+1,u =
wt,u e−η�t,u . Let � = c/ d. The per-step complexity is
�
�
� �
�
�
H
2
√
O
2 ln
+ ln(W2 + 2�) + d ,
�
β
where W2 = supw,w� ∈K �w − w� �2 , H is the cost of the
projection step used in the inner √
MD instance when computed with an accuracy of c� = 14 β� wrt �·�2 .

When K ⊂ [β, 1]d is a polytope in the positive quadrant described by m ≤ d linear equality constraints,
interior point methods can be used to achieve H =
O(d3.5 ln(d/c� )) = O(d3.5 ln(d/(cβ))) (e.g., Section 2.4.2
of den Hertog 1994, or Section 4.3.2 of Nesterov 2004).
Finally, we note in passing that instead of using mirrordescent to implement the approximate projection, one
could also use an interior point algorithm for this purpose.
Building on the results of Potra & Ye (1993), it appears

possible to compute an �-approximate projection to K as
above in time O(d3.5 (1 + ln(1/β) + ln ln(d/�))), resulting in a modest improvement of the total complexity. Fang
et al. (1997) discuss more methods, but with no complexity
analysis.
3.2. Continuous Exponential Weights Algorithm with
Dikin Walks
Consider an online linear optimization problem over a
convex closed set K ⊂ Rd . Let the sequence of loss vectors be �1 , . . . , �T and �let p1 be some positive density over
K (i.e., p1 ≥ 0 and K p1 (x) dx = 1). Then, the continuous exponential weights algorithm (CEWA) (see, e.g.,
Narayanan & Rakhlin, 2011) at time t + 1 predicts Xt+1 ,
where Xt+1 ∼ pt+1 (·) and�
pt+1 is a density over K prot
portional
to
p
(x)
exp(−η
1
s=1 ��s , x�), where �f, g� =
�
f
(x)g(x)
dx.
Here
η
>
0
is
the learning rate of the alx
gorithm.
�
When p1 ∈ L2 (K) (i.e., p21 (x)dx < ∞), the
continuous exponential weights algorithm can be interpreted as an instance of mirror descent� with the unnormalized negentropy regularizer R(p) = p(x) ln(p(x)) −
p(x) dx. Indeed, it is easy to see that in this case pt+1 =
arg minp∈D(K) {η ���t , p� + DKL (p, pt )} for t ≥ 1, where
D(K) is the set
� of densities over K (D(K) = {p :
K → [0, ∞)| K p(x)dx = 1}), ��t (u) = ��t , u� for
any u ∈ K, the inner
product over D(K) is defined as
�
where ��� , p� = u∈K �� (u)p(u) du, and DKL (p, p� ) =
�
p(x) ln(p(x)/p� (x))dx is the Kullback-Leibler diverK
gence between p, p� ∈ D(K), which is also the Bregman
divergence induced by R. As such, with a straightforward generalization of Theorem
�� 2 with c = 0, we� get that
T
�
the expected regret RT = E
t=1 ��t , Xt − U � against
any random variable U with density pU supported on K is
bounded by
RT ≤

T
�
t=1

���t , pt − qt+1 � +

DKL (pU , p1 )
η

(3)

where qt+1 (x) = pt (x)e−η�t (x) .
The advantage of CEWA is that it avoids the usual projection step associated with MD (or similar algorithms,
like “Follow the Regularized Leader” ). The complexity is
pushed to sampling from pt+1 , which, as we will see, leads
to a different tradeoff. The question of how to efficiently
sample from pt+1 , or a distribution sufficiently close to
pt+1 , was addressed by Narayanan & Rakhlin (2011). They
proposed a Markov Chain Monte-Carlo (MCMC) method
to implement sampling. The stationary distribution of the
Markov chain they design at time step t + 1 is pt+1 . However, since the chain is only run for finitely many steps, the
distribution that Xt+1 follows may differ from pt+1 . In
fact, in their paper they proposed to make only one step
with the Markov chain, that is, to use Xt+1 = Pt+1 (·|Xt ),

Online Learning in MDPs with Changing Cost Sequences

where Pt+1 (·|x) is the Markov kernel underlying the chain
they designed. They argue that this is sufficient, since pt is
close to pt+1
√ . Indeed, they prove a regret bound that shows
the usual T behavior, but the price of making only one
step with the Markov chain is that the regret blows up by
a factor that is proportional to d5 . Since we wish to avoid
this increase of the regret, we propose to run the chain for
more time steps. By following the analysis of (Narayanan
& Rakhlin, 2011), we get the following result:
Proposition 5. Assume that for any t ≥ 1, x ∈ K,
the losses satisfy ��t , x� ∈ [0, B]. Let Pt+1 (·|x) be the
Markov kernel underlying the “Dikin walk” of Narayanan
& Rakhlin (2011) at time step t + 1. Assume that ηB ≤ 1
and fix an arbitrary parameter 0 < r ≤ 1. If X1 ∼ p1 5 and
(k)
for t = 1, 2, . . . in time step t + 1, Xt+1 = Zt+1 , where
(i+1)
(i)
(1)
Zt+1 ∼ Pt+1 (·|Zt+1 ), i = 1, 2, . . . with Zt+1 = Xt then
2 3
2
if
� k� ≥ Cν d �ln((1 +
� η(e − 1)B) + 2η(e − 1)B/r) then
�pt+1 − pt+1 � = |p�t+1 (x) − pt+1 (x)|dx ≤ r, where
1
p�t+1 is the distribution of Xt+1 , C > 0 is a universal constant and ν = O(d) is a parameter that depends on the
shape of K.
The main work in making a move with the Markov chain
is to sample from a Gaussian random variable. Note that
the covariance of this distribution depends on the current
state. Thus, the cost of one step is dominated by the O(d3 )
cost of computing the Cholesky factorization of this covariance matrix once the matrix is computed. Hence, the total
cost of sampling at one time step is O(d8 ln(1 + ηB/r),
where we assumed that computing the covariance matrix
can also be done in O(d3 ) step.
Finally note that if �p�t − pt �1 ≤ r in each time step
t then the additional regret due to the “imprecise” implementation up to time T is bounded by rT B. Consider now
the case when B
√ is constant (i.e., independent of T ). Then,
setting r = 1/√ T we see that the increase of the
√regret is
bounded by B T . Now, remember
that
to
get
a
T regret
√
one should use η = O(1/ T ) (e.g., see the bound of Theorem 2). Hence, in this case the cost of sampling per time
step can be kept constant independently of the time horizon
with essentially no increase of the regret.
3.3. Bandit Information
The purpose of this section is to briefly consider bandit
online linear optimization. The difference between bandit
online linear optimization and the setting considered above
is that at the end of time step t the only information received
is the scalar loss of the vector chosen in that time step, that
is ��t , ŵt �, while �t is not revealed to the learner. For a recent survey of the literature see the review paper by Bubeck
& Cesa-Bianchi (2012). The approach followed by existing
algorithms is to construct an estimate ��t (usually unbiased)
5

This assumption is not necessary, just simplifies the analysis.

of �t and use this in place of �t in a “full-information algorithm”, like MD of the previous section. The question then
is how to construct ��t and how to control the regret. Our
main tool in this latter respect is going to be Theorem 2.
Indeed, if MD is run with ��t−1 in place of �t−1 , then from
Theorem 2 we see that, as far as��
the expected regret��
is con�
cerned, it suffices to control E �t−1 , ŵt − w̃t−1 . For
this, we have the following result extracted from Abernethy
et al. (2008):6
Lemma 6. Let w, � ∈ V = Rd , and define�
w̃u = wu e−η�u
for all u = 1, . . . , d. Then ��, w − w̃� ≤ η u wu �2u .
Note that the lemma continues to be true even if V =
L2 (K), is the space of square-integrable functions over K,
in which case the sum should be replaced by an integral
over K wrt the Lebesgue measure.

4. Learning in Online Markov Decision
Processes
In this section, we consider online learning in MDPs in
the so-called full-information setting. In the case of LFSSPs this means that �t is observed at the end of episode
t, while in the case of ergodic MDPs �t is observed at the
end of each time step. We only consider full-information
algorithms based on MD2 ; solutions using CEWA are only
provided for the bandit case (to save space).
Consider first LF-SSP problems. In order to apply MD2 ,
we need the components of the (occupation) measures to be
bounded away from zero. This will not be the case generally, since policies may choose actions with arbitrarily low
probabilities. Without loss of generality we can assume
that there exists a β > 0 and a policy πexp such that the
corresponding (occupation) measure µexp = µπexp satisfies
µexp (x, a) ≥ β for all (x, a) ∈ U . By the convexity of
.
K, µδ = (1 − δ)µ + δµexp ∈ K for any 0 < δ < 1 and
µ ∈ K (i.e., there exists a policy inducing µδ ), and for any
loss function � we have
| ��, µδ � − ��, µ� | = δ| ��, µexp − µ� |.

(4)

Therefore, we do not loose much if we use MD with
2

Kδβ = {µ ∈ K : µ(x, a) ≥ δβ for all x, a}

instead of K, since µδ ∈ Kδβ .
First we consider the simple case of the LF-SSP problem. By (4) and since �t (u) ∈ [0, 1] for all u ∈ U,
�
�
T
T
��
�
�
�
�
��t , µ� −
��t , µδ �� ≤ δLT.
(5)
�
�
�
t=1

t=1

(Recall that L is the number of layers in the state space.)
Now let us run MD2 on Kδβ with the unnormalized neg�L−1
ative entropy regularizer R(µ) =
l=0 Rl (µi ), where
6

The proof can be found in (György et al., 2013).

Online Learning in MDPs with Changing Cost Sequences

µ = (µ0 , . . . , µL−1 ), µl ∈ [0, ∞)Ul and Rl is the unnormalized negative entropy regularizer over [0, ∞)Ul . Since
it follows from Pinsker’s inequality that R is 1/L strongly
convex wrt the �·�1 -norm (see also Example 2.5 of ShalevShwartz 2012), combining Corollary 4, with Lemma 3 and
(5), we obtain the following result:
Theorem 7 (MD2 on LF-SSP, full information). Let π
be any policy, µ1 ∈ K, δ ∈ (0, 1] and Dmax ≥
√
supµ∈Kδβ DR (µ, µ1 ). Run MD2 with parameters c = βδη
T
�
Dmax
and η =
on
K
with
the
sequence
of
loss
funcδβ
LT
tions �1 , . . . , �T . Let µt be the output of MD2 on round t
and define πt = πµt (i.e., the state-conditional action probabilities). Then the regret of the agent that follows policy
πt at time t relative to policy π can be bounded as
�
√
RT ≤ 2 LT Dmax + T + LδT,
and the per-time-step computational cost is bounded by
� 3.5
�
d L
L
O √
(L + ln(L + Dmax )) , where L = ln( dT
βδ ).
βδ

The proof follows from the arguments preceding the theorem combined with Corollary 4 and the remark after it.
Also, the next theorem has an almost identical proof, hence
we decided to omit this proof. �
�
Note that Dmax

=

Θ L ln π10 , where π0

=

min(x,a)∈U πexp (x, a) (notice that πexp (x, a) ≥ β since
µexp (x, a) ≥ β). If, for example, πexp (x, ·) is selected
to be the uniform distribution over A(x), then β > 0
and π
�0 = 1/ maxx |A(x)|, making the regret
√ scale with
O(L T ln(maxx |A(x)|)) when δ = 1/ T . Also, this
√
makes the computational cost Õ(d3.5 T 1/4 / β), where
Õ(·) hides log-factors.
� Neu et al. (2010) gave an algorithm
that achieves O(L2 T ln(maxx |A(x)|)) regret with O(d)
computational complexity per time-step. Thus, our regret
bound scales better in the problem parameters than that of
Neu et al. (2010), at the price of increasing the computational complexity. It is an interesting (and probably challenging) problem to achieve the best of the two results.
Consider now the case of uniformly ergodic MDPs. In
order to apply MD2 , we need to obtain a regret bound for
online linear optimization on the corresponding set K and
show that the sequence of policies does not change too
quickly. By (4) and because �t ∈ [0, 1]d , we have
� T
�
T
T
��
�
�
�
�
�
��t , µ� −
��t , µδ �� ≤ δ
| ��t , µexp − µ� | ≤ δT.
�
�
�
t=1
t=1
t=1
(6)
Therefore, running MD2 on Kδβ with the negentropy regularizer R(µ) = Rd (µ) gives the following result:
Theorem 8 (MD2 on Ergodic MDPs, full information).
Let π be any policy, µ1 ∈ K, δ ∈ (0, 1] and Dmax ≥

√
supµ∈Kδβ DR (µ, µ1 ). Run MD2 with parameters c = βδη
T
�
max
and η = T D
on
K
with
the
sequence
of
loss
funcδβ
(2τ +3)

tions �1 , . . . , �T . Let µt be the output of MD2 on round t,
and define πt = πµt . Then the regret of the agent that follows policy πt at time t relative to policy π can be bounded
as
�
√
RT ≤ 2 (2τ + 3)T Dmax + T + δT + 4τ + 4,
and the per-time-step computational cost is bounded by
� 3.5
�
d L
dT τ
O √
(L + ln(Dmax )) , L = max(1, ln( Dmax
βδ )) .
βδ

As far as the√dependence on τ is concerned, by choosing δ = 1/ T , we can thus improve the previous
state-of-the-art
bound (Neu
�
� et al., 2013) that scales as
O(τ 3/2 T ln |A|) to O( τ T ln |A|). The update cost of
2
the algorithm of Neu et al. (2013) is O(|X |3 +
√|X | |A|),
while here the cost of the MD2 is Õ(T 1/4 d3.5 / β).

5. Learning under Bandit Information in
LF-SSP
The purpose of this section is to consider online learning in the LF-SSP problem under bandit feedback, that is,
when at time t, the only information received is �t (xt , at ),
the cost of the current transition. Based on the previous
sections, we see that to control the regret, an MDP learning
algorithm has to control the regret in an online linear bandit
problem with decision set K.
According to Bubeck et al. (2012), for online bandit
linear optimization over a compact action set √
K ⊂ Rd ,
it is possible to obtain a regret of order O(d T log T )
regardless the shape of the decision set K, which, in
our case
√ would translate into a regret bound of order
O(|U| T log T ). Whether the algorithm proposed in this
paper can be implemented efficiently depends, however, on
the particular properties of K: Designing the exploration
distribution needed by this algorithm requires the computation of the minimum volume ellipsoid containing K and
this problem is in general NP-hard even when considering
a constant factor approximation (Nemirovski, 2007).
In this section, focussing on LF-SSPs, we design computationally efficient bandit algorithms based on MD and the
continuous exponential weights algorithm. In both cases,
the immediate costs will be estimated in the same manner:
(l)

(l)

I{xt = x, at = a}
��t (x, a) =
�t (x, a).
µπt (x, a)

(7)

Note that in each stage l, ��t (x, a) is nonzero only for the
state-action pair visited in Ul ; hence, ��t is available to the
learner. It is easy to see that as long as (B) µπt (x, a) is
bounded away from zero for each state-action pair (x, a),

Online Learning in MDPs with Changing Cost Sequences

the above estimate is unbiased. In particular, denoting by
Ft the σ-algebra generated
by the
�
� history up to the beginning of episode t, E ��t (x, a)|Ft = �t (x, a) holds for all

(x, a) ∈ U.
First, let us consider the application of MD2 with the unnormalized negentropy regularizer on Kδβ to this problem.
Note that the restriction to Kδβ is now used to ensure both
that the projection step can be implemented efficiently and
that estimates in (7) are well-defined. In particular, this implies that (B) will be satisfied. Using Lemma 6 then gives
the following result:
Theorem 9 (MD2 on Bandit LF-SSP).
Let π be any policy,
�
µ ∈ K, δ ∈ (0, 1] and Dmax ≥ µ∈Kδβ DR (µ, µ1 ). Run
�
Dmax
√ and η =
MD2 with parameters c = βδη
dT on Kδβ
T
with the sequence of estimated loss functions ��1 , . . . , ��T ,
defined in (7). Let µt be the output of MD2 on round t, and
define πt = πµt . Then the regret of the agent that follows
policy πt at time t relative to policy π can be bounded as
�
√
RT ≤ 2 dT Dmax + T + LδT,
and the computational cost is bounded as in Theorem 7.

Selecting πexp (x, ·) to be the uniform distribution,
β �
> 0 and Dmax ≤ L ln(maxx |A(x)|), results in a
O(√ dLT ln(maxx |A(x)|)) bound on the regret for δ =
1/ T , while√the time-complexity of the algorithm is still
Õ(d3.5 T 1/4 / β) as in the full-information case. Neu et al.
(2010) considered the same problem under the assumption
that any policy π visits any state
� with probability at least α
for some α > 0, that is, inf π a∈A(x) µπ (x, a) ≥ α > 0.
They provided an algorithm
�with O(d) per round complexity whose regret is O(L2 T maxx (A(x) ln(A(x)))/α).
Compared to their result, we managed to lift the assumption
α > 0, and also improved the dependence on the size of the
MDP, while paying a price in terms of increased computational complexity.
Let us now consider applying CEWA with the Dikinwalk to the same problem. As in the MD2 case, we
run the algorithm on Kδβ with δ > 0. As in the full
�
U
information� case,
� we let �t (µ) = ��t , µ�, µ ∈ R ,
���t (µ) = ���t , µ , where ��t is obtained using (7). Let
�t−1
p̂t (µ) �= p1 (µ) exp(−η s=1 ���s (µ)), while pt = p̂t /Zt ,
Zt = Kδβ p̂t (µ)dµ. Let µt (= Xt ) ∈ Kδβ be the output
of the Dikin-walk at time step t when the walk is run for k
steps and let πt = πµt be the corresponding policy. Since
each coordinate of µt ∈ �Kδβ is� bounded away
� from
� zero,
�
�
�
�
�t is well-defined and E �t |Ft = �t and E � |Ft = �� .
t

t

Combining (3) with Proposition 5 and Lemma 6, we get the
following result:
Theorem
10 (CEWA on Bandit LF-SSP). Let δ =
�
dL ln(βT /d)
2β(2L+1)T ,

and assume that πt is obtained by running

the algorithm of Section 3.2 on Kδβ with the estimated
losses {��t }, started from the uniform distribution, and with
parameters r = 2δ, η = δβ/L and k ≥ Cd5 ln(βT /d) for
some universal constant C > 0. Then, for any T > 2d/β,
the regret against any µ ∈ K is bounded by
�

RT ≤ 3

dL(L + 1/2)T ln(βT /d)
β

while the per-step computational complexity is bounded by
O(d3 k).
�
Notice that the regret bound is O(L dT ln(T )/β),
while the per-step computational complexity (choosing the
smallest k) is O(d8 ln T ). Thus, this algorithm does not
achieve the performance
of MD2 ; the scaling of the re√
gret bound with 1/ β is especially not nice. On the other
hand, the computational cost of the algorithm is better in T
than that of MD2 . The regret bound of
� the algorithm could
be improved if in (7) we divided by µ(x, a)pt (µ)dµ (instead of µt ), the probability of visiting the state-action pair
(x, a). However, we cannot compute this probability in a
closed form, and its estimation would require additional
sampling, further increasing the computational cost (see
Neu & Bartók 2013 for a similar approach). We finally
note that based on the proof it is obvious that for the fullinformation setting, CEWA would achieve a regret competitive with MD2 , and again, its cost would be lower in T , but
higher in d.

6. Conclusions
In this paper, viewing online learning in MDPs as online
linear optimization, we have proposed novel algorithms
based on variants of mirror-descent. We proposed efficient
solutions, based on approximate projections and MCMC
sampling, to overcome the computational difficulty of the
projection step in MD arising from the OMDP structure.
We rigorously analyzed the complexity of these algorithms.
Our results improve upon the state-of-the-art by improving
the regret bounds and lifting some restrictive assumptions
that were used earlier in the literature. The price we pay is a
somewhat increased computational complexity, though our
algorithms still enjoy polynomial computational complexity in the problem parameters. It is an interesting (and probably challenging) problem to find out whether the tradeoff exposed is “real”. Extending our results to the banditinformation feedback case for uniformly ergodic OMDPs
is also an important problem. One promising approach in
this direction is to combine our methods with the rewardestimation technique of Neu et al. (2013).

Acknowledgements
This work was supported by the Alberta Innovates Technology Futures and NSERC.

Online Learning in MDPs with Changing Cost Sequences

References

Abernethy, J., Hazan, E., and Rakhlin, A. Competing in the
dark: An efficient algorithm for bandit linear optimization. In
Servedio, Rocco A. and Zhang, Tong (eds.), Proceedings of the
21st Annual Conference on Learning Theory (COLT 2008), pp.
263–274. Omnipress, 2008.
Beck, A. and Teboulle, M. Mirror descent and nonlinear projected
subgradient methods for convex optimization. Operations Research Letters, 31(3):167–175, 2003.
Borkar, V. S. Convex analytic methods in Markov Decision Processes. In Feinberg, E.A. and Shwartz, A. (eds.), Handbook
of Markov Decision Processes, chapter 11. Kluwer Academic
Publishers, 2002.
Bubeck, S. and Cesa-Bianchi, N. Regret analysis of stochastic
and nonstochastic multi-armed bandit problems. Foundations
and Trends in Machine Learning, 5(1):1–122, 2012.
Bubeck, S., Cesa-Bianchi, N., and Kakade, S. M. Towards minimax policies for online linear optimization with bandit feedback. Journal of Machine Learning Research - Proceedings
Track, 23:41.1–41.14, 2012.
Cesa-Bianchi, N. and Lugosi, G. Prediction, Learning, and
Games. Cambridge University Press, New York, NY, USA,
2006.
den Hertog, D. Interior Point Approach to Linear, Quadratic and
Convex Programming: Algorithms and Complexity. Springer,
1994.
Even-Dar, E., Kakade, S. M., and Mansour, Y. Experts in a
Markov decision process. In Saul, Lawrence K., Weiss, Yair,
and Bottou, Léon (eds.), Advances in Neural Information Processing Systems 17, pp. 401–408, Cambridge, MA, USA, 2005.
MIT Press.
Even-Dar, E., Kakade, S. M., and Mansour, Y. Online Markov
decision processes. Mathematics of Operations Research, 34
(3):726–736, 2009. ISSN 0364-765X. doi: http://dx.doi.org/
10.1287/moor.1090.0396.
Fang, S.-C., Rajasekera, J. R., and Tsao, H. Entropy Optimization
and Mathematical Programming. Springer, 1997.
György, A., Linder, T., Lugosi, G., and Ottucsák, Gy. The online shortest path problem under partial monitoring. Journal of
Machine Learning Research, 8:2369–2403, 2007. ISSN 15324435.
György, A., Pál, D., and Szepesvári, Cs.
Online
learning: Algorithms for big data.
Lecture Notes,
2013.
URL https://www.dropbox.com/s/
bd38n4cuyxslh1e/online-learning-book.pdf.
Hazan, E., Agarwal, A., and Kale, S. Logarithmic regret algorithms for online convex optimization. Machine Learning
Journal, 69(2-3):169–192, 2007.
Lafferty, J., Williams, C.K.I., Shawe-Taylor, J., Zemel, R.S., and
Culotta, A. (eds.). Advances in Neural Information Processing
Systems 23, 2011.
Manne, A.S. Linear programming and sequential decisions. Management Science, 6(3):259–267, 1960.

Narayanan, H. and Rakhlin, A. Random walk approach to regret
minimization. In Lafferty et al. (2011), pp. 1777–1785.
Nemirovski, A. Advances in convex optimization: Conic programming. In Proceedings of International Congress of Mathematicians, volume 1, pp. 413–444, 2007.
Nesterov, Y. Introductory Lectures on Convex Optimization.
Kluwer Academic Publishers, 2004.
Neu, G., György, A., and Szepesvári, Cs. The online loop-free
stochastic shortest-path problem. In Proceedings of the 23rd
Annual Conference on Learning Theory (COLT), pp. 231–243,
2010.
Neu, G., György, A., Szepesvári, Cs., and Antos, A. Online
Markov decision processes under bandit feedback. In Lafferty
et al. (2011), pp. 1804–1812.
Neu, G., György, A., Szepesvári, Cs., and Antos, A. Online Markov decision processes under bandit feedback.
IEEE Transactions on Automatic Control, 2013.
URL
http://www.szit.bme.hu/˜gya/publications/
NeGySzAn13.pdf. (accepted for publication).
Neu, Gergely and Bartók, Gábor. An efficient algorithm for learning with semi-bandit feedback. In Jain, Sanjay, Munos, Rémi,
Stephan, Frank, and Zeugmann, Thomas (eds.), Algorithmic
Learning Theory - 24th International Conference, ALT 2013,
Singapore, October 6-9, 2013. Proceedings, volume 8139 of
Lecture Notes in Computer Science, pp. 234–248, 2013.
Potra, F. and Ye, Y. A quadratically convergent polynomial algorithm for solving entropy optimization problems. SIAM J.
Control and Optimization, 3(4):843–860, 1993.
Rakhlin, A. Lecture notes on online learning. Lecture Notes,
2009.
Shalev-Shwartz, S. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107–
194, 2012. ISSN 1935-8237. doi: 10.1561/2200000018. URL
http://dx.doi.org/10.1561/2200000018.
Yu, J. Y., Mannor, S., and Shimkin, N. Markov decision processes
with arbitrary reward processes. Mathematics of Operations
Research, 34(3):737–757, 2009. ISSN 0364-765X. doi: http:
//dx.doi.org/10.1287/moor.1090.0397.
Zimin, A. and Neu, G. Online learning in episodic Markovian decision processes by relative entropy policy search. In Burges,
C.J.C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 26, pp. 1583–1591. 2013.

