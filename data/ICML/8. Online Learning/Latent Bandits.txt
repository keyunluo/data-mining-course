Latent Bandits.
Odalric-Ambrym Maillard
ODALRIC - AMBRYM . MAILLARD @ ENS - CACHAN . ORG
The Technion, Faculty of Electrical Engineering 32000 Haifa, ISRAEL
Shie Mannor
The Technion, Faculty of Electrical Engineering 32000 Haifa, ISRAEL

Abstract
We consider a multi-armed bandit problem
where the reward distributions are indexed by
two sets –one for arms, one for type– and can
be partitioned into a small number of clusters according to the type. First, we consider the setting
where all reward distributions are known and all
types have the same underlying cluster, the type’s
identity is, however, unknown. Second, we study
the case where types may come from different
classes, which is significantly more challenging.
Finally, we tackle the case where the reward distributions are completely unknown. In each setting, we introduce specific algorithms and derive
non-trivial regret performance. Numerical experiments show that, in the most challenging agnostic case, the proposed algorithm achieves excellent performance in several difficult scenarios.

1. Introduction
In a recommender system (Li et al., 2010; 2011; Adomavicius & Tuzhilin, 2005), an agent must display an ad to each
incoming client, and a context vector summarizes the observed properties of a client, such as its navigation history
or its geographic localization. In a cognitive radio (Avner
et al., 2012; Filippi et al., 2008), an agent must select a
communication channel, based on its current known location and network conditions, while avoiding collision with
other sources (such as radar, WiFI, etc). Both examples
can be analyzed within the contextual-multi-armed bandit framework (Langford & Zhang, 2007; Lu et al., 2010),
where the contexts summarize the information available to
the learner. However, the context alone may not be sufficient to solve these problems optimally: In recommender
systems, information such as gender or salary, is typically
missing (due to privacy). In cognitive radios, information
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

SHIE @ EE . TECHNION . AC . IL

that a source (or an existing user) is close or far is unknown. In both cases, important information about the reward structure is not observed. Such would enable to classify similar situations and possibly output much better predictions.
We study in this paper the underlying problem that we call
the latent multi-armed bandit problem (we do not consider
the contextual part of the problem, that is handled by previous work). More formally, let {νa,b }a∈A,b∈B be a set of
real-valued probability distributions, that is indexed by two
finite sets A of items (actions) and B of types. For clarity,
and to highlight the role of latent information, we assume
that both sets are finite. Extension to continuous parametric
settings such as linear contextual-bandit (Abbasi-Yadkori
et al., 2011; Dani et al., 2008) is straightforward. We denote µa,b ∈ R the mean of νa,b and assume νa,b to be Rsub-Gaussian (with known R), that is
∀λ ∈ R log Eνa,b exp (λ(X − µa,b )) 6 R2 λ2 /2 . (1)
At each step n ∈ N, Nature selects some bn ∈ B according to some unknown stochastic process Υ. Then bn is revealed, and we must select some an ∈ A. Finally, a reward
Xn is sampled from νan ,bn and observed. Our goal is to
find for all N a sequence of actions a1:N = {an }16n6N
with maximal cumulated reward. The optimal sequence is
given by {?bn }n∈N where ?b ∈ argmaxa∈A EX∼νa,b [X].
The expected regret of an algorithm A that produces a sequence of actions a1:N is then simply defined by
N
N
h i X
h i
X
RA
=
E
X
−
E
Xn .
X
∼ν
n
X
∼ν
N
n
n
?b ,bn
an ,bn
n

n=1

n=1

We model the latent information by assuming that B is partitioned into C clusters C = {Bc }c=1,...,C such that the
distributions {νa,b }a∈A are the same for each b ∈ Bc . This
common distribution is denoted νa,c and called a cluster
distribution. We denote the optimal action in Bc by ?c , and
introduce the optimality gaps ∆a,c = µ?c ,c − µa,c . Both
the partition and the number of clusters are unknown.
In the recommender system example, B would be the set
of Ids of users having a same context, partitioned for instance into C = 4 groups according to whether the user is
a Male/Female and has High/Low income. In the cognitive

Latent Bandits

radio scenario, B could represent hours of the day, partitioned into C = 23 parts according to three local radios
being active or not1 .
Previous work In (Agrawal et al., 1989) and more recently
in (Salomon & Audibert, 2011) the case when all cluster distributions are known and all users b come from the
same unknown cluster c is considered. In this already nontrivial setting, (Agrawal et al., 1989) provided an asymptotic lower bound that significantly differs from the standard lower bound known for the multi-armed bandit problem (Lai & Robbins, 1985; Burnetas & Katehakis, 1996),
thus showing that the problem is intrinsically different from
a bandit problem. They also analyze a near-optimal (yet
costly) algorithm for that problem. In (Salomon & Audibert, 2011), a simpler algorithm is introduced and analyzed
with less tight guarantee. We contribute to that setting in
Section 2 with a tighter regret bound for a simple algorithm. We then consider two challenging extensions. In
Section 3 users may come from different (instead of one)
clusters, and in Section 4 nothing is known about the environment. These new settings could be loosely related to
(Slivkins, 2011) and (Hazan & Megiddo, 2007).
Contribution In Section 2, we review the important case
when the cluster distributions {νa,c }a∈A,c∈C are known,
and all users come from the same cluster c. We provide intuition about the setting, introduce a new algorithm called
Single-K-UCB that is computationally less demanding
than that of (Agrawal et al., 1989), and prove an explicit
finite-time bound (Theorem 4) on its regret, improving on
(Salomon & Audibert, 2011).
In Section 3, we analyze the significantly harder and largely
unaddressed setting when the cluster distributions are still
known, but the users may now come from all clusters. We
provide a lower bound (Theorem 5) showing that when the
number of clusters is too large with respect to the time horizon, sub-linear regret is not attainable. We introduce an algorithm called Multiple-K-UCB and prove a non-trivial
regret bound (Theorem 6) that makes explicit the effect of
the distribution of users Υ on the regret.
In Section 4, we target the challenging setting when nothing is known (neither Υ, the cluster distributions, nor even
the number of clusters). We provide regret bounds for
benchmark UCB-like algorithms (Theorem 7), and a new
algorithm called A-UCB. Despite the very general setting
and poor available information, we are able to prove a
weak result (Proposition 1), that enables us to deduce a
regret guarantee under mild conditions on the structure of
arms (Lemma 1,2). Numerical simulations show in Section 4.2 that the introduced algorithm achieves excellent
performance in a number of hard situations. All proofs
are provided in the extended version (Maillard & Mannor,
2013).
1

We assume that radios are active at the same time everyday.

Notations. At round n, we denote the number
Pn of observations for the pair (a, b) by Na,b (n) =
t=1 I{at =
a, bt = b} and use νba,b (n) and µ
ba,b (n) to denote the empirical distribution and empirical mean built from the same
observations,
respectively. We also introduce Nb (n) =
P
a∈A Na,b (n). For observations associated to the pair
a, b, we denote Ua,b (n) a high probability upper bound on
the mean µa,b , and La,b (n) a high probability lower bound.
Unless specified, in the sequel we choose the following
Ua,b (n) coming from concentration inequality for R-subGaussian variables (see (1)), and define La,b (n) symmetris
cally:
2 log(Nb (n)3 )
.
Ua,b (n) = µ
ba,b (n) + R
Na,b (n)
One could instead use Hoeffding’s inequality if the distributions have bounded support, empirical Bernstein’s inequality to take the variance into account, self-normalized
concentration inequality such as in (Garivier & Moulines,
2008; Abbasi-Yadkori et al., 2011), or even tighter upper
bounds based on Kullback-Leibler divergence as explained
in (Cappé et al., 2013). These would lead to slightly improved constants in the regret bounds, at the price of clarity.
Thus we focus here on bounds based on the mean only. Let
the confidence set be Sa,b (n) = [La,b (n), Ua,b (n)] and its
size (the gap) be Ga,b (n) = Ua,b (n) − La,b (n). To avoid
some technical considerations, we assume that Sa,b (n) is
centered around µ
ba,b (n).

2. Known cluster distributions with single
cluster arrivals.
In this section, we consider the case when all the distributions {νa,c }a∈A,c∈C are known and arrivals {bn }n>1 belong to the same unknown cluster c ∈ C. The difference
from a standard multi-armed bandit problem is that the set
of possible distributions is finite and known. We can have
for instance three arms, two clusters and Bernoulli distributions of respective parameter 0.2, 0.6, 0.8 for one cluster,
and Bernoulli distributions of parameter 0.8, 0.1, 0.5 for the
second one. This modifies the achievable guarantees:
Theorem 1 (Agrawal et al. (1989)) Let c ∈ C be the true
class (that is supp(Υ) ⊂ Bc ), and A− = A \ {?c } be the
set of sub-optimal arms. Then, a lower performance
bound
X
is
ωc,a ∆a,c
RN
a∈A−
lim inf
> min
max X
,
N →∞ log(N ) ωc ∈P(A− ) c0 ∈C(c)
ωc,a KL(νa,c ||νa,c0 )


a∈A−
where C(c) = c0 ∈ C : ν?c ,c0 = ν?c ,c and ?c 6= ?0c .
Theorem 2 (Agrawal et al. (1989)) For each c ∈ C, let
ωc? that achieves the minimum in the lower bound of Theorem 1. The algorithm proposed by (Agrawal et al., 1989)
makes use of {ωc? }c∈C and achieves
P


?
a∈A− ωc,a ∆a,c
+o(1)
log(N ) .
RN 6 0max P
?
0
c ∈C(c)
a∈A− ωa,c KL(νa,c ||νa,c )

Latent Bandits

Although theoretically appealing, it may be in general expensive to compute the quantities {ωc? }c∈C , which makes
the algorithm less practical. On the other hand, (Salomon
& Audibert, 2011) introduced the GCL algorithm, seemingly without being aware of the work of (Agrawal et al.,
1989) and got the following non-asymptotic result:
Theorem 3 (Salomon & Audibert (2011)) Assume
that for all c, c0 ∈ C, for all a ∈ A, then either
νa,c 6= νa,c0 or (either ?c 6= a or ?0c = a), or
dν 0
∃a0 6= a : Pνa0 ,c ( dν a0 ,c0 (X) > 0) = 0. Then if
a ,c
c ∈ C with unique best arm is the true environment, then
for all β > 0 it holds for some constants C, C 0 that

X
log(n)
6 C 0 n−β .
Na,b (n) > C 2
∀n∀a 6= ?c P
∆a,c
b∈Bc
GCL is fairly easy to implement, however the way this
bound is stated makes it hard to understand, all the more
so that the constants are not explicit. Also the dependency
with ∆2a,c seems sub-optimal.

Algorithm 1 The Single-K-UCB algorithm.
Require: The cluster distributions {νa,c }a∈A,c∈C .
1: for n = 1...N do
2:
Receive bn ∼ Υ.
3:
Define n the
set
of
admissible
classes
o
Cn−1 = c ∈ C : ∀a ∈ A µa,c ∈ Sa,B (n − 1) .
4:
Define the set of “elite” admissible arms
A?n−1 = {a ∈ A; ∃c ∈ Cn−1 ?c = a}.
5:
Choose the next arm (breaks ties with round-robin)
an = argmax µ?c ,c .
(2)
a=?c ,c∈Cn−1
6: end for

For completeness, we now introduce an efficient algorithm
directly inspired from Agrawal’s work. The price for the
reduced complexity is that we lose the asymptotic optimality. We start with some intuition about our setting.

∆+
a,c

High level intuition For clarity, we focus
n on means only
(instead of distributions). Let Cn−1 = c ∈ C, ∀a ∈ A :
o
µa,c ∈ Sa,B (n − 1) be the set of admissible classes at
round n − 1, where the confidence set Sa,B (n − 1) is built
using observations for the pairs {(a, b)}b∈B . Note that by
concentration of measure, with high probability the true
class c is admissible and thus Cn−1 is not empty. Let then
c̃ ∈ Cn−1 be an admissible class. It makes sense to pull its
optimal arm ?c̃ = argmaxa∈A µa,c̃ (that is known). Now
several situations may occur:
a) For another class c0 ∈ C, if |µ?c̃ ,c0 − µ?c̃ ,c̃ | > Ga,B (n −
1), then c0 cannot be admissible. Now if when c0 is admissible then ?c̃ = ?c0 , it means that choosing to play ?c̃
for c̃ ∈ Cn−1 is safe (that is ?c̃ = ?c happens with high
probability).
b) If ∃c0 ∈ C such that both |µ?c̃ ,c0 − µ?c̃ ,c̃ | 6 Ga,B (n − 1)
and ?c̃ 6= ?c0 , there are many admissible classes that lead
to different actions to play. The situation is tricky since
playing arm ?c̃ does not separate c̃ from c0 (it may be that
ν?c̃ ,c̃ = ν?c̃ ,c0 ), and may moreover be sub-optimal since we
may have ?c̃ 6= ?c .
Algorithm (Agrawal et al., 1989) uses a fancy procedure to
handle case b). Here, we note that if we choose the class c̃
(and thus action ?c̃ ) with maximal best mean, this ensures
that µ?c ,c − µ?c̃ ,c 6 µ?c̃ ,c̃ − µ?c̃ ,c and thus a controlled
error. This observation leads to the Single-K-UCB algorithm, whose pseudo-code is provided in Algorithm 1.
Straightforwardly, if Cn−1 is empty, it reduces to playing
round-robin, in case a), A?n−1 is a singleton, and in case b),
we have a controlled error.

Regret bound Such an algorithm enjoys the following regret performance:
Theorem 4 The regret of Single-K-UCB satisfies

X 24R2 ∆a,c log(N )
π2 
,
RSingle-K-UCB
6
+ ∆a,c 1+
N
+2
3
∆a,c
?


a∈A

?

where A =

a ∈ A : ∃c ∈ C s.t. ?c = a and
n
o
0
0
0
= inf
µ
−
µ
:
?
=
a
∩
µ
>
µ
a,c
a,c
c
?c0 ,c
?c ,c .
0
c ∈C

+
The notation ∆+
a,c comes from the fact that ∆a,c > ∆a,c .
Note the link between this bound and that of Theorem 2
(also ∆+
a,c and C(c)). Of course the bound of Theorem 2
can be better and this seems to be the price for the simplicity of Single-K-UCB. On the other hand, since Theorem 4 scales with ∆+
a,c (which can be arbitrarily larger
than ∆a,c ; see Figure 1), it improves on the result of Theorem 3, and moreover provides explicit constants. Finally,
it is straightforward to improve the constants using tighter
confidence bounds as discussed in the introduction.

3. Known cluster distributions with multiple
cluster arrivals.
We now turn to the more challenging case when the distributions {νa,c }a∈A,c∈C are still known to the learner, but
when the users may come from different clusters, and the
learner does not know what class c corresponds to some input b ∈ B. In this setting, the lower bound from Theorem 1
can be strengthen. Indeed, without further assumptions, it
may be the case that if the number of clusters C is too large
with respect to the time horizon N , we don’t have time to
learn and we can not ensure to have sub-linear regret:
Theorem 5 Let Υ be the uniform distribution over B and
consider that the distributions are partitioned exactly into
C > A groups of equal size. Then, it holds
√
1
inf sup RN >
min{ N AC, N } .
algo νa,c
20
This shows that for the scaling C = Ω(N ) the problem becomes hopeless, since for any bandit algorithm there exists
a set of distributions {νa,b }a∈A,b∈B such that the regret is
linear in N .
Despite this difficulty, it is possible to slightly modify
Single-K-UCB for that setting, which leads to algorithm 2 that enjoys the following regret performance.

Latent Bandits

Algorithm 2 The Multiple-K-UCB algorithm.
Require: The cluster distributions {νa,c }a∈A,c∈C .
1: for n = 1...N do
2:
Receive b = bn ∼ Υ.
3:
Define
the
set
of
admissible
classes
n
o
Cn−1 (b) = c ∈ C, ∀a ∈ A : µa,c ∈ Sa,b (n − 1) .
4:
Define the set of “elite” admissible arms
A?n−1 = {a ∈ A; ∃c ∈ Cn−1 (bn ) ?c = a}.
5:
Choose the most optimistic “elite” arm
an =
argmax
µ?c ,c .
a=?c , c∈Cn−1 (bn )
6: end for
Theorem 6 The regret of Multiple-K-UCB satisfies

X X
24R2 ∆a,cb log(N Υ(b))
Multiple-K-UCB
RN
6
min
2
∆+
a,cb
b∈B a∈A?


−1
+ O Υ(b)
, ∆a,cb N Υ(b) ,
where cb ∈ C denotes the class corresponding to b ∈ B.
In order to see the benefit of knowing the distributions
{νa,c }a∈A,c∈C , a natural benchmark algorithm is the one
that simply plays independent copies of UCB on each b ∈ B
(see (Auer, 2003)), without using the knowledge of the
cluster distributions. We call this algorithm UCB on B; see
Algorithm 3. Importantly, due to the inequality ∆+
a,cb >
∆a,cb and because only elite arms a ∈ A? are pulled, the
regret of Multiple-K-UCB is never worse than that of
UCB on B (Theorem 7); it can potentially be much smaller.
Algorithm 3 The UCB on B algorithm
1: for n = 1...N do
2:
Receive bt ∼ Υ.
3:
Compute the empirical means µ
ba,b (n − 1).
4:
Choose the next arm (breaks ties arbitrary)
an = argmax Ua,bn (n − 1) .
(3)
5: end for
a∈A
Illustration In order to highlight the role played by ∆+
a,c ,
Figure 1 depicts the upper-bounds from Theorem 6 and and
from Theorem 7, for one randomly generated problem (we
do not compare the regret, but the bounds, to emphasize the
theoretical gap). For clarity, we reported the values of ∆+
a,c
as well as of the optimality gaps ∆a,c for each arm and
each class. Here three arms that may be pulled by UCB on
B are never pulled by Multiple-K-UCB. Note that the
improvement can sometimes be huge: for instance when all
?c are equal, then ∆+
a,c = ∞ for all sub-optimal arm and
the bound from Theorem 6 equals zero.

4. The agnostic case.
In Sections 2 and 3, using the knowledge of the cluster distributions, we derived regret bounds that may significantly
improve on their equivalent agnostic version. We now detail an improvement that is even more effective and applicable both in case cluster distributions are known or not.
We first note that using estimates from each distributions
νa,b separately in order to decide the best action for the

Figure 1. Theoretical regret bounds for Multiple-K-UCB
(Theorem 6) and UCB on B (Theorem 7) for one problem characterized by |A| = 3, |B| = 50, |C| = 4 and
1
2
3
4
µa,c : 1
0.527
0.209 0.713 0.762
2
0.717
0.193 0.575 0.230
3
0.669
0.751 0.120 0.485
∆+
0.235
0.553
0.0
0.0
a,c : 1
2
0.0
+∞
0.142
+∞
3
0.082
0.0
0.631
+∞
∆a,c : 1
0.190
0.542
0.0
0.0
2
0.0
0.558 0.138 0.533
3 0.0475
0.0
0.593 0.277

cluster c(b) = c seems sub-optimal since the number of
samples Na,b (n) available for the couple (a, b) is typically
small, while we could possibly gain much more by using
all observations in each Bc (This is basically what happens
in Section 2). Indeed, if two distributions νa,b and νa,b0
are the same, then grouping the corresponding observations
provides a faster convergence speed. In general, grouping
subsets of {νa,b }b∈B may lead to a dramatic speed-up if we
group similar distributions, and may create a bias if they
significantly differ. Thus, there is a trade-off between getting fast versus accurate convergence, and it is a priori not
clear whether we can get a provable improvement.
Benchmark We now introduce a (pseudo-)oracle that
knows the identity of the clusters perfectly. The simplest
one is an algorithm that runs a version of UCB separately
on each group Bc (and not each b). We call this benchmark
UCB on C. Note that although it knows the clusters this is
not the best oracle: In some cases, it may be better to further group some clusters together. This algorithm is easy to
analyze. To understand the kind of improvement we are targeting, the following theorem compares the regret of UCB
on B, to that of the pseudo-oracle UCB on C.
Theorem 7 The expected regret at time N of the algorithm
UCB on B is upper bounded by
n 24R2 log(N Υ(b))
XX
on B
RUCB
6
min
N
∆a,b
b∈B a∈A


o
+ O Υ(b)−1 , ∆a,b N Υ(b) ,
where ∆a,b = µπ? (b),b − µa,b is the optimality gap of arm
a for environment b. Similarly, the expected regret at time
N of UCB on C is upper bounded by

Latent Bandits

UCB on C
RN

6

C X
X

n 24R2 log(N Υ(B ))
c
∆
a,c
c=1 a∈A


o
+ O Υ(Bc )−1 , ∆a,c N Υ(Bc ) ,
min

where ∆a,c is the common value of the ∆a,b for b ∈ Bc .
As a result, the regret of UCB on C can be substantially
smaller than the one of UCB on B. Indeed, only looking at
the term in factor of log(N ), we get an improvement going
P
P
PC P
−1
from b∈B a∈A ∆−1
c=1
a∈A ∆a,c , that can be
a,b to
substantial, since typically C is much smaller than B. Note
of course that the partition C is unknown in practice. We
emphasize that the lower bound of Theorem 5 also holds
for that setting.
Grouping distributions We now detail the improvement
we are going to use. Let B ⊂ B. We define, similarly to
µ
ba,b (n), La,b (n) and Ua,b (n) the empirical group estimate
νba,B (n) with associated group mean µa,B (n), confidence
intervals Ua,B (n), La,B (n) and set Sa,B (n), where
P
ba,b0 (n)Na,b0 (n)I{b0 ∈ B}
b0 ∈B ν
P
,
νba,B (n) =
0
0
b0 ∈B Na,b (n)I{b ∈ B}
P
0
0
0
b0 ∈B µa,b Na,b (n)I{b ∈ B}
P
µa,B (n) =
.
0
0
b0 ∈B Na,b (n)I{b ∈ B}
Note that for B = Bc , then µa,Bc (n) = µa,c , which
may not hold for other sets B since there may be a bias
when the {µa,b0 }b0 ∈B are distinct. However, the speed
of
P convergence of 0 the group depends on Na,B (n) =
0
b0 ∈B Na,b (n)I{b ∈ B}, which is typically much faster
than that of a single point b (that depends on Na,b (n)).
Thus Sa,B (n) = [La,B (n), Ua,B (n)] is potentially much
smaller than Sa,b (n). Finally, note that, by construction,
we have µa,B (n) ∈ Sa,B (n) with high probability, but that
for some b ∈ B there is no reason that µa,b ∈ Sa,B (n) due
to the introduced bias.
In order to leverage the estimation bias, we restrict possible
groups B, using two observations. First, if µa,b = µa,b0 ,
then we must have Sa,b (n) ∩ Sa,b0 (n) 6= ∅ with high probability. More generally, some B such that µa,b = µa,b0
for all b, b0 ∈ B, must satisfy that for all B 0 ⊂ B and all
B 00 ⊂ B, with high probability, Sa,B 00 (n) ∩ Sa,B 0 (n) 6= ∅.
Second, we define for an adaptive ε = εa,b,b0 ,n the enlarged
confidence bounds

Ua,b (n; 1 + ε) = µ
ba,b (n) + (1 + ε) Ua,b (n) − µ
ba,b (n) ,

La,b (n; 1 + ε) = µ
ba,b (n) − (1 + ε) µ
ba,b (n) − La,b (n) ,
and then Sa,b (n; 1+ε) = [La,b (n; 1+ε), Ua,b (n; 1+ε)]. Now,
if µa,b = µa,b0 and Ga,b0 (n) 6 2ε Ga,b (n)2 , we must have
Sa,b0 (n) ⊂ Sa,b (n; 1 + ε) with high probability. Finally,
we focus only on mean-based procedures for clarity, but it
2

This is because we restrict to confidence interval centered around µ
ba,b (n); in general we would need Ga,b0 (n) 6
ε min{Ua,b (n) − µ
ba,b (n), µ
ba,b (n) − La,b (n)} .

is possible to use empirical distributions νba,b (n) to remove
b0 with obvious mismatch in Kullback-Leibler divergence.
We do not discuss this.
All in all, we define two sets of sets: First Bb (n) for compatible sets, and then B+
b (n) for maximally compatible
(or “elite”) sets, that have maximal group speed of convergence and a controlled bias:

def
Bb (n) = B ⊂ B: ∀a ∈ A∀b0,b00∈B Sa,b0 (n)⊂Sa,b00 (n; 1+ε)

0
00
00
0
∩ b ∈ B ∩ ∀B , B ⊂ B, Sa,B (n) ∩ Sa,B (n) 6= ∅ ,
def

B+
b (n) = Argmax B

(for the relation ⊂ ) .

(4)

B∈Bb (n)

(Note that Argmax returns a set, contrary to argmax.)
4.1. The Agnostic UCB for clustered-bandits.
We are now ready to introduce A-UCB, whose pseudo-code
is provided as Algorithm 4.
Proving strong regret bounds in this agnostic setting is difficult without further assumptions, since the true class may
change at each single time step. For that reason, we proceed in two steps: Proposition 1 controls the number of
pulls of sub-optimal arms under some events, that we then
handle in specific cases.
Algorithm 4 The A-UCB algorithm
Require: Parameter γ.
1: for n = 1...N do
2:
Receive bn ∼ Υ,
3:
Compute µ
ba,b (n − 1), then Ua,b (n − 1), La,b (n −
1), Sa,b (n − 1) and Ga,b (n − 1).
4:
Define the quantity ε = εbn ,b0 ,n−1 by
s

2γ log(Nb0 (n − 1))
max
− 1, 0 .
log(Nbn (n − 1))
5:
Compute the set B+
bn (n − 1) of maximally compatible aggregation sets via (4).
6:
Pull an elite arm that is the most optimistic
Ua,B (n − 1)
(5)
an ∈ argmax
max
+
7: end for

a∈A

B∈Bbn (n−1)

n
o
Proposition 1 Let Ωn = Bcn ∈ Bbn (n − 1) be the
event that
n the true class cn is admissible
o at round n, and
Enα = G?cn ,Bcn (n − 1) < α∆an ,cn the event that the
confidence interval of the optimal arm of cluster Bcn is
small enough, for small α ∈ (0, 1). Then,3 for a suboptimal an , under Ωn ∩ Enα and for all η ∈ (α, 1],

ε 2 24R2 log(Nbn (n − 1))
,
either Nan ,bn(n − 1) < 1+
2
(η − α)2 ∆2an ,cn
or Nan ,Bcn(n − 1) <
3

24R2 log(NBcn (n − 1))
m.
(1 − η)2 ∆2an ,cn

In section C.2 of the extended version (Maillard & Mannor,
2013), we show a slightly stronger result, though more difficult to
interpret.

Latent Bandits

That is, in all cases the total number of pulls, for either the
current user bn or its class cn , of a chosen sub-optimal arm
is controlled.
In particular for small ε, α and η → 1, Proposition 1 shows
that under Ωn ∩ Enα the regret of A-UCB is essentially in
between that of UCB on B and UCB on C: up to constants,
it is never worse than UCB on B, and can be significantly
better by competing occasionally with UCB on C. This is
highlighted on Figure 5. It now remains to show that Ωn ∩
Enα happens with high probability in order to deduce a nontrivial regret bound.
Illustration Ωn is the event that the true class cn is admissible at round n,. Now the event Enα essentially says that
N
P?cn ,Bcn (n − 1) > O(log(n)), that is, since N?c ,Bc (n) =
b∈Bc N?c ,b (n), it is enough that one N?cn ,b (n) be as
large to ensure that Enα happens. For illustration, let us
turn to the case of Bernoulli distributions (R = 1/2) with
C = 4 equally probable classes of equal size B = 50. Individual upper bound confidence bounds Ua,b (25000) are
non trivial (i.e. less than 1) if (a, b) is seen at least 15
times. Now if each pair (?c , b) for b ∈ Bc is visited at least
15 times (out of the ' 125 available time steps for each
b ∈ Bc ) then G?c ,Bc (25000) < 0.27, and for 50 visits, the
bound reduces to 0.145. Similarly, for B = 250 we get
abound 0.12 with 15 visits of the optimal action, which is
enough to ensure that Enα happens in non-trivial situations.
Of course these numbers can be significantly reduced by
using better confidence bounds (see (Abbasi-Yadkori et al.,
2011)). Let us now provide conditions under which both
Enα and Ωn happen.
Adaptive enlargement The reason for having an adaptive
ε and not just a constant ε = 1 is that a constant ε does
not always ensure that Bcn is admissible (that is Ωn happens) with high probability, but only that a subset of Bcn
is admissible at round n. To better understand the number
of such points that are gathered in Sa,b (n; 1 + ε) we introduce the following quantity, that only depends on the law
of arrivals Υ:
Definition 1 The γ-balance of B with respect to cluster c,
for point b ∈ Bc is defined by


0
0
Bc (b; γ) = b ∈ Bc : Υ(b) 6 γΥ(b ) .
Together with this quantity, it is natural to introduce the
distortion factor of group Bc , defined by
maxb∈Bc Υ(b)
.
minb∈Bc Υ(b)
These quantities enable us to quantify the effective number of points that are grouped with b ∈ B, which directly
defines the speed-up the algorithm can achieve for this environment. Importantly, note that if γ > γc , then it holds
that Bc (b; γ) = Bc for all b ∈ Bc . A-UCB uses an adaptive ε that ensures that if γ is essentially greater than γc ,
γc =

then Bc (b; γ) and thus Bc is admissible with high probability (but one should choose a small γ since the regret scales
with γ); more precisely
Lemma 1 In A-UCB, if γ is chosen such that γ > γc +
O n−1/2 , then it holds that


X
P(Ωn ) > 1 − O n−2 A
Υ(b)−2 − 2|B|n−2 .
b∈B

Such a O(n−2 ) control is standard in regret proofs.
Ensuring the optimal arm is pulled enough We now turn
to Enα . In full generality, there is no reason that A-UCB
makes Enα happen. The following lemma however ensures
that under a mild condition on the structure of the problem,
this actually holds with high probability. A simple regret
bound follows trivially.
Lemma 2 Let us assume that Υ is the uniform distribution,
that all clusters have the same size B0 , and that the cluster
distributions satisfy ∀c, c0 ∈ C ∀a ∈ A
3
either µ?c ,c −µ?c ,c0 <∆a,c /2 or µ?c ,c −µ?c ,c0 > ∆a,c .
2
(That is, a mismatch between two classes is either clear or
harmless.) In such a case, if A-UCB is run with γ ∼ γc =
1, then P(Enα ) > 1 − O(n−2 ) holds for α = 1/2.
Combining Proposition 1 together with Lemma 1 and
Lemma 2, we deduce that, in some specific situations we
are able to control with high probability the number of pulls
of a sub-optimal arm, and as a result, the regret of the considered strategy. We currently do not know how to extend
the analysis to handle the most general case.
4.2. Numerical experiments
In this section, we study the behavior of the algorithm
A-UCB on some experiments.
Algorithms We use the vanilla version of UCB (that aggregates all contexts), UCB on B that is the naive application of UCB separately on each context, and the pseudooracle UCB on C. We implemented a simplified version of
A-UCB where we do not compute the maximally compatible sets exactly (which is NP-hard in general), but average the means of the compatible sets instead. This slightly
worsen the numerical constants in our results, even though
characterizing entirely the effect of this relaxation in terms
of regret and numerical efficiency goes beyond the scope
of this paper.
Experiments We consider experiments with Bernoulli distributions: this is intuitively the hardest case, since one can
only rely on the means to separate distributions; it also
appears in several applications. For each experiment, we
show the number of actions |A|, of users |B|, of classes |C|,
and the parameters {µa,c }a∈A,c∈C when there are not too
many. We plot the regret of all algorithms on the same figure: A thick line is used for the mean regret and dashed
lines for quantiles at levels 0.25, 0.5, 0.75, 0.95 and 0.99.
In all experiments, the parameters {Υ(b)}b∈B are defined

Latent Bandits

P
by Υ(b) = wb / b∈B wb , where the weights wb are drawn
uniformly randomly in [0.1, 0.9]. Thus for each class, the
distortion factor γc is less than 9, and we set the parameter
γ of A-UCB to the value γ = 9. For one experiment with
given fixed parameters, the algorithms are run over several
trials (500) for a large time horizon N = 25000. We do
not report the values of {Υ(b)}b∈B since this is generally
uninformative.

Figure 2 presents an expected situation, where both the
naive UCB and UCB on B perform poorly with respect to the
pseudo-oracle, whereas A-UCB performs very well. Note
that here the best arm is different in the different classes,
with corresponding value that is always very high and well
separated from other arms.
Figure 3 presents a tricky situation: UCB on B performs
poorly, while both A-UCB compete with the pseudo-oracle,
and all are defeated by UCB, which is not surprising since
here one arm is the best in all contexts.
Figure 4 presents a variant when A is large. As expected
the performance of all algorithms degrade, but A-UCB
is still competitive with respect to the pseudo-oracle and
benchmark algorithms.

Figure 2. Regret of several algorithms in the following scenario
with |A| = 3, |B| = 50, |C| = 4 and
µa,c
1
2
3
4
1
0.527 0.209 0.713 0.762
2
0.717 0.193 0.575 0.230
3
0.669 0.751 0.120 0.485
Figure 4. Regret of several algorithms in some randomly generated situation with |A| = 50, |B| = 50, |C| = 4.

Figure 5 presents a variant when B is large. Note that in
this experiment, one only gets to see each b about 50 times,
this setting is thus challenging. It can be seen that A-UCB
still works fairly decently in this case. In accordance with
Proposition 1, let us also remark that here A-UCB behaves
initially like UCB on B, and progressively behaves like UCB
on C(though with a shifted regret due to the initial phase).
Finally figure 6 presents a variant when C is large. A-UCB
still competes with the pseudo-oracle here.

Figure 3. Regret of several algorithms in the following scenario
with |A| = 3, |B| = 50, |C| = 4 and
µa,c
1
2
3
4
1
0.370 0.750 0.609 0.207
2
0.150 0.290 0.475 0.464
3
0.671 0.897 0.781
0.9

In all these experiments, we see that A-UCB consistently
competes with UCB on C, while UCB and UCB on B sometimes obtain poor regret. This indicates that the proposed
strategy is essentially able to capture the right information
and does not under nor over-group the inputs b.

Latent Bandits

5. Discussion
We introduced a novel setting for sequential decision making problem where there are some latent variables, such
as recommender systems, cognitive radio networks and
others. We provided several contributions in a general
framework in order to precisely address the issues raised
by the latent structure. As a result, our contribution can
be straightforwardly applied for instance to the linearbandit setting (see Abbasi-Yadkori et al. (2011); Dani et al.
(2008)), where the number of actions is replaced with the
dimension of a feature space, and confidence intervals with
confidence ellipsoids, and potentially many others.
Let us remark that we assumed in this work that the reward
distributions are clustered, that is each νa,b is one of the
{νa,c }c . A natural extension is to consider the case when
each νa,b is a mixture of the {νa,c }c , with an underlying
low-rank structure. This is left for future research.
Figure 5. Regret of several algorithms in the following scenario
with |A| = 3, |B| = 500, |C| = 4 and
µa,c
1
2
3
4
1
0.1
0.621
0.1
0.362
2
0.544 0.697 0.554 0.181
3
0.512 0.409 0.234
0.1

In the non-trivial setting of Section 2, we showed that
a simple procedure improves on (Salomon & Audibert,
2011) on the theoretical side and on (Agrawal et al., 1989)
on the computational side. We then introduced the more
challenging setting of Section 3, that has not been addressed previously, and extended our procedure to that setting. We provided a lower-bound explaining why the setting is challenging and then a non trivial regret bound that
makes appear explicitly the role of the distribution Υ of
arrivals.
We finally tackled the agnostic setting, when not even the
number of clusters is known. We introduced an algorithm
that demonstrates excellent performance on a number of
difficult situations, and provided a result enabling to derive
regret guarantees in some non-trivial situation. We leave
the intricate question of extending Lemma 1 and 2 to the
fully general case as an open problem.
Acknowledgements This work was supported by
the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement 306638
(SUPREL) and the Technion.

Figure 6. Regret of several algorithms in some randomly generated situation with |A| = 3, |B| = 100, |C| = 50.

Latent Bandits

References
Abbasi-Yadkori, Yasin, Pál, Dávid, and Szepesvári, Csaba.
Improved algorithms for linear stochastic bandits. In
Shawe-Taylor, John, Zemel, Richard S., Bartlett, Peter L., Pereira, Fernando C. N., and Weinberger, Kilian Q. (eds.), Advances in Neural Information Processing Systems, pp. 2312–2320, 2011.
Adomavicius, Gediminas and Tuzhilin, Alexander. Toward
the next generation of recommender systems: A survey
of the state-of-the-art and possible extensions. IEEE
Trans. on Knowl. and Data Eng., 17(6):734–749, jun
2005. ISSN 1041-4347.
Agrawal, Rajeev, Teneketzis, Demosthenis, and Anantharam, Venkatachalam. Asymptotically Efficient Adaptive Allocation Schemes for Controlled I.I.D. processes.
IEEE Transactions on Automatic Control, 34(3):258–
267, 1989.
Auer, Peter. Using confidence bounds for exploitationexploration trade-offs. Journal of Machine Learning Research, 3:397–422, mar 2003. ISSN 1532-4435.
Avner, Orly, Mannor, Shie, and Shamir, Ohad. Decoupling
exploration and exploitation in multi-armed bandits. In
Proceedings of the 29th International conference on Machine Learning. Omnipress, 2012.
Burnetas, Apostolos N. and Katehakis, Michael N. Optimal adaptive policies for sequential allocation problems.
Adv. Appl. Math., 17(2):122–142, jun 1996. ISSN 01968858.
Cappé, Olivier, Garivier, Aurélien, Maillard, OdalricAmbrym, Munos, Rémi, and Stoltz, Gilles. Kullback–
leibler upper confidence bounds for optimal sequential
allocation. Ann. Statist., 41(3):1516–1541, 2013. ISSN
0090-5364.
Dani, Varsha, Hayes, Thomas P., and Kakade, Sham M.
Stochastic Linear Optimization under Bandit Feedback. In Servedio, Rocco A., Zhang, Tong, Servedio,
Rocco A., and Zhang, Tong (eds.), COLT, pp. 355–366.
Omnipress, 2008.
Filippi, Sarah, Cappé, Olivier, Cérot, Fabrice, and
Moulines, Eric. A near optimal policy for channel allocation in cognitive radio. In Girgin, Sertan, Loth,
Manuel, Munos, Rémi, Preux, Philippe, and Ryabko,
Daniil (eds.), Recent Advances in Reinforcement Learning, volume 5323 of Lecture Notes in Computer Science,
pp. 69–81. Springer Berlin Heidelberg, 2008.
Garivier, Aurélien and Moulines, Eric.
On UpperConfidence Bound Policies for Non-Stationary Bandit
Problems. ArXiv e-prints, may 2008. Technical report,
LTCI.

Hazan, Elad and Megiddo, Nimrod. Online learning with
prior knowledge. In Proceedings of the 20th annual
conference on Learning theory, COLT’07, pp. 499–513,
Berlin, Heidelberg, 2007. Springer-Verlag.
Lai, Tze L. and Robbins, Herbert. Asymptotically efficient
adaptive allocation rules. Advances in Applied Mathematics, 6(1):4–22, 1985.
Langford, John and Zhang, Tong. The Epoch-Greedy
Algorithm for Multi-armed Bandits with Side Information. In Platt, John C., Koller, Daphne, Singer, Yoram,
Roweis, Sam T., Platt, John C., Koller, Daphne, Singer,
Yoram, and Roweis, Sam T. (eds.), NIPS. MIT Press,
2007.
Li, Lihong, Chu, Wei, Langford, John, and Schapire,
Robert E. A contextual-bandit approach to personalized news article recommendation. In Proceedings of
the 19th international conference on World wide web,
WWW ’10, pp. 661–670, New York, NY, USA, 2010.
ACM. ISBN 978-1-60558-799-8.
Li, Lihong, Chu, Wei, Langford, John, and Wang, Xuanhui.
Unbiased offline evaluation of contextual-bandit-based
news article recommendation algorithms. In Proceedings of the fourth ACM international conference on Web
search and data mining, WSDM ’11, pp. 297–306, New
York, NY, USA, 2011. ACM. ISBN 978-1-4503-0493-1.
Lu, Tyler, Pál, Dávid, and Pal, Martin. Contextual multiarmed bandits. Journal of Machine Learning Research Proceedings Track, 9:485–492, 2010.
Maillard, Odalric-Ambrym and Mannor, Shie. Latent bandits. HAL/Open archive, 2013. URL http://hal.
inria.fr/hal-00926281.
Salomon, Antoine and Audibert, Jean-Yves. Deviations
of stochastic bandit regret. In Proceedings of the 22nd
international conference on Algorithmic learning theory, ALT’11, pp. 159–173, Berlin, Heidelberg, 2011.
Springer-Verlag. ISBN 978-3-642-24411-7.
Slivkins, Aleksandrs. Contextual bandits with similarity information. In Proceedings of the 24th annual conference
on Learning theory, COLT’11. Springer-Verlag, 2011.

