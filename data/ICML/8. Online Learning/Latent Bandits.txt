Latent Bandits.
Odalric-Ambrym Maillard
ODALRIC - AMBRYM . MAILLARD @ ENS - CACHAN . ORG
The Technion, Faculty of Electrical Engineering 32000 Haifa, ISRAEL
Shie Mannor
The Technion, Faculty of Electrical Engineering 32000 Haifa, ISRAEL

Abstract
We consider a multi-armed bandit problem
where the reward distributions are indexed by
two sets â€“one for arms, one for typeâ€“ and can
be partitioned into a small number of clusters according to the type. First, we consider the setting
where all reward distributions are known and all
types have the same underlying cluster, the typeâ€™s
identity is, however, unknown. Second, we study
the case where types may come from different
classes, which is significantly more challenging.
Finally, we tackle the case where the reward distributions are completely unknown. In each setting, we introduce specific algorithms and derive
non-trivial regret performance. Numerical experiments show that, in the most challenging agnostic case, the proposed algorithm achieves excellent performance in several difficult scenarios.

1. Introduction
In a recommender system (Li et al., 2010; 2011; Adomavicius & Tuzhilin, 2005), an agent must display an ad to each
incoming client, and a context vector summarizes the observed properties of a client, such as its navigation history
or its geographic localization. In a cognitive radio (Avner
et al., 2012; Filippi et al., 2008), an agent must select a
communication channel, based on its current known location and network conditions, while avoiding collision with
other sources (such as radar, WiFI, etc). Both examples
can be analyzed within the contextual-multi-armed bandit framework (Langford & Zhang, 2007; Lu et al., 2010),
where the contexts summarize the information available to
the learner. However, the context alone may not be sufficient to solve these problems optimally: In recommender
systems, information such as gender or salary, is typically
missing (due to privacy). In cognitive radios, information
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

SHIE @ EE . TECHNION . AC . IL

that a source (or an existing user) is close or far is unknown. In both cases, important information about the reward structure is not observed. Such would enable to classify similar situations and possibly output much better predictions.
We study in this paper the underlying problem that we call
the latent multi-armed bandit problem (we do not consider
the contextual part of the problem, that is handled by previous work). More formally, let {Î½a,b }aâˆˆA,bâˆˆB be a set of
real-valued probability distributions, that is indexed by two
finite sets A of items (actions) and B of types. For clarity,
and to highlight the role of latent information, we assume
that both sets are finite. Extension to continuous parametric
settings such as linear contextual-bandit (Abbasi-Yadkori
et al., 2011; Dani et al., 2008) is straightforward. We denote Âµa,b âˆˆ R the mean of Î½a,b and assume Î½a,b to be Rsub-Gaussian (with known R), that is
âˆ€Î» âˆˆ R log EÎ½a,b exp (Î»(X âˆ’ Âµa,b )) 6 R2 Î»2 /2 . (1)
At each step n âˆˆ N, Nature selects some bn âˆˆ B according to some unknown stochastic process Î¥. Then bn is revealed, and we must select some an âˆˆ A. Finally, a reward
Xn is sampled from Î½an ,bn and observed. Our goal is to
find for all N a sequence of actions a1:N = {an }16n6N
with maximal cumulated reward. The optimal sequence is
given by {?bn }nâˆˆN where ?b âˆˆ argmaxaâˆˆA EXâˆ¼Î½a,b [X].
The expected regret of an algorithm A that produces a sequence of actions a1:N is then simply defined by
N
N
h i X
h i
X
RA
=
E
X
âˆ’
E
Xn .
X
âˆ¼Î½
n
X
âˆ¼Î½
N
n
n
?b ,bn
an ,bn
n

n=1

n=1

We model the latent information by assuming that B is partitioned into C clusters C = {Bc }c=1,...,C such that the
distributions {Î½a,b }aâˆˆA are the same for each b âˆˆ Bc . This
common distribution is denoted Î½a,c and called a cluster
distribution. We denote the optimal action in Bc by ?c , and
introduce the optimality gaps âˆ†a,c = Âµ?c ,c âˆ’ Âµa,c . Both
the partition and the number of clusters are unknown.
In the recommender system example, B would be the set
of Ids of users having a same context, partitioned for instance into C = 4 groups according to whether the user is
a Male/Female and has High/Low income. In the cognitive

Latent Bandits

radio scenario, B could represent hours of the day, partitioned into C = 23 parts according to three local radios
being active or not1 .
Previous work In (Agrawal et al., 1989) and more recently
in (Salomon & Audibert, 2011) the case when all cluster distributions are known and all users b come from the
same unknown cluster c is considered. In this already nontrivial setting, (Agrawal et al., 1989) provided an asymptotic lower bound that significantly differs from the standard lower bound known for the multi-armed bandit problem (Lai & Robbins, 1985; Burnetas & Katehakis, 1996),
thus showing that the problem is intrinsically different from
a bandit problem. They also analyze a near-optimal (yet
costly) algorithm for that problem. In (Salomon & Audibert, 2011), a simpler algorithm is introduced and analyzed
with less tight guarantee. We contribute to that setting in
Section 2 with a tighter regret bound for a simple algorithm. We then consider two challenging extensions. In
Section 3 users may come from different (instead of one)
clusters, and in Section 4 nothing is known about the environment. These new settings could be loosely related to
(Slivkins, 2011) and (Hazan & Megiddo, 2007).
Contribution In Section 2, we review the important case
when the cluster distributions {Î½a,c }aâˆˆA,câˆˆC are known,
and all users come from the same cluster c. We provide intuition about the setting, introduce a new algorithm called
Single-K-UCB that is computationally less demanding
than that of (Agrawal et al., 1989), and prove an explicit
finite-time bound (Theorem 4) on its regret, improving on
(Salomon & Audibert, 2011).
In Section 3, we analyze the significantly harder and largely
unaddressed setting when the cluster distributions are still
known, but the users may now come from all clusters. We
provide a lower bound (Theorem 5) showing that when the
number of clusters is too large with respect to the time horizon, sub-linear regret is not attainable. We introduce an algorithm called Multiple-K-UCB and prove a non-trivial
regret bound (Theorem 6) that makes explicit the effect of
the distribution of users Î¥ on the regret.
In Section 4, we target the challenging setting when nothing is known (neither Î¥, the cluster distributions, nor even
the number of clusters). We provide regret bounds for
benchmark UCB-like algorithms (Theorem 7), and a new
algorithm called A-UCB. Despite the very general setting
and poor available information, we are able to prove a
weak result (Proposition 1), that enables us to deduce a
regret guarantee under mild conditions on the structure of
arms (Lemma 1,2). Numerical simulations show in Section 4.2 that the introduced algorithm achieves excellent
performance in a number of hard situations. All proofs
are provided in the extended version (Maillard & Mannor,
2013).
1

We assume that radios are active at the same time everyday.

Notations. At round n, we denote the number
Pn of observations for the pair (a, b) by Na,b (n) =
t=1 I{at =
a, bt = b} and use Î½ba,b (n) and Âµ
ba,b (n) to denote the empirical distribution and empirical mean built from the same
observations,
respectively. We also introduce Nb (n) =
P
aâˆˆA Na,b (n). For observations associated to the pair
a, b, we denote Ua,b (n) a high probability upper bound on
the mean Âµa,b , and La,b (n) a high probability lower bound.
Unless specified, in the sequel we choose the following
Ua,b (n) coming from concentration inequality for R-subGaussian variables (see (1)), and define La,b (n) symmetris
cally:
2 log(Nb (n)3 )
.
Ua,b (n) = Âµ
ba,b (n) + R
Na,b (n)
One could instead use Hoeffdingâ€™s inequality if the distributions have bounded support, empirical Bernsteinâ€™s inequality to take the variance into account, self-normalized
concentration inequality such as in (Garivier & Moulines,
2008; Abbasi-Yadkori et al., 2011), or even tighter upper
bounds based on Kullback-Leibler divergence as explained
in (CappeÌ et al., 2013). These would lead to slightly improved constants in the regret bounds, at the price of clarity.
Thus we focus here on bounds based on the mean only. Let
the confidence set be Sa,b (n) = [La,b (n), Ua,b (n)] and its
size (the gap) be Ga,b (n) = Ua,b (n) âˆ’ La,b (n). To avoid
some technical considerations, we assume that Sa,b (n) is
centered around Âµ
ba,b (n).

2. Known cluster distributions with single
cluster arrivals.
In this section, we consider the case when all the distributions {Î½a,c }aâˆˆA,câˆˆC are known and arrivals {bn }n>1 belong to the same unknown cluster c âˆˆ C. The difference
from a standard multi-armed bandit problem is that the set
of possible distributions is finite and known. We can have
for instance three arms, two clusters and Bernoulli distributions of respective parameter 0.2, 0.6, 0.8 for one cluster,
and Bernoulli distributions of parameter 0.8, 0.1, 0.5 for the
second one. This modifies the achievable guarantees:
Theorem 1 (Agrawal et al. (1989)) Let c âˆˆ C be the true
class (that is supp(Î¥) âŠ‚ Bc ), and Aâˆ’ = A \ {?c } be the
set of sub-optimal arms. Then, a lower performance
bound
X
is
Ï‰c,a âˆ†a,c
RN
aâˆˆAâˆ’
lim inf
> min
max X
,
N â†’âˆ log(N ) Ï‰c âˆˆP(Aâˆ’ ) c0 âˆˆC(c)
Ï‰c,a KL(Î½a,c ||Î½a,c0 )


aâˆˆAâˆ’
where C(c) = c0 âˆˆ C : Î½?c ,c0 = Î½?c ,c and ?c 6= ?0c .
Theorem 2 (Agrawal et al. (1989)) For each c âˆˆ C, let
Ï‰c? that achieves the minimum in the lower bound of Theorem 1. The algorithm proposed by (Agrawal et al., 1989)
makes use of {Ï‰c? }câˆˆC and achieves
P


?
aâˆˆAâˆ’ Ï‰c,a âˆ†a,c
+o(1)
log(N ) .
RN 6 0max P
?
0
c âˆˆC(c)
aâˆˆAâˆ’ Ï‰a,c KL(Î½a,c ||Î½a,c )

Latent Bandits

Although theoretically appealing, it may be in general expensive to compute the quantities {Ï‰c? }câˆˆC , which makes
the algorithm less practical. On the other hand, (Salomon
& Audibert, 2011) introduced the GCL algorithm, seemingly without being aware of the work of (Agrawal et al.,
1989) and got the following non-asymptotic result:
Theorem 3 (Salomon & Audibert (2011)) Assume
that for all c, c0 âˆˆ C, for all a âˆˆ A, then either
Î½a,c 6= Î½a,c0 or (either ?c 6= a or ?0c = a), or
dÎ½ 0
âˆƒa0 6= a : PÎ½a0 ,c ( dÎ½ a0 ,c0 (X) > 0) = 0. Then if
a ,c
c âˆˆ C with unique best arm is the true environment, then
for all Î² > 0 it holds for some constants C, C 0 that

X
log(n)
6 C 0 nâˆ’Î² .
Na,b (n) > C 2
âˆ€nâˆ€a 6= ?c P
âˆ†a,c
bâˆˆBc
GCL is fairly easy to implement, however the way this
bound is stated makes it hard to understand, all the more
so that the constants are not explicit. Also the dependency
with âˆ†2a,c seems sub-optimal.

Algorithm 1 The Single-K-UCB algorithm.
Require: The cluster distributions {Î½a,c }aâˆˆA,câˆˆC .
1: for n = 1...N do
2:
Receive bn âˆ¼ Î¥.
3:
Define n the
set
of
admissible
classes
o
Cnâˆ’1 = c âˆˆ C : âˆ€a âˆˆ A Âµa,c âˆˆ Sa,B (n âˆ’ 1) .
4:
Define the set of â€œeliteâ€ admissible arms
A?nâˆ’1 = {a âˆˆ A; âˆƒc âˆˆ Cnâˆ’1 ?c = a}.
5:
Choose the next arm (breaks ties with round-robin)
an = argmax Âµ?c ,c .
(2)
a=?c ,câˆˆCnâˆ’1
6: end for

For completeness, we now introduce an efficient algorithm
directly inspired from Agrawalâ€™s work. The price for the
reduced complexity is that we lose the asymptotic optimality. We start with some intuition about our setting.

âˆ†+
a,c

High level intuition For clarity, we focus
n on means only
(instead of distributions). Let Cnâˆ’1 = c âˆˆ C, âˆ€a âˆˆ A :
o
Âµa,c âˆˆ Sa,B (n âˆ’ 1) be the set of admissible classes at
round n âˆ’ 1, where the confidence set Sa,B (n âˆ’ 1) is built
using observations for the pairs {(a, b)}bâˆˆB . Note that by
concentration of measure, with high probability the true
class c is admissible and thus Cnâˆ’1 is not empty. Let then
cÌƒ âˆˆ Cnâˆ’1 be an admissible class. It makes sense to pull its
optimal arm ?cÌƒ = argmaxaâˆˆA Âµa,cÌƒ (that is known). Now
several situations may occur:
a) For another class c0 âˆˆ C, if |Âµ?cÌƒ ,c0 âˆ’ Âµ?cÌƒ ,cÌƒ | > Ga,B (n âˆ’
1), then c0 cannot be admissible. Now if when c0 is admissible then ?cÌƒ = ?c0 , it means that choosing to play ?cÌƒ
for cÌƒ âˆˆ Cnâˆ’1 is safe (that is ?cÌƒ = ?c happens with high
probability).
b) If âˆƒc0 âˆˆ C such that both |Âµ?cÌƒ ,c0 âˆ’ Âµ?cÌƒ ,cÌƒ | 6 Ga,B (n âˆ’ 1)
and ?cÌƒ 6= ?c0 , there are many admissible classes that lead
to different actions to play. The situation is tricky since
playing arm ?cÌƒ does not separate cÌƒ from c0 (it may be that
Î½?cÌƒ ,cÌƒ = Î½?cÌƒ ,c0 ), and may moreover be sub-optimal since we
may have ?cÌƒ 6= ?c .
Algorithm (Agrawal et al., 1989) uses a fancy procedure to
handle case b). Here, we note that if we choose the class cÌƒ
(and thus action ?cÌƒ ) with maximal best mean, this ensures
that Âµ?c ,c âˆ’ Âµ?cÌƒ ,c 6 Âµ?cÌƒ ,cÌƒ âˆ’ Âµ?cÌƒ ,c and thus a controlled
error. This observation leads to the Single-K-UCB algorithm, whose pseudo-code is provided in Algorithm 1.
Straightforwardly, if Cnâˆ’1 is empty, it reduces to playing
round-robin, in case a), A?nâˆ’1 is a singleton, and in case b),
we have a controlled error.

Regret bound Such an algorithm enjoys the following regret performance:
Theorem 4 The regret of Single-K-UCB satisfies

X 24R2 âˆ†a,c log(N )
Ï€2 
,
RSingle-K-UCB
6
+ âˆ†a,c 1+
N
+2
3
âˆ†a,c
?


aâˆˆA

?

where A =

a âˆˆ A : âˆƒc âˆˆ C s.t. ?c = a and
n
o
0
0
0
= inf
Âµ
âˆ’
Âµ
:
?
=
a
âˆ©
Âµ
>
Âµ
a,c
a,c
c
?c0 ,c
?c ,c .
0
c âˆˆC

+
The notation âˆ†+
a,c comes from the fact that âˆ†a,c > âˆ†a,c .
Note the link between this bound and that of Theorem 2
(also âˆ†+
a,c and C(c)). Of course the bound of Theorem 2
can be better and this seems to be the price for the simplicity of Single-K-UCB. On the other hand, since Theorem 4 scales with âˆ†+
a,c (which can be arbitrarily larger
than âˆ†a,c ; see Figure 1), it improves on the result of Theorem 3, and moreover provides explicit constants. Finally,
it is straightforward to improve the constants using tighter
confidence bounds as discussed in the introduction.

3. Known cluster distributions with multiple
cluster arrivals.
We now turn to the more challenging case when the distributions {Î½a,c }aâˆˆA,câˆˆC are still known to the learner, but
when the users may come from different clusters, and the
learner does not know what class c corresponds to some input b âˆˆ B. In this setting, the lower bound from Theorem 1
can be strengthen. Indeed, without further assumptions, it
may be the case that if the number of clusters C is too large
with respect to the time horizon N , we donâ€™t have time to
learn and we can not ensure to have sub-linear regret:
Theorem 5 Let Î¥ be the uniform distribution over B and
consider that the distributions are partitioned exactly into
C > A groups of equal size. Then, it holds
âˆš
1
inf sup RN >
min{ N AC, N } .
algo Î½a,c
20
This shows that for the scaling C = â„¦(N ) the problem becomes hopeless, since for any bandit algorithm there exists
a set of distributions {Î½a,b }aâˆˆA,bâˆˆB such that the regret is
linear in N .
Despite this difficulty, it is possible to slightly modify
Single-K-UCB for that setting, which leads to algorithm 2 that enjoys the following regret performance.

Latent Bandits

Algorithm 2 The Multiple-K-UCB algorithm.
Require: The cluster distributions {Î½a,c }aâˆˆA,câˆˆC .
1: for n = 1...N do
2:
Receive b = bn âˆ¼ Î¥.
3:
Define
the
set
of
admissible
classes
n
o
Cnâˆ’1 (b) = c âˆˆ C, âˆ€a âˆˆ A : Âµa,c âˆˆ Sa,b (n âˆ’ 1) .
4:
Define the set of â€œeliteâ€ admissible arms
A?nâˆ’1 = {a âˆˆ A; âˆƒc âˆˆ Cnâˆ’1 (bn ) ?c = a}.
5:
Choose the most optimistic â€œeliteâ€ arm
an =
argmax
Âµ?c ,c .
a=?c , câˆˆCnâˆ’1 (bn )
6: end for
Theorem 6 The regret of Multiple-K-UCB satisfies

X X
24R2 âˆ†a,cb log(N Î¥(b))
Multiple-K-UCB
RN
6
min
2
âˆ†+
a,cb
bâˆˆB aâˆˆA?


âˆ’1
+ O Î¥(b)
, âˆ†a,cb N Î¥(b) ,
where cb âˆˆ C denotes the class corresponding to b âˆˆ B.
In order to see the benefit of knowing the distributions
{Î½a,c }aâˆˆA,câˆˆC , a natural benchmark algorithm is the one
that simply plays independent copies of UCB on each b âˆˆ B
(see (Auer, 2003)), without using the knowledge of the
cluster distributions. We call this algorithm UCB on B; see
Algorithm 3. Importantly, due to the inequality âˆ†+
a,cb >
âˆ†a,cb and because only elite arms a âˆˆ A? are pulled, the
regret of Multiple-K-UCB is never worse than that of
UCB on B (Theorem 7); it can potentially be much smaller.
Algorithm 3 The UCB on B algorithm
1: for n = 1...N do
2:
Receive bt âˆ¼ Î¥.
3:
Compute the empirical means Âµ
ba,b (n âˆ’ 1).
4:
Choose the next arm (breaks ties arbitrary)
an = argmax Ua,bn (n âˆ’ 1) .
(3)
5: end for
aâˆˆA
Illustration In order to highlight the role played by âˆ†+
a,c ,
Figure 1 depicts the upper-bounds from Theorem 6 and and
from Theorem 7, for one randomly generated problem (we
do not compare the regret, but the bounds, to emphasize the
theoretical gap). For clarity, we reported the values of âˆ†+
a,c
as well as of the optimality gaps âˆ†a,c for each arm and
each class. Here three arms that may be pulled by UCB on
B are never pulled by Multiple-K-UCB. Note that the
improvement can sometimes be huge: for instance when all
?c are equal, then âˆ†+
a,c = âˆ for all sub-optimal arm and
the bound from Theorem 6 equals zero.

4. The agnostic case.
In Sections 2 and 3, using the knowledge of the cluster distributions, we derived regret bounds that may significantly
improve on their equivalent agnostic version. We now detail an improvement that is even more effective and applicable both in case cluster distributions are known or not.
We first note that using estimates from each distributions
Î½a,b separately in order to decide the best action for the

Figure 1. Theoretical regret bounds for Multiple-K-UCB
(Theorem 6) and UCB on B (Theorem 7) for one problem characterized by |A| = 3, |B| = 50, |C| = 4 and
1
2
3
4
Âµa,c : 1
0.527
0.209 0.713 0.762
2
0.717
0.193 0.575 0.230
3
0.669
0.751 0.120 0.485
âˆ†+
0.235
0.553
0.0
0.0
a,c : 1
2
0.0
+âˆ
0.142
+âˆ
3
0.082
0.0
0.631
+âˆ
âˆ†a,c : 1
0.190
0.542
0.0
0.0
2
0.0
0.558 0.138 0.533
3 0.0475
0.0
0.593 0.277

cluster c(b) = c seems sub-optimal since the number of
samples Na,b (n) available for the couple (a, b) is typically
small, while we could possibly gain much more by using
all observations in each Bc (This is basically what happens
in Section 2). Indeed, if two distributions Î½a,b and Î½a,b0
are the same, then grouping the corresponding observations
provides a faster convergence speed. In general, grouping
subsets of {Î½a,b }bâˆˆB may lead to a dramatic speed-up if we
group similar distributions, and may create a bias if they
significantly differ. Thus, there is a trade-off between getting fast versus accurate convergence, and it is a priori not
clear whether we can get a provable improvement.
Benchmark We now introduce a (pseudo-)oracle that
knows the identity of the clusters perfectly. The simplest
one is an algorithm that runs a version of UCB separately
on each group Bc (and not each b). We call this benchmark
UCB on C. Note that although it knows the clusters this is
not the best oracle: In some cases, it may be better to further group some clusters together. This algorithm is easy to
analyze. To understand the kind of improvement we are targeting, the following theorem compares the regret of UCB
on B, to that of the pseudo-oracle UCB on C.
Theorem 7 The expected regret at time N of the algorithm
UCB on B is upper bounded by
n 24R2 log(N Î¥(b))
XX
on B
RUCB
6
min
N
âˆ†a,b
bâˆˆB aâˆˆA


o
+ O Î¥(b)âˆ’1 , âˆ†a,b N Î¥(b) ,
where âˆ†a,b = ÂµÏ€? (b),b âˆ’ Âµa,b is the optimality gap of arm
a for environment b. Similarly, the expected regret at time
N of UCB on C is upper bounded by

Latent Bandits

UCB on C
RN

6

C X
X

n 24R2 log(N Î¥(B ))
c
âˆ†
a,c
c=1 aâˆˆA


o
+ O Î¥(Bc )âˆ’1 , âˆ†a,c N Î¥(Bc ) ,
min

where âˆ†a,c is the common value of the âˆ†a,b for b âˆˆ Bc .
As a result, the regret of UCB on C can be substantially
smaller than the one of UCB on B. Indeed, only looking at
the term in factor of log(N ), we get an improvement going
P
P
PC P
âˆ’1
from bâˆˆB aâˆˆA âˆ†âˆ’1
c=1
aâˆˆA âˆ†a,c , that can be
a,b to
substantial, since typically C is much smaller than B. Note
of course that the partition C is unknown in practice. We
emphasize that the lower bound of Theorem 5 also holds
for that setting.
Grouping distributions We now detail the improvement
we are going to use. Let B âŠ‚ B. We define, similarly to
Âµ
ba,b (n), La,b (n) and Ua,b (n) the empirical group estimate
Î½ba,B (n) with associated group mean Âµa,B (n), confidence
intervals Ua,B (n), La,B (n) and set Sa,B (n), where
P
ba,b0 (n)Na,b0 (n)I{b0 âˆˆ B}
b0 âˆˆB Î½
P
,
Î½ba,B (n) =
0
0
b0 âˆˆB Na,b (n)I{b âˆˆ B}
P
0
0
0
b0 âˆˆB Âµa,b Na,b (n)I{b âˆˆ B}
P
Âµa,B (n) =
.
0
0
b0 âˆˆB Na,b (n)I{b âˆˆ B}
Note that for B = Bc , then Âµa,Bc (n) = Âµa,c , which
may not hold for other sets B since there may be a bias
when the {Âµa,b0 }b0 âˆˆB are distinct. However, the speed
of
P convergence of 0 the group depends on Na,B (n) =
0
b0 âˆˆB Na,b (n)I{b âˆˆ B}, which is typically much faster
than that of a single point b (that depends on Na,b (n)).
Thus Sa,B (n) = [La,B (n), Ua,B (n)] is potentially much
smaller than Sa,b (n). Finally, note that, by construction,
we have Âµa,B (n) âˆˆ Sa,B (n) with high probability, but that
for some b âˆˆ B there is no reason that Âµa,b âˆˆ Sa,B (n) due
to the introduced bias.
In order to leverage the estimation bias, we restrict possible
groups B, using two observations. First, if Âµa,b = Âµa,b0 ,
then we must have Sa,b (n) âˆ© Sa,b0 (n) 6= âˆ… with high probability. More generally, some B such that Âµa,b = Âµa,b0
for all b, b0 âˆˆ B, must satisfy that for all B 0 âŠ‚ B and all
B 00 âŠ‚ B, with high probability, Sa,B 00 (n) âˆ© Sa,B 0 (n) 6= âˆ….
Second, we define for an adaptive Îµ = Îµa,b,b0 ,n the enlarged
confidence bounds

Ua,b (n; 1 + Îµ) = Âµ
ba,b (n) + (1 + Îµ) Ua,b (n) âˆ’ Âµ
ba,b (n) ,

La,b (n; 1 + Îµ) = Âµ
ba,b (n) âˆ’ (1 + Îµ) Âµ
ba,b (n) âˆ’ La,b (n) ,
and then Sa,b (n; 1+Îµ) = [La,b (n; 1+Îµ), Ua,b (n; 1+Îµ)]. Now,
if Âµa,b = Âµa,b0 and Ga,b0 (n) 6 2Îµ Ga,b (n)2 , we must have
Sa,b0 (n) âŠ‚ Sa,b (n; 1 + Îµ) with high probability. Finally,
we focus only on mean-based procedures for clarity, but it
2

This is because we restrict to confidence interval centered around Âµ
ba,b (n); in general we would need Ga,b0 (n) 6
Îµ min{Ua,b (n) âˆ’ Âµ
ba,b (n), Âµ
ba,b (n) âˆ’ La,b (n)} .

is possible to use empirical distributions Î½ba,b (n) to remove
b0 with obvious mismatch in Kullback-Leibler divergence.
We do not discuss this.
All in all, we define two sets of sets: First Bb (n) for compatible sets, and then B+
b (n) for maximally compatible
(or â€œeliteâ€) sets, that have maximal group speed of convergence and a controlled bias:

def
Bb (n) = B âŠ‚ B: âˆ€a âˆˆ Aâˆ€b0,b00âˆˆB Sa,b0 (n)âŠ‚Sa,b00 (n; 1+Îµ)

0
00
00
0
âˆ© b âˆˆ B âˆ© âˆ€B , B âŠ‚ B, Sa,B (n) âˆ© Sa,B (n) 6= âˆ… ,
def

B+
b (n) = Argmax B

(for the relation âŠ‚ ) .

(4)

BâˆˆBb (n)

(Note that Argmax returns a set, contrary to argmax.)
4.1. The Agnostic UCB for clustered-bandits.
We are now ready to introduce A-UCB, whose pseudo-code
is provided as Algorithm 4.
Proving strong regret bounds in this agnostic setting is difficult without further assumptions, since the true class may
change at each single time step. For that reason, we proceed in two steps: Proposition 1 controls the number of
pulls of sub-optimal arms under some events, that we then
handle in specific cases.
Algorithm 4 The A-UCB algorithm
Require: Parameter Î³.
1: for n = 1...N do
2:
Receive bn âˆ¼ Î¥,
3:
Compute Âµ
ba,b (n âˆ’ 1), then Ua,b (n âˆ’ 1), La,b (n âˆ’
1), Sa,b (n âˆ’ 1) and Ga,b (n âˆ’ 1).
4:
Define the quantity Îµ = Îµbn ,b0 ,nâˆ’1 by
s

2Î³ log(Nb0 (n âˆ’ 1))
max
âˆ’ 1, 0 .
log(Nbn (n âˆ’ 1))
5:
Compute the set B+
bn (n âˆ’ 1) of maximally compatible aggregation sets via (4).
6:
Pull an elite arm that is the most optimistic
Ua,B (n âˆ’ 1)
(5)
an âˆˆ argmax
max
+
7: end for

aâˆˆA

BâˆˆBbn (nâˆ’1)

n
o
Proposition 1 Let â„¦n = Bcn âˆˆ Bbn (n âˆ’ 1) be the
event that
n the true class cn is admissible
o at round n, and
EnÎ± = G?cn ,Bcn (n âˆ’ 1) < Î±âˆ†an ,cn the event that the
confidence interval of the optimal arm of cluster Bcn is
small enough, for small Î± âˆˆ (0, 1). Then,3 for a suboptimal an , under â„¦n âˆ© EnÎ± and for all Î· âˆˆ (Î±, 1],

Îµ 2 24R2 log(Nbn (n âˆ’ 1))
,
either Nan ,bn(n âˆ’ 1) < 1+
2
(Î· âˆ’ Î±)2 âˆ†2an ,cn
or Nan ,Bcn(n âˆ’ 1) <
3

24R2 log(NBcn (n âˆ’ 1))
m.
(1 âˆ’ Î·)2 âˆ†2an ,cn

In section C.2 of the extended version (Maillard & Mannor,
2013), we show a slightly stronger result, though more difficult to
interpret.

Latent Bandits

That is, in all cases the total number of pulls, for either the
current user bn or its class cn , of a chosen sub-optimal arm
is controlled.
In particular for small Îµ, Î± and Î· â†’ 1, Proposition 1 shows
that under â„¦n âˆ© EnÎ± the regret of A-UCB is essentially in
between that of UCB on B and UCB on C: up to constants,
it is never worse than UCB on B, and can be significantly
better by competing occasionally with UCB on C. This is
highlighted on Figure 5. It now remains to show that â„¦n âˆ©
EnÎ± happens with high probability in order to deduce a nontrivial regret bound.
Illustration â„¦n is the event that the true class cn is admissible at round n,. Now the event EnÎ± essentially says that
N
P?cn ,Bcn (n âˆ’ 1) > O(log(n)), that is, since N?c ,Bc (n) =
bâˆˆBc N?c ,b (n), it is enough that one N?cn ,b (n) be as
large to ensure that EnÎ± happens. For illustration, let us
turn to the case of Bernoulli distributions (R = 1/2) with
C = 4 equally probable classes of equal size B = 50. Individual upper bound confidence bounds Ua,b (25000) are
non trivial (i.e. less than 1) if (a, b) is seen at least 15
times. Now if each pair (?c , b) for b âˆˆ Bc is visited at least
15 times (out of the ' 125 available time steps for each
b âˆˆ Bc ) then G?c ,Bc (25000) < 0.27, and for 50 visits, the
bound reduces to 0.145. Similarly, for B = 250 we get
abound 0.12 with 15 visits of the optimal action, which is
enough to ensure that EnÎ± happens in non-trivial situations.
Of course these numbers can be significantly reduced by
using better confidence bounds (see (Abbasi-Yadkori et al.,
2011)). Let us now provide conditions under which both
EnÎ± and â„¦n happen.
Adaptive enlargement The reason for having an adaptive
Îµ and not just a constant Îµ = 1 is that a constant Îµ does
not always ensure that Bcn is admissible (that is â„¦n happens) with high probability, but only that a subset of Bcn
is admissible at round n. To better understand the number
of such points that are gathered in Sa,b (n; 1 + Îµ) we introduce the following quantity, that only depends on the law
of arrivals Î¥:
Definition 1 The Î³-balance of B with respect to cluster c,
for point b âˆˆ Bc is defined by


0
0
Bc (b; Î³) = b âˆˆ Bc : Î¥(b) 6 Î³Î¥(b ) .
Together with this quantity, it is natural to introduce the
distortion factor of group Bc , defined by
maxbâˆˆBc Î¥(b)
.
minbâˆˆBc Î¥(b)
These quantities enable us to quantify the effective number of points that are grouped with b âˆˆ B, which directly
defines the speed-up the algorithm can achieve for this environment. Importantly, note that if Î³ > Î³c , then it holds
that Bc (b; Î³) = Bc for all b âˆˆ Bc . A-UCB uses an adaptive Îµ that ensures that if Î³ is essentially greater than Î³c ,
Î³c =

then Bc (b; Î³) and thus Bc is admissible with high probability (but one should choose a small Î³ since the regret scales
with Î³); more precisely
Lemma 1 In A-UCB, if Î³ is chosen such that Î³ > Î³c +
O nâˆ’1/2 , then it holds that


X
P(â„¦n ) > 1 âˆ’ O nâˆ’2 A
Î¥(b)âˆ’2 âˆ’ 2|B|nâˆ’2 .
bâˆˆB

Such a O(nâˆ’2 ) control is standard in regret proofs.
Ensuring the optimal arm is pulled enough We now turn
to EnÎ± . In full generality, there is no reason that A-UCB
makes EnÎ± happen. The following lemma however ensures
that under a mild condition on the structure of the problem,
this actually holds with high probability. A simple regret
bound follows trivially.
Lemma 2 Let us assume that Î¥ is the uniform distribution,
that all clusters have the same size B0 , and that the cluster
distributions satisfy âˆ€c, c0 âˆˆ C âˆ€a âˆˆ A
3
either Âµ?c ,c âˆ’Âµ?c ,c0 <âˆ†a,c /2 or Âµ?c ,c âˆ’Âµ?c ,c0 > âˆ†a,c .
2
(That is, a mismatch between two classes is either clear or
harmless.) In such a case, if A-UCB is run with Î³ âˆ¼ Î³c =
1, then P(EnÎ± ) > 1 âˆ’ O(nâˆ’2 ) holds for Î± = 1/2.
Combining Proposition 1 together with Lemma 1 and
Lemma 2, we deduce that, in some specific situations we
are able to control with high probability the number of pulls
of a sub-optimal arm, and as a result, the regret of the considered strategy. We currently do not know how to extend
the analysis to handle the most general case.
4.2. Numerical experiments
In this section, we study the behavior of the algorithm
A-UCB on some experiments.
Algorithms We use the vanilla version of UCB (that aggregates all contexts), UCB on B that is the naive application of UCB separately on each context, and the pseudooracle UCB on C. We implemented a simplified version of
A-UCB where we do not compute the maximally compatible sets exactly (which is NP-hard in general), but average the means of the compatible sets instead. This slightly
worsen the numerical constants in our results, even though
characterizing entirely the effect of this relaxation in terms
of regret and numerical efficiency goes beyond the scope
of this paper.
Experiments We consider experiments with Bernoulli distributions: this is intuitively the hardest case, since one can
only rely on the means to separate distributions; it also
appears in several applications. For each experiment, we
show the number of actions |A|, of users |B|, of classes |C|,
and the parameters {Âµa,c }aâˆˆA,câˆˆC when there are not too
many. We plot the regret of all algorithms on the same figure: A thick line is used for the mean regret and dashed
lines for quantiles at levels 0.25, 0.5, 0.75, 0.95 and 0.99.
In all experiments, the parameters {Î¥(b)}bâˆˆB are defined

Latent Bandits

P
by Î¥(b) = wb / bâˆˆB wb , where the weights wb are drawn
uniformly randomly in [0.1, 0.9]. Thus for each class, the
distortion factor Î³c is less than 9, and we set the parameter
Î³ of A-UCB to the value Î³ = 9. For one experiment with
given fixed parameters, the algorithms are run over several
trials (500) for a large time horizon N = 25000. We do
not report the values of {Î¥(b)}bâˆˆB since this is generally
uninformative.

Figure 2 presents an expected situation, where both the
naive UCB and UCB on B perform poorly with respect to the
pseudo-oracle, whereas A-UCB performs very well. Note
that here the best arm is different in the different classes,
with corresponding value that is always very high and well
separated from other arms.
Figure 3 presents a tricky situation: UCB on B performs
poorly, while both A-UCB compete with the pseudo-oracle,
and all are defeated by UCB, which is not surprising since
here one arm is the best in all contexts.
Figure 4 presents a variant when A is large. As expected
the performance of all algorithms degrade, but A-UCB
is still competitive with respect to the pseudo-oracle and
benchmark algorithms.

Figure 2. Regret of several algorithms in the following scenario
with |A| = 3, |B| = 50, |C| = 4 and
Âµa,c
1
2
3
4
1
0.527 0.209 0.713 0.762
2
0.717 0.193 0.575 0.230
3
0.669 0.751 0.120 0.485
Figure 4. Regret of several algorithms in some randomly generated situation with |A| = 50, |B| = 50, |C| = 4.

Figure 5 presents a variant when B is large. Note that in
this experiment, one only gets to see each b about 50 times,
this setting is thus challenging. It can be seen that A-UCB
still works fairly decently in this case. In accordance with
Proposition 1, let us also remark that here A-UCB behaves
initially like UCB on B, and progressively behaves like UCB
on C(though with a shifted regret due to the initial phase).
Finally figure 6 presents a variant when C is large. A-UCB
still competes with the pseudo-oracle here.

Figure 3. Regret of several algorithms in the following scenario
with |A| = 3, |B| = 50, |C| = 4 and
Âµa,c
1
2
3
4
1
0.370 0.750 0.609 0.207
2
0.150 0.290 0.475 0.464
3
0.671 0.897 0.781
0.9

In all these experiments, we see that A-UCB consistently
competes with UCB on C, while UCB and UCB on B sometimes obtain poor regret. This indicates that the proposed
strategy is essentially able to capture the right information
and does not under nor over-group the inputs b.

Latent Bandits

5. Discussion
We introduced a novel setting for sequential decision making problem where there are some latent variables, such
as recommender systems, cognitive radio networks and
others. We provided several contributions in a general
framework in order to precisely address the issues raised
by the latent structure. As a result, our contribution can
be straightforwardly applied for instance to the linearbandit setting (see Abbasi-Yadkori et al. (2011); Dani et al.
(2008)), where the number of actions is replaced with the
dimension of a feature space, and confidence intervals with
confidence ellipsoids, and potentially many others.
Let us remark that we assumed in this work that the reward
distributions are clustered, that is each Î½a,b is one of the
{Î½a,c }c . A natural extension is to consider the case when
each Î½a,b is a mixture of the {Î½a,c }c , with an underlying
low-rank structure. This is left for future research.
Figure 5. Regret of several algorithms in the following scenario
with |A| = 3, |B| = 500, |C| = 4 and
Âµa,c
1
2
3
4
1
0.1
0.621
0.1
0.362
2
0.544 0.697 0.554 0.181
3
0.512 0.409 0.234
0.1

In the non-trivial setting of Section 2, we showed that
a simple procedure improves on (Salomon & Audibert,
2011) on the theoretical side and on (Agrawal et al., 1989)
on the computational side. We then introduced the more
challenging setting of Section 3, that has not been addressed previously, and extended our procedure to that setting. We provided a lower-bound explaining why the setting is challenging and then a non trivial regret bound that
makes appear explicitly the role of the distribution Î¥ of
arrivals.
We finally tackled the agnostic setting, when not even the
number of clusters is known. We introduced an algorithm
that demonstrates excellent performance on a number of
difficult situations, and provided a result enabling to derive
regret guarantees in some non-trivial situation. We leave
the intricate question of extending Lemma 1 and 2 to the
fully general case as an open problem.
Acknowledgements This work was supported by
the European Communityâ€™s Seventh Framework Programme (FP7/2007-2013) under grant agreement 306638
(SUPREL) and the Technion.

Figure 6. Regret of several algorithms in some randomly generated situation with |A| = 3, |B| = 100, |C| = 50.

Latent Bandits

References
Abbasi-Yadkori, Yasin, PaÌl, DaÌvid, and SzepesvaÌri, Csaba.
Improved algorithms for linear stochastic bandits. In
Shawe-Taylor, John, Zemel, Richard S., Bartlett, Peter L., Pereira, Fernando C. N., and Weinberger, Kilian Q. (eds.), Advances in Neural Information Processing Systems, pp. 2312â€“2320, 2011.
Adomavicius, Gediminas and Tuzhilin, Alexander. Toward
the next generation of recommender systems: A survey
of the state-of-the-art and possible extensions. IEEE
Trans. on Knowl. and Data Eng., 17(6):734â€“749, jun
2005. ISSN 1041-4347.
Agrawal, Rajeev, Teneketzis, Demosthenis, and Anantharam, Venkatachalam. Asymptotically Efficient Adaptive Allocation Schemes for Controlled I.I.D. processes.
IEEE Transactions on Automatic Control, 34(3):258â€“
267, 1989.
Auer, Peter. Using confidence bounds for exploitationexploration trade-offs. Journal of Machine Learning Research, 3:397â€“422, mar 2003. ISSN 1532-4435.
Avner, Orly, Mannor, Shie, and Shamir, Ohad. Decoupling
exploration and exploitation in multi-armed bandits. In
Proceedings of the 29th International conference on Machine Learning. Omnipress, 2012.
Burnetas, Apostolos N. and Katehakis, Michael N. Optimal adaptive policies for sequential allocation problems.
Adv. Appl. Math., 17(2):122â€“142, jun 1996. ISSN 01968858.
CappeÌ, Olivier, Garivier, AureÌlien, Maillard, OdalricAmbrym, Munos, ReÌmi, and Stoltz, Gilles. Kullbackâ€“
leibler upper confidence bounds for optimal sequential
allocation. Ann. Statist., 41(3):1516â€“1541, 2013. ISSN
0090-5364.
Dani, Varsha, Hayes, Thomas P., and Kakade, Sham M.
Stochastic Linear Optimization under Bandit Feedback. In Servedio, Rocco A., Zhang, Tong, Servedio,
Rocco A., and Zhang, Tong (eds.), COLT, pp. 355â€“366.
Omnipress, 2008.
Filippi, Sarah, CappeÌ, Olivier, CeÌrot, Fabrice, and
Moulines, Eric. A near optimal policy for channel allocation in cognitive radio. In Girgin, Sertan, Loth,
Manuel, Munos, ReÌmi, Preux, Philippe, and Ryabko,
Daniil (eds.), Recent Advances in Reinforcement Learning, volume 5323 of Lecture Notes in Computer Science,
pp. 69â€“81. Springer Berlin Heidelberg, 2008.
Garivier, AureÌlien and Moulines, Eric.
On UpperConfidence Bound Policies for Non-Stationary Bandit
Problems. ArXiv e-prints, may 2008. Technical report,
LTCI.

Hazan, Elad and Megiddo, Nimrod. Online learning with
prior knowledge. In Proceedings of the 20th annual
conference on Learning theory, COLTâ€™07, pp. 499â€“513,
Berlin, Heidelberg, 2007. Springer-Verlag.
Lai, Tze L. and Robbins, Herbert. Asymptotically efficient
adaptive allocation rules. Advances in Applied Mathematics, 6(1):4â€“22, 1985.
Langford, John and Zhang, Tong. The Epoch-Greedy
Algorithm for Multi-armed Bandits with Side Information. In Platt, John C., Koller, Daphne, Singer, Yoram,
Roweis, Sam T., Platt, John C., Koller, Daphne, Singer,
Yoram, and Roweis, Sam T. (eds.), NIPS. MIT Press,
2007.
Li, Lihong, Chu, Wei, Langford, John, and Schapire,
Robert E. A contextual-bandit approach to personalized news article recommendation. In Proceedings of
the 19th international conference on World wide web,
WWW â€™10, pp. 661â€“670, New York, NY, USA, 2010.
ACM. ISBN 978-1-60558-799-8.
Li, Lihong, Chu, Wei, Langford, John, and Wang, Xuanhui.
Unbiased offline evaluation of contextual-bandit-based
news article recommendation algorithms. In Proceedings of the fourth ACM international conference on Web
search and data mining, WSDM â€™11, pp. 297â€“306, New
York, NY, USA, 2011. ACM. ISBN 978-1-4503-0493-1.
Lu, Tyler, PaÌl, DaÌvid, and Pal, Martin. Contextual multiarmed bandits. Journal of Machine Learning Research Proceedings Track, 9:485â€“492, 2010.
Maillard, Odalric-Ambrym and Mannor, Shie. Latent bandits. HAL/Open archive, 2013. URL http://hal.
inria.fr/hal-00926281.
Salomon, Antoine and Audibert, Jean-Yves. Deviations
of stochastic bandit regret. In Proceedings of the 22nd
international conference on Algorithmic learning theory, ALTâ€™11, pp. 159â€“173, Berlin, Heidelberg, 2011.
Springer-Verlag. ISBN 978-3-642-24411-7.
Slivkins, Aleksandrs. Contextual bandits with similarity information. In Proceedings of the 24th annual conference
on Learning theory, COLTâ€™11. Springer-Verlag, 2011.

