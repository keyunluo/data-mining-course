Relative Upper Confidence Bound for the
K-Armed Dueling Bandit Problem
Masrour Zoghi1 , Shimon Whiteson1
Remi Munos2
Maarten de Rijke1
1
ISLA, University of Amsterdam, Netherlands
2
INRIA Lille - Nord Europe / MSR-NE

Abstract
This paper proposes a new method for the Karmed dueling bandit problem, a variation on the
regular K-armed bandit problem that offers only
relative feedback about pairs of arms. Our approach extends the Upper Confidence Bound algorithm to the relative setting by using estimates
of the pairwise probabilities to select a promising
arm and applying Upper Confidence Bound with
the winner as a benchmark. We prove a sharp
finite-time regret bound of order O(K log T ) on
a very general class of dueling bandit problems
that matches a lower bound proven in (Yue et al.,
2012). In addition, our empirical results using
real data from an information retrieval application show that it greatly outperforms the state of
the art.

1. Introduction
In this paper, we propose and analyze a new algorithm,
called Relative Upper Confidence Bound (RUCB), for the
K-armed dueling bandit problem (Yue et al., 2012), a variation on the K-armed bandit problem in which the feedback comes in the form of pairwise preferences. We assess
the performance of this algorithm using one of the main
current applications of the K-armed dueling bandit problem, ranker evaluation (Joachims, 2002; Yue & Joachims,
2011; Hofmann et al., 2013a), which is used in information
retrieval, ad placement and recommender systems, among
others.
The K-armed dueling bandit problem is part of the
general framework of preference learning (Fürnkranz &
Hüllermeier, 2010), where the goal is to learn, not from
real-valued feedback, but from relative feedback, which
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

{M . ZOGHI , S . A . WHITESON}@ UVA . NL
REMI . MUNOS @ INRIA . FR
DERIJKE @ UVA . NL

specifies only which of two alternatives is preferred. Developing effective preference learning methods is important
for dealing with domains in which feedback is much more
reliable if given in the form of a comparison (e.g., when
provided by a human) and specifying real-valued feedback
instead would be arbitrary or inefficient.
Other algorithms proposed for this problem are Interleaved
Filter (IF) (Yue et al., 2012), Beat the Mean (BTM) (Yue
& Joachims, 2011), and SAVAGE (Urvoy et al., 2013). All
of these methods were designed for the finite-horizon setting, in which the algorithm requires as input the exploration horizon, T , the time by which the algorithm needs
to produce the best arm. The algorithm is then judged based
upon either the accuracy of the returned best arm or the regret accumulated in the exploration phase.1 All three of
these algorithms use the exploration horizon to set their
internal parameters so that, for each T , there is a separate algorithm IFT , BTMT and SAVAGET . By contrast,
RUCB does not require this input, making it more useful
in practice, since a good exploration horizon is often difficult to guess. Nonetheless, RUCB outperforms these algorithms in terms of the accuracy and regret metrics used in
the finite-horizon setting.
The main idea of RUCB is to maintain optimistic estimates
of the probabilities of all possible pairwise outcomes, and
(1) use these estimates to select a potential champion,
which is an arm that has a chance of being the best arm,
and (2) select an arm to compare to this potential champion
by performing regular Upper Confidence Bound (Agrawal,
1995) relative to it.
We prove a finite-time high-probability bound of
O(K log T ) on the cumulative regret of RUCB, from
which we deduce a bound on the expectation and all higher
moments of cumulative regret. These bounds rely on
substantially less restrictive assumptions on the K-armed
dueling bandit problem than IF and BTM and have better
multiplicative constants than those of SAVAGE. Further1

These terms are formalized in Section 2.

Relative Upper Confidence Bound

more, our bounds are the first explicitly non-asymptotic
results for the K-armed dueling bandit problem.
More importantly, the main distinction of our result is that it
holds for all time-steps. By contrast, given an exploration
horizon T , the results for IF, BTM and SAVAGE bound
only the regret accumulated by IFT , BTMT and SAVAGET
in the first T time-steps.
Finally, we evaluate our method empirically using real data
from an information retrieval application. The results show
that RUCB can learn quickly and effectively and greatly
outperforms BTM and SAVAGE.
The main contributions of this paper are as follows:
• A novel algorithm for the K-armed dueling bandit problem that is more broadly applicable than existing algorithms,
• Regret bounds that make significantly less restrictive assumptions than IF and BTM, have better multiplicative
constants than the results of SAVAGE, apply to all timesteps, and match an existing asymptotic lower bound,
• A novel proof technique that allows us to obtain the first
logarithmic high probability regret bound for a UCBtype algorithm that does not require the probability of
failure to be passed to the algorithm as a parameter: as
a corollary, we also get the first logarithmic bounds on
all higher moments of the cumulative regret for all times,
and
• Experimental results, based on a real-world application,
demonstrating the superior performance of our algorithm
compared to existing methods.

2. Problem Setting

i+

, with k := p1k 12 for all k 2 {1, . . . , K}. Thus,
regret measures the average advantage that the Condorcet
winner has over the two arms being compared against each
other. Given our assumption on the probabilities p1k , this
implies that r = 0 if and only if the best arm is compared
against itself.
PT We define cumulative regret up to time T to
be RT := t=1 rt .
j

2

The goal of a bandit algorithm can be formalized in several
ways. We consider two standard settings:
1. The finite-horizon setting, in which the algorithm is told
in advance the exploration horizon, T , i.e., the number of time-steps that the evaluation process is given
to explore before it has to produce a single arm as the
best, which will be exploited thenceforth. In this setting, the algorithm can be assessed on its accuracy, the
probability that a given run of the algorithm reports the
Condorcet winner as the best arm (Urvoy et al., 2013),
which is related to expected simple regret: the regret associated with the algorithm’s choice of the best arm, i.e.,
rT +1 (Bubeck et al., 2009). Another measure of success
in this setting is the amount of regret accumulated during the exploration phase, as used in the explore-thenexploit problem formulation (Yue et al., 2012).
2. The horizonless setting, in which no horizon is specified and the evaluation process continues indefinitely.
Thus, it is no longer sufficient for the algorithm to maximize accuracy or minimize regret after a single horizon
is reached. Instead, it must minimize regret across all
horizons by rapidly decreasing the frequency of comparisons involving suboptimal arms, particularly those
that fare worse in comparison to the best arm. This goal
can be formulated as minimizing the cumulative regret
over time, rather than with respect to a fixed horizon
(Lai & Robbins, 1985).

The K-armed dueling bandit problem (Yue et al., 2012) is
a modification of the K-armed bandit problem (Thompson, 1933): the latter considers K arms {a1 , . . . , aK } and
at each time-step, an arm ai can be pulled, generating a reward drawn from an unknown stationary distribution with
expected value µi . The K-armed dueling bandit problem
is a variation in which, instead of pulling a single arm, we
choose a pair (ai , aj ) and receive one of them as the better
choice, with the probability of ai being picked equal to an
unknown constant pij and that of aj equal to pji = 1 pij .
We define the preference matrix P = [pij ], whose ij entry
is equal to pij .

All existing K-armed dueling bandit methods target the
finite-horizon setting. However, we argue that the horizonless setting is more relevant in practice for the following
reason: finite-horizon methods require a horizon as input
and often behave differently for different horizons. This
poses a practical problem because it is typically difficult
to know in advance how many comparisons are required
to determine the best arm with confidence and thus how
to set the horizon. If the horizon is set too long, the algorithm is too exploratory, increasing the number of evaluations needed to find the best arm. If it is set too short,
the best arm remains unknown when the horizon is reached
and the algorithm must be restarted with a longer horizon.

In this paper, we assume that there exists a Condorcet winner (Urvoy et al., 2013): an arm, which without loss of generality we label a1 , such that p1i > 12 for all i > 1. Given
a Condorcet winner, we define regret for each time-step as
follows (Yue et al., 2012): if arms ai and aj were chosen
for comparison at time t, then regret at that time is rt :=

Moreover, any algorithm that can deal with the horizonless
setting can easily be modified to address the finite-horizon
setting by simply stopping the algorithm when it reaches
the horizon and returning the best arm. By contrast, for
the reverse direction, one would have to resort to the “dou-

Relative Upper Confidence Bound

bling trick” (Cesa-Bianchi & Lugosi, 2006, Section 2.3),
which leads to substantially worse regret results: this is
because all of the upper bounds proven for methods addressing the finite-horizon setting so far are in O(log T )
and applying the doubling trick to such results would lead
to regret bounds of order (log T )2 , with the extra log factor
coming from the number of partitions.
To the best of our knowledge, RUCB is the first K-armed
dueling bandit algorithm that can function in the horizonless setting without resorting to the doubling trick. We
show in Section 4 how it can be adapted to the finitehorizon setting.

3. Related Work
The first two methods proposed for the K-armed dueling
bandit problem are Interleaved Filter (IF) (Yue et al., 2012)
and Beat the Mean (BTM) (Yue & Joachims, 2011), both
of which were designed for a finite-horizon scenario. These
methods work under the following restrictions: a total ordering of the arms, Stochastic Triangle Inequality (STI) and
either Strong Stochastic Transitivity (SST) in the case of
IF or Relaxed Stochastic Transitivity (RST) with parameter (for BTM); , which measures the degree to which
SST fails to hold, needs to be passed to the algorithm: the
higher is, the more challenging the problem becomes,
with SST holding when = 1 (cf. §8.1 of the supplementary material for formal definitions and evidence that these
assumptions are often violated in practice).
Given these assumptions, the following regret bounds have
been proven for IF and BTM. For large T we have
⇥
⇤
K log T
E RTIFT  C
, and
min

RTBTMT

C

0

7

K log T

with high probability,

min

where IFT means that IF is run with the exploration horizon
set to T and similarly for BTMT ; min is the smallest gap
1
j := p1j
2 , assuming that a1 is the best arm; and C
0
and C are universal constants that do not depend on the
specific dueling bandit problem.
The first bound holds only when = 1 but matches the
lower bound in (Yue et al., 2012, Theorem 2). The second
bound holds for
1 and is sharp when = 1. Note that
this lower bound was proven for certain K-armed dueling
bandit problems that satisfy i = j for all i, j 6= 1. In
this case, our asymptotic regret bound matches this lower
bound as well, without any dependence on (cf. Theorem
4).
Sensitivity Analysis of VAriables for Generic Exploration
(SAVAGE) (Urvoy et al., 2013) is a recently proposed algorithm that outperforms both IF and BTM by a wide mar-

gin when the number of arms is of moderate size. Moreover, one version of SAVAGE, called Condorcet SAVAGE,
makes the Condorcet assumption and has the best theoretical results among the algorithms studied in that paper (Urvoy et al., 2013, Theorem 3). However, the regret
bounds provided for Condorcet SAVAGE are of the form
O(K 2 log T ), and so are not as tight as those of IF, BTM
or our algorithm.
Finally, note that all of the above results bound only RT ,
where T is the predetermined horizon, since IF, BTM and
SAVAGE were designed for the finite-horizon setting. By
contrast, in Section 5, we bound the cumulative regret of
RUCB for all time-steps.

4. Method
Algorithm 1 Relative Upper Confidence Bound
Input: ↵ > 12 , T 2 {1, 2, . . .} [ {1}
1: W = [wij ]
0K⇥K // 2D array of wins: wij is the
number of times ai beat aj
2: B = ?
3: for t = 1, . . . , T do
q

W
↵ ln t
U := [uij ] = W+W
// All operaT +
W+WT
x
tions are element-wise; 0 := 1 for any x.
1
5:
uii
2 for each i = 1, . . . , K.
1
6:
C
ac | 8 j : ucj
2 .
7:
If C = ?, then pick c randomly from {1, . . . , K}.
8:
B
B \ C.
9:
If |C| = 1, then B
C and ac to be the unique
element in C.
10:
if |C| > 1 then
11:
Sample ac from(
C using the distribution:
0.5
if ac 2 B,
p(ac ) =
1
otherwise.
|B|
2 |C\B|
12:
end if
13:
d
arg maxj ujc , with ties broken randomly.
Moreover, if there is a tie, d is not allowed to be
equal to c.
14:
Compare arms ac and ad and increment wcd or wdc
depending on which arm wins.
15: end for
Return: An arm ac that
n beats the most oarms, i.e., c with
wcj
the largest count # j| wcj +w
> 12 .
jc

4:

We now introduce Relative Upper Confidence Bound
(RUCB), which is applicable to any K-armed dueling bandit problem with a Condorcet winner. In each time-step,
RUCB, shown in Algorithm 1, goes through the following
three stages:
(1) RUCB puts all arms in a pool of potential champions.
Then, it compares each arm ai against all other arms op-

Relative Upper Confidence Bound

timistically: for all i 6= j, it computes the upper bound
uij (t) = µij (t) + cij (t), where µij (t) is the frequentist estimate of pij at time t and cij (t) is an optimism bonus that
increases with t and decreases with the number of comparisons between i and j (Line 4). If uij < 12 for any j, then
ai is removed from the pool: the set of remaining arms is
called C. If we are left with a single potential champion
at the end of this process, we let ac be that arm and put it
in the set B of the hypothesized best arm (Line 9). Note
that B is always either empty or contains one arm; moreover, an arm is demoted from its status as the hypothesized
best arm as soon as it optimistically loses to another arm
(Line 8). Next, from the remaining potential champions,
a champion arm ac is chosen in one of two ways: if B is
empty, we sample an arm from C uniformly randomly; if B
is non-empty, the probability of picking the arm in B is set
to 12 and the remaining arms are given equal probability of
being chosen (Line 11).
(2) Regular UCB is performed using ac as a benchmark (Line 13), i.e., UCB is performed on the set of
arms a1c . . . aKc . Specifically, we select the arm d =
arg maxj ujc . When c 6= j, ujc is defined as above. When
c = j, since pcc = 12 , we set ucc = 12 (Line 5).
(3) The pair (ac , ad ) is compared and the score sheet is
updated as appropriate (Line 7).
Note that in stage (1) the comparisons are based on ucj ,
i.e., ac is compared optimistically to the other arms, making it easier for it to become the champion. By contrast,
in stage (2) the comparisons are based on ujc , i.e., ac is
compared to the other arms pessimistically, making it more
difficult for ac to be compared against itself. This is important because comparing an arm against itself yields no information. Thus, RUCB strives to avoid auto-comparisons
until there is great certainty that ac is indeed the Condorcet
winner.

ity regret bound of the form O(K 2 log T ), which is similar
to the bound for SAVAGE (Urvoy et al., 2013) but for the
horizonless setting. However, in Theorem 4 we show that
this can be lowered to O(K log T ) and we deduce an expected regret bound in Theorem 5. This result is proven
under conditions that are much more general than those for
IF (Yue et al., 2012) and without requiring the user to specify the parameter as BTM does (Yue & Joachims, 2011).
Moreover, it matches the asymptotic lower bound proven
in (Yue et al., 2012, Theorem 2).
The results in Theorems 4 and 5 are surprising because a
2
K-armed dueling bandit problem depends on roughly K2
independent parameters, so one would expect a bound of
the form O(K 2 log T ) unless strong prior information is
infused into the algorithm, as with IF and BTM. However,
these theorems show that one can get asymptotic behaviour
resembling that of a regular K-armed bandit algorithm on
a very broad class of dueling bandit problems with very
little prior knowledge. This finding is also of great practical significance because there are many situations in which
one has a choice between applying a K-armed bandit algorithm to an unreliable quantity, such as Click Through
Rate, or using a K-armed dueling bandit algorithm to conduct direct comparisons, which are known to be more reliable when dealing with humans (Hofmann et al., 2013b,
§2.1). These results show that, given such a dilemma, using
a dueling bandit approach does not come at the expense of
the asymptotic behaviour.

Eventually, as more comparisons are conducted, the estimates µ1j tend to concentrate above 12 and the optimism
bonuses c1j (t) become small. Thus, both stages of the algorithm increasingly select a1 , i.e., ac = ad = a1 , which
accumulates zero regret.

Finally, note that the high probability bound proven in Theorem 4 does not rely on the probability of failure, , being
passed to the algorithm. Thus, we can use it to also bound
higher moments (hence also the variance) of the cumulative
regret for RUCB for all times. This is in contrast to high
probability bounds that require to be specified before the
algorithm starts (Audibert et al., 2009; Srinivas et al., 2010;
Abbasi-yadkori et al., 2011), from which one cannot obtain
expected regret bounds for all times. While, given a time
T , one can set = 1/T in the algorithm to get a logarithmic expected regret bound at time T , getting a logarithmic
expected regret bound at time T 1+✏ for any ✏ > 0, requires
rerunning the algorithm with = 1/T 1+✏ .

Note that Algorithm 1 is a finite-horizon algorithm if T <
1 and a horizonless one if T = 1, in which case the for
loop never terminates.

As before, we assume without loss of generality that a1 is
the optimal arm. See Table 1 for definitions of symbols
used throughout.

5. Theoretical Results
In this section, we prove finite-time high-probability and
expected regret bounds for RUCB. We first state Lemma 1
and use it to prove a high-probability bound on the number
of comparisons for each suboptimal arm in Proposition 2.
An immediate consequence of this result is a high probabil-

Lemma 1. Let P := [pij ] be the preference matrix of a
K-armed dueling bandit problem with arms {a1 , . . . , aK }.
Then, for any dueling bandit algorithm and any ↵ > 12 and
> 0, we have
⇣
⌘
P 8 t > C( ), i, j, pij 2 [lij (t), uij (t)] > 1
.
Proof. See §8.2 in the supplementary material.

Relative Upper Confidence Bound
Symbol
K
↵
Nij (t)
wij (t)
uij (t)
lij (t)
C( )
j
ij
max

Dij
D
b )
C(
bj
D
Tb
T

a_b

Table 1. List of notation used in this section
Definition
Number of arms
The input of Algorithm 1
Number of comparisons between ai and aj until time
t
Number ofr
wins of ai over aj until time t
wij (t)
↵ ln t
+
Nij (t)
Nij (t)
1 uji (t)
Probability of failure
✓
◆ 1
(4↵ 1)K 2 2↵ 1
(2↵ 1)
p1j 0.5
i +
j
2
maxi i
4↵
4↵
, or 2 if i = 1, or 0 if i = j
2
2
min{
,
}
i
j
j
X
Dij
i<j
✓
✓ ◆
◆
2
4 max log + 2 max C
+ 2D ln 2D
2
2↵ ( j + 4 max )
2
j

Definition 3
A time between C( /2) and Tb when a1 was compared against itself
max{a, b}

Let us now turn to our first high-probability bound:
Proposition 2. Given K arms {a1 , . . . , aK } with preference matrix P = [pij ], such that a1 is the Condorcet winner, and > 0 and ↵ > 12 , then, if we apply Algorithm
1 to this K-armed dueling bandit problem, given any pair
(i, j) 6= (1, 1), the number of comparisons between arms
ai and aj performed up to time t, denoted by Nij (t), satisfies
⇣
⌘
P 9 t, (i, j) 6= (1, 1): Nij (t) > C( ) _ Dij ln t < (1)

and, Nij (t), the number of times ai was compared against
aj between time-steps C( ) and t, satisfies
⇣
⌘
P 9 t > C( ), (i, j) 6= (1, 1): Nij (t) > Dij ln t < (2)

Proof. Given Lemma 1, we know with probability 1
that pij 2 [lij (t), uij (t)] for all t > C( ). Let us first deal
with the easy case when i = j 6= 1: when t > C( ) holds,
ai cannot be played against itself, since if we get c = i in
Algorithm 1, then by Lemma 1 and the fact that a1 is the
Condorcet winner we have d 6= i since uii (t) = 12 < p1i 
u1i (t).
Now, let us assume that distinct arms ai and aj have been
compared against each other more than Dij ln t times and
that t > C( ). If s is the last time ai and aj were compared
against each other, we must have

a1
a1

1
2

ai

1
2

pi1

aj

ai

aj

i

1
2

pj1

j

Figure 1. An illustration of the proof of Proposition 2. The figure
shows an example of the internal state of RUCB at time s. The
height of the dot in the block in row am and column an represents the comparisons probability pmn , while the interval, where
present, represents the confidence interval [lmn , umn ]: we have
only included them in the (ai , aj ) and the (aj , ai ) blocks of the
figure because those are the ones that are discussed in the proof.
Moreover, in those blocks, we have included the outcomes of two
different runs: one drawn to the left of the dots representing pij
and pji , and the other to the right (the horizontal axis in these plots
has no other significance). These two outcomes are included to
address the dichotomy present in the proof. Note that for a given
run, we must have [lji (s), uji (s)] = [1 uij (s), 1 lij (s)] for
any time s, hence the symmetry present in this figure.

s

↵ ln s
Nij (t)
v
s
u
↵ ln t
u ↵ ln t
2
< 2t 4↵ ln t
Nij (t)
min{ 2 ,

uij (s)

lij (s) = 2

i

(3)
= min{
2}
j

i,

j }.

On the other hand, for ai to have been compared against
aj at time s, one of the following two scenarios must have
happened:
I. In Algorithm 1, we had c = i and d = j, in which
case both of the following inequalities must hold:
1
a. uij (s)
2 , since otherwise c could not have been
set to i by Line 5 of Algorithm 1, and
b. lij (s) = 1 uji (s)  1 p1i = pi1 , since we
know that p1i  u1i (t), by Lemma 1 and the fact
that t > C( ), and for d = j to be satisfied, we
must have u1i (t)  uji (t) by Line 6 of Algorithm
1.
From these two inequalities, we can conclude
1
uij (s) lij (s)
pi1 = i .
(4)
2

Relative Upper Confidence Bound

This inequality is illustrated using the lower right
confidence interval in the (ai , aj ) block of Figure 1,
where the interval shows [lij (s), uij (s)] and the distance between the dotted lines is 12 pi1 .
II. In Algorithm 1, we had c = j and d = i, in which
case swapping i and j in the above argument gives
1
uji (s) lji (s)
pj1 = j .
(5)
2
Similarly, this is illustrated using the lower left confidence interval in the (aj , ai ) block of Figure 1, where
the interval shows [lji (s), uji (s)] and the distance between the dotted lines is 12 pj1 .
Putting (4) and (5) together with (3) yields a contradiction,
so with probability 1
we cannot have Nij be larger than
both C( ) and Dij ln t. This gives us both (1) and (2).

✓ ◆
2

+

X
i<j

Dij ln Tb ,

With this in hand, we now state our main result:

Theorem 4. Given the setup of Proposition 2, for any >
0, we have with probability 1
that for all times T the
following bound on the cumulative regret holds:

where
b ) :=
C(

✓

4 ln

b j := D1j (
D

2

+ 2C
1j

+2

K
X
j=2

✓ ◆
2

b j ln T,
D

(6)

+ 2D ln 2D

max )

=

2↵ (

j

◆

+4
2
j

max
max )

e1 (T ) 
N

K
X
j=2

e1j (T ) 
N

K
X
j=2

b1 (T ).
D1j ln T =: N

(7)

• ⌧0 , ⌧1 , ⌧2 , . . ., where ⌧0 := T and ⌧l is the lth time arm
a1 was compared against another arm after T .
• n1 , n2 , . . ., where nl is the number of times in Algorithm
1 we had c 6= 1 6= d between ⌧l 1 and ⌧l .

which is guaranteed to exist since the expression on the left
of the inequality grows linearly with Tb and the expression
on the right grows logarithmically. Note that Tb is specified
by the K-armed dueling bandit problem.

b )+
RT  C(

eij (T ) denote the number of times arm ai was comLet N
pared against aj between times T and T . Proposition 2
eij (T ) 
shows that, again with probability 1 2 , we have N
Dij ln T for all i < j: note that this 1 2 is the same as
e1 (T ),
the one used above. In particular, this means that N
the number of times between times T and T when we had
c = 1 6= d, is bounded by

Let us introduce here two sets of random variables:

We use the next definition in what follows:
Definition 3. Let Tb be the smallest time satisfying
Tb > C

Since we have B = {a1 }, we know that when choosing
ac in Algorithm 1, the probability of choosing a1 is equal
to 12 . Given this, we can expect that from T onwards, the
algorithm will spend roughly half of its time comparing a1
against other arms. In what follows, we show that this is
indeed the case.

,

with C(·) and D as in Proposition 2, and max:=maxi i
+
and ij := i 2 j , while RT is the cumulative regret as
defined in Section 2.
Proof. If we apply Inequality (2) in Proposition 2 with
t = Tb (as in Definition 3), we know
⇣ that withi probability 1
2 C 2 , Tb when
2 there is a time T
arm a1 was compared against itself, which means that at
that time we had uj1 (T ) < 12 . This in turn implies that
B = {a1 } from that point on, since by Lemma 1 we have
that 12 < p1j  u1j (t) for all t > T > C 2 .

Now, note that RUCB chooses c 6= 1 or d 6= 1 in time-step
t if and only if uj1 (t) 12 for some j > 1 and that we can
have uj1 (t + 1) < uj1 (t) only if at the end of the tth iteration, arm a1 was compared against arm aj . In other words,
1
whenever we have uj1 (T )
2 for some j > 1, the algorithm will continue to set (c, d) 6= (1, 1) until all of the uj1
with j > 1 get submerged below 12 and that the last comparison before we get to this state must be between a1 and
another arm. With this picture in mind, with probability
1 2 , we have
RT  T

max

+

K
X

D1j

j=2

1j

ln T +

b1 (T )
N

X

nl

max ,

(8)

l=1

b1 (T ) is as in Inequality (7), and so all we need
where N
to do is bound T and the sum of the intervals nl for l =
b1 (T ). Let us deal with the former first: we know
1, . . . , N
that T  Tb and that the latter is defined to be the smallest
time-step satisfying the inequality in Definition 3, so all
we need to do is produce one number that, when plugged
in for Tb , satisfies the inequality, and one such number is
2C 2 + 2D ln 2D. To see this, let us temporarily use
the notation C := C 2 , and use the concavity of the log
function, a first order Taylor expansion, and the fact that we
have ln x < x for any x, to get
C + D ln(2C + 2D ln 2D)
2C
2D ln 2D
 C + D ln(2D)2 + C = 2C + 2D ln 2D,
 C + D ln(2D ln 2D) + ⇢
D

where we used the fact that D > 2 and so ln 2D > 1.

Relative Upper Confidence Bound

Let us now return to the task of bounding the sum of the
intervals nl . To do so, we introduce the random variables
n
b1 , n
b2 , . . ., which are independent samples from the geometric distribution with decay 21 . Note that n
bl bounds nl
from above since it counts the number of iterations it would
take for Line 11 of Algorithm 1 to produce a1 and once
we have c = 1, we are guaranteed to have a comparison
1
between a1 and another arm, as long as uj1
2 for some
j > 1. Furthermore, the sum of independent geometric random variables has a negative binomial distribution (Feller,
1968, §VI.8), with the following probability mass function,
cf. (Feller, 1968, Equation VI.8.1):
!
r
n+r 1
X
n
f (n; r) := P
n
bl = n =
,
2n+r
l=1

where in our case p = 12 and so it is eliminated from the
notation of the PMF. In order to bound this sum with high
probability, we note that when n 2r, then we have
n+r 1
(n + r 1)!
n
n+r
f (n; r)
n!(r 1)!
= 2n+r
=
(n + r)!
f (n + 1; r)
n+1
n+r+1
(n
+
1)!(r
1)! ⇥ 2
2
2(n + 1)
r 1
2r 2
4
=
=2 1
2
> .
n+r
n+r
3r
3
n 2r

n 2r

Thus, we have f (n; r)  f (2r; r) 34
 34
for
all n
2r, since f (2r; r) is a probability and so at most
equal to 1. From this we can conclude that with probability
ln
1 2 , we have n  2r + 23 < 2r 4 ln 2 : note that both
ln 4
the numerator and the denominator of the second summand
are negative andPso the fraction is positive. Now, setting
b1 (T ) := K D1j ln T and plugging the resulting
r=N
j=2
upper bound into the regret bound given in (8) give us the
desired result.
Next, we state our expected regret bound, which is a direct
consequence of Theorem 4:
Theorem 5. Given the setup of Proposition 2 together with
the notation of Theorem 4, we have the following expected
regret bound for RUCB, where the expectations are taken
across different runs of the algorithm: if we have ↵ > 1, the
expected regret accumulated by RUCB after T iterations is
bounded by
"
#
✓
◆ 1
2(4↵ 1)K 2 2↵ 1 2↵ 1
E[RT ]  8 +
max
2↵ 1
↵ 1
+ 2D

max

ln 2D +

K
X
2↵ (
j=2

j

+4

max )

2
j

Proof. See §8.3 in the supplementary material.

ln T,

Remark 6. (1) Using a very similar argument as the one
used to prove Theorem 5, we can also bound the mth moment of RT whenever we have ↵ > m+1
2 , which can be
used to bound its variance for ↵ > 1.5.
(2) In general, our regret bounds are not directly comparable to those of IF and BTM, since those bounds depend
only on min ; so, if the majority of the j are larger than
min , then our upper bound is lower than that of IF and
BTM. On the other hand, if most j are close to min , but
max is much larger, then the upper bound for IF would be
lower: the same would hold for BTM if is small.
(3) Note that RUCB uses the upper-confidence bounds
(Line 3 of Algorithm 1) introduced in the original version of UCB (Auer et al., 2002) (up to the ↵ factor). Recently refined upper-confidence bounds (such as UCB-V
(Audibert et al., 2009) or KL-UCB (Cappé et al., 2013))
have improved performance for the regular K-armed bandit problem. However, in our setting the arm distributions
are Bernoulli and the comparison value is 1/2. Thus, since
we have 2 2i  kl(p1,i , 1/2)  4 2i (where kl(a, b) =
a ln ab + (1
a) ln 11 ab is the KL divergence between
Bernoulli distributions with parameters a and b), we deduce that using KL-UCB instead of UCB does not improve
the leading constant in the logarithmic term of the regret by
a numerical factor of more than 2.

6. Experiments
To evaluate RUCB, we apply it to the problem of
ranker evaluation from the field of information retrieval
(IR) (Manning et al., 2008). A ranker is a function that
takes as input a user’s search query and ranks the documents in a collection according to their relevance to that
query. Ranker evaluation aims to determine which among
a set of rankers performs best. One effective way to achieve
this is to use interleaved comparisons (Radlinski et al.,
2008), which interleave the documents proposed by two
different rankers and presents the resulting list to the user,
whose resulting click feedback is used to infer a noisy preference for one of the rankers. Given a set of K rankers, the
problem of finding the best ranker can then be modeled as
a K-armed dueling bandit problem, with each arm corresponding to a ranker.
We evaluated RUCB, Condorcet SAVAGE and BTM using
randomly chosen subsets from the pool of 64 rankers provided by LETOR, a standard IR dataset (see §8.4 for more
details of the experimental setup), yielding K-armed dueling bandit problems with K 2 {16, 32, 64}. For each
set of rankers, we performed 100 independent runs of each
algorithm for a maximum of 4.5 million iterations. For
RUCB we set ↵ = 0.51, which approaches the limit set
by our high-probability result. Since BTM and SAVAGE

Relative Upper Confidence Bound
LETOR NP2004 Dataset with 16 rankers

LETOR NP2004 Dataset with 32 rankers

LETOR NP2004 Dataset with 64 rankers

35000

140000

30000

120000

6000

4000

cumulative regret

cumulative regret

cumulative regret

8000

25000
20000
15000

BTM
Condorcet SAVAGE
RUCB = 0.51

100000
80000
60000

10000

40000

5000

20000

2000

103

104

105
time

106

103

104

105
time

106

103

104

105
time

106

Figure 2. Average cumulative regret for 100 runs of BTM, Condorcet SAVAGE and RUCB with ↵ = 0.51 applied to three K-armed
dueling bandit problems with K = 16, 32, 64. Note the time axis uses a log scale, so that the curves depict the relation between log T
and RT ; also, the dotted curves signify best and worst regret performances across all runs.

require the exploration horizon as input, we ran BTMT and
CSAVAGET for various horizons T ranging from 1000 to
4.5 million. In the plots in Figure 2, the markers on the
green and the blue curves show the regret accumulated by
BTMT and CSAVAGET in the first T iteration of the algorithm for each of these horizons. Thus, each marker corresponds, not to the continuation of the runs that produced the
previous marker, but to new runs conducted with a larger T .
Since RUCB is horizonless, we ran it for 4.5 million iterations and plotted the cumulative regret, as shown using
the red curves in the plots in Figure 2. For all three algorithms, the middle curve shows average cumulative regret
and the dotted lines show minimum and maximum cumulative regret across runs. Note that these plots are in loglinear scale, so they depict the relation between RT and
log T , which can be seen to be asymptotically linear. The
regret curves for BTM are cut-off in these plots, since in
all three experiments RTBT MT grew linearly with T in the
first 4.5 million iterations. As can be seen from the plots in
Figure 2, RUCB accumulates the least regret of the three algorithms: the average regret accumulated by RUCB is less
than half of that Condorcet SAVAGE by the end of each of
the three experiments and even the worst performing run of
RUCB accumulated considerably less regret than the best
performing run of Condorcet SAVAGE.

7. Conclusions
This paper proposed a new method called Relative Upper Confidence Bound (RUCB) for the K-armed dueling
bandit problem that extends the Upper Confidence Bound
(UCB) algorithm to the relative setting by using optimistic
estimates of the pairwise probabilities to choose a potential
champion and conducting regular UCB with the champion
as the benchmark.
We proved finite-time high-probability and expected regret
bounds for RUCB that match an existing lower bound. Un-

like existing results, our regret bounds hold for all timesteps, rather than just a specific horizon T input to the algorithm. Furthermore, they take the form O(K log T ) while
making much less restrictive assumptions than existing algorithms with similar bounds. Finally, the empirical results showed that RUCB greatly outperforms state-of-theart methods.
In future work, we will consider two extensions to this research. First, building off extensions of UCB to the continuous bandit setting (Srinivas et al., 2010; Bubeck et al.,
2011; Munos, 2011; de Freitas et al., 2012; Valko et al.,
2013), we aim to extend RUCB to the continuous dueling
bandit setting, without a convexity assumption as in (Yue
& Joachims, 2009; Jamieson et al., 2012). Second, building off Thompson Sampling (Thompson, 1933; Agrawal &
Goyal, 2012; Kauffmann et al., 2012), an elegant and effective sampling-based alternative to UCB, we will investigate
whether a sampling-based extension to RUCB would be
amenable to theoretical analysis. Both these extensions involve overcoming not only the technical difficulties present
in the regular bandit setting, but also those that arise from
the two-stage nature of RUCB. Since the submission of this
paper, the latter of these two ideas has been validated experimentally in (Zoghi et al., 2014), although a theoretical
analysis is still lacking.
Acknowledgments
This research was partially supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreements nr 270327, nr 288024 and nr 312827, the
Netherlands Organisation for Scientific Research (NWO) under
project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013,
the Center for Creation, Content and Technology (CCCT), the
QuaMerdes project funded by the CLARIN-nl program, the
TROVe project funded by the CLARIAH program, the Dutch national program COMMIT, the ESF Research Network Program
ELIAS, the Elite Network Shifts project funded by the Royal
Dutch Academy of Sciences (KNAW), the Netherlands eScience
Center under project number 027.012.105 the Yahoo! Faculty Research and Engagement Program, the Microsoft Research PhD
program, and the HPC Fund.

Relative Upper Confidence Bound

References
Abbasi-yadkori, Y., Pal, D., and Szepesvari, C. Improved
algorithms for linear stochastic bandits. In NIPS, 2011.
Agrawal, R. Sample mean based index policies with
o(logn) regret for the multi-armed bandit problem. Advances in Applied Probability, 27(4):10541078, 1995.
Agrawal, S. and Goyal, N. Analysis of thompson sampling
for the multi-armed bandit problem. In Conference on
Learning Theory, pp. 1–26, 2012.
Audibert, J.-Y., Munos, R., and Szepesvári, C.
Exploration-exploitation tradeoff using variance estimates in multi-armed bandits. Theor. Comput. Sci.,
410(19):1876–1902, 2009.
Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235–256, 2002.
Bartók, G., Zolghadr, N., and Szepesvari, C. An adaptive algorithm for finite stochastic partial monitoring. In
ICML, 2012.
Bubeck, S., Munos, R., and Stoltz, G. Pure exploration in
multi-armed bandits problems. In Algorithmic Learning
Theory, 2009.
Bubeck, S., Munos, R., Stoltz, G., and Szepesvari, C. Xarmed bandits. Journal of Machine Learning Research,
12:1655–1695, 2011.
Cappé, O., Garivier, A., Maillard, O.-A., Munos, R., and
Stoltz, G. Kullback-Leibler upper confidence bounds for
optimal sequential allocation. Annals of Statistics, 41(3):
1516–1541, 2013.
Cesa-Bianchi, N. and Lugosi, G. Prediction, Learning, and
Games. Cambridge University Press, 2006.
Craswell, N., Zoeter, O., Taylor, M., and Ramsey, B. An
experimental comparison of click position-bias models.
In WSDM ’08, pp. 87–94, 2008.
de Freitas, N., Smola, A., and Zoghi, M. Exponential regret
bounds for Gaussian process bandits with deterministic
observations. In ICML, 2012.
Feller, W. An Introduction to Probability Theory and Its
Applications, volume 1. Wiley, 1968.
Fürnkranz, J. and Hüllermeier, E. (eds.). Preference Learning. Springer-Verlag, 2010.
Guo, F., Liu, C., and Wang, Y. Efficient multiple-click
models in web search. In WSDM ’09, pp. 124–131, New
York, NY, USA, 2009. ACM.
Hofmann, K., Whiteson, S., and de Rijke, M. A probabilistic method for inferring preferences from clicks. In
CIKM ’11, pp. 249–258, USA, 2011. ACM.
Hofmann, K., Whiteson, S., and de Rijke, M. Balancing
exploration and exploitation in listwise and pairwise online learning to rank for information retrieval. Informa-

tion Retrieval, 16(1):63–90, 2013a.
Hofmann, Katja, Whiteson, Shimon, and de Rijke,
Maarten. Fidelity, soundness, and efficiency of interleaved comparison methods. ACM Transactions on Information Systems, 31(4), 2013b.
Jamieson, K., Nowak, R., and Recht, B. Query complexity
of derivative-free optimization. In NIPS, 2012.
Joachims, T. Optimizing search engines using clickthrough
data. In KDD ’02, pp. 133–142, 2002.
Kauffmann, E., Korda, N., and Munos, R. Thompson sampling: an asymptotically optimal finite time analysis. In
International Conference on Algorithmic Learning Theory, 2012.
Lai, T. L. and Robbins, H. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics,
6(1):4–22, 1985.
Liu, T.-Y., Xu, J., Qin, T., Xiong, W., and Li, H. Letor:
Benchmark dataset for research on learning to rank for
information retrieval. In LR4IR ’07, in conjunction with
SIGIR ’07, 2007.
Manning, C., Raghavan, P., and Schütze, H. Introduction
to Information Retrieval. Cambridge University Press,
2008.
Munos, R. Optimistic optimization of a deterministic function without the knowledge of its smoothness. In NIPS,
2011.
Radlinski, F., Kurup, M., and Joachims, T. How does clickthrough data reflect retrieval quality? In CIKM ’08, pp.
43–52, 2008.
Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M.
Gaussian process optimization in the bandit setting: No
regret and experimental design. In ICML, 2010.
Thompson, W.R. On the likelihood that one unknown probability exceeds another in view of the evidence of two
samples. Biometrika, pp. 285–294, 1933.
Urvoy, T., Clerot, F., Féraud, R., and Naamane, S. Generic
exploration and k-armed voting bandits. In ICML, 2013.
Valko, M., Carpentier, A., and Munos, R. Stochastic simultaneous optimistic optimization. In ICML, 2013.
Yue, Y. and Joachims, T. Interactively optimizing information retrieval systems as a dueling bandits problem. In
ICML, 2009.
Yue, Y. and Joachims, T. Beat the mean bandit. In ICML,
2011.
Yue, Y., Broder, J., Kleinberg, R., and Joachims, T. The
K-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5):1538–1556, September 2012.
Zoghi, M., Whiteson, S., de Rijke, M., and Munos, R.
Relative confidence sampling for efficient on-line ranker
evaluation. In WSDM ’14, 2014.

