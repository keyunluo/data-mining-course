Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

Jacob Steinhardt
Percy Liang
Stanford University, 353 Serra Street, Stanford, CA 94305 USA

JSTEINHARDT @ CS . STANFORD . EDU
PLIANG @ CS . STANFORD . EDU

Abstract

D∞

We present an adaptive variant of the exponentiated gradient algorithm. Leveraging the optimistic learning framework of Rakhlin & Sridharan (2012), we obtain regret bounds that in the
learning from experts setting depend on the variance and path length of the best expert, improving on results by Hazan & Kale (2008) and Chiang et al. (2012), and resolving an open problem
posed by Kale (2012). Our techniques naturally
extend to matrix-valued loss functions, where we
present an adaptive matrix exponentiated gradient algorithm. To obtain the optimal regret bound
in the matrix case, we generalize the Follow-theRegularized-Leader algorithm to vector-valued
payoffs, which may be of independent interest.

Chiang et al. (2012)

maxi Di

maxi Vi

Di∗

Vi∗

this work

The exponentiated gradient (EG) algorithm is a powerful
tool for performing online learning in the presence of many
irrelevant features (Kivinen & Warmuth, 1997; Littlestone,
1988). EG is often used in the “learning from experts” setting, in which it is also known as the weighted majority
algorithm (Littlestone & Warmuth, 1989). In this setting,
EG entertains regret bounds of the form
T
X
log(n)
+η
kzt k2∞ ,
η
t=1

S∞
Kivinen & Warmuth (1997)

maxi Si

Hazan & Kale (2008)

1. Introduction

Regret ≤

V∞

(1)

where η is the step size, zt is the vector of losses, and n
is the number of experts. Such bounds (as well as slightly
stronger bounds based on local norms) can be obtained under the mirror descent framework, a general tool that gives
rise to many other online learning algorithms (see ShalevShwartz (2011) for a survey).
In contrast, Cesa-Bianchi et al. (2007) present a variant of this algorithm based on a multiplicative update of
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

Si∗
Cesa-Bianchi et al. (2007)

Figure 1. Summary of possible regret bounds with references to
algorithms known to achieve these bounds. An arrow A → B
indicates that A is a strictly better bound than B. Our algorithm
simultaneously improves upon several existing results. D represents the path length, V the variance, and S the second moment;
these quantities are defined formally in Section 3, Equation 24.
Even in situations where Di∗ is Θ(1), both maxi Di and Vi∗
(and hence all other entries in the lattice) can be Θ(T ).

wt+1,i ∝ wt,i (1 − ηzt,i ) rather than the usual EG update of
wt+1,i ∝ wt,i exp(−ηzt,i ). This algorithm cannot be cast
in the mirror descent framework with a fixed regularizer,
yet it achieves an improved regret bound of
Regret ≤

T
X
log(n)
2
+η
zt,i
∗.
η
t=1

(2)

Comparing the regret bounds (2) and (1), note that (2) is in
terms of the best expert i∗ instead of a maximum over all
experts. This latter bound can be much stronger;
we show
√
in Proposition 2.2 that there is in fact a Θ( T ) separation
of the worst-case regret in the setting where the best expert has loss identically equal to zero. Other differences
between these two types of updates are discussed in Arora
et al. (2012).
The fact that an algorithm achieving a better regret bound
cannot be cast in the mirror descent framework is a bit unsettling. Does this mean we should abandon mirror descent

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

as the gold standard for online learning, despite theorems
asserting its optimality (Srebro et al., 2011)? We answer
this question in the negative: the (1 − ηzt,i ) update can be
understood as a form of adaptive mirror descent (Orabona
et al., 2013), where the regularizer changes in each round
t in response to previously observed vectors z1:t . We obtain a natural interpretation of the update as performing a
second-order correction to the gradient.
Examining (2) more closely, we see that this corrected update should perform well when the best expert i∗ incurs
losses consistently
to zero; then the second term in
PT close
2
the regret is t=1 zt,i
∗ ≈ 0. However, this assumption
may be unrealistic, and many authors have recently considered variance bounds that depend only on the deviation of zt from its average, or path-length bounds in terms
of zt − zt−1 (Hazan & Kale, 2008; Chiang et al., 2012;
Yang et al., 2013). Rakhlin & Sridharan (2012) present
an optimistic learning framework that yields such bounds
for any mirror descent algorithm. However, the updates in
Hazan & Kale (2008) are not mirror descent updates (for
any fixed regularizer), and their bounds are incomparable
to the bounds obtained via optimistic learning.
In the learning from experts setting, we subsume all the previously mentioned bounds by obtaining a bound in terms of
the path length of the best expert:
Regret ≤

T
X
log(n)
+η
(zt,i∗ − zt−1,i∗ )2 .
η
t=1

(3)

Obtaining such a bound is posed as an open problem in
Kale (2012). We achieve such a regret bound (Equation 23)
by applying Rakhlin’s updates in the context of an adaptive
mirror descent algorithm, thus obtaining an adaptive optimistic exponentiated gradient algorithm. When the path
length is not known and η must be determined adaptively,
our bounds weaken slightly but are still strong enough to
answer the problem in Kale (2012), as well as to subsume
all of the previously mentioned bounds in the adaptive step
size setting.
Finally, we extend all these results to the matrix setting,
where the learner plays a positive semidefinite matrix Wt
with trace 1 (in analogy with the simplex). This setting has
been extensively studied (Tsuda et al., 2005; Arora & Kale,
2007) and is important in obtaining online and approximation bounds for various combinatorial optimization problems (Arora & Kale, 2007; Hazan et al., 2012). As far as
we are aware, the best known results in this setting are of
the form (1). Using the machinery so far developed, all
of our results extend naturally to the matrix setting. However, for the variance bound we need a new analysis tool:
a variant of FTRL for vector-valued losses ordered relative
to some cone K.
In summary, the main contributions of this paper are:

• An interpretation of the multiplicative weights update
of Cesa-Bianchi et al. (2007) as exponentiated gradient with an adaptive regularizer (Section 2).
• An improved exponentiated gradient algorithm obtaining best-known variance and path-length bounds
(Section 3).
• An adaptive matrix exponentiated gradient algorithm
attaining similar bounds (Section 4).
• A generalization of Follow-the-Regularized-Leader to
vector-valued loss functions (Lemma 4.3).
Related work. There is a rich literature on using adaptive
updates to obtain better regret bounds for online learning.
A common setting is adaptive learning of a quadratic regularizer, as in the AROW (Crammer et al., 2009), AdaGrad
(Duchi et al., 2011), and online preconditioning (Streeter
& McMahan, 2010) algorithms. Other work includes
dimension-free exponentiated gradient (Orabona, 2013),
whitened perceptron (Cesa-Bianchi et al., 2005), and online adaptation of the step size (Hazan et al., 2007). The
non-stationary setting was explored by Vaits et al. (2013),
and McMahan & Streeter (2010) obtain regret bounds relative to a family of regularizers. More recently, many of
these algorithms have been unified into a single framework
by Orabona et al. (2013). To our knowledge, adaptively
regularized exponentiated gradient has not been explicitly
explored, though many variants on the basic multiplicative updates have been proposed (Cesa-Bianchi et al., 2007;
Hazan & Kale, 2008; Chiang et al., 2012), which can be interpreted in our framework as making implicit use of an
adaptive regularizer.
In addition to the variants on exponentiated gradient discussed above, Auer & Warmuth (1998) and Herbster &
Warmuth (1998) have studied the case where the best expert can change over time. Finally, Sabato et al. (2012)
consider a generalization of the Winnow algorithm (Littlestone, 1988), which corresponds to exponentiated gradient
with a hinge-like loss, and provide a careful analysis of the
regret that is more precise than the mirror descent analysis.

2. A Tale of Two Updates
Our point of departure is the two different types of multiplicative updates mentioned in the introduction. For simplicity we will consider the setting of learning from expert
advice.1 In this setting there are n experts, and the learner
maintains a probability distribution wt ∈ ∆n over the experts. In each round t = 1, . . . , T , the learner plays wt ,
a vector zt ∈ [−1, 1]n is revealed, and the learner incurs
1
The general setting follows a nearly identical analysis and is
covered in the supplementary material.

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

Name
EG (MW1)
MW2
Variation-MW
Optimistic MW
AEG-Path
AMEG-Path

Update
βt+1 = βt − ηzt
βt+1,i = βt,i + log(1 − ηzt,i )
βt+1,i = βt,i − ηzt,i − 4η 2 (zt,i − mt,i )2
Pt−1
mt = 1t s=1 zs
βt+1,i = βt,i − ηzt,i
βt+1,i = βt,i − ηzt,i − η 2 (zt,i − zt−1,i )2
Bt+1 = Bt − ηZt − η 2 (Zt − Zt−1 )2

Prediction
exp(βt )
exp(βt )

Source
(Kivinen & Warmuth, 1997)
(Cesa-Bianchi et al., 2007)

exp(βt )

(Hazan & Kale, 2008)

exp(βt − ηzt−1 )
exp(βt − ηzt−1 )
exp(Bt − ηZt−1 )

(Chiang et al., 2012)
this work
this work

Table 1. An overview of known adaptive exponentiated gradient algorithms. The AEG-Path updates incorporate components of both
the Variation-MW and Optimistic MW algorithms, and are motivated by interpreting MW2 in terms of adaptive mirror descent. The
AMEG-Path updates extend AEG-Path to the matrix case (which had previously only been done for MW1).

loss wt> zt . The learner’s goal is to minimize the regret
supu∈∆n Regret(u), where
def

Regret(u) =

T
X
t=1

wt> zt −

T
X

u> zt .

(4)

t=1

The learner starts by playing w1 , where w1,i = n1 for 1 ≤
i ≤ n. On subsequent iterations, we consider two types of
updates for the weight vector wt , as shown in (MW1) and
(MW2) below:
wt+1,i ∝ wt,i exp(−ηzt,i )

(MW1)

wt+1,i ∝ wt,i (1 − ηzt,i ),

(MW2)

where η is the step size. The regret bounds for each of
(MW1) and (MW2) are well-known (see Shalev-Shwartz
(2011) and Cesa-Bianchi et al. (2007) respectively) but we
include them for completeness.
Theorem 2.1. For any 0 < η ≤ 12 and kzt k∞ ≤ 1, the
updates (MW1) and (MW2) obtain respective regret bounds
of
Regret(u) ≤

n X
T
X
log(n)
2
+η
wt,i zt,i
η
i=1 t=1

(5)

Regret(u) ≤

n
T
X
X
log(n)
2
+η
ui
zt,i
η
t=1
i=1

(6)

To understand why (6) may be a better bound than (5), suppose that the best expert has loss identically equal to zero.2
Then the optimal u places all mass on that expert, and (6)
reduces to log(n)
= 2 log(n) for η = 12 .
η
More formally, define a sequence of losses zt to be quasirealizable if one of the experts i∗ has identically zero loss
and
PT all other experts have non-negative cumulative loss, i.e.
t=1 zt,i ≥ 0. It is apparent by the preceding paragraph
2

Of course, if we knew that this was the case ahead of time,
there would be far better algorithms; we use this scenario purely
for illustrative purposes.

that (MW2) achieves asymptotically constant (as a function
of T ) regret for any quasi-realizable
sequence. In contrast,
√
(MW1) can suffer Ω( T ) regret:
Proposition 2.2. For any step size η and T , there is a
quasi-realizable loss sequence (zt )Tt=1 and a vector u ∈
∆n√such that the updates (MW1) result in Regret(u) =
Ω( T ).
The proof is given in the supplementary material, but the
main idea is that (MW1) will have trouble distinguishing
between an expert whose loss is always zero and an expert
whose loss alternates between 1 and −1. This establishes
that the apparent separation between (MW1) and (MW2) is
real and not an artifact of the analysis. We remark that this
separation does not exist when all losses are non-negative.
In this case both (MW1) and (MW2) enjoy O(1) regret (as
a function of T ).
Finally, note that (MW2) cannot be realized as mirror descent for any fixed regularizer. This is because, for any mirror descent algorithm,
Pt the prediction on round t + 1 must
be a function of s=1 zs , which is not the case for (MW2).
Adaptive mirror descent However, not all is lost, as
we will obtain (MW2) in terms of an adaptive regularizer
ψt (w). The mirror descent predictions for an adaptive regularizer are given by
wt =

∇ψt∗

(θt ) ,

def

θt = −η

t−1
X

zs ,

(7)

s=1
def

where ψ ∗ (x) = supw {w> x − ψ(w)} is the Fenchel conjugate of ψ. We provide general properties of Fenchel
conjugates as well as several calculations of interest in the
supplementary material. See Orabona et al. (2013) for a
more complete exposition on adaptive mirror descent, and
Shalev-Shwartz (2011) for a general survey.
We can cast (MW2) in the adaptive mirror descent framework, as detailed in Proposition 2.3 below. As we will ex-

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

plain in the next section, these updates have a natural interpretation as “pushing the regret into the regularizer”.
def Pt−1
Proposition 2.3. Define βt,i =
s=1 log(1 − ηzs,i ) and
let
def

ψt (u) =

n
X

ui log(ui ) + u> (θt − βt ).

(8)

Pt−1
and if mt = 1t s=1 zs , we obtain variance bounds. We
illustrate geometrically in Figure 2 how optimistic updates
can improve the regret bound.
We combine optimistic learning (Rakhlin & Sridharan,
2012) with adaptive regularization (Orabona et al., 2013)
to yield Algorithm 1.

i=1

Then adaptive mirror descent with regularizer ψt corresponds exactly to the updates (MW2). The corresponding
regret bound is
ψ1∗ (θ1 ) + ψT +1 (u)
η
n
T
X
X
log(n)
2
+η
≤
ui
zt,i
.
η
t=1
i=1

Regret(u) ≤

(9)
(10)

Algorithm 1 Adaptive Optimistic Mirror Descent
Given: convex regularizers ψt and hints mt
Initialize θ1 = 0
for t = 1 to T do
Choose wt = ∇ψt∗ (θt − ηmt )
Observe zt and suffer loss wt> zt
Update θt+1 = θt − ηzt
end for
The regret bound for Algorithm 1 is given in Theorem 3.1:

Proof. By standard properties of Fenchel conjugates, we
have
∇ψt∗ (θt ) = arg min ψt (w) − w> θt

(11)

w∈∆n

= arg min
w∈∆n

n
X

wi log(wi ) − w> βt .

(12)

Theorem 3.1. Suppose that for all t, ψt is convex and satisfies the loss-bounding property:
∗
ψt+1
(θt − ηzt ) ≤ ψt∗ (θt − ηmt ) − ηwt> (zt − mt ). (13)

Then
Regret(u) ≤

i=1

ψ1∗ (θ1 ) + ψT +1 (u)
.
η

(14)

Qt−1

From here we see that wt,i ∝ exp(βt,i ) = s=1 (1−ηzs,i ),
so that wt,i does indeed correspond to (MW2).
We omit the proof of the regret bound; it follows straightforwardly from the machinery in the next section (see
Proposition 3.3).
Proposition 2.3 says we can obtain bounds that depend on
2
∗
the the average squared loss zt,i
(u
∗ of the best expert i
∗
places all its mass on i ). But intuitively, we would like
to not suffer much regret even if zt,i∗ is large so long as
its variation is small. We turn to this issue in the next section.

3. Adaptive Optimistic Learning
In the previous section, we saw how to obtain regret bounds
that depend on the best expert i∗ , but involve the second
moment. Next, we show how to use the idea of optimistic
learning (Rakhlin & Sridharan, 2012) to obtain results that
depend on variance or path length.
In the optimistic learning framework, we are given a sequence of “hints” mt of what zt might be. Then rather than
choosing wt based on the negative cumulative gradients θt ,
we choose wt based on a preemptive update θt − ηmt . The
resulting regret bounds thus depend on the error in the hints
(zt − mt ) rather than zt . If mt = 0, we recover vanilla mirror descent; if mt = zt−1 , we obtain path-length bounds;

Proof. The proof is a relatively straightforward combination of known results. First note that ψt∗ is convex and that
wt = ∇ψt∗ (θt − ηmt ). Thus, ψt∗ (θt ) ≥ ψt∗ (θt − ηmt ) +
ηwt> mt . Then, by definition of the Fenchel conjugate together with telescoping sums, we have, for any u,
u> θT +1 − ψT +1 (u)
≤ ψT∗ +1 (θT +1 )
= ψ1∗ (θ1 ) +

T
X

∗
ψt+1
(θt+1 ) − ψt∗ (θt )

t=1

≤ ψ1∗ (θ1 ) +

T
X

∗
ψt+1
(θt+1 ) − ψt∗ (θt − ηmt ) − ηwt> mt .

t=1

By the conditions of the theorem, the sum is termwise upper bounded by −ηwt> zt and we have
u> θT +1 + η

T
X

wt> zt ≤ ψ1∗ (θ1 ) + ψT +1 (u).

(15)

t=1

Expanding θT +1 as −η

PT

t=1 zt

completes the proof.

The key intuition, also spelled out by Orabona et al. (2013),
is that, if we make ψt+1 − ψt large enough to “swallow
the regret” on round t, then we obtain bounds that depend

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm
ψ ∗ (θt − ηzt )

ψ ∗ (θt − ηzt )
ψ ∗ (θt − ηmt ) − ηwt> (zt − mt )
ψ ∗ (θt − ηmt )

ψ ∗ (θt ) − ηwt> zt

∗

∗

ψ (θt )

ψ (θt )

Figure 2. Illustration of how optimistic updates affect the regret bound. For a fixed regularizer ψ ∗ , the increase in regret is bounded
above by ψ ∗ (θt+1 ) − ψ ∗ (θt ) − ηwt> zt . Normally wt = ∇ψ ∗ (θt ), so that the bound is equal to the gap between ψ ∗ and its tangent line,
as illustrated on the left. For optimistic updates we instead take wt = ∇ψ ∗ (θt − ηmt ), which replaces the tangent line by the dashed
line on the right. This dashed line can be bounded by the tangent line at θt − ηmt , depicted as the solid line on the right.

on the regularizer ψT +1 (u), rather than typical bounds that
depend on Bregman divergences between θt and θt+1 .3
Regularization based on corrections While Theorem 3.1 deals with general sequences of regularizers ψt ,
for our purposes we will only need to consider regularizers
of a special form:
"
ψt (w) = ψ(w) − w> β1 − η 2

t−1
X

#
as ,

Pt−1
def
Define ψt (w) = ψ(w) − w> [β1 − η 2 s=1 as ]. Note that
ψt (w) = ψ(w) − w> (βt − θt ) and hence ψt∗ (x) = ψ ∗ (x +
(βt − θt )). Then, looking at the condition of Theorem 3.1,
∗
∗
we have ψt+1
(θt − ηzt ) = ψt+1
(θt+1 ) = ψ ∗ (βt+1 ) and
∗
∗
ψt (θt − ηmt ) = ψ (βt − ηmt ), so that the conditions on ψ
and at in this corollary match those on ψt in Theorem 3.1.
The corresponding regret bound is
Regret(u) ≤

(16)

s=1

where ψ is a fixed regularizer and at is a sequence of corrections. This choice of regularizer yields the more specialized Algorithm 2, which can be interpreted as performing
second-order corrections to the typical gradient updates.

ψ1∗ (θ1 ) + ψT +1 (u)
η

ψ ∗ (β1 ) + ψ(u) + u> [−β1 + η 2
=
η
=

PT

t=1

at ]

T
X
ψ ∗ (β1 ) + ψ(u) − u> β1
+ ηu>
at ,
η
t=1

as was to be shown.
Algorithm 2 Adaptive Optimistic Mirror Descent (specialized to corrections)
Given: convex regularizer ψ, corrections at and hints mt
Initialize β1 arbitrarily
for t = 1 to T do
Choose wt = ∇ψ ∗ (βt − ηmt )
Observe zt and suffer loss wt> zt
Update βt+1 = βt − ηzt − η 2 at
end for
Corollary 3.2. Suppose ψ is convex and at is such that
ψ ∗ (βt −ηzt −η 2 at ) ≤ ψ ∗ (βt −ηmt ) − ηwt> (zt −mt ). Then
Regret(u) ≤

T
X
ψ ∗ (β1 ) + ψ(u) − u> β1
+ ηu>
at .
η
t=1

(17)
Proof. The proof essentially consists of translating into the
language of Theorem 3.1 and making use of the property
that the Fenchel conjugate of w 7→ ψ(w) − w> c is x 7→
ψ ∗ (x + c).
3
The typical Bregman divergence bound can be recovered by
setting ψt+1 (w) to ψt (w) + Dψ∗ (θt+1 kθt ).

To give some intuition for the condition in Corollary 3.2,
note that wt = ∇ψ ∗ (βt − ηmt ), and so ψ ∗ (βt − ηzt ) ≈
ψ ∗ (βt − ηmt ) − ηwt> (zt − mt ). Since ψ ∗ is convex, we
actually have ψ ∗ (βt − ηzt ) ≥ ψ ∗ (βt − ηmt ) − ηwt> (zt −
mt ), so we can view the subtraction of η 2 at as a secondorder correction that flips the sign of the inequality. The η 2
coefficient in front of at is motivated by the fact that the
second-order term in the Taylor expansion of ψ ∗ (βt − ηzt )
is of order η 2 , and so for the η 2 at term to cancel this out
we need at to be of constant order.
Adaptive step size. The exposition so far assumes a fixed
step size η, and the subsequent bounds we present will assume that the optimal value of η is known. In practice, it
is rarely the case that we know this optimal value in advance, and it is thus necessary to choose η adaptively. We
ignore this issue in the main text, but an adaptive scheme
following Cesa-Bianchi et al. (2007) is provided in the supplementary material for the interested reader. We note that,
for the adaptive case, our regret bound is slightly worse and
corresponds to the maxi Di entry in Figure 1.
Application to exponentiated gradient. Using the adaptive optimistic mirror descent framework, we can now ob-

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

tain an adaptive exponentiated gradient algorithm that incorporates hints mt . The algorithm
Pn is obtained from Algorithm 2 by setting ψ(w) = i=1 wi log(wi ) and at,i =
(zt,i − mt,i )2 . This choice of correction at makes intuitive
sense, as it will downweight experts i for whom the hints
mt,i are inaccurate.
Proposition 3.3 (Adaptive Exponentiated Gradient). Consider the updates given by β1,i = 0 and βt+1,i = βt,i −
ηzt,i − η 2 (zt,i − mt,i )2 , with prediction wt,i ∝ exp(βt,i −
ηmt,i ). Then, assuming kzt k∞ ≤ 1, kmt k∞ ≤ 1 and
0 < η ≤ 41 , we have for all u ∈ ∆n :
n
T
X
X
log(n)
Regret(u) ≤
+η
ui
(zt,i − mt,i )2 . (18)
η
t=1
i=1

Proof. Corollary 3.2 reduces the proof toPstraightforward
n
computation. Note that, for ψ(w) =
i=1 wi log(wi )
and P
w constrained to the simplex ∆n , ψ ∗ (β) =
n
log( i=1 exp(βi )) and ∇ψ ∗ (βt − ηmt ) is equal to wt as
defined in the proposition. The updates above thus correspond to Algorithm 2 and so it suffices to check that the
main condition of Corollary 3.2 is satisfied with at,i =
(zt,i − mt,i )2 . This follows from the calculation:

verified the condition of Corollary 3.2, we obtain a regret
∗
Pn
bound of ψ (0)+ψ(u)
+ η i=1 u> at . Finally, we note that
η
Pn
ψ ∗ (0) = log(n), ψ(u) = i=1 ui log(ui ) ≤ 0, and at,i =
(zt,i − mt,i )2 , which completes the proof.
Comparison to (MW2). For mt = 0 we obtain the same
regret bound (6) that was obtained for the update (MW2).
Interestingly, the two updates are essentially the same to
second order:

versus

2
βt+1,i = βt,i − ηzt,i − η 2 zt,i

(19)

βt+1,i = βt,i + log(1 − ηzt,i ).

(20)

Since −x − x2 ≤ log(1 − x) when |x|≤ 12 , we can
think of the adaptive EG updates as a second-order underapproximation to (MW2) when mt = 0. The regret bound
(6) for (MW2) can be obtained by a near-identical calculation to the one in Proposition 3.3.
Variance bound.
a variance bound

1
t

By setting mt =

Regret ≤

n
X
= log(
exp(βt,i − ηzt,i − η 2 (zt,i − mt,i )2 ))

def

Vi =

log(n)
+ η(2Vi∗ + 6),
η

T
X
(zt,i − z̄i )2 ,

def

z̄ =

t=1

i=1
n
X
= log(
exp(βt,i − ηmt,i ) exp(−η(zt,i − mt,i )
i=1

− η 2 (zt,i − mt,i )2 ))
n
X
exp(βt,i − ηmt,i )(1 − η(zt,i − mt,i )))
≤ log(
i=1
n
X
= log(
exp(βt,i − ηmt,i )
i=1

−η

s=1 zs ,

we obtain

(21)

where i∗ is the best expert and

ψ ∗ (βt − ηzt − η 2 at )

n
X

Pt−1

exp(βt,i − ηmt,i )(zt,i − mt,i ))

T
1X
zt
T t=1

(22)

is the variance of expert i. This improves the result
in Hazan & Kale (2008), who obtain a regret based on
maxni=1 Vi rather than Vi∗ .4
The choice of mt corresponds to running an auxiliary instance of Follow-the-Regularized-Leader (Shalev-Shwartz,
2011) to minimize the regret bound (18), an idea first introduced by Rakhlin & Sridharan (2012). The details are
given in the supplementary material.
Path-length bound. For mt = zt−1 we obtain the algorithm AEG-Path given in Table 1 and achieve the bound

i=1
n
X
≤ log(
exp(βt,i − ηmt,i ))

−

i=1
Pn
exp(βt,i − ηmt,i )(zt,i −
η i=1 Pn
i=1 exp(βt,i − ηmt,i )

Regret ≤
mt,i )

= ψ ∗ (βt − ηmt ) − η∇ψ ∗ (βt − ηmt )> (zt − mt ).
The two inequalities we made use of were exp(−x−x2 ) ≤
1 − x for |x|≤ 21 and log(x − y) ≤ log(x) − y/x. Having

log(n)
+ ηDi∗ ,
η

def

Di =

T
X
(zt,i − zt−1,i )2 .
t=1

(23)
This is called a path-length bound because Di can be
thought of as the path length (squared) of the losses for expert i. This improves upon the algorithm and bound given
in Chiang et al. (2012), where Di is replaced with the quandef PT
2
tity D∞ =
t=1 kzt − zt−1 k∞ , which is always larger
4
Actually, their bound is slightly better than that, but the exact
bound is difficult to state concisely.

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

than Di∗ . We note that Di∗ ≤ 4Vi∗ + 2, so path-length
bounds subsume variance bounds.

Z can have negative eigenvalues). See Warmuth & Kuzmin
(2006) for more on this interpretation.

The path-length bound obtained above resolves a problem
posed by Kale (2012), who asked whether it is possible to
obtain bounds in terms of Di∗ .

We start by extending the adaptive EG algorithm (Proposition 3.3) to the matrix setting:

Comparison of bounds. Recall the definitions of Di ,
D∞ , and Vi , and further define V∞ , Si , and S∞ :
def PT
def PT
Di = t=1 (zt,i − zt−1,i )2 D∞ = t=1 kzt − zt−1 k2∞
def PT
def PT
Vi = t=1 (zt − z̄i )2
V∞ = t=1 kzt − z̄k2∞
P
def
def PT
T
2
Si = t=1 zt,i
S∞ = t=1 kzt k2∞
(24)
Figure 1 shows the 3 × 3 grid of potential regret bounds,
summarizing the relevant results. The original exponentiated gradient algorithm has regret in terms of S∞ , while the
adaptive algorithm proposed by Cesa-Bianchi et al. (2007)
obtains regret in terms of the smaller quantity Si∗ . Hazan &
Kale (2008) obtain a bound based on maxni=1 Vi , and Chiang et al. (2012) obtain a bound based on D∞ . All three
of these latter bounds are incomparable, but our AEG-Path
algorithm obtains a bound in terms of Di∗ , which is strictly
better than all of the above. We note that in some cases,
slightly better bounds can be obtained in terms of the behavior of the learner (see e.g. Section 1.2 of Hazan & Kale
(2008)), but we omit these results for brevity and because
the behavior of the learner is not known ahead of time.

4. Extension to Matrices
We now extend our results to the matrix setting, where
the learner chooses a positive semidefinite matrix W with
tr(W ) = 1. The flexibility of Corollary 3.2 makes the
extension to this case straightforward;
Pnessentially the only
change is replacingPthe regularizer i=1 wi log(wi ) with
n
tr(W log(W )) = i=1 λi log(λi ), where (λi )ni=1 are the
eigenvalues of W .
Setup. On each round the learner chooses a matrix Wt
with Wt  0 and tr(Wt ) = 1, and a matrix of losses Zt
is revealed; Zt is assumed to be symmetric and to satisfy
kZt kop ≤ 1, where k·kop is the operator norm (maximum
singular value). The loss in round t is tr(Wt Zt ). Note
that we can embed the vector setting in the matrix setting
via wt 7→ diag(wt ), zt 7→ diag(zt ), where diag(v) is the
diagonal matrix V with Vii = vi .
To give some intuition, the constraint that tr(W ) = 1
means
Pn that >W can be written as a convex combination
The inner product tr(W Z)
i=1 pi vi vi of unit vectors.
Pn
can then be written as i=1 pi · (vi> Zvi ). Thus an equivalent game would be for the learner to (stochastically) pick
a vector v and receive payoff v > Zv. Here the stochasticity
of the choices is crucial because v > Zv is not convex (since

Proposition 4.1 (Adaptive matrix exponentiated gradient).
For any sequence of matrices Mt , consider the updates
given by B1 = 0 and Bt+1 = Bt − ηZt − η 2 (Zt − Mt )2 ,
exp(Bt −ηMt )
. For 0 < η ≤ 14 ,
with prediction Wt = tr(exp(B
t −ηMt ))
kZt kop ≤ 1, and kMt kop ≤ 1, we have
Regret(U ) ≤

n
X
log(n)
+η
tr(U (Zt − Mt )2 )
η
i=1

(25)

for all U  0 with tr(U ) = 1.
The main additional tool we need is the Golden-Thompson
inequality tr(exp(A+B)) ≤ tr(exp(A) exp(B)) (Golden,
1965; Thompson, 1965). Otherwise, the proof proceeds as
in Proposition 3.3, so we leave the details for the supplementary material.
Path-length and variance bounds. By setting Mt to
Zt−1 as before, we obtain the algorithm AMEG-Path in
Table 1 and achieve the following path-length bound:
Regret(U ) ≤

T
X
log(n)
tr(U (Zt − Zt−1 )2 ). (26)
+η
η
t=1

We now turn our attention to the variance bound. The
path length bound already implies a variance bound, but
deriving a variance bound directly provides additional insight as well as better constants. Mimicking Rakhlin
Pt−1 &
Sridharan (2012), we would like to set Mt to 1t s=1 Zs
and then interpret this choice of Mt as playing Followthe-Regularized-Leader (FTRL) to minimize the sum in
(25). In previous applications this has been straightforward, but here, due to the adaptivity of the regularizer, the
sum (25) is a function of U , which is not known in advance. We address this issue with Lemmas 4.2 and 4.3 below. Lemma 4.3 establishes that there is an optimal value
M ∗ for Mt that is independent of U . Lemma 4.3 provides
a way of attaining the optimum; the lemma is fairly general
and may be useful in obtaining variance bounds for other
adaptive regularizers.
PT
def
1
Lemma 4.2. For any δ ≥ 0, define M ∗ = T +δ
t=1 Zt .
Then, for any symmetric matrix M 0 , we have
δ(M ∗ )2 +

T
T
X
X
(Zt − M ∗ )2  δ(M 0 )2 +
(Zt − M 0 )2 .
t=1

t=1

The proof is in the supplementary material. We remark that
the proof is almost purely algebraic, and only relies on the
property that D2  0 for any symmetric matrix D.

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

PT
Setting δ to 0, we see that Z̄ = T1 t=1 Zt is the optimal (fixed) value of Mt for any U  0. We now have a
target value Z̄ for the Mt , but we cannot simply apply the
standard FTRL Lemma, since we need a result of the form
T
X

(Zt − Mt )2 

t=1

T
X

(Zt − Z̄)2 + αI,

(27)

t=1

which cannot be straightforwardly expressed as a regret
bound (the αI term is meant to be the matrix equivalent
of a small constant α). We deal with this by deriving a generalization of the FTRL algorithm, which we call FTRL-K.
This algorithm has vector-valued losses and obtains regret
relative to a partial ordering defined by a cone K.5
An important notion is that of a global minimizer. For a
function f : X → V where V is a vector space and a cone
K ⊂ V , we say that x is a global minimizer of f relative to
K if f (x) ≤K f (y) for all y ∈ X ; that is, x+K contains the
image of f . Intuitively, K must contain all the directions in
which f can vary relative to f (x).
Lemma 4.3 (FTRL-K). Suppose that for all 1 ≤ t ≤
T + 1, there exists a global minimizer Mt of ψ(M ) +
Pt−1
s=1 fs (M ). Then for all M ,
T
X

ft (Mt ) − ft (M ) ≤K ψ(M ) − ψ(M1 )

t =1

+

T
X

(28)

ft (Mt ) − ft (Mt+1 ).

t=1

Taking ψ(M ) = M 2 , ft (M ) = (Zt − M )2 , and K the
cone of PSD matrices, we obtain the following corollary:
Pt−1
Corollary 4.4. Suppose that we choose Mt = 1t s=1 Zs .
Then, assuming kZt kop ≤ 1 for all t, we have
T
T
X
X
(Zt − Mt )2  2
(Zt − Z̄)2 + 6I,
t=1
def 1
T

for Z̄ =

(29)

t=1

PT

t=1

Zt .

Both proofs can be found in the supplementary material.
Combining Proposition 4.1 with Corollary 4.4 gives the desired variance bound:
Corollary 4.5. For 0 < η ≤ 41 and kZt kop ≤ 1, setting
Pt−1
Mt = 1t s=1 Zs achieves a bound of
" T
#
X
log(n)
2
Regret(U ) ≤
+η 2
tr(U (Zt − Z̄) ) + 6 .
η
t=1
We remark that by optimizing the proof of Corollary 4.4,
we can replace the constants 2 with 1 +  for any  > 0.
5

Recall that for a cone K satisfying K ∩ (−K) = {0}, we
define the partial order x ≤K y iff y − x ∈ K. Common choices
of K are the positive orthant and the positive semidefinite cone.

5. Discussion
We have presented an adaptive exponentiated gradient algorithm, which attains regret bounded by the variance and
path length of the best expert in hindsight. To achieve these
bounds, we relied on the synergy of adaptivity and optimism, allowing us to use “hints” for immediate prediction,
and adaptively performing a second-order correction to the
gradient updates based on the accuracy of the hints. A remaining open problem is to adaptively tune the step size to
achieve asymptotically optimal regret.
Recently, Duchi et al. (2011) proposed AdaGrad, an adaptive subgradient algorithm. A major difference is that they
update their regularizer by a large multiplicative amount
in each round, whereas our regularizer changes by a small
additive second-order term η 2 ut . We also obtain different
regret bounds; at a high level, AdaGrad can be expected to
perform well when the optimal predictor is dense but the
gradient updates are sparse. In contrast, our algorithm will
perform well when the optimal predictor is sparse but the
gradient updates are dense.
Our FTRL-K lemma (Lemma 4.3) is closely related to
Blackwell approachability (Blackwell, 1956); see Perchet
(2013) for a recent survey. As far as we can tell, the conditions in Lemma 4.3 are not equivalent to Blackwell approachability; they are (intuitively) stronger but have the
advantage of offering a potentially tighter analysis, as in
Corollary 4.4. Abernethy et al. (2011) recently provided
a very elegant connection between Blackwell approachability and regret minimization; our algorithm is, however,
different from theirs. We note that the global minimizer criterion is essentially a lower bound on the curvature of the
cumulative regularized loss near its optimum. We could
thus imagine adding to the regularizer term until the criterion held, if necessary.
Finally, we think the general idea of “pushing the regret
into the regularizer”, as in Theorem 3.1 and in earlier work
(Orabona, 2013; Orabona et al., 2013), is quite interesting, as it allows us to obtain regret bounds in terms of the
best expert rather than the learner. It P
should be the case
T
that any time our regret involves a sum t=1 kzt − mt k2wt ,
where k·kwt is a local norm, we can instead obtain a bound
PT
on Regret(u) involving t=1 kzt − mt k2u , as long as ψ ∗
is well-behaved (perhaps having a bounded third derivative). Precisely characterizing these conditions, and obtaining such local norm results for cases beyond the entropy
and von-Neumann (matrix) entropy, is an interesting direction of future work.
Acknowledgments. We thank Paul Christiano and
Jonathan Huggins for helpful discussions, as well as the
anonymous reviewers for providing a thorough review of
the paper and several helpful comments. The first author
was supported by the Hertz Foundation.

Adaptivity and Optimism: An Improved Exponentiated Gradient Algorithm

References
Abernethy, Jacob, Bartlett, Peter L, and Hazan, Elad. Blackwell
approachability and no-regret learning are equivalent. JMLR:
Workshop and Conference Proceedings (COLT), 19:27–46,
2011.
Arora, Sanjeev and Kale, Satyen. A combinatorial, primal-dual
approach to semidefinite programs. In Proceedings of the
thirty-ninth annual ACM symposium on Theory of computing,
pp. 227–236. ACM, 2007.

Kivinen, Jyrki and Warmuth, Manfred K. Exponentiated gradient
versus gradient descent for linear predictors. Information and
Computation, 132(1):1–63, 1997.
Littlestone, Nick. Learning quickly when irrelevant attributes
abound: A new linear-threshold algorithm. Machine learning,
2(4):285–318, 1988.
Littlestone, Nick and Warmuth, Manfred K. The weighted majority algorithm. In Foundations of Computer Science, 30th
Annual Symposium on, pp. 256–261. IEEE, 1989.

Arora, Sanjeev, Hazan, Elad, and Kale, Satyen. The multiplicative
weights update method: a meta-algorithm and applications.
Theory of Computing, 8(1):121–164, 2012.

McMahan, H Brendan and Streeter, Matthew. Adaptive bound
optimization for online convex optimization. arXiv preprint
arXiv:1002.4908, 2010.

Auer, Peter and Warmuth, Manfred K. Tracking the best disjunction. Machine Learning, 32(2):127–150, 1998.

Orabona, Francesco. Dimension-free exponentiated gradient.
In Advances in Neural Information Processing Systems, pp.
1806–1814, 2013.

Blackwell, David. An analog of the minimax theorem for vector
payoffs. Pacific Journal of Mathematics, 6(1):1–8, 1956.
Cesa-Bianchi, Nicolò, Conconi, Alex, and Gentile, Claudio. A
second-order perceptron algorithm. SIAM Journal on Computing, 34(3):640–668, 2005.
Cesa-Bianchi, Nicolo, Mansour, Yishay, and Stoltz, Gilles. Improved second-order bounds for prediction with expert advice.
Machine Learning, 66(2-3):321–352, 2007.
Chiang, Chao-Kai, Yang, Tianbao, Lee, Chia-Jung, Mahdavi,
Mehrdad, Lu, Chi-Jen, Jin, Rong, and Zhu, Shenghuo. Online optimization with gradual variations. Journal of Machine
Learning Research, 2012.

Orabona, Francesco, Crammer, Koby, and Cesa-Bianchi, Nicolo.
A generalized online mirror descent with applications to classification and regression. arXiv preprint arXiv:1304.2994, 2013.
Perchet, Vianney. Approachability, regret and calibration; implications and equivalences. arXiv preprint arXiv:1301.2663,
2013.
Rakhlin, Alexander and Sridharan, Karthik. Online learning with
predictable sequences. arXiv preprint arXiv:1208.3728, 2012.
Sabato, Sivan, Shalev-Shwartz, Shai, Srebro, Nathan, Hsu,
Daniel, and Zhang, Tong. Learning sparse low-threshold linear
classifiers. arXiv preprint arXiv:1212.3276, 2012.

Crammer, Koby, Kulesza, Alex, and Dredze, Mark. Adaptive regularization of weight vectors. Machine Learning, pp. 1–33,
2009.

Shalev-Shwartz, Shai. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):
107–194, 2011.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic optimization.
The Journal of Machine Learning Research, pp. 2121–2159,
2011.

Srebro, Nati, Sridharan, Karthik, and Tewari, Ambuj. On the universality of online mirror descent. In Advances in Neural Information Processing Systems, pp. 2645–2653, 2011.

Golden, Sidney. Lower bounds for the helmholtz function. Physical Review, 137(4B):B1127, 1965.
Hazan, Elad. The convex optimization approach to regret minimization. Optimization for machine learning, pp. 287, 2011.
Hazan, Elad and Kale, Satyen. Extracting certainty from uncertainty: Regret bounded by variation in costs. In Proceedings of
the Twenty First Annual Conference on Computational Learning Theory, 2008.
Hazan, Elad, Rakhlin, Alexander, and Bartlett, Peter L. Adaptive
online gradient descent. In Advances in Neural Information
Processing Systems, pp. 65–72, 2007.
Hazan, Elad, Kale, Satyen, and Shalev-Shwartz, Shai. Nearoptimal algorithms for online matrix prediction. arXiv preprint
arXiv:1204.0136, 2012.
Herbster, Mark and Warmuth, Manfred K. Tracking the best expert. Machine Learning, 32(2):151–178, 1998.
Kale, Satyen. Commentary on “online optimization with gradual
variations”. Journal of Machine Learning Research, pp. 6–24,
2012.

Streeter, Matthew and McMahan, H Brendan. Less regret via
online conditioning. arXiv preprint arXiv:1002.4862, 2010.
Thompson, Colin J. Inequality with applications in statistical mechanics. Journal of Mathematical Physics, 6:1812, 1965.
Tsuda, Koji, Rätsch, Gunnar, and Warmuth, Manfred K. Matrix
exponentiated gradient updates for on-line learning and bregman projection. In Journal of Machine Learning Research, pp.
995–1018, 2005.
Vaits, Nina, Moroshko, Edward, and Crammer, Koby. Secondorder non-stationary online learning for regression. arXiv
preprint arXiv:1303.0140, 2013.
Warmuth, Manfred K and Kuzmin, Dima. Online variance minimization. In Learning Theory, pp. 514–528. Springer, 2006.
Yang, Tianbao, Mahdavi, Mehrdad, Jin, Rong, and Zhu,
Shenghuo. Regret bounded by gradual variation for online convex optimization. Machine Learning, pp. 1–41, 2013.

