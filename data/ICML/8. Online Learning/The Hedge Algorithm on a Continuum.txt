The Hedge Algorithm on a Continuum

Walid Krichene
University of California, 652 Sutardja Dai Hall, Berkeley, CA 94720 USA

WALID @ EECS . BERKELEY. EDU

Maximilian Balandat
University of California, 736 Sutardja Dai Hall, Berkeley, CA 94720 USA

BALANDAT @ EECS . BERKELEY. EDU

Claire Tomlin
University of California, 721 Sutardja Dai Hall, Berkeley, CA 94720 USA

TOMLIN @ EECS . BERKELEY. EDU

Alexandre Bayen
University of California, 642 Sutardja Dai Hall, Berkeley, CA 94720 USA

BAYEN @ BERKELEY. EDU

Abstract
We consider an online optimization problem on
a compact subset S âŠ‚ Rn (not necessarily convex), in which a decision maker chooses, at each
iteration t, a probability distribution x(t) over
S, and
to minimize a cumulative expected
Pseeks
T
loss, t=1 Esâˆ¼x(t) [`(t) (s)], where `(t) is a Lipschitz loss function revealed at the end of iteration t. Building on previous work, we propose
âˆš a generalized Hedge algorithm and show a
O( t log t) bound on the regret when the losses
are uniformly Lipschitz and S is uniformly fat
(a weaker condition than convexity). Finally, we
propose a generalization to the dual averaging
method on the set of Lebesgue-continuous distributions over S.

1. Introduction
We consider the online optimization setting used by
Zinkevich (2003) and Hazan et al. (2007), where a decision maker chooses, at each iteration t, a probability
distribution x(t) over some compact feasible set S âŠ‚ Rn ,
and incurs a loss Esâˆ¼x(t) [`(t) (s)]. When choosing x(t) , the
decision maker only has access to (`(Ï„ ) (Â·))1â‰¤Ï„ â‰¤tâˆ’1 , i.e.
the loss functions up to iteration t âˆ’ 1. The cumulative
regret is a natural measure of performance in sequential
decision problems; it was introduced by Hannan (1957)
in the context of repeated games, then later used in the
analysis of general sequential decision problems, see
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

for example Cesa-Bianchi & Lugosi (2006) and Bubeck
& Cesa-Bianchi (2012). The cumulative regret R(t) at
iteration t is defined as the difference between the loss
incurred by the decision maker and the loss of the best
fixed decision in hindsight, that is,
Pt
Pt
R(t) = Ï„ =1 Esâˆ¼x(t) [`(Ï„ ) (s)] âˆ’ inf sâˆˆS Ï„ =1 `(Ï„ ) (s).
Zinkevich (2003) shows that if the feasible set S is convex,
and the loss functions `(t) are convex with a bounded gradient,âˆša simple online gradient descent algorithm achieves
a O( t) cumulative regret. In 2007, Hazan et al. show
that if the loss functions `(t) are exp-concave, uniformly
in t, a generalized Hedge algorithm achieves a O(log t) regret. The Hedge algorithm, also known as the multiplicative weights update (Arora et al., 2012), has been extensively studied in the discrete case, i.e. when S is a discrete set. The Hedge algorithm was introduced as the exponentially weighted average forecaster by Littlestone &
Warmuth (1989). It has also been analyzed in the context
of convex optimization, and is known as the exponentiated gradient method (Kivinen & Warmuth, 1997), or the
entropic descent method (Beck & Teboulle, 2003). The
Hedge algorithm is a simple method to implement and to
analyze, and achieves sublinear regret in the discrete case
whenever the loss functions `(t) are uniformly bounded.
More precisely, if the action
âˆš set has size N and the the
learning rates have
a
1/
t decay rate, then the regret is
âˆš
bounded by O( t log N ), see for example the analysis
in (Bubeck & Cesa-Bianchi, 2012).
We seek to generalize this regret bound to a setting in which
the action set is a continuum, while making only mild assumptions on the geometry of the set and the class of loss
functions. The logarithmic regret bound achieved by Hazan
et al. (2007) requires the feasible set to be convex, and the
loss functions to be exp-concave. We extend their analy-

The Hedge Algorithm on a Continuum

Assumptions on `(t)

convex

Î±-exp-concave

uniformly L-Lipschitz

Assumptions on S

convex

convex

v-uniformly fat

Gradient descent (Zinkevich)
âˆš
1/ t
âˆš
O 1/ t

Hedge (Hazan et al.)

Hedge (this paper)
âˆš
1/ t
p

O tâˆ’1 log t

Method
Learning rates
R(t)/t

Î±
O tâˆ’1 log t



Table 1. Some regret upper bounds for different classes of losses.

sis to a less restrictive class of problems, which only requires uniform fatness of the action set (a weaker condition than convexity) and uniform Lipschitz continuity of
the loss functions. We show that under
such assumptions
âˆš
the Hedge algorithm achieves a O( t log t) regret. Table 1
summarizes the regret bounds for these problem classes.
The online optimization model on a continuum has various applications, including machine learning (Hazan et al.,
2007), portfolio optimization (Cover, 1991; Blum & Kalai,
1999), pricing with uncertain demand and transmission
power control over noisy channels (Cope, 2009). By relaxing the assumptions of convexity of the feasible set
and exp-concavity of the loss functions, we extend the
class of problems for which the Hedge algorithm provides
bounds on the worst-case regret. For example, in the context of portfolio optimization, this would allow for nonconvex diversification constraints and non-convex transaction costs (Xidonas & Mavrotas, 2014).
In Section 2, we derive a general regret bound for Lipschitz losses. In Section 3, we specialize this bound to
convex feasible sets, then relax the convexity assumption
and show that the Hedge algorithm guarantees sublinear
regret on uniformly fat sets. In Section 4, we study the dual
averaging method and prove a regret bound on the set of
Lebesgue-continuous distributions on S, then discuss how
one can recover the Hedge regret bound as a special case.
In Section 5, we compare the Hedge algorithm to learning
on a finite cover of S. Finally, we illustrate these theoretical
results with a numerical example in Section 6.

2. A General Regret Bound on Metric Spaces
Consider a compact metric space (S, d) where d is a distance function. Let Î½ be a reference probability measure
on S, and denote by âˆ†Î½ (S) the set of probability measures
that are absolutely continuous with respect to Î½.
Let `(t) âˆˆ C 0 (S, R+ ) denote the loss function at iteration t. We assume that the losses are bounded uniformly
in t, i.e. âˆƒ M > 0 such that `(t) (s) âˆˆ [0, M ] for all t âˆˆ N
and all s âˆˆ S. The decision maker chooses, at iteration t, a
distribution over S, i.e. an element of âˆ†Î½ (S). Its density
w.r.t. Î½ will be denoted by x(t) . The Hedge algorithm with

initial density x(0) and learning rates (Î·t ), is defined by
the sequence (x(t) ) of densities as follows:


Pt
1
x(t+1) (s) = (t) x(0) (s) exp âˆ’Î·t+1 Ï„ =1 `(Ï„ ) (s) (1)
ZÌ„
where ZÌ„ (t) is the appropriateP
normalization constant, i.e.,
t
ZÌ„ (t) = Esâˆ¼x(0) exp(âˆ’Î·t+1 Ï„ =1 `(Ï„ ) (s)) . The Hedge
algorithm is summarized in Algorithm 1.
Algorithm 1 Hedge algorithm with initial density x(0) and
learning rates (Î·t ).
for t âˆˆ N do
Choose action s âˆ¼ x(t)
Observe loss function `(t)

P
Update x(t+1) (s) âˆ x(0) (s) exp âˆ’Î·t+1 tÏ„ =1 `(Ï„ ) (s)

Define r(t) (s) = Euâˆ¼x(t) [`(t) (u)] âˆ’ `(t) (s) as the instantaneous regret function at iteration t, and
Pt
R(t) = supsâˆˆS Ï„ =1 r(Ï„ ) (s)
Pt
Pt
= Ï„ =1 Esâˆ¼x(Ï„ ) [`(Ï„ ) (s)] âˆ’ inf sâˆˆS Ï„ =1 `(Ï„ ) (s) (2)
as the cumulative regret. The first term in the above expression is the expected cumulative loss of the decision maker.
The second term is the infimum of the cumulative loss function, which will be denoted by
Pt
L(t) (s) := Ï„ =1 `(Ï„ ) (s)
Since S is compact and L(t) is continuous, the infimum of
L(t) is attained on S. We write s?t âˆˆ arg minsâˆˆS L(t) (s)
for the minimizer.
We start by giving a first regret bound for Lipschitzcontinuous losses. This bound can be obtained as a consequence of Theorem 4.6 of (Audibert, 2009), and a similar
result is proved by (Dalalyan & Salmon, 2012).
Lemma 1. The Hedge algorithm with non-increasing
learning rates (Î·t ) guarantees
R(t) â‰¤

t


M2 X
Î·Ï„ + Î¾(Î·t , L(t) ) âˆ’ L(t) (s?t )
8 Ï„ =1

where Î¾ : R+ Ã— L2 (S) â†’ R is given by
Z
1
Î¾(Î·, L) = âˆ’ log x(0) (s) exp(âˆ’Î·L(s))Î½(ds)
Î·

(3)

The Hedge Algorithm on a Continuum

t
X
Ï„ =1

Ex(Ï„ ) [`(Ï„ ) ] â‰¤

tâˆ’1 
t

X
M2 X
Î·Ï„ +
Î¾(Î·Ï„ , L(Ï„ ) ) âˆ’ Î¾(Î·Ï„ +1 , L(Ï„ ) ) + Î¾(Î·t , L(t) )
8 Ï„ =1
Ï„ =1

R
Z
âˆ‚Î¾(Î·, f )
1
1 âˆ’f (s)x(0) (s) exp(âˆ’Î·f (s)) Î½(ds)
R
= 2 log x(0) (s) exp(âˆ’Î·f (s)) Î½(ds) âˆ’
âˆ‚Î·
Î·
Î·
x(0) (s) exp(âˆ’Î·f (s)) Î½(ds)
Z
Z
Z
1
1
1
1
1
1
= âˆ’ 2 log
âˆ’
xf (s)Î½(ds) âˆ’ 2
âˆ’f (s)xf (s)Î½(ds) = âˆ’ 2
log
log exp(âˆ’Î·f (s))xf (s)Î½(ds)
Î·
Zf
Î·
Î·
Zf
Î·
Z
Z
exp(âˆ’Î·f (s))
xf (s)
1
1
log
xf (s)Î½(ds) = âˆ’ 2
log (0) xf (s)Î½(ds) = âˆ’DKL (xf , x(0) )
=âˆ’ 2
Î·
Zf
Î·
x (s)

Proof. We have
Î¾(Î·t+1 , L(t+1) ) âˆ’ Î¾(Î·t+1 , L(t) )
R (0)
Pt+1
x (s) exp(âˆ’Î·t+1 Ï„ =1 `(Ï„ ) (s)) Î½(ds)
1
log R
=âˆ’
Pt
Î·t+1
x(0) (u) exp(âˆ’Î·t+1 Ï„ =1 `(Ï„ ) (u)) Î½(du)
Z
1
=âˆ’
log x(t+1) (s) exp(âˆ’Î·t+1 `(t+1) (s)) Î½(ds)
Î·t+1


1
log Ex(t+1) exp(âˆ’Î·t+1 `(t+1) )
=âˆ’
Î·t+1

 Î·t+1 M 2
â‰¥ Ex(t+1) `(t+1) âˆ’
8
where the last inequality follows from Hoeffdingâ€™s lemma.
Summing the inequalities, we find that

Pt
(Ï„ )
) âˆ’ Î¾(Î·Ï„ , L(Ï„ âˆ’1) )
Ï„ =1 Î¾(Î·Ï„ , L
Pt
2 Pt
â‰¥ Ï„ =1 Ex(Ï„ ) [`(Ï„ ) ] âˆ’ M8
Ï„ =1 Î·Ï„
(0)

Rearranging,
and observing that Î¾(Î·, L ) = Î¾(Î·, 0) =
R
âˆ’ Î·1 log x(0) (s) Î½(ds) = 0, we obtain equation (4) at the
top of the page.
Next, we show that each term of the second sum in (4) is
non-positive. Since Î·t+1 â‰¤ Î·t by assumption, it suffices
to show that, for any bounded Lipschitz function f , Î· 7â†’
Î¾(Î·, f ) is decreasing. Calculating the partial derivative
w.r.t. Î· (using Dominated Convergence to differentiate under the integral) we obtain (5), where we use xf to denote
the density function xf (s) = Zfâˆ’1 x(0) (s) exp(âˆ’Î·f (s)),
with Zf the corresponding normalization constant. Thus
âˆ‚Î· Î¾(Î·, f ) is proportional to the negative Kullback-Leibler
divergence of xf with respect to x(0) , therefore Î· 7â†’
Î¾(Î·, f ) is non-increasing. The bound (4) then reduces to
Pt
2 Pt
(Ï„ )
(t)
] â‰¤ M8
Ï„ =1 Ex(Ï„ ) [`
Ï„ =1 Î·Ï„ + Î¾(Î·t , L )
and we conclude by subtracting
minsâˆˆS L(t) (s) from both sides.

L(t) (s?t )

=

Next, we refine this regret bound by bounding the difference Î¾(Î·t , L(t) ) âˆ’ L(t) (s?t ).

(4)

(5)

For any measurable subset A âŠ† S, we define the diameter D(A) := sups,s0 âˆˆA d(s, s0 ) and the generalized volume
R
Vx(0) (A) := A x(0) (s) Î½(ds).
Lemma 2. Suppose that the loss functions `(t) are
L-Lipschitz uniformly in t. Consider a sequence (St )
of measurable subsets of S, such that s?t âˆˆ St for all
t. Then the Hedge algorithm with non-increasing learning rates (Î·t ) guarantees the following bound on the regret:
R(t) â‰¤

t
M2 X
log Vx(0) (St )
Î·Ï„ + t LD(St ) âˆ’
8 Ï„ =1
Î·t

(6)

Proof. Since the loss functions `(t) are uniformly LLipschitz, we have for all s âˆˆ St , |`(Ï„ ) (s) âˆ’ `(Ï„ ) (s?t )| â‰¤
Ld(s, s?t ) â‰¤ LD(St ). Hence, `(Ï„ ) (s) â‰¤ `(Ï„ ) (s?t ) +
LD(St ), and L(t) (s) â‰¤ L(t) (s?t ) + tLD(St ). Therefore

R
Î¾(Î·t , L(t) ) â‰¤ âˆ’ Î·1t log Stx(0) (s) exp âˆ’Î·t L(t) (s) Î½(ds)

R
â‰¤ âˆ’ Î·1t log Stx(0) (s) exp âˆ’Î·t (L(t) (s?t ) + tLD(St )) Î½(ds)
R
= L(t) (s?t ) + tLD(St )LD(St ) âˆ’ Î·1t log St x(0) (s) Î½(ds)
= L(t) (s?t ) + tLD(St ) âˆ’

1
Î·t

log Vx(0) (St )

Combining this with Lemma 1 concludes the proof.
Lemma 2 provides a regret bound in terms of any sequence
(St ) of subsets of S, with each St containing s?t , an optimal
decision in hindsight. However, this bound is only useful if
one can construct such a sequence with appropriate relative
decay rates of the diameters and generalized volumes.
More precisely, we have the following corollary.
Corollary 1. Consider
Ptthe Hedge algorithm with learning
rates (Î·t ) such that Ï„ =1 Î·Ï„ = o(t). Suppose that there
exists a sequence (St ) of subsets with s?t âˆˆ St , âˆ€ t, and
such that D(St ) = o(1) and log Vx(0) (St ) = o(tÎ·t ). Then
the regret grows sublinearly, i.e. lim suptâ†’âˆ R(t)/t â‰¤ 0.
In the next section, we give sufficient conditions on the action set S that guarantee the existence of such a sequence.

The Hedge Algorithm on a Continuum

3. Sublinear regret in Rn
We now restrict our attention to finite dimensional Euclidean spaces. Let S be a compact subset of Rn . We
start with the simple case of convex S, and construct a sequence (St ) using a homothetic transformation centered at
s?t , similarly to (Blum & Kalai, 1999). Unless stated otherwise, we make the following assumption for the remainder
of this paper:
Assumption 1. The reference measure is the Lebesgue
measure Î», and the initial distribution x(0) is the Lebesgue1
.
uniform distribution over S, i.e. x(0) (s) = Î»(S)
3.1. Sublinear Regret on Convex Sets
Lemma 3. If S is a convex compact subset of Rn and x(0)
is the Lebesgue-uniform probability density over S, then
the set St defined by the homothetic transformation

	
St = s?t + dt (s âˆ’ s?t ), s âˆˆ S
(7)
has diameter D(St ) = dt D(S) and generalized volume
t)
n
Vx(0) (St ) = Î»(S
Î»(S) = dt .
Proof. We have
D(St ) = sups,s0 âˆˆSt ks âˆ’ s0 k
= sups,s0 âˆˆS ks?t + dt (s âˆ’ s?t ) âˆ’ s?t âˆ’ dt (s0 âˆ’ s? )k
= dt sups,s0 âˆˆS ks âˆ’ s0 k
Furthermore, using a change of variable s = s?t + dt (s0 âˆ’
s?t ), s0 âˆˆ S, we can write
R
R
1
Vx(0) (St ) = St x(0) (s)Î»(ds) = Î»(S)
Î»(ds)
St
R
1
0
= Î»(S) S | det(dt In )|Î»(ds ) = dnt

Corollary 2. Under the assumptions of Theorem 1, with
2
(t)
Î·t = Î· constant, we have Rt â‰¤ M8 Î· + LD(S)
+
t
n log t
. For a given horizon T , we can choose Î· =
Î·t p
M âˆ’1 8n log T /T to minimize this bound, for which
LD(S)
R(T )
â‰¤
+M
T
T

r

n log T
2T

(9)

3.2. Sublinear Regret on Uniformly Fat Sets
The convexity assumption can, in fact, be relaxed, while
keeping the same asymptotic rate of the regret. Intuitively,
to be able to use the sequence (St ) of sets as constructed
in Lemma 3, it suffices to find, for each s?t , a convex set
t)
Kt containing s?t such that its volume Vx(0) (Kt ) = Î»(K
Î»(S)
is uniformly bounded below. This motivates the following
relaxation of convexity.
Definition 3.1 (Uniform fatness). A set S âŠ‚ Rn is vuniformly fat w.r.t. the density x if, for all s âˆˆ S, there
exists a convex
set Ks âŠ† S such that s âˆˆ Ks and
R
Vx (Ks ) = Ks x(s)Î»(ds) â‰¥ v.
Intuitively, the uniform fatness property ensures that there
is sufficient volume around any point of the set, so that it
is possible to assign sufficient probability mass around the
optimal point s?t in particular. Note that uniform fatness
excludes isolated points, but does not require the set to be
connected.
s?t

s

St
Ks

S

S
Ks?t

n

Theorem 1 (Hedge on convex compact subsets of R ). Let
S âŠ‚ Rn be convex and compact, and suppose that the `(t)
are L-Lipschitz uniformly in t. Then under
the Hedge algoâˆš
rithm with learning rates Î·t = Î¸tâˆ’Î± log t, Î± âˆˆ [0, 1), we
have
âˆš
âˆš
R(t)
M 2Î¸
log t LD(S) n log t
â‰¤
+
+
(8)
t
8(1 âˆ’ Î±) tÎ±
t
Î¸ t1âˆ’Î±

âˆš
In particular, the per-round regret is O tâˆ’Î±Ì„ log t , where
Î±Ì„ = min(Î±, 1 âˆ’ Î±).
Proof. Constructing the sequence St as in Lemma 3, it
follows from the regret bound (6) that
R(t)
t

2

Pt

Ï„ âˆ’Î±

dt
+ LD(S)dt âˆ’ nÎ¸ log
t1âˆ’Î±
âˆš
Pt
Î·Ï„
â‰¤
Î¸ log t Ï„ =1 Ï„ âˆ’Î±
Bounding
R t âˆ’Î±Ï„ =1 Î¸t1âˆ’Î± âˆšlog t
âˆš
Î¸ log t 0 Ï„ dÏ„ =
, we have
1âˆ’Î±
R(t)
t

â‰¤ M8 Î¸
Pt

â‰¤

M 2Î¸
8(1âˆ’Î±)

Ï„ =1

t

âˆš

log t
tÎ±

+ LD(S)dt +

Now (8) follows by taking dt = 1/t.

1/dt
n log âˆš
Î¸ t1âˆ’Î± log t

â‰¤

Figure 1. Illustration of the uniform fatness condition (left) and
the construction of the set St = s?t + dt (Ks?t âˆ’ s?t ) in the proof
of Theorem 2 (right).

We are now ready to give a regret bound for the Hedge
algorithm on uniformly fat sets.
Theorem 2. Let x(0) be Lebesgue uniform, and suppose
that S is v-uniformly fat w.r.t. x(0) and that the loss functions are L-Lipschitz uniformly in time. Then the regret
âˆš of
the Hedge algorithm with learning rates Î·t = Î¸ tâˆ’Î± log t,
Î± âˆˆ [0, 1), satisfies
âˆš
R(t)
M 2Î¸
log t LD(S) n log t + log v1
âˆš
â‰¤
+
+
t
8(1 âˆ’ Î±) tÎ±
t
Î¸ t1âˆ’Î± log t
(10)
In particular, if S is convex, then it is 1-uniformly fat, and
Theorem 1 becomes a special case of Theorem 2.

The Hedge Algorithm on a Continuum

Proof. Since S is v-uniformly fat, for all t, there exists a
convex measurable subset Ks?t âŠ‚ S with s?t âˆˆ Ks?t and
Vx(0) (Ks?t ) â‰¥ v. Similarly to (7), define St as the homothetic transformation of Ks?t with center s?t and ratio
dt . By Lemma 3, we have D(St ) = dt D(Kt? ) â‰¤ dt D(S)
n
and Vx(0) (St ) = dnt Vx(0) (K
âˆšt ) â‰¥ dt v. Applying the regret
âˆ’Î±
log t, we have
bound (6) with Î·t = Î¸t
âˆš
log V (0) (St )
log t
+ LD(St ) âˆ’ 1âˆ’Î±x âˆš
tÎ±
Î¸t
log t
âˆš
log(dn
v)
M 2Î¸
log t
â‰¤
+ dt LD(S) âˆ’ 1âˆ’Î± âˆšt
8(1 âˆ’ Î±) tÎ±
Î¸t
log t

M 2Î¸
R(t)
â‰¤
t
8(1 âˆ’ Î±)

`

is, Ïˆ(x) â‰¥ Ïˆ(y) + hâˆ‡Ïˆ(y), x âˆ’ yi + 2Ïˆ kx âˆ’ yk2 for all
x, y âˆˆ X . To simplify the discussion, we also assume,
without loss of generality, that inf xâˆˆX Ïˆ(x) = 0.

and we conclude by taking dt = 1/t.
Corollary 3. Under the assumptions of Theorem 2, with
2
(t)
constant learning rate Î·t = Î·, we have Rt â‰¤ M8 Î· +
L D(S)
v
+ n log Î·tâˆ’log
. For a given horizon T , we can
t
t
p
âˆ’1
choose Î·T = M
8(n log T âˆ’ log v)/T to minimizes
this bound, for which
L D(S)
R(T )
â‰¤
+M
T
T

r

n log T âˆ’ log v
2T

Algorithm 2 Dual averaging method with input sequence
(`(t) ) and learning rates (Î·t )
1: for t âˆˆ N P
do
t
2:
L(t) = Ï„ =1 `(Ï„ )
3:
Update
D
E
1
x(t+1) = arg min L(t) , x +
Ïˆ(x) (12)
xâˆˆX
Î·t+1

(11)

The dual averaging update (12) can be written in terms
of the Legendre-Fenchel transform of Ïˆ: Let Ïˆ âˆ— (L) =
âˆ’ inf xâˆˆX Ïˆ(x)âˆ’hL, xi. Note that the minimum is attained
and the minimizer is unique since Ïˆ is strongly convex and
X is closed and convex (Theorem 11.9 in (Bauschke &
Combettes, 2011)). Then âˆ‡Ïˆ âˆ— (L) = arg minxâˆˆX Ïˆ(x) âˆ’
hL, xi, and the update (12) can be written
x(t+1) = âˆ‡Ïˆ âˆ— (âˆ’Î·t+1 L(t) ).

Remark 1. If S âŠ‚ Rn is a lower-dimensional manifold, it
is not uniformly fat with respect to the Lebesgue measure
on Rn . However, if it is homeomorphic to a uniformly fat
S 0 âŠ‚ Rm , m < n, then one can run the Hedge algorithm
on S 0 instead.

To apply the dual averaging method to our problem of
learning on S, let E = L2 (S), the Lebesgue space of
square integrable Rfunctions2 on S, endowed with the inner
product hf, gi = S f (s)g(s)Î»(ds), and the feasible set

	
R
X := f âˆˆ L2 (S) : f â‰¥ 0 a.e. and S f (s)Î»(ds) = 1 .

4. Dual Averaging on L2 (S)

Note that while X is closed and convex, it may be unbounded3 . An element f âˆˆ X will be identified with the
probability distribution with density f . The dual space is
E âˆ— = L2 (S), and since S is compact, E âˆ— contains, in particular, the set C 0 (S) of continuous functions on S.

In this Section, we study a more general family of algorithms based on the dual averaging method.
Dual averaging (Nesterov, 2009) is a general method for
solving constrained optimization problems. It was applied
to online learning on a convex subset of Rn , for example
in (Xiao, 2010) and (Bubeck, 2014). Building on these
ideas, we propose to apply it to our problem of learning
on uniformly fat sets.
Consider a Hilbert space E, and a feasible set X âŠ‚ E,
assumed closed and convex. Given a sequence (`(t) ) of linear functionals in the dual space E âˆ— , the method
Pprojects,
t
at each step, the cumulative dual vector L(t) = Ï„ =1 `(Ï„ )
onto the feasible set, using a Bregman projection. This is
summarized in Algorithm 2. In constrained convex optimization, one seeks to minimize a convex function f over
X , and the dual vectors `(t) are taken to be subgradients of
f at the current iterate, but dual averaging provides regret
guarantees without requiring `(t) to be subgradient vectors. The function Ïˆ in Algorithm 2 is assumed to be `Ïˆ strongly convex with respect to a norm1 k Â· k on E, that
1
The reference norm k Â· k need not necessarily be the norm
induced by the inner product on E.

We next show that the regret of the dual averaging method
grows sublinearly under appropriate assumptions on the
feasible set S and the regularizer Ïˆ. The result extends
the regret bound of (Nesterov, 2009) to L2 (S). We will
use the following Lemma, which can be proved following
Lemma 1 in (Nesterov, 2009), mutatis mutandis.
Lemma 4. If Ïˆ is `Ïˆ -strongly convex w.r.t. k Â· k. Then Ïˆ âˆ—
is `1Ïˆ -smooth w.r.t. k Â· kâˆ— , that is, for all x, y
Ïˆ âˆ— (x) âˆ’ Ïˆ âˆ— (y) âˆ’ hâˆ‡Ïˆ âˆ— (y), x âˆ’ yi â‰¤

1
2`Ïˆ kx

âˆ’ yk2âˆ— .

Lemma 5 (Dual averaging regret bound). Suppose that
there exists M > 0 such that for all t, k`(t) kâˆ— â‰¤ M . Then
under the dual averaging method with non-increasing
learning rates (Î·t ), for all x âˆˆ X ,
2
An element of E is, in fact, an equivalence class of functions
equal almost everywhere.
3
Consider for example the simple case S = [0, 1], andâˆšthe
sequence fn = n1[0, 1 ] , for which kfn k1 = 1 but kfn k2 = n.
n

The Hedge Algorithm on a Continuum

t D
X
Ï„ =1

t
2 X

E
1
M
`(Ï„ ) , x(Ï„ ) âˆ’ x â‰¤ Ïˆ(x) +
Î·t
2`Ïˆ

Î·Ï„

(13)

Ï„ =1

Proof. We use a similar argument to the proof of Lemma 1.
Define the potential function Î¾ : R+ Ã— L2 (S) â†’ R
Î¾(Î·, L) = âˆ’ Î·1 Ïˆ âˆ— (âˆ’Î·L) =

1
Î·

inf xâˆˆX hÎ·L, xi + Ïˆ(x)

We first show the following inequality:
E
D
Î·t (t) 2
x(t) , `(t) â‰¤ Î¾(Î·t , L(t) ) âˆ’ Î¾(Î·tâˆ’1 , L(tâˆ’1) ) +
k` kâˆ—
2`Ïˆ
(14)
Since Ïˆ is `Ïˆ -strongly convex, by Lemma 4, Ïˆ âˆ— is `1Ïˆ smooth, therefore
âˆ—

(t)

âˆ—

(tâˆ’1)

Ïˆ ( âˆ’ Î·t L ) âˆ’ Ïˆ (âˆ’Î·t L
)
D
E
1
kÎ·t `(t) k2âˆ—
â‰¤ âˆ‡Ïˆ âˆ— (âˆ’Î·t L(tâˆ’1) ), âˆ’Î·t `(t) +
2`Ïˆ
D
E
Î·2
= âˆ’Î·t x(t) , `(t) + t k`(t) k2âˆ—
2`Ïˆ

Dividing by Î·t and rearranging, we have
D
E
Î·t (t) 2
x(t) , `(t) â‰¤ Î¾(Î·t , L(t) ) âˆ’ Î¾(Î·t , L(tâˆ’1) ) +
k` kâˆ— .
2`Ïˆ
To prove (14), it suffices to show that Î· 7â†’ Î¾(Î·, L) is decreasing. Taking the derivative with respect to Î·,
1 âˆ—
1
Ïˆ (âˆ’Î·L) âˆ’ hâˆ’L, âˆ‡Ïˆ âˆ— (âˆ’Î·L)i
2
Î·
Î·
1
âˆ—
= 2 (Ïˆ (âˆ’Î·L) + hÎ·L, âˆ‡Ïˆ âˆ— (âˆ’Î·L)i)
Î·
1
by convexity of Ïˆ âˆ—
â‰¤ 2 Ïˆ âˆ— (0)
Î·
1
= âˆ’ 2 inf Ïˆ(x) = 0
Î· xâˆˆX

âˆ‚Î· Î¾(Î·, L) =

which proves inequality (14). Summing over Ï„
{1, . . . , t}, and using the bound on k`(t) kâˆ— , we have

âˆˆ

Finally, by definition of Î¾,
1
inf Ïˆ(x) = 0
Î·0 xâˆˆX
D
E
D
E
1
1
Î¾(Î·t , L(t) ) =
inf Î·t L(t) , x + Ïˆ(x) â‰¤ L(t) , x + Ïˆ(x)
Î·t xâˆˆX
Î·t

Therefore
t D
t
E D
E
X
1
M2 X
`(Ï„ ) , x(Ï„ ) â‰¤ L(t) , x + Ïˆ(x) +
Î·Ï„
Î·t
2`Ïˆ Ï„ =1
Ï„ =1

which proves the claim.

In particular, if Ïˆ is bounded on X , then it follows from
Lemma 5 that
R(t) â‰¤

t
M2 X
1
sup Ïˆ(x) +
Î·Ï„
Î·t xâˆˆX
2`Ïˆ Ï„ =1

which implies that the regret is sublinear for Î·t = Î˜(tâˆ’Î± ),
Î± âˆˆ (0, 1), for any sequence of continuous losses. However, when X is unbounded, so is Ïˆ by strong convexity.
But one can still obtain a sublinear bound on the regret for
Lipschitz losses, as stated in the following Theorem.
Theorem 3 (Dual averaging regret for Lipschitz losses).
Suppose that `(t) is L-Lipschitz, and k`(t) kâˆ— â‰¤ M , uniformly in t. Then the dual averaging method with learning
rates (Î·t ) guarantees the following bound on the regret:
For any positive sequence (dt ),
Pt
M 2 Ï„ =1 Î·Ï„
1
R(t)
â‰¤
+ LD(S)dt +
inf xâˆˆBt Ïˆ(x)
t
2`Ïˆ
t
tÎ·t
(15)
where Bt âŠ‚ X denotes the set of Lebesgue-continuous densities supported on B(s?t , D(S)dt ).

t D
t
E
X
M2 X
Î·Ï„
`(Ï„ ) , x(Ï„ ) â‰¤ Î¾(Î·t , L(t) ) âˆ’ Î¾(Î·0 , L(0) ) +
2`Ïˆ Ï„ =1
Ï„ =1

Î¾(Î·0 , L(0) ) =

Lemma 5 gives an upper bound on the regret with respect
to elements of X , the set of Lebesgue-continuous densities.
This can also provide a bound on the regret (2), with respect
to elements of S, as defined in Section 2, by observing that
when S is uniformly fat4 ,

Pt 

Pt
R(t) = Ï„ =1 `(Ï„ ) , x(Ï„ ) âˆ’ minsâˆˆS Ï„ =1 `(Ï„ ) (s)
DP
E

Pt 

t
(Ï„ )
= Ï„ =1 `(Ï„ ) , x(Ï„ ) âˆ’ inf xâˆˆX
,x .
Ï„ =1 `

Proof. Since the losses are L-Lipschitz, we have, âˆ€ x âˆˆ Bt ,
DP
E Z P
t
t
(Ï„ )
(Ï„ )
`
,
x
=
(s)x(s)Î»(ds)
Ï„ =1
Ï„ =1 `
Bt
Z
Pt
(Ï„ ) ?
â‰¤
(st ) + LD(S)dt )x(s)Î»(ds)
Ï„ =1 (`
Bt
(t)

= L (s?t ) + t LD(S)dt
Thus, for all x âˆˆ Bt ,

Pt 

R(t) = Ï„ =1 `(Ï„ ) , x(Ï„ ) âˆ’ L(t) (s?t )

Pt 

â‰¤ Ï„ =1 `(Ï„ ) , x(Ï„ ) âˆ’ x + t LD(S)dt
Pt
M2
â‰¤ Î·1t Ïˆ(x) + 2`
Ï„ =1 Î·Ï„ + t LD(S)dt
Ïˆ
where the last inequality uses Lemma 5. We conclude by
dividing by t and taking the infimum over x âˆˆ Bt .
In the finite dimensional case, the Hedge algorithm is
known to be an instance of the dual averaging method,
4

Proof in the supplementary material.

The Hedge Algorithm on a Continuum

when the feasible set X is the simplex, and the distance
generating function Ïˆ is the negative entropy function, as
observed by Nesterov (2009), Beck & Teboulle (2003), and
many others. This is also true in the infinite dimensional
case, as discussed for example in (Audibert, 2009) and in
Chapter 5 in (Catoni, 2004). Next, we apply the regret
bound of Theorem 3 to the Hedge algorithm.
Example 1 (Hedge algorithm or the entropic dual averaging). Let the reference measure be the Lebesgue measure
on S, denoted by Î», and let Ïˆ be the generalized negative
entropy, defined
Z by
Ïˆ(f ) =

f (s) log f (s)Î»(ds) + log Î»(S)

(16)

S

Note that Ïˆ(f ) is the Kullback-Leibler divergence of f with
respect to the Lebesgue-uniform distribution. By Pinskerâ€™s
inequality, Ïˆ is 1-strongly
R convex with respect to the total
variation norm kf k = |f (s)|Î»(ds). Furthermore, Ïˆ is
nonnegative since it is minimal on the Lebesgue uniform
distribution, for which
R it takes value 0. One can show that
1
exp(f (s))Î»(ds), thus the definitions
Ïˆ âˆ— (f ) = log Î»(S)
S
of Î¾(Î·, L) in Lemmas 1 and 5 coincide.
With this choice of X and Ïˆ, the dual averaging update can
be shown to be equal to the Hedge density (1):
Proposition 1. Let L(t) âˆˆ E âˆ— , and consider the dual
averaging iteration (12) with Ïˆ the negative entropy (16).
Then the solution x(t+1) is given by the Hedge update rule
(t)
1
x(t+1) (s) = (t) eâˆ’Î·t+1 L (s)
ZÌ„
R
(t)
with normalization constant ZÌ„ (t) = S eâˆ’Î·t+1 L (s) Î»(ds).
A proof is provided in the supplementary material for completeness.
By Theorem 3, the regret of the Hedge algorithm with Lipschitz losses is then bounded as follows
Pt
R(t) M 2 Ï„ =1 Î·Ï„
1
â‰¤
+ LD(S)dt +
inf xâˆˆBt Ïˆ(x)
t
2
t
tÎ·t
In order to bound the last term, consider a subset St âŠ‚
B(s?t , D(S)dt ), and take x to be the uniform distribution
over St (thus x âˆˆ Bt ). Then
Z
1
Î»(S)
1
log
Î»(ds) = log
Ïˆ(x) = log Î»(S) +
Î»(S
)
Î»(S
)
Î»(S
t
t
t)
St
Now, if S is v-uniformly fat, then there exists a convex
set Kt containing s?t , such that Î»(Kt )/Î»(S) â‰¥ v, and
constructing St as in the proof of Theorem 1, as the homothetic transform of Kt with center s?t and ratio dt we
have D(St ) â‰¤ D(S)dt and Î»(St )/Î»(S) â‰¥ vdnt , therefore
inf xâˆˆBt Ïˆ(x) â‰¤ âˆ’ log(vdnt ), and taking dt = 1/t,
Pt
M 2 Ï„ =1 Î·Ï„
LD(S) n log t âˆ’ log v
R(t)
â‰¤
+
+
t
2
t
t
t Î·t
which results in a regret bound similar to Theorem 2.

5. Learning on a finite cover
In this section, we briefly compare our method to a related
idea: for a given horizon, compute a finite cover of the set,
such that the maximum difference of losses on each element of the cover is small enough, and then perform a discrete learning algorithm on the finite cover.
More precisely, fixing a horizon T and a constant T > 0,
suppose that we can compute a finite cover AT of S such
that, for all ST âˆˆ AT ,
sup |`(t) (s) âˆ’ `(t) (s0 )| â‰¤ T .

(17)

s,s0 âˆˆST

If we call RÌƒ(t) the regret with respect to the discrete set,
then running the discrete Hedge algorithm on this finite
cover with learning rate Î· guarantees (Bubeck & CesaBianchi, 2012) that
M2
log |A|
RÌƒ(T )
â‰¤
Î·+
T
8
TÎ·
and the optimal Î· given
p the horizon T yields the regret
bound RÌƒ(T )/T = O log |A | /T . Since we incur at
most T additional per-round regret due to the variation
of losses within
p each element ofthe cover, we have that
R(T )/T = O log |A| / T + T .
Since the loss functions are L-Lipschitz, a sufficient condition for (17) to hold is to have each element ST of the cover
have diameter D(ST ) â‰¤ T /L. Thus, the size of the cover
is typically |AT | = O(1/nT ). Under
p this estimate of the

size of AT , we have R(T ) /T = O âˆ’n log T / T + T ,
âˆš
and choosing T = 1/ T , we obtain the bound
p

R(T )/T = O n log T /T
which matches the regret rate of Corollary 3.
The Hedge algorithm on uniformly fat sets is conceptually
similar to the idea of working with a finite cover. This is
most visible in the proof of Theorem 2, where we rely on
the existence of a set around s?t with an appropriate relationship between diameter and volume. To apply the Hedge
algorithm, one needs to sample from the distributions x(t) ,
without having to explicitly construct a cover. Sampling
from the distribution can be more tractable, as is the case
in the example of Section 6.

6. Numerical results
We test our algorithm on a numerical example in R2 with
convex quadratic loss functions of the form
`(t) (s) =

1
(s âˆ’ Âµt )> Qt (s âˆ’ Âµt ) + ct
2

restricted to the domain S âŠ‚ R2 shown in Figure 2.

The Hedge Algorithm on a Continuum

If x(0) is the uniform distribution over S one can show that
 1

x(t+1) (s) âˆ |QÌƒt |âˆ’1/2 exp âˆ’ (s âˆ’ ÂµÌƒt )> QÌƒt (s âˆ’ ÂµÌƒt )
2

We first verify from Figure 3 that the regret bound (10)
is satisfied. Since the loss functions are generated randomly (and not adversarially), the regrets observed in simulation are much smaller than the theoretical regret bounds.
We further note that the optimal constant learning rates
Î·opt from Corollary 3 are outperformed by higher constant
learning rates5 , an observation familiar from the discrete
case (Even-Dar et al., 2008; Koolen et al., 2014).

on S, so x(t+1) is a multivariate Gaussian distribution
reP
stricted to S with covariance matrix QÌƒt = Î·t Ï„ â‰¤t QÏ„ and
P
mean ÂµÌƒt = QÌƒâˆ’1
t Î·t
Ï„ â‰¤t QÏ„ ÂµÏ„ . Hence the size of the
parameter space required to represent the cumulative loss
(and thus the Hedge densities) is independent of the horizon.

Finally, Table 2 compares the decay rates of the per-round
cumulative regret (solid lines in Figure 3), estimated using
(t)
a linear regression on log Rt as a function of log t, with
those of the corresponding theoretical bound (10) (dashed
lines). The observed rates of decay are higher than those of
the theoretical bounds.

Figure 2. The set S for the numerical example.

Remark 2. Since the Hedge distributions are, in this case,
multivariate Gaussian, sampling from these distributions
can be done efficiently. This example is one instance of a
problem in which one can directly sample from the Hedge
distributions without having to maintain a discrete cover.
More generally, the complexity of the Hedge algorithm depends on the complexity of sampling from the Hedge distributions.
For a simulation horizon of T = 104 , we randomly generated the parameters Âµt , Qt and ct of the loss functions
subject to the uniform bounds M = 10 and L = 5. The
set S has diameter D = 5.83 and is v-uniformly fat with
v = 0.273. We simulated the algorithm 2500 times for
each of the different choices of the learning rates. Figure 3
shows means (solid lines), regret bounds (dashed lines) and
regions between the 10% and 90% quantiles (shaded) of the
per-round cumulative regret over these simulations. To determine the regret, the computation of the best choice in
hindsight for each period t was performed by solving multiple quadratic programs on a convex decomposition of S.

Î·t
âˆ’0.15

t
tâˆ’0.3

simulation

bound

bound (t â†’ âˆ)

âˆ’0.778
âˆ’0.644

âˆ’0.173
âˆ’0.397

âˆ’0.150
âˆ’0.300

Table 2. Decay rates of the per-round regret.

7. Concluding remarks
We studied an online optimization problem over a compact subset S of Rn . Previous
âˆš work shows that when S
is convex, one can achieve O( t) cumulative regret using
a gradient descent method when the losses are convex, and
O(log t) cumulative regret using a generalized Hedge algorithm when the losses are uniformly exp-concave. We
consider Lipschitz losses, and relax the convexity assumption of S. In particular, we show that as long as the set
is uniformly fat, i.e. there exists a convex set of minimal
volume v around all points of âˆš
the set, then the generalized
Hedge algorithm achieves O( t log t) regret. We further
proved a regret bound for dual averaging method, a generalization of the Hedge algorithm. A question which remains open is whether the uniform fatness condition is necessary for a given class of algorithms to achieve sublinear
regret.
A related problem, which is not studied here, is bandit
learning on a continuum, in which only the current loss
value `(t) (s(t) ) is revealed. This problem is studied for
example by Bubeck et al. (2011) for Lipschitz losses and
general topological spaces. A hierarchical algorithm is proposed, which achieves a O(td1 (log t)d2 ) where d1 , d2 â‰¤ 1
depend on the geometry of the problem. The algorithm requires explicitly computing a hierarchical cover of the set.
One question is whether one can generalize the Hedge algorithm to such a bandit setting, so that sublinear regret can
be achieved without the need to explicitly maintain a cover.
5

Figure 3. Mean time-average cumulative regret (solid), 10% and
90% quantiles (shaded regions) and worst-case bounds (dashed).

Due to the rather weak assumptions on the action set and the
loss functions, the optimal learning rate Î·opt (T ) for the known
horizon T derived from Corollary (3) is very small.

The Hedge Algorithm on a Continuum

References
Arora, Sanjeev, Hazan, Elad, and Kale, Satyen. The multiplicative weights update method: a meta-algorithm and
applications. Theory of Computing, 8(1):121â€“164, 2012.
Audibert, Jean-Yves. Fast learning rates in statistical inference through aggregation. The Annals of Statistics, 37
(4):pp. 1591â€“1646, 2009. ISSN 00905364.
Bauschke, Heinz H. and Combettes, Patrick L. Convex Analysis and Monotone Operator Theory in Hilbert
Spaces. CMS Books in Mathematics. Springer, 2011.
Beck, Amir and Teboulle, Marc. Mirror descent and nonlinear projected subgradient methods for convex optimization. Oper. Res. Lett., 31(3):167â€“175, May 2003.
Blum, Avrim and Kalai, Adam. Universal portfolios with
and without transaction costs. Machine Learning, 35(3):
193â€“205, 1999.
Bubeck, SeÌbastien. Theory of convex optimization for machine learning. ArXiv, 2014.
Bubeck, SeÌbastien and Cesa-Bianchi, NicoloÌ€. Regret analysis of stochastic and nonstochastic multi-armed bandit
problems. Foundations and Trends in Machine Learning, 5(1):1â€“122, 2012.

Dalalyan, Arnak S. and Salmon, Joseph. Sharp oracle
inequalities for aggregation of affine estimators. Ann.
Statist., 40(4):2327â€“2355, 08 2012.
Even-Dar, Eyal, Kearns, Michael, Mansour, Yishay, and
Wortman, Jennifer. Regret to the best vs. regret to the
average. Machine Learning, 72(1-2):21â€“37, 2008.
Hannan, James. Approximation to Bayes risk in repeated
plays. Contributions to the Theory of Games, 3:97â€“139,
1957.
Hazan, Elad, Agarwal, Amit, and Kale, Satyen. Logarithmic regret algorithms for online convex optimization.
Machine Learning, 69(2-3):169â€“192, 2007.
Kivinen, Jyrki and Warmuth, Manfred K. Exponentiated
gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1 â€“ 63, 1997.
Koolen, Wouter M., Erven, Tim Van, and GruÌˆnwald, Peter D. Learning the learning rate for prediction with expert advice. In Advances in Neural Information Processing Systems (NIPS) 27, pp. 2294â€“2302, Dec 2014.
Littlestone, Nick and Warmuth, Manfred K. The weighted
majority algorithm. In Foundations of Computer Science, 1989., 30th Annual Symposium on, pp. 256â€“261.
IEEE, 1989.

Bubeck, SeÌbastien, Munos, ReÌmi, Stoltz, Gilles, and
Szepesvari, Csaba. X-armed bandits. Journal of Machine Learning Research (JMLR), 12(12):1587â€“1627,
2011.

Nesterov, Yurii. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120(1):
221â€“259, 2009.

Catoni, Olivier. Statistical learning theory and stochastic
optimization, Ecole dâ€™EteÌ de ProbabiliteÌs de Saint-Flour
XXXI- 2001, volume 1851. Springer, 2004.

Xiao, Lin. Dual averaging methods for regularized stochastic learning and online optimization. J. Mach. Learn.
Res., 11:2543â€“2596, December 2010. ISSN 1532-4435.

Cesa-Bianchi, NicoloÌ€ and Lugosi, GaÌbor. Prediction,
learning, and games. Cambridge University Press, 2006.

Xidonas, Panos and Mavrotas, George.
Multiobjective portfolio optimization with non-convex policy constraints: Evidence from the eurostoxx 50. The European
Journal of Finance, 20(11):957â€“977, 2014.

Cope, Eric W. Regret and convergence bounds for a class of
continuum-armed bandit problems. Automatic Control,
IEEE Transactions on, 54(6):1243â€“1253, June 2009.
Cover, Thomas M. Universal portfolios. Mathematical Finance, 1(1):1â€“29, 1991.

Zinkevich, Martin. Online convex programming and generalized infinitesimal gradient ascent. In ICML, pp. 928â€“
936, 2003.

