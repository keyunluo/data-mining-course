Entropy evaluation based on confidence intervals of frequency estimates :
Application to the learning of decision trees

Mathieu Serrurier
IRIT - Université Paul Sabatier 118 route de Narbonne 31062, Toulouse Cedex 9, France

SERRURIER @ IRIT. FR

Henri Prade
PRADE @ IRIT. FR
IRIT - Université Paul Sabatier, Toulouse, France & QCIS, University of Technology, Sydney, Australia

Abstract
Entropy gain is widely used for learning decision
trees. However, as we go deeper downward the
tree, the examples become rarer and the faithfulness of entropy decreases. Thus, misleading
choices and over-fitting may occur and the tree
has to be adjusted by using an early-stop criterion or post pruning algorithms. However, these
methods still depends on the choices previously
made, which may be unsatisfactory. We propose
a new cumulative entropy function based on confidence intervals on frequency estimates that together considers the entropy of the probability
distribution and the uncertainty around the estimation of its parameters. This function takes
advantage of the ability of a possibility distribution to upper bound a family of probabilities
previously estimated from a limited set of examples and of the link between possibilistic specificity order and entropy. The proposed measure
has several advantages over the classical one. It
performs significant choices of split and provides
a statistically relevant stopping criterion that allows the learning of trees whose size is wellsuited w.r.t. the available data. On the top of that,
it also provides a reasonable estimator of the performances of a decision tree. Finally, we show
that it can be used for designing a simple and efficient online learning algorithm.

1. Introduction
Although decision tree methods have been one of the first
machine learning approaches, they remain popular because
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

of their simplicity and flexibility. Most algorithms for
building decision trees are based on the use of information
gain function for choosing the best attribute for splitting
the data at each step of the learning process. Thus, the
ID3 algorithm is based on logarithmic entropy (Quinlan,
1986), while CART (Breiman et al., 1984) is based on the
Gini impurity measure. Numerous alternatives have been
proposed for the gain function (Buntine & Niblett, 1992;
Lee, 2001; Nowozin, 2012). However, one drawback in
this kind of approach is that the gain function becomes less
and less significant when the number of examples in the
considered node decreases. In the particular case of log
entropy-based gain, which is still one of the most largely
used, splitting a node always decreases the weighted entropy of the leaves obtained. It then leads to learn trees that
may overfit the data and then decreases the performance
of the algorithm. This can be avoided by using early-stop
criterion or post-pruning methods. However, these methods still depend on the initial choices based on the entropy
calculus, even if this evaluation may be not significant.
The main limitation of the log-based entropy (but this also
applies to some extent to its multiple refinements) is that
it does not take into account the amount of data used for
estimating the frequencies on the different classes.
The goal of this paper is to show how to extend the
classical entropy calculus in order to take into account
the amount of information available and then having a
single entropy measure that addresses the different issues
of the decision tree learning process in an elegant way.
We propose to use the upper bound of the frequency
estimates for defining a so-called possibilistic cumulative entropy. The approach relies on the building of a
possibility distribution. Quantitative possibility measures
can be viewed as upper bounds of probabilities. Then, a
possibility distribution represents a family of probability
distributions (Dubois & Prade, 1992). In agreement with
this view, a probability-possibility transformation has

Entropy evaluation based on confidence intervals of frequency estimates

been proposed (Dubois et al., 1993). This transformation
associates a probability distribution with the maximally
specific (restrictive) possibility distribution which is such
that the possibility of any event is indeed an upper bound
of the corresponding probability. Possibility distributions
are then able to describe epistemic uncertainty and to
represent knowledge states such as total ignorance, partial
ignorance or complete knowledge. Starting from the link
between the specificity order over possibility distribution
and the entropy of a probability distribution, we propose a
log-based loss function for possibility distributions based
on (Serrurier & Prade, 2013). We derive the possibilistic
cumulative entropy function for a possibility distribution
associated to a frequency distribution. Then, we build a
possibility distribution that upper bounds the confidence
intervals of the frequency values (according to the number
of data available and a confidence degree) and we compute
its relative possibilistic entropy. This cumulative entropy
has nice properties. For instance, it respects the entropy
order for a fixed level of information and this entropy
increases for a fixed frequency distribution when the
amount of data decreases. It also provides a stopping
criterion when splitting nodes is no longer significant.
Thus, it allows to choose the most relevant nodes instead
of reasoning a posteriori about the significance of the
choices made on the basis of classic entropy, as done with
early stop criteria or post-prunning methods (see (Esposito
et al., 1997) for a review of pruning methods). Thanks to
this ability, we propose a direct extension of the classical
algorithm with possibilistic entropy and we show how to
easily extend it to obtain an incremental online algorithm.
Last, possibilistic cumulative entropy also provides a
global evaluation measure of a decision tree that is a
relevant estimation of its performances outside the training
set.
The paper is organized as follows. First we provide a
short background on possibility distributions and possibility measures and their use as upper bounds of families of
probability distributions. Second, we describe possibilistic cumulative entropy with its properties. Section 4 is devoted to the presentation of the two algorithms and their
comparisons with state of the art approaches. As our goal
is to demonstrate the benefits of our measure with respect
to classical log entropy, we compare the performances of
these approaches on 16 benchmark databases in the last
section.

2. Possibility theory
Possibility theory, introduced in (Zadeh, 1978), was initially proposed in order to deal with imprecision and uncertainty due to incomplete information, as the one provided

by linguistic statements. This kind of epistemic uncertainty
cannot be handled by a single probability distribution, especially when a priori knowledge about the nature of the
probability distribution is lacking. A possibility distribution π on a discrete universe Ω = {c1 , . . . , cq } is a mapping from Ω to [0, 1]. We note Π the set of all possibility
distributions over Ω. The value π(c) is called the possibility degree of the value c in Ω. For any subset of Ω, the
possibility measure is defined as follows:
∀A ⊆ Ω, Π(A) = sup π(c).
c∈A

If it exists at least a value c ∈ Ω for which we have
π(c) = 1, the distribution is normalized. One view of possibility theory is to consider a possibility distribution as a
family of probability distributions (see (Dubois, 2006) for
an overview). Thus, a possibility distribution π will represent the family of the probability distributions for which the
measure of each subset of Ω will be respectively lower and
upper bounded by its necessity and its possibility measures.
More formally, if P is the set of all probability distributions
defined on Ω, the family of probability distributions P(π)
associated with π is defined as follows:
P(π) = {p ∈ P, ∀A ∈ Ω, P (A) ≤ Π(A)}.

(1)

where P is the probability measure associated with p. We
can distinguish two extreme cases of information situations: i) complete knowledge ∃c ∈ Ω such as π(c) = 1
and ∀c0 ∈ Ω, c0 6= c, π(c) = 0 and ii) total ignorance
(i.e. ∀c ∈ Ω, π(c) = 1) that corresponds to the case where
all probability distributions are possible. This type of ignorance cannot be described by a single probability distribution. According to this probabilistic interpretation, a
method for transforming probability distributions into possibility distributions has been proposed in (Dubois et al.,
1993). The idea behind this is to choose the most informative possibility measure that upper bounds the considered
probability measure. We note Sq the set of permutations of
the set {1, . . . , q}. We introduce the notion of σ-specificity
which is a partial pre-order:
Definition 1 (σ-specificity) The distribution π is more σspecific than π 0 , denoted π σ π 0 , if and only if :
π σ π 0 ⇔ ∃σ ∈ Sq , ∀i ∈ {1, . . . , q}, π(ci ) ≤ π 0 (cσ(i) )
(2)
Then, the possibility measure obtained by probabilitypossibility transformation corresponds to the most σ specific possibility distribution which bounds the distribution.
We denote Tp∗ the possibility distribution obtained from p
by the probability-possibility transformation. This distribution has the following property:
∀π, p ∈ P(π) ⇒ Tp∗ σ π.

(3)

Entropy evaluation based on confidence intervals of frequency estimates

For each permutation σ ∈ Sq we can build a possibility
distribution Tpσ which encodes p as follows:
X

∀j ∈ {1, . . . , q}, Tpσ (cj ) =

p(ck ).

(4)

k,σ(k)≤σ(j)

Then, each Tpσ corresponds to a cumulative distribution of
p according to the order defined by σ. We have:
∀σ ∈ Sq , p ∈ P(Tpσ )

The logarithmic-based likelihood is defined as follows:

The probability-possibility transformation (Dubois et al.,
2004) (noted P -Π transformation) uses one of these particular possibility distributions.
Definition 2 (P -Π transformation (discrete case)) Given a
probability distribution p on Ω = {c1 , . . . , cq } and a permutation σ ∗ ∈ Sq such as p(cσ∗ (1) ) ≤ . . . ≤ p(cσ∗ (q) ), the
P -Π transformation of p is noted Tp∗ and is defined as:
∗

Tp∗ = Tpσ .
Tp∗ is the cumulative distribution of p built by considering
the increasing order of p. For this order, Tp∗ is the most
specific possibility distribution that encodes p. We have
then the following properties
∀σ ∈ Sq , Tp∗ σ Tpσ .

(5)

Example 1 For instance, we consider p on Ω =
{c1 , c2 , c3 } with p(c1 ) = 0.5, p(c2 ) = 0.2 and p(c3 ) =
0.3. We obtain σ ∗ (1) = 3, σ ∗ (2) = 1, σ ∗ (3) = 2 and
then Tp∗ (c1 ) = 0.5 + 0.3 + 0.2 = 1, Tp∗ (c2 ) = 0.2 and
Tp∗ (c3 ) = 0.3 + 0.2 = 0.5.
The interest of comparing the entropy of probability distribution by considering the σ-specificity order of its P Π transformation has been emphasized in (Dubois &
Hüllermeier, 2007) with the following key property :
∀p, p0 ∈ P, Tp∗ σ Tp∗0 ⇒ H(p) ≤ H(p0 )

L(f, X) which
is linear w.r.t. X = {x1 , . . . , xn }, i.e.
Pn
i=1 L(f,xi )
L(f, X) =
, and where f is a distribution
n
(probabilistic or possibilistic). Let α1 , . . . , αq be the frequency of the elements of X that belong respectively to
{c1 , . . . , cq }. We note

1 if xi = cj
1j (xi ) =
0 otherwise.

(6)

where H(p) is an entropy function.

Llog (p|xi ) = −

q
X

1j (xi )log(pj ).

(7)

j=1

When we consider the whole set of data we obtain:
Llog (p|X) = −

q
X

αj log(pj ).

(8)

j=1

When p is estimated with respect to frequencies, we obtain
the entropy of the distribution (which corresponds to the
minimum of the loss function).
H(p) = −

q
X

pj log(pj ).

(9)

j=1

The higher the entropy, the lower the amount of information (uniform distribution). The entropy is equal to 0 when
the probability is equal to 1 for one class. Entropy is the
basis of the majority of algorithms for learning decision
trees. The goal is to build a decision tree for which each
leaf describes a probability over class with the lowest possible entropy.
3.2. Possibilistic loss function and entropy
In this section we show how to use Llog in order to define
a loss function, and the related entropy, for possibility distributions that agrees with the interpretation of a possibility
distribution in terms of a family of probability distributions.
Proofs and detailed discussion about possibilistic loss function can be found in (Serrurier & Prade, 2013). We expect
four properties:

3. Possibilistic cumulative entropy
We now explain how particular possibility distributions can
be used to take into account the amount of data used for estimating the frequencies in the computation of the entropy.

3.1. Probabilistic loss function and entropy
Probabilistic loss functions are used for evaluating the differences between a probability distribution with respect
to data. In particular, we look for concave loss function

(a) The possibilistic loss function is minimal for the possibility distribution that results from the P -Π transformation of the frequencies.
(b) As for probability distribution, the possibilistic entropy
will be a linear function of possibilistic loss function
applied to a set of data Xp that supports a probability
distribution p.
(c) The possibilistic entropy applied to P -Π transformations respects the specificity order as in (6).

Entropy evaluation based on confidence intervals of frequency estimates

(d) The possibilistic entropy increases when uncertainty
around the considered probability distribution increases.
Since a possibility distribution π can be viewed as an upper bound of a cumulative function, for all j, the pair
πj = (π(cσ(j) ), 1 − π(cσ(j) )) (σ is the permutation of
Sq such that π(cσ(1) ) ≤ . . . ≤ π(cσ(q) )) can be seen as
a Bernouilli
Sj probability distribution for the sets of events
BCj = i=1 cσ(i) and BCj . Then, the logarithmic loss
of a possibility distribution for an event will be the average
of the log loss of each binomial distribution πj re-scaled
in [0, 0.5] where the entropy function −x ∗ log(x) − (1 −
x) ∗ log(1 − x) is strictly increasing. This re-scaling is
necessary for having proposition 1 and 2 below.
Lπ-l (π|X)) =
−

q
X
πj
cdfj
πj
cdfj
∗ log( ) + (1 −
) ∗ log(1 − )).
(
2
2
2
2
j=1

(10)
P

where cdfj = k,σ(k)≤σ(j) αk . If we only consider one
piece of data x such that x ∈ cj we obtain :
Lπ-l (π|x) = −

q
X

(log(1 −

j=1

1
− ∗
2

q
X

πj
))
2

πσ(i)
πσ(i)
(log(
) − log(1 −
)).
2
2

(11)

i,σ(i)≥σ(j)

It can be checked that this loss function is indeed linear
w.r.t. X. The property (a) has been proven in (Serrurier
& Prade, 2013). We remark that cdfj corresponds to the
cumulative probability distribution of the frequencies with
respect to σ (Eq. 4). Then, we can derive a definition of the
entropy of a possibility distribution relative to a probability
distribution:
Hπ-l (p, π) =
∗

Lπ-l (π|Xp )
q ∗ log(q)
∗

T (cj )
q Tp (cj )
π(c )
π(c )
X
∗ log( 2 j )+(1− p 2 )∗log(1− 2 j )
2
=−
.
q ∗ log(q)
j=1

(12)
where Xp is a set of data that supports a probability distribution p. q ∗ log(q) is a normalization factor. The expected
property (b) is obvious if we consider the probability distribution p such as p(ci ) = αi . We can now establish some
properties of the possibilistic entropy.
Proposition 1 Given two probability distributions p and p0
on Ω = {c1 , . . . , cq } we have:
Tp∗  Tp∗0 ⇒ Hπ-l (p, Tp∗ ) ≤ Hπ-l (p0 , Tp∗0 )

Figure 1. Possibilistic cumulative function of a binomial probability distribution on Ω = {c1 , c2 } with γ = 0.05 for different
values of n. The x-axis represents the value of p(c1 ) and the y∗
axis the value Hπ-l
(p, n, 0.05).

Proof (sketch) We can assume without loss of generality
that the values of distributions p and p0 are in increasing order. It can be easily shown that the re-scaled entropy of the
binomial counterpart of p restricted to the events BCj and
BCj is less than the entropy of the binomial counterpart of
p0 on the same events.
Proposition 2 Given a probability distribution p and two
possibility distributions π and π 0 on Ω = {c1 , . . . , cq } we
have:
Tp∗  π  π 0 ⇒ Hπ-l (p, Tp∗ ) ≤ Hπ-l (p, π) ≤ Hπ-l (p, π 0 )
Proof This property is naturally obtained from the definitions of Hπ-l and the previous.
These two last propositions validate the properties (c) and
(d) and show that the possibility cumulative entropy can
be used for measuring both the entropy and the epistemic
uncertainty and is fully compatible with the interpretation
of a possibility distribution as a family of probability distributions. We can also notice that possibilistic cumulative
entropy is equal to 0 for complete knowledge (as for classical entropy) and equal to 1 for total ignorance (and not for
uniform distributions, as for classical entropy).
3.3. Possibilistic cumulative entropy of a frequency
distribution
As said previously, the entropy calculus does not take into
account the amount of information used for estimating the
frequencies. The idea behind possibilistic cumulative entropy is to consider the confidence intervals around the estimation of the frequencies to have an entropy measure that
increases when the size of the confidence interval increases.

Entropy evaluation based on confidence intervals of frequency estimates

Applying directly the entropy to the upper-bounds of the
frequency is not satisfactory since entropy only applies to
genuine probability distribution. We propose to build the
most specific possibility distribution that upper bounds the
confidence interval and compute its possibilistic entropy
relative to the frequency distribution.
We use the Agresti-Coull interval (see (Agresti & Coull,
1998) for a review of confidence intervals for binomial
distributions) for computing the upper bound value of the
probability of an event. Given p(c) the probability of the
event c estimated from n pieces of data, the upper bound
p∗γ,n of the (1 − γ)% confidence interval of p is obtained as
follows:
r
1
∗
p̃(1 − p̃)
(13)
pγ,n (c) = p̃ + z
ñ
where ñ = n + z 2 , p̃ = ñ1 (p(c) ∗ n + 2̃1 z 2 ), and z is the
1 − 21 γ percentile of a standard normal distribution. The
γ
that contains upmost specific possibility distribution πp,n
per bounds of the (1 − γ)% confidence interval of p estimated from n piece of data is computed as follows:
γ
πp,n
(cj )

=

∗
Pγ,n
(

j
[

{cσ(i) })

These two last propositions show that possibilistic cumulative entropy has the expected properties and can take effectively into account the uncertainty around the estimation of
the frequency distribution.
Example 2 We consider p on Ω = {c1 , c2 , c3 } with
p(c1 ) = 0.5, p(c2 ) = 0.2 and p(c3 ) = 0.3. For n = 10
0.05
∗
and γ = 0.05. πp,10
(c1 ) = P0.05,10
({c1 , c2 , c3 }) =
0.05
∗
0.05
1, πp,10 (c2 ) = p0.05,10 (c2 ) = 0.52 , πp,10
(c3 ) =
∗
∗
P0.05,10 ({c2 , c3 }) = 0.76 and Hπ-l (p, 10, 0.05) = 0.81.

4. Learning decision trees with possibilistic
cumulative entropy
In this section, we propose two different algorithms that are
based on the possibilistic cumulative entropy. The first one
is the classical decision tree learning algorithm for which
the gain function is now based on possibilistic cumulative
entropy. In the next subsection we show that the possibilistic cumulative entropy can be used for revising a decision
tree and then we obtain an incremental decision tree algorithm.

(14)

i=1

4.1. Possibilistic cumulative entropy for decision trees

where σ ∈ Sq is the permutation such as p(cσ(1) ) ≤ . . . ≤
γ
is built in the same way as πp∗ exp(cσ(q) ). Thus, πp,n
cept that it also takes into account the uncertainty around
γ
),
the estimation of p. Obviously, we have p ∈ P(πp,n
γ
γ
γ
,
∀n > 0, πp∗  πp,n
and lim πp,n
= πp∗ . Having πp,n
n→∞
we can now define the possibilistic cumulative entropy of a
probability distribution:
∗
γ
Hπ-l
(p, n, γ) = Hπ-l (p, πp,n
)

(15)

∗
Fig. 1 illustrates the different values of Hπ-l
for a binomial
distribution with different values of n. We can observe that
∗
the value of Hπ-l
increases when n decreases for the same
distribution.

The building of a decision tree is based on the recursive
induction of the nodes. For learning a node, the best attribute according to the gain is chosen. Given a set Z of n
examples and an attribute A (real valued attributes are handled by means of binary attributes associated with thresholds) which has v1, . . . , vr possible values. We note pZ
the probability distribution of the classes for the examples
in Z, pvk the probability distribution of the classes for the
examples for which the value of A is vk and |vk| the size
of this set. The classical gain function is

G(Z, A) = H(pZ ) −

r
X
|vk|
k=1

Proposition 3 Given a probability distribution p on Ω =
{c1 , . . . , cq } and n0 ≤ n we have:
∀γ

∗
∈]0, 1[, Hπ-l
(p, n, γ)

≤

∗
Hπ-l
(p, n0 , γ)

γ
γ
Proof Use the property πp∗  πp,n
 πp,n
0 and proposition 2.

Proposition 4 Given two probability distributions p and p0
on Ω = {c1 , . . . , cq } we have:
∗
∗
∀γ ∈]0, 1[, Tp∗  Tp∗0 ⇒ Hπ-l
(p, n, γ) ≤ Hπ-l
(p0 , n, γ)

Proof (sketch) use the same as proposition 1 and use the
0
∗
property p(c) ≤ p0 (c) ⇒ p∗γ,n (c) ≤ pγ,n
(c).

n

H(pvk ).

(16)

As pointed into the introduction, this approach suffers some
major drawbacks. First, the gain is always positive and can
not be used as a stop criterion. The idea behind this is that
splitting the set of examples always decreases the entropy.
However, when going deeper and deeper in the tree, less
and less examples are used for computing the entropy and
the result may not be significant. Moreover, the gain tends
to favor the nominal attributes having a lot of possible values. In this paper we propose to use a new gain based on
possibilistic cumulative entropy. Since it takes into account
the uncertainty around the estimation of the probability distribution, the gain can be negative if the splitting has no
statistically significant advantage. The gain function we

Entropy evaluation based on confidence intervals of frequency estimates

4.3. State of the art

propose is defined as follows:
Gπγ (Z, A)
r
X
k=1

=

∗
Hπ-l
(pZ , n, γ)

−

|vk| ∗
H (pvk , |vk|, DS(γ, r)).
n π-l

(17)

where DS(γ, r) is the Dunn−Šidàk correction of γ for r
comparison. By using Gπγ , we have a faithful stop criterion
and we penalize the attributes having a lot of possible values. Gπγ also favors well-balanced trees where the number
of examples in the nodes is significant enough for entropy
computation. This approach directly produces trees that
limit overfitting. The possibilistic cumulative entropy can
also be used as a quality measure of a tree T with a set of
leaves L :
X
∗
∗
Hπ-l
(T, γ) =
Hπ-l
(pl , nl , γ)
(18)
l∈L

where pl is the frequency distribution of nl training examples that fall in leaf l. The only parameter of the algorithm
is γ. It represents the strength of the constraint for splitting
node. This parameter has been tuned by choosing the best
value of γ inside a set of 10 possible values by the mean of
a cross-validation on the training set.
4.2. Online decision trees
A remarkable property of the possibilistic cumulative entropy and the associated gain function it that they can easily be used for revising a decision tree. We assume that the
tree saves the set of the related examples for each leaf. The
revision process for a new example x is the following:
1. browse recursively the tree to the corresponding leaf
2. add x to the set of examples
3. search the attribute with the best Gπγ
4. if the gain is positive, create a new node with the corresponding attribute, else do nothing.
Since Gπγ is positive if and only if we have enough data
for performing a split of the node which can increase the
learning power of the tree, the tree will grow up slowly.
We can reasonably suppose that it exists an upper bound
of the number of example Nmax before which a node is
always split since the size of the confidence interval decreases quickly when the number of example increases. In
this case, the complexity of the revision of the tree with one
example will be O(N BA ∗ Nmax log(Nmax )) where N BA
is the number of attributes. The γ parameter is tuned as in
the previous algorithm. Although it is not completely satisfactory in a genuine online procedure, it is not costly if
it is done at the beginning of the algorithm. We can also
imagine that the online procedure takes place on a repeated
context.

Some other approaches (see e.g. (Bernard, 2005)) have
been proposed in order to consider the amount of data used
for the evaluation the parameters of a probability distribution into the entropy calculus. The first one is to consider
an apriori probability distribution (usually the uniform one)
and to revise it with the observation. However, we can observe that the approach depends on the choice of the initial
distribution and, since it is still an entropy computed on a
single probability distribution, it does not make the difference between a uniform distribution obtained with a large
number of estimations, and the initial distribution (if it is
uniform).
Possibility distributions have already been used in machine
learning for dealing with imprecise data (Jenhani et al.,
2008), or for generalizing Ockham’s razor principle when
learning lazy decision trees (Hüllermeier, 2002). Our approach shares some ideas with the upper entropy proposed
in (Abellàn & Moral, 2005). This works is based on the
building of a belief function that encodes the confidence
intervals around the estimation of a probability distribution. Then, the entropy computed is the maximal entropy of
the probability distributions that are bounded by the belief
function (with the optional addition of a term which corresponds to a non-specificity measure). However, there are
some important differences with our work based on possibilistic cumulative entropy. First, due to the use of the
maximum, the upper entropy is not a linear function of
individual loss function (and then not a genuine entropy
function). The second problem is that finding the maximum of entropy requires to use linear optimization algorithm which may be costly when the number of classes
increases. The last difference comes from the use of the
maximum. Indeed, when the number of the examples is
small, the uniform probability distribution may be often
in the confidence interval which prevents to make an informed choice since the entropy is equal to 1. In (Abellàn
& Moral, 2005), the authors are led to restrict themselves
to small confidence intervals (rather than faithful ones, as
in our case) in order to avoid the previously mentioned pitfall.
ID3 and J4.8 use pessimistic error rate (based on confidence interval) as a pre or post pruning criteria. However,
this is only a simple stopping or pruning criterion and it
cannot be used for choosing attributes when splitting nodes.
In (Nowozin, 2012), the author takes into consideration the
numbers of examples in the parent node by using a refined
version of the entropy (Grassberger, 1988). However, the
gain is still always positive and the approach is less general
than the one proposed in the current paper. Note that confidence intervals are used in (Katz et al., 2012) in the prediction of the class by taking into account the uncertainty on
the values of the attributes, or on the split thresholds. On-

Entropy evaluation based on confidence intervals of frequency estimates

Figure 2. Entropy of the tree with respect to the size of the tree on
∗
the Yeast database. Classical entropy is on the top and Hπ-l
(T, γ)
is on the bottom. Curves computed with LOESS

line algorithms have already been proposed in (Schlimmer
& Fisher, 1986; Utgoff, 1989; Domingos & Geoff, 2000;
Gama et al., 2006), but they are based on the revision of
the whole tree with the new example and all (or a subset of)
the previous examples.

5. Experiments
As pointed out in the introduction, the goal of the paper is
to illustrate the interest of handling epistemic uncertainty
in log-entropy calculus and to show the improvement w.r.t.
the classical approach. We used 16 benchmarks from UCI1 .
3 of these datasets have nominal attributes and 13 have numerical attributes only. We note ΠTree the decision tree
learning algorithm based on the possibilistic cumulative entropy, O-ΠTree is its online counterpart. We compare them
with the LogTree algorithm which is based on the same
implementation as ΠTree, but which uses the log entropy
(without post pruning). PrunTree is logTree with post pruning based on pessimistic error rate (the parameter γ for the
confidence intervals has been tuned with the same method
1

http://www.ics.uci.edu/ mlearn/MLRepository.html

Figure 3. Accuracy of the tree on test set with respect to the entropy of the tree on the training set for the Yeast database. Classi∗
cal entropy is on the top and Hπ-l
(T, γ) is on the bottom. Curves
computed with LOESS

used for ΠTree) and J4.8 is the baseline (we use the Weka
implementation with parameters tuned in a subset of data
as for our approach) which uses more advanced pruning
such as tree raising. Figures 2 and 3 illustrate the abil∗
(T, γ) to provide meaningful trees on the Yeast
ity of Hπ-l
database. The figures are obtained as follows: we split the
database in a 90% (training)/10% (test) scheme, we generate 10 random trees of random sizes (i.e. attribute for a
node is chosen randomly and the threshold is chosen alternatively with classical entropy, and with possibilistic cumulative entropy on the training set), we evaluate the entropy of the tree on the training set and its accuracy on
the test set, we repeat this process 1000 times. Fig. 2
shows that the classical entropy of the tree always decreases
∗
when its size increases. In the case of Hπ-l
(T, γ) , it shows
∗
that Hπ-l (T, γ) first decreases with size and then increases
when the tree becomes too complex w.r.t. the number of
examples. Fig. 3 illustrates that it exists a threshold below
which decreasing log entropy doesn’t increase the accuracy
∗
(over fitting). On the contrary, decreasing Hπ-l
(T, γ) on the
training set tends to increase the accuracy of the tree on the
test set.

Entropy evaluation based on confidence intervals of frequency estimates

DATA SET

L OG T REE

P RUNED

PT REE

O-PT REE

J48

SOYBEAN
LYMPH
ZOO 2
ILPD
YEAST
WAVEFORM
DIABETES
BANKNOTE
ECOLI
VEHICLE
IONOSPHERE
SEGMENT
PENDIGITS
SPAMBASE
BREAST- WV 2
WINE 2

89.4±5.0
72.9±11.8
97.0±4.8
67.9±5.5
52.0±4.1
75.2±1.5
68.7±5.7
98.3±1.1
78.9±7.7
71.6±4.7
90.3±4.7
96.8±0.6
96.5±0.5
91.8±1.2
92.9±2.4
92.5±8.7

89.4±5.0
72.9±11.8
97.0±4.8
67.4±5.6
57.0±3.3
75.3±1.5
70.4±4.7
98.3±1.1
80.4±7.4
71.6±4.0
90.3±4.7
96.7±0.7
96.4±0.5
91.7±1.3
92.9±2.4
92.5±8.7

94.0±2.8
78.3±7.9
97.0±4.8
69.9±5.3
57.1±3.4
77.4±1.5
74.3±4.4
98.3±1.0
82.4±7.9
74.1±4.1
91.1±3.6
96.9±1.2
96.4±0.2
94.0±1.3
93.9±3.1
93.7±7.3

89.0±3.8
78.3±8.2
96.0±5.1
66.8±4.7
56.7±3.6
72.6±1.8
70.4±3.4
97.4±2.1
83.6±7.2
69.1±3.1
87.7±4.0
94.7±1.4
93.2±1.0
90.5±1.2
94.7±1.6
94.3±8.3

91.7±3.1
75.8±11.0
92.6±7.3
69.3±6.3
57.8±5.5
75.9±1.4
74.2±5.1
98.5±1.0
83.3±8.5
73.3±5.0
91.4±3.7
97.1±1.1
96.5±0.5
92.9±1.0
94.1±3.5
94.1±3.5

Table 1. Classification accuracy of LogTree, PrunTree, ΠTree and O-ΠTree, J4.8 on different databases

less stable and may in three cases induce the largest threes.
O-ΠTree is up to 10 times slower than ΠTree when considering all the examples. However, the average update time
of the decision tree is negligible (in the worst case it is 100
times faster that ΠTree). It confirms the applicability of
O-ΠTree for online learning.

6. Conclusion

Figure 4. Number leaves of LogTree, PrunTree, ΠTree and OΠTree, J4.8 comparison for different databases.

Table 1 reports the accuracy results for the different
databases. Highest values are in bold, underlined results
indicate that the algorithm is statistically better (paired
T-Test) than its two opponents (logTree, PrunTree vs OΠTree, ΠTree, J4.8 is not taken into account). We use a
Wilcoxon signed ranked test (Demšar, 2006) for comparing
the algorithms. ΠTree significantly outperforms its classical competitors (there is no significant statistical difference
with J4.8). We do not observe significant difference between O-ΠTree and LogTree and PrunTree. This can be
considered as a good result for an online algorithm.
Fig. 4 compares the number of leaves for the trees induced
by the algorithms. As expected LogTree always produces
the most complex trees. ΠTree algorithm behaves similarly to PrunTree et J4.8 w.r.t. the size of the trees. However, when the size is significantly different different, it can
be seen that the accuracy of ΠTree is better. O-ΠTree is

In this paper we have proposed an extension of the logbased information gain that takes into account the confidence intervals of the estimates of the frequencies in case
of a limited amount of data, thanks to the use of possibilitybased representation of the family of probability distribution that agree with the data. This gain function leads us to
the learning of well-balanced decision trees, which size are
comparable to the ones obtained with a post pruning algorithm. Note that post-pruning algorithm could also benefit
from the possibilistic cumulative entropy. It also allows us
to propose an incremental version of the algorithm. Experiments show that possibilistic cumulative entropy is a
valuable quality measure for decision trees, and that our
main algorithm performs very well in comparison with the
classical approach. They also confirm the interest of the
online algorithm. In the future, we plan to incorporate
the treatment of uncertainty around numerical thresholds
(like (Katz et al., 2012)) into possibilistic cumulative entropy in order to have a complete handling of uncertainty
in the entropy calculus. The approach could also be easily
extended to the learning of regression trees, especially for
online computation.

Entropy evaluation based on confidence intervals of frequency estimates

References
Abellàn, J. and Moral, S. Upper entropy of credal sets. applications to credal classification. International Journal
of Approximate Reasoning, 39:235–255, 2005.
Agresti, A. and Coull, B.A. Approximate Is Better than
”Exact” for Interval Estimation of Binomial Proportions.
The American Statistician, 52(2):119–126, May 1998.
Bernard, J.M. An introduction to the imprecise dirichlet
model for multinomial data. International Journal of
Approximate Reasoning, 39(23):123 – 150, 2005. Imprecise Probabilities and Their Applications.
Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J.
Classification and Regression Trees. Chapman & Hall,
New York, NY, 1984.
Buntine, W. and Niblett, T. A further comparison of splitting rules for decision-tree induction. Machine Learning,
8(1):75–85, 1992.
Demšar, J. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res., 7:1–30, December
2006.
Domingos, P. and Geoff, H. Mining high-speed data
streams. In Proceedings of the Sixth ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, KDD ’00, pp. 71–80, New York, NY,
USA, 2000. ACM.
Dubois, D. Possibility theory and statistical reasoning.
Computational Statistics and Data Analysis, 51:47–69,
2006.
Dubois, D. and Hüllermeier, E. Comparing probability
measures using possibility theory: A notion of relative
peakedness. International Journal of Approximate Reasoning, 45(2):364–385, 2007.
Dubois, D. and Prade, H. When upper probabilities are
possibility measures. Fuzzy Sets and Systems, 49:65–74,
1992.
Dubois, D., Prade, H., and Sandri, S. On possibility /
probability transformations. In Lowen, R. and Roubens,
M. (eds.), Fuzzy Logic - State of the Art, pp. 103–112.
Kluwer Acad. Publ., 1993.
Dubois, D., Foulloy, L., Mauris, G., and Prade, H.
Probability-possibility transformations, triangular fuzzy
sets, and probabilistic inequalities. Reliable Computing,
10:273–297, 2004.
Esposito, F., Malerba, D., and Semeraro, G. A comparative
analysis of methods for pruning decision trees. IEEE
Trans. Pattern Anal. Mach. Intell., 19(5):476–491, May
1997.

Gama, J., Fernandes, R., and Rocha, R. Decision trees for
mining data streams. Intell. Data Anal., 10(1):23–45,
January 2006.
Grassberger, P. Finite sample corrections to entropy and
dimension estimates. Physics Letters A, 128(67):369 –
373, 1988.
Hüllermeier, E. Possibilistic induction in decision-tree
learning. In Proceedings of the 13th European Conference on Machine Learning, ECML ’02, pp. 173–184,
London, UK, UK, 2002. Springer-Verlag.
Jenhani, I., Ben Amor, N., and Elouedi, Z. Decision trees
as possibilistic classifiers. Inter. J. of Approximate Reasoning, 48(3):784–807, 2008.
Katz, G., Shabtai, A., Rokach, L., and Ofek, N. Confdtree:
Improving decision trees using confidence intervals. In
ICDM, pp. 339–348, 2012.
Lee, A. Bujaand Y.-S. Data mining criteria for tree-based
regression and classification. In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’01, New York,
NY, USA, 2001. ACM.
Nowozin, S. Improved information gain estimates for decision tree induction. In Langford, John and Pineau, Joelle
(eds.), Proceedings of the 29th International Conference
on Machine Learning (ICML-12), pp. 297–304. ACM,
2012.
Quinlan, J.R. Induction of decision trees. Machine Learning, 1(1):81–106, 1986.
Schlimmer, J. C. and Fisher, D.H. A case study of incremental concept induction. In AAAI, pp. 496–501, 1986.
Serrurier, M. and Prade, H. An informational distance for
estimating the faithfulness of a possibility distribution,
viewed as a family of probability distributions, with respect to data. Int. J. Approx. Reasoning, 54(7):919–933,
2013.
Utgoff, P.E. Incremental induction of decision trees. Machine Learning, 4(2):161–186, 1989.
Zadeh, L. A. Fuzzy sets as a basis for a theory of possibility.
Fuzzy sets and systems, 1:3–25, 1978.

