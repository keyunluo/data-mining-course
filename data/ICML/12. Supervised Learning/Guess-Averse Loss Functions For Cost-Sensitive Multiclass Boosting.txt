Guess-Averse Loss Functions For Cost-Sensitive Multiclass Boosting

Oscar Beijbom∗
Mohammad Saberian∗
David Kriegman
Nuno Vasconcelos
University of California, San Diego, 9500 Gilman Drive, 92093 La Jolla, CA

Abstract
Cost-sensitive multiclass classification has recently acquired significance in several applications, through the introduction of multiclass datasets with well-defined misclassification
costs. The design of classification algorithms for
this setting is considered. It is argued that the
unreliable performance of current algorithms is
due to the inability of the underlying loss functions to enforce a certain fundamental underlying
property. This property, denoted guess-aversion,
is that the loss should encourage correct classifications over the arbitrary guessing that ensues
when all classes are equally scored by the classifier. While guess-aversion holds trivially for
binary classification, this is not true in the multiclass setting. A new family of cost-sensitive
guess-averse loss functions is derived, and used
to design new cost-sensitive multiclass boosting
algorithms, denoted GEL- and GLL-MCBoost.
Extensive experiments demonstrate (1) the importance of guess-aversion and (2) that the GLL
loss function outperforms other loss functions for
multiclass boosting.

1. Introduction
Boosting methods play an important role in classification
problems. A prominent example is the Viola-Jones face
detection algorithm (Viola & Jones, 2001c) which utilizes
a classifier cascade designed with AdaBoost (Freund &
Schapire, 1996). Other, more recent, applications include
pedestrian and object detection (Dollár et al., 2010; Torralba et al., 2004). However, while boosting classifiers are
traditionally trained to minimize the error rate, in many applications some errors are more costly than others. For exProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

OBEIJBOM @ UCSD . EDU
SABERIAN @ UCSD . EDU
KRIEGMAN @ UCSD . EDU
NVASCONCELOS @ UCSD . EDU

ample, in ImageNet (Deng et al., 2009) the cost of misclassification between, e.g., ‘Mountain Gorilla’ and ‘Western
Lowland Gorilla’ is lower than, say, ‘Giraffe’ and ‘Chair’.
This leads to the more general problem of cost-sensitive
multiclass classification, where misclassification costs are
specified through a cost matrix. For this problem, current cost-sensitive extensions of multiclass boosting methods, such as AdaBoost.M2 (Freund & Schapire, 1996), frequently result in sub-optimal classifiers (Lozano & Abe,
2008).
The problem of binary cost-sensitive learning
has received significant attention in the last decade. Early
work by (Elkan, 2001) established some fundamental results for learning algorithms that operate in this scenario.
This work recommends the explicit computation of classconditional probabilities from the training set, and the use
of these probabilities to obtain optimal decision boundaries, using Bayes rule. A popular example of this strategy is the MetaCost algorithm (Domingos, 1999), which
estimates posterior class probabilities through bootstrapping of the training data. The major difficulty of this approach is to obtain accurate conditional probability estimates. In fact, (Masnadi-Shirazi & Vasconcelos, 2011) analyzed this scheme and noted that large-margin methods,
such as boosting, only accurately predict class-conditional
probabilities close to the cost-insensitive decision boundary. In result, the accuracy of probability estimates tends
to be low in the neighborhood of the target cost-sensitive
boundary, leading to sub-optimal cost-sensitive decisions.
Another approach is to modify the loss function of the
boosting algorithm, to take the cost-matrix into account.
This approach has received significant attention in the binary classification literature, see e.g. AdaCost (Fan et al.,
1999), asymmetric-AdaBoost (Viola & Jones, 2001b), and
the more general framework of (Masnadi-Shirazi & Vasconcelos, 2011). These methods usually outperform those
based on class-conditional probability estimates. In parallel with algorithmic developments, substantial theoretical work has been devoted to the characterization of different losses, particularly for the binary case. Important
∗

The authors assert joint authorship for this work.

Guess-Averse Loss Functions

loss properties include classification calibration (Bartlett
et al., 2006; Scott, 2012) and margin maximization (Vapnik, 1999). When these properties hold, minimization of
the loss function guarantees consistency with Bayes rule
and bounds on generalization error, respectively.
For the multiclass scenario, cost-sensitive boosting has received less attention. (Abe et al., 2004) proposed GBSE,
based on a combined scheme of instance-weighting, expansion of multiclass labels and gradient descent in functional space (Friedman, 1999; Mason et al., 2000). GBSE
was shown to outperform MetaCost (Domingos, 1999),
as well as Bagging (Breiman, 1996). Later, (Lozano &
Abe, 2008) proposed a cost-sensitive multiclass boosting
method, based on a family of p-norm cost functionals, that
generalized and improved GBSE. Recently, (Wang, 2013)
proposed MultiBoost, which is derived from a simple, yet
intuitive, loss function. Alongside these algorithmic contributions, there have been important theoretical contributions
on multiclass classification. Notably, (Tewari & Bartlett,
2007; Ramaswamy & Agarwal, 2012) generalized the concept of classification consistency to the multiclass scenario.
Other concepts, such as margin maximization are still valid
for cost-sensitive multiclass classification. However, losses
with these properties do not always produce robust classifiers in practice.
In this work we argue that this is because in the multiclass setting, losses with all the above properties may fail
to enforce a simple and intuitive property. This property,
which we denote by guess-aversion, is that the loss should
encourage correct classifications over the arbitrary guessing that ensues when all classes are equally scored by the
classifier. We show that, while this property holds trivially
for most binary and multiclass loss functions, this is not
true for the cost-sensitive multiclass setting. In fact, loss
functions that have lately become popular for cost-sensitive
multiclass classification such as (Wang, 2013) do not exhibit it. We then derive a family of cost-sensitive guessaverse loss functions, and use them to derive cost-sensitive
extensions of MCBoost (Saberian & Vasconcelos, 2011),
denoted GEL- and GLL-MCBoost. Experiments on UCI
data and a large-scale biological computer vision dataset
show 1) the empirical importance of guess-aversion, and
2) that the GLL loss function outperforms alternative loss
functions for cost-sensitive multiclass boosting.
Contributions: This work makes three main contributions: 1) introduces the concept of guess-averse classification losses and empirically demonstrates its importance,
2) proposes a family of guess-averse cost-sensitive multiclass losses, and 3) show that the GLL loss function outperforms alternative loss functions for cost-sensitive multiclass boosting. The M ATLAB implementation of the proposed boosting algorithms, along with experimental details

is available in supplementary material1 .

2. Problem Definition
A multiclass classifier h(x) is a measurable mapping from
an example x ∈ X to a class label z ∈ {1, . . . M }. This
mapping is commonly of the form
h(x) = argmaxk Sk (x) k = 1 . . . M,

(1)

where Sk : X → R is a real-valued function, that is denoted the score of class k for example x. Sk (x) reflects the
confidence of the classifier h(x) in the assignment of x to
class k. The performance of a classifier, h(x), is evaluated
by its classification risk

	
R[h] = E CZ,h(X)
(2)
where C is a M × M cost matrix which encodes the cost
Cj,k ≥ 0 of classifying an example from class j into class
PM
k. We always assume Cj,j = 0 ∀j and k=1 Cj,k > 0, ∀j.
If Cj,k = 1 ∀j 6= k, the classification problem is said to be
cost-insensitive. The optimal classifier, i.e. that of minimal
risk R[h], implements Bayes decision rule
h∗ (x) = argmink

M
X

ηj (x)Cj,k ,

(3)

ηj (x) = PZ|X (j|x) j = 1 . . . M

(4)

j=1

where
is the posterior probability of class j (Lee et al., 2004). For
classifier learning, the risk of (2) is approximated by the
empirical risk
b
R[h]

=

1
n

X

Czi ,h(xi ) ,

(5)

(xi ,zi )∈D

where D = {(xi , zi )}n1 is a training set.
Modern learning algorithms, such as boosting, rely on a
surrogate loss function L[C, z, S(x)] of the cost matrix C,
class-label z, and score function S : X → RM . The optimal score function
∗

S L = argminS RL [S],

(6)

minimizes the risk defined by this loss
RL [S]

=

EX,Z {L[C, z, S(x)]}

(7)

which, in practice, is approximated by
n

b L [S]
R

=

1X
L[C, zi , S(xi )].
n i=1

(8)

1
A detailed reply to the issues raised by the reviewers of the
original submission of the paper is also part of this material.

Guess-Averse Loss Functions
6
Zero−One
Exponential
CS−Exponential
Hinge
Logistic

L

4

2

0
−2

Figure 1. Sets Sk for a 3-class classification problem. The blue
lines correspond to decision boundaries.

The choice of loss function L(C, z, S(x) significantly impacts the classification performance of the optimal score
∗
function S L (x). Surrogate loss functions have been the
subject of extensive research in the last decade (Bartlett
et al., 2006; Vapnik, 1999; Tewari & Bartlett, 2007). A
number of desirable properties, for both the binary and
multiclass classification settings, have been identified including:
Classification Calibration: A loss L is classification cali∗
brated if the use of S L (x) in (1) results in the same decision as the Bayes rule of (3) (Bartlett et al., 2006; Tewari &
Bartlett, 2007).
Margin Maximizing: Given a training pair (x, z), a loss L
b L results in
is margin maximizing if the minimization of R
the maximization of the margin
M(z, S(x)) = Sz (x) − max Sk (x).
k6=z

(9)

It has been shown that margin maximizing losses have
better generalization performance than losses without this
property (Vapnik, 1999).
The importance of these conditions is well established
for cost-insensitive binary classification. The problems
of cost-sensitive and multiclass classification have, however, proven more elusive. This is particularly true for
cost-sensitive multiclass classification. In our experiments,
losses that satisfy all the conditions above frequently produce poor decision rules. We argue that this is because in
the multiclass setting, losses with all the properties above
can still lack a simple but important property. We discuss
this property next.

3. Guess-averse losses
We start by defining the support set, or simply support, of
class k.
Definition 1. Support set of class k is set of all score vectors for which Sk is the largest score, i.e.

	
Sk = S|S ∈ RM , Sk > Sj ∀j 6= k .
(10)

−1

0
v

1

2

Figure 2. Common binary loss functions. The guess-averse
property is trivially satisfied by these functions, including the
cost-sensitive version of the exponential loss (Masnadi-Shirazi &
Vasconcelos, 2011).

If an example x belongs to class z, then Sz is the set of
scores S(x) for which the decision rule of (1) assigns x
to the correct class. For example Figure 1 shows supports
S1 , S2 , S3 , a the 3-class classification problem2 .
We next define arbitrary guess points.
Definition 2. A score vector, S(x) ∈ RM , is an arbitrary
guess point if Sk (x) is independent of k, i.e. Sk (x) =
S1 (x) ∀k. The set of all arbitrary guess points is denoted
A.
Arbitrary guess points are the points of maximal uncertainty for the classification rule of (1). Since all classes
have the same score at these points, (1) produces a tie
and the classifier h(x) selects an arbitrary class by guessing. While this is the optimal decision when the probabilities of (4) are indeed identical, it is otherwise sub-optimal.
A sensible loss function should steer learning algorithms
away from arbitrary guess points for large portions of the
example space, X . In particular, the loss should encourage
more correct classifications than arbitrary guesses, by encouraging S(x) to be in Sz for the largest possible subset
of X (where z is the label of x). When this is the case,
the loss is said to be averse to guessing. This leads to the
following definition.
Definition 3. A loss L is averse to guessing if for any cost
matrix C, any class z, any S ∈ Sz and any A ∈ A
L(C, z, S) < L(C, z, A).

(11)

In binary classification such as boosting or SVM it is common to assume S1 (x) = −S2 (x) = f (x). In these
cases, there is only one arbitrary guess point, i.e. A =
{(0, 0)} and guess-aversion reduces to L(C, 1, [0, 0]) >
L(C, 1, [v, −v]) ∀v ∈ R+ . As shown in Figure 2 this property holds for all popular binary losses, e.g. the hinge
loss of SVMs (Vapnik, 1999), the logistic loss, the exponential loss of boosting (Freund & Schapire, 1996) and its
2

For better illustration in the figures of this paper related to
3-class classification, we assume S3 = −S1 − S2 , and only show
a 2D projection of the score space

Guess-Averse Loss Functions

3

3

3

2

2

2

1

1

1

0

0

0

G

−1

−1

−2
−5

−5
−2
−5

0
0
5

0
0

5

S1

S2

−5
−2
−5

0
0

5

H

−1

−5

S2

(a)

(b)

5

5

S1

5

S1

S2

(c)

Figure 3. The guess-averse property for a three class problem: S1 increases along the x-axis, S2 along the y-axis and S3 = −S2 − S1
(not shown). The figures show loss surfaces for an example x from class 1, after a vertical shift such that L(C, 1, 0) = 0. As in Figure 1,
S1 is the frontal triangle. 3(a) shows a surface plot of the cost-insensitive version of the loss Ls of (12). Point G correspondent to score
vector [3, 2, −5] ∈ S1 , violates the guess-averse property, since Ls (C, 1, G) > Ls (C, 1, 0). The loss thus prefers arbitrary guessing to
the correct classification. 3(b) shows a surface plot of the cost-insensitive version of the Llog,exp loss of (21). In this case, the guessaverse constraint is met for all points in S1 . 3(c) shows a surface plot of Llog,exp for the cost-sensitive case where C1,2 = 1, C1,3 = 10.
This again has the guess-averse property, but one of the loss surface facets (corresponding to the boundary between S1 and S2 ) is shifted
away from S1 . Note that while H = [1.5, 2, −3.5] is clearly outside S1 it has a lower loss than the origin. The loss function thus prefers
the score of H to that of arbitrary guessing. This is reasonable: since C1,3 is large and mis-classifying x to class 2 has lower risk than
using S = 0, which assigns it to the third class with probability 0.33.

cost-sensitive extensions (Masnadi-Shirazi & Vasconcelos,
2011). In the multiclass case, however, the guess-averse
condition is not as trivial as in the binary case. For example, it does not hold for the loss function
Ls (C, z, S(x))

=

M
X

Cz,j eSj (x)

(12)

j=1

PM
where j=1 Sj (x) = 0. This loss function is classification
calibrated and popular for both cost-sensitive (Wang, 2013)
and cost-insensitive learning (Lee et al., 2004). To show
that (12) is not guess-averse, note that for [3, 2, −5] ∈ S1
and 0 = [0, 0, 0] ∈ A,
Ls (C, 1, [3, 2, −5]) ≈ 7.4 > Ls (C, 1, 0) = 2,

(13)

where C is the cost-insensitive cost matrix. Therefore this
loss prefers arbitrary guessing over correct classification,
i.e. Ls is not averse to guessing. Figure 3(a) presents a
plot of Ls (C, 1, S(x))−Ls (C, 1, 0) for the cost-insensitive
case.
Similarly, it is shown in Appendix A3 that the loss function
Lt (C, z, S(x))

=

M X
M
X

Cz,j eSj (x)−Sk (x) (14)

k=1 j=1

is classification calibrated but not guess-averse.
3

Appendices are available in the supplementary material

Finally, it should be noted that the guess-averse property has some similarity with the “c-calibration” property
of (Vernet et al., 2011). C-calibration requires that the loss
of correct classification be smaller than the loss of incorrect
classification. However, there are fundamental differences.
First, the guess-averse property is defined using the set of
scores, whereas c-calibration is defined on the probability
simplex. Second, and most important, while the guessaverse property relies on the comparison between the score
of correct classification and the score of arbitrary guessing, c-calibration compares the score of correct classification to all possible incorrect classifications. It is shown in
Appendix B that c-calibration implies guess-aversion, but
the converse is not true. For example, the GLL loss defined
in (21) is guess-averse but not c-calibrated.
3.1. A family of guess-averse losses
We introduce a family of cost-sensitive multiclass loss
functions.
Definition 4. Let x be an example of class z, C a cost
matrix, and S(x) ∈ RM a score vector. Then, for any
measurable functions γ(·) and φ(·) the loss function
P

M
Lγ,φ (C, z, S(x)) = γ
j=1 Cz,j φ(Sz (x) − Sj (x))
(15)
is denoted a γ − φ loss.
The following lemma provides a sufficient condition for

Guess-Averse Loss Functions

Lγ,φ to be a guess-averse loss.
Lemma 1. Let γ : R → R be a monotonically increasing
function and φ : R → R a function satisfying
φ(v) < φ(0)

∀v > 0.

(16)

Then, Lγ,φ is a guess-averse loss.
Proof. If example x belongs to class z and if S(x) ∈ Sz
then Sz (x)−Sj (x) > 0 ∀j 6= z. Therefore, it follows from
(16) that
φ(Sz (x) − Sj (x)) < φ(0)

∀j 6= z,

and from the non-negativity of the costs that
PM
PM
j=1 Cz,j .
j=1 Cz,j φ(Sz (x) − Sj (x)) < φ(0)
Finally since γ(·) is monotonically increasing


PM
Lγ,φ (C, z, S(x)) < γ φ(0) j=1 Cz,j .

(17)

On the other hand, for any A ∈ A, Aj = A1 ∀j and thus
P

M
Lγ,φ (C, z, A) = γ
C
φ(A
−
A
)
z,j
j
z
j=1


PM
= γ φ(0) j=1 Cz,j .
(18)
It follows from (17) and (18) that ∀S(x) ∈ Sz , ∀A ∈ A
Lγ,φ (C, z, S(x)) < Lγ,φ (C, z, A)
and thus Lγ,φ is guess-averse.

The second, Generalized Logistic Loss (GLL), is obtained
by setting γ(v) = log(1 + v) and φ(v) = e−v ,



PM
Llog,exp (C, z, S(x)) = log 1 + j=1 Cz,j eSj (x)−Sz (x) .
(21)
For cost-insensitive classification, this reduces to the multiclass logistic loss of (Friedman et al., 2000). Figure 3(b)
shows a surface plot of the GLL, for the cost-insensitive
3-class problem.
These two losses have the desired behavior for costsensitive multiclass classification. Assume that z is the
class of example x. If Sz (x) > Sj (x) ∀j 6= z, then all arguments in the exponent of (20) and (21) are negative and
the loss is small. On the other hand, if ∃j | Sj (x) > Sz (x),
then S(x) 6∈ Sj and the loss is larger than Cz,j . This
is sensible, since Cz,j is the cost of assigning x to class
j and j is a possible outcome of (1). Figure 3(c) shows
the loss surface of GLL for an example x from class 1,
with costs C1,2 = 1, C1,3 = 10. Note that the loss is
still guess-averse but one of its facets (corresponding to the
surface between S1 and S2 ) is shifted away from S1 . In
fact, while the point H = [1.5, 2, −3.5] is clearly outside
S1 , it has lower loss than the origin 0 ∈ A, which means
that the loss function prefers the score of H to arbitrary
guessing. This is not surprising since using (1) the example will be assigned to the second class, incurring a cost
of C1,2 = 1. On the other hand, the score 0 ∈ A force
the classifier to guess arbitrarily, and results in an expected
cost of 13 C1,1 + 13 C1,2 + 13 C1,3 ≈ 3.6.

4. Algorithm
It can be shown that various previous losses in the literature, such as the pairwise comparison loss of (Zhang,
2004), are γ − φ losses. For binary cost-insensitive classification, (15) reduces to
Lγ,φ (C, 1, S(x)) = γ (φ(S1 (x) − S2 (x))) .

(19)

Defining the scores as S1 (x) = −S2 (x) = 12 f (x) for some
function f (x), and using the identity map for γ, i.e. γ(v) =
v, (15) becomes the standard binary margin loss.
We next define two guess-averse, cost-sensitive multiclass
γ − φ losses. The first, Generalized Exponential Loss
(GEL), is obtained by setting γ(v) = id(v) = v and
φ(v) = e−v
Lid,exp (C, z, S(x)) =

PM

j=1

Cz,j eSj (x)−Sz (x) .

(20)

This is a straightforward cost-sensitive extension of the loss
of (Saberian & Vasconcelos, 2011). For M = 2, GEL reduces to the cost-sensitive loss of (Viola & Jones, 2001a).

In this section, we derive Boosting algorithms for the two
guess-averse γ − φ losses, Llog,exp of (20) and Lid,exp
of (21), as well as two non guess-averse losses Ls of
(14) and Lt of (12). In principle, these losses can
be combined with any of the existing multiclass boosting approaches, e.g. AdaBoost-M1 & -M2 (Freund &
Schapire, 1996), AdaBoost-MH (Schapire & Singer, 1999),
SAMME (Zhu et al., 2009), AdaBoost-MM (Mukherjee &
Schapire, 2013), or MultiBoost (Shen & Hao, 2011). In
this work, we adopt MCBoost (Saberian & Vasconcelos,
2011) because it simplifies the derivation of the new boosting algorithm and unifies most of the boosting approaches
in the literature. We start by briefly reviewing this method,
referring the reader to the original paper for further details.
4.1. MCBoost
Given an M -class classification problem, MCBoost learns a multi-dimensional predictor f (x) =
[f1 (x), f2 (x) . . . fM −1 (x)] ∈ RM −1 by minimizing the

Guess-Averse Loss Functions

risk of (8) for cost-insensitive version of (20),
LM C (z, S(x)) =

M
X

eSj (x)−Sz (x)

(22)

j=1

where z is the label of example x,
Sk (x) =

1
hyk , f (x)i,
2

(23)

h·, ·i the Euclidean dot product and Y = {y1 , y2 , ...ym } a
M −1
set
such that
PMof codewords that form a simplex in R
y
=
0
and
ky
k
=
1
∀j.
j
j
2
j=1
Given a set, G, of weak learners where g(x) ∈ G : X → R,
and a training set D = {(xi , zi )}n1 , MCBoost solves the
optimization problem

n X
M
X

1

b L [f ] =
minf R
e− 2 hyzi −yj ,f (xi )i
MC
(24)
i=1 j=1


s.t
fr ∈ span(G) ∀r = 1 . . . M − 1,
using coordinate descent in function space (Mason et al.,
2000; Friedman, 1999)4 .
In each boosting iteration, for each coordinate r, MCBoost
computes the directional derivative of the risk for an update
of fr (x) along the direction of g(x)

b L [f + g1r ] 
∂R
MC
b
δ RLM C [f ; r, g] =
(25)


∂
=0

n
X
∂LM C (zi , (f + g)(xi )1r ) 
=
 (26)
∂
=0
i=1
n
X
=
g(xi )wr (xi ),

Algorithm 1 (GLL, GEL, Ls , Lt )-MCBoost
Input: Number of classes M , a set of codewords Y =
{y 1 , . . . , y M } ∈ RM −1 , a number of iterations T , a
dataset D = {(xi , ci )}ni=1 , and a cost matrix C.
Initialization: set f = 0 ∈ Rd
for t = 1 to T do
for r = 1 to M − 1 do
Compute wr (xi ) using (34 - 37)
Find gr∗ (x), αr∗ using (30) and (31)
Update fr (x) := fr (x) + αr∗ gr∗ (x)
end for
end for
Output: decision rule: arg maxj hyj , fj (x)i

is computed with a line search. Finally, the predictor is
updated as
f := [f1 , . . . , fr + αr∗ gr∗ , . . . , fM −1 ].

Upon convergence, the posterior probabilities of (4) can be
estimated as
ehyj ,f (x)i
.
PZ|X (j|x) = PM
hyk ,f (x)i
k=1 e

Under the MCBoost framework, deriving a boosting algob Lid,exp , R
b Llog,exp , R
b L and
rithm for the minimization of R
s
b L reduces to recomputing directional gradients. Using
R
t
(23), (25) and (27), this results in the following definitions
of the boosting weights
r
wL
id,exp (xi ) =

M
X

Czi ,j h∆zi ,j , 1r iehf (xi ),∆zi ,j i

(34)

j=1

with
r

w (xi )

=

=

∂
∂fr (xi )
M
X

M
X

PM
e

hf (xi ),∆zi ,j i

(28)

r
wL
log,exp (xi )

j=1

h∆zi ,j , 1r iehf (xi ),∆zi ,j i ,

(29)

r
wL
(xi )
s

where ∆zi ,j = 21 [yj − yzi ] and 1r ∈ RM −1 is a vector
whose rth element is one and the remaining zero. MCBoost then selects the best weak learner as
gr∗

=

b L [f ; r, g]
argming∈G δ R
MC

(30)

and the optimal step size along gr∗ ,
αr∗

b L [f +
= argminα∈R R
MC

Czi ,j h∆zi ,j , 1r iehf (xi ),∆zi ,j i
(35)
PM
1 + j=1 Czi ,j ehf (xi ),∆zi ,j i

j=1

=

=

M
X

Czi ,j hyj , 1r iehf (xi ),yj i

(36)

j=1

j=1

4

(33)

4.2. Cost Sensitive MCBoost

(27)

i=1

(32)

r
wL
(xi ) =
t

M X
M
X

Czi ,j h∆j,k , 1r iehf (xi ),∆j,k i (37)

k=1 j=1

These cost-sensitive extensions of MC-Boost are presented
in Algorithm 1.

5. Experiments
αgr∗ 1r ],

(31)

While we focus on coordinate descent MCBoost, all results
are also valid for gradient descent MCBoost (Saberian & Vasconcelos, 2011).

Various experiments were designed to evaluate the importance of the guess-averse property. These experiments used
10 UCI datasets and a large scale computer vision dataset
for coral classification (Beijbom et al., 2012).

Guess-Averse Loss Functions
Table 1. Characteristics of the used UCI datasets
Dataset
#Training #Testing #Attributes # Classes
Breast Tissue
81
25
9
6
Ecoli
258
78
7
8
Image Segmentation
210
2, 100
19
7
Libras
292
68
90
15
Vertebral
239
71
6
3
Vehicle
692
154
18
4
Shuttle
43, 500 14, 500
9
7
Pen Digit
7, 494
3, 498
16
10
Optical Digits
3, 823
1, 797
64
10
Satellite Image
4, 435
2, 000
36
6

We used the boosting algorithm derived in section 4.2 to
compare the performance of the two guess-averse losses
Llog,exp of (20) and Lid,exp of (21), with the two non
guess-averse losses Ls of (14) and Lt of (12). For the sake
of completeness, we also compared these algorithms with
two baseline algorithms 1) the cost-insensitive MCBoost
algorithm of (Saberian & Vasconcelos, 2011) which uses
the loss function of (22) and 2) a cost-sensitive version of
MCBoost, denoted P-MCBoost, which computes posterior
probabilities with (33), and uses them in the cost-sensitive
Bayes decision rule of (3). In all experiments, regression
classifiers were used as weak learners, i.e. g(x) = ax + b
where a, b are found by solving a weighted least square
problem (Friedman et al., 2000). Performance was evaluated by the classification risk of (5) on the test set.
In our experiments, we focus on symmetric cost-matrices
(Cj,k = Ck,j ∀j, k), which are natural for many classification problems. For example, when the class structure is
given by a taxonomy one often imposes symmetric misclassification costs for class pairs, which vary depending
on class distances in the taxonomical tree. This is the case
for various popular computer vision datasets such as ImageNet (Deng et al., 2009), CalTech birds (Welinder et al.,
2010), or MLC (Beijbom et al., 2012). Other examples are
losses based on ordinal regression or the hamming distance
between class labels (Ramaswamy & Agarwal, 2012).
In summary, since the goal of the experiments was to investigate the importance of guess-aversion, we fixed the
boosting framework (MCBoost), weak learners (regression
[ax + b]) and the number of boosting iterations. This setup
enables a fair comparison between the loss functions.
5.1. UCI Datasets
We start with an evaluation on the ten UCI datasets of Table 1. For these, training/testing partition are either predefined or the data is randomly split into 80% training and
20% testing. For each dataset, a random symmetric cost
matrix was generated, Cj,k j 6= k drawn uniformly from
[1, 10] ∈ R, and all boosted classifiers were trained with

100 iterations. The procedure was repeated 50 times per
dataset, and the average classification risk of (5) is reported
for each classifier in Table 2. We also ranked the algorithms according to their performance on each dataset. The
average rank is shown in the last row of Table 2.
Several observations can be made. First, according to the
average ranks, all algorithms based on guess-averse losses
had significantly better performance than the non guessaverse algorithms. Note that, as shown in (Wang, 2013)
and Appendix A, Ls -MCBoost and Lt -MCBoost are costsensitive and classification calibrated, i.e. with infinite
training examples and suitable weak learners they implement Bayes rule. However, they are outperformed by all
guess-averse losses. In particular, they are outperformed by
GLL-MCBoost, which is not classification calibrated. This
experiment therefore suggests that guess-aversion may be
more important than classification calibration in practice.
Second, GLL-MCBoost consistently outperformed the alternative guess-averse boosting algorithms by a large margin. Not surprisingly, it has better performance than MCBoost, which does not account for the cost matrix. The
gains over GEL-MCBoost have a more subtle justification.
While this is a cost-sensitive method, it can be shown that
the GEL loss is insensitive to symmetric cost-matrices (Appendix C). This explains why GEL-MCBoost performs on
par with the cost-insensitive MCBoost. On the other hand,
the cost matrix is taken into account by P-MCBoost, which
is based on Bayes rule and posterior probability estimates.
However, as argued by (Masnadi-Shirazi & Vasconcelos,
2011), these estimates are accurate only in the neighborhood of the cost-insensitive decision boundary. Since the
cost-sensitive boundary can be far from its cost-insensitive
counterpart, the posterior estimates can be inaccurate in the
regions of interest for cost-sensitive classification. Still, PMCBoost shows an improvement over MCBoost.
5.2. Moorea Labeled Corals
We next evaluated the efficacy of the proposed boosting
algorithm on a large scale computer vision dataset. In
addition to the boosting methods of the previous section,
we compare against three method based on Support Vector
Machines.
Moorea Labeled Corals (MLC) comprises more than 400K
expert annotations on more than 2K coral reef survey images, assembled over the years 2008−2010 (Beijbom et al.,
2012). MLC has a natural hierarchy. For example, misclassification between coral genera is less severe than misclassification between coral and algae, or algae and sand.
Using this natural class hierarchy, we derived an appropriate cost-matrix, provided in Appendix D, and used it in all
MLC experiments. We adopted the feature representation
of (Beijbom et al., 2012), and considered all images and

Guess-Averse Loss Functions
Table 2. Cost-sensitive classification risk (5) on the UCI datasets. Results indicated as (mean ± standard error) for 50 random costmatrix trials. The strongest results for each dataset are marked in bold with significance determined by a paired t-test at the 5% level.
Guess-averse
P-MCBoost
GEL-MCBoost
1.63 ± 0.06
1.72 ± 0.06
0.84 ± 0.03
0.94 ± 0.03
0.46 ± 0.02
0.47 ± 0.01
1.16 ± 0.03
1.31 ± 0.03
0.77 ± 0.05
0.77 ± 0.04
1.41 ± 0.06
1.41 ± 0.06
0.33 ± 0.02
0.36 ± 0.02
0.49 ± 0.01
0.50 ± 0.01
0.85 ± 0.03
0.89 ± 0.03
0.24 ± 0.00
0.27 ± 0.01
1.70
3.20

The classification risk of (5) on the test sets are shown in
Figure 4. Again, the two methods based on non guessaverse losses (Ls -MCBoost and Lt -MCBoost) were significantly outperformed by the guess-averse methods. Among
the guess-averse methods, cost-insensitive MCBoost was
again the weakest, with a classification risk of ≈ 0.66.
The probability based P-MCBoost and GEL-MCBoost
achieved roughly the same performance. SVM-MC and
SVM-OVA were stronger, achieving a classification risk
≈ 0.64. Nevertheless, their performance was weaker than
those of GLL-MCBoost and SVM-CSMC, which optimize
cost-sensitive and guess-averse losses. These methods had
similar performance, achieving the lowest classification
risks of ≈ 0.62. This similar performance is not surprising since the GLL loss is a differential approximation of
the multiclass hinge loss function used in SVM-CSMC.

0.7
0.68
0.66
0.64

C
SM

−M
SV

M

−C

M
SV

−O
M
SV

C

VA

os
Bo

−M
C
t

L

t

Bo

os
s

L

−M

C

Bo
C
−M

LL

os

t
os

t
os
EL

−M
C

Bo

os
M
C

Bo

Bo

G

G

t

t

0.6

t

0.62

C

The boosting baselines of the previous section were complemented by three linear SVM algorithms. The first,
SVM-OVA, implements the popular one-versus-all approach; the second, SVM-MC, is a multiclass SVM based
on the formulation of (Crammer & Singer, 2002); and
the third, SVM-CSMC, uses the cost-sensitive multiclass
formulation of (Branson et al., 2013). As shown in Appendix E, loss functions of SVM-MC and SVM-CSMC
are both guess-averse. Among the three SVM algorithms,
SVM-CSMC is the only one that is cost-sensitive. For the
first two methods we use the L IBLINEAR implementation
(Fan et al., 2008), and for the third the publicly available
code of (Branson et al., 2013). Cross-validation was performed on the training set to tune the SVM regularization parameter, λ = {10−17 . . . 107 }, so as to minimize
the classification risk of (5). The boosting methods were
trained with 500 iterations.

0.72

M

annotations from 2008, split randomly 10 times so that 2/3
of the images were used for training and 1/3 for testing.

Non guess-averse
Ls -MCBoost Lt -MCBoost
1.88 ± 0.07
1.83 ± 0.08
1.12 ± 0.04
1.21 ± 0.03
0.97 ± 0.03
1.06 ± 0.04
2.32 ± 0.06
2.43 ± 0.06
0.87 ± 0.05
0.92 ± 0.06
1.59 ± 0.07
1.55 ± 0.07
0.83 ± 0.04
0.86 ± 0.05
1.28 ± 0.03
1.54 ± 0.04
1.38 ± 0.04
1.36 ± 0.04
0.64 ± 0.01
0.81 ± 0.02
5.30
5.70

GLL-MCBoost
1.48 ± 0.05
0.82 ± 0.03
0.45 ± 0.02
1.01 ± 0.03
0.78 ± 0.04
1.40 ± 0.06
0.17 ± 0.01
0.49 ± 0.01
0.88 ± 0.03
0.26 ± 0.00
1.50

P−

MCBoost
1.83 ± 0.06
0.96 ± 0.03
0.47 ± 0.02
1.24 ± 0.03
0.81 ± 0.05
1.42 ± 0.06
0.36 ± 0.02
0.51 ± 0.01
0.88 ± 0.03
0.26 ± 0.00
3.50

Classification Risk

Dataset
Breast Tissue
Ecoli
Image Segmentation
Libras
Vertebral
Vehicle
Shuttle
Pen Digits
Satellite Image
Optical Digits
Avg. Rank

Figure 4. Cost-sensitive classification risk (5) on Moorea Labeled Corals. Bars indicate average classification risk and standard error for 10 random splits of the data.

6. Conclusion
In this work, we have proposed that guess-aversion is an
important property for multiclass loss functions. Guided
by this property, we derived a family of guess-averse
loss functions, and developed boosting algorithms based
on two members of this family, GLL-MCBoost, and
GEL-MCBoost. Extensive experiments have demonstrated
the importance of guess-aversion and that the GLL loss
function outperforms alternative loss functions for costsensitive multiclass boosting.
Acknowledgements: This work was partially funded by
NSF awards IIS-1208522, CCF-0830535, ATM-0941760
and the Korean Ministry of Trade, Industry & Energy, grant
no. 10041126.

References
Abe, Naoki, Zadrozny, Bianca, and Langford, John. An
iterative method for multi-class cost-sensitive learning.
In SIGKDD, 2004.
Bartlett, Peter L. Jordan, Michael I. and McAuliffe,

Guess-Averse Loss Functions

Jon D.˙Convexity, classification, and risk bounds. JASA,
2006.

Mason, L., Baxter, J., Bartlett, P., and Frean, M. Functional gradient techniques for combining hypotheses.
Advances in Large Margin Classifiers, 2000.

Beijbom, Oscar, Edmunds, Peter J, Kline, David I,
Mitchell, B Greg, and Kriegman, David. Automated annotation of coral reef survey images. In CVPR, 2012.

Mukherjee, Indraneel and Schapire, Robert E. A theory of
multiclass boosting. JMLR, 2013.

Branson, Steve, Beijbom, Oscar, and Belongie, Serge. Efficient large-scale structured learning. In CVPR, 2013.

Ramaswamy, Harish G and Agarwal, Shivani. Classification calibration dimension for general multiclass losses.
In NIPS, 2012.

Breiman, Leo. Bagging predictors. ML, 1996.
Crammer, Koby and Singer, Yoram. On the algorithmic
implementation of multiclass kernel-based vector machines. JMLR, 2002.
Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai,
and Fei-Fei, Li. Imagenet: A large-scale hierarchical
image database. In CVPR, 2009.

Saberian, M. and Vasconcelos, N. Multiclass boosting:
Theory and algorithms. In NIPS, 2011.
Schapire, Robert E. and Singer, Yoram. Improved boosting
algorithms using confidence-rated predictions. Machine
Learning, 1999.
Scott, Clayton. Calibrated asymmetric surrogate losses.
Electronic Journal of Statistics, 2012.

Dollár, Piotr, Belongie, Serge, and Perona, Pietro. The
fastest pedestrian detector in the west. In BMVC, 2010.

Shen, Chunhua and Hao, Zhihui. A direct formulation for
totally-corrective multi-class boosting. In CVPR, 2011.

Domingos, Pedro. Metacost: a general method for making
classifiers cost-sensitive. In SIGKDD, 1999.

Tewari, Ambuj and Bartlett, Peter L. On the consistency of
multiclass classification methods. JMLR, 2007.

Elkan, Charles. The foundations of cost-sensitive learning.
In IJCAI, 2001.

Torralba, Antonio, Murphy, Kevin P, and Freeman,
William T. Sharing features: efficient boosting procedures for multiclass object detection. In CVPR, 2004.

Fan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang,
Xiang-Rui, and Lin, Chih-Jen. Liblinear: A library for
large linear classification. JMLR, 2008.

Vapnik, Vladimir. The nature of statistical learning theory.
springer, 1999.

Fan, Wei, Stolfo, Salvatore J, Zhang, Junxin, and Chan,
Philip K. Adacost: misclassification cost-sensitive
boosting. In ICML, 1999.
Freund, Yoav and Schapire, Robert E. Experiments with a
new boosting algorithm. In ICML, 1996.

Vernet, E., Williamson, R., and Reid, M. Composite multiclass losses. NIPS, 2011.
Viola, Paul and Jones, Michael. Fast and robust classification using asymmetric adaboost and a detector cascade.
In NIPS, pp. 1311–1318, 2001a.

Friedman, J. Greedy function approximation: A gradient
boosting machine. Annals of Statistics, 1999.

Viola, Paul and Jones, Michael. Fast and robust classification using asymmetric adaboost and a detector cascade.
NIPS, 2001b.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
Additive logistic regression: a statistical view of boosting. Annals of Statistics, 2000.

Viola, Paul and Jones, Michael. Rapid object detection
using a boosted cascade of simple features. In CVPR,
2001c.

Lee, Yoonkyung, Lin, Yi, and Wahba, Grace. Multicategory support vector machines: Theory and application
to the classification of microarray data and satellite radiance data. JASA, 2004.
Lozano, Aurélie C and Abe, Naoki. Multi-class costsensitive boosting with p-norm loss functions. In
SIGKDD, 2008.
Masnadi-Shirazi, Hamed and Vasconcelos, Nuno. Costsensitive boosting. PAMI, 2011.

Wang, Junhui. Boosting the generalized margin in costsensitive multiclass classification. JCGS, 2013.
Welinder, Peter, Branson, Steve, Mita, Takeshi, Wah,
Catherine, Schroff, Florian, Belongie, Serge, and Perona, Pietro. Caltech-ucsd birds 200. Caltech, 2010.
Zhang, Tong. Statistical analysis of some multi-category
large margin classification methods. JMLR, 2004.
Zhu, Ji, Zou, Hui, Rosset, Saharon, and Hastie, Trevor.
Multi-class adaboost. Statistics and Its Interface, 2009.

