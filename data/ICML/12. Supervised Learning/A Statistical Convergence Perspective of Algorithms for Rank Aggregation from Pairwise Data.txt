A Statistical Convergence Perspective of Algorithms for Rank Aggregation
from Pairwise Data
Arun Rajkumar
Shivani Agarwal
Indian Institute of Science, Bangalore 560012, INDIA

Abstract
There has been much interest recently in the
problem of rank aggregation from pairwise data.
A natural question that arises is: under what
sorts of statistical assumptions do various rank
aggregation algorithms converge to an ‘optimal’
ranking? In this paper, we consider this question in a natural setting where pairwise comparisons are drawn randomly and independently
from some underlying probability distribution.
We first show that, under a ‘time-reversibility’
or Bradley-Terry-Luce (BTL) condition on the
distribution, the rank centrality (PageRank) and
least squares (HodgeRank) algorithms both converge to an optimal ranking. Next, we show that a
matrix version of the Borda count algorithm, and
more surprisingly, an algorithm which performs
maximum likelihood estimation under a BTL assumption, both converge to an optimal ranking
under a ‘low-noise’ condition that is strictly more
general than BTL. Finally, we propose a new
SVM-based algorithm for rank aggregation from
pairwise data, and show that this converges to an
optimal ranking under an even more general condition that we term ‘generalized low-noise’. In
all cases, we provide explicit sample complexity
bounds for exact recovery of an optimal ranking.
Our experiments confirm our theoretical findings
and help to shed light on the statistical behavior
of various rank aggregation algorithms.

1. Introduction
Rank aggregation is a classical problem that has been studied in several contexts, starting with social choice theory
in 18th century France (Borda, 1781; Condorcet, 1785),
and more recently, in computer science, statistics, linear
algebra, and optimization, with a variety of different apProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

ARUN R @ CSA . IISC . ERNET. IN
SHIVANI @ CSA . IISC . ERNET. IN

plications, and with different forms of both input rankings
and desired aggregated rankings being considered (Dwork
et al., 2001; Hochbaum, 2006; Meila et al., 2007; Ailon
et al., 2008; Klementiev et al., 2008; Jagabathula & Shah,
2008; Guiver & Snelson, 2009; Ailon, 2010; Qin et al.,
2010; Jiang et al., 2011; Gleich & Lim, 2011; Volkovs
& Zemel, 2012; Negahban et al., 2012; Soufiani et al.,
2012; Osting et al., 2013). A prominent setting that has
gained interest in recent years is that of rank aggregation from pairwise data, where there is a set of n items
to rank (such as movies or webpages), and one is given
outcomes of various pairwise comparisons among these
items (such as pairwise movie or webpage preferences of
users); the goal is to aggregate these pairwise comparisons into a global ranking over the items. Various algorithms have been studied for this problem, including maximum likelihood under a Bradley-Terry-Luce (BTL) model
assumption, rank centrality (PageRank/MC3) (Negahban
et al., 2012; Dwork et al., 2001), least squares (HodgeRank) (Jiang et al., 2011), and a pairwise variant of Borda
count (Borda, 1781; Jiang et al., 2011) among others.
In this paper, we consider statistical convergence properties
of these rank aggregation algorithms under a natural statistical model, under which pairwise comparisons are drawn
i.i.d. from some fixed but unknown probability distribution. An ‘optimal’ ranking is then one which minimizes the
probability of disagreement with a random pairwise comparison drawn from this distribution. We consider three
conditions of increasing generality on the distribution: a
BTL condition, a ‘low-noise’ (LN) condition similar to a
condition considered by (Duchi et al., 2010) in a different setting, and a ‘generalized low-noise’ (GLN) condition.
We show that the rank centrality and least squares algorithms both converge (in probability) to an optimal ranking under the BTL condition, and that the Borda count and
BTL-ML algorithms converge to an optimal ranking under
the LN condition; we then propose a new SVM based rank
aggregation algorithm which we show converges to an optimal ranking under the more general GLN condition. In
all cases, we obtain explicit sample complexity bounds.

A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data

GLN	  
LN	  
BTL	  

R ANK C ENTRALITY (N EGAHBAN ET AL ., 2012)
L EAST S QUARES (H ODGE R ANK ) (J IANG ET AL ., 2011)
B ORDA C OUNT (B ORDA , 1781; J IANG ET AL ., 2011)
BTL-ML ( CLASSICAL )
SVM-R ANK AGGREGATION ( THIS PAPER )

BTL
√
√
√
√
√

LN

GLN

×
×
√
√
√

×
×
×
×
√

Figure 1. We consider three increasingly general conditions on the distribution generating pairwise comparisons: BTL, LN, and GLN.
The table summarizes our results on convergence of various rank aggregation algorithms to an optimal ranking under these conditions.

Related Work. The work most closely related to ours is
that of Negahban et al. (Negahban et al., 2012), who analyzed convergence of the rank centrality algorithm under a
statistical model where a fixed set of item pairs is repeatedly compared a fixed number of times, and the outcomes
of the comparisons are determined by a BTL model. Our
statistical model, where pairs to be compared are drawn
randomly, is more natural in many applications (e.g. movie
rankings). Our analysis of the rank centrality algorithm
builds on that of (Negahban et al., 2012). However, we
cannot use the standard matrix concentration tools used
in (Negahban et al., 2012) since the comparison matrix in
our case does not contain independent entries; instead, we
use a McDiarmid-like concentration result of Kutin (Kutin,
2002) to analyze each entry separately. We point out our
setting differs from the active learning settings of (Ailon,
2011; Jamieson & Nowak, 2011), where the goal is to recover a true permutation on n items by actively querying
specific pairs; in our setting, the pairs are randomly sampled. Similarly, our setting differs from that of (Wauthier
et al., 2013), where each pair of items can be compared at
most once; in the rank aggregation setting we consider, it is
common to have the same pair of items compared several
times (with possibly different random outcomes). Our setting also differs from standard learning-to-rank problems
involving pairwise preferences, where algorithms such as
RankSVM or RankBoost are typically applied (Herbrich
et al., 2000; Joachims, 2002; Freund et al., 2003), as there
are no feature vectors in our setting; instead we simply
have a finite number of objects with identifiers 1, . . . , n.
Finally, our setting also differs from the subset ranking settings studied recently in machine learning and information
retrieval (Cossock & Zhang, 2008; Duchi et al., 2010; Liu,
2011), where one ranks documents for various queries.
Summary and Organization. Figure 1 summarizes our
results. Section 2 gives preliminaries. Section 3 summarizes various useful properties of the comparison matrix in
our setting. Sections 4–6 consider conditions of increasing generality on the probability distribution generating
pairwise comparisons, and analyze convergence properties
of various rank aggregation algorithms under these conditions. Section 7 gives our experimental results. All proofs
can be found in the supplementary material.

2. Preliminaries, Notation, and Background
Setup. Let[n] = {1, . . . , n} denote	a set of n items to rank.
Let X = (i, j) : i, j ∈ [n], i < j . The learner is given
a training sample S = ((i1 , j1 , y1 ), . . . , (im , jm , ym )) ∈
(X × {0, 1})m , where for each k ∈ [m], (ik , jk ) ∈ X
denotes the k-th pair of items compared, and yk ∈ {0, 1}
denotes the outcome of the comparison; we adopt the convention that yk is 1 if item jk is ranked higher than item
ik , and 0 otherwise. Given S, the goal of the learner is to
produce a ranking or permutation of the n items, σ ∈ Sn .
We assume a probability distribution µ on X from which
item pairs are sampled. For each i < j, we denote by µij
the probability of the pair (i, j) under µ; with some abuse
of notation, we also denote µji = µij ∀ i < j. We also
assume a set of conditional label probabilities from which
labels are drawn. Specifically, for each i < j, we denote by
Pij ∈ [0, 1] the probability that item j will be ranked higher
than item i when items i and j are compared; we represent
this as a pairwise preference matrix P ∈ [0, 1]n×n with
Pji = 1 − Pij for i < j and Pii = 0. The training sample S = ((i1 , j1 , y1 ), . . . , (im , jm , ym )) ∈ (X × {0, 1})m
is then assumed to be drawn randomly according to S ∼
(µ, P)m , i.e. the item pairs (ik , jk ) are drawn randomly and
independently according to µ, and conditioned on these, the
labels are drawn as yk ∼ Bernoulli(Pik ,jk ).
Given a distribution (µ, P) as above, define the expected
pairwise disagreement error of a permutation σ ∈ Sn as
X

erPD
µij Pij 1 σ(i) < σ(j) ,
(1)
µ,P [σ] =
i6=j

where 1(·) is 1 if its argument is true and 0 otherwise; this
is the probability that σ does not agree with a pairwise comparison drawn randomly according to (µ, P). An ‘optimal’
permutation is then any permutation σ ∗ satisfying
σ ∗ ∈ argminσ∈Sn erPD
µ,P [σ] .

(2)

Clearly, an ideal algorithm would recover (with high probability, for a large enough sample) such an optimal permutation. In what follows, we will consider various conditions
on (µ, P), including a ‘time-reversibility’ or BTL condition, a ‘low-noise’ condition, and a ‘generalized low-noise’
condition, and will analyze convergence properties of various rank aggregation algorithms under these conditions.

A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data

All the algorithms we analyze take as input an empirical
b ∈ [0, 1]n×n , which in our
pairwise comparison matrix P
case is constructed from S as follows:
 (1)

if i < j and Nij > 0
Nij /Nij

(1)
Pbij = 1 − Nji
(3)
/Nji
if i > j and Nji > 0


0
otherwise;
where
Pm
Nij = k=1 1(ik = i, jk = j) ;
Pm
(1)
Nij = k=1 1(ik = i, jk = j, yk = 1) .
b differs from
Note that the empirical comparison matrix P
the true pairwise preference matrix P in an important aspect: while P satisfies Pij + Pji = 1 for all i < j, in the
b if a particular pair i < j is not observed in S,
case of P,
we can have Pbij = Pbji = 0. To reinforce this distinction,
we will use the terms pairwise preference matrix for P and
b throughout.
pairwise comparison matrix for P
b is critical in our analysis: unlike (NegahThe structure of P
ban et al., 2012), where the empirical comparison matrix
was constructed by fixing a priori some subset of pairs (i, j)
to be compared and then repeatedly comparing each such
pair a fixed number of times, which led to the entries of the
b are
matrix being independent, in our case, the entries of P
not independent (note that if some pair of items is sampled
many times, other item pairs will be less frequent in S);
therefore we cannot apply the matrix concentration tools
used in (Negahban et al., 2012). Instead, we will show the
b satisfy a bounded differences property with
elements of P
b using Kutin’s
high probability, allowing us to analyze P
extension of McDiarmid’s inequality (Kutin, 2002). This
b proved in Section 3, will then be
and other properties of P,
used to analyze various algorithms in Sections 4–6.
Notation. We will find it convenient to define
µmin = min µij ,
i<j


+ 3 ln µ12
+3 .
B(µmin ) = 3 µ12
2
2
min

(4)
(5)

min

Our results below will assume µmin > 0. We will use
capital boldface letters such as P, Q for matrices and
lower case boldface letters such as f , P
π for vectors. For
n
f ∈ Rn , we will denote by kf k1 =
i=1 |fi |, kf k2 =

Pn
1/2
2
, and kf k∞ = maxi |fi | the standard L1 ,
i=1 fi
L2 and L∞ norms. Also, for f ∈ Rn , we will denote by
argsort(f ) the set of permutations that order items i ∈ [n]
in decreasing order of scores fi , breaking ties arbitrarily:

	
argsort(f ) = σ ∈ Sn : fi > fj =⇒ σ(i) < σ(j) .
Background Results. The following definition of strongly
difference-bounded random variables and concentration result for such random variables, both due to Kutin (Kutin,
2002), will be used in our analysis of the empirical comb in Section 3.
parison matrix P

Definition 1 (Strong difference-boundedness (Kutin,
2002)). Let X = (X1 , . . . , Xm ) be a vector of independent
random variables with Xi taking values in some set Ai , and
let A = A1 × · · · × Am . Let φ : A→R be any function. Let
b, c > 0 and δ ∈ (0, 1]. The random variable φ(X) is said
to be strongly difference-bounded
by (b, c, δ) if ∃B ⊂ A

with P X ∈ B ≤ δ such that for each k ∈ [m],


φ(x) − φ(x1 , . . . , x0k , . . . , xm ) ≤ c
sup
0 ∈A
x∈B,x
/
k
k


φ(x) − φ(x1 , . . . , x0k , . . . , xm ) ≤ b .
sup
x∈A,x0k ∈Ak

Theorem 2 ((Kutin, 2002)). Let X = (X1 , . . . , Xm ) be
a vector of independent random variables with Xi taking values in some set Ai , and let A = A1 × · · · × Am .
Let φ : A→R be any function such that φ(X)
is strongly

λ
, exp(−Km) . Let 0 <  ≤
difference-bounded by b, m
√


6
6
2λ K. If m ≥ max λb , 3 K
+ 3 ln K
+ 3 , then




P φ(X) − E φ(X)  ≥  ≤ 4 exp(−m2 /8λ2 ) .

b
3. Properties of Comparison Matrix P
The following lemma summarizes various useful properties
b that are used in our
of the empirical comparison matrix P
proofs. In particular, a key property is that for large enough
b are strongly difference-bounded, alm, the elements of P
lowing us to obtain concentration results for them.
Lemma 3. Let (µ, P) be such that µmin > 0. Let S ∼
b be constructed from S as in Eq. (3).
(µ, P)m , and let P
4
1. Let i 6= j. If m ≥ µmin
, then Pbij is strongly difference

−mµ2min 
bounded by 1, mµ2min , exp
.
2
√
2. Let i 6= j. Let 0 <  < 2 2. If m ≥ B(µmin ), then

 −m2 µ2 


min
P Pbij − E[Pbij ] ≥  ≤ 4 exp
.
32

1
3. Let i 6= j. Let  > 0. If m ≥ µmin
ln 1 , then

E[Pbij ] − Pij | ≤  .
√
4. Let i 6= j. Let 0 <  < 4 2. If m ≥
1
max B(µmin ), µmin
ln 2 , then


 −m2 µ2 

min
P Pbij − Pij  ≥  ≤ 4 exp
.
128
5. Let Pij ∈ (0, 1) ∀i 6= j, and let Pmin = mini6=j Pij .

Let δ ∈ (0, 1]. If m ≥ µmin1Pmin ln n(n−1)
, then with
δ
b
probability at least 1 − δ, Pij > 0 ∀i 6= j.
The proof of Part 1 makes use of Hoeffding’s inequality and
involves a somewhat detailed, careful case-by-case analysis. Part 2 then follows from Part 1 and Theorem 2. Part
3 follows by observing E[Pbij ] = Pij (1 − (1 − µij )m ).
Part 4 follows from Parts 2 and 3. Part 5 is straightforward.
Details can be found in the supplementary material.

A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data

4. Time-Reversibility/BTL Condition
We first consider the following ‘time-reversibility’ and
BTL conditions on the preference matrix P:
Definition 4 (Time-reversibility condition). We say the
pairwise preference matrix P ∈ [0, 1]n×n satisfies the
time-reversibility condition if the Markov chain Q given
by
(
1
Pij
if i 6= j
Qij = n 1 P
(6)
1 − n k6=i Pik if i = j
is time-reversible, i.e. if Q is irreducible and aperiodic and
the stationary probability vector π of Q satisfies πi Qij =
πj Qji ∀i, j ∈ [n].
Definition 5 (Bradley-Terry-Luce (BTL) condition). We
say the pairwise preference matrix P ∈ [0, 1]n×n satisfies
the Bradley-Terry-Luce (BTL) condition if it corresponds
to a BTL model, i.e. if ∃w ∈ Rn+ with wi > 0 ∀i such that
Pij = wj /(wi + wj ) ∀i 6= j.
Clearly, if P satisfies the time-reversibility condition with
Q and π as above, then ∀i 6= j, Pij > Pji =⇒ πj > πi ,
and therefore any permutation that ranks items i ∈ [n] in
decreasing order of scores πi is an optimal permutation
w.r.t. the pairwise disagreement error (see Eq. (1)). Similarly, if P corresponds to a BTL model with parameter
vector w as above, then any permutation that ranks items
according to decreasing order of scores wi is an optimal
permutation. The following lemma shows that the timereversibility and BTL conditions are in fact equivalent:
Lemma 6. A preference matrix P ∈ [0, 1]n×n satisfies the
time-reversibility condition if and only if it satisfies the BTL
condition.
Note that if P satisfies the time-reversibility condition, then
by the above result, Pij ∈ (0, 1) ∀i 6= j.
4.1. Convergence of Rank Centrality Algorithm
We start by analyzing convergence behavior of the rank
centrality algorithm (Dwork et al., 2001; Negahban et al.,
2012) (Algorithm 1)1 in our setting under the above timereversibility/BTL condition. In particular, we first show
the following result, which establishes convergence of the
b produced by the rank centrality algorithm to
score vector π
π, the stationary vector of the matrix Q defined in Eq. (6)
(in L2 norm):
1

Note that the rank centrality algorithm as presented here differs slightly from (Negahban et al., 2012) in two aspects: rather
b by the maximum degree, which in our
than divide elements of P
b
case depends on the sample S, we divide by n in constructing Q;
similarly, since in our case the graph defining the Markov chain
b depends on S and may not be strongly connected, we allow for
Q
b = 0 in this case.
the possibility of producing a default vector π
Also, while (Negahban et al., 2012) are interested in the score
b , we are interested in the ordering σ
b.
vector π
b produced by π

Algorithm 1 Rank Centrality (PageRank/MC3) (Negahban
et al., 2012; Dwork et al., 2001)
b ∈ [0, 1]n×n
Input: Empirical comparison matrix P
Construct an empirical Markov chain with transition
b as follows:
probability matrix(Q
1 b
if i 6= j
b ij = n Pij P
Q
Pbik if i = j.
1− 1
n

k6=i

b defines an irreducible, aperiodic Markov chain, then
If Q
b
b , the stationary probability vector of Q
compute π
b = 0 ∈ Rn+
else let π
Output: Permutation σ
b ∈ argsort(b
π)
Theorem 7. Let (µ, P) be such that µmin > 0 and P satisfies the BTL condition. Let Q be defined as in Eq. (6), and
let π be the stationary probability vector of Q. Let Pmin =
mini6=j Pij , πmax = maxi πi , and πmin = mini πi . Let
0 <  ≤ 1 and δ ∈ (0, 1]. If


1024 n  πmax 3  16n2 
ln
, B(µmin ) ,
m ≥ max 2 2 2
 Pmin µmin πmin
δ
then with probability at least 1 − δ (over the random draw
b is constructed), the score
of S ∼ (µ, P)m from which P
b produced by the rank centrality algorithm satisfies
vector π
kb
π − πk2 ≤  .
The proof of Theorem 7 builds on the technique used by
(Negahban et al., 2012), and makes use of two lemmas,
which establish convergence of the empirical Markov chain
b to the true chain Q in spectral norm, and a lower bound
Q
on the spectral gap of Q. As noted previously, the eleb and therefore Q,
b are not independent in our
ments of P,
setting, and therefore we cannot apply the standard matrix
concentration tools used in (Negahban et al., 2012). Our
proof makes use of the strong difference-boundedness and
b from Lemma 3 (details can be found
related properties of P
in the supplementary material). From Theorem 7, we immediately have the following sample complexity bound for
the rank centrality algorithm to exactly recover an optimal
permutation under the time-reversibility/BTL condition:
Corollary 8. Let (µ, P) be such that µmin > 0 and P satisfies the BTL condition, and ∃(i 6= j) : Pij 6= 21 . Let Q
be defined as in Eq. (6), and let π be the stationary probability vector of Q. Let Pmin , πmin and πmax be defined as
in Theorem 7, and let rmin = mini,j:πi 6=πj |πi − πj |. Let
δ ∈ (0, 1]. If


π
3  16n2 
9216 n
max
ln
, B(µmin ) ,
m ≥ max
2 P 2 µ2
rmin
δ
min min πmin
then with probability at least 1 − δ (over the random draw
b is constructed), the permuof S ∼ (µ, P)m from which P
tation σ
b output by the rank centrality algorithm satisfies
σ
b ∈ argminσ∈Sn erPD
µ,P [σ] .

A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data

4.2. Convergence of Least Squares Algorithm
Next, we analyze convergence of the least squares
(HodgeRank) algorithm (Jiang et al., 2011) (Algorithm 2) in our framework, again under the above timereversibility/BTL condition on the preference matrix P. As
discussed in (Jiang et al., 2011), the pairwise comparison
b is converted to a skew-symmetric matrix Y
b via a
matrix P
log-odds ratio transform before applying least squares (see
Algorithm 2). Similarly, given a pairwise preference matrix P ∈ [0, 1]n×n , we can define a skew-symmetric matrix
Y ∈ Rn×n as
( P 
if i 6= j and Pij ∈ (0, 1)
ln Pij
ji
(7)
Yij =
0
otherwise.

	
Let E = (i, j) ∈ X : Pij 6= 0 or Pji 6= 0 , and let f ∗ ∈
2
P
arg minf ∈Rn (i,j)∈E (fi − fj ) − Yij . Clearly, since
Pij = 1 − Pji ∀i 6= j, we have E = X . In this case,
as discussed in (Jiang et al., 2011), the (minimum norm)
solution to the above optimization problem is given by
fi∗

n
1X
Yik .
= −
n

(8)

k=1

The following lemma shows that if P satisfies the timereversibility/BTL condition, then ranking items according
to decreasing order of scores fi∗ as above yields an optimal
ranking w.r.t. the pairwise disagreement error:
Lemma 9. Let (µ, P) be such that P satisfies the BTL
condition. Let f ∗ ∈ Rn be defined as in Eq. (8). Then
argsort(f ∗ ) ⊆ argminσ∈Sn erPD
µ,P [σ].
The following is our main result regarding convergence of
the least squares algorithm. Note that it establishes convergence of the score vector b
f produced by the least squares
algorithm to f ∗ (in L∞ norm) under any P satisfying Pij ∈
(0, 1) ∀i 6= j; however the optimality of permutations obtained from f ∗ w.r.t. pairwise disagreement is guaranteed
only when P satisfies the time-reversibility/BTL condition.
Theorem 10. Let (µ, P) be such that µmin > 0 and Pij ∈
(0, 1) ∀i 6= j. Let Y ∈ Rn×n and f ∗ ∈ Rn be defined as
in Eqs. (7) and (8). Let Pmin = mini6=j Pij . Let 0 <  ≤ 1
and δ ∈ (0, 1]. If


128 
2 2  16n2 
m ≥ max
1+
ln
, B(µmin ) ,
2 µ2
Pmin

δ
min
then with probability at least 1 − δ (over the random draw
b is constructed), the score
of S ∼ (µ, P)m from which P
b
vector f produced by the least squares algorithm satisfies
kb
f − f ∗ k∞ ≤  .
This immediately yields the following sample complexity
bound for the least squares algorithm to exactly recover an
optimal permutation under the BTL condition:

Algorithm 2 Least Squares/HodgeRank (Jiang et al., 2011)
b ∈ [0, 1]n×n
Input: Empirical comparison matrix P
b
Construct empirical skew-symmetric matrix Y:
(  Pb 
if i 6= j and Pbij ∈ (0, 1)
ln Pbij
ji
Ybij =
0
otherwise.

	
b = (i, j) ∈ X : Pbij 6= 0 or Pbji 6= 0
Let E
2
P
Compute b
f ∈ argminf ∈Rn (i,j)∈Eb (fj − fi ) − Ybij
Output: Permutation σ
b ∈ argsort(b
f)
Corollary 11. Let (µ, P) be such that µmin > 0 and P
satisfies the BTL condition, and ∃(i 6= j) : Pij 6= 21 . Let
f ∗ be as in Eq. (8), and let rmin = mini,j:fi∗ 6=fj∗ |fi∗ − fj∗ |.
Let δ ∈ (0, 1]. If


128 
6 2  16n2 
m ≥ max
ln
,
B(µ
)
,
1+
min
2 µ2
Pmin
rmin
δ
min
then with probability at least 1 − δ (over the random draw
b is constructed), the permuof S ∼ (µ, P)m from which P
tation σ
b output by the least squares algorithm satisfies
σ
b ∈ argminσ∈Sn erPD
µ,P [σ] .

5. Low-Noise (LN) Condition
In this section we consider the following ‘low-noise’ condition on the preference matrix P, which is similar to the
condition studied by (Duchi et al., 2010) in a somewhat
different context; as the lemma below shows, the low-noise
condition includes the BTL condition as a special case.
Definition 12 (Low-noise (LN) condition). We say the
pairwise preference matrix P ∈ [0, 1]n×n satisfies the lownoise (LN) condition if
n
n
X
X
∀i 6= j : Pij > Pji =⇒
Pkj >
Pki .
k=1

k=1

n×n

Lemma 13. If P ∈ [0, 1]
satisfies the BTL condition,
then it also satisfies the LN condition.
For the rest of this section (Section 5), given a pairwise
preference matrix P ∈ [0, 1]n×n , define f ∗ ∈ Rn+ as
n
1X
fi∗ =
Pki .
(9)
n
k=1

Clearly, if P satisfies the LN condition, then any permutation that ranks items i ∈ [n] in descending order of scores
fi∗ as defined above is an optimal permutation w.r.t. the
pairwise disagreement error (see Eq. (1)).
5.1. Convergence of Borda Count Algorithm
b (the matrix version
Given a pairwise comparison matrix P,
of) the Borda count algorithm (Borda, 1781; Jiang et al.,
2011) (Algorithm 3) simply averages for each item i the

A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data

Algorithm 3 Borda Count (Borda, 1781; Jiang et al., 2011)
b ∈ [0, 1]n×n
Input: Empirical comparison matrix P
P
n
For i = 1 to n: fbi = n1 k=1 Pbki
Output: Permutation σ
b ∈ argsort(b
f)
fraction of times Pbki it has beat each other item k, and
ranks items by this score.2 Here we show this algorithm
converges to an optimal ranking under the LN condition.

Algorithm 4 BTL-ML Estimator
b ∈ [0, 1]n×n
Input: Empirical comparison matrix P
Find maximum likelihood estimate of BTL score vector:

X
b ∈ arg min
bij (θj − θi )
θ
ln(1
+
exp(θ
−
θ
))
−
P
j
i
n
θ∈R

i<j

For i = 1 to n: w
bi = exp(θbi )
b
Output: Permutation σ
b ∈ argsort(w)

The following result establishes convergence of the score
vector b
f produced by Borda count to f ∗ (in L∞ norm)
under general P; however the optimality of permutations
obtained from f ∗ w.r.t. the pairwise disagreement error is
guaranteed only for P satisfying the LN condition.
Theorem 14. Let (µ, P) be such that µmin > 0,√and let
f ∗ ∈ Rn+ be defined as in Eq. (10). Let 0 <  ≤ (4 2) and
δ ∈ (0, 1]. If


 4n2 
128
m ≥ max 2 2 ln
, B(µmin ) ,
 µmin
δ

Theorem 16. Let (µ, P) be such that µmin > 0 and P
satisfies the LN condition, and ∃(i 6= j) : Pij 6= 21 . Let f ∗
be as in Eq. (10), and let rmin = mini,j:fi∗ 6=fj∗ |fi∗ − fj∗ |.
Let δ ∈ (0, 1]. If


 4n2 
1152
m ≥ max
ln
,
B(µ
)
,
min
2 µ2
rmin
δ
min

then with probability at least 1 − δ (over the random draw
b is constructed), the score
of S ∼ (µ, P)m from which P
b
vector f produced by the Borda count algorithm satisfies
kb
f − f ∗ k∞ ≤  .

σ
b ∈ argminσ∈Sn erPD
µ,P [σ] .

This immediately yields the following sample complexity
bound for the Borda count algorithm to exactly recover an
optimal permutation under the LN condition:
Corollary 15. Let (µ, P) be such that µmin > 0 and P
satisfies the LN condition, and ∃(i 6= j) : Pij 6= 21 . Let f ∗
be as in Eq. (10), and let rmin = mini,j:fi∗ 6=fj∗ |fi∗ − fj∗ |.
Let δ ∈ (0, 1]. If


 4n2 
1152
m ≥ max
ln
,
B(µ
)
,
min
2 µ2
rmin
δ
min
then with probability at least 1 − δ (over the random draw
b is constructed), the permuof S ∼ (µ, P)m from which P
tation σ
b output by the Borda count algorithm satisfies
σ
b ∈ argminσ∈Sn erPD
µ,P [σ] .
5.2. Convergence of BTL-ML Estimator
b the BTL-ML esGiven a pairwise comparison matrix P,
timator (Algorithm 4) finds a maximum likelihood score
vector assuming a BTL model. Here we show this algorithm actually converges to an optimal permutation w.r.t.
pairwise disagreement under the more general LN condition; in fact we obtain the same sample complexity bound
for BTL-ML as for the Borda count algorithm above:
2

The standard Borda count algorithm ranks items by the number of times they beat other items; this algorithm converges to an
optimal ranking under a condition involving both µ and P. For
simplicity, we count here the fraction of times an item beats other
items, which allows us to restrict our attention to conditions on P.

then with probability at least 1 − δ (over the random draw
b is constructed), the permuof S ∼ (µ, P)m from which P
tation σ
b output by the BTL-ML algorithm satisfies

6. Generalized Low-Noise (GLN) Condition
In this section we consider a more general condition on the
preference matrix P that we term ‘generalized low-noise’:
Definition 17 (Generalized low-noise (GLN) condition).
We say the pairwise preference matrix P ∈ [0, 1]n×n satisfies the generalized low-noise (GLN) condition if ∃α ∈ Rn
such that
n
n
X
X
∀i 6= j : Pij > Pji =⇒
αk Pkj >
αk Pki .
k=1

k=1

Clearly, the LN condition of Section 5 is a special case with
αk = 1 ∀k ∈ [n]. Moreover, if P satisfies the GLN condition for some vector α, then any permutation
that ranks
Pn
items in decreasing order of scores fi = k=1 αk Pki is an
optimal permutation w.r.t. the pairwise disagreement error.
As our experiments will show, none of the four common
rank aggregation algorithms considered in Sections 4–5
above are guaranteed to converge to an optimal ranking under a general probability distribution satisfying the GLN
condition. Below we propose a new SVM-based rank aggregation algorithm which satisfies this property.
6.1. New Algorithm: SVM-RankAggregation
We will need the following definition:
Definition 18 (P-Induced Dataset). For any matrix P ∈
[0, 1]n×n , define the P-induced
dataset SP = {vij , zij }i<j

as consisting of the n2 vectors vij = (Pi − Pj ) ∈ Rn
(i < j), where Pi denotes the i-th column of P, together
with binary labels zij = sign(Pji − Pij ) ∈ {±1}.

A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data

b ∈ [0, 1]n×n .
Input: Empirical comparison matrix P
b
Construct P-induced
dataset S b (see Section 6.1)
P

If SP
b is linearly separable by hyperplane through origin,
then
b ∈ Rn
train hard-margin linear SVM on SP
b ; obtain α
else
train soft-margin linear SVM (with any suitable value
b ∈ Rn
for regularization parameter) on SP
b ; obtain α
Pn
For i = 1 to n: fbi = k=1 α
bk Pbki
Output: Permutation σ
b ∈ argsort(b
f)
Proposition 19. Let P ∈ [0, 1]n×n be a preference matrix
with Pij 6= 12 ∀i, j. Then P satisfies the GLN condition
if and only if the P-induced dataset SP = {vij , zij }i<j
is linearly separable by a hyperplane passing through the
origin, i.e. ∃α ∈ Rn s.t. zij α> vij > 0 ∀i < j.
While the above proposition guarantees that a preference
matrix P satisfying the GLN condition induces a linearly
separable dataset SP , in general, the dataset SP
b induced by
b constructed from a ranan empirical comparison matrix P
dom sample S ∼ (µ, P)m may not always be linearly separable. However, as we show in the proof of Theorem 20
below, for large enough m, with high probability, SP
b is also
linearly separable by a hyperplane passing through the origin. Our SVM-RankAggregation algorithm (Algorithm 5)
tests whether SP
b is linearly separable by such a classifier; if
so, it trains a hard-margin linear SVM classifier that yields
such a separating hyperplane through the origin.
6.2. Convergence of SVM-RankAggregation Algorithm
We now show that the SVM-RankAggregation algorithm
converges to an optimal ranking under the GLN condition, and give a sample complexity bound for SVMRankAggregation to exactly recover an optimal ranking:
Theorem 20. Let (µ, P) be such that µmin > 0 and P
satisfies the GLN condition for some vector α ∈ Rn , and
α
Pij 6= 12 ∀i, j. Let γ = mini,j |Pij − 12 |, and let rmin
=
mini,j

|αT (Pi −Pj )|
.
kαk2


m ≥ max

Let δ ∈ (0, 1). If
 16n3 
2048 n
log
,
α µ
2
(rmin
δ
min )

 8n2 
128
log
,
B(µ
)
,
min
γ 2 µ2min
δ

then with probability at least 1 − δ (over the random draw
b is constructed), the permuof S ∼ (µ, P)m from which P
tation σ
b output by SVM-RankAggregation satisfies
σ
b ∈ argminσ∈Sn erPD
µ,P [σ] .

7. Experiments
In this section we report results of experiments designed to
verify our convergence results and investigate the tightness
of the corresponding sample complexity bounds.
7.1. Convergence under BTL
Our first experiment was with BTL distributions for n =
5, 10, 20. For each n, we constructed P using a random
BTL vector w ∈ Rn+ (each component wi chosen uniformly at random from [0, 1]), taking µ to be the uniform
distribution over the n2 item pairs, and generated 100 random samples from (µ, P) for each of several sample sizes
m. We then ran the 5 algorithms analyzed in Sections 4-6
on the generated samples, and for each n and m, computed
the fraction of times an optimal permutation was recovered
by each algorithm. The results are shown in Figure 2; as
can be seen, for sufficiently large sample size, all 5 algorithms recover an optimal permutation with high probability. Similar results were obtained with non-uniform µ.
n=5

1

Optimal recovery fraction

Algorithm 5 SVM-RankAggregation

n = 10

n = 20

0.9
0.8
0.7
0.6
0.5

RC
LS
Borda
SVM
ML

0.4
0.3
0.2
0.1

2

10

3

10

4

10

5

10

6

10

7

10

RC
LS
Borda
SVM
ML

RC
LS
Borda
SVM
ML
8

10

2

10

3

10

4

10

5

10

6

10

7

10

8

10

2

10

3

10

4

10

5

10

6

10

7

10

8

10

Sample size (m)

Figure 2. Fraction of times an optimal ranking was recovered by
various algorithms under a BTL distribution (out of 100 random
runs), for increasing sample sizes m, and for different numbers of
items n (left to right: n = 5, 10, 20).

7.2. Convergence under LN
For our next experiment, we constructed a distribution that
satisfies the LN condition but not the BTL condition; the
preference matrix P we used (with n = 4) is shown below:


0
0.8 0.51 0.51
 0.2
0
0.9 0.7 
.
P=
 0.49 0.1
0
0.65 
0.49 0.3 0.35
0
In this case we used a random distribution µ over the item
pairs (specifically, n2 numbers uij were each chosen uniformly at random
P from [0, 1], and then normalized to yield
µij = uij / kl ukl ). Again, we generated 100 random
samples from (µ, P) for each of several sample sizes m,
ran the 5 algorithms on these samples, and in each case
computed the fraction of times an optimal permutation was
recovered. The results are shown in Figure 3 (left); as can
be seen, the rank centrality and least squares algorithms fail
to recover an optimal permutation under this distribution.

A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data
1

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
2
10

4

10

6

10

Sample size (m)

8

10

RC
LS
Borda
SVM
ML

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
2
10

4

10

6

10

8

10

10

35

10

30

10

25

10

20

10

15

10

10

10

5

10

7.3. Convergence under GLN
For our third experiment, we constructed a distribution that
satisfies the GLN condition but not the LN condition; the
preference matrix P we used (with n = 5) is shown below:


0
0.51 0.46 0.4 0.4
 0.49
0
0.49 0.4 0.4 


0.54
0.51
0
0.4 0.4 
P=
.

 0.6
0.6
0.6
0 0.4 
0.6
0.6
0.6 0.6 0
It can be verified that P satisfies the GLN condition with
α = (−0.4530, −2.6021, 1.4660, 3.0796, 3.7197)> ; howeverP
P does notPsatisfy the LN condition since P12 > P21
but k Pk1 > k Pk2 . Again we used a random distribution µ over the item pairs as above. The results are shown
in Figure 3 (right); here only the SVM-RankAggregation
algorithm successfully recovers an optimal ranking.
7.4. Tightness of Sample Complexity Bounds
Our final set of experiments was designed to evaluate the
tightness of our sample complexity bounds. We first used
BTL distributions generated similarly as described in Section 7.1 for various n between 5 and 20, and evaluated both
the actual number of samples required by each algorithm
to recover an optimal ranking at least 95% of the time, and
the corresponding upper bounds on sample complexity, as
a function of n. The results are shown in Figure 4 (left);
in most cases, the shapes of the upper bounds are largely
similar to those of the actual sample complexity curves.
Figure 4 (left) also suggests the upper bound for the rank
centrality algorithm is significantly looser than those for
other algorithms. This bound, which builds on techniques
3
of (Negahban et al., 2012), involves an additional ππmax
min
term not present in the other bounds. To investigate this,
we designed BTL
 distributions for n = 5 with increasing
values of ππmax
(keeping the rmin term corresponding to
min
the LN bounds in Section 5 fixed), and evaluated the sample

RC upper bound
LS upper bound
SVM upper bound
Borda upper bound
ML upper bound
RC
LS
SVM
Borda
ML

30

10

25

10

20

10

15

10

10

10

5

10

0

0

10

Sample size (m)

Figure 3. Left: Fraction of times an optimal ranking was recovered by various algorithms under a distribution satisfying the LN
but not the BTL condition (out of 100 random runs), for increasing sample sizes m (here n = 4). Right: Fraction of times an
optimal ranking was recovered by various algorithms under a distribution satisfying the GLN but not the LN condition (out of 100
random runs), for increasing sample sizes m (here n = 5).

Samples needed for 95% recovery

RC
LS
Borda
SVM
ML

0.8

RC upper bound
LS upper bound
SVM upper bound
Borda upper bound
ML upper bound
RC
LS
SVM
Borda
ML

40

0.9

Samples needed for 95% recovery

Optimal recovery fraction

Optimal recovery fraction

1
0.9

5

6

8

10

12

n

14

16

18

20

10

5

50

500

5000

πmax/πmin

Figure 4. Left: Sample size required for 95% probability of recovery of an optimal ranking by various algorithms under a BTL
distribution (out of 100 random runs), together with the corresponding sample complexity upper bounds, as a function of n.
Right: Sample size required for 95% probability of recovery of
an optimal ranking by various algorithms under a BTL distribution (out of 100 random runs), together with corresponding
sam
ple complexity upper bounds, as a function of ππmax
(keeping
min
rmin term corresponding to LN bounds constant) (here n = 5).

complexity and corresponding upper bounds as a function
of this ratio. The results are shown in Figure 4 (right). As
can be seen, the dependence of the rank centrality upper
bound on this term appears to be superfluous, and likely an
artefact of the current analysis technique, which is based on
that of (Negahban et al., 2012). In future work, we intend to
explore alternative techniques for obtaining a tighter bound
for the rank centrality algorithm.3 We also plan to explore
whether the additional factor of n in the rank centrality and
SVM-RankAggregation bounds can be removed.

8. Conclusion
The problem of rank aggregation from pairwise comparison data has received much interest recently. We have analyzed various algorithms for this problem, and have shown
that under a natural statistical model, where pairwise comparisons are drawn randomly and independently from some
underlying probability distribution, the rank centrality and
least squares algorithms converge to an optimal ranking under a BTL condition, while the Borda count and BTL-ML
algorithms converge to an optimal ranking under a more
general LN condition. However, none of these existing
algorithms converges under the more general GLN condition; we have proposed a new SVM-based rank aggregation
algorithm for which such convergence is guaranteed. Future work includes improving the analysis to obtain tighter
bounds, and extending the analysis to other algorithms.
Acknowledgments. Thanks to the anonymous reviewers
for helpful comments. AR is supported by a Microsoft
Research India PhD Fellowship. SA thanks DST and the
Indo-US Science & Technology Forum for their support.
3

We note that the least squares sample complexity also shows
a slight dependence on the ππmax
term; this is due to the fact that
min
this term is connected to Pmin , the dependence on which appears
to be captured correctly in our bound for least squares.

A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data

References
Ailon, Nir. Aggregation of partial rankings, p-ratings and
top-m lists. Algorithmica, 57(2):284–300, 2010.
Ailon, Nir. Active learning ranking from pairwise preferences with almost optimal query complexity. In Advances in Neural Information Processing Systems. 2011.
Ailon, Nir, Charikar, Moses, and Newman, Alantha. Aggregating inconsistent information: Ranking and clustering. Journal of the ACM, 55(5):23, 2008.
Borda, Jean-Charles de. Mémoire sur les élections au
scrutin. Histoire de l’Académie Royale des Sciences,
1781.

Jagabathula, Srikanth and Shah, Devavrat. Inferring rankings under constrained sensing. In Advances in Neural
Information Processing Systems, pp. 753–760, 2008.
Jamieson, Kevin G. and Nowak, Rob. Active ranking using
pairwise comparisons. In Advances in Neural Information Processing Systems 24, pp. 2240–2248. 2011.
Jiang, Xiaoye, Lim, Lek-Heng, Yao, Yuan, and Ye, Yinyu.
Statistical ranking and combinatorial Hodge theory.
Mathematical Programming, 127(1):203–244, 2011.
Joachims, T. Optimizing search engines using clickthrough
data. In Proceedings of the 8th ACM Conference on
Knowledge Discovery and Data Mining, 2002.

Condorcet, Nicolas de. Essai sur l’application de l’analyse
á la probabilité des décisions rendues á la pluralité des
voix. Imprimerie Royale, Paris, 1785.

Klementiev, Alexandre, Roth, Dan, and Small, Kevin. Unsupervised rank aggregation with distance-based models. In Proceedings of the 25th International Conference
on Machine Learning, 2008.

Cossock, David and Zhang, Tong. Statistical analysis of
Bayes optimal subset ranking. IEEE Transactions on Information Theory, 54(11):5140–5154, 2008.

Kutin, Samuel. Extensions to McDiarmid’s inequality
when differences are bounded with high probability.
Technical report, University of Chicago, 2002.

Duchi, John, Mackey, Lester, and Jordan, Michael I. On
the consistency of ranking algorithms. In Proceedings of
the 27th International Conference on Machine Learning,
2010.

Liu, Tie-Yan. Learning to Rank for Information Retrieval.
Springer, 2011.

Dwork, Cynthia, Kumar, Ravi, Naor, Moni, and Sivakumar, D. Rank aggregation methods for the web. In Proceedings of the 10th International World Wide Web Conference, 2001.
Freund, Yoav, Iyer, Raj, Schapire, Robert E., and Singer,
Yoram. An efficient boosting algorithm for combining
preferences. Journal of Machine Learning Research, 4:
933–969, 2003.
Gleich, David F. and Lim, Lek-Heng. Rank aggregation via
nuclear norm minimization. In Proceedings of the 17th
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, 2011.
Guiver, John and Snelson, Edward. Bayesian inference
for Plackett-Luce ranking models. In Proceedings of
the 26th International Conference on Machine Learning,
2009.

Meila, Marina, Phadnis, Kapil, Patterson, Arthur, and
Bilmes, Jeff A. Consensus ranking under the exponential model. In Proceedings of the 23rd Conference on
Uncertainty in Artificial Intelligence, 2007.
Negahban, Sahand, Oh, Sewoong, and Shah, Devavrat.
Iterative ranking from pair-wise comparisons. In Advances in Neural Information Processing Systems, 2012.
Osting, Braxton, Brune, Christoph, and Osher, Stanley. Enhanced statistical rankings via targeted data collection.
In Proceedings of the 30th International Conference on
Machine Learning, 2013.
Qin, Tao, Geng, Xiubo, and Liu, Tie-Yan. A new probabilistic model for rank aggregation. In Advances in Neural Information Processing Systems, 2010.
Soufiani, Hossein Azari, Parkes, David C., and Xia, Lirong.
Random utility theory for social choice. In Advances in
Neural Information Processing Systems, 2012.

Herbrich, R., Graepel, T., and Obermayer, K. Large margin rank boundaries for ordinal regression. Advances in
Large Margin Classifiers, pp. 115–132, 2000.

Volkovs, Maksims N. and Zemel, Richard S. A flexible
generative model for preference aggregation. In Proceedings of the 21st International World Wide Web Conference, 2012.

Hochbaum, Dorit S. Ranking sports teams and the inverse
equal paths problem. In Proceedings of the 2nd International Workshop on Internet and Network Economics,
2006.

Wauthier, Fabian, Jordan, Michael, and Jojic, Nebojsa. Efficient ranking from pairwise comparisons. In Proceedings of the 30th International Conference on Machine
Learning, 2013.

