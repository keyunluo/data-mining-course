Support Matrix Machines

Luo Luo
RICKY @ SJTU . EDU . CN
Yubo Xie
YUBOTSE @ SJTU . EDU . CN
Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China
Zhihua Zhang
ZHIHUA @ SJTU . EDU . CN
Institute of Data Science, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China
Wu-Jun Li
LIWUJUN @ NJU . EDU . CN
National Key Laboratory for Novel Software Technology, Collaborative Innovation Center of Novel Software Technology
and Industrialization, Department of Computer Science and Technology, Nanjing University, China

Abstract
In many classification problems such as electroencephalogram (EEG) classification and image classification, the input features are naturally
represented as matrices rather than vectors or scalars. In general, the structure information of
the original feature matrix is useful and informative for data analysis tasks such as classification.
One typical structure information is the correlation between columns or rows in the feature matrix. To leverage this kind of structure information, we propose a new classification method that
we call support matrix machine (SMM). Specifically, SMM is defined as a hinge loss plus a
so-called spectral elastic net penalty which is a
spectral extension of the conventional elastic net
over a matrix. The spectral elastic net enjoys a
property of grouping effect, i.e., strongly correlated columns or rows tend to be selected altogether or not. Since the optimization problem
for SMM is convex, this encourages us to devise an alternating direction method of multipliers (ADMM) algorithm for solving the problem.
Experimental results on EEG and image classification data show that our model is more robust
and efficient than the state-of-the-art methods.

1. Introduction
Classical classification methods such as support vector machines (SVMs) (Cortes & Vapnik, 1995) and logistic regression (Hastie et al., 2001) have been originally built on
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

the case that input samples are represented as vectors or scalars. However, it is also often met that input samples are
naturally represented as two-dimensional matrices or tensors. When using classical classification methods to data of
matrix representation, we have to reshape the input matrices into vectors. However, this would destroy the structure
information of the data matrix, e.g., the correlation of different channels of electroencephalogram (EEG) data (Zhou
& Li, 2014) and the spatial relationship of the nearby pixels
of image data (Wolf et al., 2007). Moreover, if the data matrix is stacked (reshaped) into a vector, the dimensionality
of the resulting vector typically becomes very high, which
in turn leads to the curse of dimensionality.
There has been some work on classification methods which
attempt to exploit the correlation between the columns or
rows of the data matrix. Usually, such a classification
method introduces a matrix of regression coefficients to
leverage the correlation within the data matrix. For example, Wolf et al. (2007) proposed a rank-k SVM, which
models the regression matrix as the sum of k rank-one orthogonal matrices. Pirsiavash et al. (2009) devised a bilinear SVM by factorizing the regression matrix into two lowrank matrices. Cai et al. (2006) proposed a similar bilinear
framework called support tensor machines for text categorization. These methods essentially take advantage of the
low-rank assumption, which can be used for describing the
correlation within a matrix. However, their treatments result in non-convex optimization problems.
In this paper we are also concerned with the classification
problems on a set of data matrices. Our work is motivated by the use of the nuclear norm (a.k.a., the trace norm) in low-rank matrix approximation (Srebro & Shraibman, 2005), matrix completion (Candès & Recht, 2009; Liu et al., 2013; Salakhutdinov & Srebro, 2010; Huang et al.,
2013), and multi-task learning problems (Pong et al., 2010;

Support Matrix Machines

Harchaoui et al., 2012; Kang et al., 2011). The cornerstone
of these methods is to use the nuclear norm of a matrix as
a convex alternative of the matrix rank. Since the nuclear
norm is the best convex approximation of the matrix rank
over the unit ball of matrices, this makes it more tractable to
solve the resulting optimization problem. Moreover, some
nice properties such as the consistency results of the nuclear norm minimization have been studied by Bach (2008).
There has been some work which applies the nuclear norm
penalization with least square loss function to matrix regression problems (Hunyadi et al., 2012; Signoretto et al.,
2014). Recently, Zhou & Li (2014) applied the nuclear norm penalization to matrix regression problems based on generalized linear models (GLMs).
In this paper we propose a new model to address the matrix
classification problem. Our model includes two principal
ingredients. First, we consider the hinge loss due to its
widely deployed ability in sparseness and robustness modeling. Second, we employ a spectral elastic net penalty for
the regression matrix. The spectral elastic net is a spectral extension of the conventional elastic net over a matrix.
In parallel to the conventional elastic net (Zou & Hastie,
2005) which is the combination of the ridge penalty and
lasso penalty, our spectral elastic net is the combination of
the squared Frobenius matrix norm and nuclear norm. We
prove that the spectral elastic net enjoys the property of
grouping effect which is similar to the conventional elastic
net, while keeping a low-rank representation. We show that
the regression matrix in our model is indeed a combination
of a set of support matrices. We thus refer to our model as
a support matrix machine (SMM).
The optimization problem for SMM is convex but the hinge
loss function is not smooth. Fortunately, we can resort to
an alternating direction method of multipliers (ADMM) studied in (Goldstein et al., 2012). Specifically, we develop
an iteration algorithm, which is mainly built on ADMM
and a singular value thresholding (SVT) operator (Candès
& Recht, 2009; Cai et al., 2010). The algorithm converges
quickly and promises to get the global optimal solution. It
is worth pointing out that the algorithm requires repeatedly computing singular value decomposition (SVD) of matrices with the same size as the matrix of input features.
However, when represented as a matrix, the size of an input matrix is usually not too large.
Finally, we apply our SMM to EEG and image classification problems. We see that classification methods directly working on data matrices outperform those on vectors
such as the conventional SVM. When data are contaminated with non-Gaussian noise or outliers, our SMM has significant improvements over baselines. This implies that SMM is robust and has potential applications in matrix classification problems with noises. Moreover, the experiments

show that our proposed training algorithm for SMM is efficient.
The remainder of the paper is organized as follows. In Section 2, we give the notation and preliminaries. In Section 3,
we review our concerned problem. In Section 4 we present
our model and the learning algorithm. In Section 5, we conduct experimental analysis to justify our methods. Finally,
we conclude our work in Section 6.

2. Notation and Preliminaries
In this section we give the notation and preliminaries which
will be used in this paper. We let Ip denote the p×p identity
p
matrix. For a p
vector
is denotPp a ∈2 R , the Euclidean norm
p×q
.
For
a
matrix
A
∈
R
of rank
a
ed as ||a|| =
i=1 i
r where r ≤ min(p, q), we let the condensed singular valT
ue decomposition (SVD) of A be A = UA ΣA VA
where
p×r
q×r
T
UA ∈ R
and VA ∈ R
satisfy UA UA = Ir and
T
VA = Ir , and ΣA = diag(σ1 (A), · · · , σr (A)) with
VA
σ1 (A) ≥ · · · ≥ σr (A) > 0. Obviously, the rank of A is
equal to the number of nonzero
of A. AdqPsingular values
pPr
2
2
ditionally, we let kAkF =
i,j Aij =
i=1 σi (A)
Pr
be the Frobenius norm, kAk∗ = i=1 σi (A) be the nuclear norm, and kAk2 = σ1 (A) be the spectral norm.
T
where
For any τ > 0, we let Dτ [A] = UA Sτ [ΣA ]VA
Sτ [Σ] = diag([σ1 (A) − τ ]+ , · · · , [σr (A) − τ ]+ ) and
[z]+ = max(z, 0). In the literature (Candès & Recht, 2009;
Cai et al., 2010), Dτ [A] is called the singular value thresholding (SVT) operator.

It is well known that the nuclear norm kAk∗ , as a function
from Rp×q to R, is not differentiable. Alternatively, one
considers the subdifferential of kAk∗ , which is the set of
subgradients and denoted by ∂kAk∗ . It follows from the
literature (Candès & Recht, 2009; Lewis, 2003; Watson,
1992) that for a p × q real matrix A of rank r,
n
T
∂kAk∗ = UA VA
+ Z : Z ∈ Rp×q , UTA Z = 0,
o
ZVA = 0, kZk2 ≤ 1 . (1)

3. Problem Formulation and Related Work
In this paper we study a regularized matrix
classifier. We

are given a set of training samples T = Xi , yi }ni=1 , where
Xi ∈ Rp×q is the ith input sample and yi ∈ {−1, 1} is its
corresponding class label. As we have seen, Xi is represented in matrix form. To fit a classifier, a commonly used
approach is to stack Xi into a vector. Let xi , vec(XTi ) =
([Xi ]11 , . . . , [Xi ]1q , [Xi ]21 , . . . , [Xi ]pq )T ∈ Rpq . The soft
margin SVM is defined as
n

min
w,b

X
1 T
w w+C
[1 − yi (wT xi + b)]+ ,
2
i=1

(2)

Support Matrix Machines

where [1−u]+ is called the hinge loss function, w ∈ Rpq is
a vector of regression coefficients, b ∈ R is an offset term,
and C is a regularization parameter.
When reshaped into vector vec(XTi ), the correlation among columns or rows in the matrix is ignored. However, it would be more reasonable to exploit the correlation
information in developing a classifier, because the correlation is helpful and useful in improving the classification
performance.
Intuitively, we consider the following formulation:
n

min
W,b

X
1
tr(WT W)+C
{1−yi [tr(WT Xi )+b]}+ , (3)
2
i=1

where W ∈ Rp×q is the matrix of regression coefficients. However, this formulation is essentially equivalent to Problem (2) when w = vec(WT ), because tr(WT Xi ) = vec(WT )T vec(XTi ) = wT xi and
tr(WT W) = vec(WT )T vec(WT ) = wT w. This implies that the formulation in (3) cannot directly address our
concern.
To capture the correlation, a natural approach is to consider
the dependency of the regression matrix W. In particular,
one can impose a low-rank constraint on W to leverage
the structure information within Xi . For example, in the
bilinear SVM (Pirsiavash et al., 2009), the authors assumed
that W = Wy WxT where Wx ∈ Rq×d , Wy ∈ Rp×d and
d < min(p, q). Accordingly, they defined the following
problem without the offset term
1
tr(Wx WyT Wy WxT )
2

argmin
Wx ,Wy

+C

n
X
[1 − yi tr(WyT Xi Wx )]+ .

(4)

i=1

However, the resulting problem is nonconvex in both Wx
and Wy . Thus, the authors resorted to an alternately iterative update scheme for Wx and Wy ; that is, they updated
either of Wx and Wy while keeping the other fixed.
Since the dependency of W can be revealed by its rank
rank(W), it is also natural to directly impose the rank constraint to W. However, the resulting matrix rank minimization is usually NP-hard (Vandenberghe & Boyd, 1996).
Zhou & Li (2014) suggested a function of the singular values of W as an alternative penalization technique. Based
on this idea, they proposed a regularized GLM (R-GLM):
argmin

J(W) + P (W),

(5)

W

where J(W) is a loss function obtained from the negative
log-likelihood and P (W) is a penalty function defined on

the singular values of W. Typically, P (W) = λkWk∗ for
λ > 0 because the nuclear norm kWk∗ is the best convex
approximation of rank(W) over the unit ball of matrices.
Assuming that the loss function J is smooth and its derivative has the Lipschitz continuity, Zhou & Li (2014) devised
the Nesterov method (Nesterov, 1983) to solve (5).

4. The Support Matrix Machine
It is well known that the hinge loss enjoys the large margin
principle. Moreover, it embodies sparseness and robustness, which are two desirable properties for a good classifier. This thus motivates us to employ the hinge loss function
in (5) instead. In particular, we present the following formulation:
argmin
W,b

1
tr(WT W) + τ ||W||∗
2
n
X
+C
{1−yi [tr(WT Xi )+b]}+ ,

(6)

i=1

which defines a matrix classification model that we call the support matrix machine (SMM). Recall that the hinge
loss is not smooth, so our model is not a trivial variant of
the regularized GLM. On the other hand, SMM is in fact
based on a penalty function, which is the combination of
the squared Frobenius norm kWk2F and the nuclear norm kWk∗ . We call this penalty the spectral elastic net
Pmin(p,q) 2
because tr(WT W) = kWk2F =
σi (W) and
i=1
Pmin(q,p)
kWk∗ = i=1
σi (W). As we see, the spectral elastic net is parallel to the elastic net of Zou & Hastie (2005).
Again recall that tr(WT W) = vec(WT )T vec(WT ) and
tr(WT Xi ) = vec(WT )T vec(XTi ), so SMM degenerates
to the conventional linear SVM when τ = 0. However, the
nuclear norm can not be equivalently defined as a vector
norm. This implies that we cannot formulate Problem (6)
in an equivalent of the vector form. Thus, our SMM is able
to capture the correlation within the input data matrix.
4.1. Theoretical Justification
We now show that SMM possesses some elegant benefits
from the conventional SVM (Cortes & Vapnik, 1995) as
well as the conventional elastic net (Zou & Hastie, 2005).
Without loss of generality, we suppose that each feature of
the training data is normalized to have unit length. That is,
it holds that kfkl k = 1 where fkl , ([X1 ]kl , . . . , [Xn ]kl )T
for k = 1, . . . , p and l = 1, . . . , q.
Theorem 1. Suppose the minimizer of Problem (6) is
(W̃, b̃). Then
W̃ = Dτ

n
X
i=1


β̃i yi Xi ,

Support Matrix Machines

where 0 ≤ β̃i ≤ C.
Pn
Denote Ω = i=1 β̃i yi Xi . We see that Ω is the combination of those Xi associated with nonzero β̃i , while W̃ is
the SVT of Ω. In fact, we will see from Eqn. (12) in Theorem 5 that W̃ is indeed the linear combination of a set of
support matrices {Xi }.
Lemma 1. The difference between [Ω]k1 l1 and [Ω]k2 l2
meets the following inequality
([Ω]k1 l1 − [Ω]k2 l2 )2 ≤ 2nC 2 (1 − fkT1 l1 fk2 l2 ).
Recall that kfk1 l1 k = 1 and kfk2 l2 k = 1, so fkT1 l1 fk2 l2 ∈
[−1, 1] is the correlation coefficient of input features at positions (k1 , l1 ) and (k2 , l2 ) over the n training samples.
Lemma 1 says that Ω has the element-wise grouping effect. Specifically, higher (lower) correlation between two
elements of Ω leads to smaller (larger) difference. Based
on this lemma, we also have the following theorem.
Theorem 2. Let [W̃]:,l be the lth column of W̃. Then
p


X


T
[W̃]:,l1 − [W̃]:,l2 2 ≤ 2nC 2 p −
fkl
f
.
kl
2
1
k=1

and G(S) = τ ||S||∗ .
ADMM solves (7) by using the augmented Lagrangian
function:
L1 (W, b, S, Λ) =H(W, b) + G(S) + tr[ΛT (S − W)]
ρ
+ ||S − W||2F ,
2
where ρ > 0 is a hyperparameter.
The ADMM learning procedure for our SMM is summarized in Algorithm 1. The key steps of Algorithm 1 are the
c (k) , b(k) ), the derivation of
computations of S(k) and (W
which is based on Theorems 3 and 4 below.
Theorem 3. For positive numbers τ and ρ, let the matrix
ρW − Λ have SVD of the form:
ρW − Λ = U0 Σ0 V0T + U1 Σ1 V1T ,

where Σ0 is the diagonal matrix whose diagonal entries
are greater than τ , U0 and V0 are matrices of the corresponding left and right singular vectors; Σ1 , U1 and V1
correspond the rest parts of the SVD whose singular values
are less than or equal to τ . Define

Especially, if fkl1 = fkl2 for any k = 1, . . . , p, then
[W̃]:,l1 = [W̃]:,l2 .
Theorem 2 reflects the relationship of W̃ with the training input matrices Xi . Interestingly, the columns of the
regression matrix W̃ have grouping effect in our model
if the corresponding features have strong correlation. The
similar conclusion also applies to the rows of W̃. Note
that Theorem 2 can not be extended to the element-wise
case, because even if fk1 l1 = fk2 l2 , we cannot obtain that
[W̃]k1 l1 = [W̃]k2 l2 . We will present an empirical illustration for the grouping effect problem in Section 5.1.
4.2. Learning Algorithm
The Nesterov method for R-GLM (Zhou & Li, 2014) requires the derivative of the loss function in question to be
Lipschitz-continuous. However, both the hinge loss and the
nuclear norm are not smooth. Thus, it is hard to develop the
Nesterov method for finding the SMM solution. Since the
objective function of SMM is convex in both W and b, we
here derive a learning algorithm based on ADMM with the
restart rule (Goldstein et al., 2012). The problem in (6) can
be equivalently written as follows:
argmin

H(W, b) + G(S),

(7)

S∗ ,

1
1
Dτ (ρW − Λ) = U0 (Σ0 − τ I)V0T .
ρ
ρ

S − W = 0,

Since G1 is convex with respect to S, we obtain the update
equation of S from Theorem 3. That is,
b (k) ) = 1 Dτ (ρW(k) −Λ
b (k) ).
S(k+1) = argmin G(W(k) , S, Λ
ρ
S
Theorem 4. One of the solution of the following problem
ρ
argmin H(W, b) − tr(ΛT W) + ||W−S||2F
2
(W,b)
is
W∗

=

b∗

=

n

X
1 
Λ + ρS +
αi∗ yi Xi ,
ρ+1
i=1
o
1 Xn
∗ T
y
−
tr[(W
)
X
]
,
i
i
|S ∗ |
∗
i∈S

where S = {i : 0 < αi∗ < C}, and α∗ ∈ Rn is the solution of the following box constraint quadratic programming
problem:
argmax

H(W, b) =

s.t.
1
tr(WT W) + C
2

n
X
i=1

{1−yi [tr(WT Xi )+b]}+

(10)

∗

α

where

(9)

Then we have 0 ∈ ∂ G1 (S∗ ), where G1 (S) = G(S) +
ρ
tr(ΛT S) + ||W − S||2F .
2

W,b,S

s.t.

(8)

1
− αT Kα + qT α,
2
0 ≤ α ≤ C1n ,
n
X
αi yi = 0.
i=1

(11)

Support Matrix Machines

5. Experiments

Algorithm 1 ADMM for SMM
b (0) ∈ Rp×q , Λ(−1) = Λ
b (0) ∈ Rp×q , ρ >
Initialize S(−1) = S
0, t(1) = 1, η ∈ (0, 1).
for k = 1, 2, 3 . . . do
b (k)T W) +
(W(k) , b(k) ) = argmin H(W, b) − tr(Λ
(W,b)

ρ
b (k) ||2F
||W−S
2
b (k)T S) + ρ ||W(k) − S||2F
S(k) = argmin G(S) + tr(Λ
2
S
b (k) − ρ(W(k) − S(k) )
Λ(k) = Λ
b (k) ||2F + ρ||S(k) − S
b (k) ||2F
c(k) = ρ−1 ||Λ(k) − Λ
if c(k) < ηc(k−1)
then
√
(k) 2

t(k+1) = 1+ 1+4t
2
(k)
−1
b (k+1) = S(k) + t(k)
− S(k−1) )
S
(k+1) (S
b (k+1) = Λ(k) +
Λ
else
t(k+1) = 1
b (k+1) = S(k−1)
S
b (k+1) = Λ(k−1)
Λ
c(k) = η −1 c(k−1)
end if
end for

t
t(k) −1
(Λ(k)
t(k+1)

− Λ(k−1) )

=

qi

=

5.1. Group Effect Property on Synthetic Data
To intuitively demonstrate the grouping effect property described in Theorem 2, we design a simulated experiment to visualize it. We generate a synthetic data set of n
samples as follows. First, we generate V orthogonal ndimensional basis vectors ν1 , ν2 , · · · , νV with the unit Euclidean length, respectively. Second, we construct pq feature vectors of length n by the following process:
f̃kl
kl

Here K = [Kij ] ∈ Rn×n and q ∈ Rn are independent of
α; specifically,
Kij

In this section we conduct the experimental analysis of our
proposed SMM 1 . We first analyze the group effect property of SMM. Then we study the classification performance
on synthetic and real-world data sets. All experiments are
implemented in Matlab R2011b on a workstation with Intel
Xeon CPU X5675 3.06GHz (2 × 12 cores), 24GB RAM,
and 64bit Windows Server 2008 system.

yi yj tr(XTi Xj )
,
ρ+1
yi tr[(Λ + ρS)T Xi ]
.
1−
ρ+1

By Theorem 4, updating W(k) and b(k) can be done by
solving Problem (11). Several methods can be used, such
as the sequential minimization optimization algorithm.
(Platt et al., 1998; Keerthi & Gilbert, 2002)
Theorem 5. Suppose the optimal solution of Problem (7)
is (W̃, b̃, S̃). Then
X
W̃ = S̃ = Λ̃ +
α̃i yi Xi .
(12)
α̃i >0

Theorem 5 can be obtained directly by Algorithm 1 through
Eqn. (10). If α̃i > 0, we call the corresponding Xi a support matrix. Theorem 5 shows that the solution W̃ of our
SMM can be written as the linear combination of support
matrices plus an offset. This is the reason that we call our
model the support matrix machine.
Since the hinge loss and nuclear norm are weakly convex,
the convergence property of Algorithm 1 can be proved immediately based on the result in (Goldstein et al., 2012; He
& Yuan, 2012). That is, we have
Theorem 6. For any ρ > 0 and η ∈ (0, 1), the iteration
sequence given by Algorithm 1 converges to the optimal
solution of Problem (7).

=
i.i.d

∼

νdl/(0.2q)e + kl ,
N (0, δ 2 In ),

for k = 1, . . . , p, and l = 1, . . . , q. Here dl/(0.2q)e denotes the smallest integer no smaller than l/(0.2q). In
other words, the elements in each sample matrix Xi (i =
1, . . . , n) can be roughly partitioned into four groups (vertical blocks). The features within the same group have high
correlation, while the features between different groups
have low correlation. Then we generate a p × q matrix W
of rank 0.2q, and the label for each sample is generated by
yi = sign[tr(WT Xi )]. We set n = 1000, V = 4, p = 80,
q = 100 and δ = 10−3 in this simulation.
The values of the regression matrix obtained respectively
from the bilinear SVM (B-SVM) (Pirsiavash et al., 2009),
regularized GLM (R-GLM) (Zhou & Li, 2014) and SMM
are shown in Figure 1. It is easy to see that the regression
matrix of SMM is clearly partitioned into four pure color
blocks, while the blocks of B-SVM have higher noise. RGLM fails to obtain the groups structure totally. The simulation results show that SMM has a better grouping effect
property than other baselines, which also implies that SMM is able to capture the structure information in the feature
matrices.
5.2. Classification Accuracy on Synthetic Data
We now conduct the performance of SMM on synthetic data sets. We use the same data generation process as in the
previous subsection, but we set V = 10 and δ = 10−2 to
obtain more complicated data. We use 1000 samples for
training, and other 500 samples for testing. All the hyperparameters involved are selected via cross validation.
1
The code is available in http://bcmi.sjtu.edu.cn/
luoluo/code/smm.zip
˜

Support Matrix Machines
8
0.01
10

10

6

0.005

10

0.005
20

20

4

0
30

30

2

−0.005
40

40

50

−0.015

60
70

−0.005

30

−0.01

40

−0.01

0
50

−0.015

50
−2

−0.02

60

−0.025

70

−0.03

80

−0.02

60
−4

−0.025

70
−6

80
20

40

60

80

100

(a) B-SVM

−0.03

80
20

40

60

80

Table 1. Summary of four data sets

0

20

100

20

(b) R-GLM

40

60

80

100

(c) SMM

Data sets
EEG alcoholism
EEG emotion
students face
INRIA person

#positive
77
1286
200
607

#negative
45
1334
200
1214

dimension
256×64
31×10
200×200
160×96

Figure 1. (a), (b) and (c) display the values of normalized regression matrix of B-SVM, R-GLM and SMM respectively.
0.9
B−SVM
R−GLM
SMM

0.85
0.8

accuracy

0.75
0.7
0.65
0.6
0.55
0.5
0.45

0

0.1

0.2
0.3
0.4
0.5
0.6
0.7
0.8
standard deviation of Gaussian noise

0.9

1

(a) Synthetic data with Gaussian noise
0.9
B−SVM
R−GLM
SMM

0.85

accuracy

0.8

The EEG alcoholism data set2 arises to examine EEG correlates of genetic predisposition to alcoholism. It contains
two groups of subjects: alcoholic and control. For each
subject, 64 channels of electrodes are placed and the voltage values are recorded at 256 time points.
The EEG emotion data set (Zhu et al., 2014; Zheng et al.,
2014) focuses on EEG emotion analysis, which is obtained
by showing some positive and negative emotional movie
clips to persons and then recording the EEG signal via ESI
NeuroScan System from 31 pairs. Each pair contain 10
data points (two channels for one pair, and each channel
contains five frequency bands). There are 2620 movie clips
chosen to evoke the target emotion, such as Titanic, Kung
Fu Panda and so on.

0.75

The student face data set contains 400 photos of Stanford
University medical students (Nazir et al., 2010), which consists of 200 males and 200 females. Each sample is a
200 × 200 gray level image.

0.7

0.65

0.6

0.55

0

0.005

0.01
0.015
0.02
0.025
density of salt and pepper noise

0.03

0.035

(b) Synthetic data with salt and pepper noise
Figure 2. Classification accuracy on synthetic data with different
levels of noises. We use Gaussian noise with 0 mean and standard
derivation from 0.01 to 1 in (a), and salt and pepper noise with
density from 0.001 to 0.035 in (b).

We add different levels of Gaussian noise and salt and pepper noise on the test data, and repeat this procedure ten
times to compute the mean and standard deviation of classification accuracy. The results are shown in Figure 2. It
is clear that all methods achieve comparable performance
on clean data, but SMM is more robust with respect to high
level of noises.
5.3. Classification Accuracy on Real-World Data
We apply SMM to EEG and image classification problems, and compare its performance with B-SVM (Pirsiavash
et al., 2009), R-GLM (Zhou & Li, 2014), and the standard
linear SVM (L-SVM) (Cortes & Vapnik, 1995). We use
four real-world matrix classification data sets: the EEG alcoholism, the EEG emotion, the students face and INRIA
person.

The INRIA person data set3 was collected to detect whether
there exist people in the image. We normalize the samples
into 160×96 gray images and remove the same person with
different aspects. Combining with the negative samples, we
obtain 1821 samples in total.
We summarize the main information of these data sets in
Table 1. For the student face and INRIA person data sets, we directly use the pixels as input features without any
advanced visual features.
For each of the compared methods, we randomly sample 70% of the data set for training and the rest for testing. All the hyperparameters involved are selected via cross validation. More specifically, we select C from
{1 × 10−3 , 2 × 10−3 , 5 × 10−3 , 1 × 10−2 , 2 × 10−2 , 5 ×
10−2 . . . , 1 × 103 , 2 × 103 }. For each C, we tune τ manually to make the rank of classifier matrix varied from 1 to
the size of the matrix. We repeat this procedure ten times to
compute the mean and standard deviation of the classification accuracy. Table 2 shows the classification accuracy of
the four methods. We can see that SMM achieves the best
performance on all the four data sets.
2

http://kdd.ics.uci.edu/databases/eeg/
eeg.html
3
http://pascal.inrialpes.fr/data/human/

Support Matrix Machines
Table 2. The classification accuracy on four data sets (in %)

Data sets
EEG alcoholism
EEG emotion
students face
INRIA person

L-SVM
71.11 (± 8.30)
88.76 (± 1.16)
91.67 (± 1.57)
84.88 (± 1.98)

B-SVM
71.67 (± 7.83)
87.73 (± 1.18)
95.42 (± 1.72)
85.09 (± 1.46)

R-GLM
71.39 (± 6.55)
82.26 (± 1.65)
94.25 (± 2.76)
84.65 (± 1.38)

SMM
73.33 (± 5.89)
90.01 (± 0.98)
96.83 (± 1.66)
85.95 (± 0.77)

Table 3. The training time on the four data sets (in second)

Data sets
EEG alcoholism
EEG emotion
students face
INRIA person

B-SVM
86.30 (± 163.73)
292.89 (± 248.47)
23.88 (± 10.53)
19.36 (± 9.23)

We are also interested in the computational efficiency of
the three matrix classification models: B-SVM, R-GLM
and SMM. We report the training time on the four data
sets in Table 3. Recall that R-GLM is solved by the Nesterov method (Zhou & Li, 2014). We can find that R-GLM
is the slowest method on EEG alcoholism, students face
and INRIA person. This is because the main step of the
Nesterov method is affected by the dimension of the input sample (Zhou & Li, 2014). However, the main step
of B-SVM and SMM is a quadratic programming problem
whose time complexity is mainly affected by the number of
training samples. So B-SVM and SMM are more efficient
than R-GLM on the data sets with high-dimension samples.
Furthermore, we find that the running time of B-SVM are
unstable on different data sets, usually with higher variance than that of SMM. The reason might be that B-SVM
is a non-convex problem, the training procedure of which
relies heavily on the initial value of the parameter.

R-GLM
407.59 (± 100.93)
33.32 (± 3.38)
121.14 (± 87.40)
580.06 (± 229.14)

SMM
1.36 (± 0.09)
6.57 (± 6.73)
7.20 (± 0.22)
6.61 (± 2.44)

61472182) and the Fundamental Research Funds for the
Central Universities (No. 20620140510).

Appendix A: The Proof of Lemma 1
Proof. Let β̃ 0 = [β̃1 y1 , . . . , β̃n yn ]T , then we have
([Ω]k1 l1 − [Ω]k2 l2 )2
=

n
X

β̃i yi [Xi ]k1 l1 −

i=1

n
X

β̃i yi [Xi ]k2 l2

2

i=1

=

[β̃ 0T (fk1 l1 − fk2 l2 )]2

≤

||β̃ 0 ||2 ||fk1 l1 − fk2 l2 ||2

≤ 2nC 2 (1 − fkT1 l1 fk2 l2 ).

Appendix B: The Proof of Theorem 2
6. Conclusion
In this paper we have proposed a novel matrix classification method called support matrix machine (SMM). SMM
can leverage the structure of the data matrices and has the
grouping effect property. We have derived an iteration algorithm based on ADMM for learning, and applied our
method to EEG and image classification with better performance than the baselines such as B-SVM and R-GLM.
Specifically, our method is more robust than B-SVM and
R-GLM to model noisy data. Furthermore, our method is
more efficient than B-SVM and R-GLM, and more numerically stable than B-SVM.

Proof. Suppose Ω has condensed SVD Ω = UΣVT ,
where U ∈ Rp×r , Σ = diag(σ1 , . . . , σq ) and V ∈ Rq×r
satisfy UUT = Ip and VVT = Iq . Denote U =
[u1 , . . . , ur ], where for k = 1, . . . , r, uk is the kth column
of U. Since the columns of U are orthogonal, we have


[W̃]:,l1 − [W̃]:,l1 2


[Ω]:,l1 − [Ω]:,l1 2
=

 Pq
2


k=1 (σk − τ )+ ([V]l1 k − [V]l2 k )uk
 Pq
2

σk ([V]l k − [V]l k )uk 
k=1

1

2

Pq

7. Acknowledgement
Luo Luo and Zhihua Zhang are supported by the Natural Science Foundation of Shanghai City of China (No.
15ZR1424200). Wu-Jun Li is supported by the NSFC (No.

=

[(σ − τ ) ]2 ([V]l1 k − [V]l2 k )2 ||uk ||2
k=1
Pq k 2 +
2
2
k=1 σk ([V]l1 k − [V]l2 k ) ||uk ||

Pq
=

[(σ − τ ) ]2 ([V]l1 k − [V]l2 k )2
k=1
Pq k 2 +
2
k=1 σk ([V]l1 k − [V]l2 k )

≤ 1.

Support Matrix Machines

Then we can obtain the following bound based on Lemma
1


[W̃]:,l1 − [W̃]:,l2 2

2
≤ [Ω]:,l1 − [Ω]:,l2 
p
X


[Ω]kl1 − [Ω]kl2 2
=

Substituting (15) and (16) into (14) to eliminate γi and ξi ,
we obtain
L(W, b, ξ, α, γ)
1
ρ
= tr(WT W) − tr(ΛT W) + ||W − S||2F
2
2
n
X
−
αi {yi [tr(WT Xi ) + b] − 1}.

k=1


≤ 2nC 2 p −

p
X


T
fkl
f
.
1 kl2

Setting the derivative of L with respect to W to be 0, we
have the optimal value

k=1

W∗ =

Appendix C: The Proof of Theorem 3
1
U1 Σ1 V1 . Recall that U0 , U1 , V0 and
τ
V1 are column orthogonal. So we have UT0 Z0 = 0 and
b formulation (9) and
Z0 V0 = 0. By the SVD form of S,
using Eqn. (1) we have:
Proof. Let Z0 be

Thus, we have 0 ∈ ∂G1 (S∗ ).

Appendix D: The Proof of Theorem 4

n

s.t.

X
1
tr(WT W) + C
ξi
2
i=1
ρ
−tr(ΛT W) + ||W − S||2F
2
yi [tr(WT Xi ) + b] ≥ 1 − ξi

(13)

L(W, b, ξ, α, γ)
n 
X
yi tr[(Λ + ρS)T Xi ] 
αi
=
1−
ρ+1
i=1

L(W, b, ξ, α, γ)
n

X
ρ
1
ξi − tr(ΛT W) + ||W − S||2F
= tr(WT W) + C
2
2
i=1
αi {yi [tr(WT Xi ) + b] − 1 + ξi } −

i=1

n
X

(15)

Setting the derivative of L with respect to b be 0, we have
n
X
i=1

αi yi = 0.

b∗ = yi − tr[(W∗ )T Xi ].
In practice, we choose the optimal b∗ by averaging these
solutions
1 X
b∗ = ∗
{yi − tr[(W∗ )T Xi ]}.
|S |
∗
i∈S

References

Setting the derivative of L with respect to ξ to be 0, we
have
i = 1, . . . , n.

ρ
1
tr(ST S) −
||Λ + ρS||2F . Thus,
2
2(ρ + 1)
finding the minimizer of H(W, b) is equivalent to solving
Problem (11) by KKT conditions. Let the optimal solution
of (11) be α∗ , we can obtain (10) from (18) directly. The
KKT conditions also provide

	
αi∗ yi {tr[(W∗ )T Xi ] + b∗ } − 1 + ξi∗ = 0

γi ξi . (14)

i=1

γi = C − αi ≥ 0,

n
X
1
αi αi yi yj tr(XTi Xj ) + D,
2(ρ + 1) i,j=1

which means for any 0 < αi∗ < C, the corresponding γi∗ >
0, ξi∗ = 0 and yi {tr[(W∗ )T Xi ] + b∗ } − 1 = 0. Then the
optimal b∗ can be calculated by

To solve problem (13), we construct the following Lagrangian function

n
X

−

γi∗ ξi∗ = 0,

ξi ≥ 0.

−

(18)

where D =

Proof. We denote H1 (W, b) = H(W, b) − tr(ΛT W) +
ρ
||W−S||2F . Finding the minimizer of H1 (W, b) is equiv2
alent to solving the following problem:
min

n

X
1 
Λ + ρS +
αi yi Xi .
ρ+1
i=1

Substituting (18) into (17), we obtain

∂G1 (S)|S=S∗ = Λ − ρW + Dτ (ρW − Λ) + τ ∂||S||∗ |S=S∗ .

W,b,ξ

(17)

i=1

(16)

Bach, Francis R. Consistency of trace norm minimization. The Journal of Machine Learning Research, 9:
1019–1048, 2008.
Cai, Deng, He, Xiaofei, Wen, Ji-Rong, Han, Jiawei, and
Ma, Wei-Ying. Support tensor machines for text categorization. Technical report, University of Illinois at
Urbana-Champaign, 2006.

Support Matrix Machines

Cai, Jian-Feng, Candès, Emmanuel J, and Shen, Zuowei. A
singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956–1982,
2010.
Candès, Emmanuel J and Recht, Benjamin. Exact matrix completion via convex optimization. Foundations
of Computational mathematics, 9(6):717–772, 2009.
Cortes, Corinna and Vapnik, Vladimir. Support-vector networks. Machine learning, 20(3):273–297, 1995.
Goldstein, Tom, ODonoghue, Brendan, and Setzer, Simon.
Fast alternating direction optimization methods. CAM
report, pp. 12–35, 2012.
Harchaoui, Zaid, Douze, Matthijs, Paulin, Mattis, Dudik,
Miroslav, and Malick, Jérôme. Large-scale image classification with trace-norm regularization. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3386–3393, 2012.
Hastie, Trevor, Robert, Tibshirani, and Jerome, Friedman.
The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer-Verlag, 2001.
He, Bingsheng and Yuan, Xiaoming. On non-ergodic convergence rate of douglas–rachford alternating direction
method of multipliers. Numerische Mathematik, pp. 1–
11, 2012.
Huang, Jin, Nie, Feiping, and Huang, Heng. Robust
discrete matrix completion. In Proceedings of the
AAAI Conference on Artificial Intelligence, pp. 424–430,
2013.

Liu, Ji, Musialski, Przemyslaw, Wonka, Peter, and Ye,
Jieping. Tensor completion for estimating missing values in visual data. In IEEE Tansactions on Pattern Analysis and Machine Intelligence, volume 35, pp. 208–220,
2013.
Nazir, M, Ishtiaq, Muhammad, Batool, Anab, Jaffar, M Arfan, and Mirza, Anwar M. Feature selection for efficient
gender classification. In Proceedings of the WSEAS international conference, Wisconsin, pp. 70–75, 2010.
Nesterov, Yurii. A method of solving a convex programming problem with convergence rate o(1/k2). Soviet
Mathematics Doklady, 27(2):372–376, 1983.
Pirsiavash, Hamed, Ramanan, Deva, and Fowlkes, Charless C. Bilinear classifiers for visual recognition. In
Proceedings of the Advances in Neural Information Processing Systems, pp. 1482–1490, 2009.
Platt, John et al. Sequential minimal optimization: A fast
algorithm for training support vector machines. Technical report msr-tr-98-14, Microsoft Research, 1998.
Pong, Ting Kei, Tseng, Paul, Ji, Shuiwang, and Ye, Jieping.
Trace norm regularization: reformulations, algorithms,
and multi-task learning. SIAM Journal on Optimization,
20(6):3465–3489, 2010.
Salakhutdinov, Ruslan and Srebro, Nathan. Collaborative filtering in a non-uniform world: Learning with the
weighted trace norm. arXiv preprint arXiv:1002.2780,
2010.
Signoretto, Marco, Dinh, Quoc Tran, De Lathauwer,
Lieven, and Suykens, Johan AK. Learning with tensors:
a framework based on convex optimization and spectral
regularization. Machine Learning, 94(3):303–351, 2014.

Hunyadi, Borbála, Signoretto, Marco, Van Paesschen,
Wim, Suykens, Johan AK, Van Huffel, Sabine, and
De Vos, Maarten. Incorporating structural information from the multichannel eeg improves patient-specific
seizure detection. Clinical Neurophysiology, 123(12):
2352–2361, 2012.

Srebro, Nathan and Shraibman, Adi. Rank, trace-norm and
max-norm. In Proceedings of the Conference on Learning Theory, pp. 545–560. 2005.

Kang, Zhuoliang, Grauman, Kristen, and Sha, Fei. Learning with whom to share in multi-task feature learning. In
Proceedings of the International Conference on Machine
Learning, pp. 521–528, 2011.

Watson, G Alistair. Characterization of the subdifferential
of some matrix norms. Linear Algebra and its Applications, 170:33–45, 1992.

Keerthi, S. Sathiya and Gilbert, Elmer G. Convergence of
a generalized smo algorithm for svm classifier design.
Machine Learning, 46(1-3):351–360, 2002.
Lewis, Adrian S. The mathematics of eigenvalue optimization. Mathematical Programming, 97(1-2):155–
176, 2003.

Vandenberghe, Lieven and Boyd, Stephen. Semidefinite
programming. SIAM review, 38(1):49–95, 1996.

Wolf, Lior, Jhuang, Hueihan, and Hazan, Tamir. Modeling appearances with low-rank SVM. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1–6, 2007.
Zheng, Wei-Long, Zhu, Jia-Yi, Peng, Yong, and Lu, BaoLiang. Eeg-based emotion classification using deep belief networks. In Proceedings of the IEEE International
Conference on Multimedia and Expo, pp. 1–6, 2014.

Support Matrix Machines

Zhou, Hua and Li, Lexin. Regularized matrix regression.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(2):463–483, 2014.
Zhu, Jia-Yi, Zheng, Wei-Long, Peng, Yong, Duan, RuoNan, and Lu, Bao-Liang. Eeg-based emotion recognition
using discriminative graph regularized extreme learning
machine. In Proceedings of the International Joint Conference on Neural Networks, pp. 525–532, 2014.
Zou, Hui and Hastie, Trevor. Regularization and variable
selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):
301–320, 2005.

