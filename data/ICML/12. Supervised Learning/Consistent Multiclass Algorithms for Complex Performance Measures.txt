Consistent Multiclass Algorithms for Complex Performance Measures

Harikrishna Narasimhan∗
Harish G. Ramaswamy∗
Aadirupa Saha
Shivani Agarwal
Indian Institute of Science, Bangalore 560012, INDIA

Abstract
This paper presents new consistent algorithms
for multiclass learning with complex performance measures, defined by arbitrary functions
of the confusion matrix. This setting includes as
a special case all loss-based performance measures, which are simply linear functions of the
confusion matrix, but also includes more complex performance measures such as the multiclass G-mean and micro F1 measures. We give a
general framework for designing consistent algorithms for such performance measures by viewing the learning problem as an optimization problem over the set of feasible confusion matrices,
and give two specific instantiations based on the
Frank-Wolfe method for concave performance
measures and on the bisection method for ratioof-linear performance measures. The resulting
algorithms are provably consistent and outperform a multiclass version of the state-of-the-art
SVMperf method in experiments; for large multiclass problems, the algorithms are also orders
of magnitude faster than SVMperf.

1. Introduction
In many practical applications of machine learning, the performance measure used to evaluate the performance of a
classifier takes a complex form, and is not simply the expectation or sum of a loss on individual examples. Indeed,
this is the case with the G-mean, H-mean and Q-mean performance measures used in class imbalance settings (Sun
et al., 2006; Wang & Yao, 2012; Kennedy et al., 2009; Kim
et al., 2013; Lawrence et al., 1998), the micro and macro
F1 measures used in information retrieval (IR) applications
∗

Both authors made equal contributions to the paper.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

HARIKRISHNA @ CSA . IISC . ERNET. IN
HARISH GURUP @ CSA . IISC . ERNET. IN
AADIRUPA . SAHA @ CSA . IISC . ERNET. IN
SHIVANI @ CSA . IISC . ERNET. IN

(Lewis, 1991), the min-max measure used in detection theory (Vincent, 1994), and many others. Unlike loss-based
performance measures, which are simply linear functions
of the confusion matrix of a classifier, these complex performance measures are defined by general functions of the
confusion matrix. How can we design consistent learning
algorithms for such complex performance measures?
While there has been much interest in designing consistent
algorithms for various types of supervised learning problems in recent years, most of this work has focused on lossbased performance measures, including binary/multiclass
0-1 loss (Bartlett et al., 2006; Zhang, 2004a;b; Lee et al.,
2004; Tewari & Bartlett, 2007), losses for specific problems
such as multilabel classification (Gao & Zhou, 2011) and
ranking (Cossock & Zhang, 2008; Xia et al., 2008; Duchi
et al., 2010; Ravikumar et al., 2011; Buffoni et al., 2011;
Calauzènes et al., 2012), and some work on general multiclass losses (Steinwart, 2007; Ramaswamy & Agarwal,
2012; Pires et al., 2013; Ramaswamy et al., 2013).
There has also been much interest in designing algorithms
for more complex performance measures. A prominent example is the SVMperf algorithm (Joachims, 2005), which
was developed primarily for the binary setting; other examples include algorithms for the binary F1 -measure and its
multiclass and multilabel variants (Musicant et al., 2003;
Ye et al., 2012; Dembczynski et al., 2011; 2013; Parambath et al., 2014). More recently, there has been increasing interest in designing consistent algorithms for complex
performance measures; however, most of this work has focused on the binary case (Ye et al., 2012; Menon et al.,
2013; Koyejo et al., 2014; Narasimhan et al., 2014).
In this paper, we develop a general framework for designing provably consistent algorithms for complex multiclass
performance measures. Our approach involves viewing the
learning problem as an optimization problem over the set
of feasible confusion matrices, and solving (approximately,
based on the training sample) this optimization problem
using an optimization method that needs access to only
an approximate linear minimization routine and a sample-

Consistent Multiclass Algorithms for Complex Performance Measures

based confusion matrix calculator. We give two specific instantiations based on the Frank-Wolfe method for concave
performance measures (such as the multiclass G-mean, Hmean and Q-mean) and on the bisection method for ratioof-linear performance measures (such as the micro F1 ).
The resulting algorithms are provably consistent, and outperform a multiclass version of SVMperf both in terms of
generalization performance and in terms of training time.
Notation. For n ∈PZ+ , we denote [n] = {1, . . . , n} and
n
∆n = {p ∈ Rn+ P
: i=1 pi = 1}. For A, BP∈ Rn×n , we
denote kAk1 = i,j |Aij | and hA, Bi = i,j Ai,j Bi,j .
The notation argmin∗i∈[n] will denote ties being broken in
favor of the larger number.

2. Complex Performance Measures
We are interested in general multiclass learning problems
with instance space X and label space Y = [n]. Given
a finite training sample S = ((x1 , y1 ), . . . , (xm , ym )) ∈
(X × [n])m , the goal is to learn a multiclass classifier
hS : X →[n], or more generally, a randomized multiclass
classifier hS : X →∆n (which given an instance x predicts
a class label in [n] according to the probability distribution
specified by hS (x)). We assume examples are drawn iid
from some distribution D on X × [n], with marginal µ on
X , ηi (x) = P(Y = i | X = x), and πi = P(Y = i).
Definition 1 (Confusion matrix). The confusion matrix of
a classifier h w.r.t. a distribution D, denoted CD [h] ∈
[0, 1]n×n , has entries defined as

D
Cij
[h] = P Y = i, h(X) = j ,
where the probability is over the draw of (X, Y ) from D
when h is deterministic, and additionally over
P theDrandomness in h when h is randomized. Clearly, i,j Cij
[h] = 1.
We will be interested in general, complex performance
measures that can be expressed as an arbitrary function of
the entries of the confusion matrix CD [h] (see Figure 1).
Definition 2 (Performance measure). For any function ψ :
[0, 1]n×n →R+ , define the ψ-performance measure of h
w.r.t. D as follows (we will adopt the convention that higher
values of ψ correspond to better performance):
ψ
PD
[h] = ψ(CD [h]) .

As the following examples show, this formulation captures
both common loss-based performance measures, which are
effectively linear functions of the entries of the confusion
matrix, and more complex performance measures such as
the G-mean, micro F1 -measure, and several others.
Example 1 (Loss-based performance measures). Consider
a multiclass loss matrix L ∈ [0, 1]n×n , such that Lij represents the loss incurred on predicting class j when the true

Figure 1. Complex multiclass performance measures, given by arbitrary functions of the confusion matrix, generalize both common loss-based performance measures, and binary performance
measures expressed in terms of TP, TN, FP and FN. (In practice,
the distribution D is unknown; one estimates the confusion matrix
from a finite sample, and applies ψ to the estimated matrix.)

class is i (note that one can always shift and scale a loss
matrix so that its entries lie in [0, 1] without impacting the
learning problem). In such settings, the performance of a
classifier h is measured by the expected loss on a new example from D, which amounts to taking a linear function
of the confusion matrix CD [h]:


L
PD
[h] = E 1 − LY,h(X)
X
D
=
(1 − Lij ) Cij
[h] = ψ L (CD [h]) ,
i,j

where ψ L (C) = 1 − hL, Ci ∀C ∈ [0, 1]n×n . For example, for the 0-1 loss given by L0-1
ij = 1(i 6= j), we have
P
0-1
ψ (C) = i Cii (which yields 0-1 accuracy); for the ab1
solute loss used in ordinal regression, Lord
ij = n−1 |i − j|,
P
1
ord
we have ψ (C) = i,j (1 − n−1 |i − j|) Cij .
Example 2 (Binary performance measures). In the binary
setting, where n = 2 and the labels are often indexed as
Y = {−1, 1}, the confusion matrix of a classifier contains
the proportions of true negatives (C−1,−1 = TN), false
positives (C−1,1 = FP), false negatives (C1,−1 = FN), and
true positives (C1,1 = TP). Our framework therefore includes any binary performance measure that is expressed
as a function of these quantities, including the ‘balanced
accuracy’ or AM measure (Menon
et al., 2013) given by

TP
TN
ψ AM (C) = 21 TP+FN
+ TN+FP
, the Fβ -measure (β > 0)
2

) TP
given by ψ Fβ (C) = (1+β 2(1+β
) TP+β 2 FN+FP , all ‘ratio-oflinear’ binary performance measures (Koyejo et al., 2014),
and more generally, all ‘non-decomposable’ binary performance measures (Narasimhan et al., 2014).1

Example 3 (G-mean measure). The G-mean measure is
used to evaluate both binary and multiclass classifiers in
settings with class imbalance (Sun et al., 2006; Wang &
Yao, 2012), and is given by
1/n
Y
n
Cii
GM
Pn
ψ (C) =
.
j=1 Cij
i=1
1

The ‘non-decomposable’ performance measures considered
by Narasimhan et al. (2014) were expressed as functions of
TP
TN
TPR = TP+FN
, TNR = TN+FP
, and p = TP + FN.

Consistent Multiclass Algorithms for Complex Performance Measures
Table 1. Examples of complex multiclass performance measures.

ψ(C)

Performance measure

1/n
PnCii
C
ij
j=1
Pn Pnj=1 Cij −1
i=1
Cii

Qn

G-mean

i=1

n
r

H-mean

2
PnCii
i=1 1 −
j=1 Cij
P
2 n
i=2 C
Pn
Pii
2− i=1 C1i − n
i=1 Ci1
Pn
2CiiP
1
Pn
n
i=1
n
j=1 Cij +
j=1 Cji

1−

Q-mean
Micro F1
Macro F1

1
n

Pn

kC◦ k∗

Spectral norm

(where C◦ is obtained from C
by normalizing rows to sum to 1
and setting diagonal entries to 0)

mini∈[n]

Min-max

Other examples of performance measures that are given by
(complex) functions of the confusion matrix include the
macro F1 -measure (Lewis, 1991), the H-mean (Kennedy
et al., 2009), the Q-mean (Lawrence et al., 1998), the spectral norm measure (Ralaivola, 2012; Machart & Ralaivola,
2012; Koco & Capponi, 2013), and the min-max measure
in detection theory (Vincent, 1994); see Table 1.
We are interested in designing algorithms that are provably
consistent for a given performance measure ψ, in that they
converge (in probability) to the optimal ψ-performance as
the training sample size increases:
Definition 3 (Optimal ψ-performance). For any function
ψ : [0, 1]n×n →R+ , define the optimal ψ-performance
w.r.t. D as the maximal ψ-performance over all randomized classifiers:
ψ,∗
PD
=

sup
h:X →∆n

ψ,∗
ψ
PD
− PD
[h] .

Definition 5 (ψ-consistent algorithm). For any function ψ :
[0, 1]n×n →R+ , say a multiclass algorithm A that given a
training sample S returns a classifier A(S) : X →∆n is
ψ-consistent w.r.t. D if ∀ > 0:

ψ,∗
ψ
PS∼Dm PD
− PD
[A(S)] >  → 0 as m→∞ .
In developing our algorithms, we will find it useful to also
define the empirical confusion matrix of a classifier h w.r.t.
b S [h] ∈ [0, 1]n×n , as
sample S, denoted C

PnCii
j=1 Cij

Example 4 (Micro F1 -measure). The micro F1 -measure
is widely used to evaluate multiclass classifiers in information retrieval and information extraction applications
(Manning et al., 2008). Many variants have been studied;
we consider here the form used in the BioNLP challenge
(Kim et al., 2013), which treats class 1 as a ‘default’ class
and is effectively given by the function2
Pn
2 i=2 Cii
Pn
Pn
ψ microF1 (C) =
.
2 − i=1 C1i − i=1 Ci1

2

Definition 4 (ψ-regret). For any classifier h and function
ψ : [0, 1]n×n →R+ , define the ψ-regret of h w.r.t. D as the
difference between its ψ-performance and the optimal:

ψ
PD
[h] .

Another popular variant of the micro F1 involves averaging
the entries of the ‘one-versus-all’ binary confusion matrices for all
classes, and computing the F1 for the averaged matrix; as pointed
out by Manning et al. (2008), this form of micro F1 effectively
reduces to the 0-1 classification accuracy. Recently, Parambath et
al. (2014) also considered a form of micro F1 similar to that used
in the BioNLP challenge (the expression they use is slightly simpler than ours and differs slightly from the BioNLP performance
measure; see Appendix A.1 in the supplementary material).

m

1 X
S
bij
1(yk = i, h(xk ) = j) .
C
[h] =
m
k=1

As a first step towards designing ψ-consistent algorithms,
we start by examining the form of ψ-optimal classifiers.

3. Bayes Optimal Classifiers
For loss-based performance measures, it is well known that
any classifier that always picks a class that minimizes the
expected loss conditioned on the instance is optimal:
Proposition 6. Let L ∈ [0, 1]n×n be a loss matrix and
ψ L : [0, 1]n×n →R+ be the corresponding loss-based performance measure, ψ L (C) = 1 − hL, Ci (see Example 1).
Then any (deterministic) classifier h∗ satisfying
Pn
h∗ (x) ∈ argminj∈[n] i=1 ηi (x)Lij
L,∗
L ∗
[h ] = PD
.
is a ψ L -optimal classifier, i.e. PD

For binary performance measures expressed as functions
of TN, FP, FN and TP (see Example 2), the following two
results on the form of Bayes optimal classifiers for ‘ratioof-linear’ binary performance measures and ‘monotonic’
binary performance measures, respectively, are known:
Theorem 7 ((Koyejo et al., 2014)). Let Y = {−1, 1} and
let ψ : [0, 1]2×2 →R+ be a ratio-of-linear performance
TP + a10 FP + a01 FN + a00 TN
measure of the form ψ(C) = ab11
11 TP + b10 FP + b01 FN + b00 TN
for some aij , bij ∈ R. Then ∃ a ψ-optimal classifier of
∗
one of the following forms: h∗ (x) = sign(η1 (x) − θD
) or
∗
∗
∗
h (x) = sign(θD − η1 (x)), where θD ∈ [0, 1] depends on
ψ,∗ 3,4
aij ’s and bij ’s, and on the optimal ψ-performance PD
.
3

The ratio-of-linear performance measures considered by
Koyejo et al. (2014) have additional constant terms in the numerator and denominator; since the entries of a confusion matrix sum
up to 1, these terms can be absorbed in the coefficients aij ’s, bij ’s.
4
The original result of Koyejo et al. (2014) makes a continuity assumption on the marginal distribution µ; as we shall see in
Theorem 11, the result holds even without this assumption.

Consistent Multiclass Algorithms for Complex Performance Measures

Theorem 8 ((Narasimhan et al., 2014)). Let Y = {−1, 1}
and let ψ : [0, 1]2×2 →R+ be a continuous performance
measure that is monotonically increasing in TP and TN and
non-increasing in FP and FN. Let D be such that the CDF
of the random variable η1 (X), P(η1 (X) ≤ z), is continuous for all z ∈ (0, 1). Then ∃ a ψ-optimal classifier of the
∗
∗
form h∗ (x) = sign(η1 (x) − θD
) for some θD
∈ [0, 1].
In order to understand optimal classifiers for more general
multiclass performance measures ψ, we will find it useful
to view the optimal ψ-performance as the maximal value
over all feasible confusion matrices:
Definition 9 (Feasible confusion matrices). Define the set
of feasible confusion matrices w.r.t. D as the set of all confusion matrices achieved by some randomized classifier:

	
CD = CD [h] : h : X →∆n .
Proposition 10. CD is a convex set.
The set CD will play an important role in both our analysis
of optimal classifiers and the subsequent development of
consistent algorithms. Clearly, we can write
ψ,∗
PD
= sup ψ(C) .

(1)

C∈CD

While it is not clear if a classifier achieving this Bayes optimal performance exists in general, we show below that for
‘ratio-of-linear’ performance measures ψ, and for ‘monotonic’ performance measures ψ under a mild continuity assumption on D, an optimal classifier does indeed exist, and
moreover, in each case, a ψ-optimal classifier can be obtained by finding a certain loss-based optimal classifier.
Theorem 11 (Form of Bayes optimal classifier for ratio-of–
linear ψ). Let ψ : [0, 1]n×n →R+ be a ratio-of-linear performance measure of the form ψ(C) = hA,Ci
hB,Ci for some
ψ,∗
A, B ∈ Rn×n with hB, Ci > 0 ∀C ∈ CD . Let t∗D = PD
.
e ∗ = −(A−t∗ B), and let L∗ ∈ [0, 1]n×n be obtained
Let L
D
e ∗ so its entries lie in [0, 1]. Then
by scaling and shifting L
L∗
any classifier that is ψ -optimal is also ψ-optimal.
Lemma 12 (Existence of Bayes optimal classifier for
monotonic ψ). Let D be such that the probability
measure associated with the random vector η(X) =
(η1 (X), . . . , ηn (X))> is absolutely continuous w.r.t. the
base probability measure associated with the uniform distribution over ∆n , and let ψ be a performance measure
that is differentiable and bounded over CD , and is monotonically increasing in Cii for each i and non-increasing in
ψ ∗
ψ,∗
Cij for all i 6= j. Then ∃h∗ : X →∆n s.t. PD
[h ] = PD
.
Theorem 13 (Form of Bayes optimal classifier for monotonic ψ). Let D, ψ satisfy the conditions of Lemma 12.
Let h∗ : X →∆n be a ψ-optimal classifier and let C∗ =
e ∗=−∇ψ(C∗ ), and let L∗ ∈ [0, 1]n×n be obCD [h∗ ]. Let L
e ∗ so its entries lie in [0, 1].
tained by scaling and shifting L
L∗
Then any classifier that is ψ -optimal is also ψ-optimal.

Figure 2. ‘Plug-in’ algorithm used for binary performance measures ψ (Koyejo et al., 2014; Narasimhan et al., 2014). In practice,
one searches over O(|S2 |) values of the threshold θ. In the multiclass case, such a method requires searching over an exponential
number of loss matrices and is computationally intractable.

Theorems 11 and 13 generalize the results of Theorems 7
and 8 to the multiclass case; indeed, in the binary setting,
classifiers that threshold the class probability function are
known to be optimal for loss-based performance measures
(Elkan, 2001). Moreover, by virtue of Proposition 6, Theorems 11 and 13 also imply that under the above conditions,
one can always find a deterministic classifier that achieves
the ψ-optimal performance.5 Note that all performance
measures in Table 1 are ‘monotonic’ as in Theorem 13; the
micro F1 also has a ‘ratio-of-linear’ form as in Theorem 11.
The above results do not directly yield an algorithm since
∗
the linear performance measures ψ L that they suggest reψ,∗
quire knowledge of the optimal performance value PD
in
the ratio-of-linear case, or a ψ-optimal classifier h∗ or ψoptimal confusion matrix C∗ in the monotonic case. Nevertheless, a naı̈ve algorithmic approach suggested by the
above results is to search over a large range of n × n loss
matrices L, estimate a ψ L -optimal classifier for each such
L, and select among these a classifier that yields maximal
ψ-performance (e.g. on a held-out validation data set). This
is the analogue of ‘plug-in’ type methods for binary performance measures, where one searches over possible thresholds on the (estimated) class probability function (see Figure 2). However, while the binary case involves a search
over values for a single threshold parameter, in the multiclass case, searching over a suitable range of n × n loss
matrices L in general requires time exponential in n2 , and
for large n is computationally intractable.6
In what follows, we will instead design efficient learning
algorithms that search over the space of feasible confusion
matrices CD using suitable optimization methods.
5
This is not true in general; e.g. see Example 5 in Appendix B.1 for a setting where one needs a randomized classifier
to achieve the optimal performance.
6
For the special case of ratio-of-linear performance measures
ψ(C) = hA,Ci
, one can restrict the search to loss matrices of the
hB,Ci
form L = −(A − tB) for t ∈ R, and can search over the single
parameter t; indeed, this is precisely what Parambath et al. (2014)
do in the context of optimizing F -measures.

Consistent Multiclass Algorithms for Complex Performance Measures

Algorithm 1 Algorithm Based on Frank-Wolfe Method
1: Input: ψ : [0, 1]n×n →R+
S = ((x1 , y1 ), . . . , (xm , ym )) ∈ (X × [n])m
2: Parameter: κ ∈ N
 
m
3: Split S into S1 and S2 with sizes m
2 and 2
b = CPE(S1 )
4: η
b0 = C
b S2 [h0 ]
5: Initialize: h0 : X →∆n , C
6: For t = 1 to T = κm do
b t = −∇ψ(C
b t−1 ), scaled and shifted to [0, 1]n×n
7:
L
Pn
t
bt
ηbi (x)L
8:
Obtain gb : x 7→ argmin∗
j∈[n]

9:
10:

Figure 3. Overall framework of the multiclass learning algorithms
proposed in this paper. The algorithms solve (approximately)
maxC∈CD ψ(C), by using an optimization method that on each
iteration requires only solving a linear loss minimization problem
and calculating an empirical confusion matrix, both of which can
be done efficiently. Details of the constructions and updates depend on the underlying optimization method.

4. Algorithms
We design algorithms to search for ψ-optimal classifiers
via a search over the set of feasible confusion matrices CD .
While CD is a convex set, it is not available directly to
the learner: not only is D unknown, but more fundamentally, the set of all confusion matrices is hard to characterize. On the other hand, given the class probability function
b : X →∆n – one operation
η : X →∆n – or an estimate η
that is easy to perform is to find an optimal classifier for a
linear loss L: one simply returns
Pnthe classifier g : X →[n]
given by g(x) = argmin∗j∈[n] i=1 ηbi (x)Lij . Moreover,
given a classifier g and the distribution D – or a finite sample S – it is easy to calculate the confusion matrix of g: one
simply computes for each i, j the proportion of examples
(x, y) for which y = i and g(x) = j. In the following, we
will design learning algorithms based on iterative optimization methods that do not require access to the full constraint
set, but rather seek to (approximately, based on the training
sample S) solve the optimization problem maxC∈CD ψ(C)
by making use of only the above two operations that can be
performed efficiently.
In particular, we design two algorithms based on the above
approach. The first applies to concave performance measures ψ, and makes use of the classical Frank-Wolfe optimization method, which solves general constrained convex optimization problems using only a linear minimization subroutine (Frank & Wolfe, 1956). The second algorithm applies to performance measures ψ that can be expressed as a ratio of linear functions, ψ(C) = hA,Ci
hB,Ci ; in
such cases, one can test whether the optimal value of ψ(C)
exceeds a target value γ by again appealing to a linear min-

11:
12:
13:

i=1

ij

bt = C
b S2 [b
Γ
gt ] 
2
2
t
h = 1 − t+1
gbt
ht−1 + t+1

bt = 1 − 2 C
b t−1 + 2 Γ
bt
C
t+1
t+1
end For
T
Output: hFW
S = h : X →∆n

imization subroutine, leading to an efficient binary search
type algorithm based on the bisection method.
Both algorithms divide the input training sample S into a
b S1 ,
part S1 used for obtaining a class probability estimate η
and a part S2 used for calculating empirical confusion matrices. On each iteration t, the algorithms implicitly maintain a confusion matrix Ct = CD [ht ] ∈ CD by maintaining a (possibly randomized) classifier ht , construct a linear
loss Lt based on ψ and the underlying optimization method
(either the Frank-Wolfe method or the bisection method),
solve a linear minimization problem that finds a ‘plug-in’
optimal classifier for this loss w.r.t. the class probability esb S1 , calculate the empirical confusion matrix corretimate η
sponding to this classifier using S2 , and then update; after
T iterations, the final classifier hT is returned. The overall
framework is summarized in Figure 3.
4.1. Algorithm Based on Frank-Wolfe Method
The first algorithm that we describe uses the classical
Frank-Wolfe method for constrained convex optimization
(Frank & Wolfe, 1956) to learn a (randomized) classifier
for performance measures ψ that are concave over CD , such
as the G-mean measure in Example 3 (and the H-mean and
Q-mean in Table 1). An ideal version of the algorithm for
exactly solving maxC∈CD ψ(C) would maintain iterates
Ct ∈ CD , compute Lt = −∇ψ(Ct−1 ), solve exactly the
resulting linear minimization problems minC∈CD hLt , Ci,
and update Ct accordingly. As shown in Algorithm 1,
the learning algorithm we propose maintains Ct ∈ CD
implicitly via ht , and performs approximate sample-based
computations in solving the linear minimization problems
and computing confusion matrices. The final (randomized)
classifier output by the algorithm is a convex combination
of the classifiers learned across all the iterations.
The above algorithm does not amount to maximizing ψ
over an empirical constraint set, but instead maximizes ψ

Consistent Multiclass Algorithms for Complex Performance Measures

directly over CD , with the associated linear minimization
and confusion matrix calculation steps replaced with approximate, sample-based ones; this will be evident when
we discuss consistency of the algorithm in Section 5.
4.2. Algorithm Based on Bisection Method
The second algorithm we describe uses the bisection
method (Boyd & Vandenberghe, 2004) and is designed for
ratio-of-linear performance measures that can be written in
n×n
, such as
the form ψ(C) = hA,Ci
hB,Ci for some A, B ∈ R
the micro F1 -measure in Example 4. For such performance
measures, it is easy to see that maxC∈CD ψ(C) ≥ γ ⇐⇒
maxC∈CD hA − γB, Ci ≥ 0 ; thus, to test whether the optimal value of ψ is greater than γ, one can simply solve the
linear minimization problem minC∈CD −hA − γB, Ci and
test the value of ψ at the resulting minimizer. Based on this
observation, one can employ the bisection method to conduct a binary search for the maximal value (and maximizer)
of ψ(C) using only a linear minimization subroutine.
An exact version of the algorithm would maintain Ct ∈ CD
together with lower and upper bounds αt and β t on the
maximal value of ψ, determine whether this maximal value
is greater than the midpoint γ t of these bounds using the
linear minimization subroutine, and then update Ct and
αt , β t accordingly. Again, as shown in Algorithm 2, the
learning algorithm we propose maintains Ct ∈ CD implicitly via ht , and performs approximate sample-based computations in solving the linear minimization problems and
computing confusion matrices. Since for ratio-of-linear
performance measures there is always a deterministic classifier achieving the optimal performance (see Theorem 11),
here it suffices to maintain deterministic classifiers ht .7
The above bisection algorithm for ratio-of-linear performance measures generalizes and improves the method of
Parambath et al. (2014), who use a similar idea in the context of optimizing F-measures but use a brute-force line
search to estimate the optimal F-measure value; the bisection based algorithm, which essentially uses binary search,
requires exponentially fewer computations.

5. Consistency
We now show that the algorithms proposed above are ψconsistent. Our proofs rely on convergence guarantees of
the underlying optimization methods, together with Lemmas 14 and 15 below, which yield approximation guaran7
While the bisection based algorithm can be viewed as searching over a one-dimensional class of loss matrices, this is a special case; the Frank-Wolfe based algorithm for concave performance measures does not admit such an interpretation. Moreover, viewing the bisection algorithm as approximately solving
maxC∈CD ψ(C) allows us to obtain consistency results in the
same unified framework as the Frank-Wolfe based algorithm.

Algorithm 2 Algorithm Based on Bisection Method

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

hA,Ci
hB,Ci

with A, B ∈ Rn×n
S = ((x1 , y1 ), . . . , (xm , ym )) ∈ (X × [n])m
Parameter: κ ∈ N
m
 
Split S into S1 and S2 with sizes m
2 and 2
b = CPE(S1 )
η
Initialize: h0 : X →[n], α0 = 0, β 0 = 1
For t = 1 to T = κm do
γ t = (αt−1 + β t−1 )/2
b t = −(A − γ t B), scaled and shifted to [0, 1]n×n
L
Pn
bt
ηbi (x)L
Obtain gbt : x 7→ argmin∗

1: Input: ψ(C) =

j∈[n]

i=1

ij

bt = C
b S2 [b
Γ
gt ]
t
b ) ≥ γ t then αt = γ t , β t = β t−1 , ht = gbt
If ψ(Γ
else αt = αt−1 , β t = γ t , ht = ht−1
end For
T
Output: hBS
S = h : X →[n]

tees for the plug-in linear loss minimization and empirical
confusion matrix calculation steps, respectively.
Lemma 14 (L-regret of multiclass plug-in classifiers). Let
b : X →∆n and let L ∈ [0, 1]n×n . Let b
η
h : X →[n] be
Pn
defined as b
h(x) = argmin∗j∈[n] i=1 ηbi (x)Lij . Then
 

L,∗
L b
b (X) − η(X)1 .
PD
− PD
[h] ≤ EX η
Lemma 15 (Uniform convergence of confusion matrices). Let q : X →∆n and let Hq be the set of (deterministic) classifiers
h : X →[n] that satisfy h(x) =
Pn
argmin∗j∈[n] i=1 qi (x)Lij for some L ∈ [0, 1]n×n . Let
S ∈ (X × [n])m be drawn randomly from Dm . Let
δ ∈ (0, 1]. Then with probability ≥ 1 − δ (over S ∼ Dm ),


b S [h] ≤
sup CD [h] − C
∞
h ∈ Hq
r
2
n log(n) log(m) + log(n2 /δ)
C
,
m
where C > 0 is a distribution-independent constant.
5.1. Consistency of Frank-Wolfe Based Algorithm
The following result bounds the ψ-regret of Algorithm 1
for any concave and smooth performance measure ψ:
Theorem 16 (ψ-regret of Frank-Wolfe based algorithm).
Let ψ : [0, 1]n×n →R+ be concave over CD , and LLipschitz and β-smooth w.r.t. the `1 norm. Let S ∈ (X ×
b : X →∆n be
[n])m be drawn randomly from Dm . Let η
the CPE model learned in Algorithm 1 and hFW
S : X →∆n
the classifier returned after κm iterations. Let δ ∈ (0, 1].
Then with probability ≥ 1 − δ (over S ∼ Dm ),
 

ψ,∗
ψ FW
b (X) − η(X)1
PD
− PD
[hS ] ≤ 4LEX η
r
√
n2 log(n) log(m) + log(n2 /δ)
8β
2
+ 4 2βn C
+
,
m
κm + 2
where C > 0 is a distribution-independent constant.

Consistent Multiclass Algorithms for Complex Performance Measures

0.5

0.4

0.4

0.3
Optimal G−Mean
Frank−Wolfe (GM)
SVMPerf (GM)
LogReg (0−1)

0.2
0.1
0
2
10

3

10
No. of training examples

0.3
Optimal H−Mean
Frank−Wolfe (HM)
SVMPerf (HM)
LogReg (0−1)

0.2
0.1
0
2
10

4

10

Q−Mean

micro F1

0.2

0.3

0.1

4

10

0.25

0.4

0.2

3

10
No. of training examples
micro F1

0.5

0
2
10

5.2. Consistency of Bisection Based Algorithm

H−Mean

0.5

H−Mean

G−Mean

G−Mean

Q− Mean

The proof of Theorem 16 exploits Lemmas 14 and 15,
together with the standard convergence guarantee for the
Frank-Wolfe method (Jaggi, 2013). In particular, if the
b is learned by a CPE algorithm that guaranCPE model η
tees EX [kb
η (X) − η(X)k1 ]→0 as m→∞, as is done by
any algorithm that minimizes a strictly proper composite
multiclass loss over a suitably large function class (Vernet
et al., 2011), then the above result yields ψ-consistency of
the Frank-Wolfe based algorithm. For concave non-smooth
performance measures ψ such as the G-mean, H-mean and
Q-mean, Algorithm 1 can be applied to a suitable smooth
approximation to ψ; similar consistency guarantees can be
shown in this case as well (see Appendix C.5).

Optimal Q−Mean
Frank−Wolfe (QM)
SVMPerf (QM)
LogReg (0−1)

0.15
Optimal micro F1
Bisection (mF1)
SVMPerf (mF1)
LogReg (0−1)

0.1
0.05
0

3

10
No. of training examples

4

10

2

10

3

10
No. of training examples

4

10

The following result bounds the ψ-regret of Algorithm 2
for ratio-of-linear performance measures ψ:

Figure 4. Convergence to Bayes optimal performance for Gmean, H-mean, Q-mean and micro F1 measures on synthetic data.

Theorem 17 (ψ-regret of bisection based algorithm). Let
ψ : [0, 1]n×n →R+ be such that ψ(C) = hA,Ci
hB,Ci ,
n×n
where A, B ∈ R
, supC∈CD ψ(C) ≤ 1, and
minC∈CD hB, Ci ≥ b for some b > 0. Let S ∈ (X ×
b : X →∆n be
[n])m be drawn randomly from Dm . Let η
the CPE model learned in Algorithm 2 and hBS
S : X →[n]
the classifier returned after κm iterations. Let δ ∈ (0, 1].
Then with probability ≥ 1 − δ (over S ∼ Dm ),
 

ψ,∗
ψ BS
b (X) − η(X)1
PD
− PD
[hS ] ≤ 2τ EX η
r
√
n2 log(n) log(m) + log(n2 /δ)
+ 2−κm ,
+ 2 2Cτ
m

(Joachims, 2005) and a standard multiclass logistic regression algorithm that optimizes 0-1 accuracy.8 We note that
the worst-case running time of SVMperf is exponential in
the number of classes, and hence this method could not be
scaled to data sets with large numbers of classes.

where τ = 1b (kAk1 + kBk1 ) and C > 0 is a distributionindependent constant.
In this case, the proof of Theorem 17 exploits Lemmas 14
and 15, together with the well-known convergence guarantee for the bisection method (Boyd & Vandenberghe,
2004). Again, if the CPE algorithm used guarantees
EX [kb
η (X) − η(X)k1 ]→0 as m→∞, then the above result yields ψ-consistency of the bisection based algorithm.
As a concrete example, with such a CPE method, we have
that Algorithm 2 is consistent for the micro F1 -measure.

6. Experiments
We evaluated the proposed Frank-Wolfe and bisection
based algorithms on a variety of multiclass learning tasks
that differed in terms of performance measure, type of
data set, number of classes, etc. In experiments with the
Frank-Wolfe based algorithm, we considered the G-mean,
H-mean and Q-mean performance measures, all of which
are concave; in experiments with the bisection based algorithm, we considered the micro F1 -measure, which has
a ratio-of-linear form. In all cases, we compared these
algorithms against the state-of-the-art SVMperf algorithm

6.1. Convergence to Bayes Optimal Performance
In a first set of experiments, we tested the consistency
behavior of the algorithms on a synthetic data set for
which the Bayes optimal performance could be calculated.
Specifically, we used a 3-class synthetic data set with instances in X = R2 generated as follows: examples were
chosen from class 1 with probability 0.85, from class 2 with
probability 0.1 and from class 3 with probability 0.05; instances in the three classes were then drawn from multivariate Gaussian distributions with means (1, 1)> , (0, 0)> , and
(−1,−1)> , respectively and with the same covariance matrix 51 15 . The class probability function η : R2 →∆3 for
this distribution is a softmax of linear functions that can be
computed in closed form (see Appendix D.1). Note that the
distribution and all four performance measures considered
satisfy the conditions of Theorem 13.
Figure 4 shows the performance of the different algorithms
for the G-mean, H-mean, Q-mean and micro F1 measures.
In all cases, we learned a linear classification model (see
Appendix D.2 for an explanation). As can be seen, for
the G-mean, H-mean and Q-mean measures, the FrankWolfe based algorithm converges to the Bayes optimal performance, while the other algorithms fail to be consistent.
For the micro F1 -measure (for which class 1 was taken
as the default class), the bisection algorithm converges to
the Bayes optimal performance; SVMperf also seems to approach the optimal performance, but at a slower rate.
8
The CPE method used in the Frank-Wolfe and bisection
based algorithms was also based on multiclass logistic regression.

Consistent Multiclass Algorithms for Complex Performance Measures
Table 2. Data sets used in experiments in Sections 6.2–6.4.
Data set # instances
# features
# classes
UCI car
1728
21
4
pageblocks
5473
10
5
glass
214
9
6
abalone
4177
10
12
IR
cora
2708
1433
4
news20
12199
61188
4
rcv1
15564
47236
11
Table 3. Performance of Frank-Wolfe based algorithm for Gmean, H-mean and Q-mean measures on various UCI data sets.
The symbol × indicates the method did not complete after 96 hrs.
car
pgblks glass abalone
G-mean Frank-Wolfe 0.945
0.908
0.680
0.223
SVMperf
0.792
0.796
0.431
×
LogReg (0-1) 0.911
0.691
0.146
0.000
H-mean Frank-Wolfe 0.945
0.904
0.632
0.197
SVMperf
0.880
0.574
0.381
×
LogReg (0-1) 0.909
0.631
0.143
0.000
Q-mean Frank-Wolfe 0.930
0.877
0.613
0.247
SVMperf
0.909
0.651
0.481
×
LogReg (0-1) 0.898
0.660
0.490
0.223

6.2. Performance of Frank-Wolfe on UCI Data Sets
Our next set of experiments evaluates the Frank-Wolfe
based algorithm on a variety of real data sets taken from
the UCI repository (Frank & Asuncion, 2010). The data
sets varied in size and number of classes; in many cases,
there was moderate to severe imbalance across the various classes, a setting in which the G-mean, H-mean and
Q-mean performance measures are of interest. We show
results here for four of the data sets (see Table 2); results
on additional data sets can be found in Appendix D.2.

Table 4. Performance of bisection based algorithm for micro F1 measure on CoRA, 20 Newsgroups, and Reuters RCV1 data sets.
The symbol × indicates the method did not complete after 96 hrs.
cora
news20
rcv1
Micro F1 Bisection
0.690
0.772
0.502
SVMperf
0.622
×
×
LogReg (0-1) 0.687
0.770
0.428
Table 5. Training times (in secs) for various algorithms on UCI
and IR data sets. The symbol × indicates the method did not
complete after 96 hrs. See Appendix D.2 for more details.
car
pgblks
glass abalone
G-mean Frank-Wolfe
1.96
5.89
0.27
7.31
SVMperf
8327.5 63667.7 1302.8
×
LogReg (0-1)
0.59
1.70
0.07
3.84
cora news20
rcv1
Micro F1 Bisection
0.23
13.40
10.43
SVMperf
18095.98
×
×
LogReg (0-1)
0.08
19.04
11.88

a subset of the original set of classes was viewed as ‘interesting’ for prediction purposes, and the remaining classes
were merged into a single ‘default’ class (used as class 1
in evaluating the micro F1 measure); this led to 4 effective
classes for the CoRA and 20 Newsgroups data sets, and 11
effective classes for the RCV1 data set (see Table 2).
Again, we learned linear models with all the algorithms.
The results, averaged over 5 random 80%-20% train-test
splits for each data set, are shown in Table 4 (here again,
SVMperf failed to complete running after 96 hours on the
20 Newsgroups and RCV1 data sets). As can be seen, the
bisection based algorithm consistently yields micro F1 values better than or comparable to the baseline methods.

As before, we learned linear models with all the algorithms: (regularized) linear multiclass logistic regression as
the CPE method in the Frank-Wolfe based algorithm, and
linear SVMperf and linear 0-1 multiclass logistic regression
as baselines. The results, averaged over 5 random 80%20% train-test splits for each data set, are shown in Table 3 (in the case of the Abalone data set, which has 12
classes, the SVMperf method did not complete running after 96 hours). As can be seen, in practically all cases, the
Frank-Wolfe based algorithm outperforms both baselines.

6.4. Run-Time Comparisons

6.3. Performance of Bisection on IR Data Sets

7. Conclusion

Next, we evaluate the bisection based algorithm on three
information retrieval (IR) data sets, where the micro F1 measure is of interest: a version of the CoRA data set containing research papers categorized into 7 classes, the 20
Newsgroups data set containing newsgroup documents categorized into 20 classes, and the RCV1 data set containing news articles from Reuters categorized into 53 classes
(Forman, 2003; Druck et al., 2008; Lewis et al., 2004). For
each of these data sets, we considered learning tasks where

In practice, classifiers are often evaluated using complex
performance measures given by arbitrary functions of the
confusion matrix. This paper has developed a general
framework for designing consistent multiclass algorithms
for such settings, and has given two practical algorithms
that apply to a wide range of complex multiclass performance measures used in practice. The algorithms outperform existing baselines; in addition, they are computationally efficient and scale well with the number of classes.

Finally, we compare the training times of the various algorithms. Table 5 shows the training times (in seconds) for
the G-mean and micro F1 performance measures (see Appendix D.2 for training times for H-mean and Q-mean). As
can be seen, both the Frank-Wolfe based algorithm and the
bisection based algorithm proposed here are several orders
of magnitude faster than SVMperf , particularly on data sets
with large numbers of classes.

Consistent Multiclass Algorithms for Complex Performance Measures

Acknowledgements. HN acknowledges support from a
Google India PhD Fellowship. HGR acknowledges support from a TCS PhD Fellowship. SA acknowledges support from the Department of Science & Technology (DST)
of the Indian Government under a Ramanujan Fellowship,
from the Indo-US Science & Technology Forum (IUSSTF),
and from Yahoo in the form of an unrestricted grant.

References
Bartlett, P.L., Jordan, M.I., and McAuliffe, J.D. Convexity,
classification, and risk bounds. Journal of the American
Statistical Association, 101(473):138–156, 2006.
Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth,
M. Learnability and the Vapnik-Chervonenkis dimension. Journal of the ACM, 36:929–965, 1989.
Boyd, S. and Vandenberghe, L. Convex Optimization.
Cambridge University Press, 2004.
Buffoni, D., Calauzènes, C., Gallinari, P., and Usunier, N.
Learning scoring functions with order-preserving losses
and standardized supervision. In ICML, 2011.
Calauzènes, C., Usunier, N., and Gallinari, P. On the
(non-)existence of convex, calibrated surrogate losses for
ranking. In NIPS, 2012.
Cossock, D. and Zhang, T. Statistical analysis of Bayes optimal subset ranking. IEEE Transactions on Information
Theory, 54(11):5140–5154, 2008.
Dembczynski, K., Waegeman, W., Cheng, W., and
Hüllermeier, E. An exact algorithm for F-measure maximization. In NIPS, 2011.
Dembczynski, K., Jachnik, A., Kotlowski, W., Waegeman,
W., and Hullermeier, E. Optimizing the F-measure in
multi-label classification: Plug-in rule approach versus
structured loss minimization. In ICML, 2013.
Druck, G., Mann, G., and McCallum, A. Learning from
labeled features using generalized expectation criteria.
In SIGIR, 2008.
Duchi, J., Mackey, L., and Jordan, M. On the consistency
of ranking algorithms. In ICML, 2010.

Frank, M. and Wolfe, P. An algorithm for quadratic programming. Naval Research Logistics Quarterly, 3(1-2):
95–110, 1956.
Gao, W. and Zhou, Z.-H. On the consistency of multi-label
learning. In COLT, 2011.
Jaggi, M. Revisiting Frank-Wolfe: Projection-free sparse
convex optimization. In ICML, 2013.
Joachims, T. A support vector method for multivariate performance measures. In ICML, 2005.
Kennedy, K., Namee, B.M., and Delany, S.J. Learning
without default: A study of one-class classification and
the low-default portfolio problem. In ICAICS, 2009.
Kim, J-D., Wang, Y., and Yasunori, Y. The genia event
extraction shared task, 2013 edition - overview. ACL,
2013.
Koco, S. and Capponi, C. On multi-class classification
through the minimization of the confusion matrix norm.
In ACML, 2013.
Koyejo, O., Natarajan, N., Ravikumar, P., and Dhillon, I.S.
Consistent binary classification with generalized performance metrics. In NIPS, 2014.
Lawrence, S., Burns, I., Back, A., Tsoi, A-C., and Giles,
C.L. Neural network classification and prior class probabilities. In Neural Networks: Tricks of the Trade, LNCS,
pp. 1524:299–313. 1998.
Lee, Y., Lin, Y., and Wahba, G. Multicategory support vector machines: Theory and application to the classification of microarray data. Journal of the American Statistical Association, 99(465):67–81, 2004.
Lewis, D.D. Evaluating text categorization. In Proceedings
of the Workshop on Speech and Natural Language, HLT,
1991.
Lewis, D.D., Yang, Y., Rose, T.G., and Li, F. RCV1:
A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–
397, 2004.

Elkan, C. The foundations of cost-sensitive learning. In
IJCAI, 2001.

Machart, P. and Ralaivola, L. Confusion matrix stability
bounds for multiclass classification. Technical report,
Aix-Marseille University, 2012.

Forman, G. An extensive empirical study of feature selection metrics for text classification. Journal of Machine
Learning Research, 3:1289–1305, 2003.

Manning, C. D., Raghavan, P., and Schütze, H. Introduction to Information Retrieval. Cambridge University
Press, 2008.

Frank, A. and Asuncion, A. UCI machine learning repository, 2010. URL http://archive.ics.uci.
edu/ml.

Menon, A.K., Narasimhan, H., Agarwal, S., and Chawla,
S. On the statistical consistency of algorithms for binary
classification under class imbalance. In ICML, 2013.

Consistent Multiclass Algorithms for Complex Performance Measures

Musicant, D.R., Kumar, V., and Ozgur, A. Optimizing
F-measure with support vector machines. In FLAIRS,
2003.

Zhang, T. Statistical behavior and consistency of classification methods based on convex risk minimization. Annals
of Statistics, 32(1):56–134, 2004a.

Narasimhan, H., Vaish, R., and Agarwal, S. On the
statistical consistency of plug-in classifiers for nondecomposable performance measures. In NIPS, 2014.

Zhang, T. Statistical analysis of some multi-category
large margin classification methods. Journal of Machine
Learning Research, 5:1225–1251, 2004b.

Parambath, S.A.P., Usunier, N., and Grandvalet, Y. Optimizing F-measures by cost-sensitive classification. In
NIPS, 2014.
Pires, B. Á., Szepesvari, C., and Ghavamzadeh, M. Costsensitive multiclass classification risk bounds. In ICML,
2013.
Ralaivola, L. Confusion-based online learning and a
passive-aggressive scheme. In NIPS, 2012.
Ramaswamy, H. G. and Agarwal, S. Classification calibration dimension for general multiclass losses. In NIPS,
2012.
Ramaswamy, H. G., Agarwal, S., and Tewari, A. Convex
calibrated surrogates for low-rank loss matrices with applications to subset ranking losses. In NIPS, 2013.
Ravikumar, P., Tewari, A., and Yang, E. On NDCG consistency of listwise ranking methods. In AISTATS, 2011.
Steinwart, I. How to compare different loss functions and
their risks. Constructive Approximation, 26:225–287,
2007.
Sun, Y., Kamel, M.S., and Wang, Y. Boosting for learning
multiple classes with imbalanced class distribution. In
ICDM, 2006.
Tewari, A. and Bartlett, P. L. On the consistency of multiclass classification methods. Journal of Machine Learning Research, 8:1007–1025, 2007.
Vernet, E., Williamson, R. C., and Reid, M. D. Composite
multiclass losses. In NIPS, 2011.
Vincent, P.H. An Introduction to Signal Detection and Estimation. Springer-Verlag, 1994.
Wang, S. and Yao, X. Multiclass imbalance problems:
Analysis and potential solutions. IEEE Transactions on
Systems, Man, and Cybernetics, Part B: Cybernetics, 42
(4):1119–1130, 2012.
Xia, F., Liu, T.-Y., Wang, J., Zhang, W., and Li, H. Listwise
approach to learning to rank: Theory and algorithm. In
ICML, 2008.
Ye, N., Chai, K.M.A., Lee, W.S., and Chieu, H.L. Optimizing F-measures: A tale of two approaches. In ICML,
2012.

