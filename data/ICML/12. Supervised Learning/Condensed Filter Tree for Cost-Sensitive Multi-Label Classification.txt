Condensed Filter Tree for Cost-Sensitive Multi-Label Classification
Chun-Liang Li
R 01922001@ CSIE . NTU . EDU . TW
Hsuan-Tien Lin
HTLIN @ CSIE . NTU . EDU . TW
Department of Computer Science and Information Engineering, National Taiwan University

Abstract
Different real-world applications of multi-label
classification often demand different evaluation
criteria. We formalize this demand with a general setup, cost-sensitive multi-label classification (CSMLC), which takes the evaluation criteria into account during learning. Nevertheless,
most existing algorithms can only focus on optimizing a few specific evaluation criteria, and
cannot systematically deal with different ones. In
this paper, we propose a novel algorithm, called
condensed filter tree (CFT), for optimizing any
criteria in CSMLC. CFT is derived from reducing
CSMLC to the famous filter tree algorithm for
cost-sensitive multi-class classification via constructing the label powerset. We successfully
cope with the difficulty of having exponentially
many extended-classes within the powerset for
representation, training and prediction by carefully designing the tree structure and focusing on
the key nodes. Experimental results across many
real-world datasets validate that CFT is competitive with special purpose algorithms on special
criteria and reaches better performance on general criteria.

1. Introduction
The multi-label classification problem allows each instance
to be associated with a set of labels simultaneously. It
has in recent years attracted much attention among researchers (Tsoumakas et al., 2010; 2012) because the problem setting matches many different real-world applications;
these include bio-informatics (Elisseeff & Weston, 2002),
text mining (Srivastava & Zane-Ulman, 2005) and multimedia (Turnbull et al., 2008). The different applications
often come with different criteria for evaluating the performance of multi-label classification algorithms. Popular criProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

teria include the Hamming loss, the 0/1 loss, the Rank loss,
the F1 score and the Accuracy score (Tsoumakas et al.,
2010).
Currently, most algorithms are designed based on none,
one, or a few specific criteria. For instance, the labelwise decomposition approaches (Read et al., 2009) aim at
optimizing the Hamming loss by decomposing the multilabel classification problem into several binary classification problems, one for each possible label. The label powerset approach aims at optimizing the 0/1 loss by treating
each distinct label-set as a unique extended class and reducing multi-label classification to multi-class classification. The probabilistic classifier chain (PCC) (Dembczynski et al., 2010) approach estimates the probability of each
possible label-set given an instance and uses the estimate
to make a Bayes-optimal decision for any loss functions,
while the structured SVM approach (Petterson & Caetano,
2010; 2011) uses different convex surrogates for different
evaluation criteria. Somehow both approaches require either special inference rules or loss maximizers for different
evaluation criteria.
The variety of evaluation criteria calls for a more general
algorithm that can cope with different criteria systematically and automatically. We formalize this need with a
general setup, called cost-sensitive multi-label classification (CSMLC). CSMLC can be viewed as an extension
of the popular setup of cost-sensitive multi-class classification. In CSMLC, we feed the multi-label classification
algorithm with a cost function that quantifies the difference between a predicted label-set and a desired one. A
general CSMLC algorithm operates on the given cost function, with the goal being better performance on that cost
function. Compared with the existing methodology that requires specific design for every new application (criterion),
general CSMLC algorithms can be used to save those design efforts and be easily adopted towards different application needs.
In this paper, we propose a novel algorithm for CSMLC,
called the condensed filter tree (CFT). In contrast to PCC,
the proposed CFT directly takes the criterion into account
as the cost function during training, thereby averting the

Condensed Filter Tree for Cost-Sensitive Multi-Label Classification

need to design a specific inference rule for each new criterion and avoiding the possibly time-consuming inference
step during prediction. Inspired by the rich literature of
cost-sensitive multi-class classification (Domingos, 1999;
Beygelzimer et al., 2008), CFT is derived by first reducing CSMLC to cost-sensitive multi-class classification via
the label powerset approach. Nevertheless, the reduction
leads to exponentially many extended classes, which makes
training, prediction and model representation computationally challenging.
We conquer the challenge of prediction by exploiting treebased models for cost-sensitive multi-class classification.
Tree-based models use a tree structure that is constructed
by binary classifiers to make fast predictions. Then we
achieve time complexity logarithmic with respect to the
number of extended classes, which is linear with respect
to the number of possible labels. Furthermore, we conquer the challenge of model representation by proposing
proper ordering and K-classifier tricks. Interestingly, the
two tricks reveal a strong connection between the CFT
algorithm (which is derived from the label powerset approach) and the label-wise decomposition approaches.
Finally, we conquer the training challenge by modifying
the famous Filter Tree algorithm (Beygelzimer et al., 2008)
for CSMLC. The modification comes from revisiting the
theoretical bound of Filter Tree, which allows the proposed
CFT algorithm to only focus on some key tree nodes for
training efficiency.
We conduct experiments on nine real-world datasets to validate the proposed CFT algorithm. Experimental results
demonstrate that for specific evaluation criteria, CFT is
competitive with special-purpose algorithms, such as PCC
with specifically designed inference rules or the state-ofthe-art MLkNN algorithm (Zhang & Zhou, 2007). For general criteria, for which there is as yet no inference rule for
PCC, CFT can reach significantly better performance. The
results justify the superiority of the proposed CFT for general CSMLC problems.

2. Problem Setup
In a multi-label classification problem, we denote the feature vector by x ∈ Rd and its relevant label set by
Y ⊆ {1, 2, ..., K}, where K is the number of classes.
The label set Y is commonly represented as a label vector, y ∈ {0, 1}K , where y[k] = 1 if and only if k ∈ Y.
Given a dataset D = {(xn , yn )}N
n=1 , which contains N iid
training examples (xn , yn ) drawn from an unknown distribution P, the goal is to design an algorithm that uses D
to find a classifier h : Rd → {0, 1}K in the training stage,
with the hope that h(x) closely predicts y of an unseen x
in the prediction stage when (x, y) is drawn from P.

For evaluating the closeness of the prediction ŷ = h(x),
one of the most common criteria is called the Hamming
PK
1
loss Hamming(y, ŷ) = K
k=1 Jy[k] 6= ŷ[k]K. Note that
the Hamming loss evaluates each label component separately and equally weighted. In addition to the Hamming
loss, there are many other criteria that evaluate the components of ŷ jointly; these include the 0/1 loss, the Rank loss,
the F1 score and the Accuracy score (Tsoumakas et al.,
2010). In this paper, we will use loss to denote the criterion
that shall be minimized, and score to denote the criterion
that shall be maximized.
The variety of criteria calls for a general setup of multilabel classification, called cost-sensitive multi-label classification (CSMLC), which will be the main focus of
this work. CSMLC can be viewed as an extension of
the popular setup of cost-sensitive multi-class classification (Domingos, 1999). In CSMLC, we assume that there
is a known cost function (matrix) C : {0, 1}K ×{0, 1}K →
R, where C(y, ŷ) denotes the cost of predicting (x, y)
as ŷ. The cost matrix is not only part of the prediction
stage by using C(y, h(x)) to evaluate the performance of
any classifier h, but also part of the training stage by feeding C as an additional piece of information to guide the
learning algorithm.
The CSMLC setup meets the goal of optimizing many existing criteria, such as the (per-example) F1 score, the Accuracy score and the Rank loss (Tsoumakas et al., 2010).
Note that the setup above only considers a cost matrix C
indexed by a desired vector y and a predicted vector ŷ.
Thus, the setup cannot fully cover some more complicated
evaluation criteria such the micro-F1 score and the macroF1 score, which are defined on a set of vectors. Studying
the CSMLC setup can be viewed as an intermediate step
toward tackling those complicated criteria in the future.
There are many existing algorithms for tackling the multilabel classification, but they either do not seriously take the
cost matrix (criteria) into account, or only aim at a few specific cost matrices. That is, general algorithms for CSMLC
have not been well studied. One intuitive family of algorithms is label-wise decomposition. For instance, the binary relevance (BR) algorithm (Tsoumakas et al., 2010)
decomposes D = {(xn , yn )}N
n=1 into K binary classification datasets Dk = {(xn , yn [k])}N
n=1 , and trains K
independent binary classifiers hk with Dk for predicting y[k]. One extension of BR is the classifier chain (CC)
algorithm (Read et al., 2009), which takes Dk =
{(zn , yn [k])}N
n=1 and zn = (xn , yn [1], ..., yn [k − 1]), to
train hk . One practical variant of CC, named CC-P, takes
the predicted labels ŷ[k] instead of the true labels y[k] as
the features in zn .
Because CC-P (as well as BR/CC) predicts ŷ[k] separately
by each hk , arguably their main goal is to minimize the

Condensed Filter Tree for Cost-Sensitive Multi-Label Classification

Hamming loss (Tsoumakas et al., 2010). Extending CCP for general CSMLC, however, is non-trivial, because it
is difficult to embed the 2K × 2K possible C(y, ŷ) cost
components into K separate steps of training. One algorithm that solves the difficulty for some specific cost
matrices is the probabilistic classifier chain (PCC) (Dembczynski et al., 2010). PCC avoids the embedding issue
in training by adopting a soft version of CC-P/CC without
any cost information to estimate the conditional probability P (y|x). The probabilistic view allows PCC to interpret
CC as greedily maximizing the 0/1 loss through the chain
rule. During prediction, PCC considers the cost matrix for
making the Bayes-optimal decision, which is based on an
efficient inference rule that has been specifically designed
for the cost matrix.
The potential drawback of PCC is that not only is it nontrivial to estimate the conditional probability but it is also
challenging to design an efficient inference rule for each
cost matrix. Because of the latter challenge, PCC currently
can be used only to exactly tackle the Hamming loss, Rank
loss, and the F1 score (Dembczynski et al., 2010; 2012a;
2011). PCC can also be used with some search-based inference rule to approximately optimize the 0/1 loss (Dembczynski et al., 2012b; Kumar et al., 2013), but not other
criteria in CSMLC.
Another major algorithm, known as label powerset (LP),
reduces multi-label classification to multi-class classification (Tsoumakas et al., 2010). LP treats each unique pattern of the label vector as a single extended class. That is,
the K possible labels are encoded to 2K extended classes
via a bijection function enc : {0, 1}K → {1, ..., 2K }. During training, LP transforms D into Dm = {(xn , cn )}N
n=1 ,
where cn = enc(yn ), and trains a multi-class classifier hm
from Dm . Then during prediction, LP takes h(x) =
enc−1 (hm (x)). Trivially, LP focuses on the 0/1 loss, because the error rate of hm in the reduced problem is equivalent to the 0/1 loss of h. The disadvantage is that the
exponentially many extended classes makes LP infeasible
and impractical in general.
Lo et al. (2011) propose the CS-RAKEL algorithm that optimizes some weighted Hamming loss by extending from
RAKEL (Tsoumakas & Vlahavas, 2007), a representative
algorithm between the label-wise decomposition and label
powerset approaches. Somehow CS-RAKEL is designed
for specific application needs and cannot tackle general
CSMLC problems.
In summary, some related algorithms and their corresponding criteria are shown below. None of them can tackle general CSMLC problems.
Algorithms
CC-P/CC
PCC
LP
CS-RAKEL

Criteria Being Optimized
Hamming loss or 0/1 loss
Hamming loss, F1 score, Rank loss, 0/1 loss
0/1 loss
Weighted Hamming loss

3. Tree Model for CSMLC
Inspired by the connection between CSMLC and the rich
literatures of cost-sensitive classification (Domingos, 1999;
Beygelzimer et al., 2008), we design a general CSMLC algorithm via the connection. Note that LP reduces multilabel classification to multi-class classification to optimize
the 0/1 loss. If we follow the same reduction step but start
from a general CSMLC problem, we end up with a costsensitive classification problem of 2K extended classes
and (implicitly) a 2K × 2K cost matrix. Then any existing
cost-sensitive classification algorithms can be used to solve
CSMLC. We call this preliminary algorithm cost-sensitive
label powerset (CS-LP). As with LP, the exponential number of extended classes presents a computational challenge
for CS-LP. For example, using CS-LP to reduce CSMLC to
the weighted-all pair approach (Beygelzimer et al., 2005)
K
K
requires 2 (22 −1) comparisons for making each prediction.
Interestingly, PCC can be viewed as a special case of using CS-LP to reduce CSMLC to the famous Meta-Cost approach (Domingos, 1999). Meta-Cost estimates the conditional probability during training and then makes the Bayes
optimal decision with respect to a cost matrix in prediction.
Similarly, PCC estimates the probability by CC-P/CC, and
then infers the optimal decision with respect to the cost matrix by the specifically designed inference rule.
We take another route that uses CS-LP to reduce CSMLC to
tree models for cost-sensitive classification (Beygelzimer
et al., 2008). A similar idea based on using the Hamming
loss has been discussed in a blog post (Mineiro, 2011), but
the idea has not been seriously studied for general CSMLC
problems. Tree models form a binary tree by weighted
binary classifiers to conduct cost-sensitive classification.
Each non-leaf node of the tree is a binary classifier for deciding which subtree to go to, and each leaf node represents
a class. Without loss of generality, we assume that the leaf
nodes are indexed orderly by 1, 2, ..., #classes. Making
a prediction for each instance follows the decisions of binary classifiers, starting from the root to the leaf. That is,
only O (log(#classes)) decisions are required for making
each prediction. In CS-LP, tree models result in O(K) time
for each prediction, of the same order as label-wise decomposition approaches.
Nevertheless, the number of nodes on the resulting tree
structure is O(2K ), which poses challenges in representation and training. We first tackle the representation challenge in this section, and then study algorithms for training
the tree model in Section 4.
Proper Ordering. Recall that CS-LP needs a bijective functions enc(·) : {0, 1}K → {1, ..., 2K } for encoding y to c and decoding the predicted class ĉ to the corresponding label vector ŷ. Although a prediction ĉ can

Condensed Filter Tree for Cost-Sensitive Multi-Label Classification
A
B
00

C
01

r

0

10

0
11

00

0

r

1

1

0

01

10

1

0

1

1

r (0, 1-0)

(1, 1- 32 ) (1, 1-0)

0

1

(1, 1- 32 )

11

00

(a)

(0, 23 -0)

(b)

Figure 1. Proper Ordering. (a) Put labels on leaf nodes orderly (b)
Index internal nodes by paths.

be made within O(K) time in the tree model, the encoding function requires a careful design to make both enc
and enc−1 efficient for the 2K possible inputs to those
functions. We first consider the proper ordering trick,
which lets enc(y) = BinaryNumber(y)+1. That is, we
treat each y as a binary string, encode it by computing its
corresponding integer in O(K), and decode accordingly,
as illustrated in Fig. 1(a). Based on proper ordering, if we
let {0, 1} represent the decision {L, R} in each classifier of
the tree, then the label vector y (extended class c) of each
leaf node is equivalent to the sequence of binary decisions
made from the root to the leaf. More generally, we can index each node of the tree by t ∈ {0, 1}k−1 , as shown in
Fig. 1(b), where t is the sequence of binary decisions from
the root to the node on layer k.
K-Classifier Trick. Even with proper ordering, there
are 2K − 1 total internal nodes (classifiers) on the tree. The
exponential number makes representing (and training) classifiers infeasible in practice. One existing idea for feasible
representations is called the 1-classifier trick (Beygelzimer
et al., 2008), which lets all 2K − 1 internal nodes t share
one classifier h(x, t). Nevertheless, using the 1-classifier
trick often requires the classifier to be of sufficient power
to capture different characteristics of different nodes. The
requirement makes the trick less suitable for practical use.
Therefore, we propose a trade-off, K-classifier trick, between using 1 classifier and 2K − 1 classifiers.
The K-classifier trick physically works as follows. After
proper ordering, ŷ[k] corresponds to the prediction made
by one of the nodes on layer k of the tree. In other words,
the purpose of all the nodes located on layer k is similar:
predicting ŷ[k]. The similar purpose allows us to view each
node as a part of a layer classifier of the form hk (x, t),
which takes an instance x and a node index t ∈ {0, 1}k−1
for predicting the k-th component of the label vector. Then
equivalently only K classifiers (one per each layer) are required for representing the tree.
Connection to CC-P. By the proper ordering and the Kclassifier tricks, predicting the extended class ĉ by the tree
from layer 1 (root) to layer K is equivalent to predicting
ŷ[1], ..., ŷ[K] by the K classifiers {h1 , ..., hK } using x and
t. Such a prediction algorithm is exactly the same as those

1

01
0

10

11

1

2 /3

00
1

(a)

01
0

10

11

1

2 /3

(b)

Figure 2. (a) Training of Top-down Tree; (b) Training of Filter
Tree. The cost matrix used is 1−(F1 score). (0/1, w) means the
direction based on proper ordering, and the weight for training the
instance on the node. The thick edge represents the prediction for
the instance by the trained classifier on the parent node.

used in CC-P, which uses the classifiers hk with exactly the
same inputs (the instance x and the predicted labels which
form the node index t in the tree) for predicting the next
label.
The use of the two tricks reveals an interesting connection
between two very different families of approaches: labelwise decomposition can be viewed as a special case of label powerset (in prediction). In short, label powerset with
the tree model, proper ordering and K-classifiers tricks is
equivalent to CC-P for prediction. Thus, by studying the
role of the cost matrix during training, we can systematically extend CC-P to be cost-sensitive. Next, we will discuss how to train the K classifiers subject to the cost matrix
efficiently.

4. Training of Tree Model
There are two major algorithms for training the binary classifiers in the tree, Top-down Tree and Filter Tree (Beygelzimer et al., 2008).
Top-down Tree (TT). Top-down Tree trains classifiers
from layer 1 (root) to layer K. Formally, for each internal
node t on layer k, denote its left child as t0 and right child
as t1 on layer (k +1). Define t∗ as the leaf node (prediction) with the minimum cost on the subtree Tt rooted at t.
Then for each training example (xn , yn ) that reaches t during top-down training, we form a example ((xn , t), bn , wn )
to train the weighted classifier hk , where the label bn = arg mini∈{0,1} C (yn , t∗i ) represents the optimal
decision, and the weight wn = |C (yn , t∗0 ) − C (yn , t∗1 ) |
represents the cost difference. Then the training examples
are split to two sets based on the decision of the trained
classifier hk , and are used to train the child nodes t0 and t1 ,
respectively. All the training examples are taken to train the
root classifier, and the whole tree is trained recursively with
such divide-and-conquer steps. Note that as illustrated in
Fig. 2(a), each training example (xn , yn ) only contributes
to training the nodes that are on the path from the root to
the predicted leaf of the example.

Condensed Filter Tree for Cost-Sensitive Multi-Label Classification

The time complexity of Top-down Tree for CSMLC is the
same as CC-P. In fact, if we take the Hamming loss, then
1
wn = K
is the same for each instance on every node in
the k-th layer, (xn , tn ) = (xn , ŷn [1], ..., ŷn [k − 1]) and
bn = yn [k]. Thus, Top-down Tree with the Hamming loss
is equivalent to CC-P. That is, general Top-down Tree can
be viewed as a systematic extension of CC-P for general
CSMLC.
Uniform Filter Tree (UFT). It is known that Top-down
Tree may suffer from the weaker theoretical guarantee (Beygelzimer et al., 2008). An alternative algorithm is
called Filter Tree, which trains the classifiers in a bottomup manner starting from the last non-leaf layer, and each
example (xn , yn ) is used to train all nodes. As illustrated
in Fig. 2(b), the last non-leaf layer of classifiers is trained
by forming weighted examples based on the better leaf of
the two. After training, each node on the last layer decides the winning leaf of the two by predicting on xn . Then
the winning labels form the new “leaves” of a smaller filter tree, and the classifiers on the upper layer are trained
similarly. Due to the bottom-up manner, on layer k, Filter
Tree considers all the predictions from layer k + 1 to layer
K. That is, Filter Tree split one original training example
to 2k−1 examples, one for each possible node t, to train all
of the 2k−1 nodes on the layer. When k is large, training on
layer k can thus be challenging. Compared with Top-down
Tree, Filter Tree is less efficient by considering all training
examples for each node, but enjoys a stronger theoretical
guarantee (Beygelzimer et al., 2008).
One possibility for training Filter Tree efficiently is to only
train a few nodes for each layer, with the hope that other
nodes can also perform decently because of the classifiersharing in the K-classifier trick. The original Filter Tree
work (Beygelzimer et al., 2008) suggests one simple approach that splits one example to train M uniformly chosen nodes on the k-th layer to approximate the full training
of 2k−1 nodes. We call this algorithm Uniform Filter Tree
for CSMLC.
Condensed Filter Tree (CFT). In Filter Tree, there are
2K possible traversing paths from the root to the leaves for
each instance; however, many of them are seldom needed
if we have reasonably good classifiers, such as paths that
result in high costs. Therefore, we can shift our focus to
the important nodes on each layer instead of uniform sampling for each instance. Next, we revisit the regret bound
of Filter Tree, and show that the bound can be revised to
focus on a key path of the nodes on the tree.
In CSMLC, for a feature vector x and some distribution
P|x for generating the label vector y, the regret rg of a
classifier h on x is defined as

For a distribution that generates weighted binary examples
(x, b, w), the regret can be defined similarly by using w as
the cost of a wrong prediction (of b) and 0 as the cost of a
correct prediction.
Let y∗ = arg minỹ Ey∼P |x C(y, ỹ) be the ideal prediction
of x under P. When t0 is an ancestor (prefix) of t on the
tree, denote ht0 , ti as a list (path) that contains the nodes on
the path from node t0 to t. We call hr, y∗ i the ideal path
of the tree for x, where r is the root of the tree. Similarly,
for each node t, we can define the ideal path of the subtree Tt rooted at t. Beygelzimer et al. (2008) prove that for
Filter Tree, the CSMLC regret of any tree-based classifier
is upper-bounded by the total regret of all the nodes on the
tree. Next, we show that the total regret of the nodes on the
ideal path can readily be used to upper-bound the CSMLC
regret.
Theorem 1. Under the proper ordering and K-classifier
tricks, for each x and the multi-label classifier h formed by
chaining K binary classifiers (h1 , ..., hK ) as in the prediction procedure of Filter Tree, the regret rg(h, P) is no more
than


X
Jhk (x, t) 6= y[k]Krg hk (x,t),FTt (P,hk+1 ,...,hK ) ,
t∈hr,y∗ i

where k denotes the layer that t is on, and
FTt (P, hk+1 , ..., hK ) represents the procedure that
generates weighted examples (x, b, w) to train the node at
index t based on sampling y from P|x and considering the
predictions of classifiers in the lower layers.
Proof. For each node t on layer k, hk directs the prediction
procedure to move to either the node t0 or t1 . Without
loss of generality, assume hk (x, t) = 1. We denote t̂ as the
prediction (leaf) on x when starting at node t. For each leaf
node ỹ, let C̄(ỹ) ≡ Ey∼P|x C(y, ỹ). Then the node regret
rg(t) is simply C̄(t̂1 ) − mini∈{0,1} C̄(t̂i ).
In addition to the regret of nodes, we also define the
regret of the subtree Tt rooted at node t. The regret of the subtree Tt is as defined as the regret of the
predicted path (vector) t̂ within the subtree Tt , that is,
rg(Tt ) = C̄(t̂) − C̄(t∗ ) , where t∗ denotes the optimal
prediction (leaf node) in the subtree Tt . By this definition,
rg(h, P) is simply rg(Tr ).
The proof can be made by replacing the total regret with
rg(Tr ) in the original Filter Tree work (Beygelzimer et al.,
2008). Due to the space limit, we omit the complete proof
here.

In Theorem 1, the bound is related to certain nodes on
the ideal path for each training example. The bound inrg(h, P) = Ey∼P|x [C(h(x), y)]−min Ey∼P|x [C(g(x), y)] . spires us to first consider using each training example to
g

Condensed Filter Tree for Cost-Sensitive Multi-Label Classification
r

0
1

t

0
t0

0

t1

1

t10

t∗

t∗1

t̂1

Figure 3. The thick edge represents the predition of the corresponding parent node. The ideal path is hr, t∗ i and t is the first
mis-classified node; the ideal path of subtree Tt1 is ht1 , t∗1 i and
t10 is the first mis-classified node. Both nodes t and t10 are on
the predicted path hr, t̂1 i.

only train the K nodes in its ideal path to get the classifiers h1 , .., hK for each layer. Then we can find the uppermost mis-classified node t on the ideal path for each
example (xn , yn ). Without loss of generality, assume t
is on the layer k, with y[k] = 0 and hk (x, t) = 1.
According to Theorem 1, we could decrease the regret
rg(h, P) (or rg(Tr )) by decreasing the node regret rg(t) =
C̄(t̂1 ) − mini∈{0,1} C̄(t̂i ), which can be done by decreasing C̄(t̂1 ).1 Because C̄(t∗1 ) is a constant, decreasing C̄(t̂1 )
is equivalent to decreasing the regret, rg(Tt1 ) = C̄(t̂1 ) −
C̄(t∗1 ), of the subtree Tt1 . We can then recursively adopt
the above procedure to optimize the subtree regret rg(Tt1 )
as shown in Fig. 3.
The procedure suggests decreasing the regret on hr, ti and
ht, t̂i, the predicted path of xn . Therefore, the next key
path for xn that should be included for training is its predicted path. That is, we can now train Filter Tree by adding
the predicted path for each xn . We call the resulting algorithm Condensed Filter Tree, as shown in Algorithm 1. The
path-adding step can be repeated to further zoom into the
key nodes. The number of adding step can be treated as a
parameter M , and will be further discussed in Section 5.
In summary, we derive three efficient approaches for general CSMLC with trees: TT (a systematic extension of
CC-P), UFT and CFT. Next, we compare them with other
existing algorithms by experiments.

5. Experiment
We conduct the experiments of different evaluation criteria
on nine real-world datasets2 (Tsoumakas et al., 2011; Read,
2012). In the experiments, we take three kinds of algorithms in our comparison: (a) the label-wise decomposition
approaches, including classifier chain (CC), ensemble clas1

Since t̂0 relates to regret of other nodes on the ideal path of
t, we cannot easily increase C̄(t̂0 ) to decrease rg(t).
2
CAL500, emotions, enron, imdb, medical, scene,
slash, tmc and yeast.

Algorithm 1 Condensed Filter Tree for CSMLC
p
N
1: D = {(xn , yn )}N
n=1 ; D = {((xn , yn ), yn )}n=1
2: for m = 1 to M iterations do
3:
for each layer k from layer K to root do
4:
Dk = ∅
5:
for each instance ((xn , ỹn ), yn ) ∈ Dp do
6:
t = (ỹn [1], ..., ỹn [k]); zn = (xn , t)
7:
bn = arg mini∈{0,1} C(yn , t̂i )
8:
wn =|C(yn , t̂1 )-C(yn , t̂0 )|
9:
Dk ← Dk ∪ (zn , bn , wn )
10:
end for
11:
hk ← train(Dk )
12:
end for
13:
if m < M then
14:
for each instance (xn , yn ) ∈ D do
15:
ŷn = predict(h1 , ..., hK , xn )
16:
Dp ← Dp ∪ ((xn , ŷn ), yn )
17:
end for
18:
end if
19: end for

sifier chain (ECC), and probabilistic classifier chain (PCC);
(b) the tree-based models, including top-down tree (TT),
uniform filter tree (UFT) and condensed filter tree (CFT);
(c) a state-of-the-art algorithm that does not explicitly take
cost into account, MLkNN (Zhang & Zhou, 2007). We
first consider
Hamming
loss, Rank

P three cost matrices:
loss=
Jŷ[i] < ŷ[j]K+ 12 Jŷ[i] = ŷ[j]K and F1 score=
y[i]>y[j]
2ky∩ŷk1
kyk1 +kŷk1 . The

three matrices corresponds to known efficient inference rules for PCC (Dembczynski et al., 2010;
2011). Then we take other criteria for comparison in Section 5.3.
We couple PCC with L2-regularized logistic regression and
other algorithms with linear support vector machines implemented in LIBLINEAR (Fan et al., 2008). For MLkNN,
we use the implementation in Mulan (Tsoumakas et al.,
2011). In each run of the experiment, we randomly sample 50% of the dataset for training and reserve the rest for
testing. For UFT and CFT, we restrict the maximum M to
8 for efficiency. For other parameters of each algorithm,
we use cross-validation on the training set to search the
best choice. Finally, Tables 1, 2 and 3 list the results for
the three cost matrices, respectively, with the mean and
the standard error over 40 different random runs, and the
best result of each dataset is bolded. We also compare CFT
with other algorithms based on the t-test at 95% confidence
level. The number of datasets that CFT wins, ties and losses
are shown in Table 4.

Condensed Filter Tree for Cost-Sensitive Multi-Label Classification
Table 1. The result of Hamming loss (the best (lowest) ones are marked in bold)
Dataset
CAL.
emo.
enron
imdb
medical
scene
slash
tmc
yeast

CC
0.1376 ± 0.002
0.2613 ± 0.029
0.0465 ± 0.001
0.0808 ± 0.000
0.0109 ± 0.001
0.1118 ± 0.004
0.0418 ± 0.001
0.0571 ± 0.000
0.2107 ± 0.003

Dataset
CAL.
emo.
enron
imdb
medical
scene
slash
tmc
yeast

CC
1516.0 ± 60.4
2.697 ± 0.315
44.190 ± 0.736
21.312 ± 0.299
5.882 ± 0.595
1.022 ± 0.053
6.603 ± 0.132
7.704 ± 0.089
9.596 ± 0.224

ECC
0.1374 ± 0.002
0.2501 ± 0.022
0.0466 ± 0.001
0.0713 ± 0.000
0.0113 ± 0.001
0.0971 ± 0.004
0.0383 ± 0.000
0.0565 ± 0.000
0.2009 ± 0.004

MLkNN
0.1379 ± 0.002
0.2122 ± 0.012
0.0540 ± 0.001
0.0714 ± 0.000
0.0176 ± 0.001
0.0942 ± 0.004
0.0514 ± 0.001
0.0669 ± 0.000
0.1981 ± 0.003

PCC
0.1370 ± 0.002
0.2297 ± 0.011
0.0462 ± 0.001
0.0714 ± 0.000
0.0110 ± 0.001
0.0962 ± 0.003
0.0386 ± 0.001
0.0576 ± 0.000
0.2006 ± 0.003

TT(CC-P)
0.1375 ± 0.002
0.2435 ± 0.015
0.0467 ± 0.001
0.0715 ± 0.000
0.0108 ± 0.001
0.0980 ± 0.003
0.0388 ± 0.001
0.0575 ± 0.000
0.2000 ± 0.002

UFT
0.1489 ± 0.005
0.2222 ± 0.014
0.0551 ± 0.001
0.0715 ± 0.000
0.0119 ± 0.001
0.1032 ± 0.003
0.0375 ± 0.001
0.0574 ± 0.000
0.2008 ± 0.002

CFT
0.1368 ± 0.002
0.2138 ± 0.009
0.0467 ± 0.001
0.0715 ± 0.000
0.0102 ± 0.001
0.1004 ± 0.003
0.0383 ± 0.001
0.0572 ± 0.000
0.2013 ± 0.003

Table 2. The result of Rank loss (the best (lowest) ones are marked in bold)
ECC
1432.6 ± 39.0
2.350 ± 0.299
42.625 ± 0.775
22.559 ± 0.283
5.800 ± 0.564
0.922 ± 0.030
6.467 ± 0.131
7.306 ± 0.139
9.208 ± 0.143

MLkNN
1408.9 ± 21.3
1.906 ± 0.120
55.959 ± 1.386
24.396 ± 2.345
5.826 ± 0.565
0.853 ± 0.046
8.259 ± 0.259
5.329 ± 0.079
9.735 ± 0.247

PCC
967.93 ± 12.57
1.763 ± 0.102
24.379 ± 0.557
12.620 ± 0.044
2.942 ± 0.327
0.696 ± 0.024
3.835 ± 0.080
3.952 ± 0.034
8.753 ± 0.140

TT
965.49 ± 11.20
1.868 ± 0.134
25.144 ± 0.704
12.665 ± 0.047
3.611 ± 0.431
0.744 ± 0.029
4.358 ± 0.152
3.924 ± 0.042
8.752 ± 0.138

UFT
968.40 ± 12.03
1.714 ± 0.131
25.622 ± 0.576
12.638 ± 0.046
2.812 ± 0.291
0.764 ± 0.026
3.965 ± 0.065
3.912 ± 0.040
8.813 ± 0.148

CFT
963.13 ± 10.99
1.632 ± 0.093
24.907 ± 0.625
12.637 ± 0.049
3.602 ± 0.455
0.739 ± 0.028
4.289 ± 0.074
3.894 ± 0.040
8.747 ± 0.118

Table 3. The result of F1 score (the best (highest) ones are marked in bold)
Dataset
CAL.
emo.
enron
imdb
medical
scene
slash
tmc
yeast

CC
0.319 ± 0.028
0.416 ± 0.087
0.538 ± 0.010
0.256 ± 0.001
0.784 ± 0.017
0.687 ± 0.012
0.489 ± 0.012
0.684 ± 0.003
0.622 ± 0.007

ECC
0.368 ± 0.015
0.489 ± 0.068
0.547 ± 0.011
0.157 ± 0.015
0.779 ± 0.014
0.701 ± 0.010
0.496 ± 0.007
0.693 ± 0.003
0.634 ± 0.007

MLkNN
0.318 ± 0.010
0.579 ± 0.030
0.385 ± 0.021
0.001 ± 0.000
0.523 ± 0.038
0.655 ± 0.023
0.136 ± 0.054
0.606 ± 0.007
0.607 ± 0.012

PCC
0.460 ± 0.006
0.639 ± 0.018
0.574 ± 0.007
0.352 ± 0.015
0.817 ± 0.015
0.735 ± 0.011
0.577 ± 0.008
0.714 ± 0.002
0.638 ± 0.008

TT
0.447 ± 0.006
0.550 ± 0.061
0.580 ± 0.009
0.371 ± 0.001
0.789 ± 0.021
0.721 ± 0.010
0.517 ± 0.011
0.709 ± 0.002
0.639 ± 0.005

UFT
0.454 ± 0.005
0.619 ± 0.029
0.545 ± 0.011
0.358 ± 0.001
0.797 ± 0.011
0.667 ± 0.007
0.540 ± 0.005
0.687 ± 0.002
0.649 ± 0.006

CFT
0.473 ± 0.004
0.637 ± 0.016
0.598 ± 0.010
0.374 ± 0.001
0.796 ± 0.014
0.717 ± 0.010
0.514 ± 0.007
0.714 ± 0.002
0.649 ± 0.006

Table 5. The result of Acc. score, and Comp. score (best ones are marked in bold)
CAL.
emo.
enron
imdb
medical
scene
slash
tmc
yeast

Accuracy(↑)
PCC-F1
CFT
0.303 ± 0.008
0.315 ± 0.004
0.534 ± 0.021
0.535 ± 0.015
0.453 ± 0.009
0.476 ± 0.009
0.242 ± 0.010
0.268 ± 0.001
0.783 ± 0.015
0.764 ± 0.018
0.676 ± 0.011
0.669 ± 0.010
0.511 ± 0.009
0.481 ± 0.006
0.613 ± 0.004
0.614 ± 0.002
0.518 ± 0.012
0.539 ± 0.006

Table 4. CFT versus the other algorithms based on t-test at 95%
confidence level (#win/#tie/#loss)
criteria
Ham.
Rank.
F1.
Total

CC
7/1/1
9/0/0
9/0/0
25/1/1

ECC
2/4/3
9/0/0
9/0/0
20/4/3

MLkNN
5/1/3
9/0/0
9/0/0
23/1/3

PCC
4/3/2
3/2/4
4/2/3
11/7/9

TT
5/2/2
4/5/0
7/1/1
16/8/3

UFT
6/2/1
6/1/2
6/2/1
18/5/4

5.1. Cost-insensitive versus Cost-sensitive
Table 1 compares all the algorithms based on the Hamming
loss. As discussed in Section 4, CC-P is equivalent to TT
with Hamming loss. In Table 1, the five algorithms that can
reach the best performance are ECC, MLkNN, PCC, UFT
and CFT. Moreover, ECC successfully improves the performance of CC. The state-of-the-art algorithm, MLkNN,
often achieves the best results. When looking at Table 4 for
t-test results, CFT is competitive to ECC and PCC, while
often being better than MLkNN.
For the other two criteria, as shown in Tables 2 and 3, the
algorithms that do not consider the cost explicitly, such as
CC, ECC and MLkNN, are generally worse than the cost-

Composite Score(↑)
PCC-Ham or F1
CFT
−0.362 ± 0.012
−0.302 ± 0.013
−0.566 ± 0.100
−0.460 ± 0.063
0.300 ± 0.017
0.351 ± 0.012
−0.263 ± 0.055
−0.096 ± 0.001
0.758 ± 0.016
0.747 ± 0.018
0.150 ± 0.036
0.170 ± 0.022
0.263 ± 0.012
0.277 ± 0.011
0.402 ± 0.007
0.419 ± 0.004
−0.398 ± 0.019
−0.376 ± 0.019

sensitive algorithms. The results demonstrate the importance and effectiveness of properly considering the cost information in the algorithm.
5.2. Comparison with Tree-based Algorithms
In Tables 1, 2, 3 and 4, when comparing CFT with TT,
CFT wins on 16 and ties on 8 of the 27 cases by t-test.
The results justify the importance of bottom-up training of
the tree model. When comparing UFT with CFT, CFT is
better than UFT on 18 and ties on 5 out of 27 cases by ttest. The results demonstrate the effectiveness of focusing
on key paths (nodes).
0.7
0.65

F1 Score

Dataset

0.6
CFT−test
UFT−test
CFT−train
UFT−train

0.55
0.5

2

4

6

Number of paths (M)

8

Condensed Filter Tree for Cost-Sensitive Multi-Label Classification

We further study UFT and CFT for varying M . We show
the result of F1 score on emotions, while observing similar behaviors across other datasets and criteria. When number of paths increases, both the training and testing performance of CFT and UFT are improved. Moreover, CFT
converges to a better F1 score than UFT as M increases,
which explains its better performance during testing.
While CFT is usually better than UFT, on medical and
slash, CFT loses to UFT in Tables 2 and 3. We study the
reasons and find that the cause is overfitting. For instance,
the training Rank loss of CFT on medical is 0.083, which
is much smaller than the UFT result of 0.264. That result
implies that CFT indeed optimizes the desired evaluation
criteria during training, but the focus on key paths could
suffer worse generalization in a few datasets. A preliminary
study shows that a mixture of CFT and UFT is less prone
to overfitting.
5.3. Comparison with PCC and CFT
For the Hamming loss, the Rank loss and the F1 score,
the exact inference algorithms of PCC have been proposed (Dembczynski et al., 2010; 2011). From Tables 1,
2, 3 and 4, PCC and CFT are competitive to each other on
the three criteria, having similar number of winning and
losing cases.
To demonstrate the full ability of CFT, we consider two
other criteria which there is no inference rule (yet) for PCC,
including Accuracy3 (Boutell et al., 2004; Tsoumakas
et al., 2010), and a composite score from the F1 score
and the Hamming loss in Table 5. The definitions of the
ky∩ŷk1
, and Composite Score=
criteria are Accuracy = ky∪ŷk
1
F1 Score−5×HAM Loss.
Here we use the approximate inference rules for PCC. For
the Accuracy score, we couple PCC with the inference rule
of the F1 score in view of the similarity in the formula.
For the Composite score, which considers the F1 score and
the Hamming loss concurrently, we run PCC with either
the inference rule of the F1 score or the inference rule of
the Hamming loss, and optimistically report the best one in
Table 5.
Table 5 can be summarized as follows. Due to the similarity in the formula, CFT and PCC-F1 reach similar results
for the Accuracy score. For the Composite score, which is
similar to neither the F1 score nor the Hamming loss, PCC
is much worse than CFT.
When K is small, PCC can use exhaustive search to enumerate 2K possible ŷ and locate the Bayes optimal ŷ. We
further list the performance of this PCC-exhaust approach
for emotions, scene and yeast, which are of no more
3

α-Accuracy with α = 1

than 14 labels.
Acc.(↑)
Comp.(↑)
emo. scene yeast emo. scene yeast
Apprx. 0.534 0.676 0.518 -0.566 0.150 -0.398
Exhau. 0.530 0.709 0.535 -0.570 0.176 -0.383
Infer.

By the exhaustive inference, the performance of PCC is significantly improved in most cases. The good performance
highlights the importance of exact and efficient inference
rules for PCC. Nevertheless, if the desired evaluation criteria are complicated, it is non-trivial to design exact and efficient inference rules. When comparing PCC-exhaust with
CFT, we see that CFT wins on 3 cases, ties on 1 case and
loses on 2 cases. Thus, the efficient CFT is quite competitive with the inefficient PCC-exhaust in performance.

6. Conclusion
We tackle the general cost-sensitive multi-label classification problem without any specific subroutine for different
evaluation criteria, which meets the demands in real-world
applications. We proposed the condensed filter tree (CFT)
algorithm by coupling several tools and ideas: the label
powerset approach for reducing to cost-sensitive classification, the tree-based algorithms for cost-sensitive classification, the proper-ordering and K-classifier tricks that utilize the structural property of multi-label classification, and
the theoretical bound to locate the key tree nodes (paths)
for training. The resulting CFT is as efficient as the common label-wise decomposition approaches in training and
prediction, with respect to the number of possible labels.
Experimental results demonstrate that CFT is competitive
with leading approaches for multi-label classification, and
usually outperforms those approaches on the evaluation criteria that those approaches are not designed from.
CFT can currently handle evaluation criteria defined by a
desired label vector and a predicted label vector. We can
view CFT as the first step towards tackling more complicated evaluation criteria, which shall be an important future
research direction.

7. Acknowledgement
We thank Profs. Yuh-Jye Lee, Chih-Jen Lin, Shou-De
Lin, Chi-Jen Lu, Hung-Yi Lo, the anonymous reviewers,
and the members of the NTU Computational Learning Lab
for valuable suggestions. This work is mainly supported
by National Science Council (NSC 101-2628-E-002-029MY2) of Taiwan.

References
Beygelzimer, A., Dani, V., Hayes, T., Langford, J., and
Zadrozny, B. Error limiting reductions between classification tasks. In Proceedings of the 22nd International

Condensed Filter Tree for Cost-Sensitive Multi-Label Classification

Conference on Machine Learning, 2005.
Beygelzimer, A., Langford, J., and Ravikumar, P. Error
correcting tournaments, 2008. URL http://arxiv.
org/abs/0902.3176.
Boutell, M. R., Luo, J., Shen, X., and Brown, C. M. Learning multi-label scene classification. Pattern Recognition,
2004.
Dembczynski, K., Cheng, W., and Hüllermeier, E. Bayes
optimal multilabel classification via probabilistic classifier chains. In Proceedings of the 27th International
Conference on Machine learning, 2010.
Dembczynski, K., Waegeman, W., Cheng, W., and
Hüllermeier, E. An exact algorithm for f-measure maximization. In Advances in Neural Information Processing
Systems 24. 2011.
Dembczynski, K., Kotlowski, W., and Hüllermeier, E. Consistent multilabel ranking through univariate losses. In
Proceedings of the 29th International Conference on
Machine learning, 2012a.
Dembczynski, K., Waegeman, W., and Hüllermeier, E. An
analysis of chaining in multi-label classification. In Proceedings of the 20th European Conference on Artificial
Intelligence, 2012b.
Domingos, P. Metacost: a general method for making classifiers cost-sensitive. In Proceedings of the fifth ACM
SIGKDD international conference on Knowledge discovery and data mining, 1999.
Elisseeff, A. and Weston, J. A kernel method for multilabelled classification. In Advances in Neural Information Processing Systems 14, 2002.
Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and
Lin, C.-J. LIBLINEAR: a library for large linear classification. Journal of Machine Learning Research, 2008.
Kumar, A., Vembu, S., Menon, A. K., and Elkan, C.
Beam search algorithms for multilabel learning. Machine Learning, 2013.
Lo, H.-Y., Wang, J.-C., Wang, H.-M., and Lin, S.-D. Costsensitive multi-label learning for audio tag annotation
and retrieval. IEEE Transactions on Multimedia, 2011.
Mineiro, P.
Cost sensitive multi label:
an
observation,
2011.
URL http://www.
machinedlearnings.com/2011/05/
cost-sensitive-multi-label-observation.
html.
Petterson, J. and Caetano, T. S. Reverse multi-label learning. In Advances in Neural Information Processing Systems 23. 2010.

Petterson, J. and Caetano, T. S. Submodular multi-label
learning. In Advances in Neural Information Processing
Systems 24. 2011.
Read, J. Meka: a multi-label extension to weka, 2012. URL
http://meka.sourceforge.net.
Read, J., Pfahringer, B., Holmes, G., and Frank, E. Classifier chains for multi-label classification. In Proceedings
of the European Conference on Machine Learning and
Knowledge Discovery in Databases, 2009.
Srivastava, A.N. and Zane-Ulman, B. Discovering recurring anomalies in text reports regarding complex space
systems. In IEEE Aerospace Conference, 2005.
Tsoumakas, G. and Vlahavas, I. Random k-labelsets: an
ensemble method for multilabel classification. In Machine Learning: the European Conference on Machine
Learning. 2007.
Tsoumakas, G., Katakis, I., and Vlahavas, I. Mining multilabel data. In Data Mining and Knowledge Discovery
Handbook. Springer US, 2010.
Tsoumakas, G., Spyromitros-Xioufis, E., Vilcek, J., and
Vlahavas, I. Mulan: a java library for multi-label learning. Journal of Machine Learning Research, 2011.
Tsoumakas, G., Zhang, M.-L., and Zhou, Z.-H. Introduction to the special issue on learning from multi-label
data. Journal of Machine Learning Research, 2012.
Turnbull, D., Barrington, L., Torres, D. A., and Lanckriet,
G. R. G. Semantic annotation and retrieval of music and
sound effects. IEEE Transactions on Audio, Speech and
Language Processing, 2008.
Zhang, M.-L. and Zhou, Z.-H. ML-KNN: A lazy learning
approach to multi-label learning. Pattern Recognition,
2007.

