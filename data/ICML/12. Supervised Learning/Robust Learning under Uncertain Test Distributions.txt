Robust Learning under Uncertain Test Distributions:
Relating Covariate Shift to Model Misspecification
Junfeng Wen1
JUNFENG . WEN @ UALBERTA . CA
Chun-Nam Yu2
CHUN - NAM . YU @ ALCATEL - LUCENT. COM
Russell Greiner1
RGREINER @ UALBERTA . CA
1
Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8 CANADA
2
Bell Labs, Alcatel-Lucent, 600 Mountain Avenue, Murray Hill, NJ 07974 USA

Abstract
Many learning situations involve learning the
conditional distribution ppy|xq when the training
instances are drawn from the training distribution ptr pxq, even though it will later be used to
predict for instances drawn from a different test
distribution pte pxq. Most current approaches focus on learning how to reweigh the training examples, to make them resemble the test distribution. However, reweighing does not always help,
because (we show that) the test error also depends on the correctness of the underlying model
class. This paper analyses this situation by viewing the problem of learning under changing distributions as a game between a learner and an adversary. We characterize when such reweighing
is needed, and also provide an algorithm, robust
covariate shift adjustment (RCSA), that provides
relevant weights. Our empirical studies, on UCI
datasets and a real-world cancer prognostic prediction dataset, show that our analysis applies,
and that our RCSA works effectively.

1. Introduction
Traditional machine learning often explicitly or implicitly
assumes that the data used for training a model come from
the same distribution as that of the test data. However, this
assumption is violated in many real-world applications. For
example, biostatisticians often try to collect a large and diverse training set, perhaps for building prognostic predictors for patients with different diseases. When clinicians
deploy these predictors, they do not know whether the local test patient population will be even close to that training
population. Sometimes we can collect a small sample from
the target test population, but in most cases we have nothProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

ing more than weak prior knowledge about how the test
distribution may shift, such as anticipated changes in gender ratio or age distribution. It is useful to build predictors
that are robust against such changes in test distributions.
In this work, we investigate the problem of distribution change under covariate shift assumption (Shimodaira,
2000), in which both training and test distributions share
the same conditional distribution ppy|xq, while their
marginal distributions, ptr pxq and pte pxq, are different. To
correct the shifted distribution, major efforts have been
dedicated to importance reweighing (Quionero-Candela
et al., 2009; Sugiyama & Kawanabe, 2012). However,
reweighing methods will not necessarily improve the performance in test set, as prediction accuracy under covariate
shift is also dependent on model misspecification (White,
1981). Fig. 1 shows three examples of misspecified models, where we are considering the model class of straight
lines of the form y ‚Äú ax`b, for x P r¬¥1.5, 2.5s. In Fig. 1(a),
no straight line is a good fit for the cubic curve across
the whole interval, but Model 2 fits the curve reasonably
well in the small interval r¬¥0.5, 0.5s. If training data is
spread all over r¬¥1.5, 2.5s while test data concentrates on
r¬¥0.5, 0.5s, improvement via reweighing could be significant. The situation in Fig. 1(b) is different: although the
true model is a curve and not a straight line, the best linear
fit is no more than  away from the value of the true model.
In this case, no matter what test distributions we see in the
interval r¬¥1.5, 2.5s, the regression loss of the best linear
model will never be more than  from the Bayes optimal
loss. In Fig. 1(c), the true model is a straight line except at
x ‚Äú 0; perhaps this outlier is a cancer patient whose tumour
spontaneously disappeared on its own. Unless the test distribution concentrates most of its mass at x ‚Äú 0, the straight
line fit learned from the training data over the interval will
still be a very good predictor. Sometimes we can rule out
this type of covariate shift through prior knowledge. If such
outliers are extremely rare during training time, we would
not expect the test population to have many such patients.
Reweighing will not help much in cases 1(b) and 1(c).

Robust Learning under Uncertain Test Distributions

14

12

5

True model
Model 1
Model 2

4

3.5

True model
Best linear fit

True model
3

2.5

3

‚Üì

Output

Output

10

8

6

Œµ

2

‚Üë

2

Output

16

1

1.5

1

4
0
0.5

2
‚àí1

0

0

‚àí2
‚àí1.5

‚àí1

‚àí0.5

0

0.5

1

1.5

2

2.5

‚àí2
‚àí1.5

‚àí1

‚àí0.5

0

Input

0.5

1

1.5

2

2.5

‚àí0.5
‚àí1.5

‚àí1

Input

(a) Large misspecification.

(b) Small misspecification.

‚àí0.5

0

0.5

1

1.5

2

2.5

Input

(c) Single point misspecification.

Figure 1. Three different scenarios of model misspecifications.

In this paper, we relate covariate shift to model misspecification and investigate when reweighing can help a learner
deal with covariate shift. We introduce a game between a
learner and an adversary that performs robust learning. The
learner chooses a model Œ∏ from a set Œò to minimize the
loss, while the adversary chooses a reweighing function Œ±
from a set A to create new test distributions to maximize the
loss. There are two major contributions in this paper: First,
we provide an improved understanding of the relation between covariate shift and model misspecification through
this game analysis. If the learner can find a Œ∏ that minimizes the loss against any possible Œ± that the adversary
can play, then it is not necessary to perform reweighing
against covariate shift scenarios represented by A. Second, we provide a systematic method for checking a model
class Œò against different covariate shift scenarios, such as
changing gender ratio and age distributions in the prognostic predictor example, to help user decide whether importance reweighing would be beneficial.
For practical use, our method can be used to decide if the
model class is sufficient against shifts that are close to a test
sample; or robust against a known range of potential shifts
if test sample is unavailable. If the model class is insufficient, we can consider different ways to deal with covariate
shifts, such as reweighing using unlabelled test samples, or
exploring a different model class for the problem.

2. Related Work
Our work is inspired by GruÃànwald & Dawid (2004), who
interpret maximum entropy as a game between an adversary and a learner on minimizing the worst case expected
log loss. Teo et al. (2008) and Globerson & Roweis (2006)
also consider an adversarial scenario under changing test
set conditions, but they are concerned with corruption or
deletion of features rather than covariate shift.
Many results on covariate shift correction involve density
ratio estimation. Shimodaira (2000) showed that, given covariate shift and model misspecification, reweighing each
instance with pte pxq{ptr pxq is asymptotically optimal for

log-likelihood estimation, where ptr pxq and pte pxq are assumed to be known or estimated in advance. Sugiyama
& MuÃàller (2005) extended this work by proposing an
(almost) unbiased estimator for L2 generalization error.
There are several works focusing on minimizing different types of divergence between distributions in the literature (Kanamori et al., 2008; Sugiyama et al., 2008; Yamada et al., 2011). Kernel mean matching (KMM) (Huang
et al., 2007) reweighs instances to match means in a
RKHS (SchoÃàlkopf & Smola, 2002). Our work and some
other approaches (Pan et al., 2009) adapt the idea of matching means of the datasets to correct shifted distribution,
but we extend their approaches from a two-step optimization to a game framework that jointly learns a model
and weights with covariate shift correction. Some other
approaches (Zadrozny, 2004; Bickel et al., 2007; 2009;
Storkey & Sugiyama, 2007) consider different generative
models for special cases of covariate shift mechanisms.
Besides these approaches, there are many other works focusing on the theoretical analysis of statistical learning
bounds for covariate shift. Ben-David et al. (2007) gave
a bound on L1 generalization error given the presence of
mismatched distributions. Analyses on other forms of error
were also introduced in the literature (Shimodaira, 2000;
Sugiyama & MuÃàller, 2005; Cortes et al., 2010). However,
most of these analyses neglect the effect of model misspecification (White, 1981). Apart from Shimodaira (2000) who
pointed out a link between covariate shift and model misspecification with some quantitative evidence, and Huang
et al. (2007) who observed that simpler models tend to benefit more from density ratio correction, few have addressed
the question of determining when reweighing helps versus
when it is not needed. In this paper, we show the relationship between covariate shift and model misspecification.

3. Learning Under Uncertain Test
Distributions as a Game
Suppose we are given an i.i.d. (independent and identically distributed) training sample tpx1 , y1 q, ¬® ¬® ¬® , pxn , yn qu

Robust Learning under Uncertain Test Distributions

drawn from a joint distribution ptr px, yq, and know that the
test distribution pte px, yq is the same as ptr px, yq. The most
common and well-established method to learn a prediction
function f : X √û√ë Y is through solving the following empirical risk minimization (ERM) problem:
1 √øn
lpfŒ∏ pxi q, yi q ` Œª ‚Ñ¶pŒ∏q,
(1)
i‚Äú1
Œ∏PŒò n
where the prediction function fŒ∏ p¬®q is parametrized by a
vector Œ∏, lp¬®, ¬®q is a loss function, ‚Ñ¶p¬®q is a regularizer on Œ∏
to control overfitting and Œª P R is regularization parameter.
min

When there is covariate shift, the feature distribution pte pxq
is different from ptr pxq but the conditional distribution
ppy|xq representing the classification/regression rule remains the same. In this scenario, one of the most common approach to correct for the effect of covariate shift is
to reweigh the training instances in the ERM problem to
reflect their true proportions on the test set:
1 √øn
wpxi q lpfŒ∏ pxi q, yi q ` Œª ‚Ñ¶pŒ∏q,
(2)
i‚Äú1
Œ∏PŒò n
where wpxi q is a reweighing function that approximates the
density ratio pte pxi q{ptr pxi q. There are different methods
for estimating the density ratio wpxq using unlabelled test
data (Quionero-Candela et al., 2009; Sugiyama & Kawanabe, 2012). This suggests viewing the learning problem
as a two-step estimation problem, where the density ratio
wpxq is estimated first before estimating Œ∏.
min

Interestingly, in econometrics the phenomenon of covariate shift has been used to detect model misspecification.
White (1981) considered the problem of detecting model
misspecification in non-linear least squares regression for
models yi ‚Äú fŒ∏ pxi q`i , where i is i.i.d. noise. He showed
that under certain assumptions, when there is no misspecification, the objective and solution Œ∏ ‚Äπ of the problem
1 √øn
pyi ¬¥ fŒ∏ pxi qq2
Œ∏
n i‚Äú1
converge to the same limits as the reweighed problem:
√øn
min
wi pyi ¬¥ fŒ∏ pxi qq2 ,
min

Œ∏

i‚Äú1

for
≈ô any fixed set of non-negative weights wi such that
i wi ‚Äú 1. He then derived several misspecification tests
based on asymptotic approximation of the difference of the
solutions. The key idea in his work is to detect model misspecification by creating his own covariate shift wi , so that
correct inference on the effects of different variables in regression can be performed.
In this paper, we explore White‚Äôs main insight further by
modelling the reweighing functions wpxi q as an adversary
in a game against the learner. Instead of detecting model
misspecification, we want to tell whether density ratio correction is needed under a set of potential distribution shifts.

The rest of this section will introduce our game formulation. Section 4 will then explain how it can be used to detect whether density ratio correction is needed or not.
We tie the two problems of density ratio estimation and
learning a predictor together through the robust Bayes
framework (GruÃànwald & Dawid, 2004). The learner tries
to minimize the loss by selecting a model Œ∏ P Œò, while the
adversary tries to maximize the loss by selecting a reweighing function w P W. Formally, we model the learning
problem as a (regularized) minimax game:
1 √øn
wpxi q lpfŒ∏ pxi q, yi q ` Œª ‚Ñ¶pŒ∏q. (3)
min max
wPW
Œ∏PŒò
n i‚Äú1
The learner can be seen as minimizing the worst case loss
over the set of test distributions W produced by the adversary. The definition of the strategy set W used by the
adversary is important in our approach, as it determines
the extent to which any model misspecification can be exploited by the adversary to increase the loss. Depending on
the application scenario, it can be defined using our prior
knowledge on how the test distributions could change, or
based on unlabelled test data if they are available.
To refine this formulation, we assume the reweighing functions wpxq are linearly parametrized:
√øk
wŒ± pxq ‚Äú
Œ±j kj pxq,
(4)
j‚Äú1

where Œ± contains the mixing coefficients and kj pxq are
non-negative basis functions. For example, each kj pxq
could be a non-negative kernel function, say, the Gaussian
kernel
`
Àò
Kpbj , xq ‚Äú exp ¬¥||bj ¬¥ x||2 {2œÉ 2
(5)
with basis bj , or it could be Ij pxq, the indicator function for
the jth disjoint group of the data, representing groups from
different genders, age ranges, or k-means clusters, etc. It
could be viewed as the conditional probability ppx|jq of
observing x given class j in a mixture model. As for Œ±,
it is generally constrained to lie in some compact subspace
A of the non-negative quadrant of Euclidean space. This
linear formulation is flexible enough to capture many different types of uncertainties in the test distributions, and
yet simple enough to be solved efficiently as a convex optimization problem. Therefore, we consider uncertain test
distributions and optimize the following minimax game:
1 √øn
wŒ± pxi q lpfŒ∏ pxi q, yi q ` Œª ‚Ñ¶pŒ∏q
min max
i‚Äú1
Œ∏PŒò Œ±PRk n
(6)
1 √øn
s.t.
wŒ± pxi q ‚Äú 1,
0 ƒè Œ±j ƒè B.
n i‚Äú1
The sum-to-one normalization constraint ensures that
wŒ± pxq behaves like a Radon-Nikodym derivative that properly reweighs the training distribution to a potential test distribution (Shimodaira, 2000; Sugiyama et al., 2008). The

Robust Learning under Uncertain Test Distributions

bound B P R on the parameters Œ±j ensure that the reweighing function wŒ± pxq is bounded, which naturally controls
the capacity of the adversary. In this formulation, the strategy set1 An of the adversary is the intersection of a hypercube and an affine subspace:
" Àá √ø
*
Àá1 n
Àá
An ‚Äú Œ±Àá
wŒ± pxi q ‚Äú 1, 0 ƒè Œ±j ƒè B , (7)
n i‚Äú1
which is closed and convex. For the games defined above
between the learner and the adversary, a minimax solution
pŒ∏ Àö , Œ±Àö q exists (Rockafellar, 1996, Corollary 37.3.2).
3.1. Solving the Training Problem
We first define the empirical adversarial loss as
1 √øn
wŒ± pxi q lpfŒ∏ pxi q, yi q.
LAn pŒ∏q ‚Äú max
i‚Äú1
Œ±PAn n

(8)

The training problem in Eq. (6) can be solved efficiently
for loss functions lpfŒ∏ p¬®q, ¬®q that are convex in Œ∏. Notice
Eq. (8) is a convex function in Œ∏ if lpfŒ∏ p¬®q, ¬®q is convex in
Œ∏, as we are taking the maximum over a set of convex functions. A subgradient of LAn pŒ∏ 1 q at a point Œ∏ 1 is:
1 √øn
B
B
LAn pŒ∏ 1 q ‚Äú
wŒ±1 pxi q lpfŒ∏1 pxi q, yi q, (9)
i‚Äú1
BŒ∏
n
BŒ∏
where Œ±1 is the solution of the problem with Œ∏ 1 fixed:
1 √øn
Œ±1 ‚Äú argmax
wŒ± pxi q lpfŒ∏1 pxi q, yi q.
(10)
n i‚Äú1
Œ±PAn
Since the strategy set An is linearly constrained and the
objective is also linear, we can easily use linear programming to solve for Œ±1 in Eq. (10). Knowing how to compute
the subgradient, we can treat the robust training problem
as a convex ERM problem with the adversarial loss. The
optimization problem can be solved efficiently with subgradient methods or bundle methods (Kiwiel, 1990).
3.2. Incorporating Unlabelled Test Data
If unlabelled test data txn`1 , . . . , xn`m u are available, we
would require the reweighing functions wŒ± pxq used by the
adversary to produce test distributions that are close to the
unlabelled data, especially when covariate shift occurs. In
this case we can further restrict the strategy set An of the
adversary via moment matching constraints (MMC):
1 √øn
min max
wŒ± pxi q lpfŒ∏ pxi q, yi q ` Œª‚Ñ¶pŒ∏q
i‚Äú1
Œ∏PŒò Œ±PRk n
√ø
1 n
s.t.
wŒ± pxi q ‚Äú 1,
0 ƒè Œ±j ƒè B
n i‚Äú1
√ø
1 n
1 √øn`m
wŒ± pxi qœÜpxi q ‚Äú
œÜpxi q, (11)
n i‚Äú1
m i‚Äún`1
1
We use the subscript n to denote its dependence on the sample tx1 , ¬® ¬® ¬® , xn u.

where œÜp¬®q are feature functions similar to those used
in maximum entropy models (Berger et al., 1996). Let
Kn Œ± ‚Äú œÜÃÑte represent the linear constraint of Eq. (11),
then the strategy set of the adversary becomes the closed
convex set:
" Àá √ø
*
Àá1 n
Àá
AMMC
‚Äú
Œ±
w
px
q
‚Äú
1,
0
ƒè
Œ±
ƒè
B,
K
Œ±
‚Äú
œÜÃÑ
j
n
te .
n
Àá n i‚Äú1 Œ± i
In practice, it might not be feasible to satisfy all the moment
matching constraints. It is also unwise to enforce these as
hard constraints, as the small test sample might not be representative of the true test distribution. We can incorporate
an L1 or L2 penalty on the constraint violations similar to
Altun & Smola (2006) while retaining convexity of the optimization problem. The details are not shown here due to
space constraints. We refer to Eq. (11) as robust covariate
shift adjustment (RCSA).

4. Relating Covariate Shift to Model
Misspecification
This section relates covariate shift to model misspecification and describes a procedure for testing whether correcting for covariate shift could be needed, assuming the test
distribution comes from the strategy set A of the adversary.
We will also state and discuss several theoretical results to
justify our test. Their proofs are in Appendix A. We start
with a definition:
Definition 1 (Pointwise Domination). A parameter Œ∏ ‚Äπ is
said to pointwisely dominate all Œ∏ 1 P Œò over the loss function lp¬®, ¬®q if, for all x P X and for all Œ∏ 1 P Œò,
≈º
≈º
lpfŒ∏‚Äπ pxq, yqppy|xqdy ƒè lpfŒ∏1 pxq, yqppy|xqdy. (12)
This condition means there is a single Œ∏ ‚Äπ that pointwisely
minimizes the loss l for any x P X . It is easy to see that
this pointwise domination condition is implied by the traditional definition of correctly specified model class when
lp¬®, ¬®q is the log loss, ¬¥ log pŒ∏ py|xq. With log loss, the
pointwise domination condition then becomes:
≈º
≈º
¬¥ ppy|xq log pŒ∏‚Äπ py|xqdy ƒè ¬¥ ppy|xq log pŒ∏1 py|xqdy.
This inequality always holds because pŒ∏‚Äπ py|xq ‚Äú ppy|xq
minimizes the entropy on the left hand side. Therefore, a
correctly specified model always implies the existence of
a pointwise dominator Œ∏ ‚Äπ . However, the converse is not
always true, as the underlying model class Œò might be too
weak (e.g., if Œò contains only a single model Œ∏).
Note that pointwise domination condition does not depend
on the marginal distribution ppxq. If we can find such a

Robust Learning under Uncertain Test Distributions

pointwise dominator, then the test distribution can be arbitrarily shifted without damaging the performance of pointwise dominator Œ∏ ‚Äπ . However, this condition is too stringent and is almost never true on real data. This motivates
us to consider the game formulation in Section 3: Instead
of requiring Œ∏ to minimize the loss at every single point x,
we require Œ∏ to minimize the loss against every reweighing
function wŒ± pxq that the adversary can play. We define
Àá≈º
*
"
Àá
SÀá
A8 ‚Äú Œ± P A Àá wŒ± pxq dF px, yq ‚Äú 1 ,
where AS is the support of the strategy set of the adversary that does not depend on training samples (e.g., the hypercube 0 ƒè Œ±j ƒè B), and F px, yq is the joint training
distribution of px, yq. Now we define dominant strategy:
Definition 2 (Dominant Strategy). We say that Œ∏ : P Œò is a
dominant strategy for the learner if, for all Œ± P A8 , for all
Œ∏ 1 P Œò,
≈º

≈º
wŒ± pxqlpfŒ∏: pxq, yqdF px, yq ƒè wŒ± pxqlpfŒ∏1 pxq, yqdF px, yq.

It is easy to show that the pointwise domination condition
implies the existence of dominant strategy.
Theorem 3. Suppose a pointwise dominator Œ∏ ‚Äπ exists, then
Œ∏ ‚Äπ is also a dominant strategy for the learner, against any
bounded adversarial set A.
The existence of a dominant strategy of the learner is the
key criterion in deciding whether density ratio correction
is necessary. If such a dominator Œ∏ : exists, then it gives
lower or equal loss compared to other models Œ∏ 1 , no matter which reweighing function wŒ± pxq is used. Thus if one
can find Œ∏ : , no density ratio correction is needed in expectation, since Œ∏ : is asymptotically optimal as long as the
training and test distributions come from the given adversarial set. However, if no such dominator exists, then for
any model Œ∏, there exists another model Œ∏ 1 and a reweighing function wŒ±1 pxq such that Œ∏ 1 has strictly lower loss than
Œ∏ on wŒ±1 pxq. This means that a reweighing wŒ±1 pxq and its
corresponding model Œ∏ 1 are preferable. As a result, density
ratio correction will be helpful if the test set is drawn from
wŒ±1 pxq while the training set is not, provided that we can
estimate wŒ±1 pxq accurately.
How do we know if such a dominant strategy exists for a
game between Œò and A? The robust solution of the game
can help us in finding out. Let Œ∏s be the solution of the
unweighed loss minimization problem
≈º
Œ∏s ‚Äú argmin lpfŒ∏ pxq, yq dF px, yq,
(13)
Œ∏PŒò

and Œ∏p be the solution of the reweighed adversarial loss minimization problem
≈º
Œ∏p ‚Äú argmin max wŒ± pxqlpfŒ∏ pxq, yq dF px, yq. (14)
Œ∏PŒò

Œ±PA8

Our main observation is that, if a dominant strategy Œ∏ : exists, then under suitable assumptions on the adversary, the
unweighed solution Œ∏s is also a dominant strategy.
Theorem 4. Suppose the reweighing function wŒ± pxq is linear in Œ±, and the constant reweighing Œ±0 with wŒ±0 pxq ‚Äú 1
is in the relative interior of A8 . If a dominant strategy Œ∏ :
of the learner exists, then the unweighed solution Œ∏s is also
a dominant strategy for the learner.
Thm. 4 suggests a way to test for the existence of dominant
strategy. Consider the adversarial loss
≈º
LA8 pŒ∏q ‚Äú max wŒ± pxqlpfŒ∏ pxq, yq dF px, yq.
Œ±PA8

By definition, any dominant strategy Œ∏ : minimizes the adversarial loss, so Thm. 4 implies that the unweighed solution Œ∏s will also minimize the adversarial loss. Therefore,
p
by comparing the value of the minimax solution LA8 pŒ∏q
(which by definition minimizes the adversarial loss) against
s we can tell if a dominant strategy exists. If they
LA8 pŒ∏q,
are not equal, then we are certain that no such dominant
strategy exists, and density ratio correction can be helpful,
depending on the choice of training and test distributions
from A. On the other hand, if they are equal, we cannot
conclude that a dominant strategy exists, as it is possible
that the reweighed adversarial distribution matches the uniform unweighed distribution arbitrarily closely. However,
such examples are rather contrived and we never encountered such a situation in any of our experiments.
p and
The final question left is how to compare LA8 pŒ∏q
s
p
LA8 pŒ∏q in practice with a finite sample. Let Œ∏n be a solution of the robust game:
n
1 √ø
wŒ± pxi qlpfŒ∏ pxi q, yi q,
Œ∏pn ‚Äú argmin max
Œ∏PŒò Œ±PAn n i‚Äú1

(15)

and let Œ∏sn be a solution of the unweighed ERM problem:
n
1 √ø
Œ∏sn ‚Äú argmin
lpfŒ∏ pxi q, yi q.
n i‚Äú1
Œ∏PŒò

(16)

Our convergence results below states that, instead of
p and LA pŒ∏q,
s we can compare the empirical adLA8 pŒ∏q
8
versarial losses LAn pŒ∏pn q and LAn pŒ∏sn q.
Theorem 5. Suppose the support AS for Œ± and Œò for Œ∏ are
each closed, convex, and bounded. Suppose also wŒ± pxq
and lpfŒ∏ pxq, yq are bounded continuous functions in Œ± and
Œ∏ for each px, yq pair. ≈üIf the set satisfying the normalization
constraint tŒ± P AS | wŒ± pxqdF px, yq ‚Äú 1u is non-empty
in the relative interior of AS , then we have, for all Œ∏ P Œò,
LAn pŒ∏q √ë LA8 pŒ∏q
in probability, i.e., for all , Œ¥ ƒÖ 0, we can find m P N such
that for all n ƒõ m, we have
|LAn pŒ∏q ¬¥ LA8 pŒ∏q| ƒÉ 

Robust Learning under Uncertain Test Distributions

with probability at least 1 ¬¥ Œ¥.

tition into training and test sets via 10-fold cross validation. To construct reasonable adversaries, we use Eq.(4)
For simplicity, we do not consider the moment matching
with Gaussian kernel as our reweighing function. As we
ste , but this can be handled in the
constraints Kn Œ± ‚Äú œÜ
mentioned earlier, the adversarial set is determined by prior
proof with techniques similar to the normalization conknowledge of how the test distribution might change. In
straint. We use t-test with cross-validation to compare
this toy example, we use a large range of œÉ, based on the
these quantities in the experiments.
average distance from an instance to its nc -nearest neighbours, where n is the number of training points and c P
The chain of implications can be summarized as follows:
t2, 4, 8, 16, ¬® ¬® ¬® u. The smaller œÉ is, the more powerful the
No model misspecification for Œò
adversary can be, i.e., the more possible test distributions it
can generate. The bases, bj , are chosen to be the training
√±Pointwise dominator exists for Œò
points. B is set to be 5, a bound that is rarely reached in
√±Dominant strategy against any bounded adversary A exists
practice due to the normalization constraint. Therefore, this
bound does not significantly limit the adversary‚Äôs power,
We can see that ‚Äúno model misspecification‚Äù is a very
as it allows the adversary to put as much importance on
strong condition, as it requires a dominator against any
a single kernel as it wants. We tune the parameter Œª via
bounded adversary A, including pathologically spiky test
10-fold cross validation.2 Figure 2(a) shows that LAn pŒ∏pn q
distributions with tall spikes and small support. Also, there
and LAn pŒ∏sn q (mean and one standard deviation as error
is an implicit assumption in using density ratio correction
bar) are very close for all œÉ in the linear example, indicatthat covariate shifts on the test set are not represented by aring that the adversary cannot exploit the weakness of linear
bitrarily complex functions. Otherwise estimation of denlearner. Figure 2(b) shows that, for the non-linear example,
sity ratio cannot take place and there is no way to correct
even with moderate œÉ, there is a noticeable difference becovariate shift. Therefore, instead of focusing on density
tween LAn pŒ∏pn q against LAn pŒ∏sn q, strongly suggesting that
ratio correction alone, we look at another way of dealing
no dominant strategy exists in this case, which suggests that
with covariate shift, by performing model checking against
covariate shift correction is necessary if the test distribution
a set of restricted changes in test distributions represented
is shifted here. The experiments showing covariate shift
by the adversary A. In the next section we will provide
scenarios are in Appendix B due to space limits.
empirical evaluations of this particular approach.
To see how the adversary creates different empirical adverOur analysis is different from Shimodaira (2000). We do
sarial losses in a non-linear example, we fix the œÉ to the
not start with a strong condition like ‚Äúno model misspecaverage distance from an instance to its n5 -nearest neighification‚Äù, which is equivalent to requiring the learner to
bour and illustrate a concrete trial in Figure 2(c). It is obvihave a dominant strategy against any adversaries. Instead,
ous that the adversary puts more weights at the test points
given weak prior knowledge about how the distributions
where the loss of the classifier learned from training data
can shift, we provide a method that can determine whether
is large. Our robust formulation incorporates the adversary
reweighing can be helpful. This analysis is more practical
and prevents any point from having too large a loss. As
than deciding whether reweighing is needed based on the
a result, the adversary cannot undermine the robust learner
strong notion of no model misspecification, as almost all
severely, which leads to the gap of the empirical adversarial
models are misspecified on real datasets.
losses of robust and regular learners in Figure 2(b).

5. Empirical Studies

5.2. Experiments on Real-world Datasets

5.1. Experiments on Toy Datasets

This section presents the experimental results on real world
datasets to show how our formulation determines whether
dominant strategy exists against some adversaries and if so,
how to correct such covariate shifts. We investigate both
regression problems using squared loss, and classification
problems using hinge loss. A linear model is learned from
the dataset unless otherwise specified.

We first present two toy examples to show the effectiveness
of our test. We construct a linear model, f1 pxq ‚Äú x ` 1 ` ,
and a non-linear (cubic) model, f2 pxq ‚Äú x3 ¬¥ x ` 1 ` ,
where  ‚Äû N p0, 0.12 q is additive Gaussian noise (adapted
from Shimodaira (2000)). For both, we learn a linear regressor fŒ∏ pxq ‚Äú Œ∏1 ¬® x ` Œ∏0 from data to minimize squared
loss lpfŒ∏ pxi q, yi q ‚Äú ||Œ∏ T xi ¬¥ yi ||2 with L2 regularizer on
Œ∏: ‚Ñ¶pŒ∏q ‚Äú 12 ||Œ∏||22 .
To show how to detect whether a dominant strategy exists with various adversarial sets A, we generate 500 data
points uniformly in the interval r¬¥1.5, 2s, which we par-

We obtain some classification datasets from UCI reposi2

Here, as there is no covariate shift, we just use simple cross
validation. Whenever test distribution is shifted in the experiment, parameters are tuned via importance weighted cross validation (Sugiyama et al., 2007).

Robust Learning under Uncertain Test Distributions
14

0.09

Regular
Robust

12

Regular
Robust

9

8

0.07

0.06

0.05

0.04

7

10

6

8

Output

Adversarial Loss

Adversarial Loss

0.08

6

4

5

0.5

Training Point
Test Point
Regular Regressor
Robust Regressor
Test Reweigh for Regular
Test Reweigh for Robust

0.45

0.4

0.35

0.3

4

0.25

3

0.2

2

0.15

1

0.1

0

0.05

Weight

0.1

0.03

2
0.02

0.01

1.08

0.48

0.22

0.11

0.05

0.02

0

1.06

0.49

0.11

0.05

0.02

‚àí1
‚àí1.5

‚àí1

‚àí0.5

0

0.5

1

1.5

2

0

Input

œÉ

œÉ

(a) Linear example.

0.23

(b) Cubic example.

(c) Adversarial reweighing.

Figure 2. Toy examples. Adversarial test losses are shown in Figures 2(a) and 2(b), where the x-axis shows the values of œÉ. Figure 2(c)
provides a non-linear example to show how the adversary attacks the regressors by reweighing the test points, with output on the left
y-axis and weight on the right y-axis.
Table 1. Dataset Summary
DATASET
S IZE D IM
T YPE
AUSTRALIAN
690
14
C LASSIFICATION
B REAST CANCER 683
10
C LASSIFICATION
G ERMAN NUMER 1000
24
C LASSIFICATION
H EART
270
13
C LASSIFICATION
I ONOSPHERE
351
34
C LASSIFICATION
L IVER DISORDER 345
6
C LASSIFICATION
S ONAR
208
60
C LASSIFICATION
S PLICE
1000
60
C LASSIFICATION
AUTO - MPG
392
6
R EGRESSION
C ANCER
1523
40
R EGRESSION

tory3 . All are binary classification problems. For regression task, we use Auto-mpg dataset, which admits a natural covariate shift scenario, as it contains data collected
from 3 different cities. We also have a set of cancer patient
survival time data provided by our medical collaborators,
containing 1523 uncensored patients with 40 features, including gender, stage of cancer, and various measurements
obtained at the time of diagnosis. Table 1 shows the summary of the datasets we used in the experiments.
5.2.1. D OMINANT S TRATEGY D ETECTION
We first detect the existence of dominant strategy as we did
in the toy example. To construct reasonable adversaries,
Gaussian kernel is applied to Eq.(4), setting œÉ to be the average distance from an instance to its n5 -nearest neighbour,
the bases bj to be the training points and B to be 5. 4
Figure 3(a) shows the experimental results. Auto-mpg12
3

http://archive.ics.uci.edu/ml/index.html
We allow the user to set up other adversaries, as the appropriate adversary depends on user‚Äôs belief about how the test distribution may change. We use this medium power adversary to
differentiate between datasets under linear models. If the adversary is too weak, no correction is needed for all datasets. If the
adversary is too strong, all datasets require correction, as on real
data there is no ‚Äúcorrect‚Äù model. We omit these less interesting
cases due to space limits.
4

explores when the training data comes from city 1 and
test data is from city 2, while Auto-mpg13 explores
when training data comes from city 1 and test data comes
from city 3. Here we focus on the empirical adversarial
losses of robust versus regular models. A significant difference indicates that there is no dominant strategy and
thus, the linear model is vulnerable to our reweighing adversary. For classification datasets and the cancer dataset,
we apply 10-fold cross validation to obtain training and
test sets. For Auto-mpg, we fix the test set and apply
10-fold cross validation to obtain training set. Figure 3(a)
presents these losses over the datasets (mean and one standard deviation as error bar). t-test at significance level 0.05
indicates that two losses are significantly different for the
Liver disorders and Auto-mpg datasets. As a result, the linear model is vulnerable for these sets.
To further substantiate the incapability of the linear model,
we attempted to detect dominant strategy for a Gaussian
model set Œò (i.e., changing from linear kernel to Gaussian
kernel where the learner use internal cross-validation to
chose the kernel width). Results are shown in Figure 3(b).
Compared to Figure 3(a), the gap of empirical adversarial
losses between robust and regular models shrinks significantly in Figure 3(b). Our result indicates that t-test no
longer claims a significant difference between these losses,
suggesting that the adversary cannot severely undermine
the performance of regular learning. Therefore, model revision can be a good alternative to performing covariate
shift correction.
5.2.2. R EWEIGHING A LGORITHM FOR C OVARIATE
S HIFT S CENARIOS
As previously mentioned, the reweighing mechanism could
improve the performance if the model is vulnerable to the
reweighing adversary. For the covariate shift correction
task, we set the test points as the reference bases bj of the
weight function (Eq. (4)), because they are more informa-

Robust Learning under Uncertain Test Distributions
1.2

Robust
Regular

1.6

1

1.4

1.2

1

0.8

0.6

0.4

1.2

0.8

Test Loss

Adversarial Loss

1.4

Adversarial Loss

Robust
Regular

1.6

1

0.8

0.6

Unweighed
Clust
KLIEP
RuLSIF
RCSA

0.6

0.4

0.4
0.2

0.2

0

0.2

Au Br G
H
Io L
S
S
A
A
C
st ea erm ear nos iver ona plic uto uto anc
ra
‚àím ‚àím e
s
e
ph _d r
lia t_c an_ t
r
i
p
s
e
n
an n
g1 pg1
re ord
ce um
2
3
er
er
r
s

(a) Linear learner.

0

Au

Br

G

H
Io L
S
S
A
A
C
st ea erm ear nos iver ona plic uto uto anc
ra
‚àím ‚àím e
s
e
ph _d r
lia t_c an_ t
r
i
p
s
e
n
an n
g1 pg1
re ord
ce um
2
3
er
er
r
s

(b) Gaussian learner.

0

S S A A C C
B G H Io L
st rea erm ear nos iver ona plic uto uto anc anc
ra st
e ‚àím ‚àím er er
ph _d r
lia _c an_ t
pg pg ‚àíg ‚àís
er iso
n
an n
e
rd
12 13 en tag
u
ce m
e
de e
r
er
r
s
r

Au

(c) Reweighed linear model performance.

Figure 3. Experimental results for dominant strategy detection and covariate shift correction. Figure 3(a) and Figure 3(b) show the
adversarial test losses of robust and regular learners.

tive than training points about the test distribution, as suggested by Sugiyama et al. (2008). The reweighing set (œÉ
and B) is chosen as in Section 5.2.1.
To create covariate shift scenarios in the classification
datasets, we apply the following scheme to obtain shifted
test set. We first randomly pick 75% of the whole set for
robust training (6) to attain a model Œ∏. Then we evaluate
the empirical adversarial loss (8) of this model on the 25%
hold-out test set, and record all the weights of these test
points. The probability
¬¥ that a¬Øtest instance x will remain in
Œ± pxq
the test set is min 1, w1{m
, where m is the number of
test points at the moment (25% of the set). About 10% of
the whole dataset remain after filtering; these instances will
serve as the final test set with shifted distribution. The intuition is: we want some test points with large errors even for
robust learner. These points are more likely to undermine
any model, meaning covariate shift will have more significant impact on the performance. The procedure is performed 10 times, leading to the average test losses reported
in Figure 3(c). As Auto-mpg has a natural covariate shift
scenario, we do not artificially partition the dataset. We
applied 10-fold cross validation to obtain the training set.
We consider two shifted scenarios in cancer survival time
prediction:
1. Gender split. The dataset contains about 60% male
and 40% female patients. In gender split, we randomly take
20% of the male and 80% of the female patients into training set, while the rest goes to test set. That is, the training
set is dominated by male patients while the test set is dominated by female patients.
2. Cancer stage split. Approximately 70% of the dataset
are of stage-4. In cancer stage split, we randomly take 20%
of stage-1-to-3 and 80% of stage-4 patients to training set,
while the rest goes to test set. The training set is dominated
by stage-4 patients while the test set is dominated by stage1-to-3 patients.
Figure 3(c) compares the test losses of RCSA with

the regular unweighed learning algorithm, the clusteringbased reweighing algorithm (Cortes et al., 2008),
KLIEP (Sugiyama et al., 2008) and RuLSIF (Yamada et al.,
2011). Recall that our analysis in Section 5.2.1 shows that
linear model is insufficient for the Liver disorders
and Auto-mpg datasets, which suggests that reweighing
may help. This is confirmed in Figure 3(c): by putting
more weights on the training instances that are similar to
test instances, the reweighing algorithms can produce models with smaller test losses for these datasets. Although our
robust game formulation is mainly designed to detect dominant strategy, our RCSA algorithm can correct shifted distribution using moment matching constraints described in
Section 3.2. As shown in Figure 3(c), our method performs
on par with state-of-the-art algorithms when covariate shift
correction is required. For the datasets that appear linear
(i.e., where the linear model performs relatively well), we
found that the reweighing algorithms did not significantly
reduce the test losses. In some cases, reweighing actually
increased the test losses due to the presence of noise.

6. Conclusions
We have provided a method for determining if covariate
shift correction is needed, given a pre-defined set of potential changes in the test distribution. This is useful for
ensuring the learned predictor will still perform well when
there are uncertainties about the test distribution in the deployment environment, such as changes in gender ratio and
case mix in the cancer prognostic predictor example. It can
also be used to decide if a model class revision of Œò is necessary. Experimental results show that our detection test
is effective on several UCI datasets and a real-world cancer patient dataset. This analysis shows the importance of
studying the interaction of covariate shift and model misspecification, because the final test set error depends on
both factors.

Robust Learning under Uncertain Test Distributions

References
Altun, Y. and Smola, A. Unifying divergence minimization
and statistical inference via convex duality. In Learning
theory, pp. 139‚Äì153. Springer, 2006.
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F.
Analysis of representations for domain adaptation. In
Advances in Neural Information Processing Systems,
volume 19, pp. 137, 2007.
Berger, A., Pietra, V., and Pietra, S. A maximum entropy
approach to natural language processing. Computational
linguistics, 22(1):39‚Äì71, 1996.

Quionero-Candela, J., Sugiyama, M., Schwaighofer, A.,
and Lawrence, N. D. Dataset shift in machine learning.
The MIT Press, 2009.
Rockafellar, R. T. Convex analysis. Princeton University
Press, 1996.
SchoÃàlkopf, B. and Smola, A. J. Learning with kernels: support vector machines, regularization, optimization and
beyond. The MIT Press, 2002.
Shimodaira, H. Improving predictive inference under covariate shift by weighting the log-likelihood function.
Journal of Statistical Planning and Inference, 90(2):
227‚Äì244, 2000.

Bickel, S., BruÃàckner, M., and Scheffer, T. Discriminative
learning for differing training and test distributions. In
International Conference on Machine Learning, pp. 81‚Äì
88, 2007.

Storkey, A. J. and Sugiyama, M. Mixture regression for
covariate shift. In Advances in Neural Information Processing Systems, 2007.

Bickel, S., BruÃàckner, M., and Scheffer, T. Discriminative
learning under covariate shift. The Journal of Machine
Learning Research, 10:2137‚Äì2155, 2009.

Sugiyama, M. and Kawanabe, M. Machine learning in nonstationary environments: introduction to covariate shift
adaptation. The MIT Press, 2012.

Cortes, C., Mohri, M., Riley, M., and Rostamizadeh, A.
Sample selection bias correction theory. Algorithmic
Learning Theory, 5254:38‚Äì53, 2008.

Sugiyama, M. and MuÃàller, K. Model selection under covariate shift. In Artificial Neural Networks: Formal
Models and Their Applications, pp. 235‚Äì240. Springer,
2005.

Cortes, C., Mansour, Y., and Mohri, M. Learning bounds
for importance weighting. Advances in Neural Information Processing Systems, 23:442‚Äì450, 2010.
Globerson, Amir and Roweis, Sam. Nightmare at test time:
robust learning by feature deletion. In International Conference on Machine learning, pp. 353‚Äì360. ACM, 2006.
GruÃànwald, P. D. and Dawid, A. P. Game theory, maximum
entropy, minimum discrepancy and robust bayesian decision theory. The Annals of Statistics, 32(4):1367‚Äì1433,
2004.
Huang, J., Smola, A. J., Gretton, A., Borgwardt, K. M.,
and SchoÃàlkopf, B. Correcting sample selection bias by
unlabeled data. In Advances in Neural Information Processing Systems, volume 19, pp. 601, 2007.
Kanamori, T., Hido, S., and Sugiyama, M. Efficient direct
density ratio estimation for non-stationarity adaptation
and outlier detection. In Advances in Neural Information
Processing Systems, 2008.
Kiwiel, K. C. Proximity control in bundle methods for convex nondifferentiable minimization. Mathematical Programming, 46(1):105‚Äì122, 1990.
Pan, S. J., Tsang, I. W., Kwok, J. T., and Yang, Q. Domain
adaptation via transfer component analysis. In International Joint Conference on Artifical Intelligence, pp.
1187‚Äì1192, 2009.

Sugiyama, M., Krauledat, M., and MuÃàller, K. Covariate
shift adaptation by importance weighted cross validation. Journal of Maching Learning Research, 8:985‚Äì
1005, 2007.
Sugiyama, M., Nakajima, S., Kashima, H., Von Buenau,
P., and Kawanabe, M. Direct importance estimation
with model selection and its application to covariate shift
adaptation. In Advances in Neural Information Processing Systems, 2008.
Teo, C. H., Globerson, A., Roweis, S., and Smola, A. Convex learning with invariances. In Advances in Neural
Information Processing Systems, 2008.
White, H. Consequences and detection of misspecified
nonlinear regression models. Journal of the American
Statistical Association, 76(374):419‚Äì433, 1981.
Yamada, M., Suzuki, T., Kanamori, T., Hachiya, H., and
Sugiyama, M. Relative density-ratio estimation for robust distribution comparison. In Advances in Neural Information Processing Systems, pp. 594‚Äì602, 2011.
Zadrozny, B. Learning and evaluating classifiers under
sample selection bias. In International Conference on
Machine Learning, pp. 114. ACM, 2004.

