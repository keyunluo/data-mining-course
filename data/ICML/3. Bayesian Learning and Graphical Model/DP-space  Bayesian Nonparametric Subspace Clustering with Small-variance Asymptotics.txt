DP-space: Bayesian Nonparametric Subspace Clustering with
Small-variance Asymptotics
Yining Wang
Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA

YININGWA @ CS . CMU . EDU

Jun Zhu
DCSZJ @ TSINGHUA . EDU . CN
Dept. of Comp. Sci. & Tech., State Key Lab of Intell. Tech. & Sys., TNList, CBICR Center, Tsinghua University, China

Abstract
Subspace clustering separates data points approximately lying on union of affine subspaces
into several clusters. This paper presents a
novel nonparametric Bayesian subspace clustering model that infers both the number of subspaces and the dimension of each subspace from
the observed data. Though the posterior inference is hard, our model leads to a very efficient deterministic algorithm, DP-space, which
retains the nonparametric ability under a smallvariance asymptotic analysis. DP-space monotonically minimizes an intuitive objective with an
explicit tradeoff between data fitness and model
complexity. Experimental results demonstrate
that DP-space outperforms various competitors
in terms of clustering accuracy and at the same
time it is highly efficient.

1. Introduction
Given a collection of data points x1 , · · · , xn ∈ RD , subspace clustering is the task that groups the data points into
K components by finding K affine subspaces of dimensions d1 , · · · , dK < D such that each data point lies in (or
near) a particular subspace. Subspace clustering has found
many applications in computer vision, including motion
segmentation (Vidal et al., 2008) and face clustering (Ho
et al., 2003). See (Vidal, 2010) for an excellent review.
Many subspace clustering methods have been developed.
One particularly interesting example is the mixture of probabilistic PCAs (MPPCA) (Tipping & Bishop, 1999), which
represents each affine subspace by a probabilistic PCA
(pPCA) model and posits that each data point comes from
a mixture of pPCA models. Compared to alternative algeProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

braic and geometry solutions (Ma et al., 2007; Fischler &
Bolles, 1981; Elhamifar & Vidal, 2013), the MPPCA formulation enjoys the advantage of being more tolerate to
noisy and outlier values. Model parameters of MPPCA
could be learned via Expectation-Maximization (EM) (Tipping & Bishop, 1999). However, a major drawback of MPPCA is that both the number of clusters K and the dimension of each subspace dk must be specified a priori (Vidal,
2010). This causes difficult model selection problems, because unlike PCA, we need to select both K and dk at the
same time. When K or D is large, model selection soon
becomes intractable due to an exponential number of subspace dimension configurations.
In this paper, we present Dirichlet process mixture of PCAs
(DP-PCA), a novel nonparametric Bayesian model for subspace clustering that automatically resolves model complexity from data. One challenge of DP-PCA is on defining proper priors that take the nonparametric requirements
of clustering as well as unknown subspace dimensions into
consideration. For clustering, we adopt the commonly used
Chinese restaurant process (CRP (Aldous, 1985; Pitman,
1995), a marginalization of Dirichlet process) prior on cluster assignment variables. In addition, we propose a novel
hierarchical prior to facilitate a nonparametric treatment on
subspace dimensions, which is significantly different from
the mixture of factor analysis (MFA) (Chen et al., 2010)
model. More importantly, though the posterior inference is
hard, our carefully designed model leads to an efficient deterministic algorithm, named DP-space, which resembles
the EM algorithm for MPPCA but still possesses the nonparametric nature of DP-PCA to resolve model complexity.
DP-space is provable to converge by monotonically minimizing an intuitive objective during iterations, and empirically it achieves increased efficiency and clustering accuracy on synthetic and real-world datasets.
Technically, we first derive a partially collapsed Gibbs
(PCG) sampler (van Dyk & Park, 2008) for the DP-PCA
model. The PCG sampler is elegant in theory but unfortunately impractical in computation. By carefully perform-

DP-space: Bayesian Nonparametric Subspace Clustering

ing the small-variance asymptotics (SVA) analysis (Kulis
& Jordan, 2012), our PCG sampler leads to the DP-space
algorithm. Compared to existing SVA analysis for many
popular nonparametric Bayesian models (Kulis & Jordan,
2012; Jiang et al., 2012; Broderick et al., 2013; Roychowdhury et al., 2013; Wang & Zhu, 2014), our work makes
novel contributions by presenting a new nonparametric
model as well as a non-trivial generalization of existing
SVA methods for the challenging task of subspace clustering. Furthermore, we note that there are subspace clustering methods that do not require full knowledge of K and
dk . We discuss such methods in Sec. 5 and show that they
are less accurate, not as efficient as DP-space, or cannot
handle both unknown K and dk settings.

2. Preliminaries
We briefly review some preliminary knowledge and present
a proposition that will be used later.
2.1. Projection onto subspaces
A d-dimensional affine subspace S ⊆ R , d < D can be
expressed as
D

S = S(W, µ) = {x ∈ R

D

d

: x = Wy + µ, y ∈ R },
(1)
where W ∈ RD×d is a subspace projection matrix and µ ∈
RD specifies the offset of S. Without loss of generality,
we assume W has full column rank. By singular value
decomposition we know that there exist l10 ≥ · · · ≥ ld0 > 0,
Ud ∈ RD×d with orthonormal columns and an orthogonal
matrix R ∈ Rd×d such that
W = Ud diag(l10 , · · · , ld0 )R> .

(2)

For a vector x ∈ RD and a subspace S ⊆ RD , define the
distance between x and S as
d(x, S) := inf kx − yk2 .
y∈S

(3)

Proposition 1 provides an easy way to compute d(x, S).
We defer the proof to Appendix A.
Proposition 1. Fix a d-dimensional affine subspace
S(W, µ) and let U = [Ud , UD−d ] ∈ RD×D be an orthogonal matrix associated with W. Here Ud is defined in
Eq. (2) and UD−d ∈ RD×(D−d) can be any orthonormal
basis complementing Ud . Then for all x ∈ RD , we have
D
X
 >
2
d(x, S) =
U (x − µ) j .
2

(4)

j=d+1

One consequence of Proposition 1 is that d(·, S(W, µ))
does not depend on the eigenvalues l10 , · · · , ld0 of W. We
shall use this important property in the small-variance
asymptotic analysis later.

2.2. Mixtures of probabilistic PCA models
A mixture of probabilistic PCA (MPPCA) model (Tipping
& Bishop, 1999) M posits that a data point is generated
via a mixture of K subspace clusters. Mathematically, it
can be represented as M = (π, {Wk , µk , σk2 }K
k=1 ) where
Wk ∈ RD×dk and µk ∈ RD specify the subspace for the
kth cluster, σk2 indicates the variance and π is a mixture
probability distribution over the K clusters. Let zi ∈ [K]
denote the cluster assignment of data xi and define W =
{Wk }, µ = {µk }. Then, the likelihood of xi is:
p(xi |M) =

K
X

k=1

p(zi = k)p(xi |zi = k, W, µ),

(5)

where p(zi = k) = πk and the per-subspace likelihood
model (conditioned on W and µ) is defined as
p(xi |zi = k) =

Z

Rdk

N (xi ; Wk y i + µk , σk2 I)dp0 (y i )

= N (xi ; µk , Wk Wk> + σk2 I);

(6)

here we assume the prior p0 (y k ) is a standard Gaussian.
Given the number of subspaces K, the dimensions of subn
spaces {dk }K
k=1 and a collection of data points {xi }i=1 ,
an EM procedure (Tipping & Bishop, 1999) can be used
to learn the model parameters {Wk , µk , σk2 }K
k=1 . π could
also be inferred from learnt parameters.

3. Dirichlet Process Mixtures of PCAs
We now present Dirichlet process mixtures of PCAs (DPPCA), a novel nonparametric Bayesian subspace clustering
model that could handle an arbitrary number of subspaces.
Furthermore, the dimension of each subspace does not need
to be known a priori either.
3.1. The DP-PCA Model
DP-PCA is a Bayesian mixture model that adopts the same
mixture model as in Eq. (5), but with several changes.
First, for simplicity, we assume the same variance parameter σ 2 is shared among all subspaces and serves as a hyperparameter. This assumption greatly simplifies the smallvariance analysis and it does not cause performance deterioration, as shown in the experiments.
Second, we need to specify a prior distribution
p0 (z, W, µ) where z = {zi }ni=1 . To simplify derivation, we impose an improper non-informative prior
N (0, ρ2 ID×D ) with ρ2 → ∞ on subspace offsets µk . The
prior on the cluster assignment variables z is a Chinese
Restaurant Process (CRP) (Aldous, 1985; Pitman, 1995)
in order to facilitate a nonparametric treatment of clusters.

DP-space: Bayesian Nonparametric Subspace Clustering

More specifically, denoting z −i as {zj }j6=i , we have

n−i,k /Z, if n−i,k > 0;
p0 (zi = k|z −i ) =
(7)
α/Z,
otherwise.
P
k
k
where n−i,k =
j6=i δzj , δz = I[zj = k] and Z is a
normalization constant. α is a hyper-parameter of CRP.

The prior distribution on Wk is much more delicate, due
to two reasons: 1) we do not have easy conjugate priors for
Wk , and 2) we need to facilitate a nonparametric treatment
on the dimension of each Wk . Our solution is a hierarchical prior:
p0 (Wk ) =

D−1
X

dk =0

(8)

p0 (Wk |dk )p0 (dk ).

First, once the dimension dk of Wk is given, we define the
prior p0 (Wk |dk ) as follows.1 Using the strategy in (Minka,
2000; Zhang et al., 2004), we can express Wk as
(k)

(k)

(k)

Wk = Udk (Ldk − σ 2 Idk ×dk )1/2 Rdk ,
(k)

where Udk

(k)
Rdk

∈

(9)

RD×dk has orthonormal columns,

∈ Rdk ×dk is an orthogonal matrix and

(k)
Ldk

=

(k)
(k)
diag(l1 , · · · , ldk ). Note that Eq. (9) is identical to Eq.
(k)
(2) if define lj = lj0 + σ 2 . Hence, there exists an orthogh
i
(k)
(k)
onal matrix U(k) = Udk , UD−dk ∈ RD×D such that

Σk := Wk Wk> + σ 2 ID×D
(k)

(k)

= U(k) diag(l1 , · · · , ldk , σ 2 , · · · , σ 2 )U(k)> . (10)
To simplify notations, we will omit the superscript (k) in
the sequel when the meanings of the cluster assignments
are clear from the context.
To ensure identifiability, we assume following (Zhang
et al., 2004) that l1 > l2 > · · · > ldk > σ 2 , and hence
the prior distribution over l1 , · · · , ldk can be written as
2

p0 (l1 , · · · , ldk |σ ) = dk !

dk
Y

j=1

p0 (lj )·I[l1 > · · · > ldk > σ 2 ],

and the prior on lj−1 is assumed to be a Gamma distribution
with hyper-parameters a and b:
p0 (lj |a, b) =

Γ(lj−1 ; a, b)

ba −1 a−1 −blj−1
(l )
e
. (11)
=
Γ(a) j

Finally, we assume an improper non-informative prior on
the orthonormal matrix U.
1

If the dimension or rank of Wk does not agree with d then
p0 (Wk |d) should be zero. In addition, if dk = 0 then Wk = 0
with probability 1.

For the prior on the dimension, we simply use a truncated
geometric distribution:
p0 (dk ) =

1−r
· rdk ,
1 − rD

∀dk = 0, 1, · · · , D − 1, (12)

where r ∈ (0, 1) is a hyper-parameter. This prior formulates the intuition that we favor smaller subspace dimensions because smaller dimension means lower model complexity. In the sequel, we use d = (d1 , · · · , dK ) to denote
all subspace dimensions.
3.2. A Partially Collapsed Gibbs Sampler
Let X = {xi }ni=1 denote the training data. We need to
infer the posterior distribution of variables W, d, µ, z:
K+

Y
1
p(W, d, µ, z|X) =
p0 (z)
p0 (µk )p0 (dk )
Z(X)
k=1

n
Y
p(xi |zi , Wk , µk ),
· p0 (Wk |dk )

(13)

i=1

where K + is the number of active subspaces and we drop
the hyper-parameters (α, r, ρ, a, b, σ) for notational simplicity. Unsurprisingly, the posterior is intractable. Furthermore, a standard Gibbs sampler is not applicable either
because we cannot sample dk assuming the other variables
(e.g., Wk ) are fixed, since by definition this will give us
dk = rank(Wk ) almost surely, which is not acceptable.
Below, we present a partially collapsed Gibbs (PCG) sampler (van Dyk & Park, 2008) which circumvents the abovementioned challenges for an ordinary Gibbs sampler. PCG
sampling often improves the convergence by replacing
some of the conditional distributions of an ordinary Gibbs
sampler with some marginal distributions. The sampler iteratively performs the following steps:
Update of d: This is the collapsed step of our PCG sampler. Specifically, we sample dk from its conditional distribution with Wk integrated out; that is,
p(dk |X, z, µ, a, b, r) ∝ p(X|dk , z, µ, a, b)p0 (dk |r)
Z
= p0 (dk |r) · p(X|W, µ, z)dp0 (W|dk , a, b). (14)
Update of µ: Because both the prior and likelihood of µk
are Gaussian, the posterior distribution of µk is Gaussian,
too. Furthermore, since we impose a non-informative prior
on µk , its conditional distribution can be simplified as
p(µk |W, z, X) = N (µk ; x̄, Σk ),
(15)
P
n
where x̄ = n1k i=1 1[zi =k] · xi , with nk the number of
instances assigned to the kth cluster, and Σk is defined in

DP-space: Bayesian Nonparametric Subspace Clustering

Eq. (10). Intuitively, the mean of the posterior distribution
x̄ is the sample mean of all data points in cluster k.
Update of z: We need to consider two cases. First, for an
existing component k, we have
p(zi = k|z −i , Wk , µk , xi ) ∝ n−i,k · N (xi ; µk , Σk ),
where n−i,k is the number of data instances other than i
that are assigned to the k-th cluster. Second, for a new
component knew , we have
Z
p(zi = knew |α, xi ) ∝ α · p(xi |W, µ)dp0 (W, µ). (16)
Update of W: Once z is given, we can sample each Wk
separately. Specifically, when the dimension dk is fixed,
we can sample the matrix Wk by sampling its associated
orthonormal matrix U and l1 , · · · , ldk as in Eq. (10). Let
Ŝ =

1
nk

n
X

i,j=1

δzki δzkj (xi − µk )(xj − µk )>

(17)

p(lj |l−j , X, z, a, b) = Γ

D
+ a,
2



DGjj
+b
2

Update of µ: Under the scaling σ̃ 2 = σ 2 /β, we have

p(µk |W, z, X, β) = N (µk ; x̄, Wk Wk> + σ 2 /β · I).(18)
Recall that Σk = Wk Wk> + σ 2 I. Suppose Σk =
>
U(k) diag(l1 , · · · , ldk , σ 2 , · · · , σ 2 )U(k) . As β → ∞, the
update rule becomes
µk = x̄ + U(k) w,

(19)

where wj ∼ N (0, lj ) for j ≤ dk and wj = 0 for j > dk .
Alternatively, we could simply take µk = x̄ if a deterministic update rule is desired.
>

be the sample covariance matrix and Ŝ = AGA> be its
eigen decomposition. Here δzki is an indicator variable
that equals 1 if zi = k and 0 otherwise. Because a noninformative prior is imposed on U, we can take U to be
the maximum likelihood estimation 2 , which is the eigenmatrix A of the covariance matrix Ŝ (Zhang et al., 2004).
After U is determined, we use Gibbs sampling to sample
l1 , · · · , ldk separately from their conditional distribution
(Zhang et al., 2004): (assuming ldk +1 = σ 2 and l0 = +∞)
lj−1 ;

Intuitively, once we take the limit β → ∞ the posterior
variance diminishes. Similarly, we need to scale the hyperparameter b as b̃ = b/β so that the variance of the prior distribution of l−1 goes to zero. For the prior distribution over
d, we scale its hyper-parameter r using a different scaling
constant β 0 as r̃ = r exp(−β 0 ) and establish a connection
between β and β 0 as β 0 = s/δ 2 · β for some constant s.

−1 !

· I[lj+1 < lj < lj−1 ],

4. Small-variance asymptotic analysis
Though the PCG sampler is elegant, the integrals in (14)
and (16) are unfortunately very hard to evaluate due to nonconjugacy. Below, we develop an efficient algorithm with
provable convergence guarantees by performing smallvariance asymptotic (SVA) analysis to both the PCG sampler and the target posterior.
4.1. SVA behavior of the PCG sampler
We first analyze the behavior of the PCG sampler when the
variance of the likelihood model goes to zero. Specifically,
we consider the likelihood model with scaled variance parameter σ̃ 2 = σ 2 /β, where β > 0 is a scaling constant.

Update of z: Define v i,k = U(k) (xi − µk ).
Then the likelihood N (xi ; µk , Σk ) can be rewritten as
N (v i,k ; 0, diag(l1 , · · · , ldk , σ 2 , · · · , σ 2 )). Consequently,
as β → ∞, we have (for existing k)
p(zi = k|z −i , xi , µk , Wk )


D
X
1
[v i,k ]2j + o(β)
→ · exp −β/σ 2 ·
Z
j=dk +1


1
β · d(xi , Sk )2
= exp −
+
o(β)
,
(20)
Z
σ2
where Sk = S(Wk , µk ) denotes the kth subspace and the
last equality holds due to Proposition 1. Through a more
involved analysis (see Appendix B.1), we derive a similar
limiting result for the probability of creating a new cluster:
p(zi = knew |α, xi ) →

α
exp(o(β)).
Z

(21)

Finally, scale the hyper-parameter α as α = exp(−βλ/σ 2 )
and define a cost function Qi (k) as

d(xi , Sk )2 , n−i,k > 0;
Qi (k) :=
(22)
λ.
otherwise.
By Eq. (20) and (21), when β → ∞ the posterior distribution of zi will concentrate on the cluster k ∗ with the smallest cost Qi (k ∗ ). In other words, we could deterministically
update zi using
zi0 = argmin Qi (k).

(23)

k=1:K + +1

2

This is in fact an approximation of the PCG sampler. Nevertheless, in Section 4 we show that the MLE corresponds to the
update rule after applying small-variance asymptotic analysis.

Intuitively, the cost Qi (k) measures how close the given
data point xi is to an existing subspace Sk , and Eq. (23)

DP-space: Bayesian Nonparametric Subspace Clustering

says we should always put xi into the cluster whose subspace is the closest to xi . Furthermore, if xi is far away
from any existing subspaces, a new cluster is created with
only one data point, xi . This type of updates can be seen
in a number of recent development of fast inference algorithms derived using SVA analysis (Kulis & Jordan, 2012;
Jiang et al., 2012; Wang & Zhu, 2014).
Update of W: When dk is fixed and b̃ → 0, the update of
(k)
U(k) remains unchanged while the update of lj becomes
(k)

lj

=

Gjj
.
1 + 2(a − 1)/D

With U(k) and l(k) , we calculate Wk as:
q
q 
(k)
(k)
(k)
l1 , · · · , ldk ,
Wk = Udk diag

(24)

(25)

(k)

where Udk ∈ RD×dk is the left dk columns of U(k) .

Update of d: Under the SVA setting, the conditional distribution of dk is

Z

p(dk |X, z, µ, a, b, r, β, β 0 ) =

1
p0 (dk |r, β 0 )
Z(X)

p(X|W, µ, z, σ 2 , β)dp0 (W|dk , a, b, β) =: q(dk |β, β 0 ).

Through a careful limiting analysis presented in Appendix
B.2, we can show that 3


1
sdk
0
lim q(dk |β, β ) = exp −β ·
β→∞
Z
σ2
!
!
n
X d(xi , S(W, µ ))2
k
k
δzi
+ o(β) . (26)
+
inf
σ2
W∈RD×dk
i=1
Here hyper-parameter s is defined as β 0 = s/δ 2 · β, which
works as a parameter of connection between β and β 0 .
Let U be an orthogonal matrix associated with W, as
defined in Eq. (10). As we have mentioned in previous sections, when the offset µk is fixed the distance
d(xi , S(W, µk )) only depends on the projection matrix U.
By PCA, for a fixed d, the optimal U can be obtained as
U = Ad where Ad is the matrix of top d eigenvectors of
the Ŝ. By Eq. (26), as β → ∞ the posterior distribution of
dk will concentrate on one specific value and subsequently
we get a deterministic update rule for dk :
(
)
n
X
k
2
dk = argmin s · d +
δzi · d(xi , S(Ad , µk )) .(27)
d=0:D−1

i=1

Intuitively, the update rule (27) strikes for a balance between model complexity and data fitness as measured by
3

limβ→∞ f (β) = g(β) means limβ→∞ f (β)/g(β) = 1.

Algorithm 1 The DP-space algorithm
1: Input: data X = {xi }n
i=1 , parameters a, λ, s.
2: Initialize: K + = 1 cluster, with d1 = 0, µ1 = X̄.
3: while not converge do P
n
4:
Update µ: µk = n1k i=1 δzki · xi , k ∈ [K + ].
5:
for each k ∈ [K + ] do
6:
Compute Ŝk using Eq. (17);
7:
Compute the diagonalization Ŝk = AGA> .
8:
Update U(k) : U(k) = A.
9:
Update dk using Eq. (27)
(k)
10:
Update lj using Eq. (24), j ∈ [dk ];
11:
Update Wk using Eq. (25);
12:
end for
13:
Update z: set each zi using Eq. (23);
14:
Remove empty clusters and re-calculate K + .
15: end while
+
n
16: Output: K + , {Wk , dk , µk }K
k=1 and {zi }i=1 .
P
the term i δzki · d(xi , S(Ad , µk ))2 . When the subspace
dimension d increases the distance decreases and hence the
model better fits the training data. It contrasts the first term
s · d, which controls the model complexity by imposing a
linear penalty on subspace dimension d.
4.2. SVA of the posterior
We now derive a deterministic loss function defined on the
training data set via SVA analysis on the posterior distribution and shows that Algorithm 1 converges by decreasing
the loss function at each iteration. To begin with, we write
the CRP prior as an exchange partition probability function
(Aldous, 1985; Pitman, 1995):
+

p0 (z|α) = α

K + −1

K
Γ(α + 1) Y
(nk − 1)!,
Γ(α + n)

(28)

k=1

where K + is the number of non-empty clusters and nk is
the number of instances in cluster k. Subsequently, the posterior of DP-PCA can be expressed as:
K+

Y
+
1
p(z, W, d, µ|X) =
αK ψ(n)
p0 (µk |ρ)p0 (dk |r)
Z(X)
k=1

·p0 (Wk |dk , a, b)

n
Y

i=1

p(xi |zi = k, Wk , µk ).

(29)

QK +
Γ(α+1)
Here ψ(n) = k=1 (nk − 1)!. αΓ(α+n)
is absorbed in
Z(X) because it does not depend on model parameters.
With a = 1 and the scaling σ̃ 2 = σ 2 /β, r̃ = r exp(−s/δ 2 ·
β), α = exp(−λ/δ 2 · β), b̃ = b/β, we have
p(z, W, d, µ|X, β) = 1/Z(X, β) exp −β/σ 2 λK +

DP-space: Bayesian Nonparametric Subspace Clustering
+

+s

K
X

dk +

k=1

n
X
i=1





d(xi , Szi )2  + o(β) . (30)

As a result, when we take β → ∞ the posterior distribution will concentrate on its mode, that is, the model
θ ∗ = (z ∗ , W∗ , d∗ , µ∗ ) that minimizes the loss
+

+

L(z, W, d, µ) := λK + s

K
X

k=1

dk +

n
X

d(xi , Szi )2 .(31)

i=1

The loss function has an intuitive tradeoff between data fitness measured by the last term and model complexity measured by (K + , {dk }). It also reveals practical interpretations of the two hyper-parameters λ and s. For example, λ
could be viewed as the distance threshold between a data
point and the subspace it is associated with. similarly, the
parameter s characterizes the residue after PCA for each
subspace and the algorithm is encouraged to increase dk
once the residue exceeds s. In some applications (e.g. clustering gene sequences) hyper-parameters could be directly
set according to their physical meanings.
Theorem 1 states that the DP-space algorithm monotonically decreases this loss function at each iteration. We defer
its proof to Appendix B.3.
Theorem 1. Let W(t) ,d(t) , µ(t) and z (t) be model parameters output by Algorithm 1 after iteration t. Then
for all t we have L(W(t+1) , d(t+1) , µ(t+1) , z (t+1) ) ≤
L(W (t) , d(t) , µ(t) , z (t) ).

5. Related work
The Agglomerative Lossy Compression (ALC) (Ma et al.,
2007) algorithm builds a clustering solution by first creating one cluster for each data point, and then combining existing clusters in a greedy manner. The algorithm does not
need prior knowledge of the number of clusters and subspace dimensions. Instead, a distortion parameter δ is required to reflect the distortion level in the underlying data.
ALC is different from DP-space in two ways: first, ALC
minimizes the coding length needed to fit data points instead of finding solutions near MLE; second, unlike ALC,
DP-space first puts all instances into a giant cluster and tries
to create new clusters in the process.

dal, 2013) algorithm solves a LASSO regression problem
for each data point to form a similarity graph. Then spectral
clustering methods are used to cluster data into subspaces.
SSC performs well on real-world data sets (Vidal, 2010)
and in general K and dk do not need to be known a priori. However, SSC is based on `1 optimization techniques,
which could be slow on large data sets. Our experiments
show that DP-space is much faster than SSC while achieving slightly worse performance.
(Chen et al., 2010) proposed a nonparametric Bayesian
mixture of factor analysis (MFA). Similar to ours, both the
mixture number and subspace rank can be inferred from
data. However, the nonparametric treatments of subspace
dimension are different. In (Chen et al., 2010) an auxiliary
indicator vector z is introduced with a Beta process prior
while in our model we place a geometric prior directly on
the subspace rank. Our method is arguably more direct,
though it results in non-conjugacy. Furthermore, by SVA
analysis we obtain a new efficient algorithm.
Finally, small-variance asymptotic (SVA) analysis is a
powerful method that speeds up the inference of nonparametric Bayesian models, with many recent advances
on performing SVA analysis to the popular nonparametric
models, including Dirichlet process mixture models (Kulis
& Jordan, 2012; Jiang et al., 2012), nonparametric latent factor models (Broderick et al., 2013), infinite HMMs
(Roychowdhury et al., 2013) and infinite SVMs (Wang &
Zhu, 2014). Our work contributes by presenting a novel
nonparametric Bayesian model as well as a non-trivial extension of existing SVA methods to the challenging task of
subspace clustering.

6. Experiments
We compare DP-space with several competitors on both
synthetic and real-world datasets. All methods are implemented in Matlab. Throughout the experiments we always
set the hyper-parameter a to be 1, under which no shrinkage
on lj is imposed. The other hyper-parameters are selected
via cross-validation.
6.1. Synthetic examples

The Random Sample Consensus (RANSAC) (Fischler &
Bolles, 1981) algorithm fits a subspace of dimension d by
randomly sampling d + 1 points from the training data. It
does not need to know the number of subspaces. However,
the algorithm does require the knowledge of subspace dimensions, and a drawback of RANSAC is that it generally
performs bad when there are many subspaces or the subspace dimensions are incorrectly set (Yang et al., 2006).

We first analyze how DP-space uses subspaces of different
dimensions to cluster synthetic data. The dataset contains
10, 000 data points from R3 . They are divided into 4 clusters with equal probability. Among the 4 clusters, two lie
on 1-d affine subspaces and the other two lie on 2-d affine
subspaces. A Gaussian white noise N (0, 0.05ID×D ) is imposed on each data point. We show the clustering results of
DP-space under different settings of hyper-parameters, and
also compare it with K-means and the ground truth.

The Sparse Subspace Clustering (SSC) (Elhamifar & Vi-

Fig. 1 shows the ground truth (with subspaces depicted in

DP-space: Bayesian Nonparametric Subspace Clustering
4
3
2
1
0
−1
−2
−3
4
2

2
0

0
−2

−2
−4

(a)

−4

(b)

Figure 1. (a) the ground truth; and (b) the clustering result of Kmeans with K = 4.

different colors) and the clustering results of K-means with
K = 4. As shown in Figure 1(b), K-means does not capture the concept of subspace well. For instance, the last few
instances lying on the green line are misclassified because
they are closer to the center of another cluster in Euclidean
distance. Consequently, the Normalized Mutual Information (NMI) 4 score is as low as 0.610, much worse than
DP-space shown in Figure 2.
Fig. 2 shows the clustering results on the synthetic
dataset by DP-space with different hyper-parameter settings. When λ and s are set properly as in Fig. 2(a) the
clustering result is very close to the ground truth. Both the
number of clusters K and subspace dimensions dk are inferred correctly. When λ increases as in Figure 2(b), we expect to get a smaller number of clusters because we place a
larger penalty on K + . In particular, the two 1-d subspaces
are merged into a single 2-d subspace cluster. On the other
hand, smaller s would result in subspaces of higher dimension. This is shown in Fig. 2(c): we still get 4 clusters,
but now all subspaces are of dimension two. Furthermore,
s also controls the number of clusters in an indirect way
because for any additional cluster we add a penalty on its
subspace dimension. For instance, in Fig. 2(d) the λ parameter remains 1.5 but s is as large as 10.0. We can see
that again the number of clusters returned decreases to 3.
We next turn to larger synthetic datasets. The dataset now
contains 100, 000 data points from R10 . Data points are
divided with equal probability into K = 6 clusters. The
underlying subspace dimensions of the K clusters are set
as d = (d1 , · · · , d6 ) = (2, 2, 3, 3, 4, 4). A similar Gaussian white noise is imposed on each data point. We report the clustering performance and running time for DPspace and other baseline algorithms on 10 i.i.d. generated
synthetic datasets. Similar to (Kulis & Jordan, 2012), we
use NMI to evaluate clustering performance with different
cluster numbers. The hyper-parameters of non-parametric
4

Let A = {a1 , · · · , an }, B = {b1 , · · · , bm } be the
output and ground truth clusterings. The NMI score is deI(A;B)
fined as NMI(A, B) := [H(A)+H(B)]/2
, where I(A; B) =
P |ωj ∩ck |
P |ω |
N |ωj ∩ck |
|ω |
log |ωj ||ck | and H(A) = − j Nj log Nj . N
j,k
N
is the total number of examples.

(a) λ = 1.5, s = 1; K = 4, (b) λ = 5, s = 1; K = 3,
NMI: 0.910
NMI: 0.744

(c) λ = 1.5, s = 0.5; K = 4, (d) λ = 1.5, s = 10; K = 3,
NMI: 0.898
NMI: 0.727
Figure 2. Clustering results of DP-space under different settings
of λ and s, where K is the number of recovered clusters.
Table 1. Average NMI, running time (seconds), cluster number
and subspace dimension on synthetic datasets. For parametric
models the number of clusters and subspace dimensions are fixed.

Algorithm

NMI

Time

Cluster no.

Dim.

K-means
DP-means
MPPCA, d = 2
MPPCA, d = 4
MPPCA, d = 4
DP-space

.713
.697
.847
.924
.999
.972

2.6
23.6
43.6
32.6
19.8
12.5

6
26.3
6
5
6
6.3

2
4
4
4.2

models are selected on one tenth of the data set using NMI.
We can see that DP-space achieves similar NMI scores with
EM-MPPCA while being faster. Furthermore, DP-space
correctly selects the number of clusters and subspace dimensions. On the other hand, when K or dk is incorrectly
specified, the performance of EM-MPPCA deteriorates.
6.2. Application to Motion Segmentation
Motion segmentation usually refers to the task of separating the movements of multiple rigid-body objects from
video sequences. Subspace clustering methods are popular in this task by simultaneously clustering the point trajectory data, which can be extracted using tracking methods, into multiple subspaces and finding a low-dimensional
subspace fitting each group of points. See (Vidal, 2010; Elhamifar & Vidal, 2013) for a comprehensive treatment.
In Table 2, we compare the performance of several subspace clustering algorithms on the Hopkins 155 motion

DP-space: Bayesian Nonparametric Subspace Clustering
Table 2. Average classification error (%) and running time (seconds) on the Hopkins-155 dataset, where “-” means the algorithm
usually doesn’t operate under the specific projection setting.

GPCA
RANSAC
ALC
EM-MPPCA-a
EM-MPPCA-m
DP-space
LRR
SSC-ADMM

r=5
error
time

r = 4N/SP
error
time

10.34
9.76
3.76
18.56
3.49
3.32
-

11.55
3.37
24.88
7.28
3.29
3.74
2.41

13.2
1.4
347.8
3.8
3.8
2.1
-

18.2
352.1
6.2
6.2
2.1
294.5
542.3

segmentation dataset (Tron & Vidal, 2007). Each sequence
in the dataset contains motions of N = 2 or 3 different
objects. A detailed summary of statistics for the Hopkins155 dataset can be found in Appendix C.1. Performance is
measured in terms of classification error, which is the percentage of misclassified video sequences. Specifically, we
consider all permutations of cluster assignments and compare them against the ground-truth classification. If more
clusters than the number of objects are returned, all video
sequences in extra clusters will be ruled as misclassified.
We compare the performance of DP-space with EMMPPCA and several other baseline algorithms, including Generalized PCA (GPCA) (Vidal et al., 2005),
RANSAC (Fischler & Bolles, 1981), ALC (Ma et al.,
2007), LRR (Liu et al., 2013) 5 and SSC (Elhamifar &
Vidal, 2013). Segmentation performance is measured in
terms of classification error, which is the percentage of misclassified point trajectories. Following the convention in
(Vidal, 2010), we first project point projectories onto an rdimensional subspace using PCA, with r varying from 5
to 4N or a level that preserves sparsity (SP). 6 We run the
EM-MPPCA algorithm using 10 random initializations on
each video sequence and report both the average performance (MPPCA-a) and the best performance (MPPCA-m)
over different initializations. Parameters of the DP-space
algorithm are selected using 30% of the ground-truth labels
according to NMI. Values of λ range from 10−3 to 102 and
values of s range from 10−2 to 102 . Classification errors
for GPCA, RANSAC, ALC are cited from (Vidal, 2010)
and from (Elhamifar & Vidal, 2013) for LRR and SSC.
Running time for LRR is cited from (Liu et al., 2013) and
running time for SSC is measured using implementations
provided in (hop). For SSC we use the ADMM implemen5
For LRR a heuristic post-processing step is adopted, as implemented in the code (lrr).
6
This requires r ≥ 8 log(2F/r), where F is the number of
frames collected.

Table 3. Clustering error and running time for SSC-ADMM under
constrained number of ADMM iterations (T ).

T

5

10

20

30

40

error
time

5.03
2.8

3.64
5.6

3.33
10.9

3.20
16.0

2.88
21.4

tation because it is faster and also more accurate than the
original CVX implementation (Elhamifar & Vidal, 2013).
For all algorithms the reported running time does not include the time for parameter selection or cross-validation.
From Table 2 we see that DP-space performs far better than
GPCA, RANSAC and the EM-MPPCA algorithm, and its
performance is comparable to ALC and LRR. Although
DP-space does not perform as good as SSC in terms of
classification accuracy, it is significantly faster than SSC
because SSC is an optimization based algorithm and requires solving P Lasso problems, each with P variables
(where P is the number of points in each video sequence).
For the Hopkins 155 dataset P ranges from 100 to 400,
as shown in Appendix C.1. In Appendix C.2 we present a
more detailed comparison of classification error.
Since optimization based methods like SSC-ADMM are
slower but more accurate, we further investigate the tradeoff between clustering accuracy and running time. In Table
3 we constrain the maximum number of ADMM iterations
for SSC and report the corresponding clustering accuracy
and running time. The results show that when SSC-ADMM
is run with very few iterations its performance suffers. On
the other hand, SSC-ADMM takes much longer to achieve
the same level of clustering accuracy as DP-space does.
6.3. Discussion
Unlike DP-means, the DP-space algorithm always achieves
a significant improvement over the parametric MPPCA
model on real-world datasets, as shown in Table 2. There
could be several reasons for this phenomenon.
First, DP-space enjoys the advantage of selecting different subspace dimension for different clusters, while the dimension is assumed to be the same for all components in
MPPCA, for which an exhaustive search over all possible
combinations of subspace dimension is simply intractable.
This provides additional modeling flexibility for DP-space.
Second, the EM-MPPCA algorithm is very sensitive to its
initialization (Elhamifar & Vidal, 2013). This fact is verified in Table 2: when the best performance of EM-MPPCA
over 10 different initializations is reported, the classification accuracy is much better than when only the average
performance is reported, and in fact the performance is
comparable with DP-space and ALC.

DP-space: Bayesian Nonparametric Subspace Clustering

Acknowledgements
This work was conceived when Y.W. was at Tsinghua. The
work is supported by the National 973 Basic Research Program of China (No. 2013CB329403, 2012CB316301),
National NSF of China (No. 61322308, 61332007),
and Tsinghua Initiative Scientific Research Program (No.
20121088071, 20141080934).

References
Code for several subspace clustering algorithms. http:
//vision.jhu.edu/code/. [Online; accessed 16Aug-2014].
Motion segmentation and face clustering by lrr. https:
//sites.google.com/site/guangcanliu/.
[Online; accessed 17-Jan-2015].
Aldous, D. Exchangeability and related topics. École
d’Été de Probabilités de Saint-Flour XIII1983, pp. 1–
198, 1985.
Broderick, Tamara, Kulis, Brian, and Jordan, Michael.
Mad-bayes: Map-based asymptotic derivations from
bayes. In ICML, 2013.
Chen, Minhua, Silva, Jorge, Paisley, John, Wang, Chunping, Dunson, David, and Carin, Lawrence. Compressive sensing on manifolds using a nonparametric mixture
of factor analyzers: Algorithm and performance bounds.
IEEE Transactions on Signal Processing, 58(12):6140–
6155, 2010.
Elhamifar, Ehsen and Vidal, Rene. Sparse subspace clustering: Algorithm, theory and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35
(11):2765–2781, 2013.
Fischler, Martin and Bolles, Robert. Random sample consensus: a paradigm for model fitting with applications to
image analysis and automated cartography. Communications of the ACM, 24(6):381–395, 1981.
Ho, Jeffrey, Yang, Ming-Hsuan, Lim, Jongwoo, Lee,
Kuang-Chih, and Kriegman, David. Clustering appearances of objects under varying illumination conditions.
In CVPR, 2003.
Jiang, Ke, Kulis, Brian, and Jordan, Michael. Smallvariance asymptotics for exponential family dirichlet
process mixture models. In NIPS, 2012.
Kulis, Brian and Jordan, Michael. Revisiting k-means:
New algorithms via bayesian nonparametrics. In ICML,
2012.

Liu, Guangcan, Lin, Zhouchen, Yan, Shuicheng, Sun, Ju,
Yu, Yong, and Ma, Yi. Robust recovery of subspace
structures by low-rank representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35
(1):171–184, 2013.
Ma, Yi, Derksen, Harm, Hong, Wei, and Wright, John.
Segmentation of multivariate mixed data via lossy data
coding and compression. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 29(9):1546–1562,
2007.
Minka, Thomas. Automatic choice of dimensionality for
pca. In NIPS, 2000.
Pitman, J. Exchangeable and partially exchangeable random partitions. Probability Theory and Related Fields,
102(2):145–158, 1995.
Roychowdhury, Anirban, Jiang, Ke, and Kulis, Brian.
Small-variance asymptotics for hidden markov models.
In NIPS, 2013.
Tipping, Michael and Bishop, Christopher. Mixtures of
probabilistic principle component analyzers. Neural
Computation, 11(2):443–482, 1999.
Tron, Roberto and Vidal, Rene. A benchmark for the
comparison of 3-d motion segmentation algorithms. In
CVPR, 2007.
van Dyk, David and Park, Taeyoung. Partially collapsed gibbs samplers: Theory and methods. Journal
of the American Statistical Association, 103(482):790–
796, 2008.
Vidal, Rene. A tutorial on subspace clustering. IEEE Signal
Processing Magazine, 28(2):52–68, 2010.
Vidal, Rene, Ma, Yi, and Sastry, Shankar. Generalized
principal component analysis (gpca). IEEE Transactions
on Pattern Analysis and Machine Intelligence, 27(12):
1945–1959, 2005.
Vidal, Rene, Tron, Roberto, and Hartley, Richard. Multiframe motion segmentation with missing data using
power factorization and gpca. International Journal of
Computer Vision, 79(1):85–105, 2008.
Wang, Yining and Zhu, Jun. Small variance asymptotics
for dirichlet process mixtures of svms. In AAAI, 2014.
Yang, Allen, Rao, Shankar, and Ma, Yi. Robust statistical
estimation and segmentation of multiple subspaces. In
CVPR Workshop, 2006.
Zhang, Zhihua, Chan, Kap-Luk, Kwok, James, and Yeung, Dit-Yan. Bayesian inference on principle component analysis using reversible jump markov chain monte
carlo. In AAAI, 2004.

