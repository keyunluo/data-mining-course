A Convex Exemplar-based Approach to
MAD-Bayes Dirichlet Process Mixture Models

Ian E. H. Yen 1
IANYEN @ CS . UTEXAS . EDU
Xin Lin 1
JIMMYLIN @ UTEXAS . EDU
Kai Zhong 2
ZHONGKAI @ ICES . UTEXAS . EDU
Pradeep Ravikumar 1,2,3
PRADEEPR @ CS . UTEXAS . EDU
Inderjit S. Dhillon 1,2
INDERJIT @ CS . UTEXAS . EDU
1
Department of Computer Science, University of Texas at Austin, TX 78712, USA.
2
Institute for Computational Engineering and Sciences, University of Texas at Austin, TX 78712, USA.
3
Department of Statistics and Data Sciences, University of Texas at Austin, TX 78712, USA.

Abstract
MAD-Bayes (MAP-based Asymptotic Derivations) has been recently proposed as a general technique to derive scalable algorithm for
Bayesian Nonparametric models. However, the
combinatorial nature of objective functions derived from MAD-Bayes results in hard optimization problem, for which current practice employs heuristic algorithms analogous to k-means
to find local minimum. In this paper, we consider the exemplar-based version of MAD-Bayes
formulation for DP and Hierarchical DP (HDP)
mixture model. We show that an exemplarbased MAD-Bayes formulation can be relaxed
to a convex structural-regularized program that,
under cluster-separation conditions, shares the
same optimal solution to its combinatorial counterpart. An algorithm based on Alternating Direction Method of Multiplier (ADMM) is then
proposed to solve such program. In our experiments on several benchmark data sets, the proposed method finds optimal solution of the combinatorial problem and significantly improves existing methods in terms of the exemplar-based
objective.

1. Introduction
In the recent years, MAD-Bayes (MAP-based Asymptotic Derivations) has been proposed as a technique to obtain scalable algorithm for Bayesian Nonparametric modProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

els such as Dirichlet Process (DP) Mixture, Latent Feature
Allocation and Infinite Hidden Markov Model (Broderick
et al., 2013; Kulis & Jordan, 2012; Roychowdhury et al.,
2013; Jiang et al., 2012; Campbell et al., 2013). The objective function derived from MAD-Bayes, however, hard to
optimize and existing approaches employ algorithms analogous to k-means that only guarantees to find local optimum. In this paper, we consider exemplar-based version of
the MAD-Bayes objective, where the mean parameters of
each mixture are restricted to be one of the data samples.
The exemplar-based clustering, namely k-medoids problem, has been investigated since 1987 and known for its
ability to handle arbitrary dissimilarity measure (Kaufman
& Rousseeuw, 1987). The k-medoids problem, however,
is still NP-hard in general (Papadimitriou, 1981; Megiddo
& Supowit, 1984) and the best known approximate
√ algorithm guarantees an approximation ratio of 1 + 3 +
 ≈ 2.732 with theoretical limit being 1 + 2/e ≈ 1.736.
Commonly used algorithms such as Partitioning Around
Medoids (PAM) (Van der Laan et al., 2003) and Affinity
Propagation (AP) (Frey & Dueck, 2007) only guarantee to
find local optimum. On the other hand, a recent interest
in the exemplar-based formulation arose due to its natural convex relaxation (Yaron & Shalev-Shwartz, 2010; Elhamifar et al., 2012; Awasthi et al., 2015; Nellore & Ward,
2013) . The convex relaxation, without additional assumptions, could lead to fractional solutions and thus does not
guarantee to find optimum of the original problem (Yaron
& Shalev-Shwartz, 2010). However, Elhamifar et al. (Elhamifar et al., 2012) and later on Nellore et al. (Nellore
& Ward, 2013) proved that, when there exists clustering
assignments satisfying certain separation requirement (that
is, the dissimilarities within cluster are small enough compared to the dissimilarities between clusters), the convex relaxation guarantees to find the optimal solution of its com-

A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models

binatorial counterpart.
In this paper, we show that the theory for convex relaxation
of k-medoids also applies to a variety of Bayesian Nonparametric models through connection to the MAD-Bayes
formulation. This results in an exemplar-based objective
with convex structural-regularized relaxation that guarantees to recover the optimal solution under the separation
requirement.
The paper will be organized as follows. In Section 2, we
derive relaxation for the exemplar-based Dirichlet Process
(DP) and Hierarchical DP (HDP) mixture models under
MAD-Bayes formulation. Then in Section 3, we provide
sufficient conditions under which one can recover the optimal solution from the relaxation. In Section 4, an algorithm based on Alternating Direction Method of Multiplier
(ADMM) is introduced to efficiently solve the convex program. In Section 6, we experiment on several benchmark
data sets, for which the proposed approach recovers the
optimal solution of the combinatorial problem and significantly improves the objective achieved by existing methods.

2. Exemplar-Based MAD-Bayes
In this section, we derive convex relaxation for the
exemplar-based MAD-Bayes estimation of DP and HDP
mixture models.

those algorithms provides guarantee for the quality of solution w.r.t. its objective.
Here we consider the exemplar-based version of (1), which
confines the optimization space of µk to a finite set of candidates (exemplars) E = {µ̄j }Jj=1 instead of the whole parameter space Rp . For mixture model a natural choice of E
is the sample set {xj }N
j=1 itself. The problem can then be
written as

min

N X
J
X

wij ∈{0,1}

s.t.

wij D(xi , µ̄j ) + λ

i=1 j=1
J
X

J
X
j=1

max wij

i∈[N ]

(2)

wij = 1, ∀i.

j=1

PJ
where with wij ∈ {0, 1}, the term j=1 maxi∈[N ] wij
simply counts number of exemplars j assigned to any sample i. In contrast to formulation (1) where D(xi , µj ) must
be a Bregman Divergence so the minimization w.r.t. µj can
be carried out efficiently 1 , the exemplar-based objective
(2) takes the advantage that D(xi , µ̄j ) is a pre-computable
constant that can be any measure of dissimilarity. By replacing integer constraint wij ∈ {0, 1} with nonnegative
constraint wij ≥ 0, we obtain a convex problem of linear loss function, group-sparse regularization, and simplex
constraint:

2.1. Exemplar-based Dirichlet Process (DP) Mixture
Given a mixture model with DP prior p(z) and some observation distribution p(x|z) belonging to the exponential
family, the MAP inference of {zi }N
i=1 given observation
{xi }N
can
be
formulated
via
MAD-Bayes
as
i=1
min

zi ∈[K],µk ∈Rp ,K

N
X

D(xi , µzi ) + λK

(1)

i=1

by taking variance of p(x|z) asymptotically to 0, where
D(.) is Bregman divergence associated with the partition
function of p(x|z) (Kulis & Jordan, 2012; Broderick et al.,
2013; Jiang et al., 2012). For p(x|z) being Spherical Gaussian, D(.) is simply the square Euclidian distance and
for p(x|z) being Multinouli distribution, D(.) is the KLdivergence. Note that K in the formulation is also a variable to optimize, which, as in the spirit of Bayesian Nonparametrics, provides the flexibility to vary number of clusters based on the supports from the data. In (Kulis & Jordan, 2012), an algorithm analogous to k-means called DPmeans was proposed to solve (1) with Gaussian observation
distribution, which was then adopted and generalized in
(Jiang et al., 2012) to solve instances of exponential family
observation distribution, and improved in (Broderick et al.,
2013) using idea of collapsed sampling. However, none of

min

×J
W ∈RN
:W 1=1
+

D ◦ W + λkW k∞,1

(3)

where ◦ is the element-wise inner product, D is an N by
N matrix with Dij = D(xi , µ̄j ), and 1 is J by 1 vector of
all elements equal to 1. Note without imposing additional
conditions, (3) might have fractional solutions. Before discussion of the recovery conditions, we first introduce HDP
mixture, a generalization of DP mixture when data come in
groups.
2.2. Exemplar-based Hierarchical Dirichlet Process
(HDP) Mixture
In a HDP mixture model, samples {xi }N
i=1 are grouped
into D data sets T1 ,..,TD , each of which is modeled as a
local DP mixture DP (α, G), while G is a base distribution
drawn from a global DP shared among all data sets. The
resulting MAD-Bayes estimation problem, as derived in
(Kulis & Jordan, 2012; Jiang et al., 2012), has two penalty
terms, one for the number of global clusters Kg and the
1

The fact that mean serves as minimizer of a sum of Bregman
Divergence w.r.t. the second parameter is used to derive the DPmeans algorithm.

A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models

other for the number of clusters used by each data set Kd :
N
X

min

zi ∈[Kg ],µk ,Kg ,Kd

D(xi , µzi ) + θKl + λKg

Theorem 1. Suppose there exists a clustering {Sk }k∈M
for which we can find λ such that
max max Nk δij < λ <

k∈M i,j∈Sk

i=1

min

min

(k,l∈M,k6=l) (i∈Sk ,j∈Sl )

Nk δij

(4)

(7)
where Nk = |Sk | and δij = D(xi , xj ) − D(xi , xM (i) ),
then the integer solution W ∗ realizing {Sk }k∈M is unique
optimal solution to both (2) and (3).

where θ, λ are the hyper-parameters for local and global
penalty respectively. To obtain the exemplar-based version,
we constrain the space of µk to a finite candidate set E =
{µ̄j }Jj=1 , so the problem can be written as

Note that Theorem 1 is consistent with Corollary 7 given
by (Nellore & Ward, 2013), where the Lagrange multiplier
u in (Nellore & Ward, 2013) corresponds to our regularization parameter λ. Since DP is a special case of HDP, the
proof for Theorem 1 is also a special case of that for Theorem 2. When each cluster has same size, Theorem 1 simply states a separation condition that requires the dissimilarity between points of different clusters, subtracted by
the dissimilarity to cluster representative, larger than that
between points in the same cluster. Intuitively, the separation condition is very reasonable. The fractional solution
can be viewed as assigning a data point to multiple clusters. When clusters are separated with an enough distance,
it is not likely for a data point being assigned to multiple
clusters. When the measure of dissimilarity D(.) satisfies
triangular inequality, we can further simplify the result as
follows.

s.t.

Kl =

D
X

Kd

d=1

Kd = |{zi |i ∈ Td }| , d = 1, .., D.

N X
J
X

min

wij ∈{0,1}

wij D(xi , µ̄j )

i=1 j=1

+θ

D X
J
X
d=1 j=1

J
X

s.t.

max wij + λ
i∈Td

J
X
j=1

max wij (5)

i∈[N ]

wij = 1, ∀i.

j=1

By replacing wij ∈ {0, 1} with wij ≥ 0, we obtain the
corresponding convex relaxation
min

W ∈RN ×J

s.t.

D ◦ W + θkW kG + λkW k∞,1
W 0

(6)

W1 = 1

Corollary 1. Assuming D(·, ·) is a metric and Nk =
N/K, ∀k ∈ M, where K is the number of clusters, if there
exists some R such that for any i
D(xi , xM (i) ) < R,

where k.kG is a latent group norm (Obozinski et al., 2011)
defined by the collection of groups G = {gd,j |d ∈
[D], j ∈ [J]}, with each group of indices defined as gd,j =
{(i, j)|i ∈ Td }.

and for ∀i ∈ Sk , ∀j ∈ Sl with k 6= l,

3. Optimality Guarantee

then the optimal solutions of (2) and (3) are identical.

In this section, we give conditions under which the convex formulations (3), (6) share the same optimal solution
with its combinatorial counterparts (2), (5). In particular, we show that if there exists a clustering that satisfies
certain separation requirement, then it corresponds to an
unique optimal solution W ∗ of both combinatorial and convex formulations with some λ, θ. Note an integer solution W ∗ of (2), (5) corresponds to a clustering {Sk }k∈M ,
∗
where M = {j|∃i, wij
= 1} is the set of representative exemplars selected out of the candidate set E, and
∗
Sk = {i|wik
= 1} is a set containing samples belonging
to cluster represented by exemplar k. We use M (i) to denote the representative i-th sample assigned to. Note each
representative k ∈ M has minimal average dissimilarity to
∀i ∈ Sk . In the following, we will assume exemplar set E
to be the set of samples E = {xj }N
j=1 . All proofs will be
included in appendix.

Note that, according to condition (7), a clustering
{Sk }k∈M with larger extent of separation corresponds to
a larger range of λ. Solving (3) with this range of λ
leads to the solution {Sk }k∈M . One can further show that
kW ∗ k∞,1 is a non-increasing function of λ, and solutions
with same kW ∗ k∞,1 must be the same. We have following
proposition independent of Theorem 1.

D(xi , xj ) > 2R,

Proposition 1. For λ1 and λ2 with λ1 < λ2 and their
corresponding (unique) optimal solutions W1∗ := W ∗ (λ1 )
and W2∗ := W ∗ (λ2 ), we have kW1∗ k∞,1 ≥ kW2∗ k∞,1 , and
if kW1∗ k∞,1 = kW2∗ k∞,1 , then W1∗ = W2∗ , and W ∗ (λ) =
W1∗ for any λ ∈ [λ1 , λ2 ].
For HDP mixture problem (6), the analysis becomes more
involved, since the separation condition should be different for samples coming from same data set and that from
different data sets.

A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models

Algorithm 1 ADMM for exemplar-based HDP mixture (6)

Figure 1. Examples of clustering described in Corollary 1 (left)
and Corollary 2 (right).

Theorem 2. Given a clustering {Sk }k∈M , denote Sk,d =
Sk ∩ Td , Dk = {d|Sk,d 6= ∅}, and d(i) the index of data
set i belonging to. If we can find λ, θ such that, for ∀i, j ∈
Sk , ∀k ∈ M,
λ
θ
> δij
+
Nk
Nk,d(i)
λ
1 X
>
δij , ∀d ∈ Dk
Nk
Nk,d

(8)
(9)

then the optimal solutions of (5) and (6) are identical.

and for ∀i ∈ Sk , ∀j ∈ Sl6=k ,
θ
λ
< δij , Dk ∩ Dl =
6 ∅ (10)
+
Nk
Nk,d(i)


1
λ
1
−
< δij , Dk ∩ Dl = ∅ (11)
+θ
Nk
Nk,d(i)
Nd(i)
, where Nk = |Sk |, Nd = |Td |, and Nk,d = |Sk ∩ Td |,
then the integer solution W ∗ realizing {Sk }k∈M is unique
optimal solution to both (5) and (6).
For θ = 0, (10) and (11) are equivalent to the RHS of (7),
while (8) gives the LHS of (7) and implies (9), which then
gives the same condition as in Theorem 1. For θ > 0, inequality (11) requires less dissimilarity than that of inequality (10), which means the inter-cluster dissimilarity can be
smaller if two clusters do not share samples from the same
dataset. By tuning θ, the convex HDP formulation (6) can
utilize structure of datasets to realize clustering {Sk }k∈M
not realizable by the DP formulation (3). For D(., .) being
a metric, we can further simplify result as follows.
Corollary 2. Assuming D(·, ·) is a metric, Nk = N/K,
|Dk | = C is a constant, Nk,d = N/(CK) for all d ∈ Dk
and Kd = CK/D, then if there exists some R such that
(a) for ∀i,
D(xi , xM (i) ) < R,

D(xi , xj ) > 2R,
(c) for ∀i ∈ Sk , ∀j ∈ Sl with k 6= l and Dk ∩ Dl = ∅,
D(xi , xj ) > 2R − r,

where


1 
1 X
r=
R−
max
δij 
k∈M,j∈Sk ,d∈Dk Nk,d
Kd

i∈Sk,d

(b) for ∀i ∈ Sk , ∀j ∈ Sl with k 6= l and Dk ∩ Dl 6= ∅,

Initialize t = 0, W (t) = Y (t) = Z (t) ← 0.
repeat
(t+1)
1. Solve each row of W1
in (12) via Frank-Wolfe
algorithm.
(t+1)
2. Solve each column of W2
in (13) via proximal
mapping.
(t+1)
(t+1)
3. Z (t+1) = (W1
+ W2
)/2.
(t+1)
(t)
(t+1)
4. Yq
← Yq +α(Wq
−Z (t+1) ), for q = 1, 2.
5. t = t + 1.
(t)
(t)
until kZ (t) − Z (t−1) k < 1 and kW1 − W2 k < 2

i∈Sk,d

Figure 1 gives two examples that compare conditions of
Corollary 1 and 2. For HDP, we also show in the following that kW ∗ k∞,1 , kW ∗ kG are monotonically decreasing
with λ, θ respectively, and the optimal solutions in some
triangular area of the (λ, θ) parameter space are the same if
the corners of the triangular area lead to the same values of
kW ∗ k∞,1 , kW ∗ kG .
Proposition 2. kW ∗ k∞,1 , kW ∗ kG are monotonically decreasing with λ, θ respectively, and for regularization parameters (λ1 , θ1 ), (λ1 , θ2 ) and (λ2 , θ2 ) with λ1 < λ2
and θ1 < θ2 , define their corresponding optimal solutions
∗
W1∗ := W ∗ (λ1 , θ1 ), W12
:= W ∗ (λ1 , θ2 ) and W2∗ :=
∗
W (λ2 , θ2 ). Assuming unique optimal solution, then if
∗
∗
kW1∗ kG = kW12
kG and kW2∗ k∞,1 = kW12
k∞,1 ,
∗
we have W1∗ = W12
= W2∗ and furthermore for any

(λ, θ) ∈ Conv ((λ1 , θ1 ), (λ1 , θ2 ), (λ2 , θ2 ))
the corresponding optimal solution
W ∗ (λ, θ) = W1∗ ,
where Conv(·) is the convex hull function.

4. Algorithm
In this section, we propose an algorithm based on ADMM
(Alternating Direction Method of Multiplier) to solve the
convex formulation of exemplar-based HDP mixture (6).
The exemplar-based DP mixture (3) can be solved via
the same algorithm by setting θ = 0 and ignoring any
group structure imposed by data sets. Each iteration of the

A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models

ADMM algorithm decomposes (6) into two sub-problems
with augmented terms:
(t+1)

W1

=

argmin
×J
W ∈RN
:W 1=1
+

(t)

D ◦ W + Y1

◦W

ρ
+ kW − Z (t) k2
2

(12)

and
(t+1)

W2

(t)

θkW kG + λkW k∞,1 + Y2

= argmin
N ×J
W ∈R+

ρ
+ kW − Z (t) k2
2

◦W
5.2. Shrinking
(13)

(t)

problem. However, if there are multiple optimal solutions
∗
W1∗ , W2∗ , ...WM
in the original problem (5), any fractional
convex combination of them would also be optimal in the
convex relaxation (6). One simple approach resolving this
issue is to add a small perturbation to the matrix D. Since in
the combinatorial problem (5), function difference between
optimal and sub-optimal solutions can be lower bounded by
a finite constant, there exists small enough perturbation that
makes one of the solutions become uniquely optimal.

(t)

where Z (t) is a global consensus variable, and Y1 , Y2
are dual variables for constraints W1 = Z, W2 = Z respectively 2 . The two sub-problems are relatively easy to
solve, since each row of W in (12) can be solved independently, and so as each column of W in (13). For each row
of W in (12), we solve with a Frank-Wolfe algorithm specialized for simplex constraint (Jaggi, 2013), while for each
column of W in (13), we solve with a closed-form proximal
mapping. Since each group g ∈ G is a subset of a column
of W , the proximal mapping prox(.) for each column of
(13) can be decomposed as prox(.) = proxλ (proxθ (.)),
where proxλ (.) is the proximal-mapping for λkW k∞,1
and proxθ (.) is the that for θkW kG , both of which can
be computed in closed form as in (Liu et al., 2009). The
overall algorithm 1 has following convergence guarantee.
Theorem 3. Let f (W ) denotes the objective function of
(6). Provided that α is sufficiently small, the sequence
{f (W (t) )}∞
t=0 produced by Algorithm 1 converges to optimum f ∗ at a linear rate. In other words,

Since many columns of W become 0 when iterate W (t) becomes close to optimum. We employ a Shrinking technique
to shrink inactive columns of W that have primal and dual
residual both equal, or close, to 0, that is, we shrink column
j if
(t)

(t−1)

kZi,j − Zi,j

k≈0

and

(t)

(t)

kW1i,j − W2i,j k ≈ 0, ∀i.

The shrinked column will be set to 0 and never updated
until stopping condition holds for all the other columns.
Then we will check optimality of those shrinked columns
and re-iterates if optimality does not hold. In practice, the
technique reduces complexity for most iterations of Algorithm 1 from O(N 2 ) to O(N K + ), where K +  N is the
number of active columns.
5.3. Solving Subproblem Inexactly
The convergence of the ADMM algorithm does not require
solving each subproblem exactly (Boyd et al., 2011; Hong
& Luo, 2012). In practice, solving subproblem (12) by only
a few iterations of Frank-Wolfe algorithm results in faster
convergence. In our experiment, we fix maximum number
of Frank-Wolfe iterations to 30.

t ≥ c1 log(c2 /)
iterations suffice to guarantee f (W (t) ) − f ∗ ≤  and infeasibility for some constants c1 , c2 independent of t.
Since D ◦ W and kW k∞,1 , kW kG are all polyhedral functions, Theorem 3 simply follows from Theorem 3.1 of
(Hong & Luo, 2012).

5. Practical Issues
In this section, we discusses some implementation details
and practical issues.
5.1. Solution Optimality and Perturbation
Whenever an integer optimal solution W ∗ is obtained from
the convex relaxation, it is also optimal to the combinatorial
2
More details about deriving the ADMM sub-problems can
be found in, for example, (Boyd et al., 2011).

5.4. Model Selection
One arguable advantage of Bayesian Nonparametrics is not
fixing number of clusters K before learning, which however, comes with additional parameters λ. In practice,
whether one way is better than the other depends on the
prior knowledge one has. For some applications, the desired number of clusters is known a priori, and thus fixing
K before training would be preferred. However, for situations where HDP applies, one can hardly expect how many
local clusters each data set should have, and a single parameter θ, that encodes the additional loss reduction one should
achieve to create a new local cluster, is preferred. As stated
in Section 3, the parameters that result in the same optimal
solution W ∗ form a convex region of parameters (a band
of λ in case of DP mixture), and the larger extent of separation a clustering {Sk }k∈M achieves in Theorem 1, 2, the
larger the convex region. In practice, a clustering with better separation can be found in a wider range of parameters.

A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models

6. Experiment
In this section, we compare the convex, exemplar-based approach with existing approaches to MAD-Bayes DP mixture (1) and HDP mixture (4) on several public available
data sets. We use square Euclidean distance as measure
of dissimilarity D(., .) so the compared algorithms are applicable (Kulis & Jordan, 2012; Jiang et al., 2012). The
algorithms in comparison are listed as follows.
• DP-means exactly implements the k-means like algorithm proposed in (Kulis & Jordan, 2012) for the DP
objective (1). Since the algorithm gives different re(t)
sult for different order of updating {zi }N
i=1 , we run
the algorithm for 1000 rounds with random permutation on the updating in order to achieve better local optimum. Global mean is used as initialization as
specified in (Kulis & Jordan, 2012).
• DP-medoids applies the DP-means algorithm to the
exemplar-based objective function (2). Since (2) is a
special case of (1) that restricts the space of µk to a set
of exemplar E, the resulting algorithm simply replaces
the step of computing mean of cluster Sk by picking
exemplar µj ∈ E of smallest dissimilarity to points in
Sk .
• DP-convex solves (3) using Algorithm 1 with θ = 0.
• DP-convex (means) takes clustering assignments obtained from DP-convex, and compute means of each
cluster Sk as the representative for i ∈ Sk .
• HDP-means exactly implements the algorithm proposed in (Kulis & Jordan, 2012) for the HDP objective
(4). As in DP-means, we run the algorithm for 1000
rounds with different random permutation of updating
order to achieve better local optimum. Global mean is
used as initialization as specified in (Kulis & Jordan,
2012).
• HDP-medoids applies the HDP-means algorithm to
the exemplar-based objective function (5). Similar to
DP-medoids, the algorithm simply replaces the step
of computing mean w.r.t. a set of points S by picking
exemplar µj ∈ E of smallest dissimilarity to points in
S.
• HDP-convex solves (6) using Algorithm 1.
• HDP-convex (means) takes clustering assignments obtained from HDP-convex, and compute means of each
global cluster Sk as the representative for i ∈ Sk .
Our experiments for DP mixture model are conducted on
5 publicly available data sets: Iris, Glass, Wine, DNA

Table 1. Data sets employed in our experiments and their relevant
statistics.

Dataset
Iris
Wine
Glass
DNA
Segment
Wholesale
Water

N
150
178
214
2000
2310
440
527

D
4
13
9
180
19
6
38

Mean-Dist
2.1940
4.2966
2.1445
67.1564
6.3195
0.4220
1.6073

Std-Dist
2.1475
2.4209
2.2049
7.2841
5.5270
0.3444
1.5893

and Segment. For HDP mixture model, we experiment on
Wholesale and Water data sets. The Wholesale data comprises the spending of customers from different regions and
channels. We divide samples in Wholesale into 6 groups
T1 , ..., T6 based on region and channel. The Water data
contains the daily measures of sensors in a urban waste
water treatment plant. The data set is divided into 21
groups according to the month. For each group, we expect a smaller number of clusters than that appear across
all groups. All of the data sets can be downloaded from
UCI Machine Learning Repository
Figure 2 and 3 illustrate the sampled regularization path obtained from DP-convex on five data sets. For each data set,
we observed several bands of λ that give integer (and thus
optimal) solutions. Each band corresponds to a clustering
assignment {Sk }k∈M . Note there are several integer solutions marked by square mark ’’ that has a band shorter
than our sampled interval, which corresponds to clustering
with small extent of separation and is usually not preferred
in model selection. In Figure 4, we present the number of
global clusters Kg and total number of local clusters Kl
obtained from HDP-convex under different (λ, θ) pairs of
parameter. Note for fractional solution, Kg = kW ∗ k∞,1
and Kl = kW ∗ kG , and integer solutions are marked with
’’.
In Table 2, we pick λ within a band in Figure 3 and Figure 2 for each data set, and compare the objective obtained
from convex approach to existing approaches (Kulis & Jordan, 2012; Jiang et al., 2012) with 1000 random re-trials.
Note that, for the same clustering, using mean as representative should always get lower objective than using medoid
as representative since mean is the minimizer of square Euclidean distance. However, the result shows that the optimal medoids obtained from convex approach still achieves
significantly lower objective than clustering found by local
search method.
In addition, we observe that the number of clusters obtained
from local methods are consistently smaller. The reason
behind this is, the DP-means algorithm proposed in (Kulis
& Jordan, 2012; Jiang et al., 2012) creates a new cluster

A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models

Iris
12

Glass

Fractional Sample
Integer Sample
Integer Range

10

30

Wine
Fractional Sample
Integer Sample
Integer Range

25

Fractional Sample
Integer Sample
Integer Range

50
40

20

K

15

K

K

8

20

10

6

10

5

4

30

0

0

1

10

0

2

10

10

0

1

10

(b) Wine
Segment

DNA

0

10

1

12

2

10

10
lambda

10

(a) Iris

100

Fractional Sample
Integer Sample
Integer Range

Fractional Sample
Integer Sample
Integer Range

80

8

Figure 2. Number of clusters K = kW k∞,1 vs. λ obtained from
DP-convex, where each  denotes an integer solution and a horizontal line between contiguous  shows band of λ yielding the
same integer solution.

K

60

∗

6

K

0

10
lambda

(a) Glass

2

2

10

lambda

40
4

20

2
0
500

1000

1500
2000
lambda

2500

(c) DNA

only when a single observation has square distance to its
representative more than λ. However, it is often the case
that only moving several points together to a new cluster
can make a loss reduction more than λ. In that case, DPmeans can easily get stuck at local minimum. For the data
set Wine, the algorithm gets stuck even at initialization.

3000

0
0

200

400
600
lambda

800

(d) Segment

Figure 3. More results of K v.s. λ obtained from DP-convex program. Note that each  denotes an integer solution and a horizontal line between contiguous  shows band of λ yielding the same
integer solution.

Figure 5 shows the lowest objective achieved by different methods over time. Generally, a single run of DPmeans, HDP-means or their exemplar-based version converges much faster than the ADMM algorithm. However,
even with large number of random re-trials, the objective
achieved by local search methods could hard improve significantly. For Wine and Glass data set, all random retrials
of DP-means and DP-medoids get stuck at the same local
minimum.
Conclusion. In this paper, we show that the MAD-Bayes
formulation for DP and HDP mixture models can be naturally relaxed to a convex domain in the exemplar-based
setting, which is tight under cluster-separation condition.
Similar convex formulation can be derived for more interesting applications such as Latent Feature Allocation
(Broderick et al., 2013), Infinite Hidden Markov Models (Roychowdhury et al., 2013), and applications where
exemplar comes in naturally, such as Multiple Sequence
Alignment and Motif Finding. This will be an active research direction for us in the near future.
Acknowledgement. P. R. acknowledges the support of
ARO via W911NF-12-1-0390 and NSF via IIS-1149803,
IIS-1320894, IIS-1447574, and DMS-1264033. I. S. D.
acknowledges the support of NSF via grants CCF-1320746
and CCF-1117055.

(a) Wholesale-Kg

(b) Wholesale-Kl

(c) Water-Kg

(d) Water-Kl

Figure 4. Number of global and local clusters obtained from
HDP-convex, where each ’’ denotes an integer solution. (a)
Kg = kW ∗ k∞,1 , the number of global clusters in HDP. (b)
Kl = kW ∗ kG , the total number of local clusters for HDP.

A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models

Table 2. Objective value obtained from different methods, where DP-convex (means) uses mean instead of medoid as representative of
each cluster Sk obtained from W ∗ .

Data set
Iris (λ = 2)
Glass (λ = 9)
Wine (λ = 20)
DNA (λ = 1000)
Segment (λ = 600)
Data set
Wholesale
(λ = 1.0, θ = 1.0)
Water
(λ = 1.0, θ = 1.0)

DP-convex
29.26 (K=7)
137.40 (K=6)
298.55 (K=4)
105947.0 (K=2)
4749.62 (K=4)
HDP-convex
56.28
(Kg =5, Kl =16)
244.59
(Kg =32, Kl =81)

DP-convex (means)
27.97 (K=7)
128.13 (K=6)
263.79 (K=4)
68718.9 (K=2)
4572.3 (K=4)
HDP-convex (means)
53.07
(Kg =5, Kl =16)
232.07
(Kg =34, Kl =83)

DP-medoids
35.68 (K=3)
175.42 (K=2)
512.04 (K=1)
107211(K=1)
8405.71(K=1)
HDP-medoids
83.35
(Kg = 2, Kl = 6)
256.41
(Kg =33, Kl =74)

iris

1.6

DP-means
30.20 (K=4)
154.66 (K=2)
402.40 (K=1)
68156.4 (K=1)
7898.98 (K=1)
HDP-means
79.52
(Kg =2, Kl = 6)
237.73
(Kg =37, Kl =64)

glass

10

DP−convex
DP−means
DP−medoids

DP−convex
DP−means
DP−medoids

2.4

Objective

Objective

10

2.3

10

1.5

10

2.2

10

−2

10

−1

0

10

10

1

0

10

1

10

10

Time

Time

(a) Iris (DP)

(b) Glass (DP)
wholesale(HDP)

wine
DP−convex
DP−means
DP−medoids

HDP−convex
HDP−means
HDP−medoids

2

10

2.7

objective

Objective

10

2.6

10

2.5

10

−1

10

0

10
Time

(c) Wine (DP)

1

10

−1

10

0

1

10

10
time

(d) Wholesale (HDP)

Figure 5. Objective v.s. Time (in seconds). (a)-(c) show the lowest objective achieved by DP-convex, DP-means and DP-medoids over
time on three data sets: (a) Iris. (b) Glass (c) Wine. Note we run DP-means and DP-medoids for 1000 rounds with random permutation
on the updating order. (d) Running Time Comparison between HDP-convex, HDP-means and HDP-medoids on the Wholesale data set.

A Convex Exemplar-based Approach to MAD-Bayes Dirichlet Process Mixture Models

References
Awasthi, Pranjal, Bandeira, Afonso S, Charikar, Moses,
Krishnaswamy, Ravishankar, Villar, Soledad, and Ward,
Rachel. Relax, no need to round: Integrality of clustering
formulations. In Proceedings of the 2015 Conference on
Innovations in Theoretical Computer Science, pp. 191–
200. ACM, 2015.
Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning,
3(1):1–122, 2011.
Broderick, Tamara, Kulis, Brian, and Jordan, Michael I.
Mad-bayes: Map-based asymptotic derivations from
bayes. ICML, 2013.
Campbell, Trevor, Liu, Miao, Kulis, Brian, How,
Jonathan P, and Carin, Lawrence. Dynamic clustering
via asymptotics of the dependent dirichlet process mixture. In Advances in Neural Information Processing Systems, pp. 449–457, 2013.
Elhamifar, Ehsan, Sapiro, Guillermo, and Vidal, Rene.
Finding exemplars from pairwise dissimilarities via simultaneous sparse recovery. In Advances in Neural Information Processing Systems, pp. 19–27, 2012.
Frey, Brendan J and Dueck, Delbert. Clustering by passing
messages between data points. science, 315(5814):972–
976, 2007.
Hong, Mingyi and Luo, Zhi-Quan. On the linear convergence of the alternating direction method of multipliers.
arXiv preprint arXiv:1208.3922, 2012.
Jaggi, Martin. Revisiting frank-wolfe: Projection-free
sparse convex optimization. In Proceedings of the 30th
International Conference on Machine Learning (ICML13), pp. 427–435, 2013.
Jiang, Ke, Kulis, Brian, and Jordan, Michael I. Smallvariance asymptotics for exponential family dirichlet
process mixture models. In Advances in Neural Information Processing Systems, pp. 3158–3166, 2012.
Kaufman, Leonard and Rousseeuw, Peter. Clustering by
means of medoids. North-Holland, 1987.
Kulis, Brian and Jordan, Michael I. Revisiting k-means:
New algorithms via bayesian nonparametrics. ICML,
2012.
Liu, Han, Palatucci, Mark, and Zhang, Jian. Blockwise coordinate descent procedures for the multi-task lasso, with
applications to neural semantic basis discovery. In Proceedings of the 26th Annual International Conference on
Machine Learning, pp. 649–656. ACM, 2009.

Megiddo, Nimrod and Supowit, Kenneth J. On the complexity of some common geometric location problems.
SIAM journal on computing, 13(1):182–196, 1984.
Nellore, Abhinav and Ward, Rachel. Recovery guarantees for exemplar-based clustering. arXiv preprint
arXiv:1309.3256, 2013.
Obozinski, Guillaume, Jacob, Laurent, and Vert, JeanPhilippe. Group lasso with overlaps: the latent group
lasso approach. arXiv preprint arXiv:1110.0413, 2011.
Papadimitriou, Christos H. Worst-case and probabilistic
analysis of a geometric location problem. SIAM Journal
on Computing, 10(3):542–557, 1981.
Roychowdhury, Anirban, Jiang, Ke, and Kulis, Brian.
Small-variance asymptotics for hidden markov models.
In Advances in Neural Information Processing Systems,
pp. 2103–2111, 2013.
Van der Laan, Mark, Pollard, Katherine, and Bryan, Jennifer. A new partitioning around medoids algorithm.
Journal of Statistical Computation and Simulation, 73
(8):575–584, 2003.
Yaron, Margalit and Shalev-Shwartz, Shai. A convex relaxation to solve the k-medoids problem. Hebrew University, 2010.

