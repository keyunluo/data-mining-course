Approximation Analysis of Stochastic Gradient Langevin Dynamics
by using Fokker-Planck Equation and Itô Process

Issei Sato
The University of Tokyo

SATO @ R . DL . ITC . U - TOKYO . AC . JP

Hiroshi Nakagawa
The University of Tokyo

N 3@ DL . ITC . U - TOKYO . AC . JP

Abstract
The stochastic gradient Langevin dynamics
(SGLD) algorithm is appealing for large scale
Bayesian learning.
The SGLD algorithm
seamlessly transit stochastic optimization and
Bayesian posterior sampling. However, solid
theories, such as convergence proof, have not
been developed. We theoretically analyze the
SGLD algorithm with constant stepsize in two
ways. First, we show by using the Fokker-Planck
equation that the probability distribution of random variables generated by the SGLD algorithm
converges to the Bayesian posterior. Second, we
analyze the convergence of the SGLD algorithm
by using the Itô process, which reveals that the
SGLD algorithm does not strongly but weakly
converges. This result indicates that the SGLD
algorithm can be an approximation method for
posterior averaging.

1. Introduction
Bayesian learning is one of the most important fields in machine learning. It captures uncertainty and avoids overfitting. The stochastic gradient Langevin dynamics (SGLD)
algorithm (Welling & Teh, 2011) is appealing for largescale Bayesian learning. It is constructed by the combination of Robbins-Monro type stochastic approximation
(H.Robbins & S.Monro, 1951) and Langevin dynamics.
Stochastic approximation such as stochastic gradient descent is one of the most successful techniques in large scale
machine learning. It processes mini-batches of data at each
iteration and update model parameters. Langevin dynamics
injects noise into model parameters in such a way that the
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

trajectory of the parameters will converge to the full posterior distribution. Langevin dynamics requires all data to
update model parameters.
The SGLD algorithm applies stochastic approximation
to Langevin dynamics, i.e., its updates are processed
by mini-batches. With the original SGLD algorithm
(Welling & Teh, 2011), the step sizes are annealed to zero
at a certain rate. However, this stepsize condition slows
mixing rate. After a sufficient burn-in period, the stepsizes
are much smaller, thus, the trajectory of the parameters can
be local. Ahn et al. (2012) extended the SGLD algorithm
so that for large stepsizes it will sample from an approximate normal distribution of the posterior. Petterson and
Teh (2013) proposed an SGLD algorithm on a probability
simplex space.
Contributions : In this paper, we theoretically analyze the
SGLD algorithm with constant stepsize in two ways.
(1) We show that the probability distribution of random
variables generated by the SGLD algorithm converges to
the Bayesian posterior by using the Fokker-Planck (FP)
equation.
(2) We analyze the convergence of the SGLD algorithm by
using the Itô process, which reveals that the SGLD algorithm does not strongly but weakly converges.
The SGLD algorithm is regarded as a discretization approximation of a stochastic differential equation corresponding to the FP equation of the Bayesian posterior.
Therefore, by analyzing the discretization error of the
SGLD algorithm, we can analyze the convergence of the
SGLD algorithm. To the best of our knowledge, these are
the first theoretical analyses of the SGLD algorithm.
Note that our theoretical analysis is based on the onedimensional FP equation and Itô process for simplicity
(mainly for simple notations). Our analysis can be easily
extended by using multi-dimensional FP equation and Itô
process.

Approximation Analysis of Stochastic Gradient Langevin Dynamics

2. Stochastic Gradient Langevin Dynamics
(SGLD)

4. Analysis of SGLD Algorithm by using FP
Equation

Let x1:n be a data set x1:n ∏
= (x1 , x2 , · · · , xn ) with a genn
erative model p(x1:n |θ) = i=1 p(xi |θ) parameterized by
θ with prior p(θ). The aim of Bayesian learning is to compute the posterior p(θ|x
∫ 1:n ) and the predictive distribution
for a new data point, p(x∗ |θ)p(θ|x1:n )dθ.

We analyze the probability density function (pdf) of random variables generated by the SGLD algorithm. We find
that under the assumption which is often used in stochastic approximation fields, its stationary distribution is the
Bayesian posterior as is the case in the ordinary Langevin
dynamics. That is, the stochastic noise can be ignored.

We use the notation ∂θ for the partial derivatives with respect to θ.
The SGLD algorithm (Welling & Teh, 2011) contains the
following update equation:
ϵt
θt+1 = θt + ∂θ L̃(θt ) + ηt , ηt ∼ N (0, ϵt ),
(1)
2
∑
n
∂θ log p(xi |θt ), (2)
∂θ L̃(θt ) = ∂θ log p(θt ) +
|St |
i∈St

where St is a set of samples randomly selected from x1:n ,
ϵt is the step size, and N (0, ϵt ) is a Gaussian distribution
with mean 0 and variance ϵt . The step size decreases towards zero at rates satisfying
∞
∑

ϵt = ∞,

t=1

∞
∑

ϵ2t < ∞.

(3)

t=1

i=1

ξt = ∂θ L̃(θt ) − ∂θ L(θt ),
n
∑
n ∑
=
log p(xi |θt ) −
log p(xi |θt )
|St |
i=1

(11)

Let q(t, θ) be the probability density function of θ at time
t. Suppose that
lim U (θ) = ∞,

|θ|→∞

(4)

exp(−U (θ))dθ < ∞.

(5)

Let q(θ) be the stationary distribution of q(t, θ). Then, it is
known that q(θ) satisfies
q(θ) ∝ exp (−U (θ)) .

(6)

The derivation is as follows. As t → ∞, q(t, θ) → q(θ),
i.e., limt→0 ∂t q(t, θ) = 0, and
∂θ2 q(θ)

∂θ (∂θ U (θ)q(θ))) +
=0
⇔∂θ [∂θ U (θ)q(θ)) + ∂θ q(θ)] = 0
⇔∂θ q(θ)[∂θ U (θ) + ∂θ log q(θ)] = 0.

(12)

The stochastic noise is typically assumed to be a white
noise or Martingale difference noise, i.e., ξt and ξs (s ̸= t)
are independent. Moreover, we assume that
ESt [ξtℓ ] < ∞,

(13)

for some integer ℓ ≥ 2.

The FP equation (Risken & Frank, 1984; Daum, 1994) is a
partial differential equation which describes the time evolution of the probability density function given by
∂t q(t, θ) = ∂θ (∂θ U (θ)q(t, θ)) + ∂θ2 q(t, θ).

Note that the expectation over sampling set St , denoted by
ESt [ξt ], is
ESt [ξt ] = 0.

3. Review of Fokker-Planck (FP) Equation

which means that

For theoretical use, we represent Eq. (8) by using stochastic noise ξt , which is a well-known technique in stochastic
approximation fields.
√
θt+1 = θt + ϵ(∂θ L(θt ) + ξt ) + 2ηt ,
(9)
n
∑
L(θ) = log p(θ) +
log p(xi |θ),
(10)

i∈St

Typically, the step size is formulated as ϵt = τ0 /(τ1 + t)r
with r ∈ (0.5, 1].

∫

We analyze the following SGLD with constant step size ϵ.
√
θt+1 = θt + ϵ∂θ L̃(θt ) + 2ηt , ηt ∼ N (0, ϵ). (8)

(7)

Thus, we have
∂θ U (θ) + ∂θ log q(θ) = 0 ⇔ U (θ) + log q(θ) = Const. .

Let q(t, θ) be the pdf of θ at time t and ϕt (θ) be the characteristic function of q(t, θ) defined by
∫
ϕt (s) = exp(isθ)q(t, θ)dθ.
(14)
The characteristic function of ϵξt is, from Eq.(12),
[∞
]
∞
∑1
∑
1
E[exp(isϵξt )] = E
(isϵξt )ℓ =
E[ξtℓ ](isϵ)ℓ ,
ℓ!
ℓ!
ℓ=0

= 1 + O(ϵ2 ).

ℓ=0

(15)

The characteristic function of θt + ϵ∂θ L(θt ) + ϵξt is
∫
exp(isθ + isϵ∂θ L(θ))(1 + O(ϵ2 ))q(t, θ)dθ
∫
= exp(isθ + isϵ∂θ L(θ))q(t, θ)dθ + O(ϵ2 ) (16)

Approximation Analysis of Stochastic Gradient Langevin Dynamics

The characteristic function of
ηt ∼ N (0, ϵ).

√
2ηt is exp(−ϵs2 ) because

Here, we rewrite θt+1 as θt+ϵ for theoretical use. Therefore, the characteristic function of θt+ϵ (= θt+1 ) is
ϕt+ϵ (s)
∫
)
(
= exp isθ + isϵ∂θ L(θ) − ϵs2 q(t, θ)dθ + O(ϵ2 ).
(17)
Using exp(x) = 1 + x + O(x2 ),
ϕt+ϵ (s) − ϕt (s)
∫
)
]
[
(
= exp(isθ) exp isϵ∂θ L(θ) − ϵs2 − 1 q(t, θ)dθ
∫
=
∫
=

+ O(ϵ2 ),
[
]
exp(isθ) isϵ∂θ L(θ) − ϵs2 + O(ϵ2 ) q(t, θ)dθ,
+ O(ϵ2 ),
[
]
exp(isθ) isϵ∂θ L(θ) − ϵs2 q(t, θ)dθ + O(ϵ2 ).
(18)

Thus,
ϕt+ϵ (s) − ϕt (s)
ϵ
∫
= (−is)

exp(isθ)∂θ (−L(θ))q(t, θ)dθ
∫
+ (−is)2 exp(isθ)q(t, θ)dθ + O(ϵ),

By using F −1 ϕt (s) =

√
2πq(t, θ),

q(t + ϵ, θ) − q(t, θ)
ϵ
= ∂θ (∂θ (−L(θ))q(t, θ)) + ∂θ2 q(t, θ) + O(ϵ).

(22)

Therefore,
q(t + ϵ, θ) − q(t, θ)
,
ϵ→0
ϵ
= ∂θ (∂θ (−L(θ))q(t, θ)) + ∂θ2 q(t, θ),

∂t q(t, θ) = lim

(23)

which means that the probability density function of θt generated by the SGLD algorithm also follows the FP equation
(5).
When we use U (θ) = −L(θ) in Eq. (6), we have q(θ) =
p(θ|x1:n ).

5. Review of Itô Process
We found in the previous section that the random variables
generated by the SGLD algorithm can be samples from
the Bayesian posterior when ϵ → 0. That is, the SGLD
algorithm is considered to be the discretization approximation of the stochastic differential equation. Therefore,
our interest is its discretization error. In this section, we
review the Itô process which we use to analyze the discretization error of the SGLD algorithm in the next section.
Our analysis is based on the basic theories of the stochastic
differential equation (Gard, 1988; Kloeden & Platen, 1992;
Carlsson et al., 2010).

(19)
5.1. Wiener Process
Let F be the Fourier transform defined by, for an integrable
function f ,
∫
1
F[f (x)](s) = √
f (x) exp(isx)dx,
2π
∫
1
−1
F [f (x)](s) = f (x) = √
F[f (x)](s) exp(−isx)ds.
2π
and the Fourier transform of the derivatives of the ℓ-th order
f (ℓ) (x) is
F(f (ℓ) )(s) = (−is)ℓ (F(f ))(s).

(20)

Therefore,
ϕt+ϵ (s) − ϕt (s)
√
2πϵ
= (−is)F∂θ (−L(θ))q(t, θ) + (−is)2 Fq(t, θ) + O(ϵ),
= F∂θ (∂θ (−L(θ))q(t, θ)) + F∂θ2 q(t, θ) + O(ϵ).
(21)

W (t) represents the one-dimensional Wiener process, also
known as the Brownian motion , which has the following
properties:
1. with probability one, the mapping t → W (t) is continuous and W (0) = 0,
2. if we devide [0, T ] as 0 = t0 < t1 < t2 < · · · <
tN = T , then the increments ∆Wk = W (tk ) −
W (tk−1 ) (k = 1, · · · , N ) are independent, and
3. for all t > s, the increment W (t)−W (s) has a normal
distribution with
E[W (t)] = 0, E[(W (t) − W (s))2 ] = t − s. (24)
The Gaussian injective noises of the SGLD algorithm corresponds to the increments ∆Wk of the Wiener process.

Approximation Analysis of Stochastic Gradient Langevin Dynamics

5.2. Itô Process
The stochastic process X = {X(t)}t≥0 that solves
∫ t
∫ t
X(t) = X(0) +
a(s, X(s))ds +
b(X(s), s)dW (s)
0

0

(25)
is called the Itô process (Itô, 1944). a(t, X(t)) and
b(t, X(t)) are the drift and diffusion function, respectively.
The stochastic differential equation form of the Itô process
is
dX(t) = a(t, X(t))dt + b(t, X(t))dW (t).

(26)

It has a unique solution if the coefficients a and b are
Lipschitz-continuous functions of linear growth.
The first integral in Eq. (25) is an ordinary integral along
paths. The second integral in Eq. (25) is the Ito stochastic
integral defined by
∫

t

g(θ(s))dW (s) =
0

lim

∆tmax →0

m−1
∑

g(θ(tk ))∆Wk , (27)

k=0

where we divide [0, t] into 0 = t0 < t1 < t2 < · · · < tm =
t, ∆tmax = maxk (tk+1 − tk ) and ∆Wk = W (tk+1 ) −
W (tk ). The mode of convergence is in mean square.
A basic property of the Itô stochastic integral used in this
paper is
[∫ t
]
E
f (s, ·)dW (s) = 0,
(28)
0

In the stochastic differential equation, we have the following formula.
Theorem 1 (Itô Formula (Itô, 1944)). X(t) satisfies the
stochastic differential equation
dX(t) = a(t, X(t))dt + b(t, X(t))dW (t).

(33)

Let h(t, X(t)) be a given bounded function in C 2 ((0, ∞)×
R). Then, h(t, X(t)) satisfies the stochastic differential
equation
dh(t, X(t)) = L1 h(t, X(t))dt + L2 h(t, X(t))dW (t),
(34)
where L1 and L2 are linear operators defined by
1
2
L1 = ∂t + a∂X + b2 ∂X
, L2 = b∂X .
2

(35)

5.4. Basic Theorems
We introduce three theorems for the Itô process. These are
used in the next section.
Assumption 1. Suppose that
Lipschitz condition: there exists constant C > 0 such that
|a(t, x) − a(t, y)| + |b(t, x) + b(t, y)| ≤ C|x − y|. (36)
Linear growth condition: there exists constant C > 0 such
that
|a(t, x)|2 + |b(t, x)|2 ≤ C 2 (1 + |x|2 ).

(37)

There exists constant C > 0 such that
1

where f : [0, T ] × Ω → R is the Itô integrable and independent of the increments ∆Wk .

|a(s, x) − a(t, x)| + |b(s, x) − b(t, x)| ≤ C(1 + |x|)|s − t| 2 .
(38)

5.3. Itô Formula

Theorem 2 (Theorem 4.5.4 of (Kloeden & Platen, 1992)).
Suppose that E[|X(0)|2ℓ ] < ∞ for some integer ℓ ≥ 1.
Then

The Itô formula is one of the most important tools in Itô
process. Intuitively, the Itô formula corresponds to a chain
rule in the stochastic process. To explain this, we first explain the case of the ordinary differential equation
d
X(t) = a(t, X(t)).
dt

(29)

Let h be a function of X(t). The chain rule derives the
evolution of the function h as
d
dX(t) ∂
h(t, X(t)) =
h(t, X(t))
dt
dt ∂X(t)
∂
h(t, X(t)).
= a(t, X(t))
∂X

(30)

E[|X(t) − X(t0 )|2ℓ ]
≤ D2 (1 + E[|X(0)|2ℓ ])(t − t0 ) exp(D1 (t − t0 )),
for t ∈ [t0 , T ], where T < ∞, D1 = 2ℓ(2ℓ + 1)C 2 and D2
is a positive constant depending only on ℓ, C and T − t0 .
Theorem 3 (Gronwall inequality (Lemma 4.5.1 of
(Kloeden & Platen, 1992))). Let α, β: [t0 , T ] → R be integrable with
∫ t
0 ≤ α(t) ≤ β(t) + G
α(s)ds, t0 ≤ t ≤ T,
t0

By defining a linear operator L0 = a∂X , we have
dh(t, X(t)) = L0 h(t, X(t))dt,
where L0 h(t, X(t) = a(t, X(t))∂X h(t, X(t)).

E[|X(t)|2ℓ ] ≤ (1 + E[|X(0)|2ℓ ]) exp(D1 (t − t0 )),

(31)
(32)

where G > 0. Then
∫ t
α(t) ≤ β(t) + G
exp(G(t − s))β(s)ds, t0 ≤ t ≤ T.
t0

Approximation Analysis of Stochastic Gradient Langevin Dynamics

Theorem 4 (Feynman-Kac Formula (Feynman, 1948; Kac,
1948; 1951)). Suppose that a, b and g are smooth and
bounded functions. Let X be the solution of the stochastic differential equation
dX(t) = a(t, X(t))dt + b(t, X(t))dW (t),
and let u(t, x) = E[g(X(T ))|X(t) = x].
Then u is the solution of the Kolmogorov backward equation

 ∂u
∂u 1 2 ∂ 2
+a
+ b
u = 0, t < T .
(39)
∂t
∂x 2 ∂x2
 u(T,
x) = g(x)

6. Analysis of SGLD Method by using Itô
Process

6.2. Convergence in Stochastic Differential Equation
In the stochastic differential equation, we have the following two definitions of convergence: strong and weak.
Definition 1 (Strong and Weak Convergence). Let N be
an integer N > 0 and X be a stochastic process. We say
that a time discrete approximation X̄∆t over time interval
[0, T ] with step size ∆t = T /N
converges strongly to X(T ) at time T if
lim E[|X(T ) − X̄∆t (T )|] = 0,

∆t→0

and converges weakly to X(T ) at time T if, for any continuous differentiable and polynomial growth function h,
lim |E[h(X(T ))] − E[h(X̄∆t (T ))]| = 0.

∆t→0

We analyze the discretization error of the SGLD algorithm
from two aspects: strong error and weak error, which correspond to strong convergence and weak convergence, respectively. We show that the SGLD algorithm does not
converge in terms of strong convergence but converges in
terms of weak convergence.

6.3. Strong and Weak Convergence of SGLD
Assumption 2. We assume that
Initialize condition: θ̃(0) = θ(0) and is bounded.

For theoretical use, we introduce a virtual time line, 0 =
t0 < t1 < · · · < tN = T , to use the Itô process, where tk
denotes the k-th update time of the SGLD algorithm, i.e.,
θtk is the k-th sample, and N denotes the total number of
updates, i.e., the total number of samples. The time interval
is constant, i.e., tk − tk−1 = ϵ (k = 1, · · · , N ). From
ϵ = T /N , T indicates a parameter to determine ϵ in terms
of the implementation of the SGLD algorithm.

Lipschitz condition:

We consider the following Itô process
∫ T
∫
θ(T ) − θ(0) =
a(θ(t))dt +
0

(45)

In the next section, we analyze the strong and weak convergence of the SGLD algorithm.

6.1. Problem Setting

Let a(θ) = ∂θ L(θ). Note that since L(·) does not depend
on time t, a(·) does not depend on t. Let ã(θ) = ∂θ L̃(θ).

(44)

|a(θ(t)) − a(θ(s))| ≤ C1 |θ(t) − θ(s)|,
|b(θ(t)) − b(θ(s))| ≤ C2 |θ(t) − θ(s)|.

(46)
(47)

Linear growth condition:
|a(θ(t))|2 + |b(θ(t))|2 ≤ C32 (1 + |θ(t)|2 ),

(48)

and C1 , · · · , C3 do not depend on ϵ.
First, we have
Theorem 5 (Strong error).

T

b(θ(t))dW (t).

E[|θ(T ) − θ̃(T )|2 ] = O(ϵ + max E[|ξtk |2 ]).
k

(49)

0

(40)
Let θ̃ be a random variable generated by the SGLD algorithm. The SGLD update is represented as
θ̃(tk ) − θ̃(tk−1 ) = ã(θ̃(tk−1 ))ϵ + b(θ̃(tk−1 ))ηk ,
√
ηk ∼ N (0, ϵ), b(θ̄(tk−1 )) = 2.

(41)
(42)

For deriving more general results, we do not restrict b(·)
constant in this paper.

The proof is given in Appendix A. Theorem 5 indicates
the pathwise error of the SGLD algorithm is affected by
stochastic noise ξ. This also shows that the SGLD algorithm does not converge in terms of strong converge. Note
that if maxk E[|ξtk |2 ] = 0, the order of convergence is the
same as ordinary Langevin dynamics.
Next, we can show that
Theorem 6 (Weak error).

By using stochastic noise ξtk−1 , we can rewrite Eq. (41) as

|E[h(θ(T ))] − E[h(θ̃(T ))]| = O(ϵ),

θ̃(tk ) − θ̃(tk−1 ) = (a(θ̃(tk−1 )) + ξtk−1 )ϵ + b(θ̃(tk−1 ))ηk .
(43)

for any continuous differentiable and polynomial growth
function h.

(50)

Approximation Analysis of Stochastic Gradient Langevin Dynamics

The proof is given in Appendix B. Theorem 6 indicates the
statistics error of the SGLD algorithm is not affected by
stochastic noise ξ. This also shows that the SGLD algorithm converges in terms of weak converge and the order
of convergence is the same as ordinary Langevin dynamics.
Theorem 5 is a positive property of the SGLD algorithm
because the expectation of some function E[h(θ)] is more
important for Bayesian learning. One example is Bayes
predictive distribution, i.e., h(θ) = p(x∗ |θ). When calculating some statistics, the SGLD algorithm can be an alternative to ordinary Langevin dynamics.

The Itô formula applied to Z(t)2 shows, for tk−1 ≤ t < tk ,
Z(tk )2 − Z(tk−1 )2
∫ tk
1
=
2(a − ã)(θ(t))(θ(t) − θ̃(t)) + ((b − b̃)(θ(t)))2 dt
2
tk−1
∫ tk
+
2(b − b̃)(θ(t))(θ(t) − θ̃(t))dW (t).
(54)
tk−1

Since the expectation of the Itô integral is zero (see
Eq.(28)), i.e., the second integral of Eq. (54) is
[∫
]
E

tk

2(b − b̃)(θ(t))(θ(t) − θ̃(t))dW (t) = 0, (55)

tk−1

7. Conclusion
We theoretically analyzed the SGLD algorithm with a constant stepsize ϵ in two ways: using the Fokker-Planck equation and Itô process. These results show the following
properties of the SGLD algorithm.

take the expectation of Z(tk )2 ,
E[Z(tk )2 ] − E[Z(tk−1 )2 ]
∫ tk
=
E[2(a − ã)(θ(t))(θ(t) − θ̃(t))]dt
tk−1

• As stepsize ϵ → 0, the stationary distribution of random variables generated by the SGLD algorithm converges to the Bayesian posterior.
• Stochastic noise negatively affects the SGLD algorithm in a mean of strong convergence but does not
affect in a mean of weak convergence.
These properties suggest that if we use the SGLD algorithm
as a posterior averaging method, e.g., Bayesian prediction,
it can be an alternative to ordinary Langevin dynamics.

∫

tk

E[((b − b̃)(θ(t)))2 ]dt.

Since generally 2xy ≤ (x + y)2 ,
E[2(a − ã)(θ(t))(θ(t) − θ̃(t))]
≤ E[((a − ã)(θ(t)) + (θ(t) − θ̃(t)))2 ]
≤ E[|(a − ã)(θ(t))|2 ] + E[|θ(t) − θ̃(t)|2 ].
Using stochastic noise ξ, we have

= |(a(θ(t)) − a(θ̃(tk−1 )) − (ã(θ̃(tk−1 )) − a(θ̃(tk−1 )))|2
|
{z
}
(51)

=ξtk−1

≤ |a(θ(t)) − a(θ̃(tk−1 ))| + |ξtk−1 | .
2

For theoretical use, for tk−1 ≤ t < tk , define

2

(58)

By the Lipschitz condition,
(52)

and the stochastic differential equation of (41)

|a(θ(t)) − a(θ̃(tk−1 ))|2
≤ |a(θ(t)) − a(θ(tk−1 )) + a(θ(tk−1 )) − a(θ̃(tk−1 ))|2

dθ̃(t) = ã(θ(t))dt + b̃(θ(t))dW (t), tk−1 ≤ t < tk .

≤ |a(θ(t)) − a(θ(tk−1 ))|2 + |a(θ(tk−1 )) − a(θ̃(tk−1 ))|2
≤ C12 [|θ(t) − θ(tk−1 )|2 + |θ(tk−1 ) − θ̃(tk−1 )|2 ],

Let Z(t) = θ(t) − θ̃(t) for tk−1 ≤ t < tk , i.e.,
dZ(t) = (a − ã)(θ(t))dt + (b − b̃)(θ(t))dW (t),

(57)

= |(a(θ(t)) − ã(θ̃(tk−1 ))|2

Consider the stochastic differential equation of (40)

ã(θ(t)) = ã(θ̃(tk−1 )), b̃(θ(t)) = b(θ̃(tk−1 ))

(56)

tk−1

|(a − ã)(θ(t))|2

A. Proof of Theorem 5
dθ(t) = a(θ(t))dt + b(θ(t))dW (t), 0 ≤ t ≤ T.

1
+
2

and the same for |(b(θ(t)) − b̃(θ(tk−1 ))|2 .
(53)

where, for tk−1 ≤ t < tk ,

From Theorem 2, for tk−1 ≤ t < tk ,
E[|θ(t) − θ(tk−1 )|2 ]

(a − ã)(θ(t)) = a(θ(t)) − ã(θ(t)) = a(θ(t)) − ã(θ̃(tk )),

≤ D2 (1 + E[|θ(t0 )|2 ])(t − tk−1 ) exp(D1 (t − tk−1 )),

(b − b̃)(θ(t)) = b(θ(t)) − b̃(θ(t)) = b(θ(t)) − b(θ̃(tk )).

≤ D2 (1 + E[|θ(t0 )|2 ])(t − tk−1 ) exp(D1 (T − t0 )).

Approximation Analysis of Stochastic Gradient Langevin Dynamics

This means that there is a constant D3 depending only on
D1 , D2 , T and θ(0), i.e.,
E[|θ(t) − θ(tk−1 )|2 ] ≤ D3 (t − tk−1 )

(59)

where D3 = D2 (1 + E[|θ(t0 )|2 ]) exp(D1 T ).

Iterating Eq. (64) with E[Z(t0 )2 ] = 0 leads to
(
)
1 − γN
2
E[Z(tN ) ] ≤ β(t0 ) exp(ϵ)
.
1−r
Here, note that N = T /ϵ. Moreover, as ϵ → 0,

Therefore, we have

1

2

∫

ϵeϵ
ϵ
−1
= −ϵ
→
.
1−γ
e − (1 + F1 ϵ)
1 + F1

+ E[|ξtk−1 |2 ] + E[|θ(t) − θ̃(t)|2 ]dt.
tk

(66)

and, by the l’Hospital formula,

+ E[|θ(tk−1 ) − θ̃(tk−1 )|2 ]}
{z
}
|
= E[|Z(tk−1 )|2 ]

≤

1

γ ϵ = (1 + F1 ϵ) ϵ e → eF1 +1 ,

E[Z(tk ) ] − E[Z(tk−1 ) ]
∫ tk
≤
(C12 + C22 ){ E[|θ(t) − θ(tk−1 )|2 ]
|
{z
}
tk−1
≤ D3 (t − tk−1 ) by (59)
2

(65)

(C12 + C22 ){D3 (t − tk−1 ) + E[|Z(tk−1 )|2 ]}

tk−1

(67)

Therefore,
(
)
T
1 − γN
ϵeϵ
ϵ
β(t0 )e
= (F2 ϵ + E[|ξt |2 ])(1 − γ ϵ )
,
1−r
1−γ
(68)
and, by using Eqs. (66) and (67), as → 0,

+ E[|ξtk−1 |2 ] + E[|θ(t) − θ̃(t)|2 ]dt.
≤ (C12 + C22 )D3 ϵ2 + (C12 + C22 )E[|Z(tk−1 )|2 ]ϵ
∫ tk
+ E[|ξtk−1 |2 ]ϵ +
E[|θ(t) − θ̃(t)|2 ]dt.
tk−1

(60)

E[Z(tN )2 ] ≤ (F2 ϵ + E[|ξt |2 ]

eF1 +1 − 1
.
F1 + 1

(69)

That is,
E[|θ(T ) − θ̃(T )|2 ] = E[|Z(tN )|2 ] = O(ϵ + E[|ξt |2 ]).

That is,

B. Proof of Theorem 6

E[Z(tk )2 ]
≤ (1 + F1 ϵ)E[Z(tk−1 )2 ] + F2 ϵ2
∫ tk
+ E[|ξtk−1 |2 ]ϵ +
E[|θ(t) − θ̃(t)|2 ]dt,
tk−1

(61)
where F1 = C12 + C22 and F2 = (C12 + C22 )D3
The Gronwall inequality (Theorem 3) can be applied as follows.
∫ tk
E[Z(tk )2 ] ≤ β(tk−1 ) +
E[Z(t)2 ]dt,
(62)
tk−1

where let E[|ξt |2 ] = maxk E[|ξtk−1 |2 ] and β(tk−1 ) = (1 +
F1 ϵ)E[Z(tk−1 )2 ] + F2 ϵ2 + E[|ξt |2 ]ϵ.
Then,
∫
E[Z(tk )2 ] ≤ β(tk−1 ) +

tk

exp(tk − t)β(tk−1 )dt,

tk−1

= β(tk−1 ) exp(ϵ).

(63)

Moreover, let γ = (1 + F1 ϵ) exp(ϵ) and β(t0 ) = F2 ϵ2 +
E[|ξt |2 ]ϵ, i.e., (63) is
E[Z(tk )2 ] ≤ γE[Z(tk−1 )2 ] + β(t0 ) exp(ϵ).

(64)

Let
u(t, ϕ) = E[h(θ(T ))|θ(t) = ϕ].

(70)

Then, we have E[h(θ(T ))] = E[h(θ(T ))|θ(0) = θ(0)] =
u(0, θ(0)) and E[h(θ̃(T ))] = E[h(θ(T ))|θ(T ) = θ̃(T )] =
u(T, θ̃(T )).
By using the Feynman-Kac formula (Theorem (4)), u(t, ϕ)
satisfies
∂u
∂u 1 2 ∂ 2 u
+a
+ b
= 0, t < T,
∂t
∂ϕ 2 ∂ϕ2
u(T, ϕ) = h(ϕ).

(71)
(72)

The Itô formula applied to u(t, θ̃(t)) shows, for tk−1 ≤ t <
tk ,
(
)
∂u
∂u 1 2 ∂ 2 u
du(t, θ̃(t)) =
+ ã
+ b̃
(t, θ̃(t))dt
∂t
∂ϕ 2 ∂ϕ2
∂u
+ b̃ (t, θ̃(t))dW (t)
∂ϕ
)
(
∂2u
∂u 1 2
(71)
+ (b̃ − b2 ) 2 (t, θ̃(t))dt
= (ã − a)
∂ϕ 2
∂ϕ
∂u
+ b̃ (t, θ̃(t))dW (t),
(73)
∂ϕ

Approximation Analysis of Stochastic Gradient Langevin Dynamics

where, for tk−1 ≤ t < tk ,


∂u
∂u(t, ϕ) 
a (t, θ̃(t)) = a(θ̃(t))
∂ϕ
∂ϕ 

Let
,

(74)

ρ(t, θ̃(t)) = (a(θ̃(tk−1 )) − a(θ̃(t)))

ϕ=θ̃(t)


∂u
∂u(t, ϕ) 
(t, θ̃(t)) = ã(θ̃(tk−1 ))
,
∂ϕ
∂ϕ ϕ=θ̃(t)

∂u
∂u(t, ϕ) 
b (t, θ̃(t)) = b(θ̃(t))
,
∂ϕ
∂ϕ ϕ=θ̃(t)

∂u
∂u(t, ϕ) 
b̃ (t, θ̃(t)) = b(θ̃(ttk−1 ))
.
∂ϕ
∂ϕ ϕ=θ̃(t)
ã

(75)
(76)
(77)

∂u
(t, θ̃(t)),
∂ϕ

(83)

for tk−1 ≤ t < tk and, by the Itô formula,
(
)
∂ρ
∂ρ
1 2 ∂2ρ
dρ(t, θ̃(t)) =
+ ã
+ b̃
(t, θ̃(t))dt
∂t
∂ϕ 2 ∂ϕ2
∂ρ
+ b̃ (t, θ̃(t))dW (t).
(84)
∂ϕ
Since

Evaluate the integral from 0 to T , noting θ̃(0) = θ(0),
u(T, θ̃(T )) − u(0, θ(0)) =
)
∫ T(
∂u 1 2
∂2u
(ã − a)
+ (b̃ − b2 ) 2 (t, θ̃(t))dt
∂ϕ 2
∂ϕ
0
∫ T
∂u
+
(78)
b̃ (t, θ̃(t))dW (t).
∂ϕ
0
Take the expectation and use that the expected value of the
Itô integral is zero, i.e.,
[∫
]
T
∂u
E
(79)
b̃ (t, θ̃(t))dW (t) = 0
∂ϕ
0
and
E[u(T, θ̃(T ))] − E[u(0, θ(0))]
]
∫ T [
∂u
=
E (ã − a) (t, θ̃(t)) dt
∂ϕ
0
[
]
∫ T
2
1
2
2 ∂ u
+
E (b̃ − b ) 2 (t, θ̃(t)) dt.
∂ϕ
0 2

(80)

(81)

Thus,
[
]
∂u
E (ã − a) (t, θ̃(t))
∂ϕ
[
]
∂u
= E (ã(θ̃(tk−1 )) − a(θ̃(t))) (t, θ̃(t))
∂ϕ
]
[
∂u
= E (a(θ̃(tk−1 )) + ξtk−1 − a(θ̃(t))) (t, θ̃(t))
∂ϕ
[
]
∂u
= E (a(θ̃(tk−1 )) − a(θ̃(t))) (t, θ̃(t))
∂ϕ


∂u


+ E ES [ξtk−1 ]
(t, θ̃(t))
∂ϕ
| {z }
=0

]
∂u
= E (a(θ̃(tk−1 )) − a(θ̃(t))) (t, θ̃(t)) .
∂ϕ
[

=0

for tk−1 ≤ t < tk ,
[
]
dE[ρ(t, θ̃(t))]
dρ(t, θ̃(t))
=E
dt
dt
[(
)
]
∂ρ
∂ρ
1 2 ∂2ρ
=E
+ ã
+ b̃
(t, θ̃(t)) ,
∂t
∂ϕ 2 ∂ϕ2

(85)

thus, by using Weierstrass theorem, there exists a constant
Ck > 0 such that


 dE[ρ(t, θ̃(t))] 


(86)

 ≤ Ck , for tk−1 ≤ t < tk ,


dt
i.e.,

By (41) and (43),
ã(θ̃(tk−1 )) = a(θ̃(tk−1 )) + ξtk−1 .

[
]
[
]
∂ρ
∂ρ
E b̃ (t, θ̃(t))dW (t) = E b̃ (t, θ̃(t)) E [dW (t)] = 0,
∂ϕ
∂ϕ
| {z }

(82)

[
]
[
]
∂u
E ρ(t, θ̃(t)) = E (ã − a) (t, θ̃(t)) ≤ Ck ϵ.
∂ϕ

(87)

Similarly, we have
[
]
∂2u
E (b̃2 − b2 ) 2 (t, θ̃(t)) ≤ Ck ϵ, for tk−1 ≤ t < tk .
∂ϕ
Therefore, using Cmax = maxk Ck ,
|E[h(θ̃(T ))] − E[h(θ(T ))]|
= |E[u(T, θ̃(T ))] − E[u(0, θ(0))]|
∫ T
Cmax ϵdt = T Cmax ϵ
≤
0

(88)

Approximation Analysis of Stochastic Gradient Langevin Dynamics

References
Ahn, Sungjin, Balan, Anoop Korattikara, and Welling,
Max. Bayesian posterior sampling via stochastic gradient fisher scoring. In Proceedings of the 29th International Conference on Machine Learning, 2012.
Carlsson, Jesper, Moon, Kyoung-sook, Szepessy, Anders,
Zouraris, Georgios, and Tempone, Raúl. Stochastic Differential Equations: Models and Numerics 1. 2010.
Daum, Frederick E. New Exact Nonlinear Filters: Theory
and Applications. Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series. Signal and
Data Processing of Small Targets, 1994.
Feynman, R. P. Space-time approach to nonrelativistic
quantum mechanics. Rev. Mod. Phys., 20:367–387,
1948.
Gard, T. C. Introduction to Stochastic Differential Equations. M. Dekker, 1988.
H.Robbins and S.Monro. A stochastic approximation
method. In Annals of Mathematical Statistics, pp. 400–
407, 1951.
Itô, Kiyoshi. Stochastic integral. In Proc. Imperial Acad.
Tokyo, pp. 519–524, 1944.
Kac, M. On distributions of certain wiener functionals.
Trans. Amer. Math. Soc., 65:1–13, 1948.
Kac, M. On some connections between probability theory
and differential and integral equations. In Proceedings of
the Second Berkeley Symposium on Mathematical Statistics and Probability, pp. 189–215, 1951.
Kloeden, Peter E and Platen, Eckhard. Numerical solution of stochastic differential equations. Springer Verlag,
1992.
Patterson, S. and Teh, Y. W. Stochastic gradient Riemannian Langevin dynamics on the probability simplex.
In Advances in Neural Information Processing Systems,
2013.
Risken, Hannes and Frank, Till. The Fokker-Planck Equation: Methods of Solution and Applications. Springer,
1984.
Welling, M. and Teh, Y. W. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the
International Conference on Machine Learning, 2011.

