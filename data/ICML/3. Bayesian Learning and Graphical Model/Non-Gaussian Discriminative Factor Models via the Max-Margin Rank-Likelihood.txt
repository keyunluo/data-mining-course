Non-Gaussian Discriminative Factor Models
via the Max-Margin Rank-Likelihood
Xin Yuan∗
EIEXYUAN @ GMAIL . COM
Ricardo Henao∗
R . HENAO @ DUKE . EDU
Ephraim L. Tsalik
E . T @ DUKE . EDU
Duke University, Durham, NC, 27708, USA
Raymond J. Langley
RLANGLEY @ LRRI . ORG
Department of Immunology, Lovelace Respiratory Research Institute, Albuquerque, NM 87108, USA
Lawrence Carin
LCARIN @ DUKE . EDU
Duke University, Durham, NC, 27708, USA

Density

We consider the problem of discriminative factor analysis for data that are in general nonGaussian. A Bayesian model based on the
ranks of the data is proposed. We first introduce a new max-margin version of the ranklikelihood. A discriminative factor model is then
developed, integrating the max-margin ranklikelihood and (linear) Bayesian support vector machines, which are also built on the
max-margin principle. The discriminative factor model is further extended to the nonlinear case through mixtures of local linear classifiers, via Dirichlet processes. Fully local conjugacy of the model yields efficient inference
with both Markov Chain Monte Carlo and variational Bayes approaches. Extensive experiments
on benchmark and real data demonstrate superior
performance of the proposed model and its potential for applications in computational biology.

a large proportion of statistical analyses performed on these
data assume Gaussianity in one way or another. This is because customary preprocessing pipelines employ normalization and/or domain transformation approaches aimed at
making the data as Gaussian, or at least as symmetric, as
possible. For example, one popular yet simple strategy for
RNA sequencing data is to rescale each sample to correct
for technical variability, followed by log-transformation or
quantile normalization (Dillies et al., 2013). This and many
other examples have the same rationale: the data transformations are order preserving, while also trying to achieve
a desired distribution, typically Gaussian. Figure 1 shows

log(x)

Abstract

Active TB
Latent TB

Rank(x)

1. Introduction
Modern applications in computational biology and bioinformatics routinely involve data coming from different
sources, measured and quantified in different ways, e.g.,
averaged intensities in gene expression, cytokines and proteomics, or fragment counts in RNA and microRNA sequencing. However, they all share a common trait: data are
rarely Gaussian, and they are often discrete, the latter due to
digital technologies used for quantification. Nevertheless,
∗

Equal contributions.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

Density

Figure 1. Intuition behind data modeling with ranks. Top and
right panels are group-wise empirical distributions for rank(x)
and log(x), respectively.

expression for a particular gene and two phenotypes (active and latent tuberculosis), from the dataset described in
Section 4.2. The horizontal and vertical axes show respectively log(x) (log-transformed) and rank(x) (ranked) gene
intensities. We see that from either axis we could derive
a decision rule to separate the two groups, so that we can
predict the label of new data, without worrying too much
about the actual values or scaling of the axes. In fact, from
their group-wise empirical distributions, we see that log(x)
and rank(x) seem to have nearly the same optimal decision rule. Note that in general we are not required to logtransform the data, and in principle we could either use raw

Max-Margin Rank-Likelihood

data or any other monotone transformation of the gene intensities while still being able to build a classifier, if the
data support it. Motivated by this fact, and also by the
success of standard nonparametric statistical approaches
based on ranks, such as Spearman’s rank correlation and
Wilcoxon rank-sum test (Lehmann & D’Abrera, 2006), in
this paper, we propose a new discriminative factor model
based directly on the ranks of observed data as opposed to
their values. This new model enjoys three significant benefits:

Experiments on benchmark and real data, namely USPS,
MNIST, gene expression and RNA sequencing, demonstrate that the proposed model often performs better than
competing approaches. Results on the real data demonstrate the potential of our model for applications in computational biology, not only for well-established, highthroughput technologies such as gene expression and
metabolomics, but also in emerging ones such as RNA sequencing and proteomics.

(1) Since we do not model actual values, we could treat
ordinal, continuous and discrete data within the same
framework.

2. Max-margin rank-likelihood

(2) We can in principle make weaker assumptions about
the distribution of the data.
(3) We can jointly identify subsets of variables with similar
(rank) correlation structures, some of which might be
able to (partially) separate different classes and could
be combined to build a classification model.
These advantages come with the price of not being able
to account for the actual values of the data, which is not
such a big disadvantage, as long as we are only interested
in parameters of the model involving relative differences or
similarities between elements of a given dataset (which is
typically the case when building classifiers).
Modeling with ranks is not a new idea, in fact Pettitt (1982)
presented a linear regression model using a likelihood function based on the ranks of observed data, coined by the authors as rank-likelihood. More recently it was also used by
Hoff (2007) to estimate the correlation matrix of data from
disparate types, e.g., binary, discrete and continuous. Here
we employ the rank-likelihood as a building block for a discriminative factor model, with the ultimate goal of being
able to jointly perform feature extraction and classification
while decreasing the effort required to preprocess raw data.
The contributions of this paper are three-fold:
(1) We introduce a new max-margin version of the ranklikelihood geared towards Bayesian factor modeling,
and we present a novel data augmentation scheme that
allows for fast inference due to local conjugacy.
(2) We propose a discriminative factor model by integrating max-margin rank-likelihood, (linear) Bayesian support vector machines (SVMs) and global-local shrinkage priors. One key feature of our model is that likelihood functions for both data and labels have the maxmargin property.
(3) We extend the discriminative factor model to nonlinear decision functions, through a mixture of local linear classifiers implemented via a Dirichlet process imposed on the latent space of the factor model.

Ordinal probit model Consider N data samples, each
a d-dimensional vector with ordinal values; the discussion
of ordinal data helps to motivate and explain the model,
which is subsequently generalized to real-valued data. The
data are represented by the d×N matrix X, the nth column
of which represents the nth data vector. Let xi,n represent
element (i, n) of X, corresponding to component i of the
nth data vector, modeled as
xi,n = gi (wi,n ) ,
wi,n =

a>
i zn

(1)

+ vi,n ,

vi,n ∼ N (0, 1) ,

where A = [a1 . . . ad ]> ∈ Rd×K is the factor loadings
matrix with K factors, the factor scores for all N data samples are represented by Z = [z1 . . . zN ] ∈ RK×N , wi,n is
element (i, n) of W ∈ Rd×N , and gi (·) is a non-decreasing
function (such that the rankings of the N realizations of
component i are preserved). Specifically, large values in xi
(rows of X) correspond to large values in wi (rows of W).
Assume that component i of each data vector takes values in the set {1, . . . , Ji }. Then function gi (·) can be
fully specified by Ji − 1 ordered parameters hi,1 < · · · <
hi,Ji −1 , often called “cut points”, yielding
xi,n = gi (wi,n ) = j

if

hi,j−1 < wi,n < hi,j ,

(2)

where hi,0 = −∞, hi,Ji = ∞ and hi = [hi,1 . . . hi,Ji −1 ]
is a vector of thresholds for the ith row of W. Equations (1)
and (2) define a typical probit factor model for ordinal vector data (Hoff, 2009).
Inferring W for the model in (1) and (2) is not complicated, because its conditional posterior corresponds
to a truncated Gaussian distributions, i.e., wi,n ∼
T N (a>
i zn , 1, hi,j−1 , hi,j ), where hi,j−1 < wi,n < hi,j
for j = xi,n . Nevertheless, the model has three important shortcomings: (i) Specifying a prior distribution for
{hi }di=1 might be difficult because often such information
is not available to the practitioner; (ii) if Ji is large, the
number of parameters of the model that need to be estimated increases substantially, making prior specification
and inference harder; (iii) sampling from a truncated Gaussian distribution can be relatively expensive, and may be

Max-Margin Rank-Likelihood

prone to numerical instabilities, especially when samples
lie near the tails of the distribution.
Rank-likelihood model Provided that gi (wi,n ) is nondecreasing by assumption, we know that given xi,n <
xi,n0 , then gi (wi,n ) < gi (wi,n0 ) and wi,n < wi,n0 , thus
R(xi ) = {wi ∈ RN : wi,n < wi,n0 if xi,n < xi,n0 } , (3)

where R(xi ) is the set of all possible vectors wi such
that rank(xi ) = rank(wi ). Given xi , since neither wi
nor p(wi ∈ R(xi )) depend on gi (wi ), we can formulate inference for A and Z directly in terms of wi ∈
R(xi ) through the rank-likelihood representation p(wi ∈
R(xi )|A, Z) (Pettitt, 1982). Specifically, we can write
the joint probability distribution of the model above as
Qd

p(wi ∈ R(xi ), ai , Z) =
)
(Z
N
d
Y
Y
>
N (wi,n ; ai zn , 1)dwi,n . (4)
p(A)p(Z)
i=1

i=1

R(xi ) n=1

The integrals to the right hand side of (4) are in general difficult to compute. However, it is not necessary to
do so, because we can estimate the posterior of parameters {A, Z, W} via Gibbs sampling, by iteratively cycling
through their conditional posterior distributions. For A and
Z we need to sample from p(Z|W, A) and p(A|W, Z), respectively, where we instantiate W such that wi ∈ R(xi )
for i = 1, . . . , d. For wi we only need to be able to
sample wi from p(wi ∈ R(xi )|ai , Z). We can write
p([wi,n wi\n ] ∈ R(xi ), ai , Z), where wi\n contains all
elements from wi but wi,n . From (1) and (3), wi,n is Gaussian and restricted to the set R(xi ), respectively. Conditioning on zi\n , we have
l
u
p(wi,n |wi\n , ai , zn ) = p(wi,n |wi,n
, wi,n
, ai , zn )
l
u
= T N (a>
i zn , 1, wi,n , wi,n ) ,
l
u
where wi,n
= max{wi,n0 : xi,n0 < xi,n } and wi,n
=
min{wi,n0 : xi,n < xi,n0 }, which jointly guarantee that
l
u
[wi,n wi\n ] ∈ R(xi ). Note that given {wi,n
, wi,n
}, wi,n
l
u
is conditionally independent of wi \{wi,n , wi,n } and also
that the conditional posteriors for the ordered probit and
rank-likelihood based models are identical except that for
the former, constraints for wi,n come from wi\n as opposed to thresholds hi . In fact, the rank-likelihood model
can be seen as an alternative to the ordered probit model
in which the threshold variables have been marginalized
out (Hoff, 2009). It is important to point out that in applications when the connection between observed and latent variables, gi (wi ), is of interest, the rank-likelihood is
not applicable. Fortunately, in factor models we are usually only interested in A and Z, the loadings and the factor
scores, respectively (see Murray et al. (2013), for example).

w

`u + `l (loss function)
u
wi,n


wi,n


l
wi,n

xi,l

xi,n xi,u

...

x

Figure 2. Graphical representation of the loss function associated to the max-margin rank-likelihood in (6), where `u + `l =
u
l
l
` (wi,n − wi,n
) + ` (wi,n
− wi,n ). Note that wi,n
+  < wi,n <
u
wi,n −  is not penalized by the loss function.

Max-margin rank-likelihood One disadvantage of the
rank-likelihood model is that differences between elements
of wi can be arbitrarily small, as there is no mechanism
in the prior distribution of wi to prevent this from happening. In the ordered probit model we can do so via the prior
for the thresholds hi , however this is not necessarily easily
accomplished. Fortunately, for the rank-likelihood we can
alleviate this issue by modifying (3) as
Rmm (xi ) = {wi ∈ RN : wi,n < wi,n0 −  if xi,n < xi,n0 } , (5)

where we have made explicit that any two distinct elements
of wi must be separated by a gap of size no smaller than
 > 0. Furthermore, max{wi,n0 : xi,n0 < xi,n } +  <
wi,n < min{wi,n0 : xi,n < xi,n0 } − . From (5) we can
write a pseudo-likelihood for wi,n as

	
u
l
Li (wi,n |wi\n ) = exp −` (wi,n − wi,n
) − ` (wi,n
− wi,n ) ,

(6)

l
> l
u
> u
where wi,n = a>
i zn , wi,n = ai zn , wi,n = ai zn and
` (u) = 2max(0, u + ) can be interpreted as the “onesided” -sensitive loss. From (6) this means that ` (u) >
l
u
0 only if wi,n < wi,n
+  or wi,n > wi,n
− ; it also
l
means that this loss function does not penalize wi,n
+ <
u
wi,n < wi,n −  and  is called the margin. See Figure 2 for
a graphical representation of the proposed composite loss
function. Maximizing (6) is equivalent to finding wi ∈
Rmm (xi ) such that differences between neighbor elements
of wi are maximal given , hence the term max-margin is
used.

Recent work by Polson & Scott (2011b) has shown
that ` (u) admits a location-scale mixture of Gaussians,
specifically,
they showed that exp{−2max(0, u)} =
R
N (u; −λ, λ)dλ, thus we can rewrite (6) as
Z
u
Li (wi,n |wi\n ) = N (wi,n − wi,n
; − − λui,n , λui,n )
l
× N (wi,n
− wi,n ; − − λli,n , λli,n )dλui,n dλli,n , (7)

where N (u; ·) is the density function of a Gaussian distribution, and we have introduced two sets of latent variables {λui,n } and {λli,n }. This data augmentation scheme

Max-Margin Rank-Likelihood

implies that the pseudo-likelihood before marginalization,
Li (wi,n |wi\n , λui,n , λli,n ), is conjugate to a Gaussian distribution, just as in the original rank-likelihood formulation,
but without the difficulty of truncated Gaussians, because
wi,n is now exactly a>
i zn , not a random variable. Note that
as a result of this, we have transferred the uncertainty between the rank of xi,n and the factorization a>
i zn from wi,n
in the rank-likelihood (and the ordered probit) to the set of
location-scale parameters {λui,n , λli,n } in our max-margin
formulation.
Discrete and continuous data So far we have assumed
that we have ordinal data (the cut points of the ordinal
model help explain the associated mechanics of the ranklikelihood model). However, we can also use the ranklikelihood with discrete or continuous data, as long as we
acknowledge that likelihood and posteriors derived from
them only contain information about the order of the observations and not their actual values.
In general terms, factor models are concerned with learning about the covariance structure of observed data via a
low rank matrix decomposition, AZ. In this sense, the role
of the likelihood is to define the way in which covariances
are measured. This means that one important difference between standard Gaussian and rank-likelihood based factor
models is that they use different notions of covariance; very
much in the same spirit of the differences between Pearson
and Spearman correlations. Another important difference
is the generative mechanism implied by the likelihood. In
the rank-likelihood, we can generate a statistic, namely the
rank, based on a sample population but not their values.
This happens because we ignore the part of the model that
links the statistic with actual data, specifically
p(xi |ai , Z, hi ) = p(rank(xi ), xi |ai , Z, hi )
= p(rank(xi )|ai , Z)p(xi |rank(xi ), ai , Z, hi ) .
We infer ai and Z strictly from p(rank(xi )|ai , Z), the
marginal likelihood, via wi ∈ Rmm (xi ). Since we ignore
p(xi |rank(xi ), ai , Z, hi ), we do not infer the thresholds
hi , thus in the strictest sense we are not using all information provided by xi , i.e., its values, however we are assuming that ranks alone contain enough information to be able
to characterize the covariance structure of the data so we
can reliably estimate A and Z. Additional examples and
further discussion of Bayesian analysis employing similar
marginal likelihood strategies can be found in Monahan &
Boos (1992).

3. Bayesian SVM based discriminative factor
model
When the data being analyzed belong to two different
classes, encoded as {−1, 1}, labels y = [y1 . . . yN ]> ∈

{−1, 1} will encourage our factor model to learn discriminative features (loadings and scores) from the data, then
these features can be used to make predictions for new data.
This modeling approach is commonly known as supervised dictionary learning or discriminative factor analysis
(Mairal et al., 2008). From a Bayesian perspective, factor
models and probit/logit link based classifiers have been already successfully combined; see for instance Salazar et al.
(2012); Quadrianto et al. (2013).
Unlike previous work, we continue with the max-margin
theme and develop a supervised factor model using
Bayesian support vector machines (SVMs). The same result from Polson & Scott (2011b) used above to derive the
max-margin rank-likelihood provides a pseudo-likelihood
for the hinge loss, traditionally employed in the context of
SVMs (Polson & Scott, 2011b). Specifically,
Ln (yn |β, zn ) = exp{−2max(0, un )}


Z ∞
1 (un + λcn )2
1
p
dλcn ,
exp −
=
2
λcn
2πλcn
0

(8)

where un = 1 − yn β > zn , β ∈ RK is a vector of classifier coefficients and {λcn } is a vector of latent variables,
with superscript c denoting the classifier. In Polson & Scott
(2011b) covariates, zn , are observed while here they are
latent variables (factor scores) that need to be estimated
jointly with the remaining parameters of a factor model. It
has been shown empirically that linear margin-based classifiers, SVMs being a special case, often perform better than
those using logit or probit links (Polson & Scott, 2011a;
Henao et al., 2014).
Interestingly, in our factor model the max-margin mechanism plays two roles, i.e., data and labels are both connected to the factor model core via max-margin pseudolikelihoods: rank-likelihood for the data and hinge loss for
the labels. Furthermore, for the loadings, since shrinkage
for A is usually a requirement for interpretability or when
N  d, here we use a three-parameter-beta normal prior
(T PBN ) (Armagan et al., 2011), a fairly general globallocal shrinkage prior (Polson & Scott, 2010), for which
it has been demonstrated that it has better mixing properties than priors such as spike-slab (Carvalho et al., 2010).
Shrinkage for the elements of β is also employed, because
it allows us to identify the features of A that contribute to
the classification task. Intuitively, we can see A as a dictionary with K features, each feature explaining a subset of
the input variables due to shrinkage; via separate shrinkage
within the model, β selects from the K features to build
a predictor for labels y. Being able to specify global and
local properties independently makes the T PBN prior attractive for high-dimensional settings, such as gene expression and RNA sequencing, which are precisely the types of
data we will focus on in our experiments.

Max-Margin Rank-Likelihood

Linear discriminative factor model By imposing the
max-margin rank-likelihood construction in (5) to X and
the hinge loss to y via pseudo-likelihoods in (7) and (8),
respectively, one possible prior specification for the supervised factor model parameterized by {A, Z, β} is
(a)

ai,k ∼ T PBN (ra , sa , Φk ) ,
βk ∼ T PBN (rβ , sβ , Φ

(β)

zn ∼ N (0, IK ) ,

),

(a)

where Φk , Φ(β) are global shrinkage parameters for loadings A and classifier coefficients β. Furthermore, for the
T PBN prior we can write
ai,k ∼ N (0, ξi,k ) ,

βk ∼ N (0, bk ) ,

ξi,k ∼ Ga(ra , ηi,k ) ,

bk ∼ Ga(rβ , ek ) ,

ηi,k ∼

(a)
Ga(sa , Φk ) ,

ek ∼ Ga(sβ , Φ

(β)

).

Setting ra = sa = 21 (for β, it is rβ = sβ = 12 ), a special
case of T PBN corresponds to the widely known horseshoe prior (Carvalho et al., 2010). Note that each column
of the loadings has a different global shrinkage parame(a)
ter Φk , thus allowing them to have different degrees of
(a)
shrinkage. We can also infer Φk (and Φ(β) ) by letting
(a)
Φk ∼ Ga( 12 , Φ̃) and Φ̃ ∼ Ga( 12 , 1). As a result of having individual shrinkage parameters for each column of A,
we could say that the prior is capable of “turning off” unnecessary factors, hence having an automatic relevance determination flavor to it (MacKay, 1995; Wipf & Nagarajan, 2008). This is indeed the behavior we see in practice;
there are other ways to select the number of factors, e.g.,
by adding a multiplicative gamma prior to matrix A (Bhattacharya & Dunson, 2011).
Non-Linear discriminative factor model When the latent space for factor scores, zn , is not linearly separable,
nonlinear classification approaches might be more appropriate. One may use a kernel to extend the linear SVM
to its nonlinear counterpart. However, from a Bayesian
factor modeling perspective, adding kernel-based nonlinear classifiers is nontrivial, because they tend to make inference complicated and computationally expensive due to
loss of conjugacy for the parameters involved in the nonlinear component of the model, i.e., the kernel function. From
a different perspective, it is still possible to build a global
nonlinear decision rule as a mixture of local linear classifiers (Shahbaba & Neal, 2009; Fu et al., 2010). The basic
idea is to assume factor scores as coming from a mixture
model, in which each mixture component has an associated
local linear Bayesian SVM. Here we use a Dirichlet process (DP) in its stick-breaking construction (Sethuraman,
2001), represented as
P∞
Qt−1
G = t=1 qt δθ∗t ,
qt = νt l=1 (1 − νl ) ,
νt ∼ Beta(1, α) ,

θ ∗t ∼ G0 ,

(9)

P∞
where t=1 qt = 1, δθ∗t represents a point measure at
θ ∗t and α is the concentration parameter. Applied to our
model, factor scores and labels are drawn from a parametric model yn , zn ∼ f (θ n ) with parameters θ n , where θ n ∼
G. For G as in (9) and a finite number of samples N , many
of the {yn , zn } share the same parameters, therefore making {yn , zn } a draw from a mixture model. Specifically, we
make f (θ n ) = Ln (yn |β n , zn )N (zn |µn , ψn−1 IK ), G0 =
T PBN (β|rβ , sβ , Φ(β) ) × N (µ|0, IK ) × Ga(ψ|ψs , ψr )
and {β n , µn , ψn } = {β t , µt , ψt } if sample n belongs to
the t-th component of the mixture. In practice we truncate
the sum in (9) to T terms to make inference easier (Ishwaran & James, 2001) and set ψs = 1.1 and ψr = 0.001
(i.e., a non-informative prior).
Predictions Making predictions for new data using our
model is conceptually simple. We use the pair {y, X} to
estimate the parameters of the model (training), namely
{A, Z, β}, then given a test point x? , we go through three
steps: (i) Compare x? to X to determine the rank of each
component of x? w.r.t. to the training data, which amounts
l
u
to finding {wi,?
, wi,?
}, for i = 1, . . . , d. (ii) For fixed
l
u
{A, wi,? , wi,? }, estimate z? from its conditional posterior.
(iii) Make a prediction for x? using sign(β > z? ).
The first step of this prediction process is exclusive to the
proposed rank-likelihood model, and implies that we are
required to keep the training data in order to make predictions. This is in the same spirit of supervised kernel methods, in the sense that predictions are a function of the data
used to fit the model (Scholkopf & Smola, 2001). Note,
however, that for a single component of a test point, xi,? ,
we only need two elements of the training set: the two elements from xi closest to xi,? from above and below, which
is closely related to the k-nearest neighbor paradigm (rather
than k nearest neighbors, we only require the two training
neighbors “to the left and right” of a test data component).
Intuitively, what our model does at prediction is to find a
latent representation z? such that x? is in between but as
far as possible from its upper and lower bounds w.r.t. to X.
This is a very unique characteristic of our model.
Inference Due to fully local conjugacy, we can write the
conditional posterior distribution for all parameters of our
model in closed form, making Markov Chain Monte Carlo
(MCMC) inference based on Gibbs sampling a straightforward procedure. Space limitations prevent us from presenting the complete set of conditionals, however below
we show expressions for the parameters involving the maxmargin rank-likelihood in (7), namely A and Z. For convenience, we denote
yn βk [1 + λcn − yn (β > zn )\k ]
,
Γk,n =
λcn
λi,n = (λli,n )−1 + (λui,n )−1 ,
(β > zn )\k = β > zn − βk zk,n ,

Max-Margin Rank-Likelihood

In the following conditional posterior-distributions, “·”
refers to the conditioning parameters of the distributions.
Sampling A:
p(ai,k | ·) = N (µai,k , σa2i,k ) ,
PN
−1
2
σa−2
= ξi,k
+ n=1 zk,n
λ−1
i,n ,
i,k
P
(k)
N
µai,k = σa2i,k n=1 zk,n ∆i,n ,
Sampling Z:
p(zk,n | ·) =

N (µzk,n , σz2k,n ) ,

σz−2
=1+
k,n

Pd

i=1

µzk,n = σz2k,n

P

a2i,k λ−1
i,n +

d
i=1

βk2
λcn

,


(k)
ai,k ∆i,n + Γk,n ,

Table 1. Composition of different methods.
Method
Likelihood
Classifier DPM
G-L-Probit
Gaussian
probit
No
G-L-BSVM
Gaussian
BSVM
No
OR-L-Probit
ordinary rank
probit
No
OR-L-BSVM
ordinary rank
BSVM
No
R-L-BSVM
max-margin rank
BSVM
No
G-NL-BSVM
Gaussian
BSVM
Yes
R-NL-BSVM max-margin rank
BSVM
Yes

above consider discriminative factors models, and for PFA
this is very difficult, because in that case the prior for
the factor scores is not a Gaussian distribution and is thus
not conjugate to the SVM or probit-based likelihoods. As
a result, building discriminative factor models under that
framework is challenging, at least not without MetropolisHastings style inference.

where
(k)

∆i,n =



l
wi,n
+−wi,n
λli,n

−

u
wi,n +−wi,n
λu
i,n


+ ai,k zk,n λi,n .

Conditional posteriors for the remaining parameters:
(a)
{λli,n , λui,n , λcn , β} and {ξi,k , ηi,k , Φk , bk , ek , Φ(β) } can
be found in Polson & Scott (2011b) and Armagan et al.
(2011), respectively. In our experiments we set  =
0.05, however a conjugate prior (gamma distribution) exists hence  can be inferred if desired. Inference details for
the DP specification for the factor scores can be found for
instance in Ishwaran & James (2001); Neal (2000).
In applications where speed is important, we can use all
conditional posteriors including those above to derive a
variational Bayes (VB) inference algorithm for our model,
which loosely amounts to replacing the conditioning on
variables with their corresponding moments. Details of the
conditionals are not shown here for brevity, and details of
the VB procedure are found in the Supplementary Material.
Other related work For ordinal data, Xu et al. (2013)
presented a factor model using the ordered probit mechanism, but in which the probit link is replaced with a
max-margin pseudo-likelihood. Inference is very efficient,
but they still have to infer the thresholds {hi }. However, in their collaborative prediction applications, variables only take one of six possible values. For count data,
Chib et al. (1998) proposed a generalized-linear-model inspired Bayesian model for Poisson regression, that can be
easily extended to a factor model. However, expensive
Metropolis-Hastings sampling algorithms need to be used,
due to the non-conjugacy between the prior for A and the
log link. More recently, Zhou et al. (2012) presented a
novel formulation of Poisson factor analysis (PFA), based
on the beta-negative binomial process, for which inference
is efficient. Furthermore, none of the approaches discussed

4. Experiments
We present numerical results on two benchmark (USPS
and MNIST) and two real (gene expression and RNA sequencing) datasets, using part of or all methods summarized in Table 1; inference is performed via VB. The data
likelihood can be either Gaussian, rank or the max-margin
rank-likelihood. The labels (classifier) can be modeled using the probit link or the Bayesian SVM (BSVM) pseudolikelihood. When the DP mixture (DPM) model is used,
the classifier results in a nonlinear (NL) decision function.
Everywhere we set K = 20, T = 5 and performance measures were averaged over 5 repetitions (standard deviations
are also presented). We verified empirically that further increasing K or T does not significantly change the outcome
of any of our models. All code used in the experiments was
written in Matlab and executed on a 3.3GHz desktop with
16Gb RAM.
In the following experiments we focus on comparing discriminative factor models against each other to show how
each component of the model contributes to the end performance produced by our full model. In particular, we
show that the Bayesian SVM, max-margin rank-likelihood
and nonlinear decision function all improve the overall performance on their own, when compared to standard approaches such as probit regression and Gaussian models on
log-transformed data. It is important to take into consideration that our model is at the same time trying to explain
the data and to build a classifier via a linear latent representation of ranks, thus we will not attempt to match results
obtained by more sophisticated state-of-the-art classification models (e.g., a nonlinear SVM applied directly to raw
data may yield a good classifier, but it does not afford the
generative interpretability of a factor model, the latter particularly relevant to biological applications). Our model is
ultimately trying to find a good balance between covari-

Max-Margin Rank-Likelihood

Error
Runtime

Table 2. Mean error rates (%) and runtime in seconds for the test data of the USPS 3 vs. 5 subtask.
G-L-Probit
G-L-BSVM OR-L-Probit OR-L-BSVM R-L-BSVM G-NL-BSVM R-NL-BSVM
5.95±0.005 5.86±0.008
5.05±0.013
4.92±0.027
4.53±0.026
3.88±0.017
3.23±0.025
8.64
10.29
14.07
14.19
16.05
23.81
36.63

Table 3. Mean error rates (%) and runtime in seconds for the test data of the MNIST 3 vs. 5 subtask.
G-L-BSVM R-L-BSVM L-SVM G-NL-BSVM R-NL-BSVM NL-SVM
Error
5.05±0.053 4.84±0.014
4.68
4.21±0.010
2.10±0.007
2.00
Runtime
150
220
140
400
600
304

ance structure modeling, interpretability through shrinkage
and classification performance. All of this is done with the
very important additional benefit of not requiring distributional assumptions about the values of the data, as this information is usually not known in practice (as in the subsequent biological experiments below). However, in those
cases where the distribution is known a priori it should certainly be reflected in likelihood function.
4.1. Handwritten digits
Digitized images are a good example of essentially nonGaussian data traditionally modeled using Gaussian noise
models in the context of factor models and dictionary learning (Mairal et al., 2008). However, depending on preprocessing, they can be naturally treated either as continuous variables representing pixel intensities when filtering/smoothing is pre-applied, or as discrete variables representing pixel values when raw data is available. Our running hypothesis here is that a rank-likelihood representation for pixels is more expressive than its Gaussian counterpart. Intuitively, a discriminative factor model should be
able to find features (subsets of representative pixels) that
separate image subtypes. However, without the Gaussian
assumption for observations, our rank model might be able
to adapt to more general conditions, e.g., skewed or heavytailed distributions. In our results we use classification error on a test set as a quantitative measure of performance.

USPS First we consider the models in Table 1 to the well
known 3 vs. 5 subtask of the USPS handwritten digits
dataset. It consists of 1540 smoothed gray scale 16 × 16
images rescaled to fit within the [−1, 1] interval. Each observation is a 256-dimensional vector of scaled pixel intensities. Here we use the resampled version, which is 767
images for model fitting and the remaining 773 for testing.
Results in Table 2 show that consistently: rank-likelihood
based models outperform Gaussian models, BSVM outperforms the probit link, and nonlinear outperforms linear classifiers. Furthermore, the proposed max-margin rank
likelihood model performs best in both variants, linear and
nonlinear. In every case inference took less than 1 minute.

MNIST Next we consider the same 3 vs. 5 task, this time
on a larger dataset, the MNIST database. The dataset is
composed by 11552 training images and 1902 test images.
Unlike USPS, MNIST consists of 28×28 raw 8-bit encoded
images, so each observation is a 784-dimensional vector of
pixel values (discrete values from {0, . . . , 255}). Results
for four out of six methods from Table 1 are summarized
in Table 3. Results for probit based models were not as
good as those for BSVM, thus not showed here, nor in the
upcoming experiments. Instead, we include results for a
linear (L-SVM) and nonlinear (NL-SVM) SVM with RBF
kernel directly applied to the data as baselines, without the
factor model. From Table 3 we see that the proposed model
works better than the Gaussian model, and that the results
for R-NL-BSVM are close to that for NL-SVM. This is not
surprising, as a pure classification model (e.g., NL-SVM)
does not attempt to explain the data but only to maximize
classification performance. In this case, the most expensive approach, namely R-NL-BSVM has a runtime in the
neighborhood of 10 minutes which is deemed acceptable
considering the size of the dataset. Visualizations of the
factor loadings, A, learned by various models are presented
in the Supplementary Material.
4.2. Gene expression
We applied our model to a newly published tuberculosis
study from Anderson et. al. (2014), consisting of gene
expression intensities for 47323 genes and 334 subjects
(GEO accession series GSE39941). These subjects can be
partitioned in three phenotypes: active tuberculosis (TB)
(111), latent TB (54) and other infectious diseases (169),
and in whether they are positive (107) or negative (227)
for HIV. The raw data were preprocessed with background
correction, sample-wise scaling and gene filtering. For the
analysis we keep the top 4732 genes with largest intensity profiles. Results for three binary classification tasks
using a one vs. the rest scheme, the HIV classifier and
10-fold cross-validation are summarized in Table 4 (error
bars for accuracies omitted due to space limitations). We
also present area under the ROC curve (AUC) to account
for subset imbalance. As an additional baseline, we included a Poisson Factor Analysis (PFA) model (Zhou et al.,

Max-Margin Rank-Likelihood
Table 4. AUC (with error bars), accuracy and runtime in seconds for gene expression data.
Methods
PFA-L-BSVM
G-L-BSVM
R-L-BSVM
G-NL-BSVM
TB vs. Others
0.740±0.102, 0.683 0.766±0.093, 0.704 0.814±0.052, 0.740 0.847±0.061, 0.778
Active TB vs. Others 0.802±0.070, 0.775 0.857±0.050, 0.784 0.896±0.028, 0.832 0.921±0.034, 0.853
Latent TB vs. Others 0.849±0.051, 0.802 0.907±0.037, 0.841 0.923±0.041, 0.868 0.934±0.029, 0.874
HIV(+) vs. HIV(-)
0.850±0.056, 0.793 0.879±0.055, 0.844 0.900±0.055, 0.856 0.915±0.041, 0.850
130
141
180
330
One fold time

2012) with Bayesian SVMs, as a 2-step procedure. For
the Gaussian models we log-transform intensities, and for
PFA we round them to the closest integer value (raw intensities become floating point values after background correction and scaling). We can see that our models outperform the others in each of the classification subtasks, and
R-NL-BSVM performs the best overall with a reasonable
computational cost. It is important to mention that we
are not building separate discriminative factor models for
each subtask, instead a single factor model jointly learns
the four predictors, meaning that all classifiers share the
same loadings and factor scores. As a result, our model
operates here as a multi-tasking learning scheme. Figure 3

R-NL-BSVM
0.872±0.025, 0.781
0.948±0.021, 0.880
0.954±0.025, 0.889
0.959±0.051, 0.901
450

of sample-wise rescaling to compensate for differences in
library size, log-transform for Gaussian models and rounding for PFA. Subjects are split into three different groups,
namely systemic inflammatory response (SIRS) (26), sepsis survivors (SeS) (78) and sepsis complications leading to
death (SeD) (29). Three binary comparisons are the main
interest of the study: SIRS vs (all) sepsis, SeS vs. SeD and
SIRS vs. SeS. Being able to classify this sub-groupings is
important for two reasons: (i) these tasks are known to be
hard classification problems, and (ii) a recently published
study by Liu et al. (2014) showed that approximately 40%
of hospital mortality is sepsis related. Classification results
for 10-fold cross-validation including AUC, accuracy and
runtime per fold are summarized in Table 5. Once again
our model performs the best. We also tried the nonlinear
version of our model but figures were omitted due to very
minor improvements in performance.
Table 5. AUC, accuracy and runtime(s) for RNAseq data.

Figure 3. The learned coefficients β for the 4 classifiers based on
gene expression data.

shows the coefficients of the classifier learned using R-LBSVM. The leading coefficients reveal that certain factors
are key to different classes. For instance, factor 14 is specific to TB vs. others, factors 1 and 5 are specific to TB
vs. latent TB, and factors 9 and 4 are specific to TB vs.
others including HIV(+). We performed a pathway association analysis using DAVID (Huang et al., 2009) on
the top 200 genes from each factor. We found interesting
associations. Factor 14: ubiquitin-protein, ligase activity
and immunodeficiency. Factor 9: immune response, lymphocite/leukocite/T cell activation and apoptosis. Factor 5:
proteasome complex, response to stress, response to antibiotic. Factor 4: ribonucleoprotein, proteasome, ubiquitinprotein, ligase activity. The complete gene lists and the
inferred gene networks are provided in the Supplementary
Material.
4.3. RNA sequencing
Finally, we consider a new RNA sequencing (RNAseq)
sepsis study (Langley et al., 2013). The dataset consists of
133 subjects and 15158 genes (after removing genes with
more than 25% zero entries). Data preprocessing consists

Methods
SIRS vs. Se
SeD vs. SeS
SIRS vs. SeS
One fold time

PFA-L-BSVM
0.70±0.04, 0.73
0.76±0.05, 0.70
0.75±0.02, 0.71
179

G-L-BSVM
0.78±0.02, 0.76
0.76±0.01, 0.75
0.87±0.01, 0.71
175

R-L-BSVM
0.86±0.01, 0.81
0.82±0.02, 0.78
0.91±0.01, 0.87
226

5. Conclusion
We have developed a Bayesian discriminative factor model
for data that are generally non-Gaussian. This is achieved
via the integration of a new max-margin rank likelihood,
Bayesian support vector machines, global-local shrinkage
priors, and a Dirichlet process mixture model. The proposed model is built on the ranks of the data, opening the
door to treat ordinal, continuous and discrete data (e.g.,
count data) within the same framework. Experiments have
demonstrated that the proposed factor model achieves better performance than widely used log-transformed-plusGaussian models and a Poisson model, on both gene expression and RNA sequencing data. These results highlight
the potential of the proposed model in a variety of applications, especially computational biology.
Our rank based models are relatively more computationally expensive than Gaussian models on log-transformed
data. However, in applications such as gene expression
or sequencing that constitute the real data used in our experiments, runtimes are still significantly lower when compared to the time needed to generate the data. For biological studies, the quality and interpretability of the results are
of paramount importance, with speed a secondary issue.

Max-Margin Rank-Likelihood

Acknowledgments
The research reported here was funded in part by ARO,
DARPA, DOE, NGA and ONR.

References
Anderson et. al., S. T. Diagnosis of childhood tuberculosis
and host RNA expression in Africa. The New England
Journal of Medicine, 370(18):1712–1723, 2014.
Armagan, A., Clyde, M., and Dunson, D. B. Generalized
beta mixtures of gaussians. In NIPS 24, 2011.
Bhattacharya, A. and Dunson, D. B. Sparse Bayesian infinite factor models. Biometrika, 98(2):291–306, 2011.
Carvalho, C. M., Polson, N. G., and Scott, J. G. The horseshoe estimator for sparse signals. Biometrika, 97(2):
465–480, 2010.
Chib, S., Greenberg, E., and Winkelmann, R. Posterior
simulation and Bayes factors in panel count data models.
Journal of Econometrics, 86:33–54, 1998.
Dillies, M.-A. et al. A comprehensive evaluation of normalization methods for illumina high-throughput rna sequencing data analysis. Briefings in bioinformatics, 14
(6):671–683, 2013.
Fu, Z., Robles-Kelly, A., and Zhou, J. Mixing linear SVMs
for nonlinear classification. IEEE-TNN, 2010.
Henao, R., Yuan, X., and Carin, L. Bayesian nonlinear
support vector machines and discriminative factor modeling. In NIPS, 2014.
Hoff, P. D. Extending the rank likelihood for semiparametric copula estimation. The Annals of Applied StatisticsS,
1(1):265–283, 2007.
Hoff, P. D. A First Course in Bayesian Statistical Methods.
Springer, 2009.
Huang, D. W., Sherman, B. T., and Lempicki, R.A. Systematic and integrative analysis of large gene lists using
david bioinformatics resources. Nature Protoc., 1:44–
57, 2009.
Ishwaran, H. and James, L. F. Gibbs sampling methods for
stick-breaking priors. JASA, 96(453):161–173, 2001.
Langley, R. J., Tsalik, E. L., , van Velkinburgh, J. C.,
et al. An integrated clinico-metabolomic model improves prediction of death in sepsis. Science translational medicine, 5(195), 2013.
Lehmann, E. L. and D’Abrera, H. J. M. Nonparametrics:
statistical methods based on ranks. Springer New York,
2006.

Liu, V., Escobar, G .J., Greene, J. D., et al. Hospital deaths
in patients with sepsis from 2 independent cohorts. Journal of the American Medical Association, 2014.
MacKay, D. J. C. Probable networks and plausible predictions – A review of practical Bayesian methods for
supervised neural networks. Network: Computation in
Neural Systems, 6(3):469–505, 1995.
Mairal, J., Bach, F., Ponce, J., Sapiro, G., and Zisserman,
A. Supervised dictionary learning. In NIPS 21, 2008.
Monahan, J. F. and Boos, D. D. Proper likelihoods for
Bayesian analysis. Biometrika, 79(2):271–278, 1992.
Murray, J. S., Dunson, D. B., Carin, L., and Lucas, J. E.
Bayesian Gaussian copula factor models for mixed data.
JASA, 108(502):656–665, 2013.
Neal, R. M. Markov chain sampling methods for Dirichlet
process mixture models. Journal of Computational and
Graphical Statistics, 9(2):249–265, 2000.
Pettitt, A. N. Inference for the linear model using a likelihood based on ranks. JRSS-B, 44(2):234–243, 1982.
Polson, N. G. and Scott, J. G. Shrink globally, act locally:
sparse Bayesian regularization and prediction. Bayesian
Statistics, 9:501–538, 2010.
Polson, N. G. and Scott, S. L. Rejoinder: Data augmentation for support vector machines. Bayesian Analysis, 6
(1):43–48, 2011a.
Polson, N. G. and Scott, S. L. Data augmentation for
support vector machines. Bayesian Analysis, 6(1):1–23,
2011b.
Quadrianto, N., Sharmanska, V., Knowles, D. A., and
Ghahramani, Z. The supervised IBP: Neighbourhood
preserving infinite latent feature models. CoRR, 2013.
Salazar, E., Cain, M. S., Mitroff, S. R., and Carin, L. Inferring latent structure from mixed real and categorical
relational data. In ICML, 2012.
Scholkopf, B. and Smola, A. J. Learning with kernels: support vector machines, regularization, optimization, and
beyond. MIT press, 2001.
Sethuraman, J. A constructive definition of the Dirichlet
prior. Statistica Sinica, 4:639–650, 2001.
Shahbaba, B. and Neal, R. M. Nonlinear models using
Dirichlet process mixtures. JMLR, 10:1829–1850, 2009.
Wipf, D. and Nagarajan, S. A new view of automatic relevance determination. In NIPS 21, 2008.

Max-Margin Rank-Likelihood

Xu, M., Zhu, J., and Zhang, B. Fast max-margin matrix
factorization with data augmentation. In ICML, 2013.
Zhou, M., Hannah, L., Dunson, D., and Carin, L. Betanegative binomial process and Poisson factor analysis.
In AISTATS, 2012.

