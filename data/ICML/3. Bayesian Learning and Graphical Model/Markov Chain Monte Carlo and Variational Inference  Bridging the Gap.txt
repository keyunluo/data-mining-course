Markov Chain Monte Carlo and Variational Inference:
Bridging the Gap
Tim Salimans
Algoritmica

TIM @ ALGORITMICA . NL

Diederik P. Kingma and Max Welling
University of Amsterdam

Abstract
Recent advances in stochastic gradient variational inference have made it possible to perform
variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo
methods where we incorporate one or more steps
of MCMC into our variational approximation.
By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the
best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional
computation for additional accuracy. We describe the theoretical foundations that make this
possible and show some promising first results.

1. MCMC and Variational Inference
Bayesian analysis gives us a very simple recipe for learning
from data: given a set of unknown parameters or latent variables z that are of interest, we specify a prior distribution
p(z) quantifying what we know about z before observing
any data. Then we quantify how the observed data x relates
to z by specifying a likelihood function p(x|z).
R Finally, we
apply Bayes’ rule p(z|x) = p(z)p(x|z)/ p(z)p(x|z)dz
to give the posterior distribution, which quantifies what we
know about z after seeing the data.
Although this recipe is very simple conceptually, the implied computation is often intractable. We therefore need
to resort to approximation methods in order to perform
Bayesian inference in practice. The two most popular apProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

[ D . P. KINGMA , M . WELLING ]@ UVA . NL

proximation methods for this purpose are variational inference and Markov Chain Monte Carlo (MCMC). The former
has the advantage of maximizing an explicit objective, and
being faster in most cases. The latter has the advantage of
being nonparametric and asymptotically exact. Here, we
show how both methods can be combined in order to get
the best of both worlds.
1.1. Variational Inference
Variational inference casts Bayesian inference as an optimization problem where we introduce a parameterized posterior approximation q✓ (z|x) which is fit to the posterior
distribution by choosing its parameters ✓ to maximize a
lower bound L on the marginal likelihood:
log p(x)

log p(x)

DKL (q✓ (z|x)||p(z|x))

= Eq✓ (z|x) [log p(x, z)

(1)

log q✓ (z|x)] = L. (2)

Since log p(x) is independent of ✓, maximizing the
bound L w.r.t. ✓ will minimize the KL-divergence
DKL (q✓ (z|x)||p(z|x)). The bound above is tight at
DKL (q✓ (z|x)||p(z|x)) = 0, when the approximation
q✓ (z|x) perfectly matches p(z|x).
1.2. MCMC and Auxiliary Variables
A popular alternative to variational inference is the method
of Markov Chain Monte Carlo (MCMC). Like variational
inference, MCMC starts by taking a random draw z0 from
some initial distribution q(z0 ) or q(z0 |x). Rather than optimizing this distribution, however, MCMC methods subsequently apply a stochastic transition operator to the random draw z0 :
zt ⇠ q(zt |zt 1 , x).

By judiciously choosing the transition operator
q(zt |zt 1 , x) and iteratively applying it many times,
the outcome of this procedure, zT , will be a random
variable that converges in distribution to the exact posterior
p(z|x). The advantage of MCMC is that the samples it

Markov Chain Monte Carlo and Variational Inference: Bridging the Gap

gives us can approximate the exact posterior arbitrarily
well if we are willing to apply the stochastic transition
operator a sufficient number of times. The downside of
MCMC is that in practice we do not know how many
times is sufficient, and getting a good approximation using
MCMC can take a very long time.
The central idea of this paper is that we can interpret the stochastic Markov chain q(z|x)
=
QT
q(z0 |x) t=1 q(zt |zt 1 , x) as a variational approximation
in an expanded space by considering y = z0 , z1 , . . . , zt 1
to be a set of auxiliary random variables. Integrating
these auxiliary random variables into the variational lower
bound (2), we obtain
(3)

Laux

= Eq(y,zT |x) [log[p(x, zT )r(y|x, zT )]
=L

log q(y, zT |x)]

Eq(zT |x) {DKL [q(y|zT , x)||r(y|zT , x)]}

 L  log[p(x)],

where r(y|x, zT ) is an auxiliary inference distribution
which we are free to choose, and ourR marginal posterior
approximation is given by q(zT |x) = q(y, zT |x)dy. The
marginal approximation q(zT |x) is now a mixture of distributions of the form q(zT |x, y). Since this is a very rich
class of distributions, auxiliary variables may be used to obtain a closer fit to the exact posterior (Salimans & Knowles,
2013). The choice r(y|x, zT ) = q(y|x, zT ) would be optimal, but again often intractable to compute; in practice,
good results can be obtained by specifying a r(y|x, zT ) that
can approximate q(y|x, zT ) to a reasonable degree. One
way this can be achieved is by specifying r(y|x, zT ) to
be of some flexible parametric form, and optimizing the
lower bound over the parameters of this distribution. In
this paper we consider the special case where the auxiliary inference distribution also has a Markov structure just
like the posterior approximation: r(z0 , . . . , zt 1 |x, zT ) =
QT
t=1 rt (zt 1 |x, zt ), in which case the variational lower
bound can be rewritten as
Eq [log p(x, zT )

log p(x)

+

T
X
t=1

log[rt (zt

(4)

log q(z0 , . . . , zT |x)

+ log r(z0 , . . . , zt 1 |x, zT )]
⇥
= Eq log[p(x, zT )/q(z0 |x)]

1 |x, zt )/qt (zt |x, zt 1 )]

⇤

.

where the subscript t in qt and rt highlights the possibility
of using different transition operators qt and inverse models rt at different points in the Markov chain. By specifying these qt and rt in some flexible parametric form, we
can then optimize the value of (4) in order to get a good
approximation to the true posterior distribution.

2. Optimizing the lower bound
For most choices of the transition operators qt and inverse
models rt , the auxiliary variational lower bound (4) cannot
be calculated analytically. However, if we can at least sample from the transitions qt , and evaluate the inverse models
rt at those samples, we can still approximate the variational
lower bound without bias using the following algorithm:
Algorithm 1 MCMC lower bound estimate
Require: Model with joint distribution p(x, z) and a desired but intractable posterior p(z|x)
Require: Number of iterations T
Require: Transition operator(s) qt (zt |x, zt 1 )
Require: Inverse model(s) rt (zt 1 |x, zt )
Draw an initial random variable z0 ⇠ q(z0 |x)
Initialize the lower bound estimate as
L = log p(x, z0 ) log q(z0 |x)
for t = 1 : T do
Perform random transition zt ⇠ qt (zt |x, zt 1 )
p(x,zt )rt (zt 1 |x,zt )
Calculate the ratio ↵t = p(x,z
t 1 )qt (zt |x,zt 1 )
Update the lower bound L = L + log[↵t ]
end for
return the unbiased lower bound estimate L
The key insight behind the recent work in stochastic gradient variational inference is that if all the individual steps of
an algorithm like this are differentiable in the parameters of
q and r, which we denote by ✓, then so is the algorithm’s
output L. Since L is an unbiased estimate of the variational
lower bound, its derivative is then an unbiased estimate of
the derivative of the lower bound, which can be used in a
stochastic optimization algorithm.
Obtaining gradients of the Monte Carlo estimate of Algorithm 1 requires the application of the chain rule
through the random sampling of the transition operators
qt (zt |x, zt 1 ). This can in many cases be realized by drawing from these operators in two steps: In the first step
we draw a set of primitive random variables ut from a
fixed distribution p(ut ), and we then transform those as
zt = g✓ (ut , x) with a transformation g✓ () chosen in such
a way that zt follows the distribution qt (zt |x, zt 1 ). If this
is the case we can apply backpropagation, differentiating
through the sampling function to obtain unbiased stochastic estimates of the gradient of the lower bound objective
with respect to ✓ (Salimans & Knowles, 2013; Kingma &
Welling, 2014; Rezende et al., 2014). An alternative solution, which we do not consider here, would be to approximate the gradient of the lower bound using Monte Carlo
directly (Paisley et al., 2012; Ranganath et al., 2014; Mnih
& Gregor, 2014).
Once we have obtained a stochastic estimate of the gradi-

Markov Chain Monte Carlo and Variational Inference: Bridging the Gap

Algorithm 2 Markov Chain Variational Inference (MCVI)
Require: Forward Markov model q✓ (z) and backward
Markov model r✓ (z0 , . . . , zt 1 |zT )
Require: Parameters ✓
Require: Stochastic estimate L(✓) of the variational lower
bound Laux (✓) from Algorithm 1
while not converged do
Obtain unbiased stochastic estimate ĝ with Eq [ĝ] =
r✓ Laux (✓) by differentiating L(✓)
Update the parameters ✓ using gradient ĝ and a
stochastic optimization algorithm
end while
return final optimized variational parameters ✓
2.1. Example: bivariate Gaussian
As a first example we look at sampling from the bivariate
Gaussian distribution defined by

1
1
1 2
p(z , z ) / exp
(z 1 z 2 )2
(z 1 + z 2 )2 .
2
2 1
2 22
We consider two MCMC methods that update the univariate z 1 , z 2 in turn. The first method is Gibbs sampling,
which samples from the Gaussian full conditional distributions p(z i |z i ) = N (µi , i2 ). The second method is the
over-relaxation method of (Adler, 1981), which instead updates the univariate z i using q(zti |zt 1 ) = N [µi +↵(zti 1
µi ), i2 (1 ↵2 )]. For ↵ = 0 the two methods are equivalent, but for other values of ↵ the over-relaxation method
may mix more quickly than Gibbs sampling. To test this
we calculate the variational lower bound for this MCMC
algorithm, and maximize with respect to ↵ to find the most
effective transition operator.
For the inverse model r(zt 1 |zt ) we use Gaussians with
mean parameter linear in zt and variance independent of
zt 1 . For this particular case this specification allows
us to recover the q(zt 1 |zt ) distribution exactly. We use
1 = 1, 2 = 10 in our exact posterior, and we initialize the
Markov chain at ( 10, 10), with addition of infinitesimal
noise (variance of 10 10 ). Figure 1 shows the lower bound
for both MCMC methods: over-relaxation with an optimal
↵ of 0.76 clearly recovers the exact posterior much more
quickly than plain Gibbs sampling. The fact that optimization of the variational lower bound allows us to improve
upon standard methods like Gibbs sampling is promising
for more challenging applications.

Variational lower bound for 2 MCMC methods
4
3.5

variational lower bound

ent of (2) with respect to ✓, we can use this estimate in a
stochastic gradient-based optimization algorithm for fitting
our approximation to the true posterior p(z|x). We do this
using the following algorithm:

3
2.5
2
1.5
1

exact
overrelaxation
Gibbs sampling

0.5
0

1

2

3

4

5

6

7

8

9

10

nr of MCMC steps

Figure 1. The log marginal likelihood lower bound for a bivariate
Gaussian target and an MCMC variational approximation, using
Gibbs sampling or Adler’s overrelaxation.

3. Hamiltonian variational inference
One of the most efficient and widely applicable MCMC
methods is Hamiltonian Monte Carlo (HMC) (Neal, 2011).
HMC is an MCMC method for approximating continuous distributions p(z|x) where the space of unknown variables is expanded to include a set of auxiliary variables
v with the same dimension as z. These auxiliary variables are initialized with a random draw from a distribution vt0 ⇠ q(vt0 |x, zt 1 ), after which the method simulates
the dynamics corresponding to the Hamiltonian H(v, z) =
0.5v T M 1 v log p(x, z), where z and v are iteratively
updated using the leapfrog integrator, see (Neal, 2011).
Hamiltonian dynamics of this form is a very effective way
of exploring the posterior distribution p(z|x) because the
dynamics is guided by the gradient of the exact log posterior, and random walks are suppressed by the auxiliary
variables v, which are also called momentum variables.
Furthermore, the transition from vt0 , zt 1 to vt , zt in HMC
is deterministic, invertible and volume preserving, which
means that we have
q(vt , zt |zt
=

1 , x) = q(vt , zt , zt 1 |x)/q(zt 1 |x)
0
q(vt , zt 1 |x)/q(zt 1 |x) = q(vt0 |zt 1 , x)

and similarly r(vt0 , zt 1 |zt , x) = r(vt |zt , x), with zt , vt the
output of the Hamiltonian dynamics.
Using this choice of transition operator qt (vt , zt |zt 1 , x)
and inverse model rt (vt0 , zt 1 |zt , x) we obtain the following algorithm for stochastically approximating the log
marginal likelihood lower bound:

Markov Chain Monte Carlo and Variational Inference: Bridging the Gap

Algorithm 3 Hamiltonian variational inference (HVI)
Require: Unnormalized log posterior log p(x, z)
Require: Number of iterations T
Require: Momentum
initialization
distribution(s)
qt (vt0 |zt 1 , x) and inverse model(s) rt (vt |zt , x)
Require: HMC stepsize and mass matrix ✏, M
Draw an initial random variable z0 ⇠ q(z0 |x)
Init. lower bound L = log[p(x, z0 )] log[q(z0 |x)]
for t = 1 : T do
Draw initial momentum vt0 ⇠ qt (vt0 |x, zt 1 )
Set zt , vt = Hamiltonian Dynamics(zt 1 , vt0 )
p(x,zt )rt (vt |x,zt )
Calculate the ratio ↵t = p(x,z
0
t 1 )qt (vt |x,zt 1 )
Update the lower bound L = L + log[↵t ]
end for
return lower bound L, approx. posterior draw zT
Here we omit the Metropolis-Hastings step that is typically
used with Hamiltonian Monte Carlo. Section 4.1 discusses
how such as step could be integrated into Algorithm 3.
We fit the variational approximation to the true posterior
distribution by stochastically maximizing the lower bound
with respect to q,r and the parameters (stepsize and mass
matrix) of the Hamiltonian dynamics using Algorithm 2.
We call this version of the algorithm Hamiltonian Variational Inference (HVI). After running the algorithm to convergence, we then have an optimized approximation q(z|x)
of the posterior distribution. Because our approximation
automatically adapts to the local shape of the exact posterior, this approximation will often be better than a variational approximation with a fixed functional form, provided
our model for rt (vt |x, zt ) is flexible enough.

In addition to improving the quality of our approximation,
we find that adding HMC steps to a variational approximation often reduces the variance in our stochastic gradient estimates, thereby speeding up the optimization. The
downside of using this algorithm is that its computational
cost per iteration is higher than when using an approximate
q(z|x) of a fixed form, mainly owing to the need of calculating additional derivatives of log p(x, z). These derivatives may also be difficult to derive by hand, so it is advisable to use an automatic differentiation package such as
Theano (Bastien et al., 2012). As a rule of thumb, using
the Hamiltonian variational approximation with m MCMC
steps and k leapfrog steps is about mk times as expensive
per iteration as when using a fixed form approximation.
This may be offset by reducing the number of iterations,
and in practice we find that adding a single MCMC step
to a fixed-form approximation often speeds up the convergence of the lower bound optimization in wallclock time.
The scaling of the computational demands in the dimensionality of z is the same for both Hamiltonian variational
approximation and fixed form variational approximation,

and depends on the structure of p(x, z).
Compared to regular Hamiltonian Monte Carlo, Algorithm 3 has a number of advantages: The samples drawn
from q(z|x) are independent, the parameters of the Hamiltonian dynamics (M, ✏) are automatically tuned, and we
may choose to omit the Metropolis-Hastings step so as not
to reject any of the proposed transitions. Furthermore, we
optimize a lower bound on the log marginal likelihood, and
we can assess the approximation quality using the techniques discussed in (Salimans & Knowles, 2013). By finding a good initial distribution q(z0 ), we may also speed up
convergence to the true posterior and get a good posterior
approximation using only a very short Markov chain, rather
than relying on asymptotic theory.
3.1. Example: A beta-binomial model for
overdispersion
To demonstrate our Hamiltonian variational approximation
algorithm we use an example from (Albert, 2009), which
considers the problem of estimating the rates of death from
stomach cancer for the largest cities in Missouri. The data
is available from the R package LearnBayes. It consists of
20 pairs (nj , xj ) where nj contains the number of individuals that were at risk for cancer in city j, and xj is the number of cancer deaths that occurred in that city. The counts
xj are overdispersed compared to what one could expect
under a binomial model with constant probability, so (Albert, 2009) assumes a beta-binomial model with a two dimensional parameter vector z. The low dimensionality of
this problem allows us to easily visualize the results.
We use a variational approximation containing a single
HMC step so that we can easily integrate out the 2 momentum variables numerically for calculating the exact KLdivergence of our approximation and to visualize our results. We choose q✓ (z0 ), q✓ (v10 |z0 ), r✓ (v1 |z1 ) to all be multivariate Gaussian distributions with diagonal covariance
matrix. The mass matrix M is also diagonal. The means
of q✓ (v10 |z0 ) and r✓ (v1 |z1 ) are defined as linear functions
in z and rz log p(x, z), with adjustable coefficients. The
covariance matrices are not made to depend on z, and the
approximation is run using different numbers of leapfrog
steps in the Hamiltonian dynamics.
As can be seen from Figures 2 and 3, the Hamiltonian dynamics indeed helps us improve the posterior approximation. Most of the benefit is realized in the first two leapfrog
iterations. Of course, more iterations may still prove useful for different problems and different specifications of
q✓ (z0 ), q✓ (v10 |z0 ), r✓ (v1 |z1 ), and additional MCMC steps
may also help. Adjusting only the means of q✓ (v10 |z0 ) and
r✓ (v1 |z1 ) based on the gradient of the log posterior is a
simple specification that achieves good results. We find
that even simpler parameterizations still do quite well, by

Markov Chain Monte Carlo and Variational Inference: Bridging the Gap

finding a solution where the variance of q✓ (v10 |z0 ) is larger
than that of r✓ (v1 |z1 ), and the variance of q✓ (z0 ) is smaller
than that of p(v|z): The Hamiltonian dynamics then effectively transfers entropy from v to z, resulting in an improved lower bound.

10

10

10
5
−7.5 −7 −6.5 −6
logit m

10

10
5
−7.5 −7 −6.5 −6
logit m
exact

log K

5
−7.5 −7 −6.5 −6
logit m
7
log K

log K

5
−7.5 −7 −6.5 −6
logit m
6

10

10
5
−7.5 −7 −6.5 −6
logit m
5

log K

5
−7.5 −7 −6.5 −6
logit m
4
log K

log K

5
−7.5 −7 −6.5 −6
logit m
3

10

2
log K

1
log K

log K

0

5
−7.5 −7 −6.5 −6
logit m

10
5
−7.5 −7 −6.5 −6
logit m

Figure 2. Approximate posteriors for a varying number of
leapfrog steps. Exact posterior at bottom right.

Since we now have a dataset consisting of multiple datapoints xi , with separate latent variables zi per datapoint,
it is efficient to let the distribution q(z|x) be an explicit
function of the data xi , since in that case there is often no
necessity for ’local’ variational parameters ✓ per individual
datapoint xi ; instead, q maps from global parameters ✓ and
local observed value xi to a distribution over the local latent
variable(s) zi . We can then optimize over ✓ for all observations xi jointly. The joint lower bound to be optimized is
given by

i=1

log p(xi )

n
X
i=1

Eq✓ (zi |xi ) [log p(zi , xi ) log q✓ (zi |xi )],

of which an unbiased estimator (and its gradients) can be
constructed by sampling minibatches of data xi from the
empirical distribution and sampling zi from q✓ (zi |xi ).

0.95

0.9
R−squared

Our generative model p(xi , zi ) consists of a spherical
Gaussian prior p(zi ) = N (0, I), and conditional likelihood (or decoder) p✓ (xi |zi ) parameterized with either a
fully connected neural network as in (Kingma & Welling,
2014; Rezende et al., 2014), or a convolutional network as
in (Dosovitskiy et al., 2014). The network takes as input the
latent variables zi , and outputs the parameters of a conditionally independent (Bernoulli) distribution over the pixels.

n
X

1

0.85

0.8

0.75

0.7

handwritten digit. The task of modeling the distribution of
these handwritten digit images is often used as a comparative benchmark for probability density and mass modeling
approaches.

0

1

2

3
4
number of leapfrog steps

5

6

7

Figure 3. R-squared accuracy measure (Salimans & Knowles,
2013) for approximate posteriors using a varying number of
leapfrog steps.

3.2. Example: Generative model for handwritten digits
Next, we demonstrate the effectiveness of our Hamiltonian
variational inference approach for learning deep generative
neural network models. These models are fitted to a binarized version of the MNIST dataset as e.g. used in (Uria
et al., 2014). This dataset consists of 70000 data vectors
xi , each of which represents a black-and-white image of a

One flexible way of parameterizing the posterior approximation q✓ (zi |xi ) is by using an inference network as in
Helmholtz machines (Hinton & Zemel, 1994) or the related variational auto-encoders (VAE) (Kingma & Welling,
2014; Rezende et al., 2014). We can augment or replace
such inference networks with the MCMC variational approximations developed here, as the parameters ✓ of the
Markov chain can also be shared over all data vectors xi .
Specifically, we replace or augment inference networks as
used in (Kingma & Welling, 2014; Rezende et al., 2014)
with a Hamiltonian posterior approximation as described in
Algorithm 3, with T = 1 and a varying number of leapfrog
steps. The auxiliary inference model r(v|x, z) is chosen to
be a fully-connected neural network with one deterministic hidden layer with nh = 300 hidden units with softplus
(log(1 + exp(x))) activations and a Gaussian output variable with diagonal covariance. We tested two variants of
the distribution q(z0 |x). In one case, we let this distribution
be a Gaussian with a mean and diagonal covariance structure that are learned, but independent of the datapoint x.
In the second case, we let q(z0 |x) be an inference network
like r(v|x, z), with two layers of nh hidden units, softplus

Markov Chain Monte Carlo and Variational Inference: Bridging the Gap

activations and Gaussian output with diagonal covariance
structure.
In a third experiment, we replaced the fully-connected networks with convolutional networks in both the inference
model and the generative model. The inference model
consists of three convolutional layers with 5⇥5 filters,
[16,32,32] feature maps, stride of 2 and softplus activations. The convolutional layers are followed by a single
fully-connected layer with nh = 300 units and softplus
activations. The architecture of the generative model mirrors the inference model but with stride replaced by upsampling, similar to (Dosovitskiy et al., 2014). The number of
leapfrog steps was varied from 0 to 16. After broader model
search with a validation set, we trained a final model with
16 leapfrog steps and nh = 800.
Table 1. Comparison of our approach to other recent methods in
the literature. We compare the average marginal log-likelihood
measured in nats of the digits in the MNIST test set. See section 3.2 for details.

Model
HVI + fully-connected VAE:
Without inference network:
5 leapfrog steps
10 leapfrog steps
With inference network:
No leapfrog steps
1 leapfrog step
4 leapfrog steps
8 leapfrog steps
HVI + convolutional VAE:
No leapfrog steps
1 leapfrog step
2 leapfrog steps
4 leapfrog steps
8 leapfrog steps
16 leapfrog steps
16 leapfrog steps, nh = 800
From (Gregor et al., 2015):
DBN 2hl
EoNADE
DARN 1hl
DARN 12hl
DRAW

log p(x)


log p(x)
=

90.86
87.60

87.16
85.56

94.18
91.70
89.82
88.30

88.95
88.08
86.40
85.51

86.66
85.40
85.17
84.94
84.81
84.11
83.49

83.20
82.98
82.96
82.78
82.72
82.22
81.94

88.30
87.72
80.97

dation set of about 15% of the available training data. The
marginal likelihood of the test set was estimated with importance sampling by taking a Monte Carlo estimate of
the expectation p(x) = Eq(z|x) [p(x, z)/q(z|x)] (Rezende
et al., 2014) with over a thousand importance samples per
test-set datapoint.
See table 1 for our numerical results and a comparison to
reported results with other methods. Without an inference
network and with 10 leapfrog steps we were able to achieve
a mean test-set lower bound of 87.6, and an estimated
mean marginal likelihood of 85.56. When no Hamiltonian dynamics was included the gap is more than 5 nats;
the smaller difference of 2 nats when 10 leapfrog steps
were performed illustrates the bias-reduction effect of the
MCMC chain. Our best result is 81.94 nats with convolutional networks for inference and generation, and HVI with
16 leapfrog steps. This is slightly worse than the best reported number with DRAW (Gregor et al., 2015), a VAE
with recurrent neural networks for both inference and generation. Our approaches are not mutually exclusive, and
could indeed be combined for even better results.
The model can also be trained with a two-dimensional
latent space to obtain a low-dimensional visualization of
data. See figure 4 for a visualization of the latent space of
such a model trained on the MNIST digits.

84.55
85.10
84.13

Stochastic gradient-based optimization was performed using Adam (Kingma & Ba, 2014) with default hyperparameters. Before fitting our models to the full training
set, the model hyper-parameters and number of training
epochs were determined based on performance on a vali-

Figure 4. Visualization of the two-dimensional latent space of a
generative model trained with our proposed Hamiltonian variational posterior approximation; shown here are the mean images
p(x|z) corresponding to different points z in latent space. Our
proposed method results in better samples than what could be obtained when just using an inference network (without fine-tuning
by Hamiltonian dynamics) as in (Kingma & Welling, 2014).

Markov Chain Monte Carlo and Variational Inference: Bridging the Gap

4. Specification of the Markov chain
In addition to the core contributions presented above, we
now present a more detailed analysis of some possible
specifications of the Markov chain used in the variational
approximation. We discuss the impact of different specification choices on the theoretical and practical performance
of the algorithm.
4.1. Detailed balance
For practical MCMC inference we almost always use a
transition operator that satisfies detailed balance, i.e. a
transition operator qt (zt |x, zt 1 ) for which we have
p(x, zt ) q t (zt 1 |x, zt )
= 1,
p(x, zt 1 )qt (zt |x, zt 1 )

1 |x, zt )

log q t (zt

Finally, zt is set to zt0 with probability ⇢(zt 1 , zt0 ), and to
zt 1 with probability 1 ⇢(zt 1 , zt0 ). The density of the resulting stochastic transition operator qt (zt |x, zt 1 ) cannot
be calculated analytically since it involves an intractable
integral over ⇢(zt 1 , zt0 ). To incorporate a MetropolisHastings step into our variational objective we will thus
need to explicitly represent the acceptance decision as
an additional auxiliary binary random variable a. The
Metropolis-Hastings step can then be interpreted as taking
a reversible variable transformation with unit Jacobian:
zt

where q t (zt 1 |x, zt ) denotes qt (zt |x, zt 1 ) with its z arguments reversed (not q(zt 1 |x, zt ): the conditional pdf of
zt 1 given zt under q). If our transition operator satisfies
detailed balance, we can divide ↵t in Algorithm 1 by the
ratio above (i.e. 1) to give
log[↵t ] = log rt (zt

Next, the acceptance probability is calculated as

p(x, zt0 ) (zt 1 |zt0 )
⇢(zt 1 , zt0 ) = min
,1 .
p(x, zt 1 ) (zt0 |zt 1 )

1 |x, zt ).

By optimally choosing rt (zt 1 |x, zt ) in this expression, we
can make the expectation Eq log[↵t ] non-negative: what is
required is that rt () is a predictor of the reverse dynamics that is equal or better than q t (). If the iterate zt 1
has converged to the posterior distribution p(z|x) by running the Markov chain for a sufficient number of steps,
then it follows from detailed balance that q t (zt 1 |x, zt ) =
q(zt 1 |x, zt ). In that case choosing rt (zt 1 |x, zt ) =
q t (zt 1 |x, zt ) is optimal, and the lower bound is unaffected by the transition. If, on the other hand, the chain has
not fully mixed yet, then q t (zt 1 |x, zt ) 6= q(zt 1 |x, zt ):
the last iterate zt 1 will then have a predictable dependence on the initial conditions which allows us to choose
rt (zt 1 |x, zt ) in such a way that Eq log[↵t ] is positive
and improves our lower bound. Hence a stochastic transition respecting detailed balance always improves our variational posterior approximation unless it is already perfect!
In practice, we can only use this to improve our auxiliary lower bound if we also have an adequately powerful
model rt (zt 1 |x, zt ) that can be made sufficiently close to
q(zt 1 |x, zt ).
A practical transition operator that satisfies detailed balance is Gibbs sampling, which can be trivially integrated
into our framework as shown in Section 2.1. Another popular way of ensuring our transitions satisfy detailed balance is by correcting them using Metropolis-Hastings rejection. In the latter case, the stochastic transition operator
qt (zt |x, zt 1 ) is constructed in two steps: First a stochastic proposal zt0 is generated from a distribution (zt0 |zt 1 ).

1
zt0

! I[a = 1]zt0 + I[a = 0]zt
! I[a = 1]zt

1

1

+ I[a = 0]zt0

a ! a.

Evaluating our target density at the transformed variables,
we get the following addition to the lower bound:
log[↵t ] = log[p(x, zt )/p(x, zt

1 )]

+ log[rt (a|x, zt )]

+ I[a = 1] log[rt (zt
+

1 |x, zt )]
0
I[a = 0] log[rt (zt |x, zt )]
log[qt (zt0 |x, zt 1 )q(a|zt0 , zt 1 , x)].

Assuming we are working with a continuous variable z, the
addition of the binary variable a has the unfortunate effect
that our Monte Carlo estimator of the lower bound is no
longer a continuously differentiable function of the variational parameters ✓, which means we cannot use the gradient of the exact log posterior to form our gradient estimates.
Estimators that do not use this gradient are available (Salimans & Knowles, 2013; Paisley et al., 2012; Ranganath
et al., 2014; Mnih & Gregor, 2014) but these typically have
much higher variance. We can regain continuous differentiability with respect to ✓ by Rao-Blackwellizing our Monte
Carlo lower bound approximation L and calculating the expectation with respect to q(a|zt0 , zt 1 , x) analytically. For
short Markov chains this is indeed an attractive solution.
For longer chains this strategy becomes computationally
demanding as we need to do this for every step in the chain,
thereby exploring all 2T different paths created by the T
accept/reject decisions. Another good alternative is to simply omit the Metropolis-Hastings acceptance step from our
transition operators and to rely on a flexible specification
for q() and r() to sufficiently reduce any resulting bias.
4.2. Annealed variational inference
Annealed importance sampling is an MCMC strategy
where the Markov chain consists of stochastic transitions
qt (zt |zt 1 ) that each satisfy detailed balance with respect

Markov Chain Monte Carlo and Variational Inference: Bridging the Gap

to an unnormalized target distribution log[pt (z)] = (1
t ) log[q0 (z)] + t log[p(x, z)], for t gradually increasing from 0 to 1. The reverse model for annealed importance sampling is then constructed using transitions
r(zt 1 |zt ) = qt (zt |zt 1 )pt (zt 1 )/pt (zt ), which are guaranteed to be normalized densities because of detailed balance. For this choice of posterior approximation and reverse model, the marginal likelihood lower bound is then
given by
log p(x)

Eq

T
X

(

t 1 ) log[p(x, zt )/q0 (zt )].

t

t=1

With 0 = 0, T = 1 this looks like the bound we have at
t = 0, but notice that the expectation is now taken with respect to a different distribution than q0 . Since this new approximation is strictly closer to p(z|x) than the old approximation, its expectation of the log-ratio log[p(x, zt )/q0 (zt )]
is strictly higher, and the lower bound will thus be improved.
The main advantage of annealed variational inference over
other variational MCMC strategies is that it does not require explicit specification of the reverse model r, and that
the addition of the Markov transitions to our base approximation q0 (z) is guaranteed to improve the variational lower
bound. A downside of using this scheme for variational inference is the requirement that the transitions q(zt |zt 1 )
satisfy detailed balance, which can be impractical for optimizing q.
4.3. Using multiple iterates
So far we have defined our variational approximation as
the marginal of the last iterate in the Markov chain, i.e.
q(zT |x). This is wasteful if our Markov chain consists
of many steps, and practical MCMC algorithms therefore
always use multiple samples zT +1 K , . . . , zT from the
Markov chain, with K the number of samples. When using
multiple samples obtained at different points in the Markov
chain, our variational approximation effectively becomes a
discrete mixture over the marginals of the iterates that are
used:
q(z|x) =

=

1
K

T
X

t=T +1 K
T
X

t=T +1 K

q(zt |x)

4.4. Sequential MCVI
In Algorithm 2 we suggest optimizing the bound over all
MCMC steps jointly, which is expected to give the best results for a fixed number of MCMC steps. Another approach
is to optimize the MCMC steps sequentially, by maximizing the local bound contributions Eq log[↵t ]. Using this
approach, we can take any existing variational approximation and improve it by adding one or more MCMC steps,
as outlined in Algorithm 4. Improving an existing approximation in this way gives us an easier optimization problem,
and can be compared to how boosting algorithms are used
to iteratively fit regression models.
Algorithm 4 Sequential MCVI
Require: Unnormalized log posterior log p(x, z)
Require: Variational approximation q(z0 |x)
for t = 1 : T do
Add transition operator qt (zt |x, zt 1 ) and inverse
model rt (zt 1 |x, zt ).
Choose the new parameters by maximizing the local
lower bound contribution Eq(zt ,zt 1 ) log[↵t ]
Set the new
posterior approximation equal to
R
q(zt |x) = qt (zt |x, zt 1 )q(zt 1 |x)dzt 1
end for
return the final posterior approximation q(zT |x)

5. Conclusion

I(w = t)q(zt |x),

with w ⇠ Categorical(T + 1

probability on each of the K last iterates of the Markov
chain, the log of which is subtracted from our variational
lower bound (3). This term is then offset by adding the
corresponding log probability of that iterate under the inverse model r(w = t|x, z). The simplest specification for
the inverse model is to set it equal to q(w = t): In that
case both terms cancel, and we’re effectively just taking
the average of the last K lower bounds L computed by
Algorithm 1. Although suboptimal, we find this to be an
effective method of reducing variance when working with
longer Markov chains. An alternative, potentially more optimal approach would be to also specify the inverse model
for w using a flexible parametric function such as a neural
network, taking x and the sampled z as inputs.

K, . . . , T ).

To use this mixture distribution to form our lower bound,
we need to explicitly take into account the mixture indicator variable w. This variable has a categorical distribution q(w = t), t 2 [T + 1 K, . . . , T ] that puts equal

By using auxiliary variables in combination with stochastic gradient variational inference we can construct posterior
approximations that are much better than can be obtained
using only simpler exponential family forms. One way
of improving variational inference is by integrating one or
more MCMC steps into the approximation. By doing so
we can bridge the accuracy/speed gap between MCMC and
variational inference and get the best of both worlds.

Markov Chain Monte Carlo and Variational Inference: Bridging the Gap

References
Adler, Stephen L. Over-relaxation method for the monte
carlo evaluation of the partition function for multiquadratic actions. Physical Review D, 23(12):2901,
1981.
Albert, Jim. Bayesian Computation with R. Springer Science, New York. Second edition, 2009.
Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan,
Bergstra, James, Goodfellow, Ian, Bergeron, Arnaud,
Bouchard, Nicolas, Warde-Farley, David, and Bengio,
Yoshua. Theano: new features and speed improvements.
arXiv preprint arXiv:1211.5590, 2012.
Dosovitskiy, Alexey, Springenberg, Jost Tobias, and Brox,
Thomas. Learning to generate chairs with convolutional
neural networks. arXiv preprint arXiv:1411.5928, 2014.
Gregor, Karol, Danihelka, Ivo, Graves, Alex, and Wierstra,
Daan. Draw: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.
Hinton, Geoffrey E and Zemel, Richard S. Autoencoders,
minimum description length, and helmholtz free energy.
Advances in neural information processing systems, pp.
3–3, 1994.
Kingma, Diederik and Ba, Jimmy.
method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam: A
arXiv preprint

Kingma, Diederik P and Welling, Max. Auto-Encoding
Variational Bayes. Proceedings of the 2nd International
Conference on Learning Representations, 2014.
Mnih, Andriy and Gregor, Karol. Neural variational inference and learning in belief networks. In The 31st International Conference on Machine Learning (ICML),
2014.
Neal, Radford. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2, 2011.
Paisley, John, Blei, David, and Jordan, Michael. Variational
bayesian inference with stochastic search. In Proceedings of the 29th International Conference on Machine
Learning (ICML-12), pp. 1367–1374, 2012.
Ranganath, Rajesh, Gerrish, Sean, and Blei, David. Black
box variational inference. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, pp. 814–822, 2014.
Rezende, Danilo J, Mohamed, Shakir, and Wierstra, Daan.
Stochastic backpropagation and approximate inference
in deep generative models. In Proceedings of the 31st
International Conference on Machine Learning (ICML14), pp. 1278–1286, 2014.

Salimans, Tim and Knowles, David A. Fixed-form variational posterior approximation through stochastic linear
regression. Bayesian Analysis, 8(4):837–882, 2013.
Uria, Benigno, Murray, Iain, and Larochelle, Hugo. A
deep and tractable density estimator. In Proceedings of the 31th International Conference on Machine
Learning, ICML 2014, Beijing, China, 21-26 June
2014, pp. 467–475, 2014. URL http://jmlr.org/
proceedings/papers/v32/uria14.html.

