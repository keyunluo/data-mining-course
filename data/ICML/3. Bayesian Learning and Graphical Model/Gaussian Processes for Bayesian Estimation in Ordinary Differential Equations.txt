Gaussian Processes for Bayesian Estimation in Ordinary Differential Equations

Yali Wang
Department of Computer Science, Laval University, Canada

YALI . WANG .1@ ULAVAL . CA

David Barber
Department of Computer Science, University College London, U.K.

Abstract
Bayesian parameter estimation in coupled ordinary differential equations (ODEs) is challenging due to the high computational cost of numerical integration. In gradient matching a separate
data model is introduced with the property that
its gradient may be calculated easily. Parameter
estimation is then achieved by requiring consistency between the gradients computed from the
data model and those specified by the ODE. We
propose a Gaussian process model that directly
links state derivative information with system observations, simplifying previous approaches and
improving estimation accuracy.

1. Introduction
Ordinary differential equations (ODEs) are continuous
time models with the interaction between variables described by ẋ(t) = f (x(t), θ), for vector x and vector output
function f . The task is to estimate any unknown parameters
θ of the ODEs by fitting them to observed data collected at
a set of discrete observation times, t1 , . . . , tT . A principled
approach to this problem is to first numerically integrate the
ODEs for a given value of θ and initial value x(0) to obtain
a vector of values X ≡ xt1 , . . . , xtT . Parameter estimation
is achieved by finding θ such that X closely matches the
observed data. However, numerical integration is computationally demanding, rendering this otherwise ideal scheme
impractical in all but the smallest systems, see for example
(Vyshemirsky & Girolami, 2008).
In gradient matching we avoid explicit numerical integration by considering an alternative model of the data, x(t) =
g(t, φ). Given this fit to the data, one can compute the
gradients of the fitted function at the observed timepoints,
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

DAVID . BARBER @ UCL . AC . UK

ẋ(t) = ġ(t, φ). Gradient matching estimates parameters θ
of the ODE and parameters φ of the fitted function g by
requiring that the gradients in both models are consistent
at the observed timepoints. A review of this class of approaches can be found in (Ramsay et al., 2007).
As described in (Calderhead et al., 2008), previous gradient matching approaches provided only limited pointparameter estimates or can prove numerically inconsistent.
Recently, Gaussian Processes (GPs) have been considered
as data models within the gradient matching framework
(Calderhead et al., 2008; Dondelinger et al., 2013) and for
the solution of linear operator equations (Graepel, 2003).
GPs provide a distribution over fitted functions and associated gradients. Using priors on the parameters of the GP
model and the ODE, this gives a flexible Bayesian parameter estimation procedure. More concretely, in (Calderhead
et al., 2008) GP parameters φ are fitted first to the data, and
subsequently the parameters of the ODE θ are estimated.
The estimation accuracy is however limited by the lack of
feedback from ODE parameter inference to GP parameter
inference. To address this Dondelinger et al. (2013) introduced bidirectional interaction between ODE and GP parameters, demonstrating improved parameter estimation.
These GP approaches have similar computational complexity and can run up to two orders of magnitude faster than
numerical integration. The benefits of a Bayesian approach
to parameter estimation in ODEs are now well-established
and we propose to improve on previous approaches by introducing a simpler generative model that directly links
state derivatives to system observations using a GP. This
plays a similar role to numerical integration but without the
corresponding high computational cost.
1.1. ODE System Description
We consider continuous time dynamical systems in which
the motions of K states x(t) ≡ [x1 (t), x2 (t), . . . , xK (t)]T
are represented by a set of K ODEs
ẋ(t) ≡

d
x(t) = f (x(t), θ)
dt

Gaussian Processes for Ordinary Differential Equations
θ

ẋODE

θ

y

θ

x
y

ẋ

y

(a)

ẋGP

ẋODE

y

ẋGP

x

x

φ†

x0

x

θ

(b)

φ

φ

(c)

(d)

Figure 1. (a) Numerical Integration with an initial term x0 . (b) Our GP-ODE approach corresponds to a generative belief network. (c)
Calderhead et al. (2008) approach, which is based on a form of compatibility function, expressed as a chain graph. (d) The chain graph
of the Dondelinger et al. (2013) approach uses a modified compatibility function. Note that the difference between (c) and (d) is that in
(d) the links x − ẋODE and φ − x are undirected, reflecting the different normalisation requirement.

where θ is a vector of parameters of the ODE. For notational convenience, we additionally define the state matrix X ≡ [x(t1 ), x(t2 ), . . . , x(tT )] and k-th state sequence
xk ≡ [xk (t1 ), xk (t2 ), . . . , xk (tT )]T . Given potentially
noisy observations of X (see below), the task is to infer
a posterior distribution over the parameters θ.
1.2. Observation Model
The T observations Y = [y(t1 ), y(t2 ), . . . , y(tT )] are obtained from the states according to independent additive
noise y(t) = x(t) + (t) where the noise for the k-th state,
k ∈ {1, 2, . . . , K}, is Gaussian, k (t) ∼ N (0, σk2 ). This
gives then an observation model
Y
pOBS (Y|X) =
pOBS (y(t)|x(t))
t

cal value x0 for the integrated path is given by x0n+1 =
x0n + δf (x0n , θ), with x00 = x0 . This is iterated until the
desired end time. This can be considered as a procedure
that, for a given initial value, produces (in this case a deterministic) distribution p(x|x0 , θ) = δ (x − x0 (x0 )) over the
values of the state at the observation times. Here δ (·) is
the Dirac delta function. Given then a prior on θ and the
integration constant x0 , this defines a joint distribution
p(y, x, x0 , θ) = pOBS (y|x)p(x|x0 , θ)p(x0 )p(θ)
from which samples p(θ, x0 |y) can be drawn. This ideal
procedure can produce excellent results (Vyshemirsky &
Girolami, 2008); however the computational expense is
prohibitive in larger models with the bottleneck being the
explicit numerical integration that needs to be carried out
for every value of θ, x0 of interest (Calderhead et al., 2008).

with pOBS (y(t)|x(t)) = N (x(t), σ 2 I).
If unknown, the parameters of the observation model (σ in
this case) form part of the parameters that need to be estimated. This is achieved by placing a prior over their values and incorporating these parameters into the model in
the standard way. This step is unproblematic and, to avoid
notational clutter, we drop these observation parameters as
variables in the model descriptions below (they will however be included in the experiments).
1.3. Bayesian Numerical Integration
Given the ODE and an assumed initial value x0 , we can
then (in principle) numerically integrate the system. For
example1 , for K = 1, using a simple approach based
on discretising time in small intervals of δ, a numeri1

In practice we use the Runge-Kutta method.

2. The GP-ODE generative model
As an alternative to explicit Bayesian numerical integration, we propose the following generative model over states
X, their derivatives Ẋ, observations Y and remaining parameters using a simple belief network, fig(1b),
p(Y,X, Ẋ, φ† , θ) = p(θ)p(φ† )
× pGP (Y|Ẋ, φ† )pODE (Ẋ|X, θ)pGP (X|φ† )

(1)

where φ† ≡ (x0 , φ). To generate data from this model we
first sample parameters φ† , θ from their priors and then a
state X from the GP prior pGP (X|φ† ). A state derivative is
subsequently obtained by sampling from pODE (Ẋ|X, θ).
Finally, given these state derivatives Ẋ, observations Y
are generated by sampling from the GP pGP (Y|Ẋ, φ† ). In
this way we combine a smoothness prior assumption on the

Gaussian Processes for Ordinary Differential Equations

state X together with derivative information obtained from
the ODE in a single generative model2 .

and mean functions

E NCODING THE ODE: pODE (Ẋ|X, θ)

Given the state derivatives, the observations are then Gaussian distributed4

The temporal evolution of the ODE is encoded in the
distribution pODE (Ẋ|X, θ). In the deterministic ODE
case (which we assume throughout) this will simply
be
Q a delta function distribution δ(Ẋ − f (X, θ)) ≡
t δ (ẋ(t) − f (x(t), θ)), though Gaussian additive noise
would be straightforward to incorporate for the case of
Gaussian SDEs.

The GP prior assumes that each Q
state dimension is a
priori independent pGP (X|φ† ) = k pGP (xk |φ†k ), with
pGP (xk |φ†k ) formed from a GP with mean function3 µφk (t)
and covariance function cφk (t, t0 ).
I MPLICIT I NTEGRATION : pGP (Y|Ẋ, φ† )
Q
The term pGP (Y|Ẋ) = k pGP (yk |ẋk ) (dropping parameter dependencies on the r.h.s for compactness of notation)
Z
pGP (yk |ẋk ) = pOBS (yk |xk )pGP (xk |ẋk )dxk
plays a key role in our model and specifies how to implicitly
integrate a given state derivative curve to arrive at a distribution over observations. Since differentiation is a linear
operation, the derivative of a GP is also a GP – see for
example (Solak et al., 2002). Hence the joint distribution
pGP (yk , xk , ẋk ) is Gaussian distributed. Using the prior
pGP (X|φ† ) and observation model pOBS (Y|X) we obtain
the covariance functions
∂ 2 cφk (t, t0 ) ∂µφk (t) ∂µφk (t0 )
−
∂t∂t0
∂t
∂t0
0
∂cφk (t, t )
∂µφk (t)
cov(ẋk (t), xk (t0 )) =
− µφk (t0 )
∂t
∂t
cov(yk (t), ẋk (t0 )) = cov(xk (t), ẋk (t0 ))

cov(ẋk (t), ẋk (t0 )) =

0

2

0

cov(yk (t), yk (t )) = cφk (t, t ) + σ δ(t − t )
2

y|ẋ

y|ẋ

pGP (yk |ẋk ) ∼ N (µk , Σk )
where
y|ẋ

= µφk + Cxφẋk (Cẋφẋk )−1 ẋk − x̄˙ k

y|ẋ

= Cφk + σk2 I − Cxφẋk (Cẋφẋk )−1 Cẋx
φk

µk
Σk



xẋ
Cẋφẋk , Cẋx
φk and Cφk are constructed using the results above
evaluated at the observation times t1 , t2 , . . . , tT .

P RIOR ON LATENT STATE : pGP (X|φ† )

0

ȳk (t) = x̄k (t) = µφk (t), x̄˙ k (t) = ∂µφk (t)/∂t

It is natural to consider forming the joint p(y, x, ẋ)
as pOBS (y|x)pODE (ẋ|x)pGP (x).
However, the marginal
p(y, x) = pOBS (y|x)pGP (x) is then vacuous, containing no
contribution from the ODE. All models, including fig(1b,c,d), are
‘incorrect’ compared to the true model fig(1a); the challenge is
to combine aspects of numerical integration with the GP and observation model that achieves coherent parameter estimation with
reduced computational cost over explicit numerical integration.
3
There are different ways to define p(x, x0 ). One approach
is to express this as pGP (x|x0 )p(x0 ), which allows one to use
the same prior p(x0 ) as for the BNI model, section(1.3). In the
experiments we more simply defined a joint GP pGP (x, x0 ), for
each k, with mean µφk (t) equal to the mean of the observed data,
for all t. This is equivalent to defining a Gaussian prior on x0
with mean that of the observed data.

2.1. Parameter Estimation
From (1), the conditional marginal distribution over observations, latent states and parameters is given by
p(Y, X|φ† , θ)
= pGP (X|φ† )

Z

pGP (Y|Ẋ, φ† )pODE (Ẋ|X, θ)dẊ

This integral can be analytically evaluated in the case of
Gaussian additive noise in the ODE. In the deterministic
case, this reduces to simply
p(Y, X|φ† , θ) = pGP (X|φ† )pGP (Y|Ẋ = f (X, θ), φ† )
The distribution over observations, latent states and parameters is then given by
p(Y, X, φ† , θ) = p(Y, X|φ† , θ)p(φ† )p(θ)

(2)

Estimation of the parameters and latent state X can then
be carried out for example by sampling from the posterior p(X, φ† , θ|Y), see section(3). Note that, in contrast
to (Calderhead et al., 2008; Dondelinger et al., 2013), the
normalisation constant of the joint distribution (2) is known
which facilitates sampling.
Previous approaches (Calderhead et al., 2008; Dondelinger
et al., 2013) used a GP to compute p(X, Ẋ|Y); parameter estimation is achieved by requiring gradients from this
to match the desired gradient f (X, θ). The key difference
between this and our approach is our direct link from the
latent gradient Ẋ to the observation
Y. This term can
R
be expressed as p(Y|Ẋ) = p(Y|X)p(X|Ẋ)dX where
p(X|Ẋ) implicitly performs numerical integration, as we
describe below. Since Ẋ, X and Y are defined only at the
measurement times, no fine time discretization is required
in our model.
4

For a Gaussian defined on joint variables z = (x, y) with
p(z) = N (µz , Σz,z ), the conditional is Gaussian with mean and
covariance given from the block
p(x|y)
=
 mean and covariances,

−1
N µx + Σx,y Σ−1
see
y,y y − µy , Σx,x − Σx,y Σy,y Σy,x ,
e.g. (Barber, 2012).

Gaussian Processes for Ordinary Differential Equations
5

4

4
3
3
2

2

1
1

0
−1

0
−2
−3

−1

−4
10

20

30

40

50

60

70

80

90

100

(a)

10

20

30

40

50

60

70

80

90

100

(b)

Figure 2. (a) The dotted curve is the true (but unknown) derivative curve ẋ, which is only observed every 10th timepoint, giving observation ẋM at these measurement times. From this we calculate the GP posterior pGP (x|ẋM ) (assuming x0 = 0). (b) Samples from
pGP (x|ẋM ). The derivative of each sample is plotted in (a). The GP constrains the samples from pGP (x|ẋM ) such that their derivatives
match the observed derivatives ẋM . The model in section(2) describes the distribution only on the marginal quantities pGP (xM |ẋM )
and thus avoids working with the fine time discretization required in explicit numerical integration.

2.2. Informal Justification
To minimise notational issues, we consider a univariate
system with K = 1. Furthermore we discretize time so
that t = nδ, for integer time-index n ∈ {1, . . . , N } and
real discretization interval δ. Note that in the model in
section(2) the timepoints are defined only at the observation times; however in this section we need to notationally
distinguish between times that the data are observed and
a finer discretisation of time that could be used to carry
out numerical integration. The times at which data will
be observed are therefore described by a subset m ∈ M
of the fine time discretization; for example we measure
ym at time indices M = {1, 10, 20, . . .}. To emphasise
that the measurements only occur at a subset of all discrete times, we write yM for the observed measurements.
Given a curve xn , the numerical derivative is given by
ẋn = (xn − xn−1 )/δ. We assume that ẋ1 = x1 − x0 ,
where x0 is the constant of integration. For a vector x, the
derivative vector is then given by the difference equation
ẋ = Dx − b, where the square invertible matrix D has
zero elements, except for Dn,n−1 = −1/δ, Dn,n = 1/δ,
D1,1 = 1 and b is the zero vector except for b1 = x0 . To
explain the fundamental mechanism, we fix the parameters
of the GP and observation model. The posterior over the
discretized state is
p(yM , x, ẋ) = pGP (yM |ẋ)pODE (ẋ|x)pGP (x)

(3)

Writing x0 for the numerically integrated curve, we have
Z
pGP (yM |ẋ) = pOBS (yM |x0M )pGP (x0 |ẋ)dx0
Assuming that the derivative curve is obtained by differencing, we can invert this relation using Bayes’ rule
pGP (x0 |ẋ) ∝ δ(ẋ − Dx0 + b)pGP (x0 )
Since D is invertible, the GP plays no role, to give
pGP (x0 |ẋ) ∝ δ(ẋ − Dx0 + b)

Using pODE (ẋ|x) = δ(ẋ − f (x, θ)), and integrating (3) over ẋ, we obtain the joint distribution over
the observations yM and latent curve x, p(yM , x) =
p(yM |x)pGP (x), where
p(yM |x) ∝
Z
pOBS (yM |x0M )δ(ẋ − Dx0 + b)δ(ẋ − f (x, θ))dx0 dẋ


= pOBS (yM |x0M = D−1 (f (x, θ) + b)) M )
This can be interpreted as taking a set of gradients f (x, θ),
integrating them numerically (via the inversion D−1 which
performs summation of the components of f ) and taking
the measurement indices ofR this vector. The likelihood of
the observations p(yM ) = p(yM |x)pGP (x)dx is equivalent to the mass of GP curves x whose numerical derivatives match f (x, θ) weighted by how well they fit the observed data yM at the observation times M. Taking the
limit δ → 0, the above difference equation becomes the differential equation (1.1) and the Gaussian over x becomes a
GP, with ẋ the associated GP derivative, as specified by the
model (1), see figure(2).

3. Inference in the GP-ODE model
There are a number of approaches one could take to draw
samples from the GP-ODE posterior p(X, φ† , θ|Y) and
our philosophy was to choose the simplest that provides
good results. Writing Φ = {x0 , φ, σ} for all the parameters of the GP and observation model, we sample from
p(X, φ† , θ|Y) using a Gibbs procedure to produce a set
of samples Φi , θi , Xi . We initialize Φ0 , θ0 at random and
draw X0 ∼ pGP (X|Y, Φ0 ). We subsequently draw samples, indexed by i = 1 : L by alternately drawing from
1. θi , Φi ∼ p(θ, Φ|Xi−1 , Y)
2. Xi ∼ p(X|θi , Φi , Y)

Gaussian Processes for Ordinary Differential Equations

We present a naive approach for drawing from these conditionals below5 .
3.1. Parameter sampling
We draw from p(θi , Φi |Xi−1 , Y) using Gibbs sampling:
1. Set θi,0 = θi−1 ,Φi,0 = Φi−1
2. For j = 1 : Lp
(a) Φi,j ∼ p(Φ|Xi−1 , θi,j−1 , Y)
(b) θi,j ∼ p(θ|Xi−1 , Φi,j , Y)
3. Set θi = θi,Lp , Φi = Φi,Lp

using the marginal compatibility
Z
0
ω (x|θ, φ) ≡ ω(ẋ, x|θ, φ)dẋ
with presumably the intention that this has high value when
the gradient distributions overlap7 . The marginal compatibility ω 0 (x|θ, φ) is analytically computed since the terms
under the integral are Gaussian. The authors modify the deterministic ODE by the addition of fictitious noise to give a
Gaussian distribution for pODE (ẋ|x, θ) with mean f (x, θ).
To ease comparison with our approach (the extension to the
stochastic ODE case is trivial), we take the deterministic
ODE case, for which the above reduces to
p(θ|x, φ) ∝ p(θ)pGP (ẋ = f (x, θ)|x, φ)

where these conditional distributions can be obtained from
the joint (2). Where there are multiple components of a parameter, we again use Gibbs sampling to obtain a univariate
sample of a component conditioned on the remaining components. In the experiments we assume that the parameters
take values from known discrete sets (the priors are discrete), in which case sampling from these conditionals is
particularly straightforward.
3.2. State sampling
It is natural to consider drawing samples from
p(X|θ, Φ, Y) using Metropolis-Hastings (similar to
(Dondelinger et al., 2013)) with pGP (X|Φ, Y) as the
proposal. However, in our experience, this results in
poor mixing. We therefore use Gibbs sampling in which
we draw a state from p(x(t)|X\t , θ, Φ, Y), where X\t
are the states except for x(t), drawing samples in sequence from times t ∈ {t1 , . . . , tT }. To draw from
p(x(t)|X\t , θ, Φ, Y) we use either Metropolis-Hastings
with proposal pGP (x(t)|X\t , Φ, Y) or Gibbs sampling
for each component of the vector x(t) based on discrete
values6 . After Lx sweeps through all timepoints, we obtain
the new sample Xi .

4. Relation to previous approaches
4.1. Gradient Matching
(Calderhead et al., 2008) is based on matching gradients
via what could be termed a ‘compatability’ function (for
the case K = 1 and fixed σ for notational simplicity)
ω(ẋ, x|θ, φ) ≡ pGP (ẋ|x, φ)pODE (ẋ|x, θ)
This is used to define
p(θ|x, φ) ∝ p(θ)ω 0 (x|θ, φ)
5
More sophisticated sampling strategies could be considered.
However for the benchmark experiments, these approaches have
proved adequate.
6
For the experiments, the Gibbs approach proved adequate.

(4)

The joint distribution over observations, latent states and
parameters is then defined as
p(y, x, θ, φ) = pOBS (y|x)p(θ|x, φ)pGP (x|φ)p(φ) (5)
Inference is then achieved by sampling, conditioned on the
observed sequence y. The unknown normalisation term of
(4) is a function of φ and thus makes direct Gibbs sampling from this posterior problematic. The approach taken
in (Calderhead et al., 2008) is to first refactor the joint distribution in the form8
p(y, x, θ, φ) = p(θ|x, φ)p(x|φ, y)p(φ|y)p(y)
Conditioned on y, ancestral sampling is then performed:
φ ∼ p(φ|y)
x ∼ p(x|φ, y) ∝ pOBS (y|x)pGP (x|φ)
θ ∼ p(θ|x, φ)
R
Here, p(φ|y) ∝ p(φ) pOBS (y|x)pGP (x|φ)dx for which
the integral can be evaluated analytically. A disadvantage
of this model is that the posterior p(φ|y) does not take the
ODE system dynamics into consideration. Effectively, a
GP is fitted to the data first (without knowledge of the system dynamics) and the parameters θ of the ODE are subsequently adjusted to best match the fitted GP.
The gradient matching approach can be defined as a graphical model chain graph (see for example (Koller & Friedman, 2009)) distribution9 , fig(1c), with factors
pOBS (y|x)pODE (ẋODE |x, θ)pGP (ẋGP |x, φ)
× δ (ẋODE − ẋGP ) pGP (x)p(θ)p(φ)
7
The mathematical motivationRfor this is less clear. Given distributions p and q, their ‘overlap’ p(x)q(x)dx is maximal when
q(x) is a delta distribution placing all its mass on the most likely
state of p(x); this is not necessarily the same as matching q to p.
8
Whilst this can be interpreted as generative model, this is unnatural since the term p(θ|x, φ) means that the parameters of the
ODE depend on the generated state x.
9
This chain graph structure is the same for the trivial extension
to the stochastic ODE case.

Gaussian Processes for Ordinary Differential Equations

The undirected link between θ and ẋODE is necessary to
ensure that the variables ẋODE , ẋGP , θ form a component
of the chain graph. Marginalising this chain distribution
over ẋGP and ẋODE gives the marginal distribution
pOBS (y|x)pGP (x|φ)p(φ) R

p(θ)ω 0 (x|θ, φ)
p(θ)ω 0 (x|θ, φ)dθ

(6)

which matches (5). We can also write this as
pOBS (y|x)pGP (x|φ)p(φ)p(θ)mGM (x|θ, φ)

(7)

where we define the gradient matching function
mGM (x|θ, φ) ≡ R

ω 0 (x|θ, φ)
p(θ)ω 0 (x|θ, φ)dθ

(8)

4.2. Adaptive Gradient Matching
Dondelinger et al. (2013) considered a modified gradient
matching approach with joint distribution
p(y, x, ẋ, θ, φ) ∝ pOBS (y|x)pGP (x|φ)ω(ẋ, x|θ, φ)p(θ)p(φ)
and marginal
p(y, x, θ, φ) ∝ pOBS (y|x)pGP (x|φ)ω 0 (x|θ, φ)p(θ)p(φ)
A benefit of this approach is that the marginal
Z
Z
p(y|φ) ∝ pOBS (y|x)pGP (x|φ) ω 0 (x|θ, φ)p(θ)dθdx
does depend on the ODE; in contrast to (Calderhead et al.,
2008) the parameters of the GP are influenced by the ODE
(and vice versa). The marginal p(y, x, θ, φ) can also be
written in the same form as expression (7) but with the
adaptive gradient matching function
mAGM (x|θ, φ) ≡ R

ω 0 (x|θ, φ)
p(θ)p(φ)pGP (x|φ)ω 0 (x|θ, φ)dθdxdφ

The improved performance of AGM over GM (Dondelinger et al., 2013) may be attributed to the fact that
mAGM is proportional to the marginal compatibility ω 0 and
therefore always encourages matching between the GP and
the ODE, whereas mGM less strongly encourages matching due to the partial cancellation of ω 0 in both the numerator and denominator of (8). No such issues arise in the
GP-ODE approach in which the coupling between the GP
and ODE parameters occurs through the implicit numerical
integration mechanism, as described in section(2.2), which
ensures agreement between the ODE and GP curves.
The factors in the corresponding chain graph, fig(1d), are
the same as for the gradient matching method of section(4.1). However, all variables except y form a component in the chain graph, giving the correct form for the
marginal distribution on p(y, x, θ, φ). As for the gradient
matching method, this has no natural interpretation as a
generative model of the data.

5. Experiments
We illustrate our framework on two benchmark systems,
Lotka-Volterra and Signal Transduction Cascade in (Dondelinger et al., 2013). To aid comparison, wherever possible, we have chosen the same parameter settings and priors
as the original authors. Our main interest is to study the
implications of the different joint distributions specified by
the competing approaches. As such we wish to make as
similar as possible the sampling approaches for the three
competing models in order to minimize differences due to
different sampling strategies. To facilitate comparison we
therefore used the same discretized sampling strategy for
all methods. For the AGM and our GP-ODE approach we
used Gibbs sampling for a discretized set of values, analogous to section(3.1) and the Gibbs scheme of section(3.2)
for state samples. The cost of drawing a single sample in
all competing approaches is similar, scaling O(KT 3 ). We
stopped each sampling scheme (all written in Matlab) after
a similar CPU time. Following (Dondelinger et al., 2013),
we set p(θ) to a Gamma prior Ga(4, 0.5), p(φ) to a uniform
prior U (0, 100) and p(σ) to a Gamma prior Ga(1, 1). For
the sampling process, the standard deviations of the observation noise σ in both models are initialized as the ground
truth. For comparison we ran the Bayesian Numerical Integration approach using the same discretized parameter values wherever possible.
5.1. Lotka-Volterra
The Lotka-Volterra model is an ecological system that is
used to describe the periodical interaction between a prey
species [S] and a predator species [W ]:
d[S]
= [S](α − β[W ]),
dt

d[W ]
= −[W ](γ − δ[S])
dt

where θ = [α, β, γ, δ]T and x(t) = [[S], [W ]]. The ground
truth data are generated using numerical integration over
the interval [0, 2] with θ = [2, 1, 4, 1] and initial state values [S] = 5, [W ] = 3. The clean data are then sampled
with the sampling interval 0.2. Finally clean data are corrupted with additive Gaussian noise N (0, 0.52 I) to form
the observations Y. We chose the squared-exponential covariance function cφk (t, t0 ) = σkx exp(−lk (t − t0 )2 ) where
φk = [σkx , lk ]. Assuming a common parameter across
observation dimensions, the parameter vector φ is simplified to [σ x , l]; we initialize it as [1, 10]. The parameters
are initialized as θ = [1.5, 0.5, 3.5, 0.5]. We discretized
the ODE parameters α, β, γ, δ over [1.5, 2.5], [0.5, 1.5],
[3.5, 4.5], [0.5, 1.5] all with the interval 0.1; the parameter σ x is discretized over the range [0.1,1] with interval
0.1; the lengthscale l is discretized over [5, 50] with interval 5; the standard deviation of the noise σ was discretized
over [0.1, 1] with interval 0.1. The parameter x0 was, for
each state dimension k, discretized in the range [1, 10] us-

Gaussian Processes for Ordinary Differential Equations
7

7

7

7

6

6

6

6

5

5

5

5

4

4

4

4

3

3

3

3

2

2

2

2

1

1

1

0

0
0

0.5

1

1.5

2

1

0
0

0.5

1

1.5

2

0
0

0.5

1

1.5

2

4

4

4

4

3

3

3

3

2

2

2

2

1

1

1

1

0

0
0

0.5

1

1.5

2

0
0

0.5

1

1.5

2

0

0.5

1

1.5

2

0

0.5

1

1.5

2

0
0

0.5

1

1.5

2

Figure 3. Bayesian Inference for Lotka-Volterra. The results for prey and predator are respectively shown in the first and second row. In
all the plots, observations are red stars and the ground truth is the blue curve. Plotted in green are the reconstructions using the posterior
samples of θ. To aid comparison between the approaches we numerically integrated the ODE starting from the true initial state [5, 3] with
each curve generated by numerical integration using a parameter sample θ. The green plots are the mean and one standard deviation of
these resulting curves. First column: Bayesian Numerical Integration. This represents the solution that we wish to approximate. Second
column: Our GP-ODE method. Third column: The Adaptive Gradient Matching method. Fourth column: The Gradient Matching
method. All competing methods were run for approximately 300s of CPU time.

ing 20 uniformly spaced bins. After drawing ODE parameters θ from the posterior (see table(5.1)), we plot the numerically integrated curves (setting x0 to the true value to
aid visual comparison), see figure(3). The ‘best’ method
is that which most closely approximates the Bayesian Numerical Integration method of section(1.3). For small noise
levels (not shown), all three competing methods produce
similar results; however as the noise increases, the Adaptive Gradient Matching and Gradient Matching approaches
diverge markedly from the Bayesian Numerical Integration
approach, whilst the GP-ODE approach fairs well.
5.2. Signal Transduction Cascade
The Signal Transduction Cascade model is described by a
5-dimensional ODE system
d[S]
dt
d[Sd ]
dt
d[R]
dt
d[RS]
dt
d[Rpp]
dt

= −k1 [S] − k2 [S][R] + k3 [RS]
= k1 [S]
= −k2 [S][R] + k3 [RS] +

V [Rpp]
Km + [Rpp]

= k2 [S][R] − k3 [RS] − k4 [RS]
= k4 [RS] −

V [Rpp]
Km + [Rpp]

where θ = [k1 , k2 , k3 , k4 , V, Km] and x(t) =
[[S], [Sd ], [R], [RS], [Rpp]]T .
The ground truth data
are generated over the interval [0, 100] with θ =
[0.07, 0.6, 0.05, 0.3, 0.017, 0.3] and initial state [S] = 1,
[Sd ] = 0, [R] = 1, [RS] = 0, [Rpp] = 0. Then the data

are sampled at t= [0, 1, 2, 4, 5, 7, 10, 15, 20, 30, 40, 50,
60, 80, 100]. Finally the drawn samples are corrupted with
additive Gaussian noise N (0, 0.12 I) to construct the noisy
observations. The non-stationarity is captured by the covariance function10
!
0
a
+
b
tt
k
k
cφk(t, t0 ) = σkx arcsin p
(ak + bk t2 + 1)(ak + bk t02 + 1)
where φk = [σkx , ak , bk ]. The ODE parameters are initialized as θ = [0.05, 0.4, 0.03, 0.1, 0.015, 0.1]. We discretized the ODE parameters k1 , k2 , k3 , k4 , V , Km over
[0.05, 0.09], [0.4, 0.8], [0.03, 0.07], [0.1, 0.5], [0.015,
0.019], [0.1, 0.5] with the respective intervals 0.01, 0.1,
0.01, 0.1, 0.001, 0.1; the parameters σ x , a, b over [0.1,0.9],
[0.5, 2.5], [0.5, 2.5] with the respective intervals 0.2, 0.5,
0.5; the standard deviations of the noise σ over [0.06, 0.14]
with the interval 0.02. The 5 components of x0 were discretized in the intervals [0.5 1.5], [-0.1 0.1], [0.5 1.5], [-0.1
0.1], [-0.1 0.1] using 50 uniformly spaced bins. All three
competing approaches were run for approximately 30mins
CPU time. All three methods produce reasonable solutions
and the reconstructions using numerical integration with
parameters θ sampled from the respective posteriors are
similar. As such we show only the ideal Bayesian Numerical Integration procedure and our GP-ODE method in figure(4). In table(5.1) the GP-ODE method closely matches
the Bayesian Numerical Integration approach, with the
Gradient Matching and Adaptive Gradient Matching approaches producing broadly similar parameter estimates.
10

In contrast to the Lotka Volterra model, here we use a GP
with separate hyperparameters for each state dimension due to
the different length scales in each dimension.

Gaussian Processes for Ordinary Differential Equations
1.2
1

0.25

0.8

0.2

0.6

0.15

0.4

0.1

0.2

0.05

0

0

−0.2
0

0.3

0.8

0.2

0.6

0.1

40

60

80

100

−0.05
0

0.4

0.2

0

0.4

20

0.6

0.4

1

−0.1

0.2
20

40

60

80

0

100

20

40

60

80

100

−0.2
0

0
20

40

60

80

100

0

20

40

60

80

100

0

20

40

60

80

100

1.2
1

0.25

0.8

0.2

0.6

0.15

0.4

0.1

0.2

0.05

0

0

−0.2
0

0.3
0.2

0.6

0.1

40

60

80

100

−0.05
0

0.4

0.2

0

0.4

20

0.6

0.4

1
0.8

−0.1

0.2
20

40

60

80

100

0

20

40

60

80

100

−0.2
0

0
20

40

60

80

100

Figure 4. Bayesian Numerical Integration (top row) and GP-ODE results (bottom row) for the Signal Transduction Cascade. The results
for [R], [Sd ], [R], [RS], [Rpp] (from left to right). In all the plots, observations are red stars and the ground truth is the blue curve. The
green curves show reconstructions using sampled parameters θ from each posterior. Each curve is obtained by numerical integration
using the sampled parameter, starting from the same initial point x0 = [1, 0, 1, 0, 0].

PARAMETER

T RUE VALUE

N UM -BAYES

GP-ODE

AGM

GM

α
β
γ
δ

2
1
4
1

2.2680±0.1853
1.2070±0.1249
3.8330±0.2640
0.9850±0.0857

2.2380±0.1953
1.1490±0.1910
3.9590±0.3002
0.9860±0.0995

1.9480±0.2819
1.1750±0.1909
3.8390±0.2947
1.2320±0.1638

1.6429±0.2488
0.9242±0.2256
3.6449±0.2223
1.1737±0.1803

k1
k2
k3
k4
V
Km

0.07
0.6
0.05
0.3
0.017
0.3

0.0683±0.0122
0.6800±0.0876
0.0548±0.0125
0.2290±0.0498
0.0177±0.0012
0.3860±0.0792

0.0747±0.0130
0.6230±0.1246
0.0530±0.0135
0.2960±0.0281
0.0177±0.0014
0.4220±0.0690

0.0771±0.0130
0.5460±0.1259
0.0593±0.0111
0.3750±0.0999
0.0172±0.0015
0.4090±0.0911

0.0762±0.0130
0.5632±0.1256
0.0594±0.0115
0.3754±0.1051
0.0173±0.0014
0.4186±0.0953

Table 1. ODE Parameter Estimation for both the Lotka Volterra (upper) and Signal Transduction Cascade (lower) ODEs. In each we
plot the mean and standard deviation of the θ parameter samples from the respective competing approaches, namely the ‘ideal’ Bayesian
Numerical Integration procedure, our GP-ODE approach, the Adaptive Gradient Matching approach (Dondelinger et al., 2013) and the
Gradient Matching approach (Calderhead et al., 2008).

6. Discussion
Bayesian parameter estimation in ODEs using numerical integration is an ideal but computationally prohibitive
method for all but the smallest systems due to the high cost
of explicit numerical integration required to evaluate a single point in the posterior. Any other model will necessarily
make assumptions that are formally inconsistent with this
ideal approach and the aim is therefore to trade the accuracy of matching the ideal Bayesian numerical integration
approach for computational speed.
Whilst previous alternatives based on using Gaussian Processes have demonstrated some success in circumventing
the high computational cost, these are not natural generative models of the data. Despite the improvements in (Dondelinger et al., 2013), previous GP approaches use a heuristic compatibility function that leads to a complex chain
graph with unknown normalisation constant.

In contrast, our method has a natural link to numerical integration and is we believe conceptually the closest match
amongst competing GP approaches to the ideal numerical
integration mechanism. Our GP-ODE approach is a simple
generative model of data and as such is amenable to alternative approximation techniques, other than Monte Carlo.
For example, variational approximations are in principle
possible to apply directly to the posterior.
In our experience on toy benchmark problems, our GPODE approach performs at least as well as alternative GP
approaches and sometimes significantly better, particularly
in the case of observations with appreciable noise. Code is
available from github.com/odegp/code.
Acknowledgements
We would like to thank Dirk Husmeier and Benn Macdonald for helpful discussions and provision of their code.

Gaussian Processes for Ordinary Differential Equations

References
Barber, D. Bayesian Reasoning and Machine Learning.
Cambridge University Press, 2012.
Calderhead, B., Girolami, M., and Lawrence, N. D. Accelerating Bayesian Inference over Nonlinear Differential
Equations with Gaussian Processes. In NIPS, 2008.
Dondelinger, F., Filippone, M., Rogers, S., and Husmeier,
D. ODE parameter inference using adaptive gradient
matching with Gaussian processes. In AISTATS, 2013.
Graepel, T. Solving noisy linear operator equations by
Gaussian processes: application to ordinary and partial
differential equations. In ICML, 2003.
Koller, D. and Friedman, N. Probabilistic Graphical Models: Principles and Technique. MIT Press, 2009.
Ramsay, J., Hooker, G., Campbell, D., and Cao, J. Parameter Estimation for Differential Equations: A Generalized
Smoothing Approach. Journal of the Royal Statistical
Society: Series B, 69(5):741–796, 2007.
Solak, E., Murray-Smith, R., Leithead, W. E., Leith, D. J.,
and Rasmussen, C. E. Derivative observations in Gaussian Process models of dynamic systems. In NIPS, 2002.
Vyshemirsky, V. and Girolami, M. Bayesian ranking of
biochemical system models. Bioinformatics, 24:833–
839, 2008.

