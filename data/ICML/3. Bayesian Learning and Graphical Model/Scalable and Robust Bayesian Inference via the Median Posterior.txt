Scalable and Robust Bayesian Inference via the Median Posterior

Stanislav Minsker1
SMINSKER @ MATH . DUKE . EDU
Sanvesh Srivastava2,3
SS 602@ STAT. DUKE . EDU
Lizhen Lin2
LIZHEN @ STAT. DUKE . EDU
David B. Dunson2
DUNSON @ STAT. DUKE . EDU
1
2
Departments of Mathematics and Statistical Science , Duke University, Durham, NC 27708
Statistical and Applied Mathematical Sciences Institute3 , 19 T.W. Alexander Dr, Research Triangle Park, NC 27709

Abstract
Many Bayesian learning methods for massive
data benefit from working with small subsets of
observations. In particular, significant progress
has been made in scalable Bayesian learning via
stochastic approximation. However, Bayesian
learning methods in distributed computing environments are often problem- or distributionspecific and use ad hoc techniques. We propose a novel general approach to Bayesian inference that is scalable and robust to corruption
in the data. Our technique is based on the idea
of splitting the data into several non-overlapping
subgroups, evaluating the posterior distribution
given each independent subgroup, and then combining the results. Our main contribution is the
proposed aggregation step which is based on
finding the geometric median of subset posterior distributions. Presented theoretical and numerical results confirm the advantages of our approach.

1. Introduction
Massive data often require computer clusters for storage
and processing. In such cases, each machine in the cluster can only access a subset of data at a given point.
Most learning algorithms designed for distributed computing share a common feature: they efficiently use the data
subset available to a single machine and combine the “local” results for “global” learning, while minimizing communication among cluster machines (Smola & Narayanamurthy, 2010). A wide variety of optimization-based approaches are available for distributed learning (Boyd et al.,
2011); however, the number of similar Bayesian methods
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

is limited. One of the reasons for this limitation is related
to Markov chain Monte Carlo (MCMC) - one of the key
techniques for approximating the posterior distribution of
parameters in Bayesian models. While there are many efficient MCMC techniques for sampling from posterior distributions based on small subsets of the data (called subset posteriors in the sequel), there is no widely accepted
and theoretically justified approach for combining the subset posteriors into a single distribution for improved performance. To this end, we propose a new general solution for
this problem based on evaluation of the geometric median
of a collection of subset posterior distributions. The resulting measure is called the M-posterior (“median posterior”).
Modern approaches to scalable Bayesian learning in a distributed setting fall into three major categories. Methods
in the first category independently evaluate the likelihood
for each data subset across multiple machines and return
the likelihoods to a “master” machine, where they are appropriately combined with the prior using conditional independence assumptions of the probabilistic model. These
two steps are repeated at every MCMC iteration (Smola
& Narayanamurthy, 2010; Agarwal & Duchi, 2012). This
approach is problem-specific and involves extensive communication among machines. Methods from the second
category use stochastic approximation (SA) and successively learn “noisy” approximations to the full posterior
distribution using data in small mini-batches. The accuracy
of SA increases as it uses more observations. One subgroup of this category uses sampling-based methods to explore the posterior distribution through a modified Hamiltonian or Langevin Dynamics (Welling & Teh, 2011; Ahn
et al., 2012; Korattikara et al., 2013). Unfortunately, these
methods fail to accommodate discrete-valued parameters
and multimodality. The other subgroup uses deterministic variational approximations and learns the parameters of
the approximated posterior through an optimization-based
method (Hoffman et al., 2013; Broderick et al., 2013). Although these approaches often have excellent predictive
performance, it is well known that they tend to substan-

Scalable and Robust Bayesian Inference via the Median Posterior

tially underestimate posterior uncertainty and lack theoretical guarantees.
Our approach instead falls in a third class of methods,
which avoid extensive communication among machines by
running independent MCMC chains for each data subset
and obtaining draws from subset posteriors. These subset
posteriors can be combined in a variety of ways. Some
of the methods simply average draws from each subset
(Scott et al., 2013), other use an approximation to the
full posterior distribution based on kernel density estimators (Neiswanger et al., 2013), or the Weierstrass transform
(Wang & Dunson, 2013). Unlike the method proposed in
this work, none of the aforementioned algorithms are provably robust to the presence of outliers, moreover, they have
limitations related to the dimension of the parameter.
We propose a different approximation to the full data posterior for each subset, with MCMC used to generate samples from these “noisy” subset posteriors in parallel. As
a “de-noising step” that also induces robustness to outliers, we then calculate the geometric median of the subset posteriors, referred to as the M-posterior. By embedding the subset posteriors in a Reproducing Kernel Hilbert
Space (RKHS), we facilitate computation of distances, allowing Weiszfeld’s algorithm to be used to approximate
the geometric median (Beck & Sabach, 2013). The Mposterior admits strong theoretical guarantees, is provably
resistant to the presence outliers, efficiently uses all of the
available observations, and is well-suited for distributed
Bayesian learning. Our work was inspired by multivariate
median-based techniques for robust point estimation developed by Minsker (2013) and Hsu & Sabato (2013) (see also
Alon et al., 1996; Lerasle & Oliveira, 2011; Nemirovski &
Yudin, 1983, where similar ideas were applied in different
frameworks).

2. Preliminaries
We first describe the notion of geometric median and the
method for calculating distance between probability distributions via embedding them in a Hilbert space. This will
be followed by the formal description of our method and
corresponding theoretical guarantees.
2.1. Notation
In what follows, k · k2 denotes the standard Euclidean distance in Rp and h·, ·iRp - the associated dot product. Given
a totally bounded metric space (Y, d), the packing number
M (ε, Y, d) is the maximal number N such that there exist
N disjoint d-balls B1 , . . . , BN of radius ε contained in Y,
N
S
i.e.,
Bj ⊆ Y. Given a metric space (Y, d) and y ∈ Y,

words, for any Borel-measurable B, δy (B) = I{y ∈ B},
where I{·} is the indicator function. Other objects and definitions are introduced in the course of exposition when
such necessity arises.
2.2. Geometric median
The goal of this section is to introduce the geometric median, a generalization of the univariate median to higher
dimensions. Let µ be a Borel probability measure on a
normed space (Y, k · k). The Rgeometric median x∗ of µ
is defined as x∗ := argminy∈Y Y (ky − xk − kxk) µ(dx).
We use a special case of this definition and assume that
µ is a uniform distribution on a collection of m atoms
x1 , . . . , xm ∈ Y (which will later correspond to m subset posteriors identified with points in a certain space), so
that
x∗ = medg (x1 , . . . , xm ) := argmin
y∈Y

ky − xj k.

(1)

j=1

Geometric median exists under rather general conditions,
in particular, when Y is a Hilbert space. Moreover,
in this case x∗ ∈ co(x1 , . . . , xm ) – the convex hull
of x1 , . . . , xm (inPother words, there existP
nonnegative
m
αj , j = 1 . . . m, j αj = 1 such that x∗ = j=1 αj xj ).
An important property of the geometric median states that
it transforms a collection of independent and “weakly concentrated” estimators into a single estimator with significantly stronger concentration properties. Given q, α such
that 0 < q < α < 1/2, define
ψ(α, q) := (1 − α) log

α
1−α
+ α log .
1−q
q

(2)

The following result follows from Theorem 3.1 of Minsker
(2013).
Theorem 2.1. Assume that (H, h·, ·i) is a Hilbert space
and θ0 ∈ H. Let θ̂1 , . . . , θ̂m ∈ H be a collection of
independent random H - valued elements. Let the constants α, q, ν be such that 0 < q < α < 1/2, and
0 ≤ ν < α−q
1−q . Suppose ε > 0 is such that for all
j, 1 ≤ j ≤ b(1 − ν)mc + 1,


Pr kθ̂j − θ0 k > ε ≤ q.

(3)

Let θ̂∗ = medg (θ̂1 , . . . , θ̂m ) be the geometric median of
{θ̂1 , . . . , θ̂m }. Then

 h
i−m
α−ν
Pr kθ̂∗ − θ0 k > Cα ε ≤ e(1−ν)ψ( 1−ν ,q)
,

j=1

δy denotes the Dirac measure concentrated at y. In other

m
X

where Cα = (1 − α)

q

1
1−2α .

Scalable and Robust Bayesian Inference via the Median Posterior

Theorem 2.1 implies that the concentration of the geometric median around the “true” parameter value improves geometrically fast with respect to the number m of independent estimators, while the estimation rate is preserved. Parameter ν allows to take the corrupted observations into
account: if the data contain not more than bνmc outliers
of arbitrary nature, then at most bνmc estimators amongst
{θ1 , . . . , θm } can be affected. Parameter α should be
viewed as a fixed quantity and can be set to α = 1/3 for
the rest of the paper.
2.3. RKHS and distances between probability measures
The goal of this section is to introduce a special family
of distances between probability measures which provide
a structure necessary to evaluate the geometric median in
the space of measures. Since our goal is to develop computationally efficient techniques, we consider distances that
admit accurate numerical approximation.
Assume that (X, ρ) is a separable metric space, and let F =
{f : X 7→ R} be a collection of real-valued functions.
Given two Borel probability measures P, Q on X, define

Z



(4)
kP − QkF := sup  f (x)d(P − Q)(x) .
f ∈F
X

An important special case arises when F is a unit ball in a
RKHS (H, h·, ·i) with a reproducing kernel k : X × X 7→
RP so that 1
p
F = Fk := {f : X 7→ R, f ∈ H, kf kH := hf, f i ≤ 1}.
(5)
R p
Let Pk := {P is a prob. measure, X k(x, x)dP (x) <
∞}, and assume that P, Q ∈ Pk . If follows from Theorem
1 in (Sriperumbudur et al., 2010) that the corresponding
distance between measures P and Q takes the form

Z


 .
(6)
k(x,
·)d(P
−
Q)(x)
kP − QkFk = 


X

H

Note
Q are discrete measures (say, P =
PN1 that when P andP
N2
β
δ
and
Q
=
j=1 j zj
j=1 γj δyj ), then
kP − Qk2Fk =

N1
X

βi βj k(zi , zj )

(7)

i,j=1

+

N2
X
i,j=1

γi γj k(yi , yj ) − 2

N1 X
N2
X

βi γj k(zi , yj ).

i=1 j=1

R
The mapping P 7→ X k(x, ·)dP (x) is thus an embedding
of Pk into the Hilbert space H which can be seen as an application of the “kernel trick” in our setting. The Hilbert
1
We will say that k is a kernel if it is a symmetric, positive
definite function; it is a reproducing kernel for H and such that
for any f ∈ H and x ∈ X, hf, k(·, x)iH = f (x) (see Aronszajn,
1950, for details).

space structure allows to use fast numerical methods to approximate the geometric median.
In this work, we will only consider characteristic kernels,
which means that kP − QkFk = 0 if and only if P = Q.
It follows from Theorem 7 in (Sriperumbudur et al., 2010)
that a sufficient condition for k to be characteristic is strict
positive definiteness: we say that k is strictly positive definite if it is measurable, bounded,
and for all non-zero
RR
signed Borel measures µ,
k(x, y)dµ(x)dµ(y) > 0.
X×X

When X = Rp , a simple sufficient criterion for the kernel k
to be characteristic follows from Theorem 9 in (Sriperumbudur et al., 2010):
Proposition 2.2. Let X = Rp , p ≥ 1. Assume that
k(x, y) = φ(x − y) for some bounded, continuous, integrable, positive-definite function φ : Rp 7→ R.
b
1. Let φb be the Fourier transform of φ. If |φ(x)|
> 0 for
p
all x ∈ R , then k is characteristic;
2. If φ is compactly supported, then k is characteristic.
Remark 2.3.
(a) It is important to mention that in practical applications,
we (almost) always deal with empirical measures based
on a collection of independent samples from the posterior. A natural question is the following: if P and Q
are probability distributions on Rp and Pn , Qm are their
empirical
versions, what is the size of the error em,n :=



kP − QkFk − kPm − Qn kFk ? A useful fact is that em,n
often does not depend on p: under weak assumptions on
k, en,m has an upper bound of order m−1/2 + n−1/2 (see
corollary 12 in Sriperumbudur et al., 2009).
(b) Choice of the kernel determines the “richness” of the
space H and, hence, the relative strength of induced norm
k · kFk . For example, the well-known family of Matérn kernels leads to Sobolev spaces (Rieger & Zwicknagl, 2009).
Gaussian kernels often yields good results in applications.
Finally, we recall the definition of the well-known
Hellinger distance. Assume that P and Q are probability
measures on RD which are absolutely continuous with respect to Lebesgue measure with densities p and q respectively. Then
s Z
p
2
p
1
p(x) − q(x) dx
h(P, Q) :=
2 RD
is the Hellinger distance between P and Q.

3. Contributions and main results
3.1. Construction of “robust posterior distribution”
Let {Pθ , θ ∈ Θ} be a family of probability distributions
over RD indexed by Θ. Suppose that for all θ ∈ Θ, Pθ has

Scalable and Robust Bayesian Inference via the Median Posterior
θ
a Radon-Nikodym derivative pθ (·) = dP
dx (·) with respect
D
to the Lebesgue measure on R . In what follows, we equip
Θ with a “Hellinger metric”

ρ(θ1 , θ2 ) := h(Pθ1 , Pθ2 ),

(8)

and assume that the metric space (Θ, ρ) is separable.

j=1

Let X1 , . . . , Xn be i.i.d. RD -valued random vectors defined on some probability space (Ω, B, Pr) with unknown
distribution P0 := Pθ0 for θ0 ∈ Θ. A usual way to “estimate” P0 in Bayesian statistics consists in defining a prior
distribution Π over Θ (equipped with the Borel σ-algebra
induced by ρ), so that Π(Θ) = 1. The posterior distribution given the observations Xn := {X1 , . . . , Xn } is a
random probability measure on Θ defined by
Qn
Z
p (X )
R Qn i=1 θ i
dΠ(θ)
Πn (B|Xn ) :=
p
(Xi )dΠ(θ)
θ
i=1
B Θ

for all Borel measurable sets B ⊆ Θ. It is known (Ghosal
et al., 2000) that under rather general assumptions the posterior distribution Πn “contracts” towards θ0 , meaning that

j=1

small weights αj are set to 0 for improved performance;
see Algorithm 2 for details of implementation.
While Π̂n,g possesses several nice properties (such as robustness to outliers), in practice it often overestimates the
uncertainty about θ0 , especially when the number of groups
m is large. To overcome this difficulty, we suggest a mod(j)
ification of our approach where the random measures Πn
(subset posteriors) are replaced by the stochastic approximations Πn,m (·|Gj ), j = 1 . . . m of the full posterior distribution. To this end, define the stochastic approximation
to the full posterior based on the subsample Gj as
Q
bn/|Gj |c
Z
p
(X
)
dΠ(θ)
i
i∈Gj θ
.
Πn,m (B|Gj ) :=


bn/|Gj |c
R Q
p
(X
)
dΠ(θ)
θ
i
B
i∈Gj
Θ

Πn (θ ∈ Θ : ρ(θ, θ0 ) ≥ εn |Xn ) → 0

(10)

in probability as n → ∞ for a suitable sequence εn →
0. One of the question that we address can be formulated
as follows: what happens if some observations in Xn are
corrupted, e.g., if Xn contains outliers of arbitrary nature
and magnitude? In this case, the usual posterior distribution
might concentrate “far” from the true value θ0 , depending
on the amount of corruption in the sample. We show that
it is possible to modify existing inference procedures via a
simple and computationally efficient scheme that improves
robustness of the underlying method.
We proceed with the general description of the proposed
algorithm. Let 1 ≤ m ≤ n/2 be an integer, and divide the
sample Xn into m disjoint groups Gj , j = 1 . . . m of size
m
S
|Gj | ≥ bn/mc each: Xn =
Gj , Gi ∩Gl = ∅ for i 6= j.
j=1

A typically choice of m is m ' log n, so that the groups Gj
are sufficiently large (however, other choices are possible
as well depending on concrete practical scenario).
n
(j)
Let Π be a prior distribution over Θ, and let Πn (·) :=
o
Πn (·|Gj ), j = 1 . . . m be the family of posterior distributions depending on disjoint subgroups Gj , j = 1 . . . m:
Q
Z
pθ (Xi )
R Q i∈Gj
Πn (B|Gj ) :=
dΠ(θ).
i∈Gj pθ (Xi )dΠ(θ)
B Θ

Define the “median posterior” (or M-posterior) Π̂n,g as
(m)
Π̂n,g := medg (Π(1)
n , . . . , Πn ),

where the median medg (·) is evaluated with respect to
k · kFk introduced in (1) and (5). Note that Π̂n,g is always
a probability measure: due to the aforementioned properties of a geometric median, there exists α1 ≥ 0, . . . , αm ≥
m
m
P
P
(j)
0,
αj = 1 such that Π̂n,g =
αj Πn . In practice,

(9)

In other words, Πn,k (·|Gj ) is obtained as a posterior distribution given that each data point from Gj is observed
bn/|Gj |c times. Similarly to Π̂n,g , we set
Π̂st
n,g := medg (Πn,m (·|G1 ), . . . , Πn,m (·|Gm )).

(11)

While each of Πn,k (·|Gj ) might be “unstable”, the geometric median Π̂st
n,g of these random measures improves
stability and yields smaller credible sets with good coverage properties. Practical performance of Π̂st
n,g is often
superior as compared to Π̂n,g in our experiments. In all numerical simulations below, we evaluate Π̂st
n,g unless noted
otherwise.
3.2. Convergence of posterior distribution and
applications to robust Bayesian inference
Let k be a characteristic kernel defined on Θ × Θ; k defines
a metric on Θ
ρk (θ1 , θ2 ) := kk(·, θ1 ) − k(·, θ2 )kH
(12)

1/2
= k(θ1 , θ1 ) + k(θ2 , θ2 ) − 2k(θ1 , θ2 )
,
where H is the RKHS associated to kernel k. We will assume that (Θ, ρk ) is separable.
Assumption 3.1. Let h(Pθ1 , Pθ2 ) be the Hellinger distance
between Pθ1 and Pθ2 . Assume there exist positive constants
γ and C̃ such that for all θ1 , θ2 ∈ Θ,
h(Pθ1 , Pθ2 ) ≥ C̃ργk (θ1 , θ2 ).

Scalable and Robust Bayesian Inference via the Median Posterior

Example 3.2. Let {Pθ , θ ∈ Θ ⊆ Rp } be the exponential
family


dPθ
(x) := pθ (x) = exp hT (x), θiRp − G(θ) + q(x) ,
dx
where h·, ·iRp is the standard Euclidean dot product. Then
the Hellinger
h2 (Pθ1 , Pθ2 ) =
distance can be expressed
 as 


1−exp − 21 G(θ1 )+G(θ2 )−2G

θ1 +θ2
2

(Nielsen &

Garcia, 2011). If G(θ) is convex and its Hessian D2 G(θ)
satisfies D2 G(θ)  A uniformly for all θ ∈ Θ and some
symmetric positive definite operator A : Rp 7→ Rp , then


1
2
T
h (Pθ1 , Pθ2 ) ≥ 1 − exp − (θ1 − θ2 ) A(θ1 − θ2 ) ,
8
hence assumption 3.1 holds with Ck = √12 and γ = 1 for


1
k(θ1 , θ2 ) := exp − (θ1 − θ2 )T A(θ1 − θ2 ) .
8
In particular, it implies that for the family {Pθ =
N (θ, Σ), θ ∈ RD } with Σ  0 and the kernel


1
k(θ1 , θ2 ) := exp − (θ1 − θ2 )T Σ−1 (θ1 − θ2 ) ,
8
assumption 3.1 holds with Ck = √12 and γ = 1 (moreover,
it holds with equality rather than inequality).
Assume that Θ ⊂ Rp is compact, and let k(·, ·) be a kernel
defined on Rp × Rp . Suppose that k satisfies conditions
of proposition 2.2 (in particular, k is characteristic). Recall
that by Bochner’s theorem, there exists
R a finite nonnegative
Borel measure v such that k(θ) = eihx,θi dv(x).
p
R R 2
Proposition 3.3. Assume that kxk2 dv(x) < ∞ and for
Rp

all θ1 , θ2 ∈ Θ and some γ > 0,

h(Pθ1 , Pθ2 ) ≥ c(Θ)kθ1 − θ2 kγ2 .

(13)

Then assumption 3.1 holds with γ as above and C̃ =
C̃(k, c(Θ), γ).
Let δ0 := δθ0 be the Dirac measure concentrated at θ0 ∈ Θ
corresponding to the “true” distribution P0 .
Theorem 3.4. Let Xl = {X1 , . . . , Xl } be an i.i.d. sample
from P0 . Assume that εl > 0 and Θl ⊂ Θ are such that for
a universal constant K > 0 and some constant C > 0
1) log M (εl , Θl , ρ) ≤ lε2l ,
2) Π(Θ \ Θl ) ≤ exp(−lε2l (C + 4)),
!


2

p
pθ
θ
≤ ε2l , P0 log
≤ ε2l
3) Π θ : −P0 log
p0
p0
≥ exp(−lε2l C),
2

4) e−Klεl /2 ≤ εl .

Moreover, let assumption 3.1 be satisfied. Then there exists
a sufficiently large M = M (C, K, C̃) > 0


2
1
1/γ
Pr kδ0 − Πl (·|Xl )kFk ≥ M εl
≤
+ e−Klεl /2 .
Clε2l
(14)
Note that the right-hand side in (14) may decay very slowly
with l. This is where the properties of the geometric median become useful. Combination of Theorems 3.4 and 2.1
yields the following inequality for Π̂n,g which is our main
theoretical result.
Corollary 3.5. Let X1 , . . . , Xn be an i.i.d. sample from
P0 , and assume that Π̂n,g is defined with respect to the
k · kFk as in (9) above. Let l := bn/mc. Assume that
conditions of Theorem 3.4 hold, and, moreover, εl is such
1
−Klε2l /2
< 12 . Let α be such that
that q := Clε
2 + 4e
l
q < α < 1/2. Then
 h
i−m


1/γ
≤ eψ(α,q)
,
Pr δ0 − Π̂n,g F ≥ Cα M εl
k

(15)
where Cα = (1 − α)

q

1
1−2α

and M is as in Theorem 3.4.

The case when the sample Xn contains bνmc outliers of arbitrary nature can be handled similarly. This more general
bound is readily implied by Theorem 2.1.
For many parametric models (see Section
q 5 in Ghosal
et al. (2000)), (15) holds with εl ' τ m log(n/m)
for
n
τ small enough. If q
m ' log n (which is a typical sce2

log n
nario), then εl '
n . At the same time, if we
use the “full posterior” distribution Πn (·|Xn ) (which corresponds to m = 1), conclusion of Theorem 3.4 is that

1/(2γ) 

. log−1 n,
Pr kδ0 − Πn (·|Xn )kFk ≥ M logn n

while Corollary 3.5 yields a much stronger bound for Π̂n,g :

 2 1/(2γ) 

Pr δ0 − Π̂n,g F ≥ Cα M logn n
≤ rn− log n
k
for some 1 > rn → 0.
Theoretical guarantees for Π̂st , the median of “stochastic
n,g

approximations” defined in (11), are very similar to results
of Corollary 3.5, but we omit exact formulations here due
to the space constraints.

4. Numerical Experiments
In this section, we describe a method based on Weiszfeld’s
algorithm for implementing the M-posterior, and compare
the performance of M-posterior with the usual posterior in
the tasks involving simulated and real data sets.
We start with a short remark discussing the improvement
in computational time complexity achieved by M-posterior.

Scalable and Robust Bayesian Inference via the Median Posterior

Algorithm 1 Evaluating the geometric median of probability distributions via Weiszfeld’s algorithm
Input:
1. Discrete measures Q1 , . . . , Qm ;
2. The kernel k(·, ·) : Rp × Rp 7→ R;
3. Threshold ε > 0;
Initialize:
(0)
1
1. Set wj := m
, j = 1 . . . m;
m
P
(0)
1
Qj ;
2. Set Q∗ := m
j=1

(t)

=

kQ∗ −Qj k−1
F

k
m
P
(t)
kQ∗ −Qi k−1
Fk
i=1

1.

Update

2.

uate kQ∗ − Qi kFk );
m
P
(t+1)
(t+1)
Update Q∗
=
wj
Qj ;

; (apply (7) to eval-

(t)

j=1
(t+1)

The first example uses data from a Gaussian distribution
with known variance and unknown mean, and demonstrates
the effect of the magnitude of an outlier on the posterior
distribution of the mean parameter. The second example
demostrates the robustness and scalability of nonparametric regression using M-posterior in presence of outliers.
4.1.1. U NIVARIATE G AUSSIAN DATA

repeat
Starting from t = 0, for each j = 1, . . . , m:
(t+1)
wj

4.1. Simulated data

(t)

until kQ∗
− Q∗ kFk ≤ ε
(t+1)
(t+1)
, . . . , wm ).
Return: w∗ := (w1

Algorithm 2 Approximating the M-posterior distribution
Input:
Nj
1. Samples {Zj,i }i=1
∼ i.i.d. from Πn,m (·|Gj ), j =
1 . . . m (see equation (10));
Do:
Nj
P
δZj,i , j = 1 . . . m - empirical approxima1. Qj := N1j
i=1

tions of Πn,m (·|Gj ).
2. Apply Algorithm 1 to Q1 , . . . , Qm ; return w∗ =
(w∗,1 . . . w∗,m );
1
3. For j = 1, .P
. . , m, set w̄j := w∗,j I{w∗,j ≥ 2m
}; define
ŵj∗ := w̄j / m
w̄
.
i=1 i
Pm
∗
Return: Π̂st
n,g :=
i=1 ŵi Qi .

Given the data set Xn of size n, let t(n) be the running
time of the subroutine (e.g., MCMC) that outputs a single observation from the posterior distribution Πn (·|Xn ).
Assuming that our goal is to obtain a sample of size N
from the (usual) posterior, the total computational complexity is O (N · t(n)). We compare this with the running
time needed to obtain a sample of same size N from the
M -posterior given that the algorithm is running on m machines (m  n) in parallel. In this case, we need to generate O (N/m) samples from each
of m subset posteriors,
N
n
which is done in time O m
·t m
. According to Theorem 7.1 in (Beck & Sabach, 2013), Weiszfeld’s algorithm
approximates the M-posterior to degree of accuracy ε in at
most O(1/ε) steps, and each of these steps has complexity
O(N 2 ) (which
follows from(7)), so that the total running

 N2
N
N
time is O m
·t m
+ ε . If, for example, t(n) ' nr

N
n
1
for some r ≥ 1, then m
·t m
' m1+r
N nr which should
r
be compared to N · n required by the standard approach.

The goal of this example is to demonstrate the effect of
a large outlier on the posterior distribution of the mean
parameter µ. We generated 25 sets containing 100 observations each. Every sample {xi }25
i=1 , where xi =
(xi,1 , . . . , xi,100 ), contains 99 independent observations
from the standard Gaussian distribution (xi,j ∼ N (0, 1)
for i = 1, . . . , 25 and j = 1, . . . , 99), while the last
entry in each sample, xi,100 , is an outlier, and its value
linearly increases for i = 1, . . . , 25, namely, xi,100 =
i max(|xi,1 |, . . . , |xi,99 |). Index of an outlier is unknown
to the algorithm, while the variance of observations is
known. We use a flat (Jeffreys) prior on the mean µ
and obtain its posterior
distribution, which is also GausP
100

xij

1
and variance 100
. We generate
sian with mean j=1
100
1000 samples from each posterior distribution Π100 (·|xi )
for i = 1, . . . , 25. Algorithm 2 generates 1000 samples
from the M-posterior Π̂st
100,g (·|xi ) for each i = 1, . . . , 25:
to this end, we set m = 10 and generate 100 samples from
every Π100,10 (·|Gj,i ), j = 1, . . . , 10 to form the empirical
measures Qj,i ; here, ∪10
j=1 Gj,i = xi . Consensus MCMC
(Scott et al., 2013) as a representative for scalable MCMC
methods, and compared its performance with M-posterior
when the number of data subsets is fixed.

Figure 1 compares the performance of the “consensus posterior”, the overall posterior and the M-posterior using the
empirical coverage of (1-α)100% credible intervals (CIs)
calculated across 50 replications for α = 0.2, 0.15, 0.10,
and 0.05. The empirical coverages of M-posterior’s CIs
show robustness to the size of an outlier. On the contrary,
performance of the consensus and overall posteriors deteriorate fairly quickly across all α’s leading to 0% empirical
coverage as the outlier strength increases from i = 1 to
i = 25.
4.1.2. G AUSSIAN PROCESS REGRESSION
We use function f0 (x) = 1 + 3 sin(2πx − π) and simulate 90 (case 1) and 980 (case 2) values of f0 at equidistant x’s in [0, 1] (hereafter x1:90 and x1:980 ) corrupted by
Gaussian noise with mean 0 and variance 1. To demonstrate the robustness of M-posterior in nonparametric regression, we added 10 (case 1) and 20 (case 2) outliers
(sampled on the uniform grids of corresponding sizes) to

Scalable and Robust Bayesian Inference via the Median Posterior
α = 0.2

M−Posterior ●

α = 0.15

Consensus Posterior ●
α = 0.1

α = 0.05

●
●●● ●● ●●● ● ●
●
● ●●
●●
●
●
● ● ● ● ●●
● ● ●●
●●
●
●
● ●
●●●●
●
● ●
●
●●
●
●●
●
●●
●
●●
●
●
●
●
●
●
● ● ● ●
●
●● ●
●
● ●
● ● ● ● ●
●
● ●
●●
●
●
●
●
●
●
●
●
●
●● ● ● ●
●
● ●●
● ●
●●●
●
●
●
●
●
●
● ●
● ●
●
●●
● ● ●●●●
●
● ● ●
●
●
●
●
●
●
●●
●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●
● ●
●
●
●
●
●●
●●
●
●●
●●●
●●
●●
●●
● ●
●●●
●
●●●
●●
● ●
●●
● ●●
●●
●
● ●●
● ●
●●
●●●●
●●
●●
●●● ●●
●●●● ●●
●
●●●●●●
●
●●●●
●●●●●●●●
●●●●●
●●●●
●●●●
●●●●●●●
●●●
●●●●
●●●●●●
●●●
● ●●●●●●
●●●

5

10

15

20

25

5

10

15

20

25

5

10

15

20

25

5

10

15

20

25

Relative magnitude of the outlier

Figure 1. Effect of outlier on the empirical coverage of (1α)100% credible intervals (CIs). The x-axis represents the outlier magnitude. The y-axis represents the fraction of times the
CIs include the true mean over 50 replications. The panels show
the coverage results when α = 0.2, 0.15, 0.10, and 0.05. The
horizontal lines (in violet) show the theoretical coverage.

the data sets such that f0 (x91:100 ) = 10 max(f0 (x1:90 ))
and f0 (x981:1000 ) = 10 max(f0 (x1:980 )).
The gausspr function in kernlab R package (Karatzoglou et al., 2004) is used for GP regression. Based on
the standard convention in GP regression, the noise variance (or “nugget effect”) is fixed at 0.01. Using these
settings for GP regression without the “standard” posterior, gausspr obtains an estimator fb1 and a 95% confidence band for the values of the regression function at
100 equally spaced grid points y1:100 in [0, 1] (note that
these locations are different from the observed data). Algorithm 2 performs GP regression with M-posterior and obtains an estimator fb2 described below. The posterior draws
across y1:100 are obtained in cases 1 and 2 as follows. First,
{(xi , fi )} are split into m = 10 and 20 subsets (each living
on its own uniform grid) respectively, and gausspr estimates the posterior mean µj and covariance Σj for each
data subset, j = 1, . . . , m. These estimates correspond
to the Gaussian distributions Πj (·|µj , Σj ) that are used to
generate 100 posterior draws at y1:100 each. These draws
are further employed to form the empirical versions of subset posteriors. Finally, Weiszfeld’s algorithm is used to
combine them. Next, we obtained 1000 samples from the
M-posterior Πg (·|{(xi , fi )}). The median of these 1000
samples at each location on the grid y1:100 represents the
estimator fˆ2 . Its 95% confidence band corresponds to 2.5%
and 97.5% quantiles of the 1000 posterior draws across
y1:100 .
Figure 2 summarizes the results of GP regression with and
without M-posterior across 30 replications. In case 1, GP
regression without M-posterior is extremely sensitive to the
outliers, resulting in fb1 that is shifted above the truth and
distorted near the x’s that are adjacent to the outliers; in
turn, this affects the coverage of 95% confidence bands
and results in the “bumps” that correspond to the location
of outliers. In contrast, GP regression using M-posterior

M−Posterior
(out, total) = (10, 100)

M−Posterior
(out, total) = (20, 1000)

GP Posterior
(out, total) = (10, 100)

GP Posterior
(out, total) = (20, 1000)

15
10
5
0
−5

f(x)

Posterior credible interval level (1 − α)

Overall Posterior ●

1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

15
10
5
0
−5
0.0

0.2

0.4

0.6

0.8

1.0 0.0

0.2

0.4

0.6

0.8

1.0

x

Figure 2. Performance of M-posterior in Gaussian process (GP)
regression. The top and bottom row of panels show simulation
results for M-posterior (in blue) and GP regression (in red). The
size of data increases from column 1 to 2. The true noiseless curve
f0 (x) is in green. The shaded regions around the curves represent
95% confidence bands obtained over 30 replicated data sets.

produces fb2 which is close to the true curve in both cases;
however, in case 1, when the number of data points is small,
the 95% bands are unstable.
An attractive property of M-posterior based GP regression
is that numerical instability due to matrix inversion can be
avoided by working with multiple subsets. We investigated
such cases when the number of data points n was greater
than 104 . Chalupka et al. (2012) compare several low rank
matrix approximations techniques used to avoid matrix inversion in massive data GP computation. M-posterior–
based GP computation does not use approximations to obtain subset posteriors. By increasing the number of subsets (m), M-posterior based GP regression is both computationally feasible and numerically stable for cases when
n = O(106 ) and m = O(103 ). On the contrary, standard GP regression using the whole data set was intractable
for data size greater than 104 due of numerical instabilities in matrix inversion. In general, for n data points and
m subsets, the computational complexity for GP with Mn 2
) ); therefore, m > 1 is computationposterior is O(n( m
ally better than working with the whole data set. By caren
fully choosing the m
ratio depending on the available computational resources and n, GP regression with M-posterior
is a promising approach for GP regression for massive data
without low rank approximations.
4.2. Real data: PdG hormone levels vs day of ovulation
North Carolina Early Pregnancy Study (NCEPS) measured
urinary pregnanediol-3-glucuronide (PdG) levels, a progesterone metabolite, in 166 women from the day of ovulation across 41 time points (Baird et al., 1999). These data
have two main features that need to be modeled. First, the
data contains information about women in different stages

Scalable and Robust Bayesian Inference via the Median Posterior

NCEPS data have missing PdG levels for multiple women
across multiple time points. We discarded subjects that did
not have data for at least half the time points, which left us
with 3810 PdG levels across 41 time points. The size of
the data enables the use of gausspr function for GP regression, and its results are compared against M-posterior
based GP regression (similar to Section 4.1.2). The data
was divided into 10 subsets. On each stage, 9 of them
were used to evaluate the M-posterior while the remaining was a test set; the process was repeated 10 times for
different test subsets. Our goal is to obtain posterior predictive intervals for log PdG levels given the day of ovulation. Figure 3 shows the posterior predictive distribution
obtained via GP regression with and without M-posterior.
Across all folds, the uncertainty quantification based on Mposterior is much better than its counterpart without the
M-posterior. The main reason for this poor performance
of “vanilla” GP regression is that NCEPS data have many
data points for each day relative to ovulation, but with
many outliers and missing data. The vanilla GP regression
does not account for the latter feature of NCEPS data, thus
leading to over-optimistic uncertainty estimates across all
folds. M-posterior automatically accounts for outliers and
model misspecification; therefore, it leads to reliable posterior predictive uncertainty quantification across all folds.

5. Discussion
We presented a general approach to scalable and robust
Bayesian inference based on the evaluation of the geometric median of subset posterior distributions (M-posterior).
To the best of our knowledge, this is the first technique that
is provably robust and computationally efficient. The key
to making inference tractable is to embed the subset posterior distributions in a suitable RKHS, and pose the aggregation problem as convex optimization in this space,
which in turn can be solved using Weiszfeld’s algorithm,
a simple gradient descent-based method. Unlike popular
point estimators, the M-posterior distribution can be used
for summarizing uncertainty in the parameters of interest.
Another advantage of our approach is scalability, so that it
can be used for distributed Bayesian learning: first, since
it combines the subset posteriors using a gradient-based al-

GP Posterior

log PdG levels

of conception and non-conception ovulation cycles, so the
probabilistic model should be flexible and free of any restrictive distributional assumptions; therefore, we choose
non-parametric regression of log PdG on day of ovulation
using GP regression. Second, missing data and extreme observations are very common in these data due the nature of
observations and diversity of subjects in the study; therefore, we use M-posterior based GP regression as a robust
approach to automatically account for outliers and possible
model misspecification.

M−Posterior

Test Data : 6

Test Data : 7

Test Data : 8

Test Data : 9

Test Data : 10

Test Data : 1

Test Data : 2

Test Data : 3

Test Data : 4

Test Data : 5

4
3
2
1
0
−1
−2
−3
−4
4
3
2
1
0
−1
−2
−3
−4
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40

Time (days)

Figure 3. Comparison of 95% posterior predictive intervals for
GP regression with and without M-posterior. The x and y axes
represent day of ovulation and log PdG levels. Panels show the
result of GP regression of log PdG on day of ovulation across 10folds of NCEPS data. The mean dependence log PdG on day of
ovulation is shown using solid lines. The dotted lines around the
solid curves represent the 95% posterior predictive intervals. The
intervals for GP regression without M-posterior severely underestimate the uncertainty in log PdG levels. On the other hand,
posterior predictive intervals of M-posterior appear to be reasonable and the trend of dependence of log PdG on day of ovulation
is stable.

gorithm, it naturally scales to massive data. Second, since
Weiszfeld’s algorithm only uses the samples from subset
posteriors, it can be implemented in a distributed setting
via MapReduce/Hadoop.
Several important questions are not included in the present
paper and will be addressed in subsequent work. These
topic include applications to other types of models; alternative data partition methods and connections to the inference based on the usual posterior distribution; different
choices of distances, notions of the median and related subset posterior aggregation methods. More efficient computational alternatives and extensions of Weiszfeld’s algorithm
(which is currently used due to its simplicity, stability, and
ease of implementation) can be developed for estimating
the M-posterior; see (Bose et al., 2003; Cardot et al., 2013;
Vardi & Zhang, 2000), among other works. Applications of
distributed optimization methods, such as ADMM (Boyd
et al., 2011), is another potentially fruitful approach.

Acknowledgments
Authors were supported by grant R01-ES-017436 from the National Institute of Environmental Health Sciences (NIEHS) of the
National Institutes of Health (NIH). S. Srivastava was supported
by NSF under Grant DMS-1127914 to SAMSI. S. Minsker acknowledges support from NSF grants FODAVA CCF-0808847,
DMS-0847388, ATD-1222567.

Scalable and Robust Bayesian Inference via the Median Posterior

References
Agarwal, A. and Duchi, J. C. Distributed delayed stochastic optimization. In Decision and Control (CDC), 2012
IEEE 51st Annual Conference on, pp. 5451–5452. IEEE,
2012.

Hsu, D. and Sabato, S. Loss minimization and parameter
estimation with heavy tails. arXiv:1307.1827, 2013.
Karatzoglou, A., Smola, A., Hornik, K., and Zeileis, A.
kernlab – an S4 package for kernel methods in R. Journal of Statistical Software, 11(9):1–20, 2004.

Ahn, S., Korattikara, A., and Welling, M. Bayesian posterior sampling via stochastic gradient Fisher scoring. In
Proceedings of ICML, 2012.

Korattikara, A., Chen, Y., and Welling, M. Austerity in
MCMC land: cutting the Metropolis-Hastings budget.
arXiv:1304.5299, 2013.

Alon, N., Matias, Y., and Szegedy, M. The space complexity of approximating the frequency moments. In Proceedings of the 28th ACM symposium on Theory of computing, pp. 20–29. ACM, 1996.

Lerasle, M. and Oliveira, R. I. Robust empirical mean estimators. arXiv:1112.3914, 2011.

Aronszajn, N. Theory of reproducing kernels. Transactions
of the American mathematical society, 68(3):337–404,
1950.
Baird, D. D., Weinberg, C. R., Zhou, H., Kamel, F., McConnaughey, D. R., Kesner, J. S., and Wilcox, A. J.
Preimplantation urinary hormone profiles and the probability of conception in healthy women. Fertility and
sterility, 71(1):40–49, 1999.
Beck, A. and Sabach, S. Weiszfeld’s method: old and new
results. Preprint, 2013.
Bose, P., Maheshwari, A., and Morin, P. Fast approximations for sums of distances, clustering and the Fermat–
Weber problem. Computational Geometry, 24(3):135–
146, 2003.
Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J.
Distributed optimization and statistical learning via the
alternating direction method of multipliers. Foundations
and Trends in Machine Learning, 3(1):1–122, 2011.
Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., and
Jordan, M. Streaming variational Bayes. In NIPS, pp.
1727–1735, 2013.
Cardot, H., Cenac, P., and Zitt, P.-A. Efficient and fast estimation of the geometric median in Hilbert spaces with
an averaged stochastic gradient algorithm. Bernoulli, 19
(1):18–43, 2013.
Chalupka, K., Williams, C. K., and Murray, I. A framework for evaluating approximation methods for gaussian
process regression. arXiv:1205.6326, 2012.

Minsker, S. Geometric median and robust estimation in
Banach spaces. arXiv:1308.1334, 2013.
Neiswanger, W., Wang, C., and Xing, E.
totically exact, embarrassingly parallel
arXiv:1311.4780, 2013.

AsympMCMC.

Nemirovski, A. and Yudin, D. Problem complexity and
method efficiency in optimization. 1983.
Nielsen, F. and Garcia, V. Statistical exponential families:
a digest with flash cards. arXiv:0911.4863, 2011.
Rieger, C. and Zwicknagl, B. Deterministic error analysis of support vector regression and related regularized
kernel methods. JMLR, 10:2115–2132, 2009.
Scott, S. L., Blocker, A. W., Bonassi, F. V., Chipman, H. A.,
George, E. I., and McCulloch, R. E. Bayes and Big Data:
the consensus Monte Carlo algorithm. In EFaB Bayes
250 workshop, volume 16, 2013.
Smola, A. J. and Narayanamurthy, S. An architecture for
parallel topic models. In Very Large Databases (VLDB),
2010.
Sriperumbudur, B. K., Fukumizu, K., Gretton, A.,
Schölkopf, B., and Lanckriet, G. On integral probability metrics, φ-divergences and binary classification.
arXiv:0901.2698, 2009.
Sriperumbudur, B. K., Gretton, A., Fukumizu, K.,
Schölkopf, B., and Lanckriet, G. Hilbert space embeddings and metrics on probability measures. JMLR, 99:
1517–1561, 2010.
Vardi, Yehuda and Zhang, Cun-Hui. The multivariate L1 median and associated data depth. Proceedings of the
National Academy of Sciences, 97(4):1423–1426, 2000.

Ghosal, S., Ghosh, J. K., and van der Vaart, A. W. Convergence rates of posterior distributions. Annals of Statistics, 28(2):500–531, 2000.

Wang, X. and Dunson, D. B. Parallel MCMC via Weierstrass sampler. arXiv:1312.4605, 2013.

Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J.
Stochastic variational inference. JMLR, 14:1303–1347,
2013.

Welling, M. and Teh, Y. W. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of
ICML, pp. 681–688, 2011.

