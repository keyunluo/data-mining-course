Variational Inference for Sequential Distance Dependent Chinese Restaurant
Process
Sergey Bartunov
Dorodnicyn Computing Centre of the Russian Academy of Sciences, Moscow RUSSIA
Dmitry P. Vetrov
Moscow State University, Moscow RUSSIA
Higher School of Economics, Moscow RUSSIA

Abstract
Recently proposed distance dependent Chinese
Restaurant Process (ddCRP) generalizes extensively used Chinese Restaurant Process (CRP)
by accounting for dependencies between data
points. Its posterior is intractable and so far
only MCMC methods were used for inference.
Because of very different nature of ddCRP no
prior developments in variational methods for
Bayesian nonparametrics are appliable. In this
paper we propose novel variational inference
for important sequential case of ddCRP (seqddCRP) by revealing its connection with Laplacian of random graph constructed by the process. We develop efficient algorithm for optimizing variational lower bound and demonstrate
its efficiency comparing to Gibbs sampler. We
also apply our variational approximation to CRPequivalent seqddCRP-mixture model, where it
could be considered as alternative to one based
on truncated stick-breaking representation. This
allowed us to achieve significantly better variational lower bound than variational approximation based on truncated stick breaking for Dirichlet process.

1. INTRODUCTION
One of the most important problems in machine learning
and statistics is model selection. Well-known examples
of it are choosing the number of hidden layers of neural
network or the number of clusters in the mixture model.
Model selection almost always leads to a tradeoff between
model complexity and fit accuracy. While one may fit sevProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

SBOS @ SBOS . IN

VETROVD @ YANDEX . RU

eral models by varying the number of structural components and choosing the best one by some criteria, Bayesian
nonparametric methods provide an elegant alternative solution to this problem. Instead of comparing several models,
nonparametric approach is to define a distribution on model
structure which can adapt it’s complexity to data. Furthermore, it may refine and even complicate its structure when
new data is being observed.
For many nonparametric models such as widely used
Dirichlet process (Ferguson, 1973) or Indian Buffet Process (Griffiths & Ghahramani, 2006), it is common to assume exchangeability of data, the property that every permutation of data points has the same probability under the
model. While this assumption often holds, in many settings
when data has temporal, spatial or any other internal dependencies, a proper non-exchangeable prior which takes
such information into account could model data more adequately and thus fit it more accurately.
A number of such distributions were developed to date including dependent Dirichlet process (MacEachern, 1999)
and other similar processes (Duan et al., 2005; Griffin &
Steel, 2006; Xue et al., 2007), in which various constructions on top of Dirichlet process are considered. Besides
that, a distance-dependent Chinese Restaurant Process (ddCRP) was proposed recently by (Blei & Frazier, 2011),
which takes an alternative approach to modeling dependencies by considering a distribution over partitions. Given
pairwise distances of any nature (generally not symmetric),
each data point connects itself to other ones (including itself) with probability depending on the corresponding distance; after that data points that are reachable from each
other form a partition.
Besides very general yet simple formulation which allows
for many interesting applications including image segmentation (Ghosh et al., 2011) and natural language processing
tasks (Haghighi & Klein, 2010), ddCRP has another advantage of not generally exhibiting marginal invariance, the

Variational inference for seqddCRP

property that a missing observation does not affect the joint
distribution of data and parameters. This fact also distinguishes ddCRP from other mentioned processes.
As for many interesting distributions, posterior of ddCRP
is intractable, and so far only Markov Chain Monte Carlo
(MCMC) inference techniques were developed for it. Although it is theoretically guaranteed that a properly constructed Markov chain will once converge to the true posterior, in practice it’s often hard to determine convergence,
and no general methods are provided to estimate the required number of samples to obtain a good approximation.
Due to the fact that nature of ddCRP differs a lot from many
other nonparametric distributions, prior developments of
variational inference for DP, such as the one based on stickbreaking construction, are not applicable.
In this paper, we propose variational inference algorithm
for the important sequential case of ddCRP called sequential distance-dependent Chinese Restaurant Process (seqddCRP), in which (a) data points arrive in sequential order,
and (b) a data point can not be connected to those which
arrived later (though the opposite is allowed). This assumption often holds in temporal data and is natural for
many natural language processing tasks. Moreover, Chinese Restaurant Process (Aldous, 1985) could be formulated as a special case of seqddCRP.
The contributions of this paper are:
1. We introduce variational mean-field approximation
for seqddCRP mixture model by revealing connection between partition assignments of data points and
expected Laplacian of random graph modeled by the
process.
2. We show that our approximation of posterior provides
better lower bound to marginal likelihood for Dirichlet
process mixture than the one based on truncated stickbreaking process and also converges in much fewer
iterations.
3. We develop efficient coordinate-ascent inference algorithm and compare it with Gibbs sampler in terms of
computational performance and inference quality.
The rest of the paper is organized as follows. We first
review Dirichlet process and Chinese Restaurant process
(section 2), then we briefly describe distance-dependent
Chinese Restaurant Process (section 3). In section 4 we
present our variational inference framework for seqddCRP
mixture. We conclude with experiments on synthetic and
real data (section 5).

2. DIRICHLET PROCESS AND RELATED
PROCESSES
One of the most frequently used nonparametric models
is Dirichlet process (DP) introduced by (Ferguson, 1973).
It could be seen as infinite-dimensional generalization of
Dirichlet distribution parametrized by base measure G0 and
concentration parameter ↵. A draw from DP is a random distribution over draws from G0 with the property
that some draws may contain repetitive elements. The frequency of repetitions is governed by ↵.
The infinite set of probabilities representing frequencies of
unique draws from G0 is distributed according to stickbreaking process (SBP) (Sethuraman, 1994) which divides
total probability
P1 mass into diminishing probabilities ⇡k
such that k ⇡k = 1. Each ⇡k is constructed
by breaking
P
the rest of the unit-length stick 1
⇡
in
a ratio of
p<k p
vk ⇠ Beta(1, ↵). This allows for constructive definition of
DP and is often used for variational inference.
Thus one may define nonparametric mixture model using
DP by placing SBP prior over mixture component assignments zi for data points 1, 2, . . . and drawing mixture parameters ✓k from appropriate base measure.
Another representation of DP is Chinese Restaurant Process (Aldous, 1985) which could be derived by integrating out base measure from DP. This makes mixture assignments dependent on each other, but leaves them exchangeable and transforms DP into distribution over the all possible partitions of natural numbers. It is defined as follows:
consider a restaurant with infinite number of tables and no
customers at the beginning. Customers enter the restaurant one by one, each drawing her table assignment zi . By
definition for the first customer z1 = 1. All successive
customers i = 2, 3, . . . draw zi according to the following
distribution:
(
nk , k  K
p(zi = k|z\i ) /
(1)
↵, k = K + 1
where K is the number of ocuppied tables before i-th customers enters, and nk is a total number of customers already chosen the table. After all the table assignments have
been selected, they form clusters; after that cluster parameters ✓k are being drawn. Generative process is finished by
drawing each data point xi from the corresponing mixture
component ✓zi .

3. DISTANCE-DEPENDENT CHINESE
RESTAURANT PROCESS
The generative process of table assignments in CRP described above may be reformulated using customer assignments. Let each customer in CRP choose not the table zi ,

Variational inference for seqddCRP

Figure 1. Time dependent mixture. Same-colored points were drawn from the same normal distribution

but rather exactly one customer ci to share a table with:
(
1, j < i
p(ci = j|↵) /
(2)
↵, i = j
Thus the first customer always sits with herself, and successive customers sequentially choose whether they want
to join existing table (occupied by some of the previous
customers), or to initiate a new one. It is easy to show that
if we define zi (c) as the minimal index of customers that
are reachable from i through directed graph of customer
assignments (i.e. those which are sitting at the same table),
then the induced partitioning z(c) based on such reachability is equivalent to the one constructed in CRP.
(Blei & Frazier, 2011) generalized (2) to make customer
assignments distance dependent:
(
f (dij ), j < i
p(ci = j|f, D, ↵) /
(3)
↵, i = j
where f (d) is non-negative decay function such that
f (1) = 0, D is distance matrix and positive ↵ governs for
table initiation. This distribution on customer assignments
is called sequential distance-dependent Chinese Restaurant
Process (seqddCRP). Traditional CRP appears as a special
case of seqddCRP and the latter is generally a very different distribution. At first, it accounts for prior dependencies
in data which may be of any nature and hence does not
assume customers to be exchangeable. In addition, seqddCRP is not generally a distribution over partitions induced
by random measures (see (Blei & Frazier, 2011) for details).
It is also possible to define general ddCRP without assuming sequential order of customers by allowing assignments
ci > i. This makes possible to start a new table not neces-

sarily by drawing ci = i for some i. In that case, table assignments are constructed treating customer assignments as
an undirected graph. Below we focus on sequential restaurants only.
Now using (3) as a prior over mixture component assignments, we may define the seqddCRP mixture model:
0
1
N
Y
Y
@p(✓j |⌘)
p(x, c, ⇥|⌘) = p(c|⌘)
p(xi |✓j )A
j=1

p(c|⌘) =

N
Y

i=1

i,zi (c)=j

p(ci |f, D, ↵)

where ⌘ = {f, D, ↵, G0 } are process parameters, G0 is a
base measure generating mixture parameters ⇥, and N is
total number of data points.
Here we account for all N possible tables (since all customers have non-zero probability to start a new table) and
identify each table with a customer that initiated it by
choosing to sit with herself. Notice that while parameters
of empty tables affect joint distribution, marginal distribution p(x, c) does not depend on empty tables, and so does
the variational lower bound.

4. VARIATIONAL INFERENCE FOR
SEQDD-CRP MIXTURE
Before we continue with the mean-field approximation of
the posterior, let us first reformulate seqddCRP model. We
denote as rij (c) indicator function which is equal to 1 if
and only if there exists a directed path from i-th customer
to j-th customer in the graph induced by customer assignments c. We also expand customer assignment ci = j to
one-hot vector of size N such that cij = 1 and cik = 0
for k 6= j. This allows us to rewrite joint distribution of

Variational inference for seqddCRP

seqddCRP mixture in the following way:
cjj

(4)

As we are interested in connected components z(c) one
may observe that zi (c) = j if and only if cjj = 1
and rij (c) = 1 which is still correct table assignment
notation. Thus we enumerate tables not by abstract ordering 1, 2, . . . , |z(c)|, but rather by customer numbers
1, 2, . . . , N . Below we denote zij (c) = cjj rij (c).
To derive variational inference we need to define the factorized approximation of the posterior of seqddCRP parameters. Similarly to the variational algorithms for DP
we choose the factorization that holds true when there are
no data (i.e. for the priors). In DP such a property was
true for the parameters responsible for the breaking ratio
of the stick vk (see section 2). In the case of ddCRP such
parameters are ci . Indeed, in the absence of any data the
distribution p(c, ⇥|⌘) is fully factorized. We consider such
prior-induced factorization to be the most natural choice.
So we approximate posterior Q
p(c, ⇥|x, ⌘) as a fully factorized distribution q(c, ⇥) = i q(ci )q(✓i ) by minimizing
Kullback-Leibler divergence between the true posterior and
its approximation, which is equivalent to maximization of
marginal likelihood lower bound (Jordan et al., 1999):
X
N
log p(x) L = Eq
log p(ci |f, D, ↵) log q(ci )+
i=1

N
X
j=1

log p(✓j |G0 )

log q(✓j ) +

N
X
i=1

zij (c) log p(xi |✓j )

The main difficulty, both conceptual and computational in
deriving variational inference for ddCRP, is in expectation
of table assignments w.r.t variational distribution q(c), that
is Eq z(c). Table assignments z(c) are computed by deterministic function which maps independent customer assignments into table assignments. It is global in the sense
that even if we are interested only whether customer i sits
at the table started by customer j (that is zij (c)), in general case information about all customer assignments c is
required to answer this question. The opposite is also true:
changing just one customer assignment ci may cause global
change of table assignments. We now describe how to compute this expectation for seqddCRP and provide efficient
variational inference algorithm.
4.1. Expected table assignments via inverse of the
Laplacian
Consider directed graph modeled by seqddCRP. It consists
of N vertices, one for each customer, all of them having
exactly one link ci pointing to another customer j  i with

−2.25

i=1

p(xi |✓j )rij (c)

−2.30

N
Y

−2.35

j=1

p(✓j |G0 )

Per−point test likelihood

N
Y

−2.40

p(x, c, ⇥|⌘) = p(c|⌘)

Mean−field
Gibbs sampler

0

500

1000

1500

Time (sec)

Figure 2. Per-point test data likelihood as function of time on
time-dependent mixture modeling. For variational inference only
best run is drawn.

probability q(ci = j). We define then the probabilistic adjacency matrix A such that Aij = q(ci = j) for i 6= j and
Aii = 0 for all i.
Further we denote Rij = Eq(c) rij (c) which is the probability of existence of the directed path from customer i
to customer j. One may observe that vertex i may be connected with j directly with probability Aij or through other
vertices. By noting that Rii = 1 for all i1 for j > i we get:
X
X
Rij = Aij +
Ait Rtj =
Ait Rtj
t

t<i

We may vectorize this formula and obtain j-th column of
R:
R·j = A · R·j + ej = (I A) 1 ej ,
where ej is j-th column of identity matrix. Finally, we
obtain the whole matrix:
R = (I

A)

1

(5)

Matrix L = I A is non-singular since it’s triangular and
has ones on main diagonal. It could be viewed as expected
Laplacian2 of random graph induced by the seqddCRP.
1

It may be confusing that rii (c) = 1 regardless of the value
q(ci = i). We emphase that former statement is correct as long as
we are interested in reachability and not table assignments, while
q(ci = i) 1) governs tablePinitiation and 2) restrains probabilities
of outcoming links since j6=i q(ci = j) = 1 q(ci = i).
2
By definition i-th diagonal element of Laplacian is either in-

Variational inference for seqddCRP

Laplacian matrices are widely used in spectral graph theory for finding important graph properties, see e.g. (Chung,
1996).
To get expected table assignments, recall that zij (c) =
cjj rij (c) and use (5):
Ez(c) = (I

A)

1

diag(Ec)

(6)

Thus expected table assignments could be computed as
column-weighted inverse of expected Laplacian induced by
factorized distribution of customer assignments. This connection allows for deterministic inference algorithms and
also provides lower bound for their computational complexity as matrix inverse operation is being involved.
Note that equation (5) holds true not only for approximated
posterior q(c) but also for prior p(c|D, ↵, f, a) since it is
also factorized. This allows us to analytically obtain various properties ofPprior table assignments, e.g. expected
size of j-th table i j Ezij which wasn’t possible before.
Based on those properties one may select the values of
hyper-parameters for the model such as decay function f
or ↵.
4.2. Coordinate ascent algorithm
We iteratively perform fixed-point updates of each variational distribution q(ci ) by setting derivative of KL divergence with respect to corresponding distribution to zero.
Updates for q(✓j ) depend on a particular form of distributions and are quite straightforward, so we focus on updates
for q(c).
While we may naively use the result from the previous section to obtain the update equations, this would require a
number of matrix inverse operations which is computationally expensive. Thus, we derived an efficient Algorithm 1
requiring only one implicit matrix inverse per iteration and
relying on Woodbury (1950) matrix identity. We provide
the details on derivation of efficient variational updates in
the appendix.
The algorithm has complexity O(N 3 ) per iteration which
is equal to cost of a matrix inverse required to compute table assignments and variational lower bound and thus may
be considered as quite efficient. It is possible to further
improve it’s computational performance by adding and removing clusters ad-hoc, i.e. clusters with probability of existence q(ck = k) below some threshold may not be taken
into account.
Note that it is possible to update variational distributions
ner or outer degree of vertex i depending on the application (outer
in our case). Thus strictly speaking our matrix L may not be valid
Laplacian if q(ci = j) = 0 for some i and all j 6= i, but this
is insignificant for further reasoning as we do not rely on any of
Laplacian properties.

Algorithm 1 Variational inference for seqddCRP
Input: data x, initial q(c) and q(⇥), hyperparameters ⌘
Compute reachability matrix R according to eq. (5)
repeat
for i = 1 to N do
Initialize zero vector ai 2 RN
for k = 1P
to i do
aik
i i Rsi Eq log p(xs |✓k )
end for
Initialize ij = log p(ci = j|⌘) for all j
for j = 1 to i do
P
ij
ij +
ki aik Rjk q(ck = k)
end for
Exponentiate and normalize i , update q(ci )
i
Perform rank-1 update to R by Woodbury formula
end for
for k = 1 to N do
Q
Update q(✓k ) / p(✓k ) i k p(xi |✓k )Rik q(ck =k)
end for
until convergence

q(ci ) in any order and we have empirically observed that
random order updates perform much better than sequential
ones.

5. EXPERIMENTS
In this section we empirically compare our variational algorithms with a set of baselines. We start from comparison
with Gibbs sampler as it is currently the only available inference algorithm for ddCRP. Next we compare our variational inference for CRP-equivalent mixture model and
variational inference for Dirichlet Process. Then we evaluate distance-dependent mixture model against CRP mixture. We release our software implementation used for the
experiments3 .
5.1. Language modeling
Following (Blei & Frazier, 2011) we model natural text as
fully observed seqddCRP where individual tokens w are
organized in tables and each table then draws a word from
discrete distribution over words V (which is just word frequencies). This is very simple “unigram” model, yet it
is especially suitable for comparison with Gibbs sampler,
because there are no hidden variables and thus nothing to
collapse out. Note that derivation of collapsed variational
inference is usually non-trivial task and comparison of collapsed and non-collapsed model would be flawed, hence it
was decided to use this task for demonstration of computational efficiency of inference algorithms.
3

http://github.com/sbos/seqddcrp.jl

Variational inference for seqddCRP

−2400 −2000 −1600 −1200

Variational lower bound

and 200 test data points.
●●

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

●

DP VB
seqCRP

●

0

10

20

30

40

Iterations

It is common to compare different models by predictive
likelihood which is p(xtest |x) assuming that well-fitted
model assigns high probability to test data which is obtained from the same context as train data, e.g. held-out
part of the document. Unfortunately this quantity is intractable for ddCRP and thus we use the following estimate
as test likelihood:
X
ˆ =
ˆ
ˆ
p(xtest |x, ⇥)
p(c|x, ⇥)p(c
test )p(xtest |ctest , c, ⇥) (7)
ctest ,c

ˆ are cluster parameters estimated as posterior
where ⇥
mean.

Figure 3. Variational lower bound over iterations for DP mixture
with R = 5
Table 1. Best results for predictive likelihood
algorithm/R

seqddCRP

seqCRP

1
2
3
4
5

-863.55
-900.70
-864.93
-825.26
-689.38

-965.76
-905.75
-888.10
-836.58
-691.96

Our dataset consisted from 2246 news articles from the Associated Press, we performed word stemming, but did not
remove stop-words. Distances between tokens was defined
just as number of tokens between two given. Sigmoid decay function was used with hyper-parameters set as for best
model in (Blei & Frazier, 2011).
We compared absolute convergence speed of our variational algorithm comparing to Gibbs sampler by visually
assessing convergence of posterior estimates, in particular,
expected number of tables was chosen as statistics of interest since it is important quantity in nonparametric analysis.
We provide such a plot for a randomly selected document
on figure 6. It was observed that our variational algorithm
converged in just one iteration on all documents and estimates made with two considered algorithms are very close.
5.2. Mixture of Gaussians
We continue our experimental study with continuous mixture modeling. We generated 5 datasets each drawn from
mixture of 5 two-dimensional Gaussian distributions with
equal weights, spherical covariance and mean located in
(0, 0) and ( R, R), ( R, R), (R, R), (R, R) respectively, varying R from 1 to 5. R = 1 means that clusters
are almost undistinguishable and R = 5 makes them easely
separable. Each dataset was split into 200 train data points

Since the purpose of test likelihood is to measure how well
estimated cluster parameters explain unseen data, new clusters emerged in test data could flaw the results. It is impossible in traditional parametric models and in DP mixture
models based on truncated stick-breaking, however ddCRP
allows to assign ctest,i = i for some i. Thus we restrict
test data to start new tables by setting corresponding prior
probabilities to zero and re-normalizing prior.
5.2.1. SEQ CRP AND DP VB
We denote CRP-equivalent mixture model formulated as
special case of seqddCRP as seqCRP below in the paper.
We compare our variational inference algorithm for this
model and one for DP based on truncated stick-breaking
(Blei & Jordan, 2005), further we denote it as VB DP. We
set parameter ↵ = 0.1 for both models to encourage small
number of clusters and provided weak informative priors
for covariance matrix to slightly suggest it’s spherical form.
Truncation level for DP VB was set to 50.
Since both algorithms are dependent of random initialization we performed 300 runs in each setting and selected
best results. We observed that seqCRP achieved higher
variational lower bound and since both models actually represent the same mixture model, it could be admitted that
our variational approximation is tighter. Figure 4 contains
histograms of variational lower bounds on datasets with
R = 5 and R = 3. In the first case where the mixture
is easily separable, best lower bound obtained by our approximation was 1881.59 comparing to 1886.29 from
VB DP, and in the second case where it is harder to recover true number of clusters best results were 1891.73
and correspondingly 1892.66.
We also compared convergence rate for both algorithms
(see fig. 3). Our variational algorithm is considerably
slower because it involves matrix inverses extensively.
However, it converges much faster than VB DP in number
of iterations.
This empirically demonstrates potential of our variational

Variational inference for seqddCRP

Method
0.03

Method
0.04

seqCRP VB
TSB VB

seqCRP VB
TSB VB

Frequency

Frequency

0.03
0.02

0.02

0.01
0.01

0.00

0.00
-2050

-2000

-1950

Variational lower bound

-1900

-2000

-1960

-1920

Variational lower bound

-1880

Figure 4. Histograms of converged variational bounds across random initializations on R = 5 (left) and R = 3 (right) for DP-equivalent
mixture.

Predictive log−likelihood

−920 −910 −900 −890 −880 −870

were used.

●

●

●

●

●

●

●

●

●

●

●

seqddCRP
seqCRP

●

2

4

6

8

10

Iterations

Figure 5. Predictive log-likelihood over iterations for informative
and non-informative distance prior

algorithm which could be preferred over DP VB in situations when tight approximation is more important than running time.
5.2.2. SEQ CRP AND SEQDD CRP
Finally we evaluate seqddCRP with informative prior in
the sense that data points generated from the same mixture
component have lower distances than every pair of points
which are not. In particular, we have used exponential decay function with parameter a = 4 meaning moderate decay and sparseness inducing ↵ = 0.1, as in language modeling task distances were plain and measured in number of
data points between two given. Train data was generated
sequentially from each Gaussian, while test data was randomly permuted and for them uninformative CRP distances

Clearly, such information improved test likelihood comparing to plain seqCRP, see table 1. Also for both models test
likelihood converged very fast, detailed graph is on fig. 5.
Note that we didn’t tune parameters of the process and thus
even more performance increase could be achieved. This
suggests seqddCRP as an alternative to various sequential
mixture models such as Hidden Markov Model.
5.3. Time-dependent mixture
Motivated by previous experiment we evaluate noncollapsed Gibbs sampler and our variational algorithm on
modeling of time-dependent normal mixtures. Data was
generated from a number of states, each associated with its
own normal distribution, by a random process visualized
on figure 1. In each moment of time exactly one state is
active and the the process draws a point from the corresponding distribution. It also may switch between states.
Each normal distribution has mean 5 · k where k is its number and random precision drawn from Gamma distribution
with shape 0.8 and scale 1.1. This Gamma distribution was
used as a prior for both algorithms. After generative process finished, we randomly permuted several points near
state switches in order to simulate perturbations in transitive periods often observed in real data.
The setting was the same as in 5.2.2, except we used convenient average per-point test likelihood as evaluation metric
(see e.g. (Blei & Jordan, 2005)). Besides it is tractable for
both sample and analytic computation obtained from variational distribution, we empirically found that it is highly
correlated with test likelihood estimate we used before
(equation 7).

0

100

200

650
600
550

Mean number of tables − variational

520.5
519.5
518.5
517.5

Mean number of tables − Gibbs

Variational inference for seqddCRP

300

0

10

20

Time (sec)

30

40

50

60

Time (sec)

Figure 6. Mean number of tables over time for language modeling task over time

Results of the comparison are shown on figure 2. Clearly,
variational inference outperforms Gibbs sampler in convergence speed although the latter may achieve better test likelihood in time depending on the initialization.

ted for clarity):
log q(ci ) = Eq(c ) [log p(x, c, ⇥)]
\i
= log p(ci ) +

N X
N
X

s=1 k=1

6. CONCLUSION
In the paper we proposed mean-field approximation for
sequential distance-dependent Chinese Restaurant process
and developed efficient coordinate ascent algorithm. Our
inference procedure is closely connected with Laplacian of
random graph modeled by seqddCRP. We used special factorization w.r.t. customer assignments which is not only
natural for seqddCRP but is also applicable for conventional CRP. For the latter case it can be regarded as an alternative to well-known stick-breaking variational approximation. We showed that it might yield better lower bounds of
marginal probabilities for Dirichlet process mixture models. For the general case the only available inference framework is based Gibbs sampler. The proposed framework
could serve as its faster deterministic analogue.

Acknowledgments
We thank the anonymous reviewers for their helpful comments and suggestions. Sergey Bartunov is supported by
RFBR grant 14-01-31361, Dmitry P. Vetrov is supported
by RFBR grants 12-01-00938 and 12-01-33085.

Appendix: Efficient variational update
The general form of the variational update for each q(ci ) is
(with hyper-parameters and explicit dependency on c omit-

log p(xs |✓k )Eq(c

z
\i ) sk

+ const

where we denote c\i as a set of all customer assignments
except for i-th customer.
Now consider how Eq(c\i ) zsk = q(ck = k)Eq(c\i ) rsk depends on ci . First note that if s < i or k > i then rsk does
not depend on assignment of i-th customer due to sequen\i
tial property of the process. Denote rsk = 1 if there exists a
directed path from s to k that avoids i. Note that if rsk = 1
then it immediately implies rsi = 0. Then we may write
\i

rsk = 1 () (rik = 1 and rsi = 1) or (rsk = 1 and rsi = 0)

Equivalently in algebraic form
1

rsk = (1
1

\i

rik rsi )(1

rik rsk

rsk (1

\i
rsk (1

rsi ) =
\i

rsi ) + rik rsi rsk (1
(1

rsi ) = 1

rsi ) =

rik rsk

\i

rsk (1

rsi )

In last equation only the second item depends on ci . Let
ci = t. Then observing that rik = rct k we express the
dependence on ci by explicit formula:
log q(ci = t) = log p(ci = t)+
+

N X
N
X

s=1 k=1

Eq log p(xs |✓k )Eq(c

r (c)rsi (c)ckk
\i ) tk

+ const.

First note that rci k , rsi , and ckk are independent of each
other. Also note that Eq(c\i ) rsi does not depend on q(cj )
for j < i. The value of Eq(c\i ) rtk also does not depend on
q(cj ) for j > t, so we may compute it when updating q(ct )
as follows:
Eq(c\i ) rtk =

X
j<t

q(ct = j)Eq(c\i ) rjk .

Variational inference for seqddCRP

References
Aldous, D.J. Exchangeability and related topics. pp. 1–
198, 1985. Lecture Notes in Math. 1117.
Blei, David M. and Frazier, Peter I. Distance Dependent
Chinese Restaurant Processes. J. Mach. Learn. Res., 12:
2461–2488, November 2011. ISSN 1532-4435.
Blei, David M. and Jordan, Michael I. Variational inference for dirichlet process mixtures. Bayesian Analysis,
1:121–144, 2005.
Chung, Fan R. K. Spectral Graph Theory (CBMS Regional
Conference Series in Mathematics, No. 92). American Mathematical Society, December 1996. ISBN
0821803158.
Duan, A., Guindani, Michele, and Gelfand, Alan E. Generalized spatial dirichlet process models. In Duke University, pp. 05–23, 2005.
Ferguson, Thomas S. A Bayesian Analysis of Some Nonparametric Problems. The Annals of Statistics, 1(2):209–
230, 1973. ISSN 00905364. doi: 10.2307/2958008.
URL http://dx.doi.org/10.2307/2958008.
Ghosh, Soumya, Ungureanu, Andrei B., Sudderth, Erik B.,
and Blei, David M. Spatial distance dependent chinese restaurant processes for image segmentation. In
Shawe-Taylor, John, Zemel, Richard S., Bartlett, Peter L., Pereira, Fernando C. N., and Weinberger, Kilian Q. (eds.), NIPS, pp. 1476–1484, 2011.
Griffin, Jim E. and Steel, Mark F. J. Order-based dependent
dirichlet processes. Journal of the American Statistical
Association, 101(473):179–194, 2006.
Griffiths, Tom L. and Ghahramani, Zoubin. Infinite latent feature models and the Indian buffet process. In
Advances in Neural Information Processing Systems 18,
2006.
Haghighi, Aria and Klein, Dan. Coreference resolution
in a modular, entity-centered model. In Human Language Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Computational Linguistics, HLT ’10, pp. 385–393, Stroudsburg,
PA, USA, 2010. Association for Computational Linguistics. ISBN 1-932432-65-5.
Jordan, Michael I., Ghahramani, Zoubin, Jaakkola,
Tommi S., and Saul, Lawrence K. An introduction to
variational methods for graphical models. Mach. Learn.,
37(2):183–233, November 1999. ISSN 0885-6125.
MacEachern, S. Dependent Nonparametric Processes. In
ASA Proceedings of the Section on Bayesian Statistical
Science, 1999.

Sethuraman, Jayaram. A constructive definition of Dirichlet priors. Statistica Sinica, 4:639–650, 1994.
Woodbury, Max A. Inverting Modified Matrices. Number 42 in Statistical Research Group Memorandum Reports. Princeton University, Princeton, NJ, 1950.
Xue, Ya, Dunson, David, and Carin, Lawrence. The matrix stick-breaking process for flexible multi-task learning. In Proceedings of the 24th international conference on Machine learning, pp. 1063–1070, Corvalis,
Oregon, 2007. ACM. ISBN 978-1-59593-793-3. doi:
10.1145/1273496.1273630.

