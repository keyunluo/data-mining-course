High Dimensional Bayesian Optimisation and Bandits via Additive Models

Kirthevasan Kandasamy
Jeff Schneider
Barnabás Póczos
Carnegie Mellon University, Pittsburgh, PA, USA

Abstract
Bayesian Optimisation (BO) is a technique used
in optimising a D-dimensional function which
is typically expensive to evaluate. While there
have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic
are under very restrictive settings. In this paper,
we identify two key challenges in this endeavour.
We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer
class of functions than previous work. We prove
that, for additive functions the regret has only linear dependence on D even though the function
depends on all D dimensions. We also demonstrate several other statistical and computational
benefits in our framework. Via synthetic examples, a scientific simulation and a face detection
problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.

1. Introduction
In many applications we are tasked with zeroth order optimisation of an expensive to evaluate function f in D dimensions. Some examples are hyper parameter tuning in
expensive machine learning algorithms, experiment design,
optimising control strategies in complex systems, and scientific simulation based studies. In such applications, f is
a blackbox which we can interact with only by querying
for the value at a specific point. Related to optimisation is
the bandits problem arising in applications such as online
advertising and reinforcement learning. Here the objective
is to maximise the cumulative sum of all queries. In either
case, we need to find the optimum of f using as few queries
as possible by managing exploration and exploitation.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

KANDASAMY @ CS . CMU . EDU
SCHNEIDE @ CS . CMU . EDU
BAPOCZOS @ CS . CMU . EDU

Bayesian Optimisation (Mockus & Mockus, 1991) refers
to a suite of methods that tackle this problem by modeling
f as a Gaussian Process (GP). In such methods the challenge is two fold. At time step t, first estimate the unknown
f from the query value-pairs. Then use it to intelligently
query at xt where the function is likely to be high. For
this, we first we use the posterior GP to construct an acquisition function ϕt which captures the value of the experiment. Then we maximise ϕt to determine xt .
Gaussian process bandits and Bayesian optimisation (GPB/
BO) have been successfully applied in many applications
such as tuning hyperparameters in learning algorithms
(Snoek et al., 2012; Bergstra et al., 2011; Mahendran et al.,
2012), robotics (Lizotte et al., 2007; Martinez-Cantin et al.,
2007) and object tracking (Denil et al., 2012). However,
all such successes have been in low (typically < 10) dimensions (Wang et al., 2013). Expensive high dimensional
functions occur in several problems in fields such as computer vision (Yamins et al., 2013), antenna design (Hornby
et al., 2006), computational astrophysics (Parkinson et al.,
2006) and biology (Gonzalez et al., 2014). Scaling GPB/
BO methods to high dimensions for practical problems has
been challenging. Even current theoretical results suggest
that GPB/ BO is exponentially difficult in high dimensions
without further assumptions (Srinivas et al., 2010; Bull,
2011). To our knowledge, the only approach to date has
been to perform regular GPB/ BO on a low dimensional
subspace. This works only under strong assumptions.
We identify two key challenges in scaling GPB/ BO to high
dimensions. The first is the statistical challenge in estimating the function. Nonparametric regression is inherently difficult in high dimensions with known lower bounds
depending exponentially in dimension (Györfi et al., 2002).
The often exponential sample complexity for regression is
invariably reflected in the regret bounds for GPB/ BO. The
second is the computational challenge in maximising
ϕt . Commonly used global optimisation heuristics used
to maximise ϕt themselves require computation exponential in dimension. Any attempt to scale GPB/ BO to high
dimensions must effectively address these two concerns.

Additive Gaussian Process Optimisation and Bandits

In this work, we embark on this challenge by treating f
as an additive function of mutually exclusive lower dimensional components. Our contributions in this work are:
1. We present the Add-GP-UCB algorithm for optimisation and bandits of an additive function. An attractive property is that we use an acquisition function
which is easy to optimise in high dimensions.
2. In our theoretical analysis we bound the regret for
Add-GP-UCB. We show that it has only linear dependence on the dimension D when the f is additive.
3. Empirically we demonstrate that Add-GP-UCB outperforms naive BO on synthetic experiments, an astrophysical simulator and the Viola and Jones face detection problem. Furthermore Add-GP-UCB does well
on several examples when the function is not additive.
A Matlab implementation of our methods is available online at github.com/kirthevasank/add-gp-ucb.

2. Related Work
GPB/ BO methods follow a family of GP based active
learning methods which select the next experiment based
on the posterior (Osborne et al., 2012; Ma et al., 2015;
Kandasamy et al., 2015). In the GPB/ BO setting, common acquisition functions include Expected improvement
(Mockus, 1994), probability of improvement (Jones et al.,
1998), Thompson sampling (Thompson, 1933) and upper
confidence bound (Auer, 2003). Of particular interest to
us, is the Gaussian process upper confidence bound (GPUCB). It was first proposed and analysed in the noisy setting by Srinivas et al. (2010) and extended to the noiseless
case by de Freitas et al. (2012).
To our knowledge, most literature for GPB/ BO in high
dimensions are in the setting where the function varies
only along a very low dimensional subspace (Chen et al.,
2012; Wang et al., 2013; Djolonga et al., 2013). In these
works, the authors do not encounter either challenge as
they perform GPB/ BO in either a random or carefully selected lower dimensional subspace. However, assuming
that the problem is an easy (low dimensional) one hiding
in a high dimensional space is often too restrictive. Indeed, our experimental results confirm that such methods
perform poorly on real applications when the assumptions
are not met. While our additive assumption is strong in its
own right, it is considerably more expressive. It is more
general than the setting in Chen et al. (2012). Even though
it does not contain the settings in Djolonga et al. (2013);
Wang et al. (2013), unlike them, we still allow the function
to vary along the entire domain.
Using an additive structure is standard in high dimensional
regression literature both in the GP framework and other-

wise. Hastie & Tibshirani (1990); Ravikumar et al. (2009)
treat the function as a sum of one dimensional components.
Our additive framework is more general. Duvenaud et al.
(2011) assume a sum of functions of all combinations of
lower dimensional coordinates. These literature argue that
using an additive model has several advantages even if f is
not additive. It is a well understood notion in statistics that
when we only have a few samples, using a simpler model
to fit our data may give us a better trade off for estimation
error against approximation error. This observation is crucial: in many applications for Bayesian optimisation we are
forced to work in the low sample regime since calls to the
blackbox are expensive. Though the additive assumption
is biased for nonadditive functions, it enables us to do well
with only a few samples. While we have developed theoretical results only for additive f , empirically we show that
our additive model outperforms naive GPB/ BO even when
the underlying function is not additive.
Analyses of GPB/ BO methods focus on the query complexity of f which is the dominating cost in relevant applications. It is usually assumed that ϕt can be maximised to
arbitrary precision at negligible cost. Common techniques
to maximise ϕt include grid search, Monte Carlo and multistart methods (Brochu et al., 2010). In our work we use
the Dividing Rectangles (DiRect) algorithm of Jones et al.
(1993). While these methods are efficient in low dimensions they require exponential computation in high dimensions. It is widely acknowledged in the community that
this is a critical bottleneck in scaling GPB/ BO to high dimensions (de Freitas, 2014). While we still work in the
paradigm where evaluating f is expensive and characterise
our theoretical results in terms of query complexity, we believe that assuming arbitrary computational power to optimise ϕt is too restrictive. For instance, in hyperparameter
tuning the budget for determining the next experiment is
dictated by the cost of the learning algorithm. In online advertising and robotic reinforcement learning we need to act
in under a few seconds or real time.
In this manuscript, Section 3 formally details our problem
and assumptions. We present Add-GP-UCB in Section 4
and our theoretical results in Section 4.3. All proofs are deferred to Appendix B. We summarize the regrets for AddGP-UCB and GP-UCB in Table 1. The experiments
section is in Appendix C. Section 5 presents a summary.

3. Problem Statement & Set up
We wish to maximise a function f : X → R where X
is a rectangular region in RD . We will assume w.l.o.g
X = [0, 1]D . f may be nonconvex and gradient information is not available. We can interact with f only by querying at some x ∈ X and obtain a noisy observation y =
f (x)+. Let an optimum point be x∗ = argmaxx∈X f (x).

Additive Gaussian Process Optimisation and Bandits

Kernel
GP-UCB on Dth order kernel
Add-GP-UCB on additive kernel

Squared Exponential
p
DD+2 T (log T )D+2
p
dd D2 T (log T )d+2

Matérn
√
ν+D(D+1)
2D DT 2ν+D(D+1) log T
ν+d(d+1)

2d DT 2ν+d(d+1) log T

Table 1. Comparison of Cumulative Regret for GP-UCB and Add-GP-UCB for the Squared Exponential and Matérn kernels.

Suppose at time t we choose to query at xt . Then we
incur instantaneous regret rt = f (x∗ ) − f (xt ). In the
bandit P
setting, we P
are interested in the cumulative regret
T
T
RT = t=1 rt = t=1 f (x∗ ) − f (xt ), and in the optimisation setting we are interested in the simple regret ST =
mint≤T rt = f (x∗ ) − maxx f (x). Since ST ≤ T1 RT any
procedure with bounds on the cumulative regret is also a
consistent procedure for optimisation. For any algorithm, a
desirable property is to have no regret: limT →∞ T1 RT = 0.
Key structural assumption: In order to make progress in
high dimensions, we will assume that f decomposes into
the following additive form,
f (x) = f (1) (x(1) ) + f (2) (x(2) ) + · · · + f (M ) (x(M ) ). (1)
Here each x(j) ∈ X (j) = [0, 1]dj are lower dimensional
components. We will refer to the X (j) ’s as “groups” and
the grouping of different dimensions into these groups
{X (j) }M
j=1 as the “decomposition”. The groups are disjoint – i.e. if we treat the elements of the vector x as a set,
x(i) ∩x(j) = ∅. We are primarily interestd in the case when
D is very large and the group dimensionality
is bounded:
P
dj ≤ d  D. We have D  dM ≥ j dj . Paranthesised
superscripts index the groups and a union over the groups
denotes the
S reconstruction of
S the whole from the groups
(e.g. x = j x(j) and X = j X (j) ). xt denotes the point
chosen by the algorithm for querying at time t. We will
ignore log D terms in O(·) notation. Our theoretical analysis assumes that the decomposition is known but we also
present a modified algorithm to handle unknown decompositions and non-additive functions.
Some smoothness assumptions on f are warranted to make
the problem tractable. A standard in the Bayesian paradigm
is to assume f is sampled from a Gaussian Process (Rasmussen & Williams, 2006) with a covarince kernel κ :
X × X → R and that  ∼ N (0, η 2 ). Two commonly
used kernels are the squared exponential (SE) κσ,h and the
Matérn κν,h kernels with parameters (σ, h) and (ν, h) respectively. Writing r = kx − x0 k2 , they are defined as
 2
−r
0
κσ,h (x, x ) = σ exp
,
(2)
2h2
!
!
√
√
ν
21−ν
2νr
2νr
0
κν,h (x, x ) =
Bν
.
(3)
Γ(ν)
h
h
A principal convenience in modelling our problem via a
GP is that posterior distributions are analytically tractable.

In keeping with this, we will assume that each f (j) is
sampled from a GP, GP(µ(j) , κ(j) ) where the f (j) ’s
are independent. Here, µ(j) : X (j) → R is the mean
and κ(j) : X (j) × X (j) → R is the covariance for
f (j) . W.l.o.g let µ(j) = 0 for all j. This implies that
f itself is sampled from a GP with an additive kernel
P
0
κ(x, x0 ) = j κ(j) (x(j) , x(j) ). We state this formally for
nonzero mean as we will need it for the ensuing discussion.
Observation 1. Let f be defined as in Equation (1), where
0
f (j) ∼ GP(µ(j) (x), κ(j) (x(i) , x(j) )). Let y = f (x) + 
where  ∼ N (0, η 2 ). Denote δ(x, x0 ) = 1 if x =
x0 , and 0 otherwise. Then y ∼ GP(µ(x), κ(x, x0 ) +
η 2 δ(x, x0 )) where
µ(x) = µ(1) (x(1) ) + · · · + µ(M ) (x(M ) )
0

κ(x, x ) = κ

(1)

(x

(1)

(1) 0

,x

(M )

) + ··· + κ

(4)
(M )

(x

(M ) 0

,x

).

We will call a kernel such as κ(j) which acts only on d
variables a dth order kernel. A kernel which acts on all
the variables is a Dth order kernel. Our kernel for f is a
sum of M at most dth order kernels which, we will show,
is statistically simpler than a Dth order kernel.
We conclude this section by looking at some seemingly
straightforward approaches to tackle the problem. The first
natural question is of course why not directly run GP-UCB
using the additive kernel? Since it is simpler than a Dth order kernel we can expect statistical gains. While this is true,
it still requires optimising ϕt in D dimensions to determine
the next point which is expensive.
Alternatively, for an additive function, we could adopt a sequential approach where we use 1/M fraction of our query
budget to maximise the first group by keeping the rest of
the coordinates constant. Then we proceed to the second
group and so on. While optimising a d dimensional acquisition function is easy, this approach is not desirable for
several reasons. First, it will not be an anytime algorithm as
we will have to pre-allocate our query budget to maximise
each group. Once we proceed to a new group we cannot
come back and optimise an older one. Second, such an approach places too much faith in the additive assumption.
We will only have explored M d-dimensional hyperplanes
in the entire space. Third, it is not suitable as a bandit algorithm as we suffer high regret until we get to the last group.
We further elaborate on the deficiencies of this and other
sequential approaches in Appendix A.2.

Additive Gaussian Process Optimisation and Bandits

constructed using the posterior GP. The GP-UCB acquisition function, which we focus on here is,
1/2

ϕt (x) = µt−1 (x) + βt

Figure 1. Illustration of the additive GP model for 2 observations where M = 2 in (1). The squared variables are observed
while the circled variables are not. For brevity we have denoted
(j)
(j)
fi = f (j) (xi ) for i = 1, 2, ∗. We wish to infer the posterior
(j)
distributions of the individual GPs f (j) (x∗ ) (outlined in blue).

4. Algorithm
Under an additive assumption, our algorithm has two components. First, we obtain the posterior GP for each f (j) using the query-value pairs until time t. Then we maximise a
d dimensional GP-UCB-like acquisition function on each
GP to construct the next query point.
4.1. Inference on Additive GPs
Typically in GPs, given noisy labels, Y = {y1 , . . . , yn } at
points X = {x1 , . . . , xn }, we are interested in inferring
the posterior distribution for f∗ = f (x∗ ) at a new point
x∗ . In our case though, we will be primarily interested in
(j)
(j)
the distribution of f∗ = f (j) (x∗ ) conditioned on X, Y .
We have illustrated this graphically in Figure 1. The joint
(j)
distribution of f∗ and Y can be written as
"
#!
 (j) 
(j)
(j)
(j)
κ(j) (x∗ , x∗ ) κ(j) (x∗ , X (j) )
f∗
∼ N 0, (j) (j) (j)
.
Y
κ (X , x∗ ) κ(X, X) + η 2 In
(j)

(j)

(j)

The pth element of κ(j) (X (j) , x∗ ) ∈ Rn is κ(xp , x∗ )
and the (p, q)th element of κ(X, X) ∈ Rn×n is
(i)
κ(xp , xq ). We have used the fact Cov(f∗ , yp ) =
P
(i)
(i)
(j)
(i)
Cov(f∗ , j f (j) (xp ) + η 2 ) = Cov(f∗ , f (i) (xp )) =
(i)

(i)

κ(i) (x∗ , xp ) as f (j) ⊥ f (i) , ∀i 6= j. By writing ∆ =
(j)
κ(X, X) + η 2 In ∈ Rn×n , the posterior for f∗ is,
(j)

(j)

f∗ |x∗ , X, Y ∼ N κ(j) (x∗ , X (j) )∆−1 Y,

(5)

(j)
(j)
(j)
κ(j) (x∗ , x∗ ) − κ(j) (x∗ , X (j) )∆−1 κ(j) (X, x(j) )

4.2. The Add-GP-UCB Algorithm
In GPB/ BO algorithms, at each time step t we maximise
an acquisition function ϕt to determine the next point:
xt = argmaxx∈X ϕt (x). The acquisition function is itself

σt−1 (x).

Intuitively, the µt−1 term in the GP-UCB objective prefers
points where f is known to be high, the σt−1 term prefers
1/2
points where we are uncertain about f and βt negotiates
the tradeoff. The former contributes to the “exploitation”
facet of our problem, in that we wish to have low instantaneous regret. The latter contributes to the “exploration”
facet since we also wish to query at regions we do not know
much about f lest we miss out on regions where f is high.
We provide a brief summary of GP-UCB and its theoretical properties in Appendix A.1.
As we have noted before, maximising ϕt which is typically multimodal to obtain xt is itself a difficult problem.
In any grid search or branch and bound methods such as DiRect, maximising a function to within ζ accuracy, requires
O(ζ −D ) calls to ϕt . Therefore, for large D maximising
ϕt is extremely difficult. In practical settings, especially in
situations where we are computationally constrained, this
poses serious limitations for GPB/ BO as we may not be
able to optimise ϕt to within a desired accuracy.
Fortunately, in our setting we can be more efficient. We
propose an alternative acquisition function which applies to
an additive kernel. We define the Additive Gaussian Process Upper Confidence Bound (Add-GP-UCB) to be
M
X
1/2
(j)
ϕ
et (x) = µt−1 (x) + βt
σt−1 (x(j) ).
(6)
j=1

We immediately see that we can write ϕ
et as a sum of
P (j) (j)
functions on orthogonal domains: ϕ
et (x) = j ϕ
et (x )
(j)

(j)

1/2 (j)

where ϕ
et (x(j) ) = µt−1 (x(j) ) + βt σt−1 (x(j) ). This
means that ϕ
et can be maximised by maximising each
(j)
ϕ
et separately on X (j) . As we need to solve M at
most d dimensional optimisation problems, it requires only
O(M d+1 ζ −d ) calls to the utility function in total – far more
favourable than maximising ϕt .
Since the cost for maximising the acquisition function is a
key theme in this paper let us delve into this a bit more. One
call to ϕt requires O(Dt2 ) effort. For ϕ
et we need M calls
et require
each requiring O(dj t2 ) effort. So both ϕt and ϕ
the same effort in this front. For ϕt , we need to know the
posterior for only f whereas for ϕ
et we need to know the
posterior for each f (j) . However, the brunt of the work in
obtaining the posterior is the O(t3 ) effort in inverting the
t×t matrix ∆ in (5) which needs to be done for both ϕt and
ϕ
et . For ϕ
et , we can obtain the inverse once and reuse it M
times, so the cost of obtaining the posterior is O(t3 +M t2 ).
Since the number of queries needed will be super linear in
D and hence M , the t3 term dominates. Therefore obtaining each posterior f (j) is only marginally more work than

Additive Gaussian Process Optimisation and Bandits

obtaining the posterior for f . Any difference here is easily
offset by the cost for maximising the acquisition function.
The question remains then if maximising ϕ
et would result in
low regret. Since ϕt and ϕ
et are neither equivalent nor have
the same maximiser it is not immediately apparent that this
should work. Nonetheless, intuitively this seems like a reaP (j)
sonable scheme since the j σt−1 term captures some notion of the uncertainty and contributes to exploration. In
Theorem 5 we show that this intuition is reasonable – maximising ϕ
et achieves the same rates as ϕt for cumulative and
simple regrets if the kernel is additive.
We summarise the resulting algorithm in Algorithm 1. In
brief, at time step t, we obtain the posterior distribution for
(j)
(j)
f (j) and maximise ϕ
et to determine the coordinates xt .
We do this for each j and then combine them to obtain xt .
Algorithm 1 Add-GP-UCB
Input: Kernels κ(1) , . . . , κ(M ) , Decomposition (X (j) )M
j=1
• D0 ← ∅,
(j)
(j)
• for j = 1, . . . , M , (µ0 , κ0 ) ← (0, κ(j) ).
• for t = 1, 2, . . .
1. for j = 1, . . . , M ,
√
(j)
(j)
(j)
xt ← argmaxz∈X (j) µt−1 (z) + βt σt−1 (z)
SM (j)
2. xt ← j=1 xt .
3. yt ← Query f at xt .
4. Dt = Dt−1 ∪ {(xt , yt )}.
5. Perform Bayesian posterior updates conditioned
(j)
(j)
on Dt to obtain µt , σt for j = 1, . . . , M .
4.3. Main Theoretical Results
Now, we present our main theoretical contributions. We
bound the regret for Add-GP-UCB under different kernels. Following Srinivas et al. (2010), we first bound the
statistical difficulty of the problem as determined by the
kernel. We show that under additive kernels the problem
is much easier than when using a full Dth order kernel.
Next, we show that the Add-GP-UCB algorithm is able to
exploit the additive structure and obtain the same rates as
GP-UCB. The advantage to using Add-GP-UCB is that
it is much easier to optimise the acquisition function. For
our analysis, we will need Assumption 2 and Definition 3.
Assumption 2. Let f be sampled from a GP with kernel κ.
κ(·, x) is L-Lipschitz for all x. Further, the partial derivatives of f satisfies the following high probability bound.
There exists constants a, b > 0 such that,


 ∂f (x) 
2


P sup 
 > J ≤ ae−(J/b) .
∂x
x
i
The Lipschitzian condition is fairly mild and the latter
condition holds for four times differentiable stationary

kernels such as the SE and Matérn kernels for ν > 2
(Ghosal & Roy, 2006). Srinivas et al. (2010) showed
that the statistical difficulty of GPB/ BO is determined
by the Maximum Information Gain as defined below. We
bound this quantity for additive SE and Matérn kernels in
Theorem 4. This is our first main theorem.
Definition 3. (Maximum Information Gain) Let f ∼
GP(µ, κ), yi = f (xi ) +  where  ∼ N (0, η 2 ). Let
A = {x1 , . . . , xT } ⊂ X be a finite subset, fA denote the
function values at these points and yA denote the noisy observations. Let I be the Shannon Mutual Information. The
Maximum Information Gain between yA and fA is
γT =

max
A⊂X ,|A|=T

I(yA ; fA ).

Theorem 4. Assume that the kernel κ has the additive form
of (4), and that each κ(j) satisfies Assumption 2. W.l.o.g
assume κ(x, x0 ) = 1. Then,
1. If each κ(j) is a dth
j order squared exponential kernel (2) where dj ≤ d, then γT ∈ O(Ddd (log T )d+1 ).
2. If each κ(j) is a dth
order Matérn kernel (3)
j
where dj ≤ d and ν > 2, then γT ∈
d(d+1)

O(D2d T 2ν+d(d+1) log(T )).
The proof is given in Appendix B.1.
The important
observation is that the dependence on D is linear for an
additive kernel. In contrast, for a Dth order kernel this is
exponential (Srinivas et al., 2010). Next, we present our
second main theorem which bounds the regret for AddGP-UCB for an additive kernel as given in Equation 4.
Theorem 5. Suppose f is constructed by sampling f (j) ∼
GP(0, κ(j) ) for j = 1, . . . , M and then adding them. Let
all kernels κ(j) satisfy assumption 2 for some L, a, b. Further, we maximise the acquisition function ϕ
et to within
ζ0 t−1/2 accuracy at time step t. Pick δ ∈ (0, 1) and choose



M π 2 t2
+ 2d log Dt3 ∈ O (d log t) .
βt = 2 log
2δ
Then,
p Add-GP-UCB
 attains cumulative regret RT ∈
O
DγT T log T and hence simple regret ST ∈
p

O
DγT log T /T . Precisely, with probability > 1 − δ,
∀T ≥ 1,

RT ≤

p

√
8C1 βT M T γt + 2ζ0 T + C2 .

where C1 = 1/ log(1 + η −2 ) and C2 is a constant depending on a, b, D, δ, L and η.
Our proof uses ideas from Srinivas et al. (2010). We also
show that complete maximisation of ϕ
et is not required provided that the accuracy improves at rate O(t1/2 ). The proof

Additive Gaussian Process Optimisation and Bandits

is given in Appendix B.2. When we combine the results
in Theorems 4 and 5 we obtain the rates given in Table 1.
One could consider alternative lower order kernels – one
candidate is the sum of all possible dth order kernels (Duvenaud et al., 2011). Such a kernel would arguably allow us to represent a larger class of functions than our
kernel in (4). If, for instance, we choose each of them
to be a SE kernel, then it can be shown that γT ∈
O(Dd dd+1 (log T )d+1 ). Even though this is worse than our
kernel in poly(D) factors, it is still substantially better than
using a Dth order kernel. However, maximising the corresponding utility function, either of the form ϕt or ϕ
et , is still
a D dimensional problem. We reiterate that what renders
our algorithm attractive in large D is not just the statistical
gains due to the simpler kernel. It is also the fact that our
acquisition function can be efficiently maximised.
4.4. Practical Considerations
Our practical implementation differs from our theoretical
analysis in the following aspects.
Choice of βt : βt as specified by Theorems 5, usually tends
to be conservative in practice (Srinivas et al., 2010). For
good empirical performance a more aggressive strategy is
required. In our experiments, we set βt = 0.2d˜log(2t)
which offered a good tradeoff between exploration and exploitation. Here d˜ is the dimension of the space in which
we are optimising the acquisition. Note that this captures
the correct dependence on D, d and t in Theorems 5 and 6.
Data dependent prior: Our analysis assumes that we
know the GP kernel of the prior. In reality this is rarely the
case. In our experiments, we choose the hyperparameters
of the kernel by maximising the GP marginal likelihood
(Rasmussen & Williams, 2006) every Ncyc iterations.
Initialisation: Marginal likelihood based kernel tuning
can be unreliable with few data points. This is a problem in
the first few iterations. Following the recommendations in
Bull (2011) we initialise Add-GP-UCB (and GP-UCB)
using Ninit points selected uniformly at random.
Decomposition & Non-additive functions: If f is additive and the decomposition is known, we use it directly.
But it may not always be known or f may not be additive. Then, we could treat the decomposition as a hyperparameter of the additive kernel and maximise the marginal
likelihood w.r.t the decomposition. However, given that
there are D!/d!M M ! possible decompositions, computing the marginal likelihood for all of them is infeasible.
We circumvent this issue by randomly selecting a few
(O(D)) decompositions and choosing the one with the
largest marginal likelihood. Intuitively, if the function is
not additive, with such a “partial maximisation” we can
hope to capture some existing marginal structure in f . At

the same time, even an exhaustive maximisation will not do
much better than a partial maximisation if there is no additive structure. Empirically, we found that partially optimising for the decomposition performed slightly better than
using a fixed decomposition or a random decomposition
at each step. We incorporate this procedure for finding an
appropriate decomposition as part of the kernel hyper parameter learning procedure every Ncyc iterations.
How do we choose (d, M ) when f is not additive? If d is
large we allow for richer class of functions, but risk high
variance. For small d, the kernel is too simple and we have
high bias but low variance – further optimising ϕ
et is easier.
In practice we found that our procedure was fairly robust
for reasonable choices of d. Yet this is an interesting theoretical question. We also believe it is a difficult one. Using
the marginal likelihood alone will not work as the optimal
choice of d also depends on the computational budget for
optimising ϕ
et . We hope to study this question in future
work. For now, we give some recommendations at the end.
Our modified algorithm with these practical considerations
is given below. Observe that in this specification if we use
d = D we have the original GP-UCB algorithm.
Algorithm 2 Practical-Add-GP-UCB
Input: Ninit , Ncyc , d, M
• D0 ← Ninit points chosen uniformly at random.
• for t = 1, 2, . . .
1. if (t mod Ncyc = 0), Learn the kernel hyper
parameters and the decomposition {Xj } by maximising the GP marginal likelihood.
2. Perform steps 1-3 in Algorithm 1 with βt =
0.2d log 2t.
3. Dt = Dt−1 ∪ {(xt , yt )}.
4. Perform Bayesian posterior updates conditioned
(j)
(j)
on Dt to obtain µt , σt for j = 1, . . . , M .

5. Summary of Experiments
In this summary we present results in the optimisation setting. Refer Appendix C for results on bandits. Following,
Brochu et al. (2010) we use DiRect to maximise ϕt , ϕ
et . To
demonstrate the efficacy of Add-GP-UCB we optimise the
acquisition function under a constrained budget. We compare Add-GP-UCB against GP-UCB, random querying
(RAND) and DiRect. On the real datasets we also compare
it to the Expected Improvement (GP-EI) acquisition function which is popular in BO applications and the method of
Wang et al. (2013) which uses a random projection before
applying BO (REMBO). We have multiple instantiations of
Add-GP-UCB for different values for (d, M ).
In contrast to existing literature in the BO community,

Additive Gaussian Process Optimisation and Bandits
(D,d’,M’) = (10,3,3)

1

10

2

10

1

S

S

T

10

(D,d’,M’) = (24,11,2)

T

RAND
DiRect
Add−*
GP−UCB
Add−1/10
Add−5/2
Add−3/4

2

10

RAND
DiRect
Add−*
10
GP−UCB
Add−1/24
Add−3/8
Add−6/4
0 Add−12/2
200
400
600
800
Number of Queries (T)
0

0

10

0

(D,d’,M’) = (40,18,2)

200
400
600
800
Number of Queries (T)
(D,d’,M’) = (96,5,19)

(D,d’,M’) = (96,29,3)
3

10

3

10
2

10

T

2

10

0

RAND
DiRect
Add−*
GP−UCB
Add−4/24
Add−8/12
Add−16/6
Add−32/3
200
400
600
800
Number of Queries (T)

10

S

T

RAND
DiRect
Add−*
GP−UCB
Add−1/40
0
10
Add−4/10
Add−10/4
0 Add−20/2
200
400
600
800
Number of Queries (T)
10

S

S

T

2

1

RAND
DiRect
Add−*
10
GP−UCB
Add−4/24
Add−8/12
0 Add−32/3
200
400
600
800
Number of Queries (T)
1

Figure 2. Results on the synthetic experiments. The x-axis is the number of queries and the y-axis is the regret in log scale. We have
indexed the experiments by their (D, d0 , M 0 ) values. In some figures, the error bars are not visible since they are small and hidden by
the bullets. All figures were produced by averaging over 20 runs.

we found that the UCB acquisitions outperformed GP-EI.
One possible reason may be that under a constrained budget, UCB is robust to imperfect maximisation (Theorem 5)
whereas GP-EI may not be. Another reason may be our
choice of constants in UCB (Section 4.4).
5.1. Simulations on Synthetic Functions
We create a series of additive functions by replicating a
d0 dimensional function fd0 in M 0 groups. (We use the
prime to avoid confusion with our Add-GP-UCB instantiations with different (d, M ) values.) So the function
doesn’t depend on D − d0 M 0 coordinates. We have illustrated fd0 for d0 = 2 in the first figure in Fig 2 (See Eq (14)
0
in C.1). Since each fd0 has 3 modes, the function has 3M
modes. In the synthetic experiments we use an instantiation of Add-GP-UCB that knows the decomposition–i.e.
(d, M ) = (d0 , M 0 ) and the grouping of coordinates. We
refer to this as Add-?. For the rest we use a (d, M ) decomposition by creating M groups of size at most d and
find a good grouping by partially maximising the marginal
likelihood (Section 4.4). We refer to them as Add-d/M .
For GP-UCB and GP-EI we allocate a budget of
min(5000, 100D) DiRect function evaluations to optimise
the acquisition function. For all Add-d/M methods we set
it to 90% of this amount to account for the additional overhead in posterior inference for each f (j) . While the 90%
seems arbitrary, in our experiments this was hardly a factor

as the cost was dominated by the inversion of ∆. Therefore,
for our 10D problem we maximise ϕt with βt = 2 log(2t)
with 1000 evaluations whereas for Add-5/2 we maximise
(j)
each ϕ
et with βt = log(2t) with 450 evaluations.
We refer to each example by the configuration of the additive function–its (D, d0 , M 0 ) values. In the (10, 3, 3) example Add-? does best since it knows the correct model
and the acquisition function can be maximised within the
budget. However Add-3/4 and Add-5/2 models do well
too and outperform GP-UCB. Add-1/10 performs poorly
since it is statistically not expressive enough to capture the
true function (high bias). In the (24, 11, 2), (40, 18, 2) and
(96, 29, 3) examples Add-? outperforms GP-UCB. However, it is not competitive with the Add-d/M for small
d. Even though Add-? knows the correct decomposition, there are two possible failure modes since d0 is large.
The variance is very high in the absence of sufficient data
points. In addition, optimising the acquisition function is
also difficult. This illustrates our previous argument that
using an additive kernel can be advantageous even on nonadditive functions. In the (40, 5, 8), (96, 5, 19) examples
Add-? performs best as d0 is small enough. But again, almost all Add-d/M instantiations outperform GP-UCB. In
contrast to the small D examples, for large D, GP-UCB
and Add-d/M with large d perform worse than DiRect.
This is probably because the acquisition cannot be maximised to sufficient accuracy within the budget. We have

Additive Gaussian Process Optimisation and Bandits
1

tween being statistically expressive enough to capture the
function while at the same time being easy enough to optimise the acquisition function within the allocated budget.

Maximum Value

−10

2

−10

3

−10

0

100
200
300
Number of Queries (T)

RAND
DiRect
GP−EI
REMBO−9
GP−UCB
Add−1/20
Add−2/10
Add−4/5
Add−5/4
Add−10/2
400

Classification Accuracy

(a)
95
90
85
80
75
70
65
0

100
200
Number of Queries (T)

OpenCV
RAND
DiRect
GP−EI
REMBO−5
GP−UCB
Add−1/22
Add−4/6
Add−6/4
Add−8/3
Add−11/2
300

(b)
Figure 3. Results on the Astrophysical experiment (a) and the Viola and Jones dataset (b). The x-axis is the number of queries and
the y-axis is the maximum value.

only presented a subset of our simulations here. Please see
Appendix C.1 for more experiments and other details.
5.2. Real Experiments
SDSS Galaxy Data: Here, we use galaxy data from the
Sloan Digital Sky Survey to find the maximum likelihood
values for 20 cosmological parameters. The likelihood is
computed via an astrophysical simulation. Software is obtained from Tegmark et al (2006). Each query to the likelihood takes 2-5 seconds. The likelihood only depends on
9 of the parameters but we augment it to 20 dimensions to
emulate the fact that in real astrophysical applications we
may not know the relevant parameters. In order to be wall
clock time competitive with RAND and DiRect we use 500
evaluations for GP-UCB, GP-EI and REMBO and 450 for
Add-d/M to maximise the acquisition function. We have
elaborated more details in Appendix C.2. The results are
given in 3(a). Despite the fact that the function may not be
additive, all Add-d/M methods outperform GP-UCB and
GP-EI. Since the function only depends on 9 parameters
we used REMBO with a 9 dimensional projection. Despite
this advantage to REMBO it is not as competitive with the
Add-d/M methods. Here Add-5/4 performs slightly better than the rest since it seems to have the best tradeoff be-

Viola & Jones Face Detection: The Viola & Jones Cascade Classifier (VJ) (Viola & Jones, 2001) is a popular
method for face detection in computer vision based on the
Adaboost algorithm. In this experiment we use the VJ
face dataset and the OpenCV implementation (Bradski &
Kaehler, 2008) which implements the classifier as a 22stage cascade. The task is to find the 22 threshold values
for each stage to maximise classification accuracy. Each
function call takes 30-40 seconds and is the the dominant
cost in this experiment. We use 1000 DiRect evaluations
to optimise the acquisition function for GP-UCB, GP-EI
and REMBO and 900 for Add-d/M . We use REMBO
with a 5 dimensional projection. The results are given in
Figure 3(b). Not surprisingly, REMBO performs worst
since it is searching only on a 5 dimensional space. Barring Add-1/22 all other Add-d/M instantiations outperform GP-UCB and GP-EI with Add-6/4 performing best.
Interestingly, we also find a configuration for the thresholds
that outperforms the one used in OpenCV.

6. Conclusion
Recommendations: Based on our experiences, we recommend the following. If f is known to be additive, the
decomposition is known and d0 is small enough so that ϕ
et
can be efficiently optimised, then running Add-GP-UCB
with the known decomposition is likely to produce the best
results. If not, then use a small value for d and run AddGP-UCB while partially optimising for the decomposition
periodically (Section 4.4). In our experiments we found
that using d between 3 an 12 seemed reasonable choices.
However, note that this depends on the computational budget for optimising the acquisition, the query budget for f
and to a certain extent the the function f itself.
Summary: Our algorithm takes into account several practical considerations in real world GPB/ BO applications
such as computational constraints in optimising the acquisition and the fact that we have to work with a relatively
few data points since function evaluations are expensive.
Our framework effectively addresses these concerns without considerably compromising on the statistical integrity
of the model. We believe that this provides a promising
direction to scale GPB/ BO methods to high dimensions.
Future Work: Our experiments indicate that our methods
perform well beyond the scope suggested by our theory.
Developing an analysis that takes into account the biasvariance and computational tradeoffs in approximating and
optimising a non-additive function via an additive model
is an interesting challenge. We also intend to extend this
framework to other acquisition functions.

Additive Gaussian Process Optimisation and Bandits

Acknowledgements
We wish to thank Akshay Krishnamurthy and Andrew
Gordon Wilson for the insightful discussions and Andreas
Krause, Sham Kakade and Matthias Seeger for the helpful email conversations. This research is partly funded by
DOE grant DESC0011114.

References

Ghosal, Subhashis and Roy, Anindya. Posterior consistency of Gaussian process prior for nonparametric binary
regression”. Annals of Statistics, 2006.
Gonzalez, Javier, Longworth, Joseph, James, David, and
Lawrence, Neil. Bayesian Optimization for Synthetic
Gene Design. In NIPS Workshop on Bayesian Optimization in Academia and Industry, 2014.

Auer, Peter. Using Confidence Bounds for Exploitationexploration Trade-offs. J. Mach. Learn. Res., 2003.

Györfi, László, Kohler, Micael, Krzyzak, Adam, and Walk,
Harro. A Distribution Free Theory of Nonparametric Regression. Springer Series in Statistics, 2002.

Azimi, Javad, Fern, Alan, and Fern, Xiaoli Z. Batch
Bayesian Optimization via Simulation Matching. In Advances in Neural Information Processing Systems, 2010.

Hastie, T. J. and Tibshirani, R. J. Generalized Additive
Models. London: Chapman & Hall, 1990.

Bergstra, James S., Bardenet, Rémi, Bengio, Yoshua, and
Kégl, Balázs. Algorithms for Hyper-Parameter Optimization. In Advances in Neural Information Processing
Systems, 2011.
Bradski, Gary and Kaehler, Adrian. Learning OpenCV.
O’Reilly Media Inc., 2008.
Brochu, Eric, Cora, Vlad M., and de Freitas, Nando. A
Tutorial on Bayesian Optimization of Expensive Cost
Functions, with Application to Active User Modeling
and Hierarchical Reinforcement Learning. CoRR, 2010.
Bull, Adam D. Convergence Rates of Efficient Global Optimization Algorithms. Journal of Machine Learning Research, 2011.
Chen, Bo, Castro, Rui, and Krause, Andreas. Joint Optimization and Variable Selection of High-dimensional
Gaussian Processes. In Int’l Conference on Machine
Learning, 2012.
de Freitas, Nando. Talk on Current Challenges and Open
Problems in Bayesian Optimization, 2014.
de Freitas, Nando, Smola, Alex J., and Zoghi, Masrour.
Exponential Regret Bounds for Gaussian Process Bandits with Deterministic Observations. In International
Conference on Machine Learning, 2012.
Denil, Misha, Bazzani, Loris, Larochelle, Hugo, and
de Freitas, Nando. Learning Where to Attend with
Deep Architectures for Image Tracking. Neural Comput., 2012.
Djolonga, Josip, Krause, Andreas, and Cevher, Volkan.
High-Dimensional Gaussian Process Bandits. In Advances in Neural Information Processing Systems, 2013.
Duvenaud, David K., Nickisch, Hannes, and Rasmussen,
Carl Edward. Additive gaussian processes. In Advances
in Neural Information Processing Systems, 2011.

Hoffman, Matthew D., Brochu, Eric, and de Freitas,
Nando. Portfolio Allocation for Bayesian Optimization.
In Uncertainty in Artificial Intelligence, 2011.
Hornby, G. S., Globus, A., Linden, D.S., and Lohn, J.D.
Automated Antenna Design with Evolutionary Algorithms. American Institute of Aeronautics and Astronautics, 2006.
Jones, D. R., Perttunen, C. D., and Stuckman, B. E. Lipschitzian Optimization Without the Lipschitz Constant. J.
Optim. Theory Appl., 1993.
Jones, Donald R., Schonlau, Matthias, and Welch,
William J. Efficient global optimization of expensive
black-box functions. J. of Global Optimization, 1998.
Kandasamy, Kirthevasan, Schneider, Jeff, and Póczos,
Barnabás. Bayesian Active Learning for Posterior Estimation. In International Joint Conference on Artificial
Intelligence, 2015.
Lizotte, Daniel, Wang, Tao, Bowling, Michael, and Schuurmans, Dale. Automatic gait optimization with gaussian
process regression. In in Proc. of IJCAI, pp. 944–949,
2007.
Ma, Yifei, Sutherland, Dougal J., Garnett, Roman, and
Schneider, Jeff G. Active Pointillistic Pattern Search. In
International Conference on Artificial Intelligence and
Statistics, AISTATS, 2015.
Mahendran, Nimalan, Wang, Ziyu, Hamze, Firas, and
de Freitas, Nando. Adaptive MCMC with Bayesian Optimization. In Artificial Intelligence and Statistics, 2012.
Martinez-Cantin, R., de Freitas, N., Doucet, A., and Castellanos, J. Active Policy Learning for Robot Planning
and Exploration under Uncertainty. In Proceedings of
Robotics: Science and Systems, 2007.

Additive Gaussian Process Optimisation and Bandits

Mockus, J.B. and Mockus, L.J. Bayesian approach to
global optimization and application to multiobjective
and constrained problems. Journal of Optimization Theory and Applications, 1991.
Mockus, Jonas. Application of Bayesian approach to numerical methods of global and stochastic optimization.
Journal of Global Optimization, 1994.
Osborne, M., Duvenaud, D., Garnett, R., Rasmussen, C.,
Roberts, S., and Ghahramani, Z. Active Learning of
Model Evidence Using Bayesian Quadrature. In Neural
Information Processing Systems (NIPS), 2012.
Parkinson, David, Mukherjee, Pia, and Liddle, Andrew R.
A Bayesian model selection analysis of WMAP3. Physical Review, 2006.
Rasmussen, C.E. and Williams, C.K.I. Gaussian Processes
for Machine Learning. Adaptative computation and machine learning series. University Press Group Limited,
2006.
Ravikumar, Pradeep, Lafferty, John, Liu, Han, and Wasserman, Larry. Sparse Additive Models. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 2009.
Seeger, MW., Kakade, SM., and Foster, DP. Information
Consistency of Nonparametric Gaussian Process Methods. IEEE Transactions on Information Theory, 2008.
Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P.
Practical Bayesian Optimization of Machine Learning
Algorithms. In Advances in Neural Information Processing Systems, 2012.
Srinivas, Niranjan, Krause, Andreas, Kakade, Sham, and
Seeger, Matthias. Gaussian Process Optimization in the
Bandit Setting: No Regret and Experimental Design. In
International Conference on Machine Learning, 2010.
Tegmark et al, M. Cosmological Constraints from the
SDSS Luminous Red Galaxies. Physical Review, December 2006.
Thompson, W. R. On the Likelihood that one Unknown
Probability Exceeds Another in View of the Evidence of
Two Samples. Biometrika, 1933.
Viola, Paul A. and Jones, Michael J. Rapid Object Detection using a Boosted Cascade of Simple Features. In
Computer Vision and Pattern Recognition, 2001.
Wang, Ziyu, Zoghi, Masrour, Hutter, Frank, Matheson,
David, and de Freitas, Nando. Bayesian Optimization in
High Dimensions via Random Embeddings. In International Joint Conference on Artificial Intelligence, 2013.

Yamins, Daniel, Tax, David, and Bergstra, James S. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures. In International Conference on Machine Learning, 2013.

