Ordinal Mixed Membership Models

Seppo Virtanen
Mark Girolami
Department of Statistics, University of Warwick, CV4 7AL Coventry UK

Abstract
We present a novel class of mixed membership
models for joint distributions of groups of observations that co-occur with ordinal response variables for each group for learning statistical associations between the ordinal response variables
and the observation groups. The class of proposed models addresses a requirement for predictive and diagnostic methods in a wide range
of practical contemporary applications. In this
work, by way of illustration, we apply the models
to a collection of consumer-generated reviews of
mobile software applications, where each review
contains unstructured text data accompanied with
an ordinal rating, and demonstrate that the models infer useful and meaningful recurring patterns
of consumer feedback. We also compare the developed models to relevant existing works, which
rely on improper statistical assumptions for ordinal variables, showing significant improvements
both in predictive ability and knowledge extraction.

1. Introduction
There exist large repositories of user-generated assessment,
preference or review data consisting of free-form text data
associated with ordinal variables for quality or preference.
Examples include product reviews, user feedback, recommendation systems, expert assessments, clinical records,
survey questionnaires, economic or health status reports,
to name a few. The ubiquitous need to statistically model
the underlying processes and analyse such data collections
presents significant methodological research challenges necessitating the development of proper statistical models and
inference approaches.
In this work, our interest focuses on, but is not limited
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

S . VIRTANEN @ WARWICK . AC . UK
M . GIROLAMI @ WARWICK . AC . UK

to, analysing reviews of mobile software applications provided by consumers. Such analysis is useful for both software developers and consumers, inferring and understanding themes or properties of mobile applications that consumers comment about. These themes may involve consumersâ€™ preferences and experiences on properties they
(dis)appreciate or direct feature requests or problems directed to the software developers.
Our work belongs in the field of mixed membership modelling, which is a powerful and important statistical modelling methodology. Observations are grouped and each
group is modelled with a mixture model; mixture components are common to all groups, whereas mixture proportions are group-specific. The components are deemed to
capture recurring patterns of observations and each group
to exhibit a subset of components. The class of models has
been shown to be able to extract interpretable meaningful
themes, also referred to as topics, based on, for example,
text data (Blei et al., 2003). These models, however, are not
able to capture statistical associations between the groups
and co-occurring quantitative information, that is, response
variables, related to each group.
Previous work on joint models utilising both the textual
data and response variables (Blei & McAuliffe, 2007; Dai
& Storkey, 2015; Lacoste-Julien et al., 2009; Nguyen et al.,
2013; Ramage et al., 2009; Wang et al., 2009) has demonstrated the utility of joint modelling by inferring topics that
are predictive of the response leading to increased interpretability. However, these models lack proper statistical
formulations suitable for ordinal response variables and it
is not at all straightforward to correct this shortcoming. In
this work, we remove this hindrance by presenting a novel
class of joint mixed membership models.
The proposed class of models builds on our new statistical
generative response model for ordinal variables. In more
detail, we introduce a certain stick-breaking formulation to
parameterise underlying data-generating probabilities over
the ordinal variables. The response model contains groupspecific latent scores as well as mean variables that transform the scores into ordinal variables using the developed
construction. We compare the response model with exist-

Ordinal Mixed Membership Models

ing alternatives for ordinal variables (Albert & Chib, 1993;
Chu & Ghahramani, 2005) and show that our formulation
provides favourable statistical properties.
We present two different novel model formulations that
couple the developed response model with mixed membership models. Specifically, the formulations hierarchically couple the latent scores of the response model with
the mixing components of a mixed membership model
either via the mixture proportions or observation assignments capturing associations between the components and
responses. The first construction infers a correlation structure between (as well as, within) the mixture proportions
and latent scores based on the observed data, not enforcing
a priori any correlation structure or specifying which of
the components are associated with the responses. We derive a scalable variational Bayesian inference algorithm to
approximate the model posterior distribution. The model
is motivated by unsupervised correlated topic models by
Blei & Lafferty (2006) and Paisley et al. (2012). The
second construction assumes the latent scores of the response model are given by a weighted linear combination
of the mean assignments over each group, such that the
component-specific combination weights a posteriori provide a means to inspect components that have predictive
value. We present a Markov Chain Monte Carlo (MCMC)
sampling scheme for posterior inference. The model is related to supervised LDA (SLDA; Blei & McAuliffe, 2007);
our model can be seen as an extension of SLDA to ordinal
responses.
We demonstrate the developed models on a collection of
reviews of mobile software applications. We compare the
models to the relevant previous work and show that the
proper ordinal response model is valuable for learning statistical associations between the responses and text data
providing significant improvements in terms of both predictive ability and knowledge extraction by inferring interpretable and useful themes of consumer feedback.
The paper is structured as follows. Section 2 presents
the methodological contributions of this work: Section 2.1
presents our proposed generative model for ordinal variables, whereas the next two Sections 2.2 and 2.3 present
model formulations and inference approaches for joint
mixed membership modelling of groups of observations
and group-specific ordinal response variables. Related
work is reviewed in Section 3. Section 4 describes the experiments and contains the results. Section 5 concludes the
paper.

2. Joint Mixed Membership Models
The mth group of observations w(m) is paired with an ordinal response variable y (m) . The response variables, also

referred to as ratings, take values in R âˆˆ Z+ > 2 ordered categories ranging between poor (1) and excellent
(R). We note that for a simple case, when R = 2,
y (m) is binary and may be modelled by a Bernoulli distribution. The w(m) contains an unordered sequence
(m)
of D(m) words wd over a V -dimensional vocabulary,
(m)
(m)
(m)
w(m) = {w1 , w2 , . . . , wD(m) }.
2.1. Ordinal Response Variables
We assume y (m) is drawn from a categorical distribution
over R categories. The probability that y (m) takes an integer value r âˆˆ {1, . . . , R} is denoted by p(y (m) = r). Since
the categories are ordered, we propose a stick-breaking parameterisation for the probabilities; a unit length stick is
split into R smaller sticks that sum to one. We refer to
(m)
these smaller sticks as stick weights vr for the mth group
(m)
and rth category. We parameterise the vr using a function Ïƒ(Â·) mapping its argument to a value between zero and
one and introducing continuous-valued latent variables or
scores t(m) for each group as well as mean parameters Âµr
for each category. The generative model for the y (m) is
p(y (m) = r) = vr(m)

râˆ’1
Y

(m)

(1 âˆ’ vr0 ),

(1)

r 0 =1
(m)

vr(m) = Ïƒ(t

âˆ’ Âµr ).

(m)

Each vr represents a binary decision boundary, specified
by the mean variables, for the t(m) . The mean variables
are ordered, that is, Âµ1 < Âµ2 < Â· Â· Â· < ÂµR , representing boundaries between the ordered categories. For computational simplicity, we use Ïƒ(x) = (1 + exp(âˆ’x))âˆ’1
corresponding to a logit (or sigmoid) function, for which
1 âˆ’ Ïƒ(x) = Ïƒ(âˆ’x). Alternative choices include probit, log
log or Cauchy functions, to name a few. The stick-breaking
formulation guarantees that the probabilities p(y (m) = r),
for r = 1, . . . , R, are positive and sum to one for any value
of the t(m) . More importantly, the formulation leads to a
simple posterior inference algorithm; the ordering of the
mean variables is implicitly inferred based on the observed
data without enforcing explicit constraints. For identifia(m)
bility, we set, without loss generality, vR = 1. Figure 1
demonstrates the construction of probabilities based on the
t(m) for simulated mean variables Âµ.
Based on a collection of observed responses y (m) , where
m = 1, . . . , M , the model log likelihood is
L=

X
m

(m)
ln(vy(m) )

+

y (m)
Xâˆ’1

(m)

ln(1 âˆ’ vr0 ).

(2)

r 0 =1

Point estimates for the latent scores as well as mean variables may be inferred by maximising the log likelihood using unconstrained gradient-based optimisation techniques.

0.4

0.8

and vk0 are similar, the topics Î· k and Î· k0 , respectively, tend
to co-occur, assuming that Î²Ìƒk and Î²Ìƒk0 are sufficiently large.
We use (normalised) gamma-distributed variables to construct the topic proportions thus parameterising a mapping
from the continuous latent variables to the discrete topic
proportions. For simplified posterior inference we define

0.0

probability

Ordinal Mixed Membership Models

âˆ’10

âˆ’5

0

5

10

score
Figure 1. Visual demonstration of category probabilities. Here,
x-axis denotes a range of values for the latent variable or score
t(m) , whereas the vertical lines denote the category cut-off points,
referred to as mean variables Âµ.

In the following sections, we present two approaches for
parameterising the latent scores constructing statistical associations between the responses and groups. Main statistical interest focuses on the parameterisation, whereas the
mean variables are relevant mainly for computing predictions. For this reason, in the following, we assign a uniform
prior for the mean variables.
2.2. Joint Correlated Topic Model

Î²Ìƒk = Î² exp(mk ).

(5)

The process is
(m)

Î¸k

(m)

âˆ zk


âˆ¼ Gamma Î², exp(âˆ’vkT u(m) âˆ’ mk ) ,

where the Î² denotes the shape parameter and the
exp(âˆ’vkT u(m) âˆ’ mk ) denotes the rate parameter of the
gamma distribution, respectively. We see that
(m)

E[Î¸k ] âˆ Î² exp(vkT u(m) + mk ),
as desired (4), using equation (5)1 . Figure 2 illustrates a
graphical plate diagram of the model.
We complete the model description specifying distributions
for the model hyper-parameters, the root nodes in Figure 2.
We assign

In this section, we present a novel joint model (referred to
as, JTM) for the y (m) and w(m) , where m = 1, . . . , M .
At the core of the model are group-specific latent variables
u(m) that are common for y (m) and w(m) capturing statistical associations between them.

Î² âˆ¼ Gamma(Î±0 , Î²0 ),
u(m) âˆ¼ Normal(0, I)
Î¾, vk âˆ¼ Normal(0, lâˆ’1 I),

For the responses we introduce a linear mapping or projection Î¾ and construct the data-generating latent score (Equation 1) as t(m) = Î¾ T u(m) , computing a cross product between the u(m) and the mapping Î¾.

where l denotes a precision (inverse variance) parameter of
a (zero-mean) Gaussian distribution,

The generative process for the w(m) (groups of observations), for m = 1, . . . , M , is given by

where Î³ is a concentration parameter of a Dirichlet distribution, and a non-informative prior for the mk .

(m)

wd

(m)

cd

âˆ¼ Categorical(Î· c(m) ),

(3)

d

âˆ¼ Categorical(Î¸ (m) ),

where Î· k , for k = 1, . . . , K, denotes mixture components
(m)
(topics), cd , for d = 1, . . . , Dm , denotes observation assignments and Î¸ (m) mixture (topic) proportions over the K
topics.
(m)

2.2.1. I NTERPRETATION
After specifying the model, we highlight the role of the
latent variables and the corresponding mappings for the responses and topics, Î¾ and vk , where k = 1, . . . , K, respectively. We may compute a measure for similarity between
two vectors xi and xj defining a function
xTi xj
l(xi , xj ) = q
(xTi xi )(xTj xj )

(m)

We connect the Î¸
to the latent variables u
by introducing topic-specific mappings vk and gamma-distributed
(m)
variables zk (parameterised suitably) such that a priori
(m)
E[Î¸k ]

Î· k âˆ¼ Dirichlet(Î³1),

âˆ

Î²Ìƒk exp(vkT u(m) ),

(4)

where Î²Ìƒk , for k = 1, . . . , K, are positive concentration parameters. The latent mappings capture statistical associations between any two topics indexed by k and k 0 . If the vk

that outputs a value between 1 and âˆ’1 indicating similarity or dissimilarity between the vectors. We may compute
l(Î¾, vk ), where k = 1, . . . , K, and use the (dis)similarity
1

We note that for a gamma-distributed random variable
ba
x âˆ¼ Gamma(a, b) = Î“(a)
xaâˆ’1 exp(âˆ’bx), where Î“(Â·) denotes
the gamma function, E[x] = a/b.

Ordinal Mixed Membership Models

u(m)

z(m)

Î¾

Î²
vk

(m)
cd

mk

Âµ
y (m)

(m)

wd

Î·k

Figure 2. Graphical plate diagram of the joint correlated topic
model. Unshaded nodes correspond to unobserved variables,
whereas shaded nodes correspond to observed variables. Hyperparameters for the root nodes, whose values need to be fixed prior
to posterior inference, are omitted from the visualisation. Plates
indicate replication over topics, groups and words. The hidden
variables may be divided into local group-specific variables and
global variables common to all groups. That is, the unnormalised
(m)
topic proportions z(m) , topic indicators cj and latent variables
(m)
u
are defined for each group, whereas the set of topics Î· k ,
mappings from latent variables to data domains, Î¾ and vk , are
common to all groups.

scores to infer whether the topics that are positively or negatively associated with excellent or poor ratings.
Next, we present a theoretical justification for the similarity measure. Marginalisation of the latent variables u is
analytically tractable leading to a joint Gaussian distribu(m)
tion for the t(m) and auxiliary variables hk (replacing the
vkT u(m) ). The covariance matrix of the Gaussian distribution is
Î£ = WWT + I,

where WT = Î¾ v1 . . . vK .We see that the similarity values defined above correspond to correlations between the response and topical mappings, respectively. We
also note that the distribution is able to capture correlations
between any two topics. Hence, we refer to this model as
joint correlated topic model.
2.2.2. R EGULARISATION
During posterior inference the model infers statistical associations between the groups and responses. The inferred
topics summarise recurring word co-occurrences over the
corpus into interpretable themes some of which may have
significant associations with the ratings. However, for finite sample sizes the correlation structure may be weak.
Accordingly we introduce a user-defined parameter Î» > 0,
that balances for the limited sample sizes. Even though, we
expect, when the sample size M increases for fixed vocabulary size V , the role of Î» diminishes, since there are more

data to estimate the underlying correlation structure. The
joint likelihood of the model is
(m)

p(D,Î˜) =

M D
K
Y
Y Y

(m)

(m)

(m)

p(wd )p(cd )p(zk

m=1 d=1 k=1
Î»
(m)

exp(L)p(Î¾) p(u

)

)p(vk )p(Î²),

where D = {w(m) , y (m) }M
m=1 , Î˜ denotes unknown quantities of the model and L is given in Equation (2). For Î» < 1
the model focuses more on explaining the text.
2.2.3. VARIATIONAL BAYESIAN I NFERENCE
We present a variational Bayesian (VB) (Wainwright & Jordan, 2008) posterior inference algorithm for the model that
scales well for large data collections and can readily be extended to stochastic online learning (Hoffman et al., 2013).
We approximately marginalise over the topic assignments
and proportions using non-trivial factorised distributions,
whereas we use point distributions (estimates) for several
variables to simplify computations, in essence, adopting an
empirical Bayes approach for these variables. The corresponding inference algorithm is able to prune out irrelevant
topics from the model based on the observed data. Full
variational inference would be possible using techniques
presented by BoÌˆhning (1992); Jaakkola & Jordan (1997)
and Wang & Blei (2013), for example, lower bounding
analytically intractable log sigmoid function appearing in
the log likelihood function (2). Alternatively, MCMC sampling strategies may provide appealing approaches for posterior inference. However, it is far from trivial to design
suitable proposal distributions for the latent variables.
We introduce a factorised posterior approximation
(m)

q(Î˜) =

M D
K
Y
Y Y

(m)

(m)

q(cd )q(zk

),

m=1 d=1 k=1

omitting the point distributions for clarity, and minimise the
KL-divergence between the factorisation q(Î˜) and the posterior p(Î˜|D). Alternatively, we maximise a lower bound
of the model evidence with respect to the parameters of the
q(Î˜),
ln p(D) â‰¥ LV B = E[ln p(D, Î˜)] âˆ’ E[p(Î˜) ln p(Î˜)],
where expectations are taken with respect to the q(Î˜).
We choose the following distributions for the topic assignments and unnormalised topic proportions
(m)

(m)

(m)

q(cd ) = Categorical(cd |Ï†d ),
(m)

q(zk

(m)

) = Gamma(zk

(m)

(m)

|ak , bk ),

Ordinal Mixed Membership Models

where I[Â·] denotes the indicator function equaling 1 if the
argument is true and zero otherwise, representing an empirical topic distribution for the mth group. We use the
quantity to construct a linear mapping to the ratings. The
model (see Figure 3 for an illustration of a graphical plate
diagram) is

whose parameters are
(m)

(m)

Ï†w,k âˆ Î·k,w exp(E[ln zk
(m)
ak

=Î²+

(m)
D
X

]),

(m)

Ï†j,k ,

j=1
(m)

bk

= exp(âˆ’vkT u(m) âˆ’ mk ) + PK

D(m)

(m)
k=1 E[zk ]

t(m) = Î¾ T e
c(m)

.

(m)

wd

In the derivations, we applied Jensenâ€™s inequality lower
PK
(m)
bounding analytically intractable E[ln k=1 zk ] needed
(m)
for normalisation of zk , for k = 1, . . . , K, by introducing additional auxiliary parameters for each group. The expectations appearing above with respect to the variational
factorisation are
(m)

] = Ïˆ(ak ) âˆ’ ln bk ,

(m)

]=

E[ln zk

(m)

(m)

(m)

E[zk

ak

(m)

âˆ¼ Categorical(Î· c(m) ),
d

(m)
cd
(m)

Î¸

âˆ¼ Categorical(Î¸

(m)

),

âˆ¼ Dirichlet(Î±),

Î· k âˆ¼ Dirichlet(Î³1),
Î¾k âˆ¼ Normal(0, Î¶).
Based on the observed data D the model infers a set of
topics that explain not only word co-occurrences but also
the responses.

,

bk

Î¸ (m)

where Ïˆ(Â·) denotes the digamma function.
The lower bound of the model evidence, a cost function to
maximise, with respect to the u(m) is
X
(m)
LVu B = Î»L+
E[ln p(zk |u(m) , vk , m, Î²)]+ln p(u(m) ),
m,k

Î±k

Î¾
(m)

cd

Î·k

Âµ
y

whereas for v, m and Î² the cost function is
X
(m)
B
LVv,m,Î²
=
E[ln p(zk |u(m) , vk , m, Î²)]+ln p(vk , m, Î²).

(m)
wd

(m)

m,k

To infer the mapping Î¾ we maximise LVÎ¾ B = L + ln p(Î¾).
Unconstrained gradient-based optimisation techniques may
be used to infer point estimates for these unobserved quantities (optimising Î² in log-domain). Finally, the topics are
updated as
X (m)
Î·k,w âˆ
Ï†d,k + Î³ âˆ’ 1.

2.3.1. MCMC S AMPLING S CHEME

d,m

2.3. Ordinal Supervised Topic Model
In this section, we propose a novel topic model for the ordinal responses and groups of observations. The model assumes a generative process for the words similar to that in
(m)
Equation 3 introducing topic assignments cd for words
(m)
wd , where d = 1, . . . , D(m) , and topic proportions Î¸ (m)
for the mth group. Here, the generative model for the rat(m)
ings depends on the cd , where d = 1, . . . , D(m) . In more
detail, we define
(m)

e
ck

=

1
D(m)

Figure 3. Graphical plate diagram for the ordinal supervised topic
model. The topic proportions Î¸ (m) are group-specific and generated from an asymmetric Dirichlet distribution. The ordinal gen(m)
erative model for the ratings depends on topic assignments cd ,
that specify the topical content (textual themes via topics Î· k ) of
the mth group.

(m)
D
X

j=1

(m)

I[cj

We present a MCMC sampling scheme for the model. We
consecutively sample the topic assignments given current
value of Î¾ using collapsed Gibbs sampling, building on the
work by Griffiths & Steyvers (2004), analytically marginalising out topics as well as topic proportions. Then, given
the newly sampled assignments we update the value for the
Î¾ as well as the concentration parameters Î±. The topic assignment probabilities are given by
âˆ’c

(m)
p(cd

= k) âˆ

(m)
âˆ’cd

Nk
= k],

(m)

Nw,kd

+Î³

(m)

âˆ’c

(Nk,dd

+ Î±k )Ã—

+VÎ³
(m) D (m)
(m)
}j=1,j6=d , cd

p(y (m) |{cj

= k),

Ordinal Mixed Membership Models
(m)

where Nw,k denotes the counts word w (here, wd = w)
PV
is assigned to the kth topic, Nk = w=1 Nw,k and Nk,d
denotes counts tokens in document d are assigned to the
(m)
kth topic. Upper index âˆ’cd means excluding the current
count. The parameters of the response distribution are inferred by maximising LÎ¾ = L+ln p(Î¾). The concentration
parameters are updated recursively
Î±k = PM

Î±k

PM

m=1 ln

m=1

P

Ïˆ(Nk,m + Î±k ) âˆ’ M Ïˆ(Î±k )
,

P
1
j Î±j )
2 âˆ’ M Ïˆ(

j Nj,m + Î±j âˆ’

building on Minkaâ€™s fixed point iteration (Minka, 2000). In
the denominator, we approximate Ïˆ(x) â‰ˆ ln(x âˆ’ 1/2),
that is accurate when x > 1. This is the case, since all
w(m) , for m = 1, . . . , M , contain at least one word token.
The asymmetric Dirichlet prior enables pruning irrelevant
topics based on the observed data (Wallach et al., 2009).
We note that due to recursive sampling of the topic assignments computational cost of inference may become considerable for large data sets. The recursive property carries also to a corresponding variational Bayesian treatment,
since the topic assignments are dependent on each other.

3. Related Work
Previous works on statistical models for ordinal data (Albert & Chib, 1993; Chu & Ghahramani, 2005) assume
y (m) = j

if

Âµjâˆ’1 < z (m) â‰¤ Âµj ,

z (m) âˆ¼ Normal(t(m) , 1),
where z (m) , for m = 1, . . . , M , denote Gaussiandistributed auxiliary variables. Marginalisation of the z (m)
leads to an ordinal probit model. The corresponding inference algorithm relies on truncated Gaussian distributions
and takes into account explicit ordering constraints for the
mean variables leading to a complicated inference algorithm that is sensitive to initialisation thus potentially leading to local minima.
The original supervised LDA model (SLDA; Blei &
McAuliffe, 2007) uses canonical exponential family distributions for the response model. Under the canonical formulations the expectation of a response variable is
E[y (m) ] = g(t(m) ), where g(Â·) denotes a link function
specific for each member of the family. Examples of the
most common members of this family include Gaussian,
Bernoulli and Poisson distributions suitable for continuousvalued, binary or count variables, respectively. However,
more importantly, the formulation does not support ordinal
variables.
Previous applications of SLDA by Blei & McAuliffe
(2007); Dai & Storkey (2015) and Nguyen et al. (2013) for

ordinal responses, such as product or movie reviews, have
made a strong model mis-specification; they treat ordinal
variables as continuous-valued. In this approach, the ordinal variables are represented as distinct values in the real
domain with arbitrary user-defined intervals between them,
enabling use of a Gaussian response model. The model is
y (m) âˆ¼ Normal(t(m) +Âµ, Ï„ âˆ’1 ), where Âµ is a mean variable
and Ï„ is a precision (inverse variance) parameter. There
are a number of statistical flaws in this approach undermining interpretability. First, we note that the mean parameter of the Gaussian distribution, in general, may lead
to results that make no sense in terms of the ordinal categories, especially for non-equidistant between-category intervals. Second, observed ratings still take discrete values but the predictions will not correspond to these values.
Third, the Gaussian error assumption is not supported by
discrete data.
Wang et al. (2009) present an important and non-trivial extension of SLDA to unordered, that is, nominal response
variables, motivated by classification tasks. The nominal
variables represent logically separate concepts that do not
permit ordering.
Ramage et al. (2009) and Lacoste-Julien et al. (2009)
present alternative joint topic models, where functions of
the nominal response variables (class information) affect
topic proportions. The response variables are not explicitly
modelled using generative formulations. The approach by
Mimno & McCallum (2008) uses a similar model formulation suitable for a wide range of observed response variables (or features, in general) performing linear regression
from the responses, which are treated as covariates, to the
concentration parameters of Dirichlet distributions of the
topic proportions. However, it is not obvious how to use
these formulations for ordinal response variables.

4. Experiments and Results
We collect consumer-generated reviews of mobile software
applications (apps) from Appleâ€™s App Store. The review
data for each app contains an ordinal rating taking values
in five categories ranging from poor to excellent as well as
free-flowing text data. We select the vocabulary using tfidf scores. After simple pre-processing, the data collection
contains M = 5511 apps with
PMvocabulary size V = 3995
and total number of words m=1 D(m) = 1.5 Ã— 106 . The
relatively small data collection is chosen to keep algorithm
running times reasonable especially for the sampling-based
inference approaches.
4.1. Experimental Setting
We compare the joint correlated topic model (JTM; Section 2.2) and ordinal supervised topic model (SLDA) (Sec-

Ordinal Mixed Membership Models

tion 2.3) to SLDA with a Gaussian response model as
adopted in previous work by Blei & McAuliffe (2007); Dai
& Storkey (2015) and Nguyen et al. (2013) (see Section
3 for more details) as well as to sparse ordinal and Gaussian linear regression models. For Gaussian response models we represent the ratings as unit-spaced integers starting
from one. The likelihood-specific parameters for the Gaussian model are mean and precision. We adopt the inference
procedure described in Section 2.3 using collapsed Gibbs
sampling also for Gaussian SLDA. For the regression models we infer a linear combination of the word counts and assign a sparsity-inducing prior distribution for the regression
weights over the vocabulary in order to improve predictive
ability. We maximise the corresponding joint log likelihood
of the model for a fixed prior precision2 . For all the models that use a Gaussian response model, the mean variable
is inferred by computing an empirical response mean. We
initialise the models randomly.
For the joint correlated topic model, referred to as, JTM, we
bound the maximum number of active topics to K = 100,
set dimensionality of the latent variables to L = 30,
Î±0 = 1, Î²0 = 10âˆ’6 and prior precision to l = L.
The results are shown for Î» = 0.001, although, Î» â‰¤ 0.1
provided also good performance with little statistical variation. We terminated the algorithm (both in training and
testing phase), when the relative difference of the (corresponding) lower bound fell below 10âˆ’4 . The SLDA models were also computed for K = 100 and we used Î¶ = 1.
We used 500 sweeps of sampling for inferring the topics
and response parameters. For testing we used 500 sweeps
of collapsed Gibbs sampling. Although we omit formal
time comparisons due to difficulties in comparing VB to
MCMC approaches, we find that the sampling approach
is roughly one order of magnitude slower. In general,
determining convergence for MCMC approaches remains
an open research problem, whereas VB provides a local
bound for model evidence. For all the topic models we
used Î³ = 0.01. For JTM, this (effectively) equals a topic
Dirichlet concentration parameter value Î³ + 1 due to point
estimate shifting the value by minus one. For the regression models we sidestep proper cross-validation of the prior
precision and show results for the values providing the best
performance, potentially leading to over-optimistic results.
4.2. Rating Prediction
We evaluate the models quantitatively in terms of predictive ability. Even though the developed joint mixed membership models are formulated primarily for exploring sta2
We use t(m)
=
Î¾T x(m) , where x(m) denotes
word counts over the V -dimensional vocabulary, and
QV
p(Î¾|) âˆ
d=1 exp(âˆ’ ln(cosh(Î¾d ))), where  denotes a
precision parameter of the prior distribution.

tistical associations between the ratings and text data, they
can readily be used as predictive models. More specifically,
we predict the ordinal rating based on the text. We partition available data into multiple training and test sets using 10-fold cross validation. For each model (and fold) we
compute the test-set log likelihood (probability) of ratings
(the higher, the better) and use these values for comparison.
Despite various predictive criteria have been proposed, the
selected measure is well motivated by statistical modelling.
In the test phase, for JTM, we infer the latent variables u,
topic proportions (unnormalised gamma-distributed vari(m)
(m)
ables zk ) and topic assignments cd given the values
for the remaining parameters inferred in the training phase.
For SLDA models the test phase corresponds to estimating
the topic assignments using standard LDA model algorithm
(using collapsed Gibbs sampling) with fixed topics inferred
based on the training data. Finally, we compute the corresponding latent scores t(m) for the models, obtaining the
predictions.
Table 1 shows the test-set log likelihoods for the models. The ordinal linear regression model resulted in significantly better predictions than the Gaussian regression
model (paired one-sided Wilcoxon; p < 10âˆ’3 ) showing
that it is important to substitute a statistically poorly motivated Gaussian response distribution with a proper generative model. For both models the sparsity assumption improves predictive ability. For the ordinal regression model,
the most relevant words predictive of low (poor) ratings
include waste and free and those of high (excellent) ratings include amazing and perfect. The model, however,
falls short in providing in-depth interpretations, necessitating the use of topic models.
All the topic models perform substantially better than
the regression models. The ordinal SLDA model provides the best predictive performance, JTM is the second
best and Gaussian SLDA is the worst. All (pair-wise)
comparisons are statistically significant (paired one-sided
Wilcoxon; p < 0.005). We discovered K = 100 is a
sufficiently large threshold value for the number of topics; some of the inferred topics are inactive. This, together with good predictive accuracy, establish evidence
the developed models have captured the relevant statistical variation in the observed data. For JTM, we also performed a sensitivity analysis of the dimensionality of the
latent variables L and found little statistical variation for
30 â‰¤ L â‰¤ 100 = K. The test log likelihoods range
between a minimum of âˆ’669.42(9.68) for L = 80 and a
maximum of âˆ’661.98(11.73) for L = 50.
Next, we compared the inferred topics of different models quantitatively using a measure, referred to as, semantic
coherence proposed by Mimno et al. (2011) for quantifying topic trustworthiness. Table 2 shows the average topic

Ordinal Mixed Membership Models
Table 1. Rating prediction test set log likelihoods for different
methods. The table shows values for mean and standard deviation computed over 10 folds obtained by cross-validation.

model

log likelihood

Ordinal SLDA
JTM
Gaussian SLDA
Ordinal regression
Gaussian regression

âˆ’638.53(13.38)
âˆ’667.79(15.91)
âˆ’681.71(17.69)
âˆ’704.30(13.21)
âˆ’735.40(14.70)

One of the topics associated with high ratings (Figure 4)
captures word co-occurrence patterns containing adjectives
with positive semantics. The remaining topics capture
themes customers appreciate, such as games, health monitoring, calculations (for example, for unit conversions),
learning languages, social networking and education. One
of the topics captures positive customer feedback about app
interface and design.
designed
giving disappointment
uninstalled grade

downloading
functional read

advertising
writing terrible
trust horrible

marketing avoid
ten link
dark

coherences (the higher, the better). The topics inferred by
JTM have significantly larger coherence (two sample onesided Wilcoxon, p < 0.0002).

installed
wrote

useless

waste
work

ap minus

gave

compatible function
respond

device

shuts
complete
ago
market

search

trash poorly
dont
completely

effort
deleting
freaking
suppose
froze add shown
pull
sell
advertised

bother
apple

removed

sadly

worthless stars

false
luck
rating put
idiots

deleted garbage

downloaded

needed

instructions
times
wrong

people good

select

expect

Table 2. Average semantic coherence values for the inferred topics of different models.

model

review

reason

error
place
result
show

due

time
hope
long

screen

problem
paid
great

lost small

updated
bugsappears

work update

button
bad
poor

supposed

fully

coherence

users

hard

response

simply issue
start
show
fine

updated

reviewer user

response

option users
decent

support

unable
find touch

supposed

button

glad

thought

main

disappointed

bug

concept

developer expect

blank

close work

memory

background
expect today apparently
happened
part open disappointed
updates
recently big deleted problems
downloaded
ago waiting
support save
reviews
asked coming

reason

unable
point
decent fixes

review

updated
made care

bad
rating

place

due
original

result

mentioned
useless
case

released
back
fully

fixed

settings

thing

add

happen

good

closepoor

people

change great

back
fix work

fine select
agree

fact
bug giving

make appears

lot
st

continue

paid

upgrade

start

problem give
delete

problems

wrong

show
stop
hard

buggy
place
stopped
screencompletely decided
hoping worked
main complete
thought
issues option
supposed

hope long
button
guess slow

automatically appears
make background
poorly give
device
decent rating
improvements

functioning frustrating
properly screen reinstall
dropped apparently till time suddenly complained
rating purchased strange
supported gen recently
released
original
shame stating
due
delete bad
update
purchase

fixed bugs
issue
time loststars guys

update

working
pretty
concept
switch

solve

white

screen

developers

great

big screen

review
works software

left support picture
message find thing space anymore
screenshot worth great kill
waste
software free page
runningmake doesnt
false
stars click money stop review
worthless
night total blue
double
shows
cents reviews ipads back read
crash
hit makes
mini
gray people
press entire
useless
battery
showing
simply
pay
dont
reading
decent
buck vision
broken
opens
user
crap
open
bought
launch gb
button clear device
shot
run
advertise
dollar buy apple gs tap black home
give genbackground reason
install
thoughtexit
closeddeveloper
tells appears spent show
downloaded long
recommend hold lot
warning

hope reason

work installed
developers initial

good stars
long

support

developers

working

error

idea

bug

mentioned
guess original complete
idea
simply potential
updates
installed waiting

âˆ’52.64(19.94)
âˆ’66.30(26.43)
âˆ’67.84(26.54)

JTM
Oridinal SLDA
Gaussian SLDA

totally

works rating
correctly
close
add

product

upset ripped

reviews wasted

months buying crap fails
worth fine review contacted loaded
claims recognize
paying thing
immediately promised charge
thought

stated program

properly
main

worked

spent

negative

properlypoor

fine

put worked
start
agree

bucks
time
email
supported
scam

improve
contact
obvious
pretty
upgrade change
development
problem hoping
product
ago function continue
close update
part
correct test
stopped
select
bad corrected
updates time
fails
option
wrong
totally testing
earlier worked
apparently
mode
correctly

purchase

thing

delete save

ago

workbuy

crashes

sell contact

load
program
support job appears
disappointed rip paid save
shame price

developer

issue make
poorly wrong

people

money

customers
asked

sends

upgrade
give back
change
working stars
fixed
pretty icon find
care unable

dollar

purchased

asked giving
completely
point problems
part big apparently
case
hoping

downloadeddeleted

refundbought

itunes failedyear
wasteread
apple
back days
fix

recommend
downloaded
company

poor downloadorder
advertisement
developers whatsoever license impossible
supposed screwed
pointless

continue
concept fix

developer

purchase

review

piece

deserves fail
green

remove cents hope
simply
emailed
response removed
unable
purchasing problem
installed
customer working happy
piecereturn
open
clear
dollars
works
beware

absolutely basically

time
give back
awful junk complete

bad

literally pos
wasted people
agree jokehit

seconds pathetic

locks

crap reviewstotal freezes

opened
press remove
totally

put

reason

amount

spam extremely
paying
ruin started constant
tap advertising makes wanted
enjoy horrible
takes downloaded
pops seconds point
good
put
constantly open hard full click minutes space
bottom delete deleted hit download
awful turn
people
deleting
free
bother
rid
impossible
stars
totally terrible
thing minute
making pay
hate give
random
uninstalled stop
ups frustrating
playing times
slow make understand
removed stupid paid
page start
remove add popup
ridiculous
accidentally bad
opens

addsannoying

ads
pop

close

time screen

run

forced worstupgrade top popping touch
middle fine
switch advertisements
rate
banner button
crazy
advertisement completely
recommend
purchase obnoxious

4.3. Inspection of Inferred Topics

randomly

made

unable fix support

restore problem

worked
fixed

expect
happened
broken fixes
start

long

paid
shut
stop
reset
versions

upgraded

upgrade
working
issue

desperately back work

updated ten

completely

useless
longerreturn

today

compatible

decided
happen

works
owned
main
hope
locks
devices

fine disappointed

fully

stoppedlost st
issues box bug
response ago
locked select
updating
problems sadly
disappointing
inability bought software bugs earliermonths
correctly
acceptable causing
appears
poorly returned

buggy

upgrading
appeared

reading
wasting guys downloaded
understand
picture review
description apple
touch deserve
barely thought reviews

horrible

terrible download guess

sense
wast
negative stars put
joke free
hard

dont

mirror wow

pro hate

people

created waist

ur

made

buying

true

waste

lame
crappy total
awful
bunch

cents
dumb

back

chance

upgrade

real wont

bother

fake

writemake

bad
time
work crap

life dollar

confusing

worth
buy worst
rip stupid doesnt
screen

point
cuz

scam false

piece

lie

money lite
totally

delete
read move didnt
save
garbage give
space thing downloading ripoff
wasted stinks buck
instructions pointless worse
thinking

developer

complete rate called

Figure 5. Visual illustration of topics associated with low ratings.

Finally, we visualise and interpret the topics inferred by the
JTM model. Figures 4 and 5 visualise nine topics associated with high (excellent) and low (poor) ratings, respectively. As explained in Section 2.2.1, the associations (both
sign and strength) are given by computing the similarity
scores (that is, correlations).
possibilities
review sticker suggest
tons downloaded lets easy
sick
fav download special
soooo
greatest wonderful add
watching
hands found absolutely lots
long packs
awsome friends
loved super
haha
making make thx earn
stop free happy lol
honestly
sooo
ways perfect
follow job
alot
bit
helped works
create neat
wise things
lot hope
wait
fabulous enjoy
stuff friend
cute
likes ur
title yay
favorite features
pretty
good
creative
luv
finally
helps

awesome

love
great

makes

cool

fun amazing

dope wow
loving
rocksrecommend
stickers sweet
brilliant totally nice
family omg ap
points
soo made creating
incredible crazy today writing
forever instagram

money
glitches
loved challenges
quick
hooked
pretty graphics
difficult
frustrating nice long recommend
unlock
puzzles start angry
buy challenge
enjoy
hard
find
awsome
games
stop bored
stars minutes
times
level
bird
playing thinking
give brainhigh
birds
puzzle time
makes made
highly
day
wait ago
hope
harder
cute
worth
kill
waiting
totally
relaxing
fast past
enjoyed
coins
easy
jump pass
cool
lot
back
lol put
beat
addictive mind
run awesome
played favorite
flappy addicting
challenging make boring
impossible

levels

fun

game
love

addition

quick
future
nice ui

interesting

feature

solid

additions

design

great excellent
execution

fantastic

limited
add job
allowing stable
pretty adding
thought functionality

matter

users
favorite

stars

team

simple

world post
community

experience

stuff

ability user improved
clean

silver friendly
excellent

high

site

back

quick

add

ability

nice

spot gold

students

accurate works

shows day

charts

point real
awesome

trading

makes

current buy

user

functional

interface

designgraphs

stock designed
features tool

price data
good

check job
change
perfect

make pay track
updates
hard
update portfolio money access experience
stocks

functions information
beautiful
amount daily stars wow
performance rate issue basic users
fully
improvement
notifications

effort

chinese

allah

time

understand

big

learn

practice
writing

word

german
study
flash

korean
yor
option
correct
search

bu vocabulary

spanishwork
languages
cok bless foreign
qurantool easy

ba

found
love
cards excellent

lot find letters

dictionary french
make

pronunciation
dictionaries
read

helpful translation

ama
helped
audio
feature

daha
program russian
japanese uygulama

amazing

working

translations
perfect translator
recommend voice
meaning
examples

minutes
love week

videos
give

run days
exercise
life make

workout

fitness

progress
minutepush
made
lot
variety perfectsit
years
worth
back yogastay
complete
makes simple works times feeling
amazing nice pace awesome helping
programs
stars
thing home change instructions
ago

easy running
exercises

multiple real
functions tools
people grade easier homework college
intuitive
check
easily makes worth nice figurethoughtcool
perfect basic takes
wanted unit times

number

adding made
daily answer

quick
features
tape
units

work

easy simple
needed

good add
tool

handy

excellent

equations
percentage
quickly

time helpful

great love

calculator
works

input stars
review give

thing
life

student
clean
excel

calculations
convert

program

find
highly
make
enter

helps interface

results
lot accurate tip fast

calc checking

math
found
numbers back
recommend

conversion calculate

grades
conversions type job clear
change
function school feature
practice

bir
words
english great

day add

definition
basic

classes voice

pretty

weeks fit

great

helped

wonderful

learninglanguage reward
memorize

studying
arabic

body

learned

definitions hope knowledge

good updateinclude sentences
verb helps phrases application guzel
test
letter
speak improve
thing
worthtranslate

functionality student
chart
quickly fast
alerts give

time

find made
feel nice big

idea connect

lots spelling

love clean exchange
navigate

feature

account user ability

people

view prices easily super

market ui

quotes give
kind

posts

trade

great

part
comments

greatfun

recommended

simpleeasy

posted

daypictures
super

personal

investment option set
guys

back
met

interested sharing place
support
highly group
updates
follow friendly
members posting

clear fast standard
attractive
promises
improvement
resolved

precious

people

love

started feel

awesome

facebook

added options change
tech lot
fully
including missing additional beautifully
improve updates improvements

month display official
sites
options

photos
add

fb
interesting
friend
fan
signstay
things easy life
boyfriend real awesome instagram amazing
feature
concept meet

kudos
simply smart
easily

layout

make

twitter guys
enjoy lot

designed

works

social

cool

day workouts routine
set

good time

follow ups
class runs

music

shape

calculation understand calculators
problems spreadsheet

recommend

friends

share

good

create

helps

motivated

level

helpful

gym start

work training found
long

runner thought

time download memories

media

check

visual

touch lets wait basic

forward

talk thing
design chat
sports

features

developers
small worth
provide stars

interface
simple

pros

finished coming

connected
perfect family fans wonderful
status makes
moments

intuitive

features

great

play

super
lots simple amazing
finish
download addicted
hours enjoyable
entertaining score
concept

landscape impressed
logical
suggestion users perfect simpler large
developer view top
retina quickly
option
hope friendly
happy frequent faster love
multi
potential
nicely
update making
found
bar
bit menus
easier
finally

good

5. Discussion

equipment
daily
quick results couple beginner
helped find
motivation coach
highly recommend forward

effective trainer
weight program
short
end track

adorable
makes grandson
favorite happy
downloaded excited
enjoy
button
teacher educational
page games
teach
bought
picture make
shapes
easy
activities
playing
yr
mini
sounds
touch sound
months
countinglearn
month perfect
number
letter enjoys
school
parent
kid good part

children

baby daughter
child

learning

stories lot play
colors story
home
put students

loves

kids
year

older

fun toddler count

thing
cute family free
adult

great

write
interactive
letters adults
numbers parents animals likes ages
worth young color age find words
time
things years
coloring
pictures
plays recommend
simple
teaching babies
purchased
absolutely

animal loved

bit
voice nice fine

love

The topics associated with low ratings (Figure 5) contain
customersâ€™ negative experiences or feature requests such
as removal of adds, software updates and problems with
functionality.

Figure 4. Visual illustration of topics associated with high ratings.

In this work, we develop a new class of ordinal mixed membership models suitable for capturing statistical associations between groups of observations and co-occurring ordinal response variables for each group. We depart from the
existing dominant approach that relies on improper model
assumptions for the ordinal response variables. We successfully demonstrate the developed models for analysing
reviews of mobile software applications provided by consumers. The proposed class of models as well as inference
approaches are applicable for a wide range of present-day
applications. In the future, we expect to see improvements
in statistical inference including fully Bayesian treatments
and nonparametric Bayesian formulations. Stochastic online learning or model formulations for streaming data may
be applied to scale the statistical inference to cope with current data repositories containing review data for a few millions of groups.

Ordinal Mixed Membership Models

Acknowledgement
This research is supported by an EPSRC program grant
A Population Approach to Ubicomp System Design
(EP/J007617/1).

Mimno, David, Wallach, Hanna M, Talley, Edmund, Leenders, Miriam, and McCallum, Andrew. Optimizing semantic coherence in topic models. In Empirical Methods
in Natural Language Processing, 2011.
Minka, Thomas. Estimating a Dirichlet distribution, 2000.

References
Albert, James H and Chib, Siddhartha. Bayesian analysis
of binary and polychotomous response data. Journal of
the American Statistical Association, 88(422):669â€“679,
1993.
Blei, David and Lafferty, John. Correlated topic models.
In Advances in Neural Information Processing Systems,
2006.
Blei, David M and McAuliffe, Jon D. Supervised topic
models. In Advances in Neural Information Processing
Systems, 2007.
Blei, David M, Ng, Andrew Y, and Jordan, Michael I. Latent Dirichlet allocation. the Journal of Machine Learning Research, 3:993â€“1022, 2003.
BoÌˆhning, Dankmar. Multinomial logistic regression algorithm. Annals of the Institute of Statistical Mathematics,
44(1):197â€“200, 1992.
Chu, Wei and Ghahramani, Zoubin. Gaussian processes
for ordinal regression. Journal of Machine Learning Research, 6:1019â€“1041, 2005.
Dai, Andrew and Storkey, Amos J. The supervised hierarchical Dirichlet process. IEEE Transactions on Pattern
Analysis & Machine Intelligence, 37(2):243â€“255, 2015.
Griffiths, Thomas L and Steyvers, Mark. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(suppl 1):5228â€“5235, 2004.
Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John. Stochastic variational inference. Journal
of Machine Learning Research, 14(1):1303â€“1347, 2013.
Jaakkola, T and Jordan, Michael I. A variational approach
to Bayesian logistic regression models and their extensions. In Artificial Intelligence and Statistics, 1997.
Lacoste-Julien, Simon, Sha, Fei, and Jordan, Michael I.
DiscLDA: Discriminative learning for dimensionality reduction and classification. In Advances in Neural Information Processing Systems, 2009.
Mimno, David and McCallum, Andrew. Topic models conditioned on arbitrary features with Dirichlet-multinomial
regression. In Uncertainty in Artificial Intelligence,
2008.

Nguyen, Viet-An, Boyd-Graber, Jordan L, and Resnik,
Philip. Lexical and hierarchical topic regression. In Advances in Neural Information Processing Systems, 2013.
Paisley, John, Wang, Chong, Blei, David M, et al. The
discrete infinite logistic normal distribution. Bayesian
Analysis, 7(2):235â€“272, 2012.
Ramage, Daniel, Hall, David, Nallapati, Ramesh, and
Manning, Christopher D. Labelled LDA: A supervised
topic model for credit attribution in multi-labeled corpora. In Empirical Methods in Natural Language Processing, 2009.
Wainwright, Martin J and Jordan, Michael I. Graphical
models, exponential families, and variational inference.
Foundations and Trends in Machine Learning, 1(1-2):1â€“
305, 2008.
Wallach, Hanna M, Minmo, David, and McCallum, Andrew. Rethinking LDA: Why priors matter. In Advances
in Neural Information Processing Systems, 2009.
Wang, Chong and Blei, David M. Variational inference
in nonconjugate models. Journal of Machine Learning
Research, 14(1):1005â€“1031, 2013.
Wang, Chong, Blei, David, and Li, Fei-Fei. Simultaneous
image classification and annotation. In Computer Vision
and Pattern Recognition, 2009.

