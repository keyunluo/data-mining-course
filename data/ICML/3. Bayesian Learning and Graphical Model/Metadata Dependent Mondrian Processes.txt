Metadata Dependent Mondrian Processes

Yi Wang‡†
YI . WANG @ NICTA . COM . AU
Bin Li†
BIN . LI @ NICTA . COM . AU
Yang Wang†
YANG . WANG @ NICTA . COM . AU
Fang Chen†‡
FANG . CHEN @ NICTA . COM . AU
†
Machine Learning Research Group, National ICT Australia, Eveleigh, NSW 2015, Australia
‡
School of Computer Science & Engineering, University of New South Wales, Kensington, NSW 2033, Australia

Abstract
Stochastic partition processes in a product space
play an important role in modeling relational data. Recent studies on the Mondrian process have
introduced more flexibility into the block structure in relational models. A side-effect of such
high flexibility is that, in data sparsity scenarios,
the model is prone to overfit. In reality, relational entities are always associated with meta information, such as user profiles in a social network.
In this paper, we propose a metadata dependent
Mondrian process (MDMP) to incorporate meta
information into the stochastic partition process
in the product space and the entity allocation process on the resulting block structure. MDMP
can not only encourage homogeneous relational interactions within blocks but also discourage
meta-label diversity within blocks. Regularized
by meta information, MDMP becomes more robust in data sparsity scenarios and easier to converge in posterior inference. We apply MDMP to
link prediction and rating prediction and demonstrate that MDMP is more effective than the baseline models in prediction accuracy with a more
parsimonious model structure.

sent the relational data as an adjacency matrix and cluster rows and columns simultaneously to uncover the block
structure. Such “co-clustering” operation can be understood as permuting row/column entities on each dimension
of the data matrix, with the objective to make the intensity
of relational interactions consistent within blocks.
Block models (White et al., 1976) have been widely used
in modeling, analyzing and predicting relational data. Stochastic block models were first proposed in (Holland
et al., 1983) to establish a stochastic generalization of block
models. By imposing a Chinese restaurant process on each
dimension of the adjacency matrix, the infinite relational
model (IRM) (Kemp et al., 2006) discards the restriction on
the number of blocks. While IRM and its variants (Airoldi
et al., 2009) have obtained the flexibility of the number of
blocks, their block structures are restricted to regular grids (Muthukrishnan et al., 1999). The Mondrian process (MP) (Roy & Teh, 2009) was proposed to relax this restriction
with a more flexible structure of blocks, which are generated recursively as a kd-tree.

Relational data exist widely and many real-world applications boil down to relational data modeling, such as community detection, link prediction, and collaborative filtering. Although in different applications relational data may
appear in different forms (e.g., binary links in a social network and discrete ratings in a recommender system), the
essence of relational data modeling is similar – To repre-

While the Mondrian process is a powerful prior for complex relational modeling, its flexibility may lead to some
side-effects compared to other regular block models: 1) It
is prone to overfit in data sparsity scenarios; and 2) it is
hard to converge based on a uniform prior assumption. In
this paper, we aim to incorporate meta information of entities into MP to relieve these problems. The rationale behind
is that the entities with similar meta information are more
likely to have similar behaviors than those entities with different meta information. For example, people graduating
from the same university are more likely to become friends
on an online social network. Based on this observation, we
propose a metadata dependent Mondrian process (MDMP), which seamlessly integrates the meta information into
an MP-like hierarchical partition process.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

MDMP can be viewed as a generalization of MP by integrating meta information into both the stochastic partition
process in the product space and the entity allocation pro-

1. Introduction

Metadata Dependent Mondrian Processes

cess on the resulting block structure. MDMP adopts a similar hierarchical partition process as MP to generate blocks;
while at each step of MDMP, a block is first reshaped according to the meta label distribution on each dimension for
uniformly sampling the cutting position, such that the partition is more likely to occur on the dimension with relatively
diverse meta labels. Due to the reshaping of blocks, partitioning a block with diverse meta labels becomes cheaper
and MDMP has higher probability to accept the partition
proposal on it. Thus, MDMP will produce a very different block structure compared to MP. For entity clustering
on each dimension (i.e., allocating rows/columns onto the
block structure), we rescale the cutting intervals on each
dimension of the block structure such that a row/column
is more likely to be allocated to an interval with the same
meta label as the majority.
We empirically study the performance of MDMP and baseline models on three real-world data sets for link prediction
and rating prediction. The experimental results demonstrate that, by incorporating meta information, MDMP is
able to outperform the baselines in prediction accuracy
with a more parsimonious model structure.
The remainder of the paper is organized as follows: Section 2 introduces the related work. The proposed MDMP
relational model and its posterior inference method will be
described in Section 3 and Section 4, respectively. The experimental results are reported in Section 5 and the paper
is concluded in Section 6.

2. Related Work
The infinite relational model (IRM) (Kemp et al., 2006) is
a Bayesian nonparametric model that does not require to
know the number of partitions in advance. The Chinese
restaurant process on each dimension of IRM enables an
infinite partition on entities. IRM is only restricted to generate a regular grid partition (see Figure 1(b)), which is
one of the three types of rectangular partitions (Muthukrishnan et al., 1999). IRM was extended by incorporating
temporal dynamics to analyze time-varying relational data (Ishiguro et al., 2010). Then the mixed-membership stochastic blockmodel (MMSB) (Airoldi et al., 2009) was
introduced to enable mixed memberships over the latent clusters. Another expressive feature-based block model,
named nonparametric latent feature model (Miller et al.,
2009), was proposed to model relational data through the
combination of latent groups. In this paper, we only consider hard-membership block models.
The Mondrian process (MP) (Roy & Teh, 2009) is the baseline model of MDMP (which can naturally degrade to MP
if meta labels are uniformly distributed). MP is a stochastic partition process that recursively generates axis-aligned

(a)

(b)

(c)

Figure 1. Block models on a synthetic relational data set: (a) The
input data; (b) a posterior regular block structure; and (c) a posterior hierarchical block structure.

cuts in a unit hypercube (see Figure 1(c)). In contrast to
IRM, MP can partition the space in a hierarchical fashion (Muthukrishnan et al., 1999), known as kd-tree, and
results in an irregular block structure. An MP in the product space [0, 1] × [0, 1] is started from a random cut on
the perimeter and results in two sub-rectangles, in each
of which a random cut is made in the same way and so
>
` a
forth. Before cutting on [a⊥
k , ak ] × [bk , bk ] (block k),
a cost is drawn from an exponential distribution Ek ∼
`
a
⊥
Exponential(a>
k − ak + bk − bk ). If λ − Ek < 0 (λ
is the budget), the recursive procedure halts; otherwise, a
>
` a
random cut is made mk ∼ MP(λ, [a⊥
k , ak ], [bk , bk ]) and
set λ = λ − Ek . Recently, rectangular tiling process (RTP) (Nakano et al., 2014) was proposed to produce arbitrary
partitions (Muthukrishnan et al., 1999).
Under many circumstances, a more sophisticated model is
required to capture dependence among entities (e.g., temporal dependence and spatial dependence). This constraint
has been introduced into Bayesian nonparametric mixture
models, such as dependent Dirichlet process (MacEachern, 2000) and distance-dependent Chinese restaurant process (Blei & Frazier, 2011), and Bayesian nonparametric
latent feature models, such as dependent Indian buffet process (Williamson et al., 2010), dependent hierarchical beta
process (Zhou et al., 2011), kernel beta process (Ren et al.,
2011), and distance dependent Indian buffet process (Gershman et al., 2014). In the scope of Bayesian nonparametric relational models with regular block structures, the
dependence based on the side information has been introduced in (Choi et al., 2011; Kim et al., 2012). In addition
to considering constraints for grouping entities, MDMP directly uses meta information to rectify the generating process of hierarchical block structures.

3. Metadata Dependent Mondrian Process
The input relational data can be represented as an N × M
matrix1 Y, where N and M are the numbers of entities in
the two interacted sets, respectively, and yn,m denotes the
1
MDMP can be straightforwardly extended to the cases of
multi-arrays as the Mondrian process (Roy & Teh, 2009).

Metadata Dependent Mondrian Processes

value of the interaction between entity n from one set and
entity m from the other. Each entity is also associated with
a meta label, can ∈ Ca for entity n and cbm ∈ Cb for entity m. In a social network, Y can be a symmetric binary
matrix indicating links between users, which have meta information like locations as meta labels; in a recommender
system, Y can be an asymmetric integer matrix indicating
ratings of movies provided by users, which have meta information like occupations and genres as meta labels.
The quality of the block structure uncovered by an MP
largely relies on the likelihood homogeneity of the relational data within blocks. In the cases that within-block
interactions are very sparse, the MP is prone to overfit. To
address this limitation, we incorporate meta information into the model, such that the block structure relies not only
on the likelihood homogeneity but also the meta-label homogeneity. The goal of MDMP is to make use of metadata
as side information to improve relational modeling by uncovering better block structures.
In the following, we introduce the proposed MDMP relational model, including one cutting strategy for generating partitions and one indexing strategy for allocating
rows/columns to the resulting block structure. Both strategies are dependent on the meta information and will be exploited in Section 4 for inferring the block structure and
row/column allocations.
3.1. Cutting Strategy
Given an axis-aligned block k in the product space [0, 1] ×
>
` a
[0, 1], bounded in [a⊥
k , ak ] on the vertical axis and [bk , bk ]
on the horizontal axis, an MP makes a cut on either
` a
>
[a⊥
k , ak ] or [bk , bk ] in proportional to its length (see Figure 2(a–b)). This cutting strategy is based on a reasonable
assumption that the longer side is more likely to cover more
heterogeneous entities. In the case of meta information being provided, an MDMP also reduces the meta label diversity on each side. Take community detection for example:
Communities (blocks in a relational model) are likely to
comprise users with similar occupations; in other words, it
is more likely to detect reasonable communities by discouraging occupation diversity within blocks while modeling
social interaction data.
To this end, an intuitive strategy is to increase (or decrease)
the side-length of block k if high (or low) label diversity is
observed on that side. We rescale the side-lengths of block
k in the following way
>
` a
[a⊥
k , ak ] × [bk , bk ] ⇒

tions) on the vertical side of block k and Entropy(hak ) measures the diversity of meta label distribution in hak ; and wkb
is defined similarly for the horizontal side.
In this way, a uniform sampling of cutting position on
>
` a
[â⊥
k , âk ] × [b̂k , b̂k ] will be in terms of both the true sidelength, corresponding to the number of entities, and the
meta label diversity on that side, corresponding to the heterogeneity of entities (see Figure 2(c–d)).
After sampling a cutting position γ from a uniform distribution on the perimeter of the reshaped block

⊥
a
`
γ ∼ Uniform 0, â>
(2)
k − âk + b̂k − b̂k
the physical cutting position mk should be mapped back to
its original coordinate system (see Figure 2(d–e))
mak

= a⊥
k +

mbk

= b`k +

γ
⊥
, if γ < â>
k − âk
wka
⊥
γ − (â>
k − âk )
, otherwise
wkb

(3)
(4)

where mak (or mbk ) denotes the cutting position on the vertical (or horizontal) axis of block k.
It is worth noting that cost sampling is also influenced by
⊥
a
`
the reshaping: Ek ∼ Exponential(â>
k − âk + b̂k − b̂k ).
If the meta label diversity has become reasonably low in
block k, the cost will increase and the recursive partition
process in this branch is likely to halt earlier.
3.2. Indexing Strategy
Given the current block structure (e.g., Figure 3(a)) obtained in the cutting step, the indexing step aims to allocate
rows/columns of Y to the vertical/horizontal axis of the
partition space [0, 1] × [0, 1] by making each block have
homogeneous relational data (e.g., Figure 1(c)). Since Y
is separately exchangeable given the partition structure and
the meta information, the allocation of rows/columns is equivalent to the permutation of rows/columns.
>
` a K
Suppose the current blocks are {[a⊥
k , ak ] × [bk , bk ]}k=1
2
(K is the number of blocks ). We can obtain all the vertical/horizontal cutting positions projected onto the axes.
⊥
>
Let [r1:E
, r1:E
] and [s`1:F , sa1:F ] be the intervals on the vertical and horizontal axes, respectively. An MP assumes a
uniform prior distribution for indexing
rows/columns on
SE
these intervals: ξn ∼ Uniform( i=1 [ri⊥ , ri> ]) and ηm ∼
SF
Uniform( j=1 [s`j , saj ]), where ξn and ηm are the indexing
variables of the nth row and mth column in Y.

(1)

To make within-block meta label distribution more homogeneous, we can increase the probability of sampling ξn (or

where wka = exp(Entropy(hak )) ∈ [1, +∞), where hak is
the normalized histogram of meta labels (i.e., label propor-

2
The blocks correspond to the leaves of the underlying kdtree, which is produced by hierarchically partitioning the space.

>
b ` a
⊥ >
` a
wka [a⊥
k , ak ] × wk [bk , bk ] = [âk , âk ] × [b̂k , b̂k ]

Metadata Dependent Mondrian Processes

⇒

⇒

(a)

(b)

⇒

(c)

(d)

(e)

Figure 2. Cutting strategy: (a–b) MP only considers side-length (number of entities) and performs a uniform sampling on the perimeter.
(c–e) MDMP considers both side-length (number of entities) and meta label diversity (heterogeneity of entities). The original block (c)
with meta label information (different colors denote different labels) is first reshaped to (d); after a uniform sampling on the perimeter
of the reshaped block, the cutting position should be mapped back to the original block.

ηm ) from the intervals with higher proportion of the corresponding meta label can (or cbm ). We also use rescaling to
this end: An MDMP samples indexing variables, ξn and
ηm , on the rescaled intervals conditioned on the index assignments of the other rows ξ¬n and columns η¬m . The
rescaled intervals are calculated as
[ri⊥ , ri> ] ⇒
[s`j , saj ] ⇒
ca

ca

⊥
>
vi n [ri⊥ , ri> ] = [r̂i,c
a , r̂i,ca ]
n
n
cbm

vj [s`j , saj ] = [ŝ`j,cbm , ŝaj,cbm ]

λ

M
can

cbm

(5)
(6)

yn,m

ξn

ηm

cb

where vi n and vj m are rescaling weighs implicitly defined
in Eqs. 7 and 8; can and cbm denote meta labels. For example,
in a rating matrix, can ∈ {student, engineer, professor} can
be occupation of users while cbm ∈ {classic, folk, jazz} can
be genre of music.
We rescale the intervals as follows: Calculate the normalized portion of meta labels over vertical cuts and horizon>
⊥
] and
, r1:E
tal cuts; then use this proportion to weight [r1:E
a
`
[s1:F , s1:F ]
>
⊥
(r̂i,c
a − r̂i,ca )
n
n

=

(ŝaj,cbm − ŝ`j,cbm )

=

(ri> − ri⊥ )Ni,can
PE
>
⊥
0 a
i0 =1 (ri0 − ri0 )Ni ,cn
(saj − s`j )Nj,cbm
PF

a
j 0 =1 (sj 0

−

s`j 0 )Nj 0 ,cbm

α0

β0

θk

Figure 4. The graphical representation of the MDMP relational
model (with the beta-Bernoulli model in each block).

• θk ∼ Beta(α0 , β0 ) (link data) or θk ∼ Dirichlet(α0 )
(rating data), k = 1, . . . , K;

(7)

> ` a
• [a⊥
k , ak , bk , bk ]
1, . . . , K;

(8)

SE
⊥
>
• ξn ∼ Uniform( i=1 [r̂i,c
a , r̂i,ca ]), n = 1, . . . , N ;
n
n

where Ni,can denotes the number of rows with meta label
can allocated to the ith interval on the vertical axis; Nj,cbm
is similarly defined for the columns. In implementation, a
small number can be added to Ni,can and Nj,cbm for regularization. This rescaling method is illustrated in Figure 3.
We will use the rescaled lengths of intervals for sampling
indexing variables in Eqs. 16 and 17.
3.3. Graphical Model
The generative process of the MDMP relational model is
as follows (the corresponding graphical model is shown in
Figure 4):

∼

MDMP(λ, ca1:N , cb1:M ), k

=

SF
• ηm ∼ Uniform( j=1 [ŝ`j,cb , ŝaj,cb ]), m = 1, . . . , M ;
m

m

• yn,m ∼ Bernoulli(θh̄(ξn ,ηm ) ) (link data) or yn,m ∼
Discrete(θh̄(ξn ,ηm ) ) (rating data), n = 1, . . . , N , m =
1, . . . , M .
> ` a K
where M = {[a⊥
k , ak , bk , bk ]}k=1 denotes the block structures (a kd-tree on [0, 1] × [0, 1]) and h̄(ξn , ηm ) denotes a
mapping from a row-column index pair to a block index
in M. Note that we neglect some intermediate steps, such
⊥
>
E
`
a
F
as {[r̂i,c
a , r̂i,ca ]}i=1 and {[ŝ
j,cbm , ŝj,cbm ]}j=1 are calculated
n
n
⊥ > ` a K
based on {[ak , ak , bk , bk ]}k=1 .

Metadata Dependent Mondrian Processes

(a)

(b)

(c)

(d)

(e)

Figure 3. Indexing strategy: (a) The block structure; (b–c) rescaled intervals on the vertical axis for two different meta labels; (d–e)
rescaled intervals on the horizontal axis for two different meta labels. After rescaling, indices ξn and ηm are more likely to be assigned
to those vertical/horizontal intervals with higher proportion of the same meta label.

4. Inference

Algorithm 1 Approximate inference for MDMP

The joint probability of relational data Y, model parameters {M, θ1:K }, and indexing variables {ξ1:N , η1:M } is
p(Y, M, θ1:K , ξ1:N , η1:M |λ, α0 , β0 , ca1:N , cb1:M )
=

M
N Y
Y

p(yn,m |M, θ1:K , ξn , ηm )

n=1 m=1

×

N
Y
n=1

p(ξn |M, ca1:N )

M
Y

p(ηm |M, cb1:M )

(9)

m=1

× p(M|λ, ca1:N , cb1:M ) ×

K
Y

p(θk |α0 , β0 )

k=1

where θ1:K can be marginalized out (if we use betaBernoulli, Dirichlet-multinomial, or other conjugate distributions). Thus, we need to estimate M and {ξ1:N , η1:M }.
The inference framework for MDMP is outlined in Algorithm 1. We adopt two nested loops of MCMC sampling
for approximate inference: The outer loop is to infer the
block structure M by proposing adding or removing a cut.
Since this part of inference involves dimensionality change
of the parameter space, we adopt the reversible-jump MCMC (Green, 1995) algorithm (see Section 4.1). The inner loop is to infer row/column indexing variables ξ1:N and
η1:M , given the current M. This part of inference can be
simply solved by using the collapsed Gibbs sampling algorithm (see Section 4.2).
4.1. Sampling Partitions M
The reversible-jump MCMC (RJMCMC) (Green, 1995) is
aimed to sample posterior distributions in which the dimensionality of parameter space varies between iterations of a
Markov chain. RJMCMC has been used in MP-based coclustering ensembles in (Wang et al., 2011). We also adopt
this technique for sampling the block structure M in an
MDMP. Each step of the RJMCMC algorithm proposes to
add or remove a cut in M.
Suppose from iteration t to t + 1 (the outer loop in Algo-

Input: Y and {λ, α0 , β0 , ca1:N , cb1:M }
Output: M and {ξ1:N , η1:M }
repeat
Initialize M as [0, 1] × [0, 1];
Propose a partition (add/remove a cut) in M;
Accept/Reject the proposal (RJMCMC);
if Accepted then
Sample ξ1:N and η1:M from M (Gibbs sampling);
end if
until Exceed the number of iterations

rithm 1), the RJMCMC algorithm proposes to add a cut (the
change of the block structure is denoted by Mt → Mt+1 ),
the acceptance ratio of this proposal is
αMt →Mt+1

p(Mt+1 |Y, ξ1:N , η1:M , λ, α0 , β0 )
= min 1,
p(Mt |Y, ξ1:N , η1:M , λ, α0 , β0 )

q(Mt+1 → Mt )
×
× |JMt →Mt+1 |
q(Mt → Mt+1 )

(10)

t+1 |−)
where the first term p(M
p(Mt |−) is the ratio of the posterior
probabilities of the two block structures given the data; the
t+1 →Mt )
second term q(M
q(Mt →Mt+1 ) is the ratio of the proposal probabilities; and the last term |JMt →Mt+1 | is the determinant
of the Jacobian inter-model transition matrix.

Let Mt+1 = {Mt , mk , Ek } (as defined before, mk denotes a cutting and Ek denotes the associated cost) and
{u1 , u2 } (generated using the sample proposal distributions as {mk , Ek }) be the corresponding auxiliary variables for
{mk , Ek }, there is a bijection between {Mt , u1 , u2 } and
Mt+1 characterised by an identity inter-model transition
matrix; thus we have |JMt →Mt+1 | = 1. For simplicity,
we can also assume that the state transition proposal distribution is symmetric.
t+1 |−)
The ratio of the posterior probabilities p(M
p(Mt |−) can be
rewritten as a production of a prior ratio and a likelihood

Metadata Dependent Mondrian Processes

ratio

p(Mt+1 |λ)
p(Mt |λ)

×

L(Mt+1 )
L(Mt ) .

vertical interval) with beta-Bernoulli likelihood gives

The prior ratio is

p(Mt+1 |λ)
p(mk )p(Ek )%k0 %k00
=
p(Mt |λ)
%k

(11)

where p(mk )p(Ek ) = exp(−φk Ek ) denotes the probabili⊥
a
ty of sampling a cut mk in block k (φk = â>
k − âk + b̂k −
`
b̂k ); %k = exp(−φk λk ) denotes the probability of terminating at block k (same definitions for %k0 and %k00 in block
k 0 and k 00 , respectively).
If we adopt the compound beta-Bernoulli distribution, the
likelihoods given Mt+1 and Mt are
L(Mt )

=

Kt
Y

Y

p(yn,m |θkt )

kt =1 yn,m ∈Mkt

=

Kt
Y

N
θktkt ,+ (1

Nkt ,−

− θkt )

(12)

¬n
p(ξn ∈ [ri⊥ , ri> ]|Y, M, ξ1:N
, η1:M , ca1:N ) ∝
i,k
i,k 
Y Nn,+
+ Nn,−
⊥
>
(r̂i,c
a − r̂i,ca )
i,k
n
n
Nn,+
k∈Si
i,k
i,k
B(Nn,+
+ αk , Nn,−
+ βk )
B(αk , βk )

>
⊥
where (r̂1:E,c
a − r̂1:E,ca ) are the rescaled vertical intervals
n
n
according to Eq. 7, Si denotes the set of blocks which have
i,k
deinteractions with the ith vertical interval, and Nn,+/−
notes the number of positive/negtive entries in the nth row
if it is assigned to the kth block which is traversed by the
ith vertical interval.

The conditional posterior of ξn with Dirichlet-multinomial
likelihood gives

kt =1
Kt+1

L(Mt+1 )

Y

=

Y

p(yn,m |θkt+1 )

>
(r̂i,c
a
n

kt+1 =1 yn,m ∈Mkt+1
Kt+1

Y

=

Nk

θkt+1t+1 (1 − θkt+1 )Nkt+1 ,−(13)
,+

kt+1 =1

where Kt denotes the number of blocks in Mt and
Nkt ,+/− denotes the number of observed positive/negtive
entries in block kt .
Similarly, if we adopt the compound Dirichlet-multinomial
distribution, the likelihoods given Mt+1 and Mt are
L(Mt )

=

Kt Y
L
Y

L(Mt+1 )

=

L
Y Y

i,k
where Nn,l
denotes the number of entries with meta label
l in the nth row if it is assigned to the kth block which is
traversed by the ith vertical interval.

The conditional posterior of ηm can be derived in the same
way as ξn . The conditional posterior of θk is simple. For
beta-Bernoulli likelihood, we have

(14)

α0 + Nk,+
α0 + β0 + Nk,+ + Nk,−

(18)

while for Dirichlet-multinomial likelihood, we have

kt =1 l=1
Kt+1

¬n
p(ξn ∈ [ri⊥ , ri> ]|Y, M, ξ1:N
, η1:M , ca1:N ) ∝
QL
i,k
Y l=1 Γ(Nn,l
+ αk,l )
Γ(αk )
⊥
− r̂i,c
a)
QL
P
L
i,k
n
l=1 Γ(αk,l )
l=1 Nn,l + αk )
k∈Si Γ(
(17)

θk ∝

N
θktk,lt ,l

(16)

Nk

θkt+1t+1
,l

,l

θk,l ∝ Nk,l + α0

(15)

(19)

kt+1 =1 l=1

where θkt,1:L denotes the parameter of the Dirichlet distribution in block kt of Mt ; L denotes the number of meta
label categories; and Nkt ,l denotes the number of observed
entries with meta label l in block kt .
The RJMCMC algorithm for removing a cut from Mt is as
similar as adding a cut. We assume that the probability of
proposing to add or remove a cut is equal.
4.2. Sampling Indices ξ1:N and η1:M
Given a block structure M, we can use Gibbs sampling (Geman & Geman, 1984) to approximate the posterior distribution of the indexing variables ξ1:N and η1:M .
Based on the joint probability Eq. 9, the conditional posterior of ξn (i.e., probability of allocating nth row to the ith

5. Experiment
We empirically test the proposed MDMP relational model on three real-world data sets with various meta information. We compare MDMP to IRM (Kemp et al., 2006)
(block model with Bernoulli distribution in each block) for
link prediction, BiLDA (Porteous et al., 2008) (block model with discrete distribution in each block) for rating prediction, and MP (Roy & Teh, 2009) for both.
We adopt the following performance measures: 1) Loglikelihood (LL) for measuring the fitness of block modeling; 2) Bayesian information criterion (BIC) for measuring
the fitness of block modeling penalized by free parameters;
3) Area under curve (AUC) for measuring the link prediction performance; and 4) Root mean square error (RMSE)
for measuring the rating prediction performance.

Metadata Dependent Mondrian Processes

In our experiments, each data set is partitioned into 5 splits,
and each time 4 splits are used for training and the rest
one is used for testing. All the reported results are average
values over five runs. For MP and MDMP, we perform 500
iterations of RJMCMC sampling.
(a) Input

(b) IRM

(c) MP

(d) MDMP

5.1. Link Prediction: Lazega’s Lawyer
The first data set adopted for link prediction is the Lazega’s lawyer data3 (Lazega, 2003). In this data set, there are
three different relationships among 71 lawyers in a law firm, which are “Advisory”, “Friendship” and “Workmate”.
For each lawyer in the network, seven different types of
side information is also provided, including gender, law
school they graduated from, office, practice, status, years
with the firm, and age. The first five types of this side information is incorporated into MDMP for evaluation.
The block modeling results are visualized in Figure 5 and
the performance comparison results are reported in Table 1.
For a fair comparison, we select proper hyper-parameters to
make the total number of blocks in IRM, MP and MDMP be approximately at the same level. From Figure 5, we
can see that MDMP can uncover clearer block structures
than IRM and MP. From Table 1, we can see that MDMP
not only most fits the data (in LL) but also has the most
parsimonious model structure (in BIC). It is worth noting that, among five types of meta information, “gender”
is most helpful for predicting friendships, “law school” is
most helpful for predicting advisory, and “practice” is most
helpful for predicting workmates.
5.2. Link Prediction: Douban
Douban is an SNS provider which allows users to share and
review movies, books, and music. The user connections in
Douban form an asymmetric network, on which each user
has a profile with demographical information. We adopt
“City” information of each user as the meta information
for MDMP. In this experiment, we adopt a preprocessed
data set4 (Ma et al., 2011), which comprises 21593 users.
We randomly select 50 users for evaluation. The density of
links in the resulting network is around 14.3%.
The block modeling results are visualized in Figure 6 and
the performance comparison results are reported in Table 2.
We can see that MDMP has a more parsimonious block
structure than IRM and a clearer block structure than MP.
MDMP performs best; while MP performs even worse than
IRM and seems to have not yet converged given the same
number of sampling iterations.
3

https://www.stats.ox.ac.uk/˜snijders/
siena/Lazega_lawyers_data.htm
4
https://www.cse.cuhk.edu.hk/irwin.king.
new/pub/data/douban

Figure 6. Block structure visualization on the Douban dataset.

Dataset: Douban
LL (Blockmodeling)
BIC (Blockmodeling)
AUC (Prediction)

IRM
-782.8
1690.7
0.7420

MP
-935.6
1925.8
0.6987

MDMP
-757.5
1509.5
0.7638

Table 2. Performance comparison on the Douban dataset.

5.3. Rating Prediction: MovieLens
We adopt the MovieLens data set5 for rating prediction. It
comprises 6040 users and 3883 movies. Each user is associated with three types of meta information: gender, age
and career. We don’t consider movie meta information for
simplicity. The three types of user meta information are incorporated into MDMP. We randomly select 70 users and
70 items from the entire data set and keep the sparsity of
the rating matrix being 80% for evaluation.
The block modeling results are visualized in Figure 7 and
the performance comparison results are reported in Table 3.
From Figure 7, we can see that the ratings within blocks
are more homogeneous in MDMP than in BiLDA and MP,
especially in the case of incorporating gender information.
From Table 3, we can see that MDMP with gender gives
the best block modeling result; while MDMP with career
gives the best rating prediction result.

6. Conclusion
In this paper, we propose a metadata dependent Mondrian process (MDMP) that incorporates meta information of
entities into the partition process. MDMP can not only encourage homogeneous relational data within blocks but also discourage meta-label diversity within blocks. By incorporating meta information, MDMP becomes more robust
in data sparsity scenarios and converges faster in posterior
inference. The empirical tests on three real-world data sets
demonstrate that, regularized by meta information, MDMP
can uncover clearer block structures than IRM and MP with
a more parsimonious model structure and higher prediction
accuracy. In our future work, we will 1) investigate how to
make better use of meta information and 2) exploit MDMP
for generating related rating matrices (Li et al., 2009).
5

https://movielens.org/

Metadata Dependent Mondrian Processes

(a) Input:Adv.

(b) IRM

(c) MP

(d) MDMP:G

(e) MDMP:L

(f) MDMP:O

(g) MDMP:P

(h) MDMP:S

(a) Input:Fri.

(b) IRM

(c) MP

(d) MDMP:G

(e) MDMP:L

(f) MDMP:O

(g) MDMP:P

(h) MDMP:S

(a) Input:Work.

(b) IRM

(c) MP

(d) MDMP:G

(e) MDMP:L

(f) MPMP:O

(g) MDMP:P

(h) MDMP:S

Figure 5. Block structure visualization on the Lazega’s Lawyer dataset: (a) Input data with three types of links (Advisory, Friend,
Workmate); (b) IRM; (c) MP; (d–h) MDMP with 5 different meta information (Gender, Lawschool, Office, Practice, Status).

Dataset: Lazega Lawyer’s
LL (Blockmodeling): Advisory
LL (Blockmodeling): Friend
LL (Blockmodeling): Work
BIC (Blockmodeling): Advisory
BIC (Blockmodeling): Friend
BIC (Blockmodeling): Work
AUC (Prediction): Advisory
AUC (Prediction): Friend
AUC (Prediction): Work

IRM
-1961.3
-1546.6
-1960.7
3990.7
3127.0
3972.4
0.7418
0.7208
0.6752

MP
-2206.8
-1653.6
-2048.5
4481.8
3315.7
4207.8
0.7026
0.6954
0.6662

MDMP:G
-1996.0
-1507.9
-1955.2
4000.4
3024.2
3918.9
0.7497
0.7715
0.6265

MDMP:L
-1953.7
-1521.8
-1958.2
3916.0
3052.2
3924.8
0.7645
0.7547
0.6379

MDMP:O
-1970.7
-1546.0
-1958.1
3949.9
3100.5
3924.6
0.7605
0.7116
0.6629

MDMP:P
-1994.2
-1552.3
-1954.3
3996.9
3113.2
3917.1
0.7507
0.6857
0.6767

MDMP:S
-1974.9
-1534.0
-1954.7
3958.4
3076.5
3917.9
0.7518
0.7407
0.6842

Table 1. Performance comparison on the Lazega’s Lawyer dataset.

(a) Input

(b) BiLDA

(c) MP

(d) MDMP:Age

(e) MDMP:Career

(f) MDMP:Gender

Figure 7. Block structure visualization on the MovieLens dataset (five rating scores are denoted by the color ranging from green to
purple; unobserved ratings are denoted by white entries).

Dataset: MovieLens
LL (Blockmodeling)
BIC (Blockmodeling)
RMSE (Prediction)

BiLDA
-1256.4
2650.8
0.8116

MP
-1244.7
2493.5
0.7916

MDMP:Age
-1246.7
2497.4
0.7982

MDMP:Career
-1243.2
2491.4
0.7239

Table 3. Performance comparison on the MovieLens dataset.

MDMP:Gender
-1225.4
2456.8
0.7755

Metadata Dependent Mondrian Processes

References
Airoldi, Edoardo M., Blei, David M., Fienberg, Stephen E.,
and Xing, Eric P. Mixed membership stochastic blockmodels. In NIPS, pp. 33–40, 2009.
Blei, David M. and Frazier, Peter I. Distance dependent
chinese restaurant processes. Journal of Machine Learning Research, 12:2383–2410, 2011.
Choi, David S., Wolfe, Patrick, and Airoldi, Edoardo M.
Confidence sets for network structure. In NIPS, pp.
2097–2105, 2011.
Geman, Stuart and Geman, Donald. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6(6):721–741, 1984.
Gershman, Samuel, Frazier, Peter, and Blei, David. Distance dependent infinite latent feature models. In IEEE
Transactions on Pattern Analysis and Machine Intelligence, 2014.

MacEachern, Steven N. Dependent Dirichlet processes.
Unpublished manuscript, Department of Statistics, The
Ohio State University, 2000.
Miller, Kurt, Jordan, Michael I., and Griffiths, Thomas L.
Nonparametric latent feature models for link prediction.
In NIPS, pp. 1276–1284, 2009.
Muthukrishnan, S., Poosala, Viswanath, and Suel, Torsten.
On rectangular partitionings in two dimensions: Algorithms, complexity and applications. In ICDT, pp. 236–
256, 1999.
Nakano, Masahiro, Ishiguro, Katsuhiko, Kimura, Akisato,
Yamada, Takeshi, and Ueda, Naonori. Rectangular tiling
process. In ICML, pp. 361–369, 2014.
Porteous, Ian, Bart, Evgeniy, and Welling, Max. MultiHDP: A non parametric Bayesian model for tensor factorization. In AAAI, pp. 1487–1490, 2008.
Ren, Lu, Wang, Yingjian, Carin, Lawrence, and Dunson,
David B. The kernel beta process. In NIPS, pp. 963–
971, 2011.

Green, Peter J. Reversible jump Markov chain Monte
Carlo computation and Bayesian model determination.
Biometrika, 82(4):711–732, 1995.

Roy, Daniel M. and Teh, Yee W. The Mondrian process. In
NIPS, pp. 1377–1384, 2009.

Holland, Paul W., Laskey, Kathryn Blackmond, and Leinhardt, Samuel. Stochastic blockmodels: First steps. Social Networks, 5(2):109–137, 1983.

Wang, Pu, Laskey, Kathryn B., Domeniconi, Carlotta,
and Jordan, Michael I. Nonparametric Bayesian coclustering ensembles. In SDM, pp. 331–342, 2011.

Ishiguro, Katsuhiko, Iwata, Tomoharu, Ueda, Naonori, and
Tenenbaum, Joshua B. Dynamic infinite relational model for time-varying relational data analysis. In NIPS, pp.
919–927, 2010.

White, Harrison C., Boorman, Scott A., and Breiger,
Ronald L. Social structure from multiple networks. i.
blockmodels of roles and positions. American Journa of
Sociology, pp. 730–780, 1976.

Kemp, Charles, Tenenbaum, Joshua B., Griffiths,
Thomas L., Yamada, Takeshi, and Ueda, Naonori.
Learning systems of concepts with an infinite relational model. In AAAI, pp. 381–388, 2006.

Williamson, Sinead, Orbanz, Peter, and Ghahramani,
Zoubin. Dependent Indian buffet processes. In AISTATS,
pp. 924–931, 2010.

Kim, Dae Il, Hughes, Michael, and Sudderth, Erik. The
nonparametric metadata dependent relational model. In
ICML, pp. 1559–1566, 2012.
Lazega, Emmanuel. The collegial phenomenon: The social
mechanisms of cooperation among peers in a corporate
law partnership. Administrative Science Quarterly, 48
(3):525–529, 2003.
Li, Bin, Yang, Qiang, and Xue, Xiangyang. Transfer learning for collaborative filtering via a rating-matrix generative model. In ICML, pp. 617–624, 2009.
Ma, Hao, Zhou, Dengyong, Liu, Chao, Lyu, Michael R.,
and King, Irwin. Recommender systems with social regularization. In WSDM, pp. 287–296, 2011.

Zhou, Mingyuan, Yang, Hongxia, Sapiro, Guillermo, Dunson, David B., and Carin, Lawrence. Dependent hierarchical beta process for image interpolation and denoising. In AISTATS, pp. 883–891, 2011.

