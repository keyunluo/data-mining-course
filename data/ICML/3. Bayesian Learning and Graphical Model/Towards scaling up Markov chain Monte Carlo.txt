Towards scaling up Markov chain Monte Carlo:
an adaptive subsampling approach

Rémi Bardenet
Arnaud Doucet
Chris Holmes
Department of Statistics, University of Oxford, Oxford OX1 3TG, UK

Abstract
Markov chain Monte Carlo (MCMC) methods
are often deemed far too computationally intensive to be of any practical use for large datasets.
This paper describes a methodology that aims
to scale up the Metropolis-Hastings (MH) algorithm in this context. We propose an approximate implementation of the accept/reject step of
MH that only requires evaluating the likelihood
of a random subset of the data, yet is guaranteed
to coincide with the accept/reject step based on
the full dataset with a probability superior to a
user-specified tolerance level. This adaptive subsampling technique is an alternative to the recent approach developed in (Korattikara et al.,
2014), and it allows us to establish rigorously that
the resulting approximate MH algorithm samples
from a perturbed version of the target distribution of interest, whose total variation distance to
this very target is controlled explicitly. We explore the benefits and limitations of this scheme
on several examples.

(2004, Chapter 7.3)). MH consists in building an ergodic
Markov chain of invariant distribution π(θ). Given a proposal q(θ0 |θ), the MH algorithm starts its chain at a userdefined θ0 , then at iteration k + 1 it proposes a candidate
state θ0 ∼ q(·|θk ) and sets θk+1 to θ0 with probability
α(θk , θ0 )

A standard approach to sample approximately from π(θ) is
the Metropolis-Hastings algorithm (MH; Robert & Casella
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

=

1∧

π(θ0 ) q(θk |θ0 )
π(θk ) q(θ0 |θk )

=

1∧

n
p (θ0 ) q(θk |θ0 ) Y p(xi |θ0 )
, (1)
p (θk ) q(θ0 |θk ) i=1 p(xi |θk )

while θk+1 is otherwise set to θk . When the dataset is large
(n  1), evaluating the likelihood ratio appearing in the
MH acceptance ratio (1) is too costly an operation and rules
out the applicability of such a method.
The aim of this paper is to propose an approximate implementation of this “ideal” MH sampler, the maximal approximation error being pre-specified by the user. To achieve
this, we first present the “ideal” MH sampler in a slightly
non-standard way.
In practice, the accept/reject step of the MH step is implemented by sampling a uniform random variable u ∼ U(0,1)
and accepting the candidate if and only if

1. Introduction
Consider a dataset X = {x1 , ..., xn } and denote by
p(x1 , ..., xn |θ) the associated likelihood for parameter θ ∈
Θ. Henceforth we assume that the dataQare conditionally inn
dependent, so that p(x1 , ..., xn |θ) = i=1 p(xi |θ). Given
a prior p(θ), Bayesian inference relies on the posterior
π(θ) ∝ p(x1 , ..., xn |θ)p(θ). In most applications, this posterior is intractable and we need to rely on Bayesian computational tools to approximate it.

REMI . BARDENET @ GMAIL . COM
DOUCET @ STATS . OX . AC . UK
CHOLMES @ STATS . OX . AC . UK

u<

π(θ0 ) q(θk |θ0 )
.
π(θk ) q(θ0 |θk )

(2)

In our specific context, it follows from (2) and the independence assumption that there is acceptance of the candidate
if and only if
Λn (θk , θ0 ) > ψ(u, θk , θ0 ),

(3)

where for θ, θ0 ∈ Θ we define the average log likelihood
ratio Λn (θ, θ0 ) by


n
1X
p(xi |θ0 )
Λn (θ, θ0 ) =
log
(4)
n i=1
p(xi |θ)
and where
ψ(u, θ, θ0 ) =



1
q(θ0 |θ)p(θ)
log u
.
n
q(θ|θ0 )p(θ0 )

Towards scaling up MCMC: an adaptive subsampling approach

The pseudocode of MH is given in Figure 1, unusually formulated using the expression (3). The advantage of this
presentation is that it clearly outlines that the accept/reject
step of MH requires checking whether or not (3) holds.
MH p(x|θ), p(θ), q(θ0 |θ), θ0 , Niter , X



1 for k ← 1 to Niter
2

θ ← θk−1

3

5

θ0 ∼ q(.|θ), u ∼ U(0,1) ,
h
i
p(θ)q(θ 0 |θ)
ψ(u, θ, θ0 ) ← n1 log u p(θ
0 )q(θ|θ 0 )
h
i
0
Pn
i |θ )
Λn (θ, θ0 ) ← n1 i=1 log p(x
p(xi |θ)

6

if Λn (θ, θ0 ) > ψ(u, θ, θ0 )

4

7
8

θk ← θ 0

. Accept

else θk ← θ . Reject

9 return (θk )k=1,...,Niter

Figure 1. The pseudocode of the MH algorithm targeting the posterior π(θ) ∝ p(x1 , ..., xn |θ)p(θ). The formulation singling
out Λn (θ, θ0 ) departs from conventions (Robert & Casella, 2004,
Chapter 7.3) but serves the introduction of our main algorithm
MHS UB L HD in Figure 3.

So as to save computational efforts, we would like to be
able to decide whether (3) holds using only a Monte Carlo
approximation of Λn (θ, θ0 ) based on a subset of the data.
There is obviously no hope to be able to guarantee that
we will take the correct decision with probability 1 but
we would like to control the probability of taking an erroneous decision. Korattikara et al. (2014) propose in a
similar large datasets context to control this error using an
approximate confidence interval for the Monte Carlo estimate. Similar ideas have actually appeared earlier in the
operations research literature. Bulgak & Sanders (1988),
Alkhamis et al. (1999) and Wang & Zhang (2006) consider
maximizing a target distribution whose logarithm is given
by an intractable expectation; in the large dataset scenario
this expectation is w.r.t the empirical measure of the data.
They propose to perform this maximization using simulated annealing, a non-homogeneous version of MH. This
is implemented practically by approximating the MH ratio
log π(θ0 )/π(θ) through Monte Carlo and by determining
an approximate confidence interval for the resulting estimate; see also (Singh et al., 2012) for a similar idea developed in the context of inference in large-scale factor graphs.
All these approaches rely on approximate confidence intervals so they do not allow to control rigorously the approximation error. Moreover, the use of approximate confidence
intervals can yield seriously erroneous inference results as
demonstrated in Section 4.1.
The method presented in this paper is a more robust alternative to these earlier proposals, which can be analyzed
theoretically and whose properties can be better quantified.

As shown in Section 2, it is possible to devise an adaptive
sampling strategy which guarantees that we take the correct decision, i.e. whether (3) holds or not, with at worst
a user-specified maximum probability of error. This sampling strategy allows us to establish in Section 3 various
quantitative convergence results for the associated Markov
kernel. In Section 4, we compare our approach to the one
by Korattikara et al. (2014) on a toy example and demonstrate the performance of our methodology on a large-scale
Bayesian logistic regression problem. All proofs are in the
supplemental paper.

2. A Metropolis-Hastings algorithm with
subsampled likelihoods
In this section, we use concentration bounds so as to obtain
exact confidence intervals for Monte Carlo approximations
of the log likelihood ratio (4). We then show how such
bounds can be exploited so as to build an adaptive sampling
strategy with desired guarantees.
2.1. MC approximation of the log likelihood ratio
Let θ, θ0 ∈ Θ. For any integer t≥1, a Monte Carlo approximation Λ∗t (θ, θ0 ) of Λn (θ, θ0 ) is given by
Λ∗t (θ, θ0 )



t
p(x∗i |θ0 )
1X
log
,
=
t i=1
p(x∗i |θ)

(5)

where x∗1 , . . . , x∗t are drawn uniformly over {x1 , . . . , xn }
without replacement.
We can quantify the precision of our estimate Λ∗t (θ, θ0 ) of
Λn (θ, θ0 ) through concentration inequalities, i.e., a statement that for δt > 0,
P(|Λ∗t (θ, θ0 ) − Λn (θ, θ0 )| ≤ ct ) ≥ 1 − δt ,

(6)

for a given ct . Hoeffding’s inequality without replacement
Serfling (1974), for instance, uses
r
2(1 − ft∗ ) log(2/δt )
ct = Cθ,θ0
,
(7)
t
where
Cθ,θ0 = max | log p(xi |θ0 ) − log p(xi |θ)|
1≤i≤n

(8)

and ft∗ = t−1
n is approximately the fraction of used samples. The term (1 − ft∗ ) in (7) decreases to n1 as the number t of samples used approaches n, which is a feature
of bounds corresponding to sampling without replacement.
Let us add that Cθ,θ0 typically grows slowly with n. For
instance, if the likelihood is Gaussian, then Cθ,θ0 is proportional to maxni=1 |xi |, so that if the data actually were

Towards scaling up MCMC: an adaptive subsampling approach

sampled from a Gaussian, Cθ,θ0 would grow in
(Cesa-Bianchi & Lugosi, 2009, Lemma A.12).

p
log(n)

If the empirical standard deviation σ̂t of {log p(xi |θ0 ) −
log p(xi |θ)} is small, a tighter bound known as the empirical Bernstein bound (Audibert et al., 2009)
r
2 log(3/δt ) 6Cθ,θ0 log(3/δt )
ct = σ̂t
+
,
(9)
t
t
applies. While the bound of Audibert et al. (2009) originally covers the case where the x∗i are drawn with replacement, it was early remarked (Hoeffding, 1963) that Chernoff bounds, such as the empirical Bernstein bound, still
hold when considering sampling without replacement. Finally, we will also consider the recent Bernstein bound of
Bardenet & Maillard (2013, Theorem 3), designed specifically for the case of sampling without replacement.

A slight variation of this procedure is actually implemented
in practice; see Figure 3. The sequence (δt ) is decreasing, and each time we check in Step 19 whether or not we
should break out of the while condition, we have to use a
smaller δt , yielding a smaller ct . Every check of Step 19
thus makes the next check less likely to succeed. Thus, it
appears natural not to perform Step 19 systematically after
each new x∗i has been drawn, but rather draw several new
subsamples x∗i between each check of Step 19. This is why
we introduce the variable tlook is Steps 6, 16, and 17 of Figure 3. This variable simply counts the number of times the
check in Step 19 was performed. Finally, as recommended
in a related setting in (Mnih et al., 2008; Mnih, 2008), we
augment the size of the subsample geometrically by a userinput factor γ > 1 in Step 18. Obviously this modification
does not impact the fact that the correct decision is taken
with probability at least 1 − δ.

2.2. Stopping rule construction
The concentration bounds given above are helpful as they
can allow us to decide whether (3) holds or not. Indeed,
on the event {|Λ∗t (θ, θ0 ) − Λn (θ, θ0 )| ≤ ct }, we can decide whether or not Λn (θ, θ0 ) > ψ(u, θ, θ0 ) if |Λ∗t (θ, θ0 ) −
ψ(u, θ, θ0 )| > ct additionally holds. This is illustrated
in Figure 2. Combined to the concentration inequality
(6), we thus take the correct decision with probability at
least 1 − δt if |Λ∗t (θ, θ0 ) − ψ(u, θ, θ0 )| > ct . In case
|Λ∗t (θ, θ0 ) − ψ(u, θ, θ0 )| ≤ ct , we want to increase t until the condition|Λ∗t (θ, θ0 ) − ψ(u, θ, θ0 )| > ct is satisfied.
Let δ ∈ (0, 1) be a user-specified parameter. We provide a
construction which ensures that at the first random time T
such that |Λ∗T (θ, θ0 ) − ψ(u, θ, θ0 )| > cT , the correct decision is taken with probability at least 1 − δ. This adaptive
stopping rule adapted from (Mnih et al., 2008) is inspired
by bandit algorithms, Hoeffding races (Maron & Moore,
1993) and procedures developed to scale up boosting algorithms to large datasets (Domingo & Watanabe, 2000).
Formally, we set the stopping time
T = n ∧ inf{t ≥ 1 :

|Λ∗t (θ, θ0 ) − ψ(u, θ, θ0 )|

> ct }, (10)

where a ∧ b denotes the minimum of a and b. In other
words, if the infimum in (10) is larger than n, then we
stop as our sampling without replacement procedure ensures Λ∗n (θ, θ0 ) = Λn P
(θ, θ0 ). Letting p > 1 and selecting
p−1
δt = ptp δ, we have t≥1 δt ≤ δ. Setting (ct )t≥1 such
that (6) holds, the event
\
E=
{|Λ∗t (θ, θ0 ) − Λn (θ, θ0 )| ≤ ct }
(11)

MHS UB L HD p(x|θ), p(θ), q(θ0 |θ), θ0 , Niter , X , (δt ), Cθ,θ0



1 for k ← 1 to Niter
2

θ ← θk−1

3
4

θ0 ∼ q(.|θ), u ∼ U(0,1) ,
h
i
p(θ)q(θ 0 |θ)
ψ(u, θ, θ0 ) ← n1 log u p(θ
0 )q(θ|θ 0 )

5

t←0

6

tlook ← 0

7
8

Λ∗ ← 0
X ∗ ← ∅ . Keeping track of points already used

9

b ← 1 . Initialize batchsize to 1

10

D ONE ← FALSE

11

while D ONE == FALSE do

12

x∗t+1 , . . . , x∗b ∼w/o repl. X \ X ∗ . Sample new
batch without replacement

14

X ∗ ← X ∗ ∪ {x∗t+1 , . . . , x∗b }

h ∗ 0 i
Pb
p(x |θ )
Λ∗ ← 1b tΛ∗ + i=t+1 log p(xi∗ |θ)

15

t←b

16

c ← 2Cθ,θ0

17

tlook ← tlook + 1

18

b ← n ∧ dγte . Increase batchsize geometrically

19

if |Λ∗ − ψ(u, θ, θ0 )| ≥ c or b > n

13

i

20
21

q

(1−ft∗ ) log(2/δtlook )
2t

D ONE ← T RUE
if Λ∗ > ψ(u, θ, θ0 )

22

θk ← θ0 . Accept

23

else θk ← θ . Reject

24 return (θk )k=1,...,Niter

t≥1

has probability larger than 1−δ under sampling without replacement by a union bound argument. Now by definition
of T , if E holds then Λ∗T (θ, θ0 ) yields the correct decision,
as pictured in Figure 2.

Figure 3. Pseudocode of the MH algorithm with subsampled likelihoods. Step 16 uses a Hoeffding bound, but other choices of
concentration inequalities are possible. See main text for details.

Towards scaling up MCMC: an adaptive subsampling approach

Figure 2. Schematic view of the acceptance mechanism of MHS UB L HD given in Figure 3: if |Λ∗t (θ, θ0 ) − ψ(u, θ, θ0 )| > ct , then
MHS UB L HD takes the acceptance decision based on Λ∗t (θ, θ0 ), without requiring to compute Λn (θ, θ0 ).

3. Analysis
In Section 3.1 we provide an upper bound on the total
variation norm between the iterated kernel of the approximate MH kernel and the target distribution π of MH applied to the full dataset, while Section 3.2 focuses on the
number T of subsamples required by a given iteration of
MHS UB L HD. We establish a probabilistic bound on T and
give a heuristic to determine whether a user can expect a
substantial gain in terms of number of samples needed for
the problem at hand.
3.1. Properties of the approximate transition kernel
For θ, θ0 ∈ Θ, we denote by

R
norm, where ν(f ) = Θ f (θ)ν (dθ). For any Markov
kernel Q on (Θ, B (Θ)) , we denote by Qk be the k-th
iterate kernel Rdefined by induction for k ≥ 2 through
Qk (θ, dθ0 ) = Θ Q(θ, dϑ)Qk−1 (ϑ, dθ0 ) with Q1 = Q.
Proposition 3.2. Let P be uniformly geometrically ergodic, i.e., there exists an integer m and a probability measure ν on (Θ, B (Θ)) such that for all θ ∈ Θ, P m (θ, ·) ≥
(1 − ρ) ν(·). Hence there exists A < ∞ such that
∀θ ∈ Θ, ∀k > 0, kP k (θ, ·) − πkTV ≤ Aρbk/mc .

(14)

Then there exists B < ∞ and a probability distribution π̃
on (Θ, B (Θ)) such that for all θ ∈ Θ and k > 0,
m

P (θ, dθ0 ) = α(θ, θ0 )q(θ0 |θ)dθ0


Z
+ δθ (dθ0 ) 1 − α(θ, ϑ)q(ϑ|θ)dϑ

kP̃ k (θ, ·) − π̃kTV ≤ B[1 − (1 − δ) (1 − ρ)]bk/mc . (15)
(12)

the “ideal” MH kernel targeting π with proposal q, where
the acceptance probability α(θ, θ0 ) is defined in (1). Denote
the acceptance probability of MHS UB L HD in Figure 3 by
0

α̃(θ, θ ) = E1{Λ∗T (θ,θ0 )>ψ(u,θ,θ0 )} ,

(13)

where the expectation in (13) is with respect to u ∼ U(0,1)
and the variables T and x∗1 , . . . , x∗T described in Section 2.
Finally, denote by P̃ the MHS UB L HD kernel, obtained by
substituting α̃ to α in (12). The following Lemma states
that the absolute difference between α and α̃ is bounded by
the user-defined parameter δ > 0.
Lemma 3.1. For any θ, θ0 ∈ Θ, we have |α(θ, θ0 ) −
α̃(θ, θ0 )| ≤ δ.
Lemma 3.1 can be used to establish Proposition 3.2, which
states that the chain output by the algorithm MHS UB L HD
in Figure 3 is a controlled approximation to the original
target π. For any signed measure ν on (Θ, B (Θ)), let
kνkTV = 21 supf :Θ→[−1,1] ν (f ) denote the total variation

Furthermore, π̃ satisfies
kπ − π̃kTV ≤

Amδ
.
1−ρ

(16)

One may obtain tighter bounds and ergodicity results by
weakening the uniform geometric ergodicity assumption
and using recent results on perturbed Markov kernels (Ferré
et al., 2013), but this is out of the scope of this paper.
3.2. On the stopping time T
3.2.1. A BOUND FOR FIXED θ, θ0
The following Proposition gives a probabilistic upper
bound for the stopping time T , conditionally on θ, θ0 ∈ Θ
and u ∈ [0, 1] and when ct is defined by (7). A similar
bound holds for the empirical Bernstein bound in (9).
0
Proposition 3.3. Let δ > 0 and δt = p−1
ptp δ. Let θ, θ ∈ Θ
such that Cθ,θ0 6= 0 and u ∈ [0, 1]. Let

∆=

|Λn (θ, θ0 ) − ψ(u, θ, θ0 )|
2Cθ,θ0

(17)

Towards scaling up MCMC: an adaptive subsampling approach

and assume ∆ 6= 0. Then if p > 1, with probability at least
1 − δ,






4p
2p
4
p log
+ log
∨ 1. (18)
T ≤
∆2
∆2
δ(p − 1)

mension is larger than 2 and 50% else (Roberts & Rosenthal, 2001). Hyperparameters of MHS UB L HD were set to
p = 2, γ = 2, and δ = 0.01. The first two were found to
work well with all experiments. We found empirically that
the algorithm is very robust to the choice of δ.

The relative distance ∆ in (18) characterizes the difficulty of the step. Intuitively, at equilibrium, i.e., when
(θ, θ0 ) ∼ π(θ)q(θ0 |θ) and u ∼ U[0,1] , if the log likelihood
log p(x|θ) is smooth in θ, the proposal could be chosen so
that Λn (θ, θ0 ) has positive expectation and a small variance,
thus leading to high values of ∆ and small values of T .

4.1. On the use of asymptotic confidence intervals

3.2.2. A HEURISTIC AT EQUILIBRIUM
Integrating (18) with respect to θ, θ0 to obtain an informative quantitative bound on the average number of samples
required by MHS UB L HD at equilibrium would be desirable but proved difficult. However the following heuristic can help the user figure out whether our algorithm will
yield important gains for a given problem. For large n,
standard asymptotics (van der Vaart, 2000) yield that the
log likelihood is approximately a quadratic form
log p(x|θ) ≈ −(θ − θ∗ )T Hn (θ − θ∗ )
with Hn of order n. Assume the proposal q(·|θ) is a Gaussian random walk N (·|θ, Γ) of covariance Γ, then the expected log likelihood ratio under π(θ)q(θ0 |θ) is approximately Trace(Hn Γ). According to (Roberts & Rosenthal,
2001), an efficient random walk Metropolis requires Γ to
be of the same order as Hn−1 , that is, of order 1/n. Finally,
the expected Λn (θ, θ0 ) at equilibrium is of order 1/n, and
can thus be compared to ψ(u, θ, θ0 ) = log(u)/n in Line 19
of MHS UB L HD in Figure 3. The RHS of the first inequality in Step 19 is the√concentration bound ct , which has a
leading term in σ̂t / t in the case of (9). In the applications we consider in Section 4,√
σ̂t is typically proportional
to kθ − θ0 k, which is of order n since Γ ≈ Hn−1 . Thus,
to break out of the while loop in Line 19, we need t ∝ n.
At equilibrium, we thus should not expect gains of several
orders of magnitude: gains are fixed by the constants in
the proportionality relations above, which usually depend
on the empirical distribution of the data. We provide a detailed analysis for a simple example in Section 4.3.

MCMC algorithms based on subsampling and asymptotic
confidence intervals experimentally lead to efficient optimization procedures (Bulgak & Sanders, 1988; Alkhamis
et al., 1999; Wang & Zhang, 2006), and perform well in
terms of classification error when used, e.g., in logistic regression (Korattikara et al., 2014). However, in terms of
approximating the original posterior, they come with no
guarantee and can provide unreliable results.
To illustrate this, we consider the following setting. X
is a synthetic sample of size 105 drawn from a Gaussian N (0, 0.12 ), and we estimate the parameters µ, σ of
a N (µ, σ 2 ) model, with flat priors. Analytically, we know
that the posterior has its maximum at the empirical mean
and variance of X . Running the approximate MH algorithm of Korattikara et al. (2014), using a level for the test
 = 0.05, and starting each iteration by looking at t = 2
points so as to be able to compute the variance of the log
likelihood ratios, leads to the marginal of σ shown in Figure 4(a).
The empirical variance of X is denoted by a red triangle, and the maximum of the marginal is off by 7% from
this value. Relaunching the algorithm, but starting each
iteration with a minimum t = 500 points, leads to better agreement and a smaller support for the marginal, as
depicted in Figure 4(b). Still, t = 500 works better for
this example, but fails dramatically if X are samples from
a lognormal logN (0, 2), as depicted in Figure 4(c). The
asymptotic regime, in which the studentized statistic used
in (Korattikara et al., 2014) actually follows a Student distribution, depends on the problem at hand and is left to the
user to specify. In each of the three examples of Figure 4,
our algorithm produces significantly better estimates of the
marginal, though at the price of a significantly larger average number of samples used per MCMC iteration. In particular, the case X ∼ logN (0, 2) in Figure 4(c) essentially
requires to use the whole dataset.

4. Experiments

4.2. Large-scale Bayesian logistic regression

All experiments were conducted using the empirical
Bernstein-Serfling bound of Bardenet & Maillard (2013),
which revealed equivalent to the empirical Bernstein
bound in (9), and much tighter in our experience with
MHS UB L HD than Hoeffding’s bound in (7). All MCMC
runs are adaptive Metropolis (Haario et al., 2001; Andrieu
& Thoms, 2008) with target acceptance 25% when the di-

In logistic regression, an accurate approximation of the
posterior is often needed rather than minimizing the classification error, for instance, when performing Bayesian
variable selection. This makes logistic regression for large
datasets a natural application for our algorithm, since the
constant Cθ,θ0 in concentration inequalities such as (9) can

Towards scaling up MCMC: an adaptive subsampling approach
1800
1600
1400
1200
1000
800
600
400
200
0
0.080

Approx. CI, 3% of n
MHSubLhd, 54% of n

Approx. CI, 16% of n
MHSubLhd, 54% of n

2500
2000
1500
1000
500

0.085

0.090

0.095

0.100

0.105

2

0
0.095

0.096

0.097

0.098

0.099

0.100

0.101

0.102

0.103

(b) X ∼ N (0, 0.12 ), starting at t = 500

(a) X ∼ N (0, 0.1 ), starting at t = 2
3.0
2.5
2.0

Approx. CI, 21% of n
MHSubLhd, 99% of n

1.5
1.0
0.5
0.0
25

30

35

40

45

50

(c) X ∼ lnN (0, 2), starting at t = 500

(d) train error vs. time

Figure 4. (a,b,c) Estimates of the marginal posteriors of σ obtained respectively by the algorithm of Korattikara et al. (2014) using
approximate confidence intervals and our algorithm MHS UB L HD given in Figure 3, for X sampled from each of the distributions
indicated below the plots, and with different starting points for the number t of samples initially drawn from X at each MH iteration.
On each plot, a red triangle indicates the true maximum of the posterior, and the legend indicates the proportion of X used on average
by each algorithm. (d) The synthetic dataset used in Section 4.2.2. The dash-dotted line indicates the Bayes classifier.

be computed as follows. The log likelihood
log p(y|x, θ) = − log(1 + e

−θ T x

T

) − (1 − y)θ x

(19)

is L-Lipschitz in θ with L = kxk, so that we can set
Cθ,θ0 = kθ − θ0 k max kxj k.
1≤j≤n

We expect the Lipschitz inequality to be tight as (19) is
almost linear in θ.
4.2.1. T HE covtype DATASET
We consider the dataset covtype.binary1 described in (Collobert et al., 2002). The dataset consists of 581,012 points,
of which we pick n = 400, 000 as a training set, following
the maximum training size in (Collobert et al., 2002). The
original dimension of the problem is 54, with the first 10
attributes being quantitative. To illustrate our point without requiring a more complex sampler than MH, we consider a simple variant of the classification problem using
the first q = 2 attributes only. We use the preprocessing
and Cauchy prior recommended by Gelman et al. (2008).
We draw four random starting points and launch independent runs of both the traditional MH in Figure 1 and
1
available
at
http://www.csie.ntu.edu.tw/
cjlin/libsvmtools/datasets/binary.html
˜

our MHS UB L HD in Figure 3 at each of these four starting points. Figure 5 shows the results: plain lines indicate traditional MH runs, while dashed lines indicate
runs of MHS UB L HD. Figures 5(c) and 5(d) confirm that
MHS UB L HD accurately approximates the target posterior. In all Figures 5(a) to 5(d), MHS UB L HD reproduces
the behaviour of MH, but converges up to 3 times faster.
However, the most significant gains in number of samples used happen in the initial transient phase. This allows fast progress of the chains towards the mode but, once
in the mode, the average number of samples required by
MHS UB L HD is close to n. We observed the same behaviour when considering all q = 10 quantitative attributes
of the dataset, as depicted by the train error in Figure 7(a).
4.2.2. S YNTHETIC DATA
To investigate the rôle of n in the gain, we generate a 2D binary classification dataset of size n = 107 . Given the label,
both classes are sampled from unit Gaussians centered on
the x-axis, and a subsample of X is shown in Figure 4(d).
The results are shown in Figure 6. The setting appears more
favorable than in Section 4.2.1, and MHS UB L HD chains
converge up to 5 times faster. The average number of samples used is smaller, but it is still around 70% after the tran-

Towards scaling up MCMC: an adaptive subsampling approach

(a) train error vs. time

(b) test error vs. time

(c) 30% quantile of the chain vs. time

(d) mean of the chain vs. time

Figure 5. Results of 4 independent runs of MH (plain lines) and MHS UB L HD (dashed lines) for the 2 first attributes of the covtype
dataset. The legend indicates the average number of samples required as a proportion of n.

(a) train error vs. time

(b) test error vs. time

(c) 30% quantile of the chain vs. time

(d) mean of the chain vs. time

Figure 6. Results of 4 independent runs of MH (plain lines) and MHS UB L HD (dashed lines) for the synthetic dataset described in
Section 4.2.2. The legend indicates the average number of samples required as a proportion of n. On Figures 6(a) and 6(b), a dashdotted line indicates the error obtained by the Bayes classifier.

Towards scaling up MCMC: an adaptive subsampling approach

(a) train error vs. time

(b)

Figure 7. (a) Results of 4 independent runs of MH (plain lines) and MHS UB L HD (dashed lines) for the 10 quantitative attributes of the
covtype dataset. The legend indicates the average number of samples required as a proportion of n. (b) Running proportion of samples
needed vs. iteration number for different values of n, for the Gaussian experiment of Section 4.3.

5. Conclusion

sient phase for all approximate chains.
4.3. A Gaussian example
To further investigate when gains are made at equilibrium,
we now consider inferring the mean θ of a N (θ, 1) model,
2
using a sample X ∼ N ( 21 , σX
) of size n. Although simple, this setting allows us analytic considerations. The log
likelihood ratio is
p(x|θ0 )
θ + θ0
log
= (θ0 − θ)(x −
)
p(x|θ)
2
so that we can set
C

θ,θ 0

|θ + θ0 |
= 2|θ − θ| max |xi | +
1≤i≤n
2
0

We also remark that
s
Vx log



p(x|θ0 )
= |θ − θ0 |σX .
p(x|θ)


.

(20)

Under the equilibrium assumptions of Section 3.2.2, |θ −
θ0 | is of order n−1/2 , so that the leading term t−1/2 σ̂t of
the concentration inequality (9) is of order σX n−1/2 t−1/2 .
Thus, to break out of the while loop in Line 19 in Figure 3,
2
we need t ∝ σX
n. In a nutshell, larger gains are thus to be
expected when data are clustered in terms of log likelihood.
To illustrate this phenomenon, we set σX = 0.1. To investigate the behavior at equilibrium, all runs were started
at the mode of a subsampled likelihood, using a proposal
covariance matrix proportional to the covariance of the target. In Figure 7(b), we show the running average number of
samples needed for 6 runs of MHS UB L HD, with n ranging
from 105 to 1015 . With increasing n, the number of samples needed progressively drops to 25% of the total n. This
is satisfying, as the number of samples required at equilibrium should be less than 50% to actually improve on usual
MH, since a careful implementation of MH in Figure 1 only
requires to evaluate one single full likelihood per iteration,
while methods based on subsampling require two.

We have presented an approximate MH algorithm to perform Bayesian inference for large datasets. This is a robust
alternative to the technique in (Korattikara et al., 2014),
and this robustness comes at an increased computational
price. We have obtained theoretical guarantees on the resulting chain, including a user-controlled error in total variation, and we have demonstrated the methodology on several applications. Experimentally, the resulting approximate chains achieve fast burn-in, requiring on average only
a fraction of the full dataset. At equilibrium, the performance of the method is strongly problem-dependent.
Loosely speaking, if the expectation w.r.t. π(θ)q(θ0 |θ) of
the variance of log p(x|θ0 )/p(x|θ) w.r.t. to the empirical
distribution of the observations is low, then one can expect
significant gains. If this expectation is high, then the algorithm is of limited interest as the Monte Carlo estimate
Λ∗t (θ, θ0 ) requires many samples t to reach a reasonable
variance. It would be desirable to use variance reduction
techniques but this is highly challenging in this context.
Finally, the algorithm and analysis provided here can be
straightforwardly extended to scenarios where π is such
that log π(θ0 )/π(θ) is intractable, as long as a concentration inequality for a Monte Carlo estimator of this log ratio
is available. For models with an intractable likelihood, it
is often possible to obtain such an estimator with low variance, so the methodology discussed here may prove useful.
ACKNOWLEDGMENTS
Rémi Bardenet acknowledges his research fellowship
through the 2020 Science programme, funded by EPSRC
grant number EP/I017909/1. Chris Holmes is supported
by an EPSRC programme grant and a Medical Research
Council Programme Leader’s award.

Towards scaling up MCMC: an adaptive subsampling approach

References
Alkhamis, T. M., Ahmed, M. A., and Tuan, V. K. Simulated
annealing for discrete optimization with estimation. European Journal of Operational Research, 116:530–544,
1999.
Andrieu, C. and Thoms, J. A tutorial on adaptive MCMC.
Statistics and Computing, 18:343–373, 2008.

Maron, O. and Moore, A. Hoeffding races: Accelerating
model selection search for classification and function approximation. In Advances in Neural Information Processing Systems. The MIT Press, 1993.
Mnih, V. Efficient stopping rules. Master’s thesis, University of Alberta, 2008.
Mnih, V., Szepesvári, Cs., and Audibert, J.-Y. Empirical
Bernstein stopping. In Proceedings of the 25th International Conference on Machine Learning (ICML), 2008.

Audibert, J.-Y., Munos, R., and Szepesvári, Cs.
Exploration-exploitation trade-off using variance estimates in multi-armed bandits. Theoretical Computer
Science, 2009.

Robert, C. P. and Casella, G. Monte Carlo Statistical Methods. Springer-Verlag, New York, 2004.

Bardenet, R. and Maillard, O.-A. Concentrations inequalities for sampling without replacement. preprint, 2013.
URL arxiv.org/abs/1309.4029.

Roberts, G. O. and Rosenthal, J. S. Optimal scaling for
various Metropolis-Hastings algorithms. Statistical Science, 16:351–367, 2001.

Bulgak, A. A. and Sanders, J. L. Integrating a modified
simulated annealing algorithm with the simulation of a
manufacturing system to optimize buffer sizes in automatic assembly systems. In Proceedings of the 20th Winter Simulation Conference, 1988.

Serfling, R. J. Probability inequalities for the sum in sampling without replacement. The Annals of Statistics, 2
(1):39–48, 1974.

Cesa-Bianchi, N. and Lugosi, G. Combinatorial bandits. In
Proceedings of the 22nd Annual Conference on Learning
Theory, 2009.
Collobert, R., Bengio, S., and Bengio, Y. A parallel mixture
of SVMs for very large scale problems. Neural Computation, 14(5):1105–1114, 2002.
Domingo, C. and Watanabe, O. MadaBoost: a modification
of AdaBoost. In Proceedings of the Thirteenth Annual
Conference on Computational Learning Theory (COLT),
2000.
Ferré, D., L., Hervé, and Ledoux, J. Regular perturbation
of V-geometrically ergodic Markov chains. Journal of
Applied Probability, 50(1):184–194, 2013.
Gelman, A., Jakulin, A, Pittau, M. G., and Su, Y-S. A
weakly informative default prior distribution for logistic
and other regression models. Annals of applied Statistics, 2008.
Haario, H., Saksman, E., and Tamminen, J. An adaptive
Metropolis algorithm. Bernoulli, 7:223–242, 2001.
Hoeffding, W. Probability inequalities for sums of bounded
random variables. Journal of the American Statistical
Association, 58:13–30, 1963.
Korattikara, A., Chen, Y., and Welling, M. Austerity in
MCMC land: Cutting the Metropolis-Hastings budget.
In Proceedings of the International Conference on Machine Learning (ICML), 2014.

Singh, S., Wick, M., and McCallum, A. Monte carlo
mcmc: Efficient inference by approximate sampling. In
Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, 2012.
van der Vaart, A. W. Asymptotic Statistics. Cambridge
University Press, 2000.
Wang, L. and Zhang, L. Stochastic optimization using simulated annealing with hypothesis test. Applied Mathematics and Computation, 174:1329–1342, 2006.

