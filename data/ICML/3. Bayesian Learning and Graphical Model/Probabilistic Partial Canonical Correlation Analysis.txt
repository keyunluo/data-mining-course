Probabilistic Partial Canonical Correlation Analysis

Yusuke Mukuta
Graduate School of Information Science and Technology, The University of Tokyo
7–3–1, Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan

MUKUTA @ MI . T. U - TOKYO . AC . JP

Tatsuya Harada
Graduate School of Information Science and Technology, The University of Tokyo
7–3–1, Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan

HARADA @ MI . T. U - TOKYO . AC . JP

Abstract
Partial canonical correlation analysis (partial
CCA) is a statistical method that estimates a
pair of linear projections onto a low dimensional
space, where the correlation between two multidimensional variables is maximized after eliminating the influence of a third variable. Partial
CCA is known to be closely related to a causality measure between two time series. However,
partial CCA requires the inverses of covariance
matrices, so the calculation is not stable. This
is particularly the case for high-dimensional data
or small sample sizes. Additionally, we cannot estimate the optimal dimension of the subspace in the model. In this paper, we have addressed these problems by proposing a probabilistic interpretation of partial CCA and deriving a Bayesian estimation method based on the
probabilistic model. Our numerical experiments
demonstrated that our methods can stably estimate the model parameters, even in high dimensions or when there are a small number of samples.

1. Introduction
Partial canonical correlation analysis (partial CCA) was
proposed by Rao (1969). It is a statistical method used to
estimate a pair of linear projections onto a low-dimensional
space, where the correlation between two multidimensional
variables is maximized after eliminating the influence of a
third variable. This is calculated using a CCA of the residuals of a linear regression of the third variable. This method
is a generalized version of the partial correlation coeffiProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

cient for multidimensional data. We define the variables
d2
d1
and {yn2 }N
{yn1 }N
n=1 ∈ R , the third variable
n=1 ∈ R
dx
N
{xn }n=1 ∈ R and the dimension of the subspace dz .
Then the partial CCA is calculated using the general eigenvalue problem
1
Σ12|x Σ−1
22|x Σ21|x u

=

ρ2 Σ11|x u1 ,

2
Σ21|x Σ−1
11|x Σ12|x u

=

ρ2 Σ22|x u2 ,

(1)

where Σm1 m2 |x = Σm1 x − Σm1 x Σ−1
xx Σxm2 , and Σab is
a sample covariance matrix. Partial CCA has various applications in areas such as social science (Kowalski et al.,
2003), and can be used as a causality measure.
Causality measures are indices that measure the influence
of one time series on another. Transfer entropy (Schreiber,
2000) is a measure based on information theory. It measures the magnitude of a change to the conditional distribution of y given x, and is calculated using
ZZZ
(l)
(k)
Tx→y =
p(yt , yt−1 , xt−1 )
(l)

log2

(k)

p(yt |yt−1 , xt−1 )
(l)
p(yt |yt−1 )

(l)

(k)

dyt dyt−1 dxt−1 , (2)

where k and l denote the embedding dimensions,
T
(k)
(l)
T
T
T
yt−1
yt−2
· · · yt−l+1
, and xt−1 =
yt−1 =

T
xTt−1 xTt−2 · · · xTt−k+1 . Shibuya et al. (2009) showed
that when we assume that the variables are normally distributed and estimate the model parameters using maximum likelihood estimation, transfer entropy is equivalent
to Granger causality (Granger, 1969). Granger causality
is based on changes to the estimation error of an autoregressive model. Shibuya et al. (2011) showed that we
can use the partial canonical correlations, ρi , calculated us(k)
ing partial CCA on yt and xt−1 and eliminate the effect
(l)
of yt−1 . Then, the transfer entropy can be calculated usPmin(d ,kd )
1
ing Tx→y = 21 i=1 y x log2 1−ρ
2 . Transfer entropy
i

Probabilistic Partial Canonical Correlation Analysis

has many applications such as brain analysis (Chávez et al.,
2003), medical science (Verdes, 2005), cognitive development modelling (Sumioka et al., 2008), and detecting motion in a movie (Yamashita et al., 2012).
However, partial CCA requires the inverses of sample covariance matrices, so the calculation is unstable when the
variables are highly correlated, the dimension of the data is
large, or there are not enough data. Yamashita et al. regularized the covariance matrix to solve this problem (2012),
but the appropriate optimization of the plural regularization
parameters has not been determined. Additionally, we cannot estimate the proper dimension of the subspace of the
model.
We have addressed these problems by proposing a probabilistic interpretation of partial CCA, and by deriving a
variational Bayesian estimation algorithm for the model
parameters based on this probabilistic interpretation. Our
experiments show that the proposed methods can more accurately estimate the subspace dimension, and can more
stably estimate the model parameters on both synthetic and
real data, even in high dimensions or when there are few
samples.

2. Canonical Correlation Analysis and its
Extension
In this section, we review canonical correlation analysis,
which is a statistical method similar to partial CCA. We
also consider it from a probabilistic perspective.
Canonical correlation analysis (CCA) was proposed by
Hotelling (1936). It is a method for finding statistical
dependencies between two data sources. Given variables
d1
d2
{yn1 }N
and {yn2 }N
n=1 ∈ R
n=1 ∈ R , and the dimension of
the subspace dz ≤ min(d1 , d2 ), the CCA can be calculated
using the general eigenvalue problem
1
Σ12 Σ−1
22 Σ21 u
−1
Σ21 Σ11 Σ12 u2

=
=

ρ2 Σ11 u1 ,
ρ2 Σ22 u2 ,

(3)

where Σm1 m2 represents a sample covariance matrix between y m1 and y m2 . The projection is a dz × di (i = 1, 2)
matrix with the d-th row eigenvector corresponding to the
d-th largest eigenvalue. Each eigenvalue equals the correlation in each dimension. Numerous studies have extended
CCA, including a nonlinear extension using kernels (Lai &
Fyfe, 2000; Melzer et al., 2001), online inferences of the
model parameters (Vı́a et al., 2007; Yger et al., 2012), and
sparse variants (Hardoon & Shawe-Taylor, 2009).
Bach and Jordan gave a probabilistic interpretation of CCA
(2005), such that the maximum likelihood estimates of the
model parameters can be derived from the CCA. Given this
probabilistic interpretation, we can extend CCA to probabilistic models. Figure 1 shows a graphical model of the

Figure 1. Graphical model for probabilistic CCA.
dz
are the latent variinterpretation, where {zn }N
n=1 ∈ R
ables. The generative model is

zn

∼

N (0, Idz ),

ynm

∼

N (W m zn + µm , Ψm ) ,

(4)

where N (µ, Σ) denotes the multivariate normal distribution with mean µ and covariance Σ, and Id denotes the d
dimensional identity matrix. W m ∈ Rdm ×dz and Ψm ∈
Rdm ×dm are the model parameters that we must estimate.
We define the Udmz matrices as having their d-th column
equal to the d-th eigenvector, and Pdz ∈ Rdm ×dz as a diagonal matrix with d-th element equal to the d-th eigenvalue
of Equation (3). Then, the maximum likelihood solution is
Wm

=

Σmm Udmz Mm ,

Ψm
µm

=
=

Σmm − W m (W m )T ,
ym ,

(5)

where Mm ∈ Rdm ×dm are arbitrary matrices such that
M1 M2T = Pdz and the spectral norms of Mm are smaller
PN
than one. y m is the sample mean N1 n=1 ynm . There are
some extensions of this probabilistic model. They include
a robust estimation method that assumes a student distribution for noise (Archambeau et al., 2006), and a nonlinear extension that uses a Gaussian process latent variable
model (Leen & Fyfe, 2006; Ek et al., 2008).
Bayesian CCA (Klami & Kaski, 2007; Wang, 2007) assumes that the model parameters are also random variables.
Wang used a Wishart prior for the precision matrices of the
noise, an ARD prior (Neal, 1995) for each column of the
projection matrices, and derived a variational Bayesian estimation algorithm for the posterior distribution of the parameters. Virtanen et al. (2011) reduced the number of
model parameters by assuming that the noise was isotropic
and by introducing non-shared latent variables. Klami et al.
(2013) derived an algorithm that simultaneously inferred
the projection matrices for the shared and non-shared variables. Damianou et al. (2012) studied a Bayesian extension
of a Gaussian process latent variable model. Fujiwara et al.
(2009) used Bayesian CCA to estimate image bases from
fMRI data.

Probabilistic Partial Canonical Correlation Analysis

We can also show that if the data space is spanned by the
samples, L is the negative definite quadratic form of Wx .
So L is maximized when Wx is such that the partial derivative is zero. Therefore,
Wxm = Σmx Σ−1
xx .

(8)

When we substitute this into Equation (6), the model is
equivalent to the probabilistic CCA model with input varim
ables y ′ n = yenm − Σmx Σ−1
en . Because the covariance
xx x
matrices of these data are
N
1 X ′ m1 ′ m2 T
y y
N n=1 n n

Figure 2. Graphical model for probabilistic partial CCA.

3. Probabilistic Interpretation of Partial CCA
In this section, we propose a generative model that estimates the maximum likelihood parameters using partial
CCA. We also derive an expectation-maximization (EM)
algorithm that estimates the model parameters and latent
variables.

∼
∼

zn
ynm

N (0, Idz ),
N (Wxm xn + Wzm zn + µm , Ψm ) .

(6)

We will show that the maximum likelihood solution
arg max log p(y|x; Wx , Wz , Ψ) can be calculated using
Wx ,Wz ,Ψ

partial CCA. To this end, we show that the proposed model
can be reduced to the generative model of probabilistic
CCA Equation (4). When we define the log likelihood L
and
  1 
 1

Wz
Ψ 0
1T
2T
C=
,
W
W
2 +
2
z
z
0 Ψ
Wz
it holds that

 1   1   1  
N
X
∂L
yn
µ
Wx
−
+
xn .
C −1
=−
µ2
yn2
Wx2
∂µ
n=1
Because C is positive definite, the likelihood is maximized when µ is such that the partial derivative equals zero.
Therefore,
(7)
µm = y m − Wxm x.
We denote each datum minus the sample mean as yen1 =
yn1 − y 1 , and substitute Equation (7). Then,


 1 

N
X
∂L
Wx1
yen
T
T
x
e
x
e
x
e
−
C −1
=
n n .
n
Wx2
yen2
∂Wx
n=1

Σm1 m2 − Σm1 x Σ−1
xx Σxm2

=

Σm1 m2 |x ,

(9)

the parameter estimation is reduced to partial CCA. To
summarize, the maximum likelihood solution of the proposed model can be written as

3.1. Generative Model
We consider a generative model that combines the regressions of variables that have effects we want to eliminate and
shared latent variables, as shown in Figure 2. The model is
defined as

=

Wxm

=

Σmx Σ−1
xx ,

Wzm
m

=

Σmm|x Udmz Mm ,

Ψ

=

Σmm|x − Wzm Wzm T ,

µm

=

y m − Wxm x,

(10)

where Udmz denotes matrices that have their d-th column
equal to the d-th eigenvector, Pd denotes the diagonal matrix with its d-th element equal to the d-th canonical correlation of Equation (1), and Mm are arbitrary matrices that
satisfy M1 M2T = Pdz and have spectral norms smaller
than one. From this point, we assume that samples have
zero mean and we do not infer a sample mean.
3.2. EM Parameter Estimation
As with CCA, we can estimate the latent variables using the
EM algorithm without integrating them out. In this case, zn
follows a normal distribution and the update rule for time t
is
(Σz )t
hZit
m
Wt+1
m
Ψt+1

=

T

(I + (Wz )t (Ψt )−1 (Wz )t )−1 ,
T

(Σz )t (Wz )t (Ψt )−1 (Y − (Wx )t X),
T
−1

T
X
XX T XhZit
m
, (11)
= Y
hZit
hZit X T hZZ T it
 




1
X
=
,
YT
Y m Y m T − Wt+1
hZit
N
mm
=

where Ψt is the matrix with Ψm
t on its diagonal, Wx ,
and Wz are the matrices that have Wxm and Wzm in their
columns, Amm is the block matrix corresponding to each
view, Ym is the matrix that has ynm in its rows, and Y =

Y1
. Additionally, X and Z are matrices with xn and
Y2
yn in their rows, and h·i are the expectations of the random
variables.

Probabilistic Partial Canonical Correlation Analysis

rithm. The full posterior p(Z, Θ|X, Y ) is approximated as


dm
2
Y
Y
q(Ψm )q(αm )
q(wjm ) , (13)
q(Z, Θ) = q(Z)
m=1

j=1

where wjm is the j-th row of W m . We apply standard cyclical updates to the separate terms
Q of q. When the factorized
distribution q has the form i q(θi ), the update rule is

q(θi ) ∝ exp hlog p(X, Y, Z, Θ)iZ,θk6=i ,

Figure 3. Graphical model for BPCCA.

q(Z)

4. Bayesian Partial CCA
To address the previously mentioned weakness of partial
CCA, we propose a hierarchical Bayesian approach to the
probabilistic partial CCA proposed in the previous section.
4.1. Model that Directly uses Probabilistic Partial CCA
In this section, we follow Wang’s approach (2007) and consider the generative model shown in Figure 3. It treats the
model parameters proposed in the previous section as random variables. We use an ARD prior (Neal, 1995) for each
column of the projection matrices, and an inverse Wishart
prior for the covariance matrices of the noise. The generative model is

∝

exp (hlog p(X, Y, Z, Θ)iΘ ) .

(14)

Because p(X) is independent of the other variables, it follows that

q(θi ) ∝ exp hlog p(Y, Z, Θ|X)iZ,θk6=i ,
∝

q(Z)

exp (hlog p(Y, Z, Θ|X)iΘ ) ,

(15)

where h·i with subscripts denote the expectation with respect to the approximate posterior distribution of the corresponding variables. The approximate posterior distribution
has the shape
q(zn )
q(Ψm )
q(wjm )

=
=
=

q(αm )

=

N (µzn , Σzn ),
IW(νm , Km ),
N (µwjm , Σwjm ),
Y
Gamma(am , bmk ).

(16)

k

Furthermore, the parameters are updated as

αkm

∼

Gamma(a0 , b0 ),

m
W:,k
Ψm

∼
∼

N (0, (αkm )−1 Idm ),
IW(ν0m , K0m ),

Σzn =

zn
ynm

∼
∼

N (0, Idz ),
N (Wxm xn + Wzm zn , Ψm ),

µzn = Σzn

I+

X

h(Wzm )T (Ψm )−1 Wzm i

m

(12)

X

!−1

,

h(Wzm )T ih(Ψm )−1 iynm

m


− h(Wzm )T (Ψm )−1 Wxm ixn ,

where the prior for the third variable p(x) does not affect
the inference when p(xn ) > 0 for each sample, because
we consider the conditional distribution given xn . Here
Gamma(a, b) is the Gamma distribution with shape parameter a and scale parameter b, and IW(ν, K) is the inverse
m
Wishart distribution. W m = Wxm Wzm . W:,k
is the
m
k-th column of W . The hyperparameters a0 , b0 , ν0m , K0m
should be small so that the priors are broad, but from the
definition of the Wishart distribution, ν0m > dm − 1. In our
experiments, we set a0 , b0 = 10−14 , ν0m = dm , K0m =
10−14 · Idm . The ARD prior drives unnecessary components to zero, so we can estimate the dimensions of the
latent variables by choosing sufficiently large dz , or by first
choosing a small dz and then gradually increasing it according to the output projection matrices. We refer to this
model as Bayesian PCCA (BPCCA).

Km = K0m + Y m (Y m )T


XX T XZ T
(W m )T i
+ hW m
ZX T ZZ T

− Y m X T hZ T i h(W m )T i


X
Y m,
− hW m i
hZi
νm = ν0m + N,
−1


T
XX T XhZi
,
Σwjm = diaghαm i+h(Ψm )−1
i
j,j
hZiX T hZZ T i

m
XT ZT
µwjm = h(Ψm )−1
j,: iY


T
X
XX T XhZi
m
−
,
ihW
i
h(Ψm )−1
l,:
j,l
hZiX T hZZ T i

Next, we propose a variational Bayesian inference algo-

(17)

l6=j

am = a0 + dm /2,
m
bmk = b0 + hkW:,k
ki/2,

Probabilistic Partial Canonical Correlation Analysis

and the shape
q(Z)

=

Y

N (µzn , Σz ),

n

q(W m )

=

Y

m , ΣW m ),
N (µWd,:

d

q(αm )

=

Y

),
Gamma(aαm , bαm
k

k

q(τ m )

where diaghαm i is the diagonal matrix with k-th element
hαkm i.

The model proposed in the previous subsection requires a
large number of calculations to infer noise precision matrices. Additionally, the prior distribution has a large influence when there are a small number of samples, because
ν0m > dm − 1. Therefore, following the approach used
by Klami et al. (2013), we propose a model that uses
isotropic noise and non-shared latent variables. The generative model is
N (0, Idz ),

znm

∼

N (0, Idzm ),

ynm

∼

N



(18)
.

When the
are integrated out, this model is equivalent to the model proposed in the previous subsection with
Ψm = B m (B m )T + (τ m )−1 Idm . So we can consider
this model as equivalent to imposing a low-rank assumption on the covariance matrices. To simultaneously esti
A(1) B (1)
0
mate A and B, we write Wz =
,
A(1)
0
B (1)

W = Wx Wz , and consider the model
αkm
m
W:,k

∼
∼

Gamma(a0 , b0 ),
N (0, (αkm )−1 Idm ),

τm
zn

∼
∼

Gamma(a0 , b0 ),
N (0, I(dz +dz1 +dz2 ) ),

ynm

∼


N Wxm xn + Wzm zn , (τ m )−1 Idm , (19)

as shown in Figure 4. This representation reduces the number of model parameters. We refer to this model as group
sparse PCCA (GSPCCA). This model also requires small
hyperparameters. We have used a0 , b0 = 10−14 in our experiments. Additionally, we choose the approximate posterior
Y
q(Z, Θ) = q(Z)
(q (τ m ) q (αm ) q (W m )) ,
(20)
m



−1
T
XX T XhZi
diaghαm i+hτ m i
,
hZiX T hZZ T i

= Y m X T hZ T i ,
!−1
X
= I+
hτ m ih(Wzm )T Wzm i
,

µW m
Σz

m

hZi = Σz

X

m

hτ i

h(Wzm )T iY m−h(Wzm )T Wxm iX

m

aαm = a0 + dm /2,
b αm
= b0 + h(W m )T W m ik,k /2,
k
aτ m = a0 + N dm /2,

!


,

1
Tr Y m (Y m )T
2


− 2Y m X T hZ T i h(W m )T i



T
XX T XhZi
. (22)
+ Tr h(W m )T W m i
hZiX T hZZ T i

bτ m = b0 +

Wxmxn+Amzn +B mznm , (τ m )−1Idm

znm

(21)

ΣW m =

4.2. Model with Isotropic Noise

∼

Gamma(aτ m , bτ m ).

The parameters are updated as

Figure 4. Graphical model for GSPCCA.

zn

=

4.3. Optimization of the Linear Transformation of the
Latent Variables
The maximum likelihood solution of probabilistic partial
CCA has the same degrees of freedom as the linear transformation of latent variables. In the Bayesian model, we
optimize this transformation in each iteration to obtain an
approximate distribution that is closer to the prior distribution. We expect that this speeds up the convergence and
that the latent variables are more independent. The function to be maximized is similar to that in (Virtanen et al.,
2011), and is defined as
Tr(R−1 hZZ T iR−T )
+(d1 +d2 −N ) log |R|
2
dz
2
X
1 X
−
dm
log(rkT h(Wzm )T Wzm irk ).(23)
2 m=1

L(R) = −

k=1

To solve this, we use the L-BFGS method (Liu & Nocedal,
1989) initialized with the identity matrix. Using the opti-

Probabilistic Partial Canonical Correlation Analysis

0.8

0.25
accuracy

W estimation error

0.3

1

0.2
0.15

CV
BIC
GSPCCA (ours)
BPCCA (ours)

0.6
0.4

0.1
0.2

0.05
0

25

50

100

200
N

400

0

800

25

50

100

200
N

400

1.2

1.2

1.1

1.1

1
0.9
0.8
0.7
N=25
N=50
N=100
N=200

0.6
0.5
0.4
6

800

output dz divided by true dz

CV
BIC
GSPCCA (ours)
BPCCA (ours)

output dz divided by true dz

0.35

0.8
0.7
N=25
N=50
N=100
N=200

0.6
0.5
0.4
12

7
8
9
10
the number of columns of Wz

(a) low-dimensional data

(a) High-dimensional data

1
0.9

14
16
18
20
the number of columns of Wz

(b) high-dimensional data

0.35
1

Figure 6. Comparison of the estimates of dz . The left panel shows
the performance on low-dimensional data. The right panel corresponds to high-dimensional data.

0.6

CV
BIC
GSPCCA (ours)
BPCCA (ours)

0.4

2.5

0.1
0.2

0.05
0

25

50

100

200
N

400

800

0

25

50

100

200
N

400

800

(b) Low-dimensional data

1.5

x−>y
y−>x

1
0.5
0

10

2.5

2

25 50 100 200 400
N

causality measure

0.15

causality measure

CV
BIC
GSPCCA (ours)
BPCCA (ours)

0.2

2
1.5

x−>y
y−>x

1
0.5
0

25

50 100 200 400
N

(a) λ = 0

−1

hZi
ΣZ

←
←

R hZi,
R−1 ΣZ R−T ,

µWzm

←

µWzm R,

ΣWzm

←

RT ΣWzm R.

6
4
x−>y
y−>x

2

25 50 100 200 400
N

(b) λ = 0.01
x−>y
y−>x

2
1
0

8

25 50 100 200 400
N

10
causality measure

mal R, the approximate distributions are transformed into

3
causality measure

Figure 5. Comparison of the Wx estimation error and the model
accuracy. The left panel shows the relative estimation error of
Wx . The right panel shows the accuracy of dz .

causality measure

0.8

0.25
accuracy

W estimation error

0.3

8
6
4
x−>y
y−>x

2

25 50 100 200 400
N

(c) λ = 0.1

(24)

5. Experiments
We have applied our methods to synthetic and real data, to
verify that they can be used with a small number of samples
or high-dimensional data. We compared the stability of the
model selection and the causality measures.
5.1. Model Selection
We first investigated the estimates of Wx and dz using
synthetic data. We did not consider Wz because the
maximum likelihood solution of Wz is not unique. We
compared our methods (BPCCA, GSPCCA) with the
model selection techniques using the Bayesian information
criterion (BIC) and five-fold cross validation (CV). In our
methods, we considered that a component k of the solution
was active when hαkm i < 50, and let dz be an estimate of
the number of k for that are active for each view. We set
d1 = 5, d2 = 4, dx = 3, and dz = 2 for low-dimensional
data, and d1 = 50, d2 = 50, dx = 5, and dz = 5 for

Figure 7. Comparison of the stability of a causality measure.
(a) and the left panels of (b) and (c) show the performance of
GSPCCA (ours). The right panels of (b) and (c) show the performance of PCCA. The blue line shows the estimated causality
measures for the true direction. The green line shows the estimates for the reverse direction.

high-dimensional data. In each setting, we generated N =
25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000
samples from a generative model. Each column of the
projection matrix was sampled from a Normal distribution
with zero mean and unit variance, and the noise covariance
P⌊ d2z ⌋
matrices were Idz + i=1
ui uTi , for ui ∼ N (0, Idz ).
Wz had five columns for low-dimensional data, and 10
columns for high-dimensional data. We conducted 50
experiments for each parameter. For the Bayesian methods, we determined that the method had converged if the
relative change in the variational lower bound was below
10−4 . As it converges to a local maxima, we initialized the
model by randomly sampling the latent variables from the
prior, and ran the algorithm 10 times choosing the solution
with the best variational lower bound. For each method,
we calculated the mean of the relative error of Wx using

Probabilistic Partial Canonical Correlation Analysis
Tr((Wx −Ŵx )T (Wx −Ŵx ))
,
Tr(WxT Wx )

where Ŵx is an estimate of Wx .
We also recorded the accuracy rate of the system, dz .
The results are presented in Figure 5. In the right subfigure
of Figure 5(a), the CV result is hidden because it has been
overwritten by the BIC result. Because we cannot stably
calculate the BIC and CV of non-Bayesian methods when
D = 50 and N = 25, 50, and the BIC and CV of BPCCA
cannot be calculated when D = 50 and N = 25, we have
not included these results. These plots show that existing
model selection methods perform poorly and that the accuracy decreases to zero in high dimensions. Conversely,
the two Bayesian methods are very accurate, even for highdimensional data. BPCCA’s performance degrades when
D = 50 and N = 50, but GSPCCA’s performance degrades more gradually. The estimate of Wx follows a similar trend. These results demonstrate that our methods calculate the model selection and parameter estimation more
accurately than non-Bayesian methods, and that GSPCCA
is the best method.
Next, we compared the model-selection performance of
GSPCCA by varying the number of columns of Wz to 6, 7,
8, 9, and 10 for low-dimensional data, and 12, 14, 16, 18,
20 for high-dimensional data, with N = 25, 50, 200, 800.
The performance was measured using the mean of the number of active components divided by the true dz . The results
are shown in Figure 6. In high dimensions, the performance
is almost one for all the parameters. In low dimensions, if
N = 25 the performance decreases gradually. However,
this effect can be ignored because the true dz is two. These
results indicate that the number of columns in Wz has little
effect on the performance, if it is sufficiently large.
5.2. Causality Measure with Synthetic Data
To evaluate the stability of the causality calculations for
a small sample of high-dimensional data, we generated a
time series using the following linear model.
xt

=

0.5xt−1 + ǫt,x ,

y 2t

=

yt

=

0.5y2t−1 + W xt−1 + ǫt,y2 ,
T
y2Tt y2Tt
+ ǫt,y ,

(25)

where the first two columns of W are sampled from
N (0, 0.5 · I20 ) and the other columns are zero. ǫt,x , ǫt,y2
denotes Gaussian noise with zero mean and unit variance. ǫt,y is 0 when r = 0, and is Gaussian noise
with zero mean and variance r · I40 otherwise. The
true causality direction is x → y. The first and second halves of yt are strongly correlated. This correlation is strong when r is small. The optimal dimension
of the latent variables is two. Using this model, we set
the embedding dimension to 1, r to 0, 0.01, 0.1, and the
sample size to N = 25, 50, 100, 200, 400, for each pa-

rameter. We expect that the causality measures derived
from PCCA and probabilistic PCCA are equivalent, so
we compared PCCA
GSPCCA (the best performing
Pwith
20
1
method). We used d=1 21 log2 1−ρ
2 as a causality mead
sure for PCCA. For GSPCCA, we let ρk be the correlation between hYt−1(k,:) |Yt i and hYt−1(k,:) |Xt−1 i, and used
P 1
1
k 2 log2 1−ρ2k as a causality measure, where the summation is over the active components. Figure 7 shows the results. We have not included results if the solution could not
be stably evaluated. The causality measure using PCCA
diverged when N was below 200, irrespective of λ. This
measure also increased in the direction of y → x, so this
measure is unreliable when N is small. However, the measure using GSPCCA was zero in the y → x direction when
N was larger than 100, because the Bayesian model makes
directions that have a negligible influence converge to zero.
This behavior helps eliminate false causality relations, but
this model may overlook true causality relations when the
influence is small. In such cases, we could detect small
influences by modifying the hyperparameters of the ARD
prior. This measure tended to diverge when λ was less than
0.01 and N = 50, or λ = 0.1 and N = 25. However, the
measure in the x → y direction was larger than that in the
y → x direction. The Bayes model also becomes unstable
when there are an insufficient number of samples.
5.3. Causality Measure with Real Data
Next, we applied GSPCCA and PCCA to meteorological
data, using the Global Summary of the Day (GSOD) provided by the National Climatic Data Center (NCDC) on its
website. For this experiment, we used data from the USA
between December 24, 2008 and February 28, 2009. Figure 8 shows the observed jet stream during that same period. We selected seven types of variables that did not have
a substantial amount of missing data: mean temperature,
mean dew point, mean visibility, mean wind speed, maximum sustained wind speed, maximum temperature, and
minimum temperature. Therefore, the time series has seven
dimensions. The length was 66. We randomly chose 224
targets based on distance, after excluding targets with many
missing values. We conducted a zero-order hold for missing values. We set the embedding dimensions to 2, 3, and
4 and used the same causality measure as in the synthetic
data experiments. Figure 9 shows our results. Figure 9
shows the largest 50 index values. Because the causality
measure that used PCCA with the embedding dimension
of four diverged in some pairs, we have included all the
index values that diverged. When the embedding dimension was two, GSPCCA and PCCA had a similar tendency
to show a strong information flow from west to east over
the eastern region, and from north to south in the central
region. This is consistent with Figure 8. When the embedding dimensions were four, the arrows drawn using PCCA

Probabilistic Partial Canonical Correlation Analysis

GSPCCA (ours)
PCCA
Average arrow length:
Average arrow length:
1.0 × 103 km/day
9.8 × 102 km/day
(a) Embedding dimension = 2

(a) 19/1/2009

GSPCCA (ours)
PCCA
Average arrow length:
Average arrow length:
1.1 × 103 km/day
1.2 × 103 km/day
(b) embedding dimension = 3

GSPCCA (ours)
PCCA
Average arrow length:
Average arrow length:
1.1 × 103 km/day
1.7 × 103 km/day
(c) Embedding dimension = 4

(b) 19/2/2009

Figure 8. Weather information flow map for the USA (source:
The California Regional Weather Server, San Francisco University).

were scattered over the mainland, although the index values using GSPCCA had a similar tendency to those with
the embedding dimension of two. This result implies that
PCCA overfits the data when the embedding dimension is
high.
Next, we calculated the average arrow length using the
Hubeny formula1 . It was 1.0 × 103 , 1.1 × 103 , 1.1 × 103
km/day for GSPCCA and 9.8 × 102 , 1.2 × 103 , 1.7 × 103
km/day for PCCA. This shows that the causality measure
using GSPCCA was more stable and similar to the actual
air current, which was approximately 8.6 × 102 km/day
(Shibuya et al., 2011), even when the embedding dimension was high. Because the true embedding dimension is
unknown, GSPCCA is a more reliable method.
1
http://www.kashmir3d.com/kash/manual-e/
std_siki.htm

Figure 9. Weather information flow map of USA (2008/12/24 –
2009/02/28). Maps on the left were calculated using GSPCCA,
and maps on the right were calculated using PCCA.

6. Conclusion
We proposed a probabilistic interpretation of partial CCA.
We also presented a Bayesian extension and an inference
algorithm based on the probabilistic interpretation. Our experiments have demonstrated that the proposed methods
are more appropriate for model selection and estimating
causal relations from time series than existing methods,
when there are a small number of samples or in high dimensions. We expect that PCCA and causality measures
will be extensively applied to many areas using our methods.
Our Bayesian partial CCA method can be extended to a
robust estimation method using a Student distribution for
the noise (Archambeau et al., 2006), or to an inference
method using the online variational Bayes technique (Hoffman et al., 2013). Additionally, by considering the projection matrices as random variables, we can construct a more
complex model that allows the causal relation to change
over time.

Probabilistic Partial Canonical Correlation Analysis

References
Archambeau, Cédric, Delannay, Nicolas, and Verleysen,
Michel. Robust probabilistic projections. In ICML, pp.
33–40, 2006.
Bach, Francis R and Jordan, Michael I. A probabilistic interpretation of canonical correlation analysis. Technical
Report 688, Department of Statistics, University of California, Berkeley, 2005.
Chávez, Mario, Martinerie, Jacques, and Le Van Quyen,
Michel. Statistical assessment of nonlinear causality:
application to epileptic eeg signals. Journal of Neuroscience Methods, 124(2):113–128, 2003.
Damianou, Andreas, Ek, Carl, Titsias, Michalis K, and
Lawrence, Neil D. Manifold relevance determination.
In ICML, pp. 145–152, 2012.
Ek, Carl Henrik, Rihan, Jon, Torr, Philip HS, Rogez,
Grégory, and Lawrence, Neil D. Ambiguity modeling
in latent spaces. In MLMI, pp. 62–73, 2008.
Fujiwara, Yusuke, Miyawaki, Yoichi, and Kamitani,
Yukiyasu. Estimating image bases for visual image reconstruction from human brain activity. In NIPS, pp.
576–584, 2009.
Granger, Clive WJ. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: Journal of the Econometric Society, 37(3):424–
438, 1969.
Hardoon, David R and Shawe-Taylor, John. Sparse canonical correlation analysis. stat, 1050:19, 2009.
Hoffman, M, Blei, D, Wang, Chong, and Paisley, John.
Stochastic variational inference. JMLR, 14:1303–1347,
2013.
Hotelling, Harold. Relations between two sets of variates.
Biometrika, 28(3/4):321–377, 1936.

Leen, Gayle and Fyfe, Colin. A gaussian process latent variable model formulation of canonical correlation
analysis. In ESANN, pp. 413–418, 2006.
Liu, Dong C and Nocedal, Jorge. On the limited memory
bfgs method for large scale optimization. Mathematical
programming, 45(1-3):503–528, 1989.
Melzer, Thomas, Reiter, Michael, and Bischof, Horst. Kernel canonical correlation analysis. In ICANN, pp. 353–
360, 2001.
Neal, Radford M. Bayesian learning for neural networks.
PhD thesis, University of Toronto, 1995.
Rao, B Raja. Partial canonical correlations. Trabajos de
estadistica y de investigación operativa, 20(2):211–219,
1969.
Schreiber, Thomas. Measuring information transfer. Physical Review Letters, 85(2):461, 2000.
Shibuya, Takashi, Harada, Tatsuya, and Kuniyoshi, Yasuo.
Causality quantification and its applications: structuring
and modeling of multivariate time series. In KDD, pp.
787–796, 2009.
Shibuya, Takashi, Harada, Tatsuya, and Kuniyoshi, Yasuo.
Reliable index for measuring information flow. Physical
Review E, 84(6):061109, 2011.
Sumioka, Hidenobu, Yoshikawa, Yuichiro, and Asada, Minoru. Development of joint attention related actions
based on reproducing interaction contingency. In ICDL,
pp. 256–261, 2008.
Verdes, PF. Assessing causality from multivariate time
series. Physical Review E, 72(2):026222.1–026222.9,
2005.
Vı́a, Javier, Santamarı́a, Ignacio, and Pérez, Jesús. A learning algorithm for adaptive canonical correlation analysis
of several data sets. Neural Networks, 20(1):139–152,
2007.

Klami, Arto and Kaski, Samuel. Local dependent components. In ICML, pp. 425–432, 2007.

Virtanen, Seppo, Klami, Arto, and Kaski, Samuel.
Bayesian cca via group sparsity. In ICML, pp. 457–464,
2011.

Klami, Arto, Virtanen, Seppo, and Kaski, Samuel.
Bayesian canonical correlation analysis. JMLR, 14:965–
1003, 2013.

Wang, Chong. Variational bayesian approach to canonical
correlation analysis. IEEE Transactions on Neural Networks, 18(3):905–910, 2007.

Kowalski, J, Tu, XM, Jia, G, Perlis, M, Frank, E, CritsChristoph, P, and Kupfer, DJ. Generalized covarianceadjusted canonical correlation analysis with application
to psychiatry. Statistics in medicine, 22(4):595–610,
2003.

Yamashita, Yuya, Harada, Tatsuya, and Kuniyoshi, Yasuo.
Causal flow. IEEE Transactions on Multimedia, 3(3):
619–629, 2012.

Lai, Pei Ling and Fyfe, Colin. Kernel and nonlinear canonical correlation analysis. IJNS, 10(05):365–377, 2000.

Yger, Florian, Berar, Maxime, Gasso, Gilles, and Rakotomamonjy, Alain. Adaptive canonical correlation analysis based on matrix manifolds. In ICML, pp. 1071–
1078, 2012.

