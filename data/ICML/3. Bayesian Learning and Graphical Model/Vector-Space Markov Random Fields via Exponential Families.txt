Vector-Space Markov Random Fields via Exponential Families

Wesley Tansey
Department of Computer Science, The University of Texas, Austin, TX 78712, USA

TANSEY @ CS . UTEXAS . EDU

Oscar Hernan Madrid Padilla
OSCAR . MADRID @ UTEXAS . EDU
Department of Statistics and Data Sciences, The University of Texas, Austin, TX 78712, USA
Arun Sai Suggala
Pradeep Ravikumar
Department of Computer Science, The University of Texas, Austin, TX 78712, USA

Abstract
We present Vector-Space Markov Random Fields
(VS-MRFs), a novel class of undirected graphical models where each variable can belong to an
arbitrary vector space. VS-MRFs generalize a recent line of work on scalar-valued, uni-parameter
exponential family and mixed graphical models,
thereby greatly broadening the class of exponential families available (e.g., allowing multinomial
and Dirichlet distributions). Specifically, VSMRFs are the joint graphical model distributions
where the node-conditional distributions belong
to generic exponential families with general vector space domains. We also present a sparsistent M -estimator for learning our class of MRFs
that recovers the correct set of edges with high
probability. We validate our approach via a set
of synthetic data experiments as well as a realworld case study of over four million foods from
the popular diet tracking app MyFitnessPal. Our
results demonstrate that our algorithm performs
well empirically and that VS-MRFs are capable
of capturing and highlighting interesting structure in complex, real-world data. All code for our
algorithm is open source and publicly available.

1. Introduction
Undirected graphical models, also known as Markov Random Fields (MRFs), are a popular class of models for probability distributions over random vectors. Popular parametric instances include Gaussian MRFs, Ising, and Potts
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

ARUNSAI @ CS . UTEXAS . EDU
PRADEEPR @ CS . UTEXAS . EDU

models, but these are all suited to specific data-types: Ising
models for binary data, Gaussian MRFs for thin-tailed continuous data, and so on. Conversely, when there is prior
knowledge of the graph structure but limited information
otherwise, nonparametric approaches are available (Sudderth et al., 2010). A recent line of work has considered
the challenge of specifying classes of MRFs targeted to
the data-types in the given application, when the structure is unknown. For the specific case of homogeneous
data, where each variable in the random vector has the
same data-type, (Yang et al., 2012) proposed a general subclass of homogeneous MRFs. In their construction, they
imposed the restriction that each variable conditioned on
other variables belong to a shared exponential family distribution, and then performed a Hammersley-Clifford-like
analysis to derive the corresponding joint graphical model
distribution, consistent with these node-conditional distributions. As they showed, even classical instances belong to
this sub-class of MRFs; for instance, with Gaussian MRFs
and Ising models, the node-conditional distributions follow univariate Gaussian and Bernoulli distributions respectively.
Yang et al. (2014) then proposed a class of mixed MRFs that
extended this construction to allow for random vectors with
variables belonging to different data types, and allowing
each node-conditional distribution to be drawn from a different univariate, uni-parameter exponential family member (such as a Gaussian with known variance or a Bernoulli
distribution). This flexibility in allowing for different univariate exponential family distributions yielded a class of
mixed MRFs over heterogeneous random vectors that were
capable of modeling a much wider class of distributions
than was previously feasible, opening up an entirely new
suite of possible applications.
To summarize, the state of the art can specify MRFs over
heterogeneous data-typed random vectors, under the re-

Vector-Space MRFs via Exponential Families

striction that each variable conditioned on others belong
to a uni-parameter, univariate exponential family distribution. But in many applications, such a restriction would
be too onerous. For instance, a discrete random variable
is best modeled by a categorical distribution, but this is a
multi-parameter exponential family distribution, and does
not satisfy the required restriction above. Other multiparameter exponential family distributions popular in machine learning include gamma distributions with unknown
shape parameter and Gaussian distributions with unknown
variance. Another restriction above is that the variables be
scalar-valued; but in many applications the random variables could belong to more general vector spaces, for example a Dirichlet distribution.
As modern data modeling requirements evolve, extending
MRFs beyond such restrictive paradigms is becoming increasingly important. In this paper, we thus extend the
above line of work in (Yang et al., 2012; 2014). As opposed
to other approaches which merely cluster scalar variables
(Vats & Moura, 2012), we allow node-conditional distributions to belong to a generic exponential family with a
general vector space domain. We then perform a subtler
Hammersley-Clifford-like analysis to derive a novel class
of vector-space MRFs (VS-MRFs) as joint distributions
consistent with these node-conditional distributions. This
class of VS-MRFs provides support for the many modelling requirements outlined above, and could thus greatly
expand the potential applicability of MRFs to new scientific analyses.
We also introduce an M -estimator for learning this class
of VS-MRFs based on the sparse group lasso, and show
that it is sparsistent, and that it succeeds in recovering the
underlying edges of the graphical model. To solve the M estimation problem, we also provide a scalable optimization algorithm based on Alternating Direction Method of
Multipliers (ADMM) (Boyd et al., 2011). We validate our
approach empirically via synthetic experiments measuring
performance across a variety of scenarios. We also demonstrate the usefulness of VS-MRFs by modeling a real-world
dataset of over four million foods from the MyFitnessPal
food database.
The remainder of this paper is organized as follows. Section 2 provides background on mixed MRFs in the uniparameter, univariate case. Section 3 details our generalization of the mixed MRF derivations to the vector-space
case. Section 4 introduces our M -estimator and derives its
sparsistency statistical guarantees. Section 5 contains our
synthetic experiments and the MyFitnessPal case study. Finally, Section 6 presents concluding remarks and potential
future work.

2. Background: Scalar Mixed Graphical
Models
Let X = (X1 , X2 , · · · Xp ) be a p-dimensional random vector, where each variable Xr has domain Xr . An undirected
graphical model or a Markov Random Field (MRF) is a
family of joint distributions over the random vector X that
is specified by a graph G = (V, E), with nodes corresponding to each of the p random variables {Xr }pr=1 , and edges
that specify the factorization of the joint as:

Y

P(X) ∝

ψC (XC ),

C∈C(G)

where C(G) is the set of fully connected subgraphs
(or cliques) of the graph G, XC = {Xs }s∈C denotes the subset of variables in the subset C ⊆ V ,
and {ψC (XC )}C∈C(G) are clique-wise functions, each of
which is a “local function” in that it only depend on the
variables in the corresponding clique, so that ψC (XC ) only
depends on the variable subset XC .
Gaussian MRFs, Ising MRFs, etc. make particular parametric assumptions on these clique-wise functions, but a
key question is whether there exists a more flexible specification of the form of these clique-wise functions that is
targeted to the data-type and other characteristics of the
random vector X.
For the specific case where the variables are scalars, so
that the domains Xr ⊆ R, in a line of work, (Yang
et al., 2012; 2014) used the following construction to derive a subclass of MRFs targeted to the random vector
X. Suppose that for variables Xr ∈ Xr , the following (single-parameter) univariate exponential family distribution P (Xr ) = exp{θr Br (Xr ) + Cr (Xr ) − Ar (θr )},
with natural parameter scalar θ, sufficient statistic scalar
Br (Xr ), base measure Cr (Xr ) and log normalization constant Ar (θ), serves as a suitable statistical model. Suppose
that we use these univariate distributions to specify conditional distributions:

P (Xr |X−r ) = exp{

Er (X−r )Br (Xr )+
,
Cr (Xr ) − Ar (X−r )}

(1)

where Er (·) is an arbitrary function of the rest of the variables X−r that serves as the natural parameter. Would these
node-conditional distributions for each node r ∈ V be consistent with some joint distribution for some specification
of these functions {Er (·)}r∈V ? Theorem 1 from Yang
et al. (2014) shows that there does exist a unique joint MRF

Vector-Space MRFs via Exponential Families

tion constant Ar (θ). We assume the sufficient statistics
Brj : Xr 7→ R lie in some Hilbert space Hs , and moreover specify a minimal exponential family so that:

distribution with the form:
(
X
P (X; θ) = exp
θr Br (Xr )
r∈V
X X
+
θrt Bt (Xt )Br (Xr ) + . . .

mr
X

r∈V t∈N (r)

+

X

θt1 ...tk (X)

k
Y

,

(2)

Btj (Xtj )

j=1

(t1 ,...,tk )∈C

)
+

P

r∈V

Cr (Xr ) − A(θ)

where A(θ) is the log-normalization constant. Their proof
followed an analysis similar to the Hammersley-Clifford
Theorem (Lauritzen, 1996), and entailed showing that for
a consistent joint, the only feasible conditional parameter
functions Er (·) had the following form:
X
Er (X−r ) = θr +
θrt Bt (Xt ) + . . .
+

t2 ,...,tk ∈N (r)

θrt2 ...tk (X)

k
Y

,

Suppose we use these general exponential family distributions to specify node-conditional distributions of variables
Xr conditioned on the rest of the random variables:
P (Xr |X−r )

Pmr
exp{ j=1
Erj (X−r )Brj (Xr )

=

Btj (Xtj )

(6)
+Cr (Xr ) − Ar (X−r )} ,

j=2

(3)
where θr· := {θr , θrt , . . . , θrt2 ...tk } is a set of parameters,
and N (r) is the set of neighbors of node r.
While their construction allows the specification of targeted
classes of graphical models for heterogeneous random vectors, the conditional distribution of each variable conditioned on the rest of the variables is assumed to be a singleparameter exponential family distribution with a scalar sufficient statistic and natural parameter. Furthermore, their
Hammersley-Clifford type analysis and sparsistency proofs
relied crucially on that assumption. However in the case
of multi-parameter and multivariate distributions, the sufficient statistics are a vector; indeed the random variables
need not be scalars at all but could belong to a more general
vector space. Could one construct classes of MRFs for this
more general, but prevalent, setting? In the next section,
we answer in the affirmative, and present a generalization
of mixed MRFs to the vector-space case, with support for
more general exponential families.

r
where {Erj (X−r )}m
j=1 are arbitrary functions of the rest of
the variables that serve as natural parameters for the conditional distribution of Xr . As before, we ask the question
whether these node-conditional distributions can be consistent with some joint distribution for some specification
r
of the parameter functions {Erj (X−r )}m
j=1 ; the following
theorem addresses this very question.

Theorem 1. Let X = (X1 , X2 , . . . , Xp ) be a pdimensional random vector with node-conditional distribution of each random vector Xr conditioned on the
rest of random variables as defined in (6). These nodeconditionals are consistent with a joint MRF distribution
over the random vector X, that is, Markov with respect to
a graph G = (V, E) with clique-set C, and with factors of
size at most k, if and only if the functions {Er ()}r∈V specifying the node-conditional distributions have the form:
Eri (X−r ) =θri +

j=1

r
with natural parameters {θrj }m
j=1 , and sufficient statismr
tics {Brj }j=1 , base measure Cr (Xr ) and log normaliza-

mt
X X

θri;tj Btj (Xt ) + . . .

t∈N (r) j=1

3. Generalization to the Vector-space Case
Let X = (X1 , X2 , · · · Xp ) be a p-dimensional random vector, where each variable Xr belongs to a vector space Xr .
As in the scalar case, we will assume that a suitable statistical model for variables Xr ∈ Xr is an exponential family
distribution
mr
X
P (Xr ) = exp{
θrj Brj (Xr ) + Cr (Xr ) − Ar (θ)}, (4)

(5)

for any constant c and any vector α 6= 0. We note that
even though the variables {Xr } could lie in general vector spaces, the exponential family distribution above is
finite-dimensional. However, it has multiple parameters,
which is the other facet that distinguishes it from the singleparameter univariate setting of (Yang et al., 2012; 2014).
We defer a generalization of our framework to infinitedimensional exponential families to future work.

t∈N (r)

X

αj Brj (Xr ) 6= c ,

j=1

+

X

X

t2 ,...,tk i2 =1...mt2
∈N (r) ...
ik =1...mtk

θri;...;tk ik

k
Y

, (7)
Btj ij (Xtj )

j=2

where θr· = {θri , θri;tj , θri;...;tk ik } is a set of parameters, mt is the dimension of the sufficient statistic vector
for the tth node-conditional distribution, and N (r) is the
set of neighbors of node r in graph G. The corresponding
consistent joint MRF distribution has the following form:

Vector-Space MRFs via Exponential Families

(
P (X|θ) = exp

mr
XX

and h., .i represents dot product between two vectors. Thus,
the joint distribution has the form

θri Bri (Xr ) + . . .

r∈V i=1

X

+

X

θt1 i1 ;...;tk ik

t1 ,...,tk ∈C i1 =1...mt1
...
ik =1...mtk

k
Y

Btj ij (Xtj )

j=1

. (8)

)
X

+

Cr (Xr ) − A(θ)

r∈V

P (X|θ)
 =*
+
X
X
Br (Xr ), θr +
θrt Bt (Xt )
exp

, (10)
r∈V
t∈N
(r)
)
X
+
Cr (Xr ) − A(θ)
r∈V

We provide a Hammersley-Clifford type analysis as proof
of this theorem in the supplementary material, which however has subtleties not present in (Yang et al., 2012; 2014),
due to the arbitrary vector space domain of Xr , and the
multiple parameters in the exponential families, which consequently entailed leveraging the geometry of the corresponding Hilbert spaces {Hs {s ∈ V } underlying the sufficient statistics {Bsj }.
The above Theorem 1 provides us with a general class
of vector-space MRFs (VS-MRFs), where each variable
could belong to more general vector space domains, and
whose conditional distributions are specified by more general finite-dimensional exponential families. Consequently,
many common distributions can be incorporated into VSMRFs that were previously unsupported or lacking in
(Yang et al., 2012; 2014). For instance, gamma and Gaussian nodes, though univariate, require vector-space parameters in order to be fully modeled. Additionally, multivariate distributions that were impossible to use with previous
methods, such as the multinomial and Dirichlet distributions are now also available.
3.1. Pairwise conditional and joint distributions
Given the form of natural parameters in (7), the conditional
distribution of a node Xr given all other nodes X−r for the
special case of pairwise MRFs (i.e. k = 2) has the form

mr
X
θri Bri (Xr )
P (Xr |X−r , θr , θrt ) = exp

i=1

+

mr X
mt
X X

θri;tj Btj (Xt )Bri (Xr )

t∈N (r) i=1 j=1



+Cr (Xr ) − Ar (X−r , θr· )
,

*
+

X
= exp
Br (Xr ), θr +
θrt Bt (Xt )

t∈N (r)



X
+Cr (Xr ) − Ar θr +
θrt Bt (Xt )


(9)

t∈N (r)

r
where θr is a vector formed from scalars {θri }m
i=1 , θrt is a
matrix of dimension mr × mt obtained from scalars θri;tj

withR the Plog-normalization P
constant A(θ)
=
log X exp{ r∈V hBr (Xr ), θr + t∈N (r) θrt Bt (Xt )i +
P
r∈V Cr (Xr )}. Since A(θ) is generally intractable to
calculate, we next present an efficient approach to learning
the structure of VS-MRFs.

4. Learning VS-MRFs
To avoid calculation of the log-normalization constant, we
approximate the joint distribution in (10) with the independent product of node conditionals, also known as the
pseudo-likelihood,
Y
P (X|θ) ≈
P (Xr |X−r , θr , θrt ) .
(11)
r

Let θr· = {θr , θ\r } be the set of parameters related to
the node-conditional distribution of node r, where θ\r =
{θrt }t∈V \r . Since Ar () is convex for all exponential families (Wainwright & Jordan, 2008), this gives us a loss function that is convex in θr· :
*
+
n
X
X
(i)
(i)
1
 Br (Xr ), θr +
θrt Bt (Xt )
`(θr· ; D) = − n
i

t∈V \r


− Ar θr +

.


X

(i)
θrt Bt (Xt )

t∈V \r

(12)
We then seek to find a sparse solution in terms of both edges
and individual parameters by employing the sparse group
lasso regularization penalty (Friedman et al., 2010; Simon
et al., 2013):
X √
 
R(θr· ) = λ1
νrt ||θrt ||2 + λ2 θ\r 1 ,
(13)
t∈V \r

where νrt = mr × mt is the number of parameters in the
pseudo-edge from node r to node t (i.e., the edge (r, t) in
the rth node-conditional). This yields a collection of independent convex optimization problems, one for each nodeconditional.
minimize `(θr· ; D) + R(θr· )
θr·

(14)

Vector-Space MRFs via Exponential Families

We next present an approach to solving this problem based
on Alternating Direction Method of Multipliers (ADMM)
(Boyd et al., 2011).

Updating uk+1 .
(19).

4.1. Optimization Procedure
We first introduce a slack variable z into (14) to adhere to
the canonical form of ADMM. For notational simplicity,
we omit the data parameter D from the loss function and
the subscripts in θr· and Ar since it is clear we are dealing
with the optimization of a single node-conditional.
minimize
θ

`(θ) + R(z)
,

(15)

subject to θ = z
where length(θ) = τ . The augmented Lagrangian is
2

Lα (θ, z, ρ) = `(θ) + R(z) + ρT (θ − z) + (α/2) ||θ − z||2 .
(16)
Defining the residual of the slack r = θ − z, we instead
use the scaled form with u = (1/α)ρ. ADMM proceeds in
an alternating fashion, performing the following updates at
each iteration:

2 

θk+1 = argmin `(θ) + (α/2) θ − z k + uk 2
(17)
θ



2
z k+1 = argmin R(z) + (α/2) θk+1 − z + uk 2
(18)
z

uk+1 = uk + θk+1 − z k+1

where S(x, λ) is the soft-thresholding operator on x with
cutoff at λ.

(19)

Updating θk+1 . The j th subgradient of θ is gj (θ) =
−B j + ∇j A(θ) + α(θj + zjk − ukj ). Note that the logpartition function, A(η), over the natural parameters, η =
Bθ, is available in closed form for most commonly-used
exponential families. Thus, ∇2 A(θ) is a weighted sum
of rank-one matrices. In cases where the number of samples is much less than the total number of parameters (i.e.
n << τ ), we can efficiently calculate an exact Newton
update in O(τ ) by leveraging the matrix inversion lemma
(Boyd & Vandenberghe, 2009). Otherwise, we use a diagonal approximation of the Hessian and perform a quasiNewton update.

Per ADMM, closed-form is given in

We iterate each of the above update steps in turn until convergence, then AND pseudo-edges when stitching the graph
back together.
4.2. Domain constraints
Many exponential family distributions require parameters
with bounded domain. These bounds correspond to affine
constraints on subsets of θ in the ADMM algorithm.1
Often these constraints are simple implicit restrictions to
R+ or R− . In these cases the log-normalization function
A(η) serves as a built-in log-barrier function. For instance, a normal distribution with unknown mean µ and
unknown variance σ 2 has natural parameters η1 = σµ2
and η2 = − 2σ1 2 , implying η2 < 0. However, since
η2

A(η) = − 4η12 − 21 ln(−2η2 ), this constraint will be effectively enforced so long as we are given a feasible starting
point for η. Such a feasible point can always be discovered
using a standard phase I method (Boyd & Vandenberghe,
2009). In the case of equality requirements, such as categorical and multinomial distributions, we can directly incorporate the constraints into the ADMM algorithm and
solve an equality-constrained Newton’s method when updating θ.
4.3. Sparsistency
We next provide the mathematical conditions that ensure
with high probability our learning procedure recovers the
true graph structure underlying the joint distribution. Our
results rely on similar sufficient conditions to those imposed in papers analyzing the Lasso (Wainwright, 2009)
and the l1 /l2 penalty in (Jalali et al., 2011). Before stating the assumptions, we introduce the notation used in the
proof.
4.3.1. N OTATION

k+1

Updating z
. We can reformulate (18) as the proximal
operator (Parikh & Boyd, 2013) of R(z):


2
proxR/α (y) = argmin R(z) + (α/2) ||z − y||2 , (20)
z

k+1

where y = θ
+ uk . From Friedman et al. (2010), it is
straightforward to show that the update has a closed-form
solution for each j th block of edge parameters,

√
ν
λ
S(α(yj ), λ2 )
||S(α(y
),
λ
)||
−
j
2
j
1
2
+
zjk+1 =
,
√
α ||S(α(yj ), λ2 )||2 + νj λ1 (1 − α)
(21)

∗
Let N (r) = {t : θrt
6= 0} be the true neighbourhood
of node r and let dr be the degree of r, i.e, dr = |N (r)|.
∗
And Sr be the index set of parameters {θrj;tk
: t ∈ N (r)}
c
∗
and similarly Sr be the index of parameters {θrj;tk
: t ∈
/
N (r)}. From now on we will overload the notation and
(ex)
simply use S and S c instead of Sr and Src . Let Sr
=
∗
∗
{θrj;tk
: θrj;tk
6= 0 ∧ t ∈ N (r)}.
1
Note that these subsets are different than the edge-wise
groups that are L2 -penalized. Rather, these constraints apply to
the sum of the ith value of each edge parameter and the ith bias
weight.

Vector-Space MRFs via Exponential Families
∗
Let Qnr = ∇2 `(θr·
; D) be the sample Fischer Information
matrix at node r. As before, we will ignore subscript r and
use Qn instead of Qnr . Finally, we write QnSS for the submatrix indexed by S.

We use the group structured norms defined in (Jalali et al.,
2011) in our analysis. The group structured norm ||u||G,a,b
of a vector u with respect to a set of disjoint groups G =
{G1 , . . . , GT } is defined as ||(||uG1 ||b , . . . , ||uGT ||b )||a . We
ignore the group G and simply use ||u||a,b when it is
clear from the context. Similarly the group structured
norm ||M ||(a,b),(c,d) of a matrix Mp×p is defined as

 

  1 
, . . . , ||M p || ) . In our analysis we always
( M
c,d

c,d

Then, there exists a constant kh such that
∗
∂ 2 Ar,i (η;θr·
)
maxu:|u|≤1
(u) ≤ kh .
∂η 2
Assumption 5. For all r ∈ V , the log-partition function
Ar (.) of the node wise conditional distribution satisfy that
there exists functions k1 (n, p) and k2 (n,
 p) such that for
all feasible pairs θ and X, ∇2 Ar (a)max ≤ k1 (n, p)
where a ∈ P
[b, b + 4 k2 (n, p) max {log (n) , log (p)} 1] for
b := θr + t∈V \r θrt Bt (Xt ), where for vectors u and v
we
 define [u,
 v] := ⊗i [ui , vi ]. Moreoever, we assume that
∇3 Ar (b)
≤ k3 (n, p) for all feasible pairs X and θ.
max
4.3.3. S PARSISTENCY T HEOREM

a,b

use b = 2, d = 2 and to minimize the notation we use
||M ||a,c to denote ||M ||(a,2),(c,2) . And we define ||M ||max
as max |Mi,j |, i.e, element wise maximum of M.
i,j

4.3.2. A SSUMPTIONS
Let us begin by imposing assumptions on the sample Fisher
Information matrix Qn .
Assumption 1. Dependency condition: Λmin (QnSS ) ≥
Cmin .
Assumption
2. Incoherence
condition:
 n

mmin (1−α)
Q c (Qn )−1 
√
for some α ∈ (0, 1] ,
≤
S S
SS
mmax
∞,2
dr
where mmax = max mt , mmin = min mt .
t

t

Assumption 3. Boundedness:

T
Λmax (E[B XV \r
 B XV \r ]) ≤ Dmax <  ∞,
where B XV \r is a vector such that B XV \r =
{Bt (Xt )}t∈V \r .
r
Note that the sufficient statistics {Bri (Xr )}m
i=1 of node r
need not be bounded. So to analyze the M-estimation problem, we make the following assumptions on log-partition
functions of joint and node-conditional distributions. These
are similar to the conditions imposed for sparsistency analysis of GLMs.
Assumption 4. The log partition function of the joint distribution satisfies the following conditions: for all r ∈ V
and i ∈ [mr ]

1. there exists constants km , kv such that E[Bri (Xr )] ≤
km and E[Bri (Xr )2 ] ≤ kv ,
2. there
exists
constant
kh
such
2
∗
∗
maxu:|u|≤1 ∂ ∂θA(θ)
(θri
+ u, θr·
) ≤ kh ,
2

Given these assumptions in 4.3.2 we are now ready to state
our main sparsistency result.
Theorem 2. Consider the vector space graphical model
distribution in (10) with true parameters θ∗ , edge set E and
vertex set V such that the assumptions 1-5 hold. Suppose
∗
||2 ≥ 10 Cmminmax (λ1 + λ2 )
that θ∗ satisfies min ||θrt
(r,t)∈E

regularization
parameters
λ1 , λ2
satisfy
q
p
log(pm2max )
2−α mmax
M1 α mmin k1 (n, p)
≤ λ1 + λ2 ≤
n
M2 2−α
k
(n,
p)
k
(n,
p)
for
positive
constants
M1
1
2
α


and

and M2 and λ2 < 2−α+2 mαmax /mmin λ1 . Then,
there exists constants L, c1 , c2 and c3 such that if n ≥

 m9
2
2
d2 k1 (n, p) (k3 (n, p)) (logp0 ) log p m2max ,
max L mmax
min
	
P
4 log(p m2max )
8 k2
, h log ( t mt ) , with probability at
k1 (n,p) k4 k2 (n,p)2 k42
−3 P
least 1 − c (p0 ) ( t mt ) − exp(−c2 n) − exp(−c3 n),
the following statements hold.
• For each node r ∈ V , the solution of the M-estimation
problem (14) is unique
• Moreover, for each node r ∈ V the M-estimation
problem recovers the true neighbourhood exactly.
where mmax = max mt , mmin = min mt , p0 = max(n, p).
t
t
The proof of Theorem
2 follows along
similar lines to the
sparsistency proof in (Yang et al., 2014), albeit with a subtler analysis to support general vector-spaces. It is based
on the primal dual witness proof technique and relies on
the previous results. We refer the interested reader to the
supplementary material for additional details regarding the
proofs.

that

5. Experiments

ri

3. for scalar variable η , we define a function Ār,i as:
n
R
P
Ār,i (η; θ) = log Xp exp ηBri (Xr )2 + s∈V Cs (Xs )
*
+
o
X
P
+ s∈V Bs (Xs ), θs +
θst Bt (Xt )
d(x)
t∈N (s)

(22)

We demonstrate the effectiveness of our algorithm on both
synthetic data and a real-world dataset of over four million
foods logged on the popular diet app, MyFitnessPal.
5.1. Synthetic experiments
The synthetic experiments were run on a vector-space
mixed MRF consisting of eight Bernoulli, eight gamma

Vector-Space MRFs via Exponential Families
Edges
(High Sparsity)

1.0

Edges
(Low Sparsity)

1.0

0.6

0.6

TPR

0.8

TPR

0.8

0.4

0.4

0.2

0.2

0.0
0.0

1.0

0.2

0.4

0.6

0.8

FPR
Within-Edge Parameters
(High Sparsity)

0.0
0.0

1.0

1.0

0.6

0.6

0.4

0.6

0.8

1.0

5.2. MyFitnessPal Food Dataset

TPR

0.8

TPR

0.8

0.2

FPR
Within-Edge Parameters
(Low Sparsity)

0.4

0.4

0.2

0.2

0.0
0.0

0.2

0.4

0.6

FPR

0.8

1.0

0.0
0.0

n=25000
n=5000
n=1000
n=100
0.2

0.4

0.6

FPR

0.8

Figure 5 shows the ROC curves at both the edge and parameter levels. The results demonstrate that our algorithm
improves well as the dataset size scales. They also illustrate
that graphs with a higher degree of sparsity are easier to recover with fewer samples. In both the high and low sparsity
graphs, the algorithm is better able to recover the coarsegrained edge structure than the more fine-grained withinedge parameter structure, though both improve favourably
with the size of the data.

1.0

Figure 1. ROC curves for our synthetic experiments. The top left
and bottom left plots show both edge as well as within-edgeparameter recovery performance respectively, for graphs with a
high degree of sparsity. The two right plots show the same performance measures, but for graphs with a relatively low degree of
sparsity. The low sparsity scenario is more challenging, requiring
more data to recover the majority of the graph.

(with unknown shape and rate), eight Gaussian (with unknown mean and variance), and eight Dirichlet (k=3)
nodes. The choice of these node-conditional distributions
is meant to highlight the ability of VS-MRFs to model
many different types of distributions. Specifically, the
Bernoulli represents a univariate, uni-parameter distribution that would still be possible to incorporate into existing mixed models. The gamma and Gaussian distributions are both multi-parameter, univariate distributions
which would have required fixing one parameter (e.g. fixing the Gaussians’ variances) to be compatible with previous approaches. Finally, the Dirichlet distribution is multiparameter and multivariate, thereby making VS-MRFs
truly unique in their ability to model this joint distribution.
For each experiment, we conducted 30 independent trials by generating random weights and sampling via Gibbs
sampling with a burn-in of 2000 and thinning step size
of 10. We consider two different sparsity scenarios: high
(90% edge sparsity, 50% intra-edge parameter sparsity) and
low (50% edge sparsity, 10% intra-edge parameter sparsity). Edge recovery capability is examined by fixing λ2
to a small value and varying λ1 over a grid of values in
the range [0.0001, 0.5]; parameter recovery is examined
analogously by fixing λ1 and varying λ2 . We use AND
graph stitching and measure the true positive rate (TPR)
and false positive rate (FPR) as the number of samples increases from 100 to 25K.

MyFitnessPal2 (MFP) is one of the largest diet-tracking
apps in the world, with over 80M users worldwide. MFP
has a vast crowd-sourced database of food data, where each
food entry contains a description, such as “Trader Joe’s Organic Carrots,” and a vector of sixteen macro- and micronutrients, such as fat and vitamin C.
We treat these foods entries as random vectors with an underlying VS-MRF distribution, which we learn treating the
food entries in the database as samples from the underlying
VS-MRF distribution. The text descriptions are tokenized,
resulting in a dictionary of approximately 2650 words; we
use a Bernoulli distribution to model the conditional distribution of each word. The conditional distribution of each
nutrient (on a per-calorie basis) is generally gamma distributed, but contains spikes at zero3 and large outlier values.4 The gamma distribution is undefined at zero, and
the outlier values can result in numerical instability during learning, which thus suggests using a distribution other
than the vanilla gamma distribution. Such zero-inflated
data are common in many biostatistics applications, and
are typically modeled via a mixture model density of the
form p(Z) = π δ0 + (1 − π) g(z), where δ0 is the dirac
delta at zero, and g(z) is the density of the non-zero-valued
data. Unfortunately, such mixture models are not generally
representable as exponential families.
To overcome this, we introduce the following class of
point-inflated exponential family distributions. For any
random variable Z ∈ Z, consider any exponential family
P (Z) = exp(η T B(Z) + C(Z) − A(η)), with sufficient
statistics B(·), base measure C(·), and log-normalization
constant A(·). We consider an inflated variant of this
random variable, inflated at some value j; note that this
could potentially lie outside the domain Z, in which case
the domain of the inflated random variable would become
Z ∪ {j}. We then define the corresponding point-inflated
2

http://myfitnesspal.com
This is common in foods since many dishes are marketed as
“fat free” or contain low nutrient density (e.g. soda).
4
This occurs when foods contain few calories but a large
amount of some micro-nutrient (e.g. multi-vitamins)
3

Vector-Space MRFs via Exponential Families
sweet
bread

creamer

rice

sandwich tuna

pasta

pork

baked

lean
subway

carbs

grain

potato

lettuce

vinaigrette

roasted

trans fat

black

grilled
tomato

protein
powder

balsamic

wheat
beans

blueberry

whey

steak

protein
beef

corn

fiber

bacon italian
pizza sausage

turkey
egg
ham

breast

fat

sugar

milk

vitamin A
fat

calcium

light

cholesterol
vitamin C

yogurt
chips

cheese

sodium

dressing
fresh

nonfat

cream

cookies

cheddar

ice

sauce
butter

potassium

value

chocolate

peanut

beer

vanilla

chicken

soup

fruit

bar

great

saturated fat

Figure 2. The top 100 edges in the MyFitnessPal food graph. Purple rectangular nodes correspond to macro- and micro-nutrients; green
oval nodes correspond to food description terms. Edge color is determined by the approximate effect of the edge on the means of the
node-conditionals: darker, blue edges represent lower means; brighter, orange edges represent higher means; thickness corresponds to
the norm of the edge weight.

exponential family distribution as:

	
Pinfl (Z) = exp η0 I(Z = j)+η1T B(z)+C(z)−Ainfl (η) ,
where Ainfl (η) is the log-normalization constant of the
point-inflated distribution which can be expressed in terms
of the log-normalization constant A(·) of the uninflated exponential family distribution: Ainfl (η) =
 log exp{η0 } −
T
exp{η1 B(j)I(j ∈ Z)} + exp{A(η1 )} . Thus, as long as
we have a closed form A(·) for the log-partition function of
the base distribution, we can efficiently calculate Pinfl (Z).
The definition also permits an arbitrary number of inflated
points by recursively specifying the base distribution as another point-inflated model. We model each of the MFP
nutrients via a two-point-inflated gamma distribution, with
points at zero and a winsorized outlier bucket.
Due to the size of the overall graph, presenting it in closer
detail here is not feasible. To give a qualitative perspective
of the relationships captured by our algorithm, we selected
the top 100 edges in the MFP food graph by ranking the
edges based on their L2-norm. We then calculated their
approximate contribution to the mean of their corresponding node-conditionals to determine edge color and thickness. Figure 2 shows the results of this process, with edges
that contribute positively colored in orange and edges that
contribute negatively colored in blue; edge thickness corresponds to the magnitude of the contribution. A high-level
view of the entire learned graph is available in the supplementary materials.
Several interesting relationships can be discovered, even
from just this small subset of the overall model. For
instance, the negative connection between “peanut” and
sodium may seem counter-intuitive, given the popularity
of salted nuts, yet on inspection of the raw database it
appears that indeed many peanut-based foods are actually

very low in sodium on a per-calorie basis. As another example, “chips” are often thought of as a high-carb food,
but the graph suggests that they actually tend to be a bigger
indicator of high fat. In general, we believe there is great
potential for wide-ranging future uses of VS-MRFs in nutrition and other scientific fields, with the MFP case study
only scratching the surface of what can be achieved.

6. Conclusion
We have presented vector-space MRFs as a flexible and
scalable approach to modeling complex, heterogeneous
data. In particular, we generalize the concept of mixed
MRFs to allow for node-conditional distributions to be distributed according to a generic exponential family distribution, that is potentially multi-parameter and even multivariate. Our VS-MRF learning algorithm has reassuring
sparsistency guarantees and was validated against a variety
of synthetic experiments and a real-world case study. We
believe that the broad applicability of VS-MRFs will make
them a valuable addition to the scientific toolbox. All code
for our VS-MRF implementation is publicly available.5

7. Acknowledgments
We acknowledge the support of ARO via W911NF-121-0390 and NSF via IIS-1149803, IIS-1320894, IIS1447574, and DMS-1264033.

References
Boyd, Stephen and Vandenberghe, Lieven. Convex optimization. Cambridge university press, 2009.
Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
5

https://github.com/tansey/vsmrfs

Vector-Space MRFs via Exponential Families

Eckstein, Jonathan. Distributed optimization and statistical learning via the alternating direction method of mulR in Machine Learntipliers. Foundations and Trends
ing, 3(1):1–122, 2011.
Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
A note on the group lasso and a sparse group lasso.
arXiv preprint arXiv:1001.0736, 2010. URL http:
//arxiv.org/abs/1001.0736.
Jalali, Ali, Ravikumar, Pradeep, Vasuki, Vishvas, and
Sanghavi, Sujay. On learning discrete graphical models
using group-sparse regularization. AI STAT, 2011.
Lauritzen, Steffen L. Graphical models. Oxford University
Press, 1996.
Parikh, Neal and Boyd, Stephen. Proximal algorithms.
Foundations and Trends in Optimization, 1(3):123–231,
2013.
Simon, Noah, Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. A sparse-group lasso. Journal of
Computational and Graphical Statistics, 22(2):231–245,
2013.
Sudderth, Erik B, Ihler, Alexander T, Isard, Michael, Freeman, William T, and Willsky, Alan S. Nonparametric belief propagation. Communications of the ACM, 53(10):
95–103, 2010.
Vats, Divyanshu and Moura, José MF. Finding nonoverlapping clusters for generalized inference over
graphical models. Signal Processing, IEEE Transactions
on, 60(12):6368–6381, 2012.
Wainwright, M. J. Sharp thresholds for noisy and highdimensional recovery of sparsity using l1-constrained
quadratic programming (lasso). IEEE Transactions on
Information Theory, pp. 2183–2202, 2009.
Wainwright, Martin J and Jordan, Michael I. Graphical
models, exponential families, and variational inference.
R in Machine Learning, 1(1-2):
Foundations and Trends
1–305, 2008.
Yang, Eunho, Allen, Genevera, Liu, Zhandong, and
Ravikumar, Pradeep. Graphical models via generalized
linear models. In Advances in Neural Information Processing Systems, pp. 1358–1366, 2012.
Yang, Eunho, Baker, Yulia, Ravikumar, Pradeep, Allen,
Genevera, and Liu, Zhandong. Mixed graphical models
via exponential families. Proceedings of the Seventeenth
International Conference on Artificial Intelligence and
Statistics, pp. 1042–1050, 2014.

