Admixture of Poisson MRFs:
A Topic Model with Word Dependencies

David I. Inouye
Pradeep Ravikumar
Inderjit S. Dhillon
Dept. of Computer Science, University of Texas, Austin, TX 78712, USA

Abstract
This paper introduces a new topic model based
on an admixture of Poisson Markov Random
Fields (APM), which can model dependencies
between words as opposed to previous independent topic models such as PLSA (Hofmann, 1999), LDA (Blei et al., 2003) or SAM
(Reisinger et al., 2010). We propose a class of
admixture models that generalizes previous topic
models and show an equivalence between the
conditional distribution of LDA and independent
Poissons—suggesting that APM subsumes the
modeling power of LDA. We present a tractable
method for estimating the parameters of an APM
based on the pseudo log-likelihood and demonstrate the benefits of APM over previous models
by preliminary qualitative and quantitative experiments.

DINOUYE @ CS . UTEXAS . EDU
PRADEEPR @ CS . UTEXAS . EDU
INDERJIT @ CS . UTEXAS . EDU

lacunae that arise from its basic use of the multinomial distribution to model word distributions for topics. There are
several reasons which make the multinomial distribution
an inadequate distribution for documents and topics. The
primary issue is that it does not model dependencies between words: if the word “kernels” appears in a document
(specifically, a machine learning paper), the appearance of
the word “graphs” might be less likely. Alternatively, if the
word “classification” appears, “supervised” is more likely
to appear than in general documents. Indeed, typical coherence metrics that quantitatively measure the goodness of fit
of various topic models primarily test for such dependence
among estimated top words for the topics (see Sec. 6.2). A
second caveat is that the multinomial distribution does not
model absences of words. Lastly, the multinomial word
distribution does not leverage varying document lengths.
french
For instance, with large counts of other
words, some specific word might become less likely.
northern

southern

france

light

produced

To address the issue of modeling word absences, Reisinger
forms
et al. (2010) proposed the use of von Mises-Fisher distribution for topic distributions.
Butgovernment
while this addresses one
north
modern
system
issue with multinomials,
it british
does not
model
word dependencentral
including time
form

1. Introduction

power

german

human

Topic models can be understood as a class of statistical
models for document collections that model documents as
admixtures over topics. Specifically, each topic is modeled
as a distribution over words, and each document is a separate mixture of such topics (or specifically, the word distributions comprising the topics). Such an admixture can
be contrasted with a vanilla mixture of topics, where each
document would be drawn from a single topic.

west

america

times

own

east

de

region

united

american

europe

"Development"

Interesting Hubs

social

population

economic
developed

sq

development

late

Interesting Subgraphs

km
mi

deg

country

period

century

york

million

age

england

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

air

people

english

A popular set of topic models is PLSA (Hofmann, 1999),
which uses the multinomial distribution as the word distribution for any topic, and its Bayesian counterpart, LDA
(Blei et al., 2003), which adds Dirichlet priors. While these
topic models have proved enormously useful in modeling
varied document collections and have attracted a long line
of work with numerous extensions (see (Blei, 2012) for a
review of LDA applications and trends), it has some crucial

south

control

king
charles

death

body

"England"

law

major

roman

church

city

center

life

president

james
history robert
western

land
water

public

study

repr

river

university

william

john

species

national

established "Politics"

largest

music

style

led

political

party

war
world

ed

include

example

usually ft
common

sea

theory

family

found
art

name

called

Figure 1: A Poisson MRF can provide interesting insights
into a text corpus including multiple word senses (hubs of
graph) and semantic concepts (coherent subgraphs).

Admixture of Poisson MRFs

2. Poisson MRFs (PMRFs)
First, we review the Poisson MRF model (PMRF) as proposed by Yang et al. (2012). Second, we contextualize the
independent PMRF model by showing an equivalence with
the conditional distributions of LDA. Finally, we propose a
novel prior distribution for PMRFs that can be viewed as a
generalization of the Gamma distribution.
2.1. PMRF Definition
By assuming that the conditionals of the joint distribution
are univariate Poisson, Yang et al. (2012) recently proposed
a PMRF model that provides a joint distribution over multivariate count data. They also provided a tractable way
to estimate the parameters of such a PMRF using `1 regularization and proved that the estimator is guaranteed to
recover the underlying dependency structure with some assumptions including sparsity of the parameters. The model

T

Pr (x | θ, Θ) ∝ exp θ x + x Θx −

PMRF

p
X

)
ln(xs !) ,

s=1

where θ ∈ Rp and Θ ∈ {Rp×p
: diag (Θ) = 0}. By
−
construction, the conditional distribution of a variable xs
given all other variables x\s is a univariate Poisson with
canonical parameter ηs = θs + ΘTs x and mean (standard)
parameter λs = exp(ηs ). An illustration of the density of
a 2D Poisson with negative, zero and positive dependency
can be seen in Fig. 2. Other observations about a PMRF:
• The dependency parameter Θ is analogous to the
Gaussian precision matrix Σ−1 in a Gaussian MRF.
• If Θ = 0, then the PMRF reduces to an independent
multivariate Poisson distribution.
• Negative dependencies can help model sparse data
(i.e. data with many 0’s) because the density can be
concentrated on the axes as seen seen in Fig. 2 (left).
Independent PMRF

PMRF Negative Dependency

PMRF Positive Dependency

8

8
6
4
2

8
Count of Word 2

We position the Poisson MRF in context of topic models
by showing that the conditional distributions of the classical LDA model can be written as a Poisson MRF where
the underlying graph has no edges and hence no dependencies between words; this connection—which was only
recently discovered in the context of matrix factorization
(Gopalan et al., 2013)—not only puts into relief the assumptions made by LDA but also opens the door to other
approximate inference schemes for LDA (which however
we do not explore here). In other contributions of this paper, we define a new class of models called admixtures
and show that this class generalizes previous topic models,
which thus opens the door to other topic models based on
non-Poisson distributions. Finally, we provide qualitative
as well as quantitative evidence for the benefits of APM
by training the APM model on both a subset of the Grolier
encyclopedia and the CMU 20 Newsgroup dataset.

T

Count of Word 2

In this paper, we propose using Poisson MRFs (Yang et al.,
2012) for topic distributions and using the resulting admixture of Poisson MRFs (APM) for modeling document collections. These Poisson MRFs allow modeling multivariate
count data and use dependencies among the count variables
to represent the joint distribution compactly. Moreover,
since these are graphical model distributions, the dependencies are Markov with respect to an undirected graph,
which thus provides a visually appealing representation of
any topic—see Fig. 1—in contrast to just a list of words as
in PLSA/LDA.

PMRF(θ, Θ) is defined as follows:
(

Count of Word 2

cies, nor does it leverage document lengths in any substantative way.

6
4
2
0

0
0

2
4
6
Count of Word 1

8

6
4
2
0

0

2
4
6
Count of Word 1

8

0

2
4
6
Count of Word 1

8

Figure 2: The densities of three 2D Poisson MRFs that
show possible dependency structures between two words.
Negative dependencies (left) suggest that two words rarely
co-occur whereas positive dependencies (right) suggest that
two words often co-occur.
The negativity constraint on Θ is required under the formulation above to ensure that the distribution is normalizable
(Yang et al., 2012). However, Yang et al. (2013) propose a
slight modification to the sufficient statistics of the PMRF
that removes this constraint and allows positive parameter
values—see reference for details. For simplicity of notation and wording, throughout the rest of the paper, the basic PMRF notation will be used for derivations though the
APM model uses this slightly modified PMRF.
2.2. LDA Conditionals Equivalent to Independent
Poissons
In this section, we place Poisson MRFs in the context
of topic models by showing the equivalence between the
conditionals of LDA and an independent Poisson MRF.1
LDA assumes the following generative process for a new
document given that the topic weights for the document
1

Gopalan et al. (2013) recently introduced the connection between LDA and Poisson models in the context of matrix factorization.

Admixture of Poisson MRFs

w and the topic distribution parameters φ1...k are known:
1) Draw x̃ ∼ Poisson(λ̃) 2) For each of the x̃ words:
(a) Draw topic index z ∼ Categorical(w) (b) Draw word
v ∼ Categorical(φz ). Notice that because x̃ is independent of the other variables in LDA, it is often simply ignored when estimating the model parameters. In
our model, however, x̃ cannot be ignored because words
can be dependent. By marginalizing out the topic variable z, step 2 can be collapsed into a draw from a Multinomial with a single parameter φ̃, which is simply a
weighted average over the topic distribution parameters φz .
This yields the following modified step: 2’) Draw docuPk
ment x ∼ Mult(φ̃ = j=1 wj φj | N = x̃). Therefore,
the probability of a document x given w and φ1...k is:
Pk
PrPoiss (x̃ | λ̃) PrMult (x | φ̃ = j=1 wj φj , N = x̃).
Amazingly, this Poisson-Multinomial joint distribution is
equivalent to p independent Poissons (Bishop et al., 2007):
p
Y
e−λs xs
λ
Pr (x | λ1 , · · · , λp ) =
Ind. Poiss
xs ! s
s=1
!xs
p
x̃! e−λ̃ Y λ̃λs
= Qp
x̃! s=1 xs ! s=1
λ̃
p
Y  λs xs
x̃!
e−λ̃ x̃
Q
λ̃
=
p
x̃!
λ̃
s=1 xs ! s=1




= Pr x̃ | λ̃ Pr x | θ = (λ1 , · · · , λp ) /λ̃, N = x̃
Poiss

Mult

Pp

Pp
where λ̃ =
s=1 λs and x̃ =
s=1 xs . Therefore,
a PMRF directly generalizes the conditional distribution
of PLSA/LDA by relaxing the independence assumption.
To more fully generalize LDA, priors must be added to a
PMRF as proposed next.
2.3. Adding Priors to a PMRF
Similar to LDA’s prior, a conjugate prior on the parameters
of a PMRF can be defined as being proportional to:
exp{β T θ+β T Θβ−γA (θ, Θ)−λθ kθk22 −λkvec(Θ)k1 },
where ∀s, βs > 0, γ ≥ 0, λθ > 0 and λ > maxi,j βi βj .2
One observation is that when Θ = 0, exp(θs ) is essentially
Gam(shape = β s ; scale = 1). Therefore, for independent
Poissons, this is similar to using Gamma priors. The posterior distribution merely modifies the hyperparameters to be
β̃ = β + x and γ̃ = γ + 1. Therefore, because this prior
adds pseudo-counts β to the observations for parameter estimation, this prior for PMRFs is analogous to a Dirichlet
prior for multinomials as in LDA.
2
The conditions on the hyperparameters are needed for normalization. In practice, λθ can be set arbitrarily small and is thus
ignored in subsequent discussions. The λ hyperparameter is used
for `1 regularization as discussed in Sec. 5.1.

w1
"Documents"

w1

Mixture
Components

Sparse
"Document"
Dense
"Document”

w2

w3

w2

Dense
"Topic"
Sparse
"Topic"

w3

Figure 3: (Left) In mixtures, documents are drawn from
exactly one component distribution. (Right) In admixtures,
documents are drawn from a distribution whose parameters
are a convex combination of component parameters.

3. Generalized Admixtures
In a simple mixture model, an observation is assumed to
come from exactly one of k possible components. An illustration of this type of model is shown in Fig. 3 (left) in
which documents are drawn from exactly one of two component distributions—the “topics” in the case of document
modeling. On the other hand, for admixtures each document is drawn from a distribution whose parameters can
be any convex combination of the component parameters,
allowing each document to be explained by multiple components as illustrated in Fig. 3 (right).3
Given this intuition about admixtures, the probability of a
single observation x from an admixture of some base distribution (e.g. multinomial, von Mises-Fisher, PMRF)—
assuming that the admixture weights w and component
canonical parameters Φ = φ1...k are given—is defined as:
 



k
X

Pr (x | w, Φ) = Pr x  φ̄ = Ψ−1 
wj Ψ(φj ) ,
Base
Admix.

j=1
(1)
where Ψ allows for the mixing to occur in a suitable transformation of the parameter space. In the context of exponential families, the mixing could occur either in the canonical parameter space in which case Ψ would be the identity function, or it could occur in the mean parameter space
(such as the mean µ and covariance Σ for a multivariate
Gaussian). For this paper, unless otherwise specified, we
will assume that Ψ is equal to the identity function.
If priors are given for the admixture weights w and the
topic parameters φ1...k with parameters α and β hyperparameters respectively, the joint distribution of a single
observation and the parameters is:
 


k
k
X
Y

Pr x  φ̄ =
wj φj  Pr(w)
Pr(φj )
Base

j=1
j=1
3
Fig. 3 is only meant as an illustration and not as a rigorous
visualization of mixtures or admixtures. It should be noted that,
in general, the KL-divergence should be minimized rather than `2
distance as suggested by the figure.

Admixture of Poisson MRFs

This gives the joint distribution over a set of n independent
observations as:
Pr (X, W, Φ) =
(2)
Admix.
 


k
n
k
X
Y
Y

wi,j φj  Pr(wi )
Pr xi  φ̄ =
Pr(φj )
Base

j=1
i=1
j=1
Intuitively, this admixture model formulation means that
each observation can be explained by a mixture of a relatively small number of component distributions parameterized by φj . In the special case where w is an indicator
vector, this distribution becomes a standard mixture model
where each observation is explained by only one component. In the special case where k = 1, the admixture simply reduces to every observation being drawn from a single base distribution. Therefore, this admixture formulation generalizes both single and mixture distributions.
This admixture model also generalizes previous topic models and provides a general framework for defining new admixture models based on any parametric distribution. In
the next sections, several examples of previous admixture
models are given followed by the formulation of this paper’s main model—an admixture of Poisson MRFs which,
to the authors’ best knowledge, is the first admixture model
to allow dependencies between words.
Example 1 - LDA As shown in Sec. 2.2, LDA assumes
that each document is draw from an admixture of multinomials. The admixture weights and the parameters for each
topic multinomial are drawn from a Dirichlet prior. It is
important to notice that LDA mixes in the standard multinomial mean parameter space (i.e. ΨLDA is the canonical
to mean parameter transformation).
Example 2 - Population Admixtures In the genetic
community, the term admixture has been used to describe a population produced by interbreeding several
previously-isolated populations into a new admixed population. Pritchard et al. (2000) use a model equivalent to
LDA to explore this concept. Under this population model,
the original ancestors of a population correspond to topics
and individuals correspond to documents.
Example 3 - SAM The Spherical Admixture Model
(SAM) as proposed by Reisinger et al. (2010) is an admixture model where the base distribution is a Von MisesFisher distribution—the independent Gaussian analog defined on the unit hypersphere. The model, which is motivated by the observation that cosine distance is an important document similarity, assumes Dirichlet and Von MisesFisher priors on the admixture weights and component parameters respectively.

4. Admixture of Poisson MRFs
With the background on PMRFs and the development of
admixtures, the main model of this paper—an admixture
of Poisson MRFs (APM)—can be developed. Relaxing
the independence assumption of previous admixture models such as LDA, APM assumes that the base distribution
is a PMRF. This yields the following joint distribution:
Pr (x, w, θ1...k , Θ1...k )
(3)
 


k
k
X
X


wj θj , Θ̄ =
wj Θj 
= Pr x  θ̄ =
PMRF

j=1
j=1

APM

× Pr(w)
Dir

k
Y

Pr(θj , Θj )

j=1

where PrDir (w | α) is a Dirichlet prior on the admixture
weights (similar to LDA) and Pr(θj , Θj ) is the PMRF prior
defined in Sec. 2.3. Because of the equivalence described
in Sec. 2.2, APM subsumes the expressive power of LDA.
The primary difference between an independent APM and
LDA is that LDA mixes in the standard Multinomial parameter space whereas APM mixes in the canonical parameter space. An interesting open area for future research
could be admixing the component PMRFs in a different
parameter space such as the mean parameter space.4 Fundamentally, however, this model is much more expressive
than all previous admixture models because it allows for
dependencies between words.
4.1. Topic Representation
In the APM model, topics are represented as PMRFs, and
therefore, each topic provides a full graph over words
showing word dependencies rather than just a list of words
as in independent models such as LDA (see Fig. 4 in Sec. 6
for example topic graphs). This representation opens up a
whole new area for interpreting, exploring and visualizing
topics using a graph. In addition, all the metrics and algorithms on graphs such as tree width or shortest path could
be used to explore each topic.
4.2. Document Representation
Documents could be represented in at least two different
ways under the APM model. First, they could be represented by their admixture weights, and therefore, APM
could be used as a type of dimensionality reduction technique. Second, each document can be represented as a full
graph over words just like a topic because each document
is associated with an admixed PMRF. This graph represen4

For more information on the relationship between the mean
and canonical parameter spaces, see (Wainwright & Jordan,
2008).

Admixture of Poisson MRFs

tation provides a powerful new way to visualize and summarize a document that was not possible with independent
models like LDA.

where λ is the `1 regularization parameter, W is the set of
all possible matrices such that the columns are probability
vectors and δW (W) = {0, if W ∈ W; ∞, otherwise}.

5. Parameter Estimation by Optimizing
Approximate Posterior

5.3. Proximal Optimization Algorithms

The parameters of an admixture of Poisson MRFs can be
estimated by minimizing the negative log posterior. Because the true log-likelihood of a Poisson MRF is computationally intractable for complex multivariate distributions
(Wainwright & Jordan, 2008), the pseudo log-likelihood—
which approximates the joint distribution as a product of
node conditionals—will be used instead. With the Dirichlet prior on w and the prior described in Sec. 2.3 on the
component parameters, the approximate posterior is:
P ≈ P̂(W, θ1...k , Θ1...k | X)
(4)
(" p
#
)
n
X X
T
∝
ηs,i x̂s,i −(γ+1)A (ηs,i ) +(α−1) ln(wi ) ,
i=1

s=1

Pk
where x̂ = x+β and ηs,i = j=1 wi,j (θj,s + Θj,s x̂i,\s ) is
the canonical parameter of a univariate Poisson.
5.1. Enforcing Sparsity of Θj by `1 Regularization
For interpretability, generalizability and computational
tractability, the parameters of high-dimensional MRFs are
often assumed to be sparse (i.e. a small number of nonzeros compared to zeros). This sparsity assumption is usually incorporated into the problem by adding an `1 regularization term to the objective function. This `1 regularized estimator has been shown to have theoretical guarantees on structural recovery for Bernoulli MRFs/Ising Models (Ravikumar et al., 2010), Gaussian MRFs (Ravikumar
et al., 2011) and, more recently, Poisson MRFs (Yang et al.,
2012; 2013). For similar reasons, APM assumes that the
parameter matrices (Θj ) for each topic PMRF are sparse
and, like the aforementioned methods, estimates this sparse
solution by using an `1 regularization term. Intuitively,
this sparsity assumption makes sense because most words
are only directly related to a small subset of other relevant
words.
5.2. Unconstrained Optimization
Along with the regularization of the Θj parameter matrices,
APM requires that the columns of the admixture weights
matrix W be probability vectors (i.e. properly defined mixture weights that lie on the k-dimensional simplex). This
leads to the following unconstrained optimization problem:
arg min
W,θ1...k ,Θ1...k

−P̂ + δW (W) + λ

k
X
j=1

Because the objective in (5) is composed of a differentiable
term (i.e. P̂) and two non-differentiable terms (i.e. δW (W)
Pk
and λ j=1 kvec(Θj )k1 ), a simple gradient descent algorithm cannot be used to solve the problem. Therefore,
in this paper, we use a proximal optimization algorithm
(Parikh & Boyd, 2013). Essentially, a proximal algorithm
is an iterative algorithm that computes each new parameter
estimate using only the previous estimate and the differentiable term. After finding a new estimate, a proximal algorithm applies the prox operator(s) to this estimate to incorporate the non-differentiable terms of the objective function
and iterates until convergence.
The prox operator for the `1 regularization term in
(5) is the simple soft thresholding operator: Sλ (z) =
sign(z) max(|z| − λ, 0). The prox operator for δW (W)
is simply the best Euclidean projection onto the simplex
which can be computed using the algorithm from (Chen
& Ye, 2011). With these operators, any proximal algorithm can optimize (5) but, in this paper, a FISTA-like
algorithm was used (Beck & Teboulle, 2009). Because
APM is a model with many parameters and FISTA-like
algorithms are only first-order optimization methods, the
models trained for this paper required many iterations to
converge (> 5000). Using faster and more complex proximal optimization algorithms such as a proximal Newton
method would be an excellent area for future work.

6. Preliminary Experiments
Because previous admixture models have been independent, it is difficult to directly compare APM to previous
models. Therefore, first, an experiment was conducted by
running APM (with k = 5 and p = 500) on approximately
31,000 articles of the Grolier encyclopedia.5 Visualizations
of the topics were constructed using the graph visualization
program Gephi6 in order to show some qualitative results
on the model output and suggest that APM can provide a
more interesting, intuitive and visually appealing representation of topics than merely a list of words as in standard
topic models. Two topics of this run can be seen in Fig. 4
and other topic graph examples are given in the appendix.
Also, a simple experiment was conducted to give some evidence, though inconclusive, that the APM model subsumes
the power of the LDA model because of the model equivalence described in Sec. 2.2.
5

kvec(Θj )k1

(5)

6

www.cs.nyu.edu/˜roweis/data.html
www.gephi.org

Admixture of Poisson MRFs
france

jean

french

surface

novel

died

novels

plays

roman

air

stage
theater

soviet

emperor

heat

space

movement

church
sound

musical

production

western

country

century

spain

late

sun
gas

nuclear
heat
energy

music
eastern

centuries

introduced

earth

russian

rome

human

spanish

field

peter

life eng

kingdom

temperature

modern
literature

deg

foreign

wood

india

average
temperatures

language

mm

languages

indian

development

research

indians

rock

million
persons

(a) “Music and Fine Arts”

annual

(b) “Temperature”

Figure 4: These APM topic visualizations illustrate that PMRFs are much more intuitive than multinomials (as in
LDA/PLSA), which can only be represented as a list of words. Word size signifies relative word frequency and edge
width signifies the strength of word dependency (only positive dependencies shown).
6.1. Qualitative Experiment
The graphs as seen in Fig. 4 have many interesting structural features that can be interpreted.7 In the first example
(Fig. 4a), the word “musical” is a hub word that connects
the two concepts of “theatre” and “music”. A similar idea
happens in the second example (Fig. 4b) where “temperature” connects the concepts of “heat”, “gas” and “deg”.
Another interesting feature is chains of words whose endpoints are not directly related but only related through other
words. For example, the chain “music” ↔ “musical” ↔
“theater” ↔ “plays” suggests that “music” and “plays” are
related, albeit indirectly. The chain “sun” ↔ “gas” ↔
“temperature” ↔ “heat” ↔ “nuclear” in Fig. 4b shows
the connection that the sun is related to nuclear reactions
through the words “heat”, “gas” and “temperature”. For
other chains, the endpoints are not related even though each
edge seems reasonable. For example, the chain “novel” ↔
“eng” ↔ “plays” ↔ “theater” ↔ “musical” ↔ “music” ↔
“church” has logical connections for each edge but “novel”
is not usually associated with “church”.
Though these features give evidence for the usefulness and
power of APM, they do not capture any of the negative dependencies between words—words that do not tend to cooccur. For example, the words “novel” and “math” would
not tend to co-occur. This might be helpful in excluding
documents from certain categories in document categorization. For example, if the words “history”, “war” and “politics” appear in a document, the document is unlikely to be
science literature. Though it may be more difficult to visualize these negative dependencies, negative dependencies
7

These graphs were manually filtered to simplify the whole
graph. A future area of research could be to automatically filter
the graph to important clusters.

can provide interesting structural information of the underlying dataset.
6.2. Coherence Experiment
Though the APM model gives significantly more information than simply a list of topic words as in LDA, another
experiment was conducted to give evidence that APM can
be used to produce a list of topic words similar to other
topic models. The APM model was applied to the CMU
20 Newsgroup dataset evaluated with the two metrics explained next. Because of the complexity of APM, only the
top 200 words were used. Please see Sec. 8 for some possible future research that could make APM more scalable.
This experiment is meant only to be a preliminary experiment. Extensive experiments on larger datasets and exploration of the parameters of the model are significant areas
of future work but are outside the scope of this paper since
this paper focuses on model definition, contextualization
and parameter estimation.
Two topic coherence metrics that have been shown to correlate with human annotators were used in this experiment.
First, the UMass coherence metric introduced by Mimno
et al. (2011) and further explored by Stevens & Kegelmeyer
(2012), evaluates the intrinsic coherence of the generated
topics by computing co-occurrence statistics from the training data. Letting each topic t be an ordered list of top m
words t = (v1 , · · · , vm ), the UMass coherence metric is
defined as follows:
cohUMass (t) =

m a−1
X
X
a=2 b=1


ln

D(va , vb ) + 
D(vb )



where D(va , vb ) and D(vb ) are the co-occurrence and
marginal co-occurrence statistics in the training corpus and

Admixture of Poisson MRFs

b6=a

where Pr(va , vb ) and Pr(va ) are computed from the local
co-occurrence statistics in a sliding window of an external
corpus. To compute the probabilities for the PMI metric, a
recent dump of Wikipedia was used with a sliding window
of 20 words.
Stevens & Kegelmeyer (2012) explored the importance of
 in both coherence metrics and, in light of this,  was set
to 10−12 as it was in (Stevens & Kegelmeyer, 2012). For
simplicity, a set of topic words was chosen based on the θ
parameter of the PMRF. In general, because a PMRF contains information about word dependencies, the best words
could be chosen using some sort of graph density algorithm
such as the one described in (Yuan & Zhang, 2013).
LDA was trained using the MATLAB Topic Modeling
Toolbox,8 which uses the Gibbs sampling method described in (Steyvers & Griffiths, 2007) and was run for
5000 iterations. For both LDA and APM, the hyperparameters α and β were set to α = 200/p and β = 50/k
respectively as suggested by the documentation of the toolbox. For APM, the parameter λ was set near 10−7 , which
was chosen so that there would be some edges in the initial
iterations—however, as discussed in the following section,
the final converged APM solution did not have any edges.
Because LDA and APM might perform differently with different number of topics k, three values for k = {5, 10, 15}
were evaluated for both LDA and APM. We might expect
that LDA will need more topics to model the data because
LDA assumes independence and hence has less parameters.
6.3. Results from Preliminary Experiments
Results for APM and LDA on the 20 Newsgroup dataset
can be seen in Fig. 5. The topic words chosen for both
models when k = 10 can be found in the appendix. APM
seems to outperform LDA in this simple 200-word experiment for the UMass metric whereas APM is only comparable to LDA for the PMI metric. Because APM directly
models the co-occurrence of words, it seems reasonable
that APM would perform better in the UMass coherence
metric, which focuses on model fit and internal coherence.
However, on the PMI coherence metric, APM only seems
8
www.psiexp.ss.uci.edu/research/programs_
data/toolbox.htm

Interestingly, though some of the initial iterations of APM
included many word dependencies, the final iterations gave
admixtures of independent Poisson MRFs. This is likely
due to the fact that only a small text collection was used,
and therefore, the power of dependencies is not needed
to appropriately model the data. This result could also
be caused by the choice of the regularization parameter λ.
Though λ was chosen by simply trying several values, an
important area for future research is how to choose λ appropriately for the application. However, this result shows
that APM can effectively model words even in the independent case and can perform competitively with LDA as
expected by the equivalence discussion in Sec. 2.2.
LDA
APM

−20
−40
−60
−80
5

10
15
Num. Topics

PMI Coherence Score

The second coherence metric, Pointwise Mutual Information (PMI), was introduced by Newman et al. (2010) and
has also been shown to correlate with human judgments of
topic coherence. The PMI metric is defined as follows:


m X
X
Pr(va , vb ) + 
1
ln
cohPMI (t) =
m(m − 1) a=1
Pr(va ) Pr(vb )

to do at least as well as LDA suggesting APM should probably be studied through an extensive experimental comparison as suggested in the future works section. In addition,
as expected because of model complexity, LDA seems to
perform better with larger k. APM’s performance seems
to degrade as the k increases which might be due to the
estimation procedure not fully converging given the high
model complexity.

UMass Coherence Score

 is introduced to avoid taking the log of zero. Loosely, this
measures how well the model fits the training data.

−2.4
LDA
APM

−2.6
−2.8
−3
−3.2
−3.4
5

10
15
Num. Topics

Figure 5: APM seems to outperform LDA in a simple 200word experiment when the number of topics is small but
is only comparable to LDA for a larger number of topics.
(Median score is shown to reduce the effect of outliers.)

7. Related Work
Many probabilistic models for documents have been constructed using the multinomials. Nigam et al. (2000) introduced a mixture of multinomials to model document collections, and later, Hofmann (1999) proposed an admixture of multinomials called Probabilistic Latent Semantic
Analysis (PLSA). This model was followed by the very
successful Latent Dirichlet Allocation (LDA) topic model
proposed by Blei et al. (2003) that added priors to the distributions as well as provided a more coherent framework
for extending the model. There have been numerous extensions of LDA that incorporate other knowledge such as author information (Steyvers et al., 2004), time (Blei & Lafferty, 2006) and topic dependency (Blei & Lafferty, 2005).
However, none of these models considers dependencies between words since the base distribution is multinomial.

Admixture of Poisson MRFs

Replicated Softmax (Hinton & Salakhutdinov, 2009) uses
a restricted Boltzmann machine (RBM) with parameter biases to create a generative model for word count vectors.
The hidden layer is binary-valued and allows for topic
parameters to be mixed in the canonical parameter space
(similar to APM). Wordfish (Slapin & Proksch, 2008) is a
Poisson IRT (Item Response Theory) model that attempts
to characterize the latent position of a political party based
on political manifestos (e.g. determining left or right wing
political views). Though Wordfish also adds fixed-effect
parameters, this model is similar to an independent APM
model with k = 2 (i.e. only one latent dimension). Both
Replicated Softmax and Wordfish significantly differ from
APM because they do not consider word dependencies.
Sparse Word Graphs (Nallapati et al., 2007) attempts to
create graph visualizations of the topics by combining
LDA and Bernoulli MRFs (Ising model) in a two-stage approach. First, LDA is used to estimate the topic assignments for every word in the corpus. Then, these topic assignments are used to train k independent Bernoulli MRFs
for each topic. To transform the LDA output into the input
for the Bernoulli MRF estimation algorithm, binary worddocument matrices are constructed for each topic based on
the LDA topic assignments. Though this leads to a graph
over words for each topic, one major difference with APM
is that this two-stage method is not a unified probabilistic
model but rather two separate probability models. Another
significant difference is that Sparse Word Graphs estimates
simpler Bernoulli MRFs instead of PMRFs as in APM.
In (Hu et al., 2011), users can interactively add soft constraints to LDA so that the probability of the words in the
constraint set will tend to be similar (e.g. either all low
or all high probability). The soft dependency is added
through a latent constraint variable and only provides indirect dependence of words rather than direct dependence
between words as in APM. Another difference is that these
constraints can only be supplied as user-specified disjoint
groups of words rather than automatically-discovered arbitrary structure as in APM.
Collins et al. (2001) develop a generalization of PCA by using the likelihood of exponential families as the loss function instead of squared loss—which would correspond to
Gaussian errors. While exponential PCA is related to admixtures, it does not place constraints on the admixture
weights but rather allows them to be arbitrary real numbers. This is analagous to the difference between SVD and
constrained non-negative matrix factorization (NMF).

8. Future Work
Scalability Because APM allows for dependencies between all words, the model is quadratic in the number of
words p. Therefore, scalability could be an significant obstacle to overcome in future research. However, since sparsity is assumed on the dependencies, the effective number of parameters can be reduced significantly. For Gaussian MRFs, this fact was recently exploited by Hsieh et al.
(2013) to find the dependency parameters—the precision
matrix in this case—even for very high dimensional data.
We believe that some of the intuitions in (Hsieh et al., 2013)
could be employed to effectively scale APM.
Empirical Study Because this paper focuses on model
definition, extensive empirical experiments were not conducted. In future work, several parameter settings such as
the choice of hyperparameters (α, β) or the regularization
parameter λ could be evaluated or automatically fitted in a
Bayesian manner. Also, extensive user studies on the effectiveness of visualizing the topics could be conducted to
consider the usefulness of this model in real-world settings.

9. Conclusion
This work lays the foundation for a new class of topic
models based on an admixture of Poisson MRFs that can
model dependencies between words unlike all previous
topic models that assume word independence. Independent
Poisson MRFs are shown to generalize the conditional distributions of LDA, which thus suggests that APM subsumes
the expressive power of LDA and adds significantly greater
modeling power than LDA. In addition to APM, a generalized class of admixture models is defined which opens the
way for admixtures of any parametric distribution. For parameter estimation of this new model, a tractable method
using the approximate posterior is explained. Finally, several experiments give evidence that APM can provide visually appealing and interpretable results as well as subsume
the power of the LDA model. The development of APM
opens up a whole new area of research with many interesting open questions in both theory (e.g. scalability, other
admixtures, hyperparameter choice) and applications (e.g.
visualization, user interaction, document exploration).

Acknowledgments
D. Inouye was supported by the NSF Graduate Research
Fellowship via DGE-1110007. P. Ravikumar acknowledges support from NSF via IIS-1149803 and DMS1264033, and ARO via W911NF-12-1-0390. I. Dhillon acknowledges support from NSF via CCF-1117055.

Admixture of Poisson MRFs

References
Beck, A. and Teboulle, M. A fast iterative shrinkagethresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, January 2009. ISSN 1936-4954.
Bishop, Y., Fienberg, S., and Holland, P. Sampling models for discrete data. In Discrete Multivariate Analysis:
Theory and Practice, chapter 13, pp. 435–456. Springer,
2007.
Blei, D. Probabilistic topic models. Communications of the
ACM, 55(4):77–84, November 2012. ISSN 1053-5888.
Blei, D., Ng, A., and Jordan, M. Latent dirichlet allocation.
JMLR, 3:993–1022, 2003.
Blei, D. M. and Lafferty, J. D. Correlated topic models. In
NIPS, pp. 147–154, 2005.
Blei, D. M. and Lafferty, J. D. Dynamic topic models. In
ICML, pp. 113–120, 2006. ISBN 1595933832.
Chen, Y. and Ye, X. Projection onto a simplex. arXiv
preprint arXiv:1101.6081, pp. 1–7, 2011.
Collins, M., Dasgupta, S., and Schapire, R. E. A generalization of principal component analysis to the exponential family. In NIPS, pp. 617–624, 2001.
Gopalan, P., Hofman, J., and Blei, D. Scalable recommendation with poisson factorization. arXiv preprint
arXiv:1311.1704, 2013.
Hinton, G. and Salakhutdinov, R. Replicated softmax: An
undirected topic model. NIPS, pp. 1607–1614, 2009.
Hofmann, T. Probabilistic latent semantic analysis. In Uncertainty in Artificial Intelligence (UAI), pp. 289–296.
Morgan Kaufmann Publishers Inc., 1999.
Hsieh, C., Sustik, M. A., Dhillon, I. S., Ravikumar, P. K.,
and Poldrack, R. A. BIG & QUIC: Sparse inverse covariance estimation for a million variables. In NIPS, pp.
3165–3173, 2013.
Hu, Y., Boyd-Graber, J., and Satinoff, B. Interactive topic
modeling. In ACL, pp. 248–257, 2011.
Mimno, D., Wallach, H. M., Talley, E., Leenders, M., and
McCallum, A. Optimizing semantic coherence in topic
models. In EMNLP, pp. 262–272, 2011.
Nallapati, R., Ahmed, A., Cohen, W., and Xing, E. Sparse
word graphs: A scalable algorithm for capturing word
correlations in topic models. ICDM, pp. 343–348, 2007.

Newman, D., Noh, Y., Talley, E., Karimi, S., and Baldwin, T. Evaluating topic models for digital libraries.
In ACM/IEEE Joint Conference on Digital Libraries
(JCDL), pp. 215–224, 2010. ISBN 9781450300858.
Nigam, K., Mccallum, A. K., Thrun, S., and Mitchell,
T. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2-3):103–134,
2000.
Parikh, N. and Boyd, S. Proximal algorithms. Foundations
and Trends in Optimization, 1(3):123–231, 2013.
Pritchard, J. K., Stephens, M., and Donnelly, P. Inference
of population structure using multilocus genotype data.
Genetics, 155(2):945–59, June 2000. ISSN 0016-6731.
Ravikumar, P., Wainwright, M. J., and Lafferty, J. D. Highdimensional Ising model selection using l1-regularized
logistic regression. The Annals of Statistics, 38(3):1287–
1319, June 2010. ISSN 0090-5364.
Ravikumar, P., Wainwright, M. J., Raskutti, G., and Yu,
B. High-dimensional covariance estimation by minimizing l1-penalized log-determinant divergence. Electronic
Journal of Statistics, 5:935–980, 2011. ISSN 1935-7524.
Reisinger, J., Waters, A., Silverthorn, B., and Mooney, R. J.
Spherical topic models. In ICML, pp. 903–910, 2010.
Slapin, J. and Proksch, S. A scaling model for estimating
time-series party positions from texts. American Journal
of Political Science, 52(3):705–722, 2008.
Stevens, K. and Kegelmeyer, P. Exploring topic coherence
over many models and many topics. In EMNLP-CoNLL,
pp. 952–961, 2012.
Steyvers, M. and Griffiths, T. Probabilistic topic models.
In Latent Semantic Analysis: A Road to Meaning, pp.
424–440. 2007.
Steyvers, M., Smyth, P., Rosen-Zvi, M., and Griffiths, T.
Probabilistic author-topic models for information discovery. In KDD, pp. 306–315, 2004. ISBN 1581138881.
Wainwright, M. J. and Jordan, M. I. Graphical models,
exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(12):1–305,
2008. ISSN 1935-8237.
Yang, E., Ravkiumar, P., Allen, G. I., and Liu, Z. Graphical models via generalized linear models. In NIPS, pp.
1367–1375, 2012.
Yang, E., Ravikumar, P., Allen, G., and Liu., Z. On poisson
graphical models. In NIPS, pp. 1718–1726, 2013.
Yuan, X. and Zhang, T. Truncated power method for sparse
eigenvalue problems. JMLR, 14:899–925, 2013.

