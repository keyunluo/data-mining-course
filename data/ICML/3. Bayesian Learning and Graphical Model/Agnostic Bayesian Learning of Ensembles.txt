Agnostic Bayesian Learning of Ensembles

Alexandre Lacoste∗
ALEXANDRE . LACOSTE .1@ ULAVAL . CA
Département d’informatique et de génie logiciel, Université Laval, Québec, Canada, G1K-7P4
Hugo Larochelle
HUGO . LAROCHELLE @ USHERBROOKE . CA
Département d’informatique, Université de Sherbrooke, Québec, Canada, J1K-2R1
Mario Marchand
M ARIO .M ARCHAND @ IFT. ULAVAL . CA
Département d’informatique et de génie logiciel, Université Laval, Québec, Canada, G1K-7P4
François Laviolette
F RANCOIS .L AVIOLETTE @ IFT. ULAVAL . CA
Département d’informatique et de génie logiciel, Université Laval, Québec, Canada, G1K-7P4

Abstract
We propose a method for producing ensembles of
predictors based on holdout estimations of their
generalization performances. This approach uses
a prior directly on the performance of predictors
taken from a finite set of candidates and attempts
to infer which one is best. Using Bayesian inference, we can thus obtain a posterior that represents our uncertainty about that choice and construct a weighted ensemble of predictors accordingly. This approach has the advantage of not requiring that the predictors be probabilistic themselves, can deal with arbitrary measures of performance and does not assume that the data was
actually generated from any of the predictors in
the ensemble. Since the problem of finding the
best (as opposed to the true) predictor among a
class is known as agnostic PAC-learning, we refer to our method as agnostic Bayesian learning.
We also propose a method to address the case
where the performance estimate is obtained from
k-fold cross validation. While being efficient and
easily adjustable to any loss function, our experiments confirm that the agnostic Bayes approach
is state of the art compared to common baselines
such as model selection based on k-fold crossvalidation or a learned linear combination of predictor outputs.

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

1. Introduction
When designing a machine learning system that relies on
a trained predictor, one is usually faced with the problem
of choosing this predictor from a finite class of models.
In practice, the class of models might correspond to different learning algorithms or to different choices of hyperparameters for a specific learning algorithm. A common approach to this problem is to estimate the generalization performance of each predictor on a holdout
dataset (through a training/validation set split or using kfold cross-validation) and use the predictor with the best
performance. However, this approach is invariably noisy
and overfitting can become a problem. A more successful procedure is to construct an ensemble of many different learned predictors. Many machine learning contests are
won this way (Guyon et al., 2010). For instance, the winning team of the Netflix’s contest relied on a final predictor trained on the output of the learned models (Bell et al.,
2007). Great care must be taken however to avoid overfitting, e.g. by carefully tuning the predictor’s own regularization hyperparameters. The choice of the final predictor
is likely to influence the end result as well.
At the heart of this selection problem is our inability to
know for sure which predictor is the best among our model
class. One natural way to reason about such uncertainty
would be to formulate it in probabilistic terms. In this paper, we propose to follow this paradigm by formulating priors about the expected performance of each predictor in our
chosen class of models. We then use the observed loss measurements on each held-out example as evidence for updating our posterior over the identity of the best predictor in
the model class. At test time, we can use this posterior to
weight the contribution of each predictor in the ensemble
that performs the final prediction.

Agnostic Bayesian Learning of Ensembles

We explore different ways of expressing priors over predictor performances and discuss how to perform Bayesian
inference. As we will see, this simple paradigm naturally takes into account the correlation between the predictor’s output so as to leverage diversity among the ensemble, which is another desiderata for ensemble learning and
model averaging methods.
Unlike Bayesian model averaging (Hoeting et al., 1999),
our approach does not require that the predictors be themselves probabilistic. It can also deal with arbitrary performance measures. More crucially, this approach does not
assume that the observed data has been generated by a predictor from the model class. In other words, we are not
looking for the predictor that best explains the observed
data, assuming it was generated by a predictor coming from
our model class. Instead, at the centre of our approach, we
want to find the best predictor in terms of a task’s performance measure and among all available predictors, while
reasoning about our uncertainty around this problem in a
Bayesian way.
The non-reliance on the assumption that the true underlying data generating function belongs to our model class is
also at the center of agnostic PAC-learning. For this reason,
we refer to the proposed framework as agnostic Bayesian
learning.
Section 2 formally describes the agnostic Bayes approach.
We then propose a few methods for obtaining a posterior
distribution over a set of predictors. Section 4 presents
an adaptation to k-fold cross-validation estimation of the
losses. Finally, several experimental results are presented
in Section 6.

2. Theoretical Setup
Throughout this paper, we use the inductive learning
paradigm and make the usual assumptions of PAC learning theory (Kearns et al., 1994; Valiant, 1984). Thus, a
task D corresponds to a probability distribution over the
input-output space X × Y. Given a training set S ∼ Dm ,
the objective is to find, among a set H, the best function
h? : X → Y. In general, H could be any set. However,
this work will focus on the case where H is a finite set of
predictors obtained from one or many learning algorithms,
with various hyperparameters. We will refer to a member
of H as an hypothesis.
To assess the quality of an hypothesis, we use a loss function L : Y × Y → R that quantifies the penalty incurred
when h predicts h(x) while the true answer is y. Then, we
can define the risk RD (h) as being the expected loss of h
on task D, i.e. RD (h) def
= E L (h(x), y). Finally, the
x,y∼D

best1 function is simply the one minimizing the risk, i.e.
h? def
= argmin RD (h).
h∈H

Since we do not observe D, it is not generally possible to
find h? with certainty. For this reason, we are interested in
inferring h? while modeling our uncertainty about it, using
a posterior probability distribution p(h?= h|S). Then, after
marginalizing h? , we obtain a probabilistic prediction
X
p(y ?= y|x, S) =
p(h?= h|S)p(y ?= y|x, h),
h∈H
?

where y stands for the prediction made by h? for a given
x. We note that the uncertainty in this prediction solely
comes from our lack of knowledge about h? .
In order to perform a final prediction ŷ for a given x it is
tempting to use the optimal Bayes decision theory
X
ŷ = argmin
p(y o= y|x, S)L (y 0 , y) ,
y 0 ∈Y

y∈Y

o

where y is the random variable corresponding to the observed values of y. However, the contrast between p(y o =
y|x, S) and p(y ? = y|x, S) prevents us from using this approach. To this end, we use:
ŷ = argmax p(y ?= y|x, S),
y∈Y

the most probable answer. This yields the following ensemble method:
X
E ? (x) def
= argmax
p(h? = h|S)I[h(x) = y] (1)
y∈Y

h∈H

Before going further, we first review the usual Bayesian
model averaging approach to highlight the fact that it does
not exactly use p(h? = h|S).
2.1. Standard Bayesian Model Averaging
To address the inductive learning paradigm, a variant of
Bayesian model averaging can be used, where we suppose
that a deterministic function h→ , belonging to H, is at the
origin of the observed relationship between x and y. To
perform inference on h→ , we treat it as a random variable
and assume that the observations in S have been altered by
a noise model2 Q
p(y o = y|x, h). Using the i.i.d. assumpm
tion, p(S|h) = i=1 p(yi |xi , h)p(xi ). Next, by defining a
prior distribution over H, we can perform Bayesian inference to compute p(h→= h|S) ∝ p(S|h)p(h). Finally, after
marginalization of h, we obtain
X
p(y o= y|x, S) =
p(h→= h|S)p(y o= y|x, h),
h∈H
1

The best solution may not be unique.
The noise model could also be inferred. In this work, we use
a fixed noise model.
2

Agnostic Bayesian Learning of Ensembles

which can be used with the optimal Bayes decision theory,
to give the following ensemble decision rule
X
E → (x) def
= argmin
p(y o= y|x, S)L (y 0 , y) . (2)
y 0 ∈Y

y∈Y

This formulation has proven to be very useful. However,
if the true data-generating hypothesis does not belong to
H, the posterior p(h→ = h|S) may not converge to a posterior peaked at the best hypothesis h? , as m → ∞. This
misbehavior has been studied by Grünwald and Langford
(2007) for the zero-one loss scenario. It was shown that
under some reasonable restrictions on the prior, there exists a distribution D where the risk of the Bayes predictor
is significantly higher than RD (h? ).
One way to overcome this inconsistency is to commit to
a noise model that leverages the loss function, such as
p(y o = y|h, x) ∝ e−βL(h(x),y) for some fixed β > 0.
Then, we have that p(h→= h|S) ∝ p(h)e−mβRS (h) , where
RS (h) is the empirical risk measured on S. As m → ∞,
the exponential part of the posterior ensures that any hypothesis not having a risk as low as RD (h? ) will have a
negligible weight. We will examine this ensemble method
to show that it is outperformed by the methods we propose
in this paper.
2.2. Agnostic Bayes
Our main contribution is to propose a method for obtaining
p(h? = h|S), to be used in our ensemble decision E ? (x).
The core idea of our approach is to directly reason about
h? instead of assuming the existence in H of a data generating h→ and trying to infer it. Since the observed losses in
S suffice to distinguish h? from other hypotheses in H, we
do not have to commit to a particular model for the relationship between x and y, and can limit ourselves to modeling
the losses under each hypothesis.
Specifically, we propose to treat the risk rh def
= RD (h) of
each hypothesis h as a random variable, over which we will
be defining a prior distribution. Let lh,i def
= L (h (xi ) , yi )
be the observed loss of hypothesis h for a sample (xi , yi ) ∈
S. We also treat lh,i as random variables, governed by
a conditional distribution p(lh,i |rh ). For example, in the
zero-one loss L(y, y 0 ) = I[y 6= y 0 ] case, a natural choice
would be to treat the observed losses lh,i as Bernoulli trials
of parameter rh . Assuming a beta prior over rh , we could
then perform Bayesian inference in order to reason about
the uncertainty over rh given the losses observed from S.
In the case of ensemble learning where we have multiple competing hypotheses, the losses lh,i are dependent
across the different hypotheses h for the same example
(xi , yi ). Hence, we need to model the losses l i def
=
l1,i , l2,i , . . . , l|H|,i for a given example jointly, given the


joint risk for all hypotheses r def
= r1 , r2 , . . . , r|H| . Section 3 will discuss different joint priors p(r) and observation models p(ll i |r). For now, we just note that from
p(ll i |r), we can derive the likelihood
of the set of losses
Qm
L def
= {ll i }m
i=1 as p(L|r) =
i=1 p(ll i |r) and, combined
with our prior p(r), perform Bayesian inference to obtain
p(r|L) ∝ p(L|r)p(r).
After obtaining p(r|L), we can now compute the posterior
probability that a given hypothesis h is the best hypothesis
h? with the lowest risk among H
P r (∀g ∈ H : rh ≤ rg |L)
=

E
r∼p(·|L)

=

E
r∼p(·|L)

p (rh ≤ rg , ∀g 6= h|r)
I (rh ≤ rg , ∀g 6= h) .

We propose to use this posterior as our ensemble posterior
in Equation (1). Under this model, L is a sufficient statistic
for r and thus for h, i.e. p (h|S) = p (h|L). Hence, to sample from p(h|S), it suffices to sample a joint risk r from
p (r|L) and to search for the hypothesis with the smallest
risk. With repeated sampling, we can then approximately
compute our ensemble decision rule. When Y is continuous, this approximation can affect argmax p(y ? = y|S, x).
y∈Y

To address this issue, we consider a simple Gaussian model
to smooth p(y ? = y|S, x). This
P yields a weighted average
of the predictions: E ? (x) = h∈H p(h? = h|S)h(x).

3. Priors Over the Joint Risk
In this section, we propose a few choices for the prior p(r)
and observation model p(ll i |r). We also discuss how to perform inference for p(r|L) under different assumptions of
the loss function.
3.1. Dirichlet Distribution
We start with a proposal for the specific case of the zeroone loss. As described in Section 2, the observations
lh,i ∈ {0, 1} are correlated and put together in a vector l i ∈ {0, 1}d , where d def
= |H|. We propose to consider the collection of observations {ll i }m
i=1 as coming from
a categorical distribution of N def
= 2d possible states (i.e.
outcomes). Therefore, the counts of observations k def
=
(k1 , k2 , . . . , kN ) ∈ NN come from a multinomial distribution of parameters q and m, where q is the probability
of observing each event and sums to 1. With these assumptions, it is natural to use the Dirichlet distribution of parameter α as the model for the prior over q. The posterior
distribution p (q|k) is then a Dirichlet distribution of parameter α +k. To convert the sample from p(q|L) to a sample from p(r|L), we define the state matrix G ∈ {0, 1}d×N
where the j th column corresponds to the binary representation of j. Then, to obtain a sample from p(r|L), we sample

Agnostic Bayesian Learning of Ensembles

α + k) and use r = Gq. Equivalently, we
q from Dir (α
have p (r|L) = E q∼Dir(α
α+k) I (Gq = r).
Naively sampling from this posterior yields an algorithm
with computational complexity of O(d2d ). However, using a neutral prior of the form α = α
e1N and the stick
breaking representation of the Dirichlet (see Lemma 3.1 of
Sethuraman (1991)), we have the following identity

κm = κ0 + m

θXα + (1 − θ)Xk = Xα +k ,

νm = ν0 + m

α), Xk ∼ Dir(k),
where θ ∼ Beta(e
αN, m), Xα ∼ Dir(α
α + k). Since most values in k are zeros,
Xα +k ∼ Dir(α
samples from Dir(k) can be obtained in O(m). Thus, we
α), which can
are left with the task of sampling from Dir(α
be approximated efficiently using 10· α
eN samples from the
stick breaking process (Sethuraman, 1991). Since α
eN >
m yields too much importance to the prior, one can safely
assume that α
eN ≤ m and obtain a sample from the prior
with computational complexity O(m) .
3.2. Bootstrap Inference
We point out that the Dirichlet posterior presented in Section 3.1 is a generalization of Rubin’s Bayesian bootstrap
(Rubin, 1981) and is equivalent in the limit α
e → 0. Also,
Rubin showed that the Bayesian bootstrap is statistically
tightly related to Efron’s bootstrap (Efron, 1979). For these
reasons, we also consider the bootstrap as a candidate for
a simple and generic method to sample from p(r|L). This
m
is done by sampling with replacement
{ll 0i }i=1 from
Pm a set
m
1 0
{ll i }i=1 . To obtain r, we use rh ← i=1 m lh,i ; ∀h ∈ H.
3.3. t Distribution
In this section, we make the assumption that the variables l i
are observations coming from a multivariate normal distribution of dimensionality |H| def
= d, whose mean parameter
corresponds to the true risk r. While the normal assumption is generally not true, it can be justified from the central
limit theorem. As we will see, experiments in Section 6
show that this assumption works well in practice even with
the zero-one loss function, which is one of the most extreme cases of non Gaussian samples.
Specifically, assuming that p(ll i |r, Λ) is normal, the likelihood of L def
= {ll }m
i=1 is
p (L|r, Λ) ∝



1 Pm
T
m
−2
j=1 (ll j−r) Λ(ll j−r)
2
.
|Λ| e

where N and W are the normal and Wishart distributions
respectively, r0 and T0 are the mean and covariance prior,
while κ0 and ν0 are parameters related to the confidence we
have in r0 and T0 respectively (with restrictions κ0 > 0 and
ν0 > d − 1). Thanks to conjugacy, after observing L, we
have that the posterior p (r, Λ|L) is also a normal-Wishart
distribution of parameters κm , νm , rm and Tm as follows:

(3)

We want to favor the use of priors over r and covariance
matrix Λ−1 such that the posterior p(r, Λ|L) is tractable.
This can be achieved using the normal-Wishart distribution
(DeGroot, 2005, p. 178)

 

−1
p (r, Λ) = N rr0 , (κ0 Λ)
W (Λ|T0 , ν0 ) ,

rm =

κ0 r0 + mll
κm

(4)


T
κ0 
r0 − l r0 − l
Tm = T0 + mS + m
κm
P
Pm
m
def 1
1
T
where l def
= m
i=1 l i and S = m i=1 (ll i −ll )(ll i −ll ) .
Since our goal is to obtain a posterior distribution over r
only, we have to marginalize out Λ from p (r, Λ|L). By
doing so, we obtain the multivariate Student’s t distribution
with νe def
= νm −d+1 degrees of freedom (DeGroot, 2005, p.
179)
 


p (r|L) = t re
ν , rm , κTmmνe .
(5)
Samples from this multivariate t-distribution are
 done by

sampling from the normal distribution z ∼ N 0, κTmmνe ,

sampling from theqchi-squared distribution ξ ∼ χ2 (e
ν ) and
ν
e
computing rm +z ξ . This gives an overall computational

complexity of O d2 (m + k + d) to obtain k samples.
For setting the parameters r0 , T0 , κ0 and ν0 of the prior, we
chose values that were as neutral as possible and numerically stable: r0 = 0.5 × 1d , T0 = 0.25 × I, κ0 = 1 and
ν0 = d.
3.4. Posterior Behavior with Correlated Hypotheses
One advantage of the agnostic Bayes posterior for constructing an ensemble is that it naturally encourages diversity among the predictors, even in the presence of correlation between the predictors in H. We illustrate this with
a simple example, shown in Table 1, comparing an agnostic Bayes ensemble with bootstrap inference (Eb? ) and
a Bayesian model averaging ensemble with a loss-based
noise model and flat prior over the hypotheses (E → ). Table 1(top) illustrates the case of three equally good but different hypotheses, based on three observed losses for each
predictor. We see that both Eb? and E → equally weight the
three hypotheses, as expected.
Now, in Table 1(bottom), we include into H an additional
hypothesis h4 , which is identical to h3 . We then observe
that Eb? naturally maintains diversity within the ensemble,
by reducing the mass of the identical hypotheses h3 and

Agnostic Bayesian Learning of Ensembles
Table 1. Illustration of the posteriors in an agnostic Bayes ensemble (Eb? ) and in Bayesian model averaging (E → ). top: Uncorrelated predictors. bottom: Addition of a correlated predictor.

h1
h2
h3

l1
1
0
0

l2
0
1
0

l3
0
0
1

p(h? |S)
0.33
0.33
0.33

p(h→ |S)
0.33
0.33
0.33

p(h? |S)
0.31
0.31
0.19
0.19

p(h→ |S)
0.25
0.25
0.25
0.25

↓
h1
h2
h3
h4

l1
1
0
0
0

l2
0
1
0
0

l3
0
0
1
1

h4 , compared to E → which still weights all hypotheses
equally. Diversity is usually considered to be beneficial
when constructing an ensemble of predictors (Roy et al.,
2011), motivating the use of agnostic Bayes for this task.

4. Model Averaging for Trained Predictors
As mentioned in Section 2, one natural application for
the inference of the best hypothesis is model averaging
of trained predictors. Namely, let Aγ be a learning algorithm with a hyperparameter configuration γ ∈ Γ and let
hγ = Aγ (T ) represent the classifier obtained using a training set T ∼ Dn , disjoint from S. The set H contains all
classifiers obtained from each γ ∈ Γ, when Aγ is trained
on T , i.e. H def
= {hγ |γ ∈ Γ}. Finally, to obtain the posterior p(h?γ = hγ |S), we rely on the set S. Experiments in
Section 6 will show that this approach significantly outperforms the usual method of selecting the hypothesis minimizing RS (hγ ).
Unfortunately, this scenario requires that the hypotheses hγ
be trained on a set of data T separate from S, in a training/validation split fashion, wasting an opportunity to measure the hypotheses performance on T as well. Our next
step is thus to adapt our agnostic Bayes approach to the kfold cross-validation scenario, which more fully uses the
available data.
4.1. Adapting to k-fold Cross-Validation
Let {V1 , V2 , . . . , Vk } be a partition of S, and let hγ,j def
=
Aγ (S \ Vj ). Now, denote the loss of model γ on the
example (xi , yi ) as e
lγ,i def
= L (hγ,ji (xi ), yi ), where ji is
the unique
index
j
such

that (xi , yi ) ∈ Vj . Finally, let
el i def
e
e
e
= l1,i , l2,i , . . . , l|Γ|,i . Unlike {ll }m , it is well known
i=1

that the set of k-fold generated losses {el }m
i=1 contains dependencies across the different examples that are induced

by the k-fold procedure (Bengio and Grandvalet, 2004).
Since the posteriors described in Section 3 relied on independence across examples, we cannot simply ignore the
dependencies induced within this process and must adapt
our approach.
Specifically, we make the simplifying assumption that
these dependencies only affect the effective number of samples. Intuitively, since samples are correlated, there may
not be as many as it seems and the estimation of p(r|L)
may be overly confident. We thus propose to add an extra
parameter ρ, the effective sample size ratio, to compensate
for these dependencies. While this parameter requires calibration, we describe in Section 4.2 an efficient method for
automatically adjusting its value.
To include the effective sample size ratio in the methods
described in Section 3, we will effectively act as if the collection {ll }m
i=1 had been generated by artificially replicating
a set of m original samples b times each, to give a new set
of bm0 def
= m samples. Thus, the effective number of samples would be m0 = m/b. Now, supposing that we know
ρ = m0 /m, we want to adapt the posterior’s parameters
in such a way that the posterior’s distribution remains the
same, on average, as before the “corruption”.
Bootstrap: This is probably the simplest method to adapt.
Out of the m observed events, we sample with replacement
m0 events instead, where m0 = dρme.
Dirichlet: In this case, each observed event is made to
count for ρ instead of 1. After observing m events, the
0
) will now sum to
vector of counts k0 def
= (k10 , k20 , . . . , kN
m0 instead of m.
t-Distribution: In this case, we adapt the quantities described in Equation (4) as follows: νm0 = ν0 + m0 ,
0
l
and Tm0 = T0 + m0 S +
νm0 = ν0 + m0 , rm0 = κ0 rκ0 +m
m0


T
m0 κκ00 r0 − l r0 − l .
m

4.2. Tuning Parameters
To adjust ρ, we treat it as a parameter and fit it by
optimizing the resulting ensemble’s performance on S,
thereby measuring how well the ensemble’s weighting posterior can predict each label yi in S from the hypotheses (h1,ji (xi ), h2,ji (xi ), .., h|Γ|,ji (xi )). We’ve found this
to work well in practice. This procedure is also akin
to methods that learn a parameterized linear combination
def
of predictors by training on generated

	mexamples S̃ =
(h1,ji (xi ), h2,ji (xi ), .., h|Γ|,ji (xi )), yi i=1 . The best ρ
from a set of 20 values equally spaced from 0.1 to 0.8 is
used. We use a similar procedure to tune the prior parameter α
e of the ensemble based on a Dirichlet prior.

Agnostic Bayesian Learning of Ensembles

5. Related Work
To overcome some mentioned weaknesses of Bayesian
model averaging (such as the reliance on the existence of
a single data-generating hypothesis belonging to H), Kim
and Ghahramani (2012) proposed an alternative method for
Bayesian combination of classifiers. They suppose that, for
a given x, the true label is at the origin of the behavior of
each individual classifier. Therefore, by modeling the dependencies between each classifier on a validation set, they
can perform inference of the original label. Unfortunately,
it relies on a combination of MCMC and rejection sampling
methods and the computational complexity of certain dependency models grows exponentially with |H|. Thus, this
approach is viable only for combining a small set of classifiers. It also only tackles classification tasks and doesn’t
take into account the loss related to the task at hand, as we
do here.
Alternatively, ensemble pruning is an important approach
to ensemble methods. Zhang et al. (2006) used semidefinite programming for solving a heuristic based on the covariance of the predictors. Interestingly, the core of their
idea is highly related to the covariance matrix used in our
t-distribution approach. Unfortunately, they can only address an approximation of their heuristic and it is limited to
the zero-one loss.

6. Experiments
We performed experiments to assess the performance of the
agnostic Bayes ensemble approach and compared with a
few commonly used methods:
ArgMin (AMin): This method represents the common
approach of selecting the model hγ with the best estimated
Pm
1
holdout risk rγ def
= m
i=1 lγ,i . When the minimum is not
unique, we select one at random.
SoftMin (SMin): We use the Gibbs distribution with parameter β to produce a posterior distribution over the collection of hγ from rγ . i.e., p(hγ |S) ∝ e−βrγ and β is selected with the method described in Section 4.2. This represents the alternative Bayesian model averaging approach
described in Section 2.1.
?
?
Eb?, ED
, EB
, Et? : The different agnostic Bayes ensemble
decision methods based on Equation (1) and using posterior
inference based on the bootstrap, the Dirichlet distribution,
the Bayesian bootstrap and the t-distribution respectively.
Effective sample size ratio ρ and Dirichlet prior parameter α
e are adjusted according to Section 4.2, while the tdistribution prior parameters are fixed to the values specified in Section 3.3. We use 1000 samples from p(r|L) to
estimate p(h|S).

MetaSVM (MSVM): We use MetaSVM to represent the

state of the art approach i.e., methods that learn a linear
model over the set of models as a final predictor. This is
done by using the collection S̃ described in Section 4.2 as
a training set for the linear SVM. Traditional cross validation is used to select the best soft margin parameter over 20
candidates values ranging from 10−3 to 100 on a logarithmic scale.
Meta Ridge Regression (MRR): When performing experiments on regression tasks, we use ridge regression as
a substitution for MetaSVM. The regularization parameter
is selected by the leave one out method over 30 candidates
ranging from 10−4 to 104 on a logarithmic scale.
6.1. Comparing Learning Algorithms On Multiple
Datasets
The different model selection methods presented in the previous section are generic and are meant to work across different tasks. It is thus crucial that we test them on several datasets. For that, we have to rely on methods that
do not assume commensurability across tasks, such as the
sign test, the Wilcoxon signed rank test (WSR) (Demšar,
2006) and the Poisson binomial test (PB test) (Lacoste
et al., 2012). The PB test is a Bayesian analogue of the
sign test meant for comparing learning algorithms on a collection of tasks, called a context. More precisely, it provides a probabilistic answer to the question “Does algorithm A have a higher probability of producing a better
predictor than algorithm B in the given context?”, denoted
by p (A  B|W), where W represents the context.
To build a substantial collection of datasets, we used the
AYSU collection (Ulaş et al., 2009) coming from the
UCI and the Delve repositories and we added the MNIST
dataset. We also converted the multiclass datasets to binary
classification by either merging classes or selecting pairs
of classes. The resulting context contains 38 datasets. We
have also collected 22 regression datasets from the Louis
Torgo collection.3 to perform experiments using different
loss functions.
The set Γ of models used in this experiment is a combination of SVMs, Artificial Neural Networks (ANN), random
forests, extra randomized trees (Geurts et al., 2006) and
gradient tree boosting (Friedman, 2001) with several variants of hyperparameters. Considering the algorithm name
as a hyperparameter and a grid search for each algorithm,
this yields a set of 692 hyperparameter configurations, all
of which are evaluated using 10 folds cross validation. For
the experiments on regression datasets, we used a combination of Kernel Ridge Regression (KRR), Support Vec3

These datasets were obtained from the following source :
http://www.dcc.fc.up.pt/˜ltorgo/Regression/
DataSets.html

Agnostic Bayesian Learning of Ensembles

tor Regression (SVR), random forests, extra randomized
trees and gradient boosted regression, yielding a total of
480 hyperparameter configurations. Except for a custom
implementation of ANN and KRR, we used scikit-learn
(Pedregosa et al., 2011) for all other implementations. For
more details on the choice of hyperparameters, we refer the
reader to the supplementary material.
6.2. Result Table Notation
Each conducted experiment compares the generalization
performances of a set of M algorithms on a set of N
datasets. In order to evaluate if the observed differences are
statistically significant, we use the pairwise PB test where
each cell of the table represents p (row  column). Since
the table has a form of symmetry, we have grayed out redundant information and removed the first column. In addition, we also highlight in blue the results having p-values
lower than 0.1 according to the one tail sign test. In general,
we have observed a strong correlation between the p-values
of the sign test and the probabilities obtained from the PB
test. Note however that their values may differ and a highlighted cell does not imply a strong PB probability, nor the
converse. Finally, we added a column to each table which
reports the expected rank of each algorithm across the collection of datasets. The rank of predictor hi = Ai (Sj ) on
test set Tj is defined as
Rankhi ,Tj def
=

M
X


I RTj (hl ) ≤ RTj (hi ) .
l=1

Then, the expected rank P
is obtained from the empirical avN
erage E [Rank]hi def
= N1 j=1 Rankhi ,Tj .
6.3. Comparison of Ensemble Decision Methods on
Classification Tasks
Our first experiment compares the different methods and
baselines in the setting where the hypotheses have been
trained and validated on a single split of the dataset. In this
scenario, the training data generates the set of hypotheses
while the validation data provides observations for building an ensemble. Finally, a testing set is used to report the
performances. The effective sample size ratio is fixed to 1
in this scenario.
From Table 2, there are no significant differences between
our methods except for a slight reduction in generalization
?
?
performances for EB
, which corresponds to ED
with α
e
fixed to 0. In this experiment, the only adjusted parameter
?
is α
e in the method ED
. This may explain why it is ranked
first according to the expected rank metric. To simplify the
result tables, further evaluations only includes Eb? and Et? .
Table 3 exhibits a clear conclusion : The agnostic Bayes
ensemble generalizes better than AMin. Next, when com-

Table 2. Comparison of the four proposed agnostic model averaging methods, in the single training/validation split experiment
(refer to Section 6.2 for notation).
?
ED
Et?
Eb?
?
EB

?
ED
0.500
0.491
0.476
0.348

Et?
0.509
0.500
0.459
0.338

Eb?
0.524
0.541
0.500
0.360

?
EB
0.652
0.662
0.640
0.500

E[rank]
2.43 /4
2.43 /4
2.46 /4
2.67 /4

paring against MSVM and Softmin, while the results are
note statistically significant, the expected rank is in favor of
both agnostic Bayes ensembles. Also, we note that MSVM
is not significantly better than AMin.
Table 3. Comparison with the baseline models in the single training/validation split experiment (refer to Section 6.2 for notation).
Et?
Eb?
MSvm
SMin
AMin

Eb?
0.541
0.500
0.408
0.237
0.095

MSvm
0.613
0.592
0.500
0.377
0.211

SMin
0.787
0.763
0.623
0.500
0.241

AMin
0.911
0.905
0.789
0.759
0.500

E[rank]
2.63 /5
2.66 /5
2.92 /5
3.19 /5
3.57 /5

It is well known that k-fold cross-validation provides a better estimate of the generalization performance of a learning
algorithm than a single training/validation fold experiment.
We thus performed another comparison for this setting. In
this scenario, the agnostic Bayes method must now take
into account the effective sample size ratio, as described
in Section 4.1. Selected values ranges from 0.1 to 1 and
were mainly concentrated between 0.3 and 0.6. The results
are expressed in Table 4 and are similar to that of Table 3.
Again, agnostic Bayes is significantly better than Argmin
while MSVM is not.
Table 4. Comparison with the baseline models in the crossvalidation experiment (refer to Section 6.2 for notation).
Eb?
Et?
MSvm
SMin
AMin

Et?
0.507
0.500
0.422
0.280
0.160

MSvm
0.575
0.578
0.500
0.423
0.275

SMin
0.707
0.720
0.577
0.500
0.318

AMin
0.840
0.840
0.725
0.682
0.500

E[rank]
2.70 /5
2.75 /5
2.95 /5
3.12 /5
3.46 /5

6.4. Changing the Loss Function
The results from the last section clearly demonstrate the
advantage of mixing models over selecting a single one.
While the agnostic Bayes methods outperform the baselines, we saw that simply using a linear learning algorithm
also exhibits good performances. But what happens when
the loss function changes? For example, we cannot use

Agnostic Bayesian Learning of Ensembles

MetaSVM for combining models on a regression task. We
can adapt and use ridge regression but, since it minimizes
the quadratic loss, it may not perform well if our task
is to minimize the expected absolute difference loss (i.e.,
L(y, y 0 ) = |y − y 0 |). In other words, to perform a linear
combination of models, we have to redesign the learning
algorithm for every loss functions. Moreover, some loss
functions yield a non-convex optimization problem which
requires some form of approximation, e.g., SVM uses the
hinge loss in place of the zero-one loss. In contrast, the proposed agnostic Bayes approach is designed to work with
any loss function.
Table 5. Comparison with the baseline models on regression tasks
for the quadratic loss function (refer to Section 6.2 for notation).
Et?
MRR SMin AMin E[rank]
?
Eb 0.839 0.547 0.929 0.992
2.22 /5
Et? 0.500 0.468 0.793 0.986
2.64 /5
MRR 0.532 0.500 0.554 0.809
2.88 /5
SMin 0.207 0.446 0.500 0.992
3.02 /5
AMin 0.014 0.191 0.008 0.500
4.23 /5

Table 6. Comparison with the baseline models on regression tasks
for the absolute loss function (refer to Section 6.2 for notation).
Eb?
Et?
SMin
MRR
AMin

Et?
0.735
0.500
0.068
0.179
0.005

SMin
0.953
0.932
0.500
0.231
0.018

MRR
0.859
0.821
0.769
0.500
0.515

AMin
0.995
0.995
0.982
0.485
0.500

E[rank]
2.10 /5
2.37 /5
3.06 /5
3.39 /5
4.08 /5

To outline the independence to the loss function of the agnostic Bayes methods, we performed experiments on regression tasks using both the quadratic loss and the absolute difference loss. We compared against the same baseline methods except for MetaSVM which was replaced by
meta ridge regression (MRR) and its regularization parameter was selected by minimizing the appropriate loss function during cross validation. Table 5 presents the results
obtained when using the quadratic loss function. While we
worked with a totally different collection of datasets, the
conclusions that follow from this experiment are surprisingly similar to the previous one. In this case, AMin is
far down in ranking and the statistical significance of the
observed differences are even stronger. Also, MRR is still
performing relatively well.
Now, let us see what happens when we change the loss
function to the absolute difference loss. Table 6 clearly
shows an important degradation of MRR while the relative
performances of the other methods are almost unchanged.
In addition, the agnostic Bayes approach is now significantly better than the linear model. This clearly shows
the importance of optimizing the appropriate loss function.

Thus, justifying the usage of the agnostic Bayes ensemble.

7. Conclusion
We proposed the agnostic Bayes framework, which can
be used to tackle the ubiquitous problem of model selection. This framework’s central idea is to model the relationship between the hypotheses risks and observed empirical
losses, without relying on assumptions about the true datagenerating model. For one, this idea provides a new way
of reasoning about machine learning problems. Also, the
application to model selection has several desirable characteristics.
Generalization: The generalization performance of the
agnostic Bayes ensemble is significantly better than just selecting the model minimizing the empirical expected loss.
Also, our expected rank is systematically higher than any
other evaluated methods on all experiments.
Flexibility: While most existing model selection algorithms is limited to a particular loss function, the agnostic
Bayes ensemble can be used with any loss function. Also,
our experiments showed how optimizing with the wrong
loss function can be detrimental.
Speed: The bootstrap algorithm is simple to implement
and has a linear computational complexity in the size of the
dataset. When measuring the learning speed, we observed
that the bootstrap algorithm can be several thousand times
faster than MetaSVM.

Acknowledgement
Thanks to Calcul Québec for providing support and access
to Colosse’s high performance computer grid. This work
was supported by NSERC Discovery Grants 122405 (M.
M.) and 262067 (F. L.).

Agnostic Bayesian Learning of Ensembles

References
Isabelle Guyon, Amir Saffari, Gideon Dror, and
Gavin Cawley.
Model selection: Beyond the
bayesian/frequentist divide. The Journal of Machine
Learning Research, 11:61–87, 2010.

Alexandre Lacoste, François Laviolette, and Mario Marchand. Bayesian comparison of machine learning algorithms on single and multiple datasets. Journal of Machine Learning Research - Proceedings Track, 22:665–
675, 2012.

Robert M Bell, Yehuda Koren, and Chris Volinsky. The
bellkor solution to the netflix prize. KorBell Team’s Report to Netflix, 2007.

Aydın Ulaş, Murat Semerci, Olcay Taner Yıldız, and Ethem
Alpaydın. Incremental construction of classifier and
discriminant ensembles. Information Sciences, 179(9):
1298–1318, April 2009.

Jennifer A Hoeting, David Madigan, Adrian E Raftery, and
Chris T Volinsky. Bayesian model averaging: a tutorial.
Statistical science, pages 382–401, 1999.

Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. Machine learning, 63(1):3–
42, 2006.

M.J. Kearns, R.E. Schapire, and L.M. Sellie. Toward efficient agnostic learning. Machine Learning, 17(2):115–
141, 1994.

Jerome H Friedman. Greedy function approximation: a
gradient boosting machine. Annals of Statistics, pages
1189–1232, 2001.

Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort,
Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent
Dubourg, et al. Scikit-learn: Machine learning in python.
The Journal of Machine Learning Research, 12:2825–
2830, 2011.

Peter Grünwald and John Langford. Suboptimal behavior
of bayes and mdl in classification under misspecification.
Machine Learning, 66(2-3):119–149, 2007.
Jayaram Sethuraman. A constructive definition of dirichlet
priors. Technical report, DTIC Document, 1991.
D.B. Rubin. The bayesian bootstrap. The annals of statistics, 9(1):130–134, 1981.
B. Efron. Bootstrap methods: another look at the jackknife.
The annals of Statistics, 7(1):1–26, 1979.
M.H. DeGroot. Optimal statistical decisions, volume 82.
Wiley-interscience, 2005.
Jean-Francis Roy, François Laviolette, and Mario Marchand. From pac-bayes bounds to quadratic programs for
majority votes. In ICML, pages 649–656, 2011.
Y. Bengio and Y. Grandvalet. No unbiased estimator of
the variance of k-fold cross-validation. The Journal of
Machine Learning Research, 5:1089–1105, 2004.
Hyun-Chul Kim and Zoubin Ghahramani. Bayesian classifier combination. Journal of Machine Learning Research
- Proceedings Track, 22:619–627, 2012.
Yi Zhang, Samuel Burer, and W Nick Street. Ensemble
pruning via semi-definite programming. The Journal of
Machine Learning Research, 7:1315–1338, 2006.
Janez Demšar. Statistical comparisons of classifiers over
multiple data sets. The Journal of Machine Learning
Research, 7:1–30, 2006.

