Predictive Entropy Search for Bayesian
Optimization with Unknown Constraints

José Miguel Hernández-Lobato1
Harvard University, Cambridge, MA 02138 USA

JMH @ SEAS . HARVARD . EDU

Michael A. Gelbart1
Harvard University, Cambridge, MA 02138 USA

MGELBART @ SEAS . HARVARD . EDU

Matthew W. Hoffman
University of Cambridge, Cambridge, CB2 1PZ, UK

MWH 30@ CAM . AC . UK

Ryan P. Adams
Harvard University, Cambridge, MA 02138 USA

RPA @ SEAS . HARVARD . EDU

Zoubin Ghahramani
University of Cambridge, Cambridge, CB2 1PZ, UK

ZOUBIN @ ENG . CAM . AC . UK

Abstract
Unknown constraints arise in many types of expensive black-box optimization problems. Several methods have been proposed recently for
performing Bayesian optimization with constraints, based on the expected improvement (EI)
heuristic. However, EI can lead to pathologies
when used with constraints. For example, in the
case of decoupled constraints—i.e., when one
can independently evaluate the objective or the
constraints—EI can encounter a pathology that
prevents exploration. Additionally, computing
EI requires a current best solution, which may
not exist if none of the data collected so far satisfy the constraints. By contrast, informationbased approaches do not suffer from these failure modes. In this paper, we present a new
information-based method called Predictive Entropy Search with Constraints (PESC). We analyze the performance of PESC and show that it
compares favorably to EI-based approaches on
synthetic and benchmark problems, as well as
several real-world examples. We demonstrate
that PESC is an effective algorithm that provides
a promising direction towards a unified solution
for constrained Bayesian optimization.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

1. Introduction
We are interested in finding the global minimum x? of an
objective function f (x) over some bounded domain, typically X ⊂ Rd , subject to the non-negativity of a series of
constraint functions c1 , . . . , cK . This can be formalized as
min f (x) s.t.

x∈X

c1 (x) ≥ 0, . . . , cK (x) ≥ 0 .

(1)

However, f and c1 , . . . , cK are unknown and can only
be evaluated pointwise via expensive queries to blackboxes that provide noise-corrupted evaluations of f and
c1 , . . . , cK . We assume that f and each of the constraints ck are defined over the entire space X . We seek
to find a solution to (1) with as few queries as possible.
Bayesian optimization (Mockus et al., 1978) methods approach this type of problem by building a Bayesian model
of the unknown objective function and/or constraints, using this model to compute an acquisition function that represents how useful each input x is thought to be as a next
evaluation, and then maximizing this acquisition function
to select a suggestion for function evaluation.
In this we work we extend Predictive Entropy Search (PES)
(Hernández-Lobato et al., 2014) to solve (1), an approach
that we call Predictive Entropy Search with Constraints
(PESC). PESC is an acquisition function that approximates
the expected information gain about the value of the constrained minimizer x? . As we will show below, PESC is
effective in practice and can be applied to a much wider
variety of constrained problems than existing methods.
1

Authors contributed equally.

Predictive Entropy Search for Bayesian Optimization with Unknown Constraints

2. Related Work and Challenges

2.3. Integrated expected conditional improvement

Most previous approaches to Bayesian optimization with
unknown constraints are variants of expected improvement
(EI) (Mockus et al., 1978; Jones et al., 1998). EI measures
the expected amount by which observing at x leads to improvement over the current best value or incumbent η:
Z
αEI (x|η, D) = max(0, f (x) − η)p(f (x)|D) df (x) , (2)

Gramacy & Lee (2011) propose an acquisition function
based on the integrated expected conditional improvement
(IECI), which is given by
Z
αIECI (x) = [αEI (x0 ) − αEI (x0 |x)] h(x0 )dx0 ,
(4)

where D is the collected data.
2.1. Expected improvement with constraints
One way to use EI with constraints works by discounting EI
by the posterior probability of a constraint violation. The
resulting acquisition function, which we call expected improvement with constraints (EIC), is given by
αEIC (x) = αEI (x|η, Df )

K
Y

p(ck (x) ≥ 0|Dk ),

(3)

where αEI (x0 ) is the expected improvement at x0
and αEI (x|x0 ) is the expected improvement at x0 when the
objective has been evaluated at x, but without knowing the
value obtained. The IECI at x is the expected reduction in
improvement at x0 under the density h(x0 ) caused by observing the objective at that location, where h(x0 ) is the
probability of all the constraints being satisfied at x0 . Gelbart et al. (2014) compare IECI with EIC for optimizing
the hyper-parameters of a topic model with constraints on
the entropy of the per-topic word distribution and show that
EIC outperforms IECI for this problem.
2.4. Expected volume reduction

k=1

where Df is the set of objective function observations and
Dk is the set of observations for constraint k. Initially proposed by Schonlau et al. (1998), EIC has recently been
independently developed in Snoek (2013); Gelbart et al.
(2014); Gardner et al. (2014). In the constrained case, η
is the smallest value of the posterior mean of f such that all
the constraints are satisfied at the corresponding location.
2.2. Augmented Lagrangian
Gramacy et al. (2014) propose a combination of the
expected improvement heuristic and the augmented Lagrangian (AL) optimization framework for constrained
blackbox optimization. AL methods are a class of algorithms for constrained nonlinear optimization that work by
iteratively optimizing the unconstrained AL:

K 
X
1
min(0, ck (x))2 − λk ck (x)
LA (x|λ, p) = f (x) +
2p
k=1

where p > 0 is a penalty parameter and λ ≥ 0 is an approximate Lagrange multiplier, both of which are updated
at each iteration.
The method proposed by Gramacy et al. (2014) uses
Bayesian optimization with EI to solve the unconstrained
inner loop of the augmented Lagrangian formulation. AL
is limited by requiring noiseless constraints so that p and
λ can be updated at each iteration. In section 4.3 we show
that PESC and EIC perform better than AL on the synthetic
benchmark problem considered in Gramacy et al. (2014),
even when the AL method has access to the true objective
function and PESC and EIC do not.

Picheny (2014) proposes to sequentially explore the location that yields that largest the expected volume reduction
(EVR) of the feasible region below the best feasible objective value η found so far. This quantity is given by integrating the product of the probability of improvement and the
probability of feasibility. That is,
Z
αEVR (x) = − p[f (x0 ) ≤ min(η, f (x))]h(x0 )dx0 , (5)
where, as in IECI, h(x0 ) is the probability that the constraints are satisfied at x0 . This step-wise uncertainty reduction approach is similar to PESC in that both methods
work by reducing a specific type of uncertainty measure
(entropy for PESC and expected volume for EVR).
2.5. Challenges
EI-based methods for constrained optimization have several issues. First, when no point in the search space is
feasible under the above definition, η does not exist and
the EI cannot be computed. This issue affects EIC, IECI,
and EVR. To address this issue, Gelbart et al. (2014) modify EIC to ignore the factor EI(x|η, Df ) in (3) and only
consider the posterior probability of the constraints being
satisfied when η is not defined. The resulting acquisition
function focuses only on searching for a feasible location
and ignores learning about the objective f .
Furthermore, Gelbart et al. (2014) identify a pathology with
EIC when one is able to separately evaluate the objective
or the constraints, i.e., the decoupled case. The best solution x? must satisfy a conjunction of low objective value
and high (non-negative) constraint values. By only evaluating the objective or a single constraint, this conjunction

Predictive Entropy Search for Bayesian Optimization with Unknown Constraints

cannot be satisfied by a single observation under a myopic
search policy. Thus, the new observed x cannot become the
new incumbent as a result of a decoupled observation and
the expected improvement is zero. Therefore standard EIC
fails in the decoupled setting. Gelbart et al. (2014) circumvent this pathology by treating decoupling as a special case
and using a two-stage acquisition function: first, x is chosen with EIC, and then, given x, the task (whether to evaluate the objective or one of the constraints) is chosen with
the method in Villemonteix et al. (2009). This approach
does not take full advantage of the available information
in the way a joint selection of x and the task would. Like
EIC, the methods AL, IECI, and EVR are also not easily
extended to the decoupled setting.
In addition to this difficulties, EVR and IECI are limited
by having to compute the integrals in (4) and (5) over the
entire domain, which is done numerically over a grid on x0
(Gramacy & Lee, 2011; Picheny, 2014). The resulting acquisition function must then be globally optimized, which
also requires a grid on x. This nesting of grid operations
limits the application of this method to small d.
Our new method, PESC, does not suffer from these
pathologies. First, the PESC acquisition function does not
depend on the current best feasible solution, so it can operate coherently even when there is not yet a feasible solution. Second, PESC naturally separates the contribution of
each task (objective or constraint) in its acquisition function. As a result, no pathology arises in the decoupled case
and, thus, no ad hoc modifications to the acquisition function are required. Third, likewise EVR and IECI, PESC
also involves computing a difficult integral (over the posterior on x? ). However, this can be done efficiently using the
sampling approach described in Hernández-Lobato et al.
(2014). Furthermore, in addition to its increased generality, our experiments show that PESC performs favorably
when compared to EIC and AL even in the basic setting of
joint evaluations to which these methods are most suited.

3. Predictive entropy search with constraints
We seek to maximize information about the location x? , the
constrained global minimum, whose posterior distribution
is p(x? |D0 , . . . , DK ). We assume that f and c1 , . . . , cK
follow independent Gaussian process (GP) priors (see, e.g.,
Rasmussen & Williams, 2006) and that observation noise
is i.i.d. Gaussian with zero mean. GPs are widely-used
probabilistic models for Bayesian nonparametric regression which provide a flexible framework for working with
unknown response surfaces.
In the coupled setting we will let D = {(xn , yn )}n≤N denote all the observations up to step N , where yn is a vector collecting the objective and constraint observations at

step n. The next query xN +1 can then be defined as that
which maximizes the expected reduction in the differential
entropy H[·] of the posterior on x? . We can write the PESC
acquisition function as
α(x) = H [x? |D] − Ey {H [x? |D ∪ (x, y)]}

(6)

where the expectation is taken with respect to the posterior
distribution on the noisy evaluations of f and c1 , . . . , cK
at x, that is, p(y|D, x).
The exact computation of the above expression is infeasible in practice. Instead, we follow Houlsby et al. (2012);
Hernández-Lobato et al. (2014) and take advantage of the
symmetry of mutual information, rewriting this acquisition
function as the mutual information between y and x? given
the collected data D. That is,
α(x) = H [y|D, x] − Ex? {H [y|D, x, x? ]}

(7)

where the expectation is now with respect to the posterior
p(x? |D) and where p(y|D, x, x? ) is the posterior predictive distribution for objective and constraint values given
past data and the location of the global solution to the constrained optimization problem x? . We call p(y|D, x, x? )
the conditioned predictive distribution (CPD).
The first term on the right-hand side of (7) is straightforward to compute: it is the entropy of a product of independent Gaussians, which is given by
H(y|D, x) = log vf +

K
X

log vk +

k=1

K +1
log(2πe) , (8)
2

where vf and vk are the predictive variances of the objective and constraints, respectively. However, the second
term in the right-hand side of (7) has to be approximated.
For this, we first approximate the expectation by averaging
over samples of x? approximately drawn from p(x? |D). To
sample x? , we first approximately draw f and c1 , . . . , cK
from their GP posteriors using a finite parameterization of
these functions. Then we solve a constrained optimization problem using the sampled functions to yield a sample
of x? . This optimization approach is an extension of the
approach described in more detail by Hernández-Lobato
et al. (2014), extended to the constrained setting. For each
value of x? generated by this procedure, we approximate
the CPD p(y|D, x, x? ) as described in the next section.
3.1. Approximating the CPD
Let z = [f (x), c1 (x), . . . , cK (x)]T denote the concatenated vector of the noise-free objective and constraint values at x. We can approximate the CPD by first approximating the posterior predictive distribution of z conditioned on D, x, and x? , which we call the noise free CPD

Predictive Entropy Search for Bayesian Optimization with Unknown Constraints

(NFCPD), and then convolving that approximation with ad2
ditive Gaussian noise of variance σ02 , . . . , σK
.
We first consider the distribution p(x? | f, c1 , . . . , cK ). The
variable x? is in fact a deterministic function of the latent
functions f, c1 , . . . , cK : in particular, x? is the global minimizer if and only if (i) all constraints are satisfied at x? and
(ii) f (x? ) is the smallest feasible value in the domain. We
can informally translate these deterministic conditions into
a conditional probability:
"
p(x? | f, c1 , . . . , cK ) =

K
Y

#
Θ [ck (x? )]

Y

Ψ(x0 ) ,

(9)

x0 ∈X

k=1

where Ψ(x ) is defined as
!


Θ ck (x0 ) Θ[f (x0 ) − f (x? )] +

1−

k=1

K
Y

and the symbol Θ denotes the Heaviside step function with
the convention that Θ(0) = 1. The first product in (9) encodes condition (i) and the infinite product over Ψ(x0 ) encodes condition (ii). Note that Ψ(x0 ) also depends on x?
and f, c1 , . . . , cK ; we use the notation Ψ(x0 ) for brevity.
Because z is simply a vector containing the values of
f, c1 , . . . , cK at x, z is also a deterministic function of
f, c1 , . . . , cK and we can write p(z | f, c1 , . . . , cK , x) using Dirac delta functions to pick out the values at x:
K
Y

(12)

where p(f , c1 , . . . , cK |D) is the GP predictive distribution
for objective and constraint values. Because (12) is not
tractable, we approximate the normalized version of q1
with a product of Gaussians using expectation propagation
(EP) (Minka, 2001). In particular, we obtain

!


Θ ck (x0 )

k=1

p(z | f, c1 , . . . , cK , x) = δ[z0 − f (x)]

q1 (f , c1 , . . . , cK ) =
i
i hQ
hQ
N
K
n=1 Ψ(xn ) p(f , c1 , . . . , cK |D)
k=1 Θ[ck0 ]

Z1−1 q1 (f , c1 , . . . , cK ) ≈ q2 (f , c1 , . . . , cK ) =
QK
N (f |m0 , V0 ) k=1 N (ck |mk , Vk ) , (13)

0

K
Y

rather than the full f, c1 , . . . , cK . We first approximate the
factors in (11) that do not depend on x as

δ[zk − ck (x)] . (10)

k=1

We can now write the NFCPD by i) noting that z is independent of x? given f, c1 , . . . , cK , ii) multiplying the product of (9) and (10) by p(f, c1 , . . . , cK |D) and iii) integrating out the latent functions f, c1 , . . . , cK :
Z
hQ
i
K
p(z|D, x, x? ) ∝ δ[z0 − f (x)]
δ[z
−
c
(x)]
k
k
k=1
hQ
i hQ
i
K
0
k=1 Θ [ck (x? )]
x0 6=x Ψ(x ) Ψ(x)
p(f, c1 , . . . , cK |D) df dc1 . . . dck ,

(11)

where p(f, c1 , . . . , cK |D) is an infinite-dimensional Gaussian given by the GP posterior on f, c1 , . . . , cK , and we
have separated Ψ(x) out from the infinite product over x0 .
We find a Gaussian approximation to (11) in several steps.
The general approach is to separately approximate the factors that do and do not depend on x, so that the computations associated with the latter factors can be reused rather
than recomputed for each x. In (11), the only factors that
depend on x are the deltas in the first line, and Ψ(x).
Let f denote the (N +1)-dimensional vector containing objective function evaluations at x? and x1 , . . . , xN , and define constraint vectors c1 , . . . , cK similarly. Then, we approximate (11) by conditioning only on f and c1 , . . . , cK ,

where Z1 is the normalization constant of q1 and (mk , Vk )
for k = 0, . . . , K are the mean and covariance terms determined by EP. See the supplementary material for details
on the EP approximation. Roughly speaking, EP approximates each true (but intractable) factor in (12) with a Gaussian factor whose parameters are iteratively refined. The
product of all these Gaussian factors produces a tractable
Gaussian approximation to (12).
We now approximate the portion of (11) that does depend
on x, namely the first line and the factor Ψ(x), by replacing
the deltas with p(z|f , c1 , . . . , cK ), the K + 1 dimensional,
Gaussian conditional distribution given by the GP priors
on f, c1 , . . . , cK . Our full approximation to (11) is then
p(z|D, x, x? ) ≈ Z2−1

Z
p(z|f , c1 , . . . , cK )Ψ(x)

q2 (f , c1 , . . . , cK ) df dc1 · · · dcK ,

(14)

where Z2 is a normalization constant. From here, we analytically marginalize out all integration variables except
f0 = f (x? ); see the supplementary material for the full
details. This calculation, and those that follow, must be repeated for every x; however, the EP approximation in (13)
can be reused over all x. After performing the integration,
we arrive at
p(z|D, x, x? ) ≈
Z
K
Y
1
Ψ(x)N ([z0 , f0 ]|m00 , V00 )
N (zk |m0k , vk0 ) df0 , (15)
Z3
k=1

where z0 = f (x). Details on how to compute the means
0
, as well as the 2m01 , . . . , m0K and variances v10 , . . . , vK
0
dimensional mean vector m0 and the 2 × 2 covariance matrix V00 can be found in the supplementary material.
We perform one final approximation to (15). We approximate this distribution with a product of independent Gaussians that have the same marginal means and variances as
(15). This corresponds to a single iteration of EP; see the
supplementary material for details.

Predictive Entropy Search for Bayesian Optimization with Unknown Constraints

3.2. The PESC acquisition function
By approximating the NFCPD with a product of independent Gaussians, we can approximate the entropy in the
CPD by performing the following operations. First, we add
the noise variances to the marginal variances of our final
approximation of the NFCPD and second, we compute the
entropy with (8). The PESC acquisition function, which
approximates (6), is then
(
αPESC (x) =

log vfPD (x)

+

K
X

)
log vkPD (x)

−

k=1

(
)
M
K

 X


1 X
CPD
(m)
CPD
(m)
log vf
x | x?
+
log vk
x | x?
,
M m=1

up as in Hennig & Schuler (2012) and Hernández-Lobato
et al. (2014). The search space is the unit hypercube of
dimension d, and the ground truth objective f is a sample
from a zero-mean GP with a squared exponential covariance function of unit amplitude and length scale ` = 0.1 in
each dimension. We represent the function f by first sampling from the GP prior on a grid of 1000 points generated
using a Halton sequence (see Leobacher & Pillichshammer, 2014) and then defining f as the resulting GP posterior mean. We use a single constraint function c1 whose
ground truth is sampled in the same way as f . The evaluations for f and c1 are contaminated with i.i.d. Gaussian
noise with variance σf2 = σ12 = 0.01.

k=1

(16)

where M is the number of samples drawn from p(x? |D),
(m)
x? is the m-th of these samples, vfPD (x) and vkPD (x) are
the predictive variances for the noisy evaluations of f and
(m)
(m)
ck at x, respectively, and vfCPD (x|x? ) and vkCPD (x|x? )
are the approximated marginal variances of the CPD for the
(m)
noisy evaluations of f and ck at x given that x? = x? .
Marginalization of (16) over the GP hyper-parameters can
be done efficiently as in Hernández-Lobato et al. (2014).
The PESC acquisition function is additive in the expected
amount of information that is obtained from the evaluation of each task (objective or constraint) at any particular location x. For example, the expected information gain
obtained from the
evaluation of f at x is given
i by the
PM h
(m)
1
PD
CPD
log
v
(x)
−
log
v
(x|x
)
in (16).
term M
?
f
f
m=1
The other K terms in (16) measure the corresponding contribution from evaluating each of the constraints. This allows PESC to easily address the decoupled scenario when
one can independently evaluate the different functions at
different locations. In other words, Equation (16) is a sum
of individual acquisition functions, one for each function
that we can evaluate. Existing methods for Bayesian optimization with unknown constraints (described in Section
2) do not possess this desirable property. Finally, the complexity of PESC is of order O(M KN 3 ) per iteration in the
coupled setting. As with unconstrained PES, this is dominated by the cost of a matrix inversion in the EP step.

4. Experiments
We evaluate the performance of PESC through experiments with i) synthetic functions sampled from the GP
prior distribution, ii) analytic benchmark problems previously used in the literature on Bayesian optimization with
unknown constraints and iii) real-world constrained optimization problems.
For case i) above, the synthetic functions sampled from the
GP prior are generated following the same experimental set

4.1. Accuracy of the PESC approximation
We first analyze the accuracy of the approximation to (7)
generated by PESC. We compare the PESC approximation
with a ground truth for (7) obtained by rejection sampling
(RS). The RS method works by discretizing the search
space using a uniform grid. The expectation with respect
to p(x? |Dn ) in (7) is then approximated by Monte Carlo.
To achieve this, f and c1 , . . . , cK are sampled on the grid
and the grid cell with positive c1 , . . . , cK (feasibility) and
the lowest value of f (optimality) is selected. For each sample of x? generated by this procedure, H [p(y|Dn , x, x? )] is
approximated by rejection sampling: we select those samples of f and c1 , . . . , cK whose corresponding feasible optimal solution is the sampled x? and reject the other samples. We then assume that the selected samples for f and
c1 , . . . , cK are independent and have Gaussian marginal
distributions. Under this assumption, H [p(y|Dn , x, x? )]
can be approximated using the formula for the entropy
of independent Gaussian random variables, with the variance parameters in this formula being equal to the empirical marginal variances of the selected samples of f and
c1 , . . . , cK at x plus the corresponding noise variances σf2
2
.
and σ12 , . . . , σK
The left plot in Figure 1 shows the posterior distribution
for f and c1 given 5 evaluations sampled from the GP prior
with d = 1. The posterior is computed using the optimal
GP hyperparameters. The corresponding approximations
to (7) generated by PESC and RS are shown in the middle
plot of Figure 1. Both PESC and RS use a total of 50 samples from p(x? |Dn ) when approximating the expectation
in (7). The PESC approximation is very accurate, and importantly its maximum value is very close to the maximum
value of the RS approximation.
One disadvantage of the RS method is its high cost, which
scales with the size of the grid used. This grid has to be
large to guarantee good performance, especially when d is
large. An alternative is to use a small dynamic grid that
changes as data is collected. Such a grid can be obtained

0.8

Predictive Entropy Search for Bayesian Optimization with Unknown Constraints

0.6
0.0

Objective
Constraint

0.0

0.2

0.4

x

0.6

0.8

(a) Marginal posteriors

1.0

Methods
●
●
●
●

−0.5

●
●

●
●
●

−1.5

0.2

0.4

x

0.6

0.8

1.0

PESC

●

RS

●

RSDG

●
●
●

●●
●
●●
● ●
●
●●
●
●
●●
●●●
●
●●
●
●●●●●
●
●●
●
●●●●●●●●●
●●
●●●
●
●●●●●
●●●
●
●● ●●
● ●
●●
●
●●●●●●●●●●●
●
●●● ●●
●●
●
●●●
●●●●●●●●●
●
● ●
●●●●●●●●●
●●●● ●
●●●●●●●●●●●●●●●●
●● ●
●●●●
●●●●●●
●●●● ●●●●
●●●●●●
●●●●●●
●●
●●●●●●●●●● ●●
●●●●● ●
●●● ●●●●●●
● ●●●●● ● ●
●
●●●●●●
● ●●
●●●●●
●●●●●●
●●●● ● ● ●
●●●●●●●●●●
● ● ● ●●●
●●
●●●●●●●●●
●● ●●●●●●●●
●

−2.5

−3.5

0.0

●

●
●
●●

10

x

Log1Median1Utility1Gap

x

0.4

x x
x
x

x

Acqusition Function

−3 −2 −1

0

x
x

x

0.2

1

2

3

●

PESC
RS

0

(b) Acquisition functions

25
50
75
Number of Function Evaluations

100

(c) Performance in 1d

Figure 1. Assessing the accuracy of the PESC approximation. (a) Marginal posterior predictive distributions for the objective and
constraint given some collected data denoted by ×’s. (b) PESC and RS acquisition functions given the data in (a). (c) Median utility gap
for PESC, RS and RSDG in the experiments with synthetic functions sampled from the GP prior with d = 1.

by sampling from p(x? |Dn ) using the same approach as in
PESC. The samples obtained would then form the dynamic
grid. The resulting method is called Rejection Sampling
with a Dynamic Grid (RSDG).
We compare the performance of PESC, RS and RSDG in
experiments with synthetic data corresponding to 500 pairs
of f and c1 sampled from the GP prior with d = 1. At
each iteration, RSDG draws the same number of samples
of x? as PESC. We assume that the GP hyperparameter
values are known to each method. Recommendations are
made by finding the location with lowest posterior mean
for f such that c1 is non-negative with probability at least
1 − δ1 , where δ1 = 0.05. For reporting purposes, we set
the utility u(x) of a recommendation x to be f (x) if x satisfies the constraint, and otherwise a penalty value of the
worst (largest) objective function value achievable in the
search space. For each recommendation at x, we compute
the utility gap |u(x) − u(x? )|, where x? is the true solution of the optimization problem. Each method is initialized with the same three random points drawn with Latin
hypercube sampling.
The right plot in Figure 1 shows the median of the utility
gap for each method across the 500 realizations of f and c1 .
The x-axis in this plot is the number of joint function evaluations for f and c1 . We report the median because the
empirical distribution of the utility gap is heavy-tailed and
in this case the median is more representative of the location of the bulk of the data than the mean. The heavy
tails arise because we are measuring performance across
500 different optimization problems with very different degrees of difficulty. In this and all following experiments,
standard errors on the reported plot are computed using
the bootstrap. The plot shows that PESC and RS are better than RSDG. Furthermore, PESC is very similar to RS,
with PESC even performing slightly better at the end of the
data collection process since PESC is not limited by a finite

grid as RS is. These results show that PESC yields a very
accurate approximation of the information gain. Furthermore, although RSDG performs worse than PESC, RSDG
is faster because the rejection sampling operation (with a
small grid) is less expensive than the EP algorithm. Thus,
RSDG is an attractive alternative to PESC when the available computing time is very limited.
4.2. Synthetic functions in 2 and 8 input dimensions
We also compare the performance of PESC and RSDG
with that of EIC (Section 2.1) using the same experimental
protocol as in the previous section, but with dimensionalities d = 2 and d = 8. We do not compare with RS here because its use of grids does not scale to higher dimensions.
Figure 4.1 shows the utility gap for each method across 500
different samples of f and c1 from the GP prior with d = 2
(a) and d = 8 (b). Overall, PESC is the best method, followed by RSDG and EIC. RSDG performs similarly to
PESC when d = 2, but is significantly worse when d = 8.
This shows that, when d is high, grid based approaches (e.g.
RSDG) are at a disadvantage with respect to methods that
do not require a grid (e.g. PESC).
4.3. A toy problem
We compare PESC with EIC and AL (Section 2.2) in the
toy problem described in Gramacy et al. (2014). We seek
to minimize the function f (x) = x1 + x2 , subject to the
constraint functions c1 (x) ≥ 0 and c2 (x) ≥ 0, given by
c1 (x) = 0.5 sin (2π(x21 − 2x2 )) + x1 + 2x2 − 1.5 , (17)
c2 (x) = −x21 − x22 + 1.5 ,

(18)

where x is confined to the unit square. The evaluations for
f , c1 and c2 are noise-free. We compare PESC and EIC
with δ1 = δ2 = 0.025 and a squared exponential GP kernel. PESC uses 10 samples from p(x? |Dn ) when approx-

Predictive Entropy Search for Bayesian Optimization with Unknown Constraints

●

●

●
●

●
●
●
●●
●●
●●●●
●●
●●●●
●
● ●●
●
●
●
●

●
●
●
●●
●
●
●
●
●

●

Methods
EIC

●

PESC

10

●

RSDG

●
●●

●
●●

●

●
●

●

●●

●

●
●●
●●
●●
●●
●●

●●

●

●●●

●●●

●●●

●
●●
●
●●
●
●●
●●
●●
●●
●●
●●
●●

●●

●●●

●●●

● ● ●
● ● ●●●●●

●●●

●●●●●●●

●●● ●
● ●●●●●●●

●●
●
●●●
●●
●●
●●●
●●●
●●●
●●
●
●●●●● ●●● ●
● ●●
●●
●●●
●
●●●●●●● ●
●●● ●●● ●
● ●●●●●●
● ●
●●●●●●

●●

●●

−2.6
0

●

●

●

●

●
●

●

●

●

●

●

●

●
●

●●●●●●●●

●●●●

●●●●●●●

●●●●●●●●●

●
●
●
●

25
50
75
Number of Function Evaluations

●

●

●

●

●

●

●

●

●

●

●
●
●
●

●

●
●

Methods

●

●

●

●

●

●

●

●

●

●

EIC

●

●

0.2

100

(a) Optimizing GP samples in d = 2

0

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

PESC

●

RSDG

●●●●●

●● ●
● ●●●●

−0.3
●

0.5

●

●

●
●

●

●
●

●

●

−1.6

●

●

●

●

●

●

●

●
●

●

●

10
20
Number of Function Evaluations

(b) Optimizing GP samples in d = 8

●

●
●

●

●
●

●

●

●
●

●

●
●

●

●

●

●
●

●

●

●

●
●

●

●
●
●

●
●
●

●
●

●

●
●

●
●

●
●

●
●

●
●

●
●

●

●

●

●
●

●
●

●
●

●
●

●
●

●
●

●

●

●

●

−1.3

●

●

●

●

●

●
●

●

●

●

●

Methods

●

●

●

●

●
●

●
●

●
●
●

●

●

AL

●

EIC

●

PESC

●

●

●

10

−0.6

●
●
●
●

●
●

Log Mean Utility Gap

●
●●
●
●
●●
●●
●

10

●
●
●●
●
●●
●
●

Log Median Utility Gap

Log Median Utility Gap

●

●
●
●

−2.3
0

●

●

●
●

●

●
●

●

●

●
●

10
20
30
Number of Function Evaluations

(c) 2d toy problem

Figure 2. Assessing PESC on synthetic problems. (a,b) Compare PESC to EIC and RSDG on optimizing samples from the GP in
dimension 2 and 8 respectively, and (c) compares PESC to AL and EIC.

imating the expectation in (7). We use the AL implementation provided by Gramacy et al. (2014) in the R package
laGP which is based on the squared exponential kernel and
assumes the objective f is known. Thus, in order for this
implementation to be used, AL has an advantage over other
methods in that it has access to the true objective function.
In all three methods, the GP hyperparameters are estimated
by maximum likelihood.
Figure 2(c) shows the mean utility gap for each method
across 500 independent realizations. Each realization corresponds to a different initialization of the methods with
three data points selected with Latin hypercube sampling.
Here, we report the mean because we are now measuring
performance across realizations of the same optimization
problem and the heavy-tailed effect described in Section
4.1 is less severe. The results show that PESC is significantly better than EIC and AL for this problem. EIC is
superior to AL, which performs slightly better at the beginning, presumably because it has access to the ground truth
objective f .
4.4. Finding a fast neural network
In this experiment, we tune the hyperparamters of a threehidden-layer neural network subject to the constraint that
the prediction time must not exceed 2 ms on a GeForce
GTX 580 GPU (also used for training). The search space
consists of 12 parameters: 2 learning rate parameters (initial and decay rate), 2 momentum parameters (initial and
final), 2 dropout parameters (input layer and other layers), 2 other regularization parameters (weight decay and
max weight norm), the number of hidden units in each of
the 3 hidden layers, the activation function (RELU or sigmoid). The network is trained using the deepnet package1 ,
and the prediction time is computed as the average time of
1000 predictions, each for a batch of size 128. The network is trained on the MNIST digit classification task with
1

https://github.com/nitishsrivastava/deepnet

momentum-based stochastic gradient descent for 5000 iterations. The objective is reported as the classification error
rate on the validation set. As above, we treat constraint
violations as the worst possible value (in this case a classification error of 1.0).
Figure 3(a) shows the results of 50 iterations of Bayesian
optimization. In this experiment and the next, the y-axis
represents observed objective values, δ1 = 0.05, a Matérn
5/2 GP covariance kernel is used, and GP hyperparameters are integrated out using slice sampling (Neal, 2000) as
in Snoek et al. (2012). Curves are the mean over 5 independent experiments. We find that PESC performs significantly better than EIC. However, when the noise level is
high, reporting the best objective observation is an overly
optimistic metric (due to “lucky” evaluations); on the other
hand, ground-truth is not available. Therefore, to validate
our results further, we used the recommendations made at
the final iteration of Bayesian optimization for each method
(EIC and PESC) and evaluted the function with these recommended parameters. We repeated the evaluation 10
times for each of the 5 repeated experiments to compute
a ground-truth score averaged of 50 function evaluations.
This procedure yields a score of 7.0 ± 0.6% for PESC and
49 ± 4% for EIC (as in the figure, constraint violations are
treated as a classification error of 100%). This result is consistent with Figure 3(a) in that PESC performs significantly
better than EIC, but also demonstrates that, due to noise,
Figure 3(a) is overly optimistic. While we may believe this
optimism to affect both methods equally, the ground-truth
measurement provides a more reliable result and a much
clearer understanding of the classification error attained by
Bayesian optimization.
4.5. Tuning Markov chain Monte Carlo
Hybrid Monte Carlo, also known as Hamiltonian Monte
Carlo (HMC), is a popular Markov Chain Monte Carlo
(MCMC) technique that uses gradient information in a numerical integration to select the next sample. However,

Predictive Entropy Search for Bayesian Optimization with Unknown Constraints

EIC
PESC

0.0

log10 objective value

−log10 effective sample size

0.2

0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
0

10

20

30

40

50

Number of function evaluations

(a) Tuning a fast neural network

0
EIC
PESC

-1
-2
-3
-4
-5

0

20

40

60

80

Number of function evaluations

10 0

(b) Tuning Hamiltonian MCMC

Figure 3. Comparing PESC and EIC for (a) minimizing classification error of a 3-hidden-layer neural network constrained to make
predictions in under 2 ms, and (b) tuning Hamiltonian Monte Carlo to maximize the number of effective samples within 5 minutes of
compute time.

using numerical integration gives rise to new parameters
like the integration step size and the number of integration
steps. Following the experimental set up in Gelbart et al.
(2014), we optimize the number of effective samples produced by an HMC sampler limited to 5 minutes of computation time, subject to passing of the Geweke (Geweke,
1992) and Gelman-Rubin (Gelman & Rubin, 1992) convergence diagnostics, as well as the constraint that the numerical integration should not diverge. We tune 4 parameters
of an HMC sampler: the integration step size, number of
integration steps, fraction of the allotted 5 minutes spent in
burn-in, and an HMC mass parameter (see Neal, 2011). We
use the coda R package (Plummer et al., 2006) to compute
the effective sample size and the Geweke convergence diagnostic, and the PyMC python package (Patil et al., 2010)
to compute the Gelman-Rubin diagnostic over two independent traces. Following Gelbart et al. (2014), we impose
the constraints that the absolute value of the Geweke test
score be at most 2.0 and the Gelman-Rubin score be at most
1.2, and sample from the posterior distribution of a logistic
regression problem using the UCI German credit data set
(Frank & Asuncion, 2010).
Figure 3(b) evaluates EIC and PESC on this task, averaged
over 10 independent experiments. As above, we perform a
ground-truth assessment of the final recommendations. The
average effective sample size is 3300 ± 1200 for PESC and
2300 ± 900 for EIC. From these results we draw a similar
conclusion to that of Figure 3(b); namely, that PESC outperforms EIC but only by a small margin, and furthermore
that the experiment is very noisy.

5. Discussion
In this paper, we addressed global optimization with unknown constraints. Motivated by the weaknesses of existing methods, we presented PESC, a method based on the
theoretically appealing expected information gain heuristic. We showed that the approximations in PESC are quite
accurate, and that PESC performs about equally well to a
ground truth method based on rejection sampling. In sections 4.2 to 4.5, we showed that PESC outperforms current
methods such as EIC and AL over a variety of problems.
Furthermore, PESC is easily applied to problems with decoupled constraints, without additional computational cost
or the pathologies discussed in Gelbart et al. (2014).
One disadvantage of PESC is that it is relatively difficult to
implement: in particular, the EP approximation often leads
to numerical instabilities. Therefore, we have integrated
our implementation, which carefully addresses these numerical issues, into the open-source Bayesian optimization
package Spearmint at https://github.com/HIPS/
Spearmint/tree/PESC. We have demonstrated that
PESC is a flexible and powerful method and we hope the
existence of such a method will bring constrained Bayesian
optimization into the standard toolbox of Bayesian optimization practitioners.
Acknowledgements
José Miguel Hernández-Lobato acknowledges support
from the Rafael del Pino Foundation. Zoubin Ghahramani acknowledges support from Google Focused Research Award and EPSRC grant EP/I036575/1. Matthew
W. Hoffman acknowledges support from EPSRC grant
EP/J012300/1.

Predictive Entropy Search for Bayesian Optimization with Unknown Constraints

References
Frank, Andrew and Asuncion, Arthur. UCI machine learning repository, 2010.
Gardner, Jacob R., Kusner, Matt J., Xu, Zhixiang (Eddie), Weinberger, Kilian Q., and Cunningham, John P.
Bayesian optimization with inequality constraints. In
ICML, 2014.

Neal, Radford. Slice sampling. Annals of Statistics, 31:
705–767, 2000.
Neal, Radford. MCMC using Hamiltonian dynamics. In
Handbook of Markov Chain Monte Carlo. Chapman and
Hall/CRC, 2011.
Patil, Anand, Huard, David, and Fonnesbeck, Christopher.
PyMC: Bayesian stochastic modelling in Python. Journal of Statistical Software, 2010.

Gelbart, Michael A., Snoek, Jasper, and Adams, Ryan P.
Bayesian optimization with unknown constraints. In
UAI, 2014.

Picheny, Victor. A stepwise uncertainty reduction approach
to constrained global optimization. In AISTATS, 2014.

Gelman, Andrew and Rubin, Donald R. A single series
from the Gibbs sampler provides a false sense of security. In Bayesian Statistics, pp. 625–32. 1992.

Plummer, Martyn, Best, Nicky, Cowles, Kate, and Vines,
Karen. CODA: Convergence diagnosis and output analysis for MCMC. R News, 6(1):7–11, 2006.

Geweke, John. Evaluating the accuracy of sampling-based
approaches to the calculation of posterior moments. In
Bayesian Statistics, pp. 169–193, 1992.

Rasmussen, C. and Williams, C. Gaussian Processes for
Machine Learning. MIT Press, 2006.

Gramacy, Robert B. and Lee, Herbert K. H. Optimization
under unknown constraints. Bayesian Statistics, 9, 2011.

Schonlau, Matthias, Welch, William J, and Jones, Donald R.
Global versus local search in constrained
optimization of computer models.
Lecture NotesMonograph Series, pp. 11–25, 1998.

Gramacy, Robert B., Gray, Genetha A., Digabel, Sebastien Le, Lee, Herbert K. H., Ranjan, Pritam, Wells,
Garth, and Wild, Stefan M. Modeling an augmented Lagrangian for improved blackbox constrained optimization, 2014. arXiv:1403.4890v2 [stat.CO].
Hennig, Philipp and Schuler, Christian J. Entropy search
for information-efficient global optimization. JMLR, 13,
2012.
Hernández-Lobato, J. M, Hoffman, M. W., and Ghahramani, Z. Predictive entropy search for efficient global
optimization of black-box functions. In NIPS. 2014.
Houlsby, N., Hernández-Lobato, J. M, Huszar, F., and
Ghahramani, Z. Collaborative Gaussian processes for
preference learning. In NIPS. 2012.
Jones, Donald R, Schonlau, Matthias, and Welch,
William J. Efficient global optimization of expensive
black-box functions. Journal of Global optimization, 13
(4):455–492, 1998.
Leobacher, Gunther and Pillichshammer, Friedrich. Introduction to quasi-Monte Carlo integration and applications. Springer, 2014.
Minka, Thomas P. A family of algorithms for approximate
Bayesian inference. PhD thesis, Massachusetts Institute
of Technology, 2001.
Mockus, Jonas, Tiesis, Vytautas, and Zilinskas, Antanas.
The application of Bayesian methods for seeking the extremum. Towards Global Optimization, 2, 1978.

Snoek, Jasper. Bayesian Optimization and Semiparametric
Models with Applications to Assistive Technology. PhD
thesis, University of Toronto, Toronto, Canada, 2013.
Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P.
Practical Bayesian optimization of machine learning algorithms. In NIPS, 2012.
Villemonteix, Julien, Vázquez, Emmanuel, and Walter,
Eric. An informational approach to the global optimization of expensive-to-evaluate functions. Journal of
Global Optimization, 44(4):509–534, 2009.

