Skip Context Tree Switching

Marc G. Bellemare
Joel Veness
Google DeepMind

BELLEMARE @ GOOGLE . COM
VENESS @ GOOGLE . COM

Erik Talvitie
Franklin and Marshall College

Abstract
Context Tree Weighting is a powerful probabilistic sequence prediction technique that efficiently performs Bayesian model averaging over
the class of all prediction suffix trees of bounded
depth. In this paper we show how to generalize
this technique to the class of K-skip prediction
suffix trees. Contrary to regular prediction suffix
trees, K-skip prediction suffix trees are permitted
to ignore up to K contiguous portions of the context. This allows for significant improvements in
predictive accuracy when irrelevant variables are
present, a case which often occurs within recordaligned data and images. We provide a regretbased analysis of our approach, and empirically
evaluate it on the Calgary corpus and a set of
Atari 2600 screen prediction tasks.

1. Introduction
The sequential prediction setting, in which an unknown environment generates a stream of observations which an algorithm must probabilistically predict, is highly relevant to
a number of machine learning problems such as statistical
language modelling, data compression, and model-based
reinforcement learning. A powerful algorithm for this
setting is Context Tree Weighting (CTW, Willems et al.,
1995), which efficiently performs Bayesian model averaging over a class of prediction suffix trees (Ron et al., 1996).
In a compression setting, Context Tree Weighting is known
to be an asymptotically optimal coding distribution for DMarkov sources.
A significant practical limitation of CTW stems from the
fact that model averaging is only performed over prediction suffix trees whose ordering of context variables is
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

ERIK . TALVITIE @ FANDM . EDU

fixed in advance. As we discuss in Section 3, reordering these variables can lead to significant performance improvements given limited data. This idea was leveraged
by the class III algorithm of Willems et al. (1996), which
performs Bayesian model averaging over the collection of
prediction suffix trees defined over all possible fixed variable orderings. Unfortunately, the O(2D ) computational
requirements of the class III algorithm prohibit its use in
most practical applications.
Our main contribution is the Skip Context Tree Switching
(SkipCTS) algorithm, a polynomial-time compromise between the linear-time CTW and the exponential-time class
III algorithm. We introduce a family of nested model
classes, the Skip Context Tree classes, which form the basis of our approach. The K th order member of this family
corresponds to prediction suffix trees which may skip up
to K runs of contiguous variables. The usual model class
associated with CTW is a special case, and corresponds to
K = 0. In many cases of interest, SkipCTS‚Äôs O(D2K+1 )
running time is practical and provides significant performance gains compared to Context Tree Weighting.
SkipCTS is best suited to sequential prediction problems
where a good fixed variable ordering is unknown a priori.
As a simple example, consider the record aligned data depicted by Figure 1. SkipCTS with K = 1 can improve on
the CTW ordering by skipping the five most recent symbols
and directly learning the lexicographical relation.
While Context Tree Weighting has traditionally been used
as a data compression algorithm, it has proven useful in
a diverse range of sequential prediction settings. For example, Veness et al. (2011) proposed an extension (FACCTW) for Bayesian, model-based reinforcement learning
in structured, partially observable domains. Bellemare
et al. (2013b) used FAC-CTW as a base model in their
Quad-Tree Factorization algorithm, which they applied to
the problem of predicting high-dimensional video game
screen images. Our empirical results on the same video
game domains (Section 4.2) suggest that SkipCTS is par-

Skip Context Tree Switching

2.1. Bayesian Mixture Models
A

F

R

A

I

D

A

G

A

I

N

!

A

L

W

A

Y

S

A

M

A

Z

E

D

B

E

C

O

M

E

B

E

H

O

L

D

B

E

T

T

E

R

Figure 1. A sequence of lexicographically sorted fixed-length
strings, which is particularly well-modelled by SkipCTS.

ticularly beneficial in this more complex prediction setting.

2. Background
We consider the problem of probabilistically predicting the
output of an unknown sequential data generating source.
Given a finite alphabet X , we write x1:n := x1 x2 . . . xn ‚àà
X n to denote a string of length n, xy to denote the concatenation of two strings x and y, and xi to denote the concatenation of i copies of x. We further denote x<n := x1:n‚àí1
and the empty string by . Given an arbitrary finite length
string y, we denote its length by |y|. The space of probability distributions over a finite alphabet X is denoted by
P(X ). A sequential probabilistic model œÅ is defined by a
sequence of probability mass functions {œÅi ‚àà P(X i )}i‚ààN
that satisfy, forPany n ‚àà N, for any string x1:n ‚àà X n ,
the constraint xn ‚ààX œÅn (x1:n ) = œÅn‚àí1 (x<n ). Since the
subscript to œÅn is always clear from its argument, we henceforth write œÅ(x1:n ) for the probability assigned to x1:n by œÅ.
We use œÅ(xn | x<n ) to denote the probability of xn conditional on x<n , defined as œÅ(xn | x<n ) := œÅ(x1:n )/œÅ(x<n )
provided
œÅ(x<n ) > 0, from which the chain rule œÅ(x1:n ) =
Qn
œÅ(x
i | x<i ) follows.
i=1
We assess the quality of a model‚Äôs predictions
through
its cumulative, instantaneous logarithmic loss
Pn
‚àí
log
œÅ(xi | x<i ) = ‚àí log œÅ(x1:n ). Given a set of
i=1
models M, we define the regret of œÅ with respect to M as

One way to construct a model with guaranteed low regret
with respect to some model class M is to use a Bayesian
mixture model
X
ŒæMIX (x1:n ) :=
wœÅ œÅ(x1:n ),
œÅ‚ààM

P
where wœÅ > 0 are prior weights satisfying œÅ‚ààM wœÅ = 1.
It can readily be shown that, for any œÅ ‚àà M, we have
Rn (ŒæMIX , {œÅ}) ‚â§ ‚àí log wœÅ ,
which implies that the regret of ŒæMIX (x1:n ) with respect to
M is bounded uniformly by a constant that depends only
on the prior weight assigned to the best model in M. For
example, the Context Tree Weighting approach of Willems
et al. (1995) applies this principle recursively to efficiently
construct a mixture model over a doubly-exponential class
of tree models.
A more refined nonparametric Bayesian approach to mixing is also possible. Given a model class M, the switching method (Koolen & de Rooij, 2013) efficiently maintains a mixture model ŒæSWITCH over all sequences of models in M. We review here a restricted application of this
technique based on the work of Veness et al. (2012) and
Herbster & Warmuth (1998). More formally, given an
indexed set of models {œÅ1 , œÅ2 , . . . , œÅk } and an index sequence i1:n ‚àà {1, 2, . . . , k}n let
œÅi1:n (x1:n ) :=

ŒΩ‚ààM

œÅit (xt | x<t )

t=1

be a model which predicts at each time step t according to
the model with index it . The switching technique implicitly computes a Bayesian mixture over the exponentially
many possible index sequences. This mixture is efficiently
computed in O(k) per step by using
X
ŒæSWITCH (x1:n ) =
wœÅ,n‚àí1 œÅ(xn | x<n )
œÅ‚ààM

where, for t ‚àà 1 . . . n, we have that
wœÅ,t

Rn (œÅ, M) := ‚àí log œÅ(x1:n ) ‚àí min ‚àí log v(x1:n ).

n
Y

:=

t
wœÅ,t‚àí1 œÅ(xi | x<t ) +
t+1
X
1
wŒΩ,t‚àí1 ŒΩ(xi | x<t )
t+1

(1)

ŒΩ‚ààM\{œÅ}

Our notion of regret corresponds to the excess total loss
suffered from using œÅ in place of the best model in
M. In our later analysis, we will show that the regret of our technique grows sublinearly and therefore that
limn‚Üí‚àû Rn (œÅ, M)/n = 0. In other words, the average
instantaneous excess loss of our technique with respect to
the best model in M asymptotically vanishes.

and in the base case wœÅ,0 := 1/k for each œÅ ‚àà M. It can
be shown (Veness et al., 2012) that for any œÅi1:n we have
Rn (ŒæSWITCH , {œÅi1:n }) ‚â§ [m(i1:n ) + 1] (log k + log n)
Pn
where m(i1:n ) := t=2 Jit‚àí1 6= it K counts the number of
times the index sequence switches models. In particular, if

Skip Context Tree Switching

0

‚á¢(xc1:n )

1

‚úì0

‚á†0c (x1:n )
0

‚á†c (x1:n )
‚á†1c (x1:n )

1

‚úì01

‚úì11

Figure 2. A prediction suffix tree.

a single model performs best throughout, ŒæSWITCH only incurs an additional log n cost compared to a Bayesian mixture model using a uniform prior. The switching method
is a key component of the Context Tree Switching algorithm, which we review in Section 2.3, as well as our new
SkipCTS algorithm. In practice, especially when the models in M are both adaptive and of varying complexity, the
ability to rapidly switch between models often leads to an
empirical performance improvement (Erven et al., 2007).
A more comprehensive overview of switching strategies
can be found in the work of Koolen & de Rooij (2013).
2.2. Prediction Suffix Trees
A Prediction Suffix Tree (Ron et al., 1996; Figure 2) is a
type of variable order Markov model. Informally, a prediction suffix tree combines a collection of models using
a data-partitioning tree, whose purpose is to select which
particular model to use at each time step.
Given finite strings c := c1 . . . cm ‚àà X m and x1:n :=
x1 . . . xn ‚àà X n , we say that c is a suffix of x1:n if xn‚àíi =
cm‚àíi for all i ‚àà {0, . . . , m ‚àí 1}, and that c is the context
of x1:n if it is a suffix of x<n . We also write Tc (x1:n ) :=
{i ‚àà N : c is a suffix of x<i , 1 ‚â§ i ‚â§ n} to denote the
set of time indices occurring in context c, and denote by
xc1:n := hxi : i ‚àà Tc (x1:n )i the subsequence of x1:n that
matches context c. Furthermore, given an alphabet X and
an upper
SDbound on the maximum Markov order D ‚àà N, let
XÃÑ := i=0 X i , with X 0 := {}. A set S ‚äÜ XÃÑ is called
a proper suffix set over XÃÑ if for any finite string x there
is exactly one c ‚àà S such that c is a suffix of x. We also
write S(x) to denote the matching context c ‚àà S of a string
x. The key property of S(¬∑) is that given x1:n , it defines a
partition of the time indices {1, . . . , n} in the sense that the
collection of sets {Tc (x1:n )}c‚ààS is mutually exclusive and
exhaustive.
A prediction suffix tree is a tuple (S, Œò) where S is a proper
suffix set over XÃÑ and Œò := {Œ∏c }c‚ààS is a set of sequential probabilistic models, with each model being associated
with a particular context in S. If ct = S(x<t ) is the context
in S corresponding to x<t , then the tth symbol is predicted

Figure 3. The Context Tree Switching recursive operation. For
every context c we construct a model which switches between a
base estimator œÅ and a recursively defined split estimator.
t
as Œ∏ct (xt | xc<t
). Since S is a proper suffix set, this gives
the sequential probabilistic model
Y
Y
Y
œàS,Œò (x1:n ) :=
Œ∏c (xt | xc<t ) =
Œ∏c (xc1:n ).

c‚ààS t‚ààTc (x1:n )

c‚ààS

2.3. Context Tree Switching
Context Tree Switching (CTS, Veness et al., 2012) is a recent extension of Context Tree Weighting (CTW, Willems
et al., 1995) that retains the strong theoretical properties
of CTW but performs better in practice. Let X := {0, 1}
be the binary alphabet, D ‚àà N be arbitrary but fixed, and
let CD be the collection of all binary prediction suffix trees
(S, Œò) of depth less than or equal to D. Let ŒæCTS (x1:n )
denote the probability assigned to x1:n by CTS. The regret
Rn (ŒæCTS , CD ) of CTS with respect to CD is upper bounded
by

n
ŒìD (S) + (‚àÜ(S) + 1) log n + |S|Œ≥ |S|
,
(2)
where ŒìD (S) := |S| ‚àí 1 + |{c : c ‚àà S, |c| 6= D}| is the
description length of S, ‚àÜ(S) := maxc‚ààS |c|, and
(
z
if 0 ‚â§ z < 1
Œ≥(z) := 1
log
(z)
+
1
if
z ‚â• 1.
2
2
Notice that the bound makes explicit an Occam bias towards smaller prediction suffix trees. The last summand in
Equation 2 arises from having to learn the parameters of
|S| unknown Bernoulli distributions. This, combined with
the fact that ŒìD (S) = O(|S|), causes CTS to perform well
whenever a small prediction suffix tree is sufficient to describe the data.
Algorithm. CTS is best understood as a recursive application of the switching technique described in Section 2.1.
As depicted in Figure 3, for each context c ‚àà XÃÑ we define a
switching model between two components: a base estimator œÅ which predicts the substring xc1:n and a split estimator which subdivides c into 0c and 1c and predicts xc1:n by
querying the corresponding switching models for the further partitioned data. The latter operation is well-defined

Skip Context Tree Switching

by the partitioning property of proper suffix sets: xn be1c
longs to either x0c
1:n or x1:n but not both. The algorithm
then assigns a probability to x1:n ‚àà X n using its top-level
switching model, i.e. ŒæCTS (x1:n ) := Œæ (x1:n ).
The algorithmic core of CTS is a context tree data structure: a perfect binary tree of depth D whose nodes correspond to all possible strings in XÃÑ . Each node c ‚àà XÃÑ
stores a base estimator œÅc as well as the data-dependent
quantities Œæc , Œ±c , and Œ≤c . Informally, Œ±c (x<t ) and Œ≤c (x<t )
correspond to wœÅ,t‚àí1 in Equation 1, while Œæc corresponds
to ŒæSWITCH (x1:t ). CTS incrementally maintains these quantities as follows. Given a new symbol xt and its associated history x<t , CTS updates the D + 1 nodes along the
path , xt‚àí1 , xt‚àí2:t‚àí1 , . . . , xt‚àíD:t‚àí1 ; all other nodes are
left unchanged. CTS performs a post-order traversal along
this path, first updating each node‚Äôs base estimator and the
other quantities as follows. At the leaf c = xt‚àíD:t‚àí1 , CTS
sets Œ±c (x1:t ) ‚Üê Œ±c (x<t )œÅc (xt | xc<t ) and then Œæc (x1:t ) ‚Üê
Œ±c (x1:t ). At the internal nodes, the following updates occur:
Œ±c (x<t )œÅc (xt | xc<t ) + Œ≤c (x<t )zc,t
t‚àí1
1
Œæc (x1:t ) +
Œ±c (x<t )œÅc (xt | xc<t )
Œ±c (x1:t ) ‚Üê
t+1
t+1
1
t‚àí1
Œ≤c (x1:t ) ‚Üê
Œæc (x1:t ) +
Œ≤c (x<t )zc,t
t+1
t+1
Œæc (x1:t ) ‚Üê

where zc,t := Œæx0 c (x1:t )/Œæx0 c (x<t ) is the probability assigned to xt by the recursively-defined split estimator, with
x0 := xt‚àí|c|‚àí1 . Every node c is initialized with Œ±c () =
Œ≤c () = 21 , except for leaf nodes where we set Œ±c () = 1
and Œ≤c () = 0 to reflect the fact that no further splitting
occurs at depth D.
2.4. Choice of Base Estimator
If the alphabet is binary, a natural choice of base model
is the KT estimator (Krichevsky & Trofimov, 1981). This
estimator probabilistically predicts each symbol according
to a Beta-Binomial model, using a Beta( 12 , 21 ) prior over
the unknown parameter. The regret of the KT estimator
with respect to any Bernoulli process is known to be at
most 12 log n + 1. Non-binary alphabets can be handled
in a number of ways. The most direct approach is to use a
Dirichlet-Multinomial model with a Dirichlet( 12 ) prior over
X , leading to the multi-alphabet KT estimator, whose regret is O(|X | log n) (Tjalkens et al., 1993). When |X | is
large and only a small fraction of the possible symbols are
observed, this approach is inefficient (e.g. Volf, 2002). A
recently developed solution to this problem is the Sparse
Adaptive Dirichlet (SAD) estimator (Hutter, 2013). The
SAD approach enjoys regret guarantees close to those of
the multi-alphabet KT restricted to the subalphabet A ‚äÜ X
of symbols which occur in x1:n . In Section 4 we describe a

x1:n
x1:n

m irrelevant
variables

O(log n) regret
2m O(log n) regret

Figure 4. Worst-case regret when using CTS instead of SkipCTS.

large experiment whose performance is much improved by
the use of a SAD-like estimator.

3. Skip Context Tree Switching
In this section we generalize CTS to partial context matches
to produce the Skip Context Tree Switching (SkipCTS) algorithm. We begin with some notation describing partial
context matches, then describe how the SkipCTS algorithm
incorporates these. Finally we provide a bound on the regret of SkipCTS with respect to the set of all bounded Kskip prediction suffix trees.
To gain some intuition as to why ignoring irrelevant variables matters, consider what happens internally in the context tree in the presence of irrelevant variables. As the righthand side of Figure 4 shows, in the worst case, the data used
to train the base models can be dispersed uniformly across
2m bins. On the other hand, SkipCTS can ignore the intervening variables and directly aggregate all the available
data into a single bin (Figure 4, left). In the particular case
where the base estimator is the KT estimator (with regret
O(log n) for memoryless sources), we see that SkipCTS
can enjoy an exponential reduction in regret compared to
CTS.
3.1. Partial Context Matches
We begin with some notation. Let ? denote a wildcard symbol and let Y := X ‚à™ {?} be the wildcard extension of X .
We say that a string a ‚àà X m matches b ‚àà Y m if for all
1 ‚â§ i ‚â§ m such that bi 6= ?, we have ai = bi . For example, if X is the set of uppercase letters then BAY, BOY,
BUY , etc. all match B ? Y . Given a finite string x, we say
that c ‚àà Y m is a suffix of x iff x = yc0 for some c0 ‚àà X m
such that c0 matches c. We call a string c k-skip contiguous
if it contains at most k contiguous runs of ? symbols. For
example, 0 ? ?1 ? 0 is 2-skip contiguous. Finally, we denote
the set of all k-skip contiguous strings of length i by Yki
and the union of all such sets for i = 0 . . . D as YÃÑk .

Skip Context Tree Switching

3.2. Algorithm
The SkipCTS algorithm is parametrized by a maximum
depth D ‚àà N and a number of allowable skips K ‚àà N.
The key idea is to maintain a context tree whose nodes correspond to all possible contexts c ‚àà YÃÑK . To do so, we generalize the CTS update equations described in Section 2.3,
leading to a recursive switching model which chooses between not only a base estimator and a split estimator, but
also between a variable number of recursively defined skip
estimators. Effectively, these additional estimators correspond to ignoring one or more symbols in the context. As
we will show in Section 3.3, the addition of these new models allows us to obtain a competitive regret bound with
respect to the set of all bounded K-skip prediction suffix
trees.
In designing SkipCTS, one additional subtle issue arises:
some of the contexts in YÃÑK are redundant for the purpose of sequential prediction. To see this, consider two
contexts from YÃÑK , c = 010 and c0 = ?010. Given any
string x1:n ‚àà X n , both contexts induce the same substring
0
xc1:n = xc1:n , so that only one of them needs to be considered by our algorithm. More generally, the contexts
c ‚àà YÃÑK for which c = ?l c0 with l ‚àà N are equivalent.
We refine the algorithm by considering only the set of representative contexts, i.e. the contexts c such that c = xc0
where x ‚àà X . In other words, representative contexts are
those contexts which do not contain trailing wildcard symbols. This results in a more efficient algorithm than if we
were to consider all possible contexts in YÃÑK . To avoid notational clutter, from here onwards we will use the notation
YÃÑK to denote the set of representative contexts. For a given
string x, we call a representative context c which is a suffix
of x a representative suffix.
For every c ‚àà YÃÑK we incrementally maintain a base estimator œÅc and the quantities Œæc , Œ±c , Œ≤c,0 , Œ≤c,1 , . . . . The number of Œ≤c,¬∑ quantities depends on c as follows. Define Œ∫(c)
as the number of contiguous runs of ? symbols in c. For
c ‚àà YÃÑ, define r(c) := 1 if either |c| = D or Œ∫(c) = K, and
D ‚àí |c| otherwise. The Œ≤c,0 term then corresponds to the
split estimator, while the remaining Œ≤c,¬∑ terms correspond
to r(c) ‚àí 1 skip estimators; in particular, when r(c) = 1 no
skip estimators are updated for c.
Upon observing a new symbol xt , SkipCTS first updates
Œ±c (x1:t ) ‚Üê Œ±c (x<t )œÅc (xt | xc<t ) and then Œæc (x1:t ) ‚Üê
Œ±c (x1:t ) for all c ‚àà YÃÑK representative suffixes of x<t with
|c| = D. The remaining representative suffixes of x<t are
updated recursively as
r(c)‚àí1
X
Œæc (x1:t ) ‚Üê Œ±c (x<t )bc,t +
Œ≤c,l (x<t )zc,l,t
(3)
 l=0 
t
‚àí Œ∑t Œ±c (x<t )bc,t (4)
Œ±c (x1:t ) ‚Üê Œ∑t Œæc (x1:t )+ t+1

Œ≤c,l (x1:t ) ‚Üê Œ∑t Œæc (x1:t )+



t
t+1


‚àí Œ∑t Œ≤c,l (x<t )zc,l,t (5)

where Œ∑t = 1/ [r(c)(t + 1)], bc,t := œÅc (xt | xc<t ), and zc,l,t
is the probability assigned to xt by the lth skip estimator,
which is defined as
zc,l,t := Œæx0 c0 (x1:t ) / Œæx0 c0 (x<t ),

(6)

where c0 := ?l c and x0 := x0t‚àí|c0 |‚àí1 . As with CTS, any
node not corresponding to a representative suffix of x<t is
left unchanged. The probability ŒæS KIP CTS (x1:n ) output by
SkipCTS is the probability assigned by the root switching
model Œæ (x1:n ).
The particular update structure of SkipCTS, i.e. the contexts c for which Œæc (x1:n ) are updated, depends on both K
and D. As with CTS, we set Œ±c () = 1 whenever |c| = D.
For |c| < D, we set Œ±c () = 21 and
1. Œ≤c,0 () =

1
2

if Œ∫(c) = K;

2. otherwise
Œ≤c,l () =

Ô£±
Ô£≤

1
4

Ô£≥

0

1
4r(c)

if l = 0;
if 1 ‚â§ l < r(c);
otherwise.

It can be verified that for any c ‚àà YÃÑK we have Œ±c () +
PD‚àí1
l=0 Œ≤c,l () = 1. In general, we may freely initialize the
non-zero Œ±c and Œ≤c terms, provided they are nonnegative
and sum to one. Because Œ∫(c) in Equation 3 ranges from 0
to K and |c| from 0 to D, performing one update requires
O(D2K+1 ) operations ‚Äì effectively the number of representative suffixes c ‚àà YÃÑK which match x<t . In particular,
note that when K = 0 we recover the original Context Tree
Switching algorithm of Veness et al. (2012).
3.3. Regret Analysis
In this section we provide a bound on the regret of Skip
Context Tree Switching with respect to any bounded Kskip prediction suffix tree (S, Œò), where K is fixed and S is
a proper suffix set over YÃÑ. At a high level, Lemma 1 bounds
the regret induced by the base estimators at the leaves of
(S, Œò). Lemma 2 bounds the regret contributed from a single level of internal nodes in the tree, and Lemma 3 applies
a recursive argument to combine Lemmas 1 and 2. Theorem 1 finally uses Lemma 3 to bound the regret of SkipCTS
with respect to an arbitrary K-skip prediction suffix tree.
Lemma 1. For any proper suffix set S over YÃÑK and for any
x1:n ‚àà X n , we have
Y
c‚ààS

Œ±c (x1:n ) ‚â•

1 Y
Œ±c ()œÅc (xc1:n ).
n+1
c‚ààS

Proof. Let ct := S(x<t ) be the context in S which is a
suffix of x<t , which is guaranteed to be unique as S is a

Skip Context Tree Switching

0

Lemma 2. Let S be a proper suffix set over YÃÑK , and let
SÃÉd := {(c, l) ‚àà SÃÉ : `(c) = d}. For any d ‚àà {0, . . . , D ‚àí1}
and any x1:n ‚àà X n , we have that

1

?

Y
0

0

‚úì01

1

‚úì0?0

1

(c,l)‚ààSÃÉd

‚úì11

‚úì1?0

Figure 5. A 1-skip prediction suffix tree.

proper suffix set. By combining Equations 3 and 4 we have
t
t
Œ±ct (x<t )œÅct (xt | xc<t
). By definition, for
Œ±ct (x1:t ) ‚â• t+1
all other c ‚àà S we have Œ±c (x1:t ) = Œ±c (x<t ). Recalling
that Tc (x1:n ) := {t ‚àà N : c is a suffix of x<t }, we expand
Equation 4 as
Y
c
t
Œ±c (x1:n ) ‚â• Œ±c ()
t+1 œÅc (xt | x<t )
t‚ààTc (x1:n )

=

Y

Œ±c ()œÅc (xc1:n )

Œ≤c,l (x1:n ) ‚â•

t
t+1 ,

1
n+1

Y

Y

Œ≤c,l ()

Œæx?l c (x1:n ).

x‚ààX

(c,l)‚ààSÃÉd

Proof. For any (c, l) ‚àà SÃÉ, and any t ‚àà {1, . . . , n}, let
c0 = ?l c and x0 = xt‚àí|c0 |‚àí1 . Now for any x ‚àà X \{x0 }, we
have Œæxc0 (x1:t ) = Œæxc0 (x<t ), which enables us to rewrite
Equation 6 as
Y
zc,l,t =
Œæxc0 (x1:t )/Œæxc0 (x<t ),
x‚ààX

which, following a similar approach to the proof of
Lemma 1 leads to the desired result.
Lemma 3. For every proper suffix set S over YÃÑK , with SÃÉ
denoting the set of choice nodes of S, we have
Y
Y
Œæ (x1:n ) ‚â• n‚àí`(S)
Œ≤c,l ()
Œ±c (x<n )œÅc (xn | xc<n ).
(c,l)‚ààSÃÉ

c‚ààS

t‚ààTc (x1:n )

hence
Y
c‚ààS

Œ±c (x1:n ) ‚â•

Y
c‚ààS

Œ±c ()œÅc (xc1:n )

Y

t
t+1 ,

t‚ààTc (x1:n )

and the desired result follows by recalling that
{Tc (x1:n )}c‚ààS is a partition of {1, . . . , n}.

Proof. For any c ‚àà YÃÑK and l ‚â§ r(c), Œæc (x1:n ) ‚â•
Œ±c (x<n )œÅc (xn | xc<n ) and Œæc (x1:n ) ‚â• Œ≤c (x<n )zc,l,n . As
S is a proper suffix set, at any time step t at most one
c ‚àà SÃÉ ‚à™ S of effective length d ‚â§ `(S) matches x<t .
By recursively applying Lemma 2 to the right hand side
of Equation 3 for every c ‚àà SÃÉ, and keeping the left hand
side whenever c ‚àà S, we obtain
`(S)‚àí1

We now define the various kinds of decision points (e.g.
split only; split or skip) within the context tree.

Œæ (x1:n ) ‚â•

Definition 1. Let S be a proper suffix set over YÃÑ. A string
c ‚àà YÃÑ is a choice point in S whenever c is a strict suffix of
some c0 ‚àà S and c = xc0 , where x ‚àà X .

The result then follows since SÃÉ = ‚à™d=0

Definition 2. Let S be a proper suffix set over YÃÑ,
and for c ‚àà YÃÑ let ES (c) := {l ‚àà N :
?l c is a strict suffix of some c0 ‚àà S}. We call SÃÉ := {(c, l) :
c is a choice point in S, l ‚àà ES (c)} the set of choice nodes
of S.
Figure 5, for example, is described by S = {0 ? 0, 1 ?
0, 01, 11}, of which 0 and 1 are choice points. The set of
choice nodes corresponding to S is SÃÉ := {(0, 1), (1, 0)}.
Intuitively, these choice nodes correspond exactly to the
nodes with more than one child.
Definition 3. Let c := c1 c2 . . . cm ‚àà Y m . We define the
effective length of c as `(c) := |{i ‚àà {1, . . . m} : ci 6= ?}|
and for a set V of such strings define `(V) := maxc‚ààV `(c).
Recall that our aim is to bound ‚àí log Œæ (x
Q1:n ). Having
bounded the product of terms at the leaves, c‚ààS Œ±c (x1:n ),
we now derive a similar bound for the internal nodes.

Y

Y

1
n Œ≤c,l ()

d=0 (c,l)‚ààSÃÉd

Y

Œ±c (x<n )œÅc (xn | xc<n ),

c‚ààS
`(S)‚àí1

SÃÉd .

We are now in a position to bound the regret of Skip Context Tree Switching with respect to any skipping prediction suffix tree structure that can be obtained by pruning
the SkipCTS context tree.
Theorem 1. Let œàS denote a D ‚àà N bounded k-skip prediction suffix tree (S, Œò), where S is a proper suffix set
over YÃÑk and Œò := {œÅc }c‚ààS is a set of sequential probabilistic models inside the SkipCTS context tree. For any
x1:n ‚àà X n , the regret of SkipCTS run with parameters D
and K ‚â• k with respect to œàS is bounded as
Rn (ŒæS KIP CTS , {œàS }) ‚â§ [`(S) + 1] log n + ŒìK
D (S),
P
P
where ŒìK
D (S) := ‚àí
c‚ààS log Œ±c ().
(c,l)‚ààSÃÉ log Œ≤c,l ()‚àí
Proof. (Sketch) Beginning with Rn (ŒæS KIP CTS , {œàS }) we
apply LemmaQ
3, then Lemma 1, and finally simplify using
œàS (x1:n ) := c‚ààS œÅc (xc1:n ).

Skip Context Tree Switching

If the data is generated by some unknown prediction suffix
tree and the base estimators are KT estimators, the above
regret bound leads to a result that is similar to the regret
bound for CTS given by Veness et al. (2012), save for two
main differences. First, recall that CD , the collection of
models considered by CTS, is a subset of CD,K , the collection of models considered by SkipCTS (with equality only
when K = 0). Our bound thus covers a broader collection of models. Second, for a proper suffix set S defined
over X , i.e. a no skip prediction suffix tree, the description
length ŒìK
D (S) under SkipCTS with K > 0 is necessarily
larger than its CTS counterpart. While these differences
negatively affect our regret bound, we have seen in Section 3 that we should expect significant savings whenever
the data can be well-modelled by a small K-skip prediction suffix tree. We explore these issues further in the next
section.

4. Experiments
We tested the Skip Context Tree Switching on a series
of prediction problems. The first set of experiments uses
a popular data compression benchmark, while the second
set of experiments investigates performance on a diverse
set of structured image prediction problems taken from
an open source Reinforcement Learning test framework.
A reference implementation of SkipCTS is provided at:
http://github.com/mgbellemare/SkipCTS.
4.1. The Calgary Corpus
Our first experiment evaluated SkipCTS in a pure compression setting. Recall that any algorithm which sequentially
assigns probabilities to symbols can be used for compression by means of arithmetic coding (Witten et al., 1987). In
particular, given a model Œæ assigning a probability Œæ(x1:n )
to x1:n ‚àà X n , arithmetic coding is guaranteed to produce
a compressed file size of essentially ‚àí log2 Œæ(x1:n ).
We ran SkipCTS (with D = 48, K = 1) and CTS (with
D = 48) on the Calgary Corpus (Bell et al., 1989), an
established compression benchmark composed of 14 different files. The results, provided in Table 1, show that
SkipCTS performs significantly better than CTS on certain
files, and never suffers by more than a negligible amount.
Of interest, the files best improved by SkipCTS are those
which contain highly-structured binary data: GEO, OBJ 1,
and OBJ 2. For reference, we also included some CTW experiments, indicated by the CTW and and SkipCTW rows,
that measured the performance of skipping using the original recursive CTW weighting scheme; here we see that
the addition of skipping also helps. Table 1 also provides
results for CTW‚àó , an enhanced version of CTW for bytebased data (Willems, 2013). Here both CTS and SkipCTS
outperform CTW, with SkipCTS providing the best results

Figure 6. The game P ONG, in which the player controls the vertical position of a paddle in order to return a ball and score points.

overall. Finally, it is worth noting that averaged over the
Calgary Corpus, the bits per byte performance of SkipCTS
is superior (2.10 vs 2.12) to DEPLUMP (Gasthaus et al.,
2010), a state-of-the-art n-gram model. While SkipCTS is
consistently slightly worse for text data, it is significantly
better on binary data. It is also worth pointing out that no
regret guarantees are yet known for DEPLUMP.
4.2. Atari 2600 Frame Prediction
We also tested our algorithm on the task of video game
screen prediction. We used the Arcade Learning Environment (Bellemare et al., 2013a), an interface that allows
agents to interact with Atari 2600 games. Figure 6 depicts
the well-known P ONG, one of the Atari 2600‚Äôs flagship
games. In the Atari 2600 prediction setting, the alphabet X
is the set of all possible Atari 2600 screens. Because each
screen contains 160 √ó 210 7-bit pixels, it is both impractical and undesirable to learn a model which predicts each
xt ‚àà X atomically. Instead, we take a similar approach to
that of Bellemare et al. (2013b): we divide the screen into
16 √ó 16 blocks and predict each block atomically using
SkipCTS or CTS combined with the SAD estimator.
Each block prediction is made using a context composed
of the symbol value of neighbouring blocks at previous
timesteps, as well as the last action taken, for a total of
11 variables. In this setting, skipping irrelevant variables
is particularly important because of the high branching factor at each level. For example, when predicting the motion
of the opponent‚Äôs paddle in P ONG, SkipCTS can disregard
horizontally neighbouring blocks and the player‚Äôs action.
We trained SkipCTS with K = 0 and 1 on 54 Atari 2600
games. Each experiment consisted of 10 trials, each lasting
100,000 time steps, where one time step corresponds to 4
emulated frames. Each trial was assigned a specific random
seed which was used for all values for K. We report the
average log-loss per frame over the last 4500 time steps,
corresponding to 5 minutes of real-time Atari 2600 play.
Throughout our trials actions were selected uniformly at
random from each game‚Äôs set of legal actions.
The full table of results is provided as supplementary mate-

Skip Context Tree Switching
Table 1. Compression results on the Calgary Corpus, in average bits needed to encode each byte. Highlights indicate improvements
greater than 3% from CTW to SkipCTW and from CTS to SkipCTS, respectively. CTW* results are taken from Willems (2013).
File

bib

book1

book2

geo

news

obj1

obj2

paper1

paper2

pic

progc

progl

progp

trans

CTW*

1.83

2.18

1.89

4.53

2.35

3.72

2.40

2.29

2.23

0.80

2.33

1.65

1.68

1.44

CTW
S KIP CTW

2.25
2.15

2.31
2.32

2.12
2.10

5.01
3.91

2.78
2.77

4.63
4.57

3.19
2.96

2.84
2.75

2.59
2.54

0.90
0.90

3.00
2.91

2.11
2.00

2.24
2.08

2.09
1.83

Diff.

4.4%

-0.4%

0.9%

22.0%

0.4%

1.3%

7.2%

3.2%

1.9%

0.0%

3.0%

5.2%

7.1%

12.4%

CTS
S KIP CTS

1.81
1.75

2.20
2.20

1.90
1.89

4.18
3.60

2.34
2.34

3.66
3.40

2.36
2.19

2.28
2.26

2.23
2.22

0.79
0.76

2.33
2.30

1.61
1.59

1.64
1.61

1.39
1.35

Diff.

3.3%

0.0%

0.5%

13.9%

0.0%

7.1%

7.2%

0.9%

0.4%

3.8%

1.3%

1.2%

1.8%

2.9%

rial. For each game we computed the improvement in logloss per frame and determined whether the difference in
loss was statistically significant using the Wilcoxon signed
rank test. As a whole, SkipCTS achieved lower log-loss
than CTS in 54 out of 55 games; all these differences are
significant. While SkipCTS performed slightly worse in
E LEVATOR ACTION, the difference was not statistically
significant. The average overall log-loss improvement was
9.0% and the median, 8.25%; improvements ranged from 2% (E LEVATOR ACTION) to 36% (F REEWAY). SkipCTS
with K = 1 processed on average 34 time steps (136
frames) per second, corresponding to just over twice the
real-time speed of the Atari 2600. We further ran our
algorithm with K = 2 and observed an additional, significant increase in predictive performance on 18 games
(up to 21.7% over K = 1 for T IME P ILOT). On games
where K = 2 is unnecessary, however, the performance
of SkipCTS degraded somewhat. As discussed above, this
behaviour is an expected consequence of the larger ŒìK
D (S).

5. Discussion
We have seen that by allowing context trees to skip over
variables, SkipCTS can achieve substantially better performance over CTS in problems where a good variable ordering may not be known a priori. Theoretically we have seen
that SkipCTS can, in the extreme case, have exponentially
lower regret. Empirically we observe substantial benefits
in practice over state of the art lossless compression algorithms in problems involving highly structured data (e.g.
the GEO problem in the Calgary Corpus). The dramatic and
consistent improvement seen across over 50 Atari prediction problems indicate that SkipCTS is especially important in multi-dimensional prediction problems where issues
of variable ordering are naturally exacerbated.
The main drawback of SkipCTS is the increased computational complexity of inference as a result of the more
expressive model class. However, our experiments have
demonstrated that small values of K can make a substantial difference. Furthermore, the computational and
memory costs of SkipCTS can be alleviated in practice.

The tree structure induced by the recursive SkipCTS update (Equations 3‚Äì5) can naturally be parallelized, while
the SkipCTS memory requirements can easily be bounded
through hashing. Finally note that sampling from the model
remains a O(D) operation, so, for instance, planning with a
SkipCTS-based reinforcement learning model is nearly as
efficient as planning with a CTS-based model.
Tree-based models have a long history in sequence prediction, and the persistent issue of variable ordering has been
confronted in many ways. The main strengths of SkipCTS
are inherited from CTW ‚Äì efficient, incremental, and exact
Bayesian inference, and strong theoretical guarantees on
asymptotic regret. Other approaches with more representational flexibility lack these strengths. In the model based
reinforcement learning setting, some methods (e.g. McCallum, 1996; Holmes & Isbell, 2006; Talvitie, 2012) extend
the traditional predictive suffix tree by allowing variables
from different time steps to be added in any order, or by allowing the tree to excise portions of history, but these methods are not incremental and do not provide regret guarantees. Bayesian decision tree learning methods (e.g. Chipman et al., 1998; Lakshminarayanan et al., 2013) could in
principle be applied in the sequential prediction setting.
These typically allow arbitrary variable ordering, but require approximate inference to remain tractable.

6. Conclusion
In this paper we presented Skip Context Tree Switching, a
polynomial-time algorithm which efficiently mixes over sequences of prediction suffix trees that may skip over K contiguous runs of variables. Our results show that SkipCTS is
practical for small K and can produce significant empirical
improvements compared to members of the Context Tree
Weighting family (even with K = 1) in problems where
irrelevant variables are naturally present.

Acknowledgments
The authors would like to thank Alex Graves, Andriy Mnih
and Michael Bowling for some helpful discussions.

Skip Context Tree Switching

References
Bell, Timothy, Witten, Ian H., and Cleary, John G. Modeling for text compression. ACM Computing Surveys
(CSUR), 21(4):557‚Äì591, 1989.
Bellemare, Marc G., Naddaf, Yavar, Veness, Joel, and
Bowling, Michael. The Arcade Learning Environment:
An evaluation platform for general agents. Journal of
Artificial Intelligence Research, 47, June 2013a.
Bellemare, Marc G., Veness, Joel, and Bowling, Michael.
Bayesian learning of recursively factored environments.
In Proceedings of the Thirtieth International Conference
on Machine Learning, 2013b.
Chipman, Hugh A., George, Edward I., and McCulloch,
Robert E. Bayesian CART model search. Journal of
the American Statistical Association, 93(443):935‚Äì948,
1998.
Erven, Tim Van, Grunwald, Peter, and de Rooij, Steven.
Catching up faster in Bayesian model selection and
model averaging. In NIPS, 2007.
Gasthaus, Jan, Wood, Frank, and Teh, Yee Whye. Lossless
compression based on the sequence memoizer. In Data
Compression Conference (DCC), 2010.
Herbster, Mark and Warmuth, Manfred K. Tracking the
best expert. Machine Learning, 32(2):151‚Äì178, 1998.
Holmes, Michael P. and Isbell, Jr, Charles Lee. Looping
suffix tree-based inference of partially observable hidden
state. In Proceedings of the 23rd International Conference on Machine Learning, pp. 409‚Äì416, 2006.
Hutter, Marcus. Sparse adaptive Dirichlet-multinomial-like
processes. In Proceedings of the Conference on Learning
Theory (COLT), 2013.
Koolen, Wouter M. and de Rooij, Steven. Universal codes
from switching strategies. IEEE Transactions on Information Theory, 59(11):7168‚Äì7185, 2013.
Krichevsky, R. and Trofimov, V. The performance of universal encoding. IEEE Transactions on Information Theory, 27(2):199‚Äì207, 1981.
Lakshminarayanan, Balaji, Roy, Daniel M., and Teh,
Yee Whye. Top-down particle filtering for Bayesian decision trees. In Proceedings of the 30th International
Conference on Machine Learning, 2013.
McCallum, Andrew K. Reinforcement learning with selective perception and hidden state. PhD thesis, University
of Rochester, 1996.

Ron, Dana, Singer, Yoram, and Tishby, Naftali. The power
of amnesia: Learning probabilistic automata with variable memory length. Machine learning, 25(2):117‚Äì149,
1996.
Talvitie, Erik. Learning partially observable models using
temporally abstract decision trees. In Advances in Neural Information Processing Systems (25), 2012.
Tjalkens, Tj. J, Shtarkov, Y.M., and Willems, F.M.J. Context tree weighting: Multi-alphabet sources. In 14th
Symposium on Information Theory in the Benelux, pp.
128‚Äì135, 1993.
Veness, Joel, Ng, Kee Siong, Hutter, Marcus, Uther,
William T. B., and Silver, David. A Monte-Carlo AIXI
approximation. Journal of Artificial Intelligence Research, 40:95‚Äì142, 2011.
Veness, Joel, Ng, Kee Siong, Hutter, Marcus, and Bowling,
Michael H. Context tree switching. In Data Compression Conference (DCC), pp. 327‚Äì336, 2012.
Volf, P. Weighting techniques in data compression: Theory
and algorithms. PhD thesis, Eindhoven University of
Technology, 2002.
Willems, Frans M., Shtarkov, Yuri M., and Tjalkens,
Tjalling J. The context tree weighting method: Basic
properties. IEEE Transactions on Information Theory,
41:653‚Äì664, 1995.
Willems, Frans M., Shtarkov, Yuri M., and Tjalkens,
Tjalling J. Context weighting for general finite-context
sources. IEEE Transactions on Information Theory, 42
(5):1514‚Äì1520, 1996.
Willems, Frans M. J. CTW website. http://www.ele.
tue.nl/ctw/, 2013.
Witten, Ian H., Neal, Radford M., and Cleary, John G.
Arithmetic coding for data compression. Communications of the ACM, 30(6):520‚Äì540, 1987.

