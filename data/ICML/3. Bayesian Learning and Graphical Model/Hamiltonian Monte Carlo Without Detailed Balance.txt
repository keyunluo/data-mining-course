Hamiltonian Monte Carlo Without Detailed Balance
Jascha Sohl-Dickstein
Stanford University, Palo Alto. Khan Academy, Mountain View

JASCHA @ STANFORD . EDU

Mayur Mudigonda
MUDIGONDA @ BERKELEY. EDU
Redwood Institute for Theoretical Neuroscience, University of California at Berkeley
Michael R. DeWeese
Redwood Institute for Theoretical Neuroscience, University of California at Berkeley

Abstract
We present a method for performing Hamiltonian
Monte Carlo that largely eliminates sample rejection. In situations that would normally lead to
rejection, instead a longer trajectory is computed
until a new state is reached that can be accepted.
This is achieved using Markov chain transitions
that satisfy the fixed point equation, but do not
satisfy detailed balance. The resulting algorithm
significantly suppresses the random walk behavior and wasted function evaluations that are typically the consequence of update rejection. We
demonstrate a greater than factor of two improvement in mixing time on three test problems. We
release the source code as Python and MATLAB
packages.

1. Introduction
High dimensional and otherwise computationally expensive probabilistic models are of increasing importance for
such diverse tasks as modeling the folding of proteins
(SchuÃàtte & Fischer, 1999), the structure of natural images
(Culpepper et al., 2011), or the activity of networks of neurons (Cadieu & Koepsell, 2010).
Sampling from the described distribution is typically the
bottleneck when working with these probabilistic models.
Sampling is commonly required when training a probabilistic model, when evaluating the model‚Äôs performance,
when performing inference, and when taking expectations
(MacKay, 2003). Therefore, work that improves sampling
is fundamentally important.
The most common way to guarantee that a sampling algoProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

DEWEESE @ BERKELEY. EDU

rithm converges to the correct distribution is via a concept
known as detailed balance. Sampling algorithms based on
detailed balance are powerful because they allow samples
from any target distribution to be generated from almost
any proposal distribution, using for instance MetropolisHastings acceptance criteria (Hastings, 1970). However,
detailed balance also suffers from a critical flaw. Precisely because the forward and reverse transitions occur
with equal probability, detailed balance driven samplers go
backwards exactly as often as they go forwards. The state
space is thus explored via a random walk over distances
longer than those traversed by a single draw from the proposal distribution. A random walk only travels a distance
1
dN 2 in N steps, where d is the characteristic step length.
The current state-of-the-art sampling algorithm for probability distributions with continuous state spaces is Hamiltonian Monte Carlo (HMC) (Duane et al., 1987; Neal, 2010).
By extending the state space to include auxiliary momentum variables, and then using Hamiltonian dynamics to traverse long iso-probability contours in this extended state
space, HMC is able to move long distances in state space in
a single update step. However, HMC still relies on detailed
balance to accept or reject steps, and as a result still behaves
like a random walk ‚Äì just a random walk with a longer step
length. Previous attempts to address this have combined
multiple Markov steps that individually satisfy detailed balance into a composite step that does not (Horowitz, 1991),
with limited success (Kennedy & Pendleton, 1991).
The No-U-Turn Sampler (NUTS) sampling package (Hoffman & Gelman, 2011) and the windowed acceptance
method of (Neal, 1994) both consider Markov transitions
within a set of discrete states generated by repeatedly simulating Hamiltonian dynamics. NUTS generates a set of
candidate states around the starting state by running Hamiltonian dynamics forwards and backwards until the trajectory doubles back on itself, or a slice variable constraint is
violated. It then chooses a new state at uniform from the

Hamiltonian Monte Carlo Without Detailed Balance

candidate states. In windowed acceptance, a transition is
proposed between a window of states at the beginning and
end of a trajectory, rather than the first state and last state.
With the selected window, a single state is then chosen using Boltzmann weightings. Both NUTS and the windowed
acceptance method rely on detailed balance to choose the
candidate state from the discrete set.

fixed point equation (Equation 2). Detailed balance guarantees that if samples are drawn from the equilibrium distribution p (x), then for every pair of states x and x0 the
probability of transitioning from state x to state x0 is identical to that of transitioning from state x0 to x,

Here we present a novel discrete representation of the HMC
state space and transitions. Using this representation, we
derive a method for performing HMC while abandoning
detailed balance altogether, by directly satisfying the fixed
point equation restricted to the discrete state space. As a
result, random walk behavior in the sampling algorithm is
greatly reduced, and the mixing rate of the sampler is substantially improved.

By substitution for T (x0 |x) in the left side of Equation 2,
it can be seen that if Equation 3 is satisfied, then the fixed
point equation is also satisfied.

2. Sampling
We begin by briefly reviewing some key concepts related
to sampling. The goal of a sampling algorithm is to draw
characteristic samples x ‚àà RN from a target probability
distribution p (x). Without loss of generality, we will assume that p (x) is determined by an energy function E (x),
p (x) =

1
exp (‚àíE (x)) .
Z

(1)

2.1. Markov Chain Monte Carlo
Markov Chain Monte Carlo (MCMC) (Neal, 1993) is
commonly used to sample from probabilistic models. In
MCMC a chain of samples is generated by repeatedly
drawing new samples x0 from a conditional probability distribution T (x0 |x), where x is the previous
R sample. Since
T (x0 |x) is a probability density over x0 , T (x0 |x) dx0 =
1 and T (x0 |x) ‚â• 0.
2.2. Fixed Point Equation

p (x) T (x0 |x) = p (x0 ) T (x|x0 ) .

(3)

An appealing aspect of detailed balance is that a transition distribution satisfying it can be easily constructed
from nearly any proposal distribution, using MetropolisHastings acceptance/rejection rules (Hastings, 1970). A
primary drawback of detailed balance, and of MetropolisHastings, is that the resulting Markov chains always engage
in random walk behavior, since by definition detailed balance depends on forward and reverse transitions happening
with equal probability.
The primary advance in this paper is demonstrating how
HMC sampling can be performed without resorting to detailed balance.

3. Hamiltonian Monte Carlo
Hamiltonian Monte Carlo (HMC) can traverse long distances in state space with single Markov transitions. It
does this by extending the state space to include auxiliary
momentum variables, and then simulating Hamiltonian dynamics to move long distances along iso-probability contours in the expanded state space.
3.1. Extended state space
The state space is extended by the addition of momentum
variables v ‚àà RN , with identity-covariance Gaussian distribution,


1 T
‚àíN
2
exp ‚àí v v .
(4)
p (v) = (2œÄ)
2

An MCMC algorithm must satisfy two conditions in order to generate samples from the target distribution p (x).
The first is mixing, which requires that repeated application
of T (x0 |x) must eventually explore the full state space of
p (x). The second condition is that the target distribution
p (x) must be a fixed point of T (x0 |x). This second condition can be expressed by the fixed point equation,
Z
p (x) T (x0 |x) dx = p (x0 ) ,
(2)

We refer to the combined state space of x and v as Œ∂, such
that Œ∂ = {x, v}. The corresponding joint distribution is

which requires that when T (x0 |x) acts on p (x), the resulting distribution is unchanged.

1
H (Œ∂) = H (x, v) = E (x) + vT v.
2

‚àíN
2

(2œÄ)
p (Œ∂) = p (x, v) = p (x) p (v) =
Z

exp (‚àíH (Œ∂)) ,
(5)

2.3. Detailed Balance
Detailed balance is the most common way of guaranteeing
that the Markov transition distribution T (x0 |x) satisfies the

(6)

H (Œ∂) has the same form as total energy in a physical system, where E (x) is the potential energy for position x and
1 T
2 v v is the kinetic energy for momentum v (mass is set
to one).

Hamiltonian Monte Carlo Without Detailed Balance

In HMC samples from p (x) are generated by drawing samples from the joint distribution p (x, v), and using only the
x variables as samples from the desired distribution.

‚á£
F‚á£

Hamiltonian dynamics govern how physical systems
evolve with time. It might be useful to imagine the trajectory of a skateboarder rolling in an empty swimming pool.
As she rolls downwards she exchanges potential energy for
kinetic energy, and the magnitude of her velocity increases.
As she rolls up again she exchanges kinetic energy back for
potential energy. In this fashion she is able to traverse long
distances across the swimming pool, while at the same time
maintaining constant total energy over her entire trajectory.
In HMC, we treat H (Œ∂) as the total energy of a physical system, with spatial coordinate x, velocity v, potential
energy E (x), and kinetic energy 12 vT v. In an identical
fashion to the case of the skateboarder in the swimming
pool, running Hamiltonian dynamics on this system traverses long distances in x while maintaining constant total
energy H (Œ∂). By Equation 5, moving along a trajectory
with constant energy is identical to moving along a trajectory with constant probability.
Hamiltonian dynamics can be run exactly in reverse by reversing the velocity vector. They also preserve volume in
Œ∂. As we will see, all these properties together mean that
Hamiltonian dyamics can be used to propose update steps
that move long distances in state space while retaining high
acceptance probability.
3.3. Operators
The Markov transitions from which HMC is constructed
can be understood in terms of several operators acting on
Œ∂. These operators are illustrated in Figure 1a. This representation of the actions performed in HMC, and the corresponding state space, is unique to this paper and diverges
from the typical presentation of HMC.
3.3.1. M OMENTUM F LIP
The momentum flip operator F reverses the direction of
the momentum. It is its own inverse, leaves the total energy
unchanged, and preserves volume in state space:
FŒ∂ = F {x, v} = {x, ‚àív} ,
F

1

F‚á£

L‚á£

F‚á£

‚á£

LF‚á£

L

L‚á£

3.2. Hamiltonian dynamics

‚àí1

L

Œ∂ = FŒ∂,

H (FŒ∂) = H (Œ∂) ,





det ‚àÇFŒ∂  = 1.

‚àÇŒ∂ T 

(7)

‚á£

‚á£

1

‚á£

R( )‚á£

(a)

(b)

Figure 1. (a) The action of operators involved in Hamiltonian
Monte Carlo (HMC). The base of each red or green arrow represents the position x, and the length and direction of each of these
arrows represents the momentum v. The flip operator F reverses
the momentum. The leapfrog operator L approximately integrates
Hamiltonian dynamics. The trajectory taken by L is indicated by
the dotted line. The randomization operator R (Œ≤) corrupts the
momentum with an amount of noise that depends on Œ≤. (b) The
ladder of discrete states that are accessible by applying F and L
starting at state Œ∂. Horizontal movement on the ladder occurs by
flipping the momentum, whereas vertical movement occurs by integrating Hamiltonian dynamics.

3.3.2. L EAPFROG I NTEGRATOR
Leapfrog, or StoÃàrmer-Verlet, integration provides a discrete
time approximation to Hamiltonian dynamics (Hairer et al.,
2003). The operator L (, M ) performs leapfrog integration for M leapfrog steps with step length . For conciseness, L (, M ) will be written only as L,
(The state resulting from M steps of
LŒ∂ =

leapfrog integration of Hamiltonian
dynamics with step length .

(11)

Like exact Hamiltonian dynamics, leapfrog dynamics are
exactly reversible by reversing the velocity vector, and they
also exactly preserve volume in state space. L can be inverted by reversing the sign of the momentum, tracing out
the reverse trajectory, and then reversing the sign of the
momentum again so that it points in the original direction;
L‚àí1 Œ∂ = FLFŒ∂,





det ‚àÇLŒ∂  = 1.


T
‚àÇŒ∂

(12)
(13)

(8)
(9)
(10)

The momentum flip operator F causes movement between
the left and right sides of the state ladder in Figure 1b.

Unlike for exact dynamics, the total energy H (Œ∂) is only
approximately conserved by leapfrog integration, and the
energy accumulates errors due to discretization. This discretization error in the energy is the source of all rejections
of proposed updates in HMC.
The leapfrog operator L causes movement up the right side

Hamiltonian Monte Carlo Without Detailed Balance

of the state ladder in Figure 1b, and down the left side of
the ladder.
3.3.3. M OMENTUM R ANDOMIZATION
The momentum randomization operator R (Œ≤) mixes an
amount of Gaussian noise determined by Œ≤ ‚àà [0, 1] into
the velocity vector,
R (Œ≤) Œ∂ = R (Œ≤) {x, v} = {x, v0 } ,
p
p
v0 = v 1 ‚àí Œ≤ + n Œ≤,
n ‚àº N (0, I) .

(14)
(15)
(16)

Unlike the previous two operators, the momentum randomization operator is not deterministic. R (Œ≤) is however a
valid Markov transition operator for p (Œ∂) on its own, in
that it satisfies both Equation 2 and Equation 3.
The momentum randomization operator R (Œ≤) causes
movement off of the current state ladder and on to a new
state ladder.
3.4. Discrete State Space
As illustrated in Figure 1b, the operators L and F generate
a discrete state space ladder, with transitions only occurring
between Œ∂ and three other states. Note that every state on
the ladder can be represented many different ways, depending on the series of operators used to reach it. For instance,
the state in the upper left of the figure pane can be written
L‚àí1 FŒ∂ = FLŒ∂ = LFLLŒ∂ = ¬∑ ¬∑ ¬∑ .
Standard HMC can be viewed in terms of transitions on
this ladder. Additionally, we will see that this discrete state
space view allows Equation 2 to be solved directly by replacing the integral over all states with a short sum.
3.5. Standard HMC
HMC as typically implemented consists of the following
steps. Here, Œ∂ (t,s) represents the state at sampling step t,
and sampling substep s. Each numbered item below corresponds to a valid Markov transition for p (Œ∂), satisfying
detailed balance. A full sampling step consists of the composition of all three Markov transitions.

Metropolis-Hastings rules,


p (Œ∂ 0 )
,
(18)
œÄaccept = min 1,
p (Œ∂)

Œ∂0
with probability œÄaccept
Œ∂ (t,1) =
.
(t,0)
Œ∂
with probability 1 ‚àí œÄaccept
(19)
Note that since the transition FL is its own inverse, the forward and reverse proposal distribution probabilities cancel in the MetropolisHastings rule in Equation 18.
On rejection, the computations performed in
Equation 17 are discarded. In our new technique,
this will no longer be true.
2. Flip the momentum,
Œ∂ (t,2) = FŒ∂ (t,1) .

(20)

If the proposed update from Step 1 was accepted, then
this moves Œ∂ (t,1) from the left back to the right side
of the state ladder in Figure 1b, and prevents the trajectory from doubling back on itself. If the update was
rejected however, and Œ∂ (t,1) is already on the right side
of the ladder, then this causes it to move to the left side
of the ladder, and the trajectory to double back on itself.
Doubling back on an already computed trajectory is
wasteful in HMC, both because it involves recomputing nearly redundant trajectories, and because the
distance traveled before the sampler doubles back is
the characteristic length scale beyond which HMC explores the state space by diffusion.
3. Corrupt the momentum with noise,
Œ∂ (t+1,0) = R (Œ≤) Œ∂ (t,2) .

(21)

It is common to set Œ≤ = 1, in which case the momentum is fully randomized every sampling step. In
our experiments (Section 5) however, we found that
smaller values of Œ≤ produced large improvements in
mixing time. This is therefore a hyperparameter that
is probably worth adjusting1 .

4. Look Ahead HMC
1. (a) Generate a proposed update,
Œ∂ 0 = FLŒ∂ (t,0) .

Here we introduce an HMC algorithm that relies on
Markov transitions that do not obey detailed balance, but
(17)

On the state ladder in Figure 1b, this corresponds
to moving up one rung (L), and then moving
from the right to the left side (F).
(b) Accept or reject the proposed update using

1
One method for choosing Œ≤ (Culpepper et al., 2011) which
we have found to be effective is to set it such that it randomizes a
fixed fraction Œ± of the momentum per unit simulation time,
1

Œ≤ = Œ± M .

(22)

Hamiltonian Monte Carlo Without Detailed Balance

still satisfy the fixed point equation. This algorithm eliminates much of the momentum flipping that occurs on rejection in HMC, and as a result greatly reduces random walk
behavior. It also prevents the trajectory computations that
would typically be discarded on proposal rejection from being wasted. We call our algorithm Look Ahead Hamiltonian Monte Carlo (LAHMC).

2D Anisotropic Gaussian
1
HMC Œ≤=1
LAHMC Œ≤=1
HMC Œ≤=0.1
LAHMC Œ≤=0.1

Autocorrelation

0.8
0.6

4.1. Intuition

0.4
0.2
0
0

2
4
Gradient Evaluations

(a)

6
4

x 10

LAHMC can be understood in terms of a series of modifications of standard HMC. The net effect of Steps 1 and 2
in Section 3.5 is to transition from state Œ∂ into either state
LŒ∂ or state FŒ∂, depending on whether the update in Section
3.5 Step 1 was accepted or rejected.

100D Anisotropic Gaussian
1
HMC Œ≤=1
LAHMC Œ≤=1
HMC Œ≤=0.1
LAHMC Œ≤=0.1

Autocorrelation

0.8
0.6
0.4
0.2
0
0

1

(b)

2
3
Gradient Evaluations

4
4

x 10

2d Rough Well
1
HMC Œ≤=1
LAHMC Œ≤=1
HMC Œ≤=0.1
LAHMC Œ≤=0.1

Autocorrelation

0.8
0.6
0.4
0.2
0
0

(c)

2000

4000
6000
8000
Gradient Evaluations

In LAHMC, in situations that would correspond to a rejection in Step 1 of Section 3.5, we will instead attempt
to travel even farther by applying the leapfrog operator L
additional times. This section provides intuition for how
this update rule was discovered, and how it can be seen to
connect to standard HMC. A more mathematically precise
description will follow in the next several sections.

10000

Figure 2. Autocorrelation vs. number of function evaluations for
standard HMC (no momentum randomization, Œ≤ = 1), LAHMC
with Œ≤ = 1, persistent HMC (Œ≤ = 0.1), and persistent LAHMC
(Œ≤ = 0.1) for (a) a two dimensional ill-conditioned Gaussian,
(b) a one hundred dimensional ill-conditioned Gaussian, and (c) a
two dimensional well conditioned energy function with a ‚Äúrough‚Äù
surface. In all cases the LAHMC sampler demonstrates faster
mixing.

We wish to minimize the transitions into state FŒ∂. In
LAHMC we do this by replacing as many transitions from
Œ∂ to FŒ∂ as possible with transitions that instead go from Œ∂
to L2 Œ∂. This would seem to change the number of transitions into both state FŒ∂ and state L2 Œ∂, violating the fixed
point equation. However, the changes in incoming transitions from Œ∂ are exactly counteracted because the state
FL2 Œ∂ is similarly modified, so thatit makes fewer transi2
tions into the state L2 Œ∂ = F FL
Œ∂ , and more transitions

2
2
into the state FŒ∂ = L FL Œ∂ .
For some states, after this modification there will still be
transitions between the states Œ∂ and FŒ∂. In order to further
minimize these transitions, the process in the proceeding
paragraph is repeated for these remaining transitions and
the state L3 Œ∂. This process is then repeated again for states
L4 Œ∂, L5 Œ∂, etc, up to some maximum number of leapfrog
applications K.
4.2. Algorithm
LAHMC consists of the following two steps,
1. Transition to a new state by applying the leapfrog operator L between 1 and K ‚àà Z + times, or by applying
the momentum flip operator F,

Ô£±
(t,0)
with probability œÄL1 Œ∂ (t,0) 
Ô£¥
Ô£¥ LŒ∂
Ô£¥
Ô£¥
with probability œÄL2 Œ∂ (t,0)
Ô£≤ L2 Œ∂ (t,0)
(t,1)
¬∑¬∑¬∑
Œ∂
=
 .
Ô£¥
K (t,0)
(t,0)
Ô£¥
K
L
Œ∂
with
probability
œÄ
Œ∂
Ô£¥
L
Ô£¥

Ô£≥
FŒ∂ (t,0)
with probability œÄF Œ∂ (t,0)
(23)

Hamiltonian Monte Carlo Without Detailed Balance

Distribution
2d Gaussian
2d Gaussian
2d Gaussian
2d Gaussian
100d Gaussian
100d Gaussian
100d Gaussian
100d Gaussian
2d Rough Well
2d Rough Well
2d Rough Well
2d Rough Well

Sampler
HMC Œ≤ = 1
LAHMC Œ≤ = 1
HMC Œ≤ = 0.1
LAHMC Œ≤ = 0.1
HMC Œ≤ = 1
LAHMC Œ≤ = 1
HMC Œ≤ = 0.1
LAHMC Œ≤ = 0.1
HMC Œ≤ = 1
LAHMC Œ≤ = 1
HMC Œ≤ = 0.1
LAHMC Œ≤ = 0.1

FŒ∂
0.079
0.000
0.080
0.000
0.147
0.047
0.147
0.047
0.446
0.292
0.446
0.292

LŒ∂
0.921
0.921
0.920
0.921
0.853
0.852
0.853
0.852
0.554
0.554
0.554
0.554

L2 Œ∂
0
0.035
0
0.035
0
0.059
0
0.059
0
0.099
0
0.100

L3 Œ∂
0
0.044
0
0.044
0
0.035
0
0.035
0
0.036
0
0.036

L4 Œ∂
0
0.000
0
0.000
0
0.006
0
0.006
0
0.019
0
0.019

Table 1. A table showing the fraction of transitions which occurred to each target state for the conditions plotted in Figure 2. Note that
LAHMC has far fewer momentum flips than standard HMC.

Note that there is no longer a Metropolis-Hastings accept/reject step. The state update in Equation 23 is a
valid Markov transition for p (Œ∂) on its own.
2. Corrupt the momentum with noise in an identical fashion as in Equation 21,
Œ∂ (t+1,0) = R (Œ≤) Œ∂ (t,1) .

(24)

4.3. Transition Probabilities
We choose the probabilities œÄLa (Œ∂) for the leapfrog transitions from state Œ∂ to state La Œ∂ to be

X
œÄLa (Œ∂) = min 1 ‚àí
œÄLb (Œ∂) ,
(25)

4.4. Fixed Point Equation
We can substitute the transition rates from Section 4.3 into
the left side of Equation 2, and verify that they satisfy the
fixed point equation. Note that the integral over all states
is transformed into a sum over all source states from which
transitions into state Œ∂ might be initiated.
Z
dŒ∂ 0 p (Œ∂ 0 ) T (Œ∂|Œ∂ 0 )
X
Z
0
0
= dŒ∂ p (Œ∂ )
œÄLa (Œ∂ 0 ) Œ¥ (Œ∂ ‚àí La Œ∂ 0 )
(28)
a


+ œÄF (Œ∂ 0 ) Œ¥ (Œ∂ ‚àí FŒ∂ 0 ) ,

b<a
a

p (FL Œ∂)
p (Œ∂)

1‚àí

X

!
a
œÄLb (FL Œ∂) .

=

X





p L‚àía Œ∂ œÄLa L‚àía Œ∂ + p F‚àí1 Œ∂ œÄF F‚àí1 Œ∂ ,

a

(29)

b<a

Equation 25 greedily sets the transition probability œÄLa (Œ∂)
as large as possible, subject to the restrictions that the total
transition probability out of state Œ∂ not exceed 1, and that
the transition rate in the forward direction (Œ∂ ‚Üí La Œ∂) not
exceed the transition rate in the reverse direction (FLa Œ∂ ‚Üí
FŒ∂). Some algebra shows that under these transition probabilities
a

a

p (Œ∂) œÄLa (Œ∂) = p (FL Œ∂) œÄLa (FL Œ∂) .

(26)

Any remaining unassigned probability is assigned to the
momentum flip transition,
X
œÄF (Œ∂) = 1 ‚àí
œÄLa (Œ∂) .
(27)
a

Note that transitions will be performed in a greedy fashion.
It is only necessary to compute the state La Œ∂ and the transition probability œÄLa (Œ∂) if none of the transitions to states
Lb Œ∂, for b < a, have been taken.

=

X

a

a

p (FL FŒ∂) œÄLa (FL FŒ∂) + p (FŒ∂) œÄF (FŒ∂) ,

a

(30)
=

X

p (FŒ∂) œÄLa (FŒ∂) + p (FŒ∂) œÄF (FŒ∂) ,

(31)

a

"
= p (FŒ∂)

#
X

œÄLa (FŒ∂) + œÄF (FŒ∂) ,

(32)

a

= p (Œ∂) .

(33)

5. Experimental Results
As illustrated in Figure 2, we compare the mixing time for
our technique and standard HMC on three distributions.
HMC and LAHMC both had step length and number of
leapfrog steps set to  = 1, and M = 10. Values of Œ≤ were
set to 1 or 0.1 as stated in the legend. For LAHMC the maximum number of leapfrog applications was set to K = 4.
In all cases, LAHMC outperformed standard HMC for the

Hamiltonian Monte Carlo Without Detailed Balance
LAHMC, Œ≤=1

HMC, Œ≤=1
5000

LAHMC, M=10
5000

5000

0.7

0.7

0.8

0.8

0.8

0.9
1.0

1.0

3000

3000

1.1

1.2

2000

1.3

1.2

2000

1.0

3000

1.1

3000

1.1

1.2

Œµ

Œµ

1.1

4000

0.9

4000

0.9

1.0

0.8

4000

Œµ

0.9

0.7

Œµ

4000

5000

0.7

HMC, M=10

2000

1.3

1.2

2000

1.3

1.3

1.4

1000

1.5

1.4

1.4

1000

1.5

1.6
.01 .11 .21 .31 .41
Œ≤

(a)

0

1000

1.5

.01 .11 .21 .31 .41
Œ≤

0

(b)

1000

1.5

1.6

1.6

1.4

1

10 50 100
Leap Steps

0

(c)

1.6
1

10 50 100
Leap Steps

0

(d)

Figure 3. Images illustrating mixing time as a function of HMC hyperparameters for a two dimensional ill-conditioned Gaussian distribution. Pixel intensity indicates the number of gradient evaluations required to reach an autocorrelation of 0.5. LAHMC always
outperforms HMC for the same hyperparameter settings. (a) LAHMC as a function of  and Œ≤, for fixed M = 10, (b) HMC as a
function of  and Œ≤, for fixed M = 10, (c) LAHMC as a function of  and M , for fixed Œ≤ = 1, (d) HMC as a function of  and M , for
fixed Œ≤ = 1.

same setting of hyperparameters, often by more than a factor of 2.

ance of the Gaussian are 1 and 105 in Figure 3, rather than
1 and 106 as in Figure 2a.

The first two target distributions are 2 and 100 dimensional
ill-conditioned Gaussian distributions. In both Gaussians,
the eigenvalues of the covariance matrix are log-linearly
distributed between 1 and 106 .

MATLAB
and
Python
implementations
of
LAHMC are available at http://github.com/
Sohl-Dickstein/LAHMC. Figure 2 and Table 1
can be reproduced by running generate figure 2.m or
generate figure 2.py.

The final target distribution was chosen to demonstrate
that LAHMC is useful even for well conditioned distributions. The energy function used was the sum of an isotropic
quadratic and sinusoids in each of two dimensions,





1
œÄx1
œÄx2
2
2
E (x) =
x + x2 + cos
+ cos
,
2œÉ12 1
œÉ2
œÉ2
(34)
where œÉ1 = 100 and œÉ2 = 2. Although this distribution is well conditioned the sinusoids cause it to have a
‚Äúrough‚Äù surface, such that traversing the quadratic well
while maintaining a reasonable discretization error requires
many leapfrog steps.
The fraction of the sampling steps resulting in each possible
update for the samplers and energy functions in Figure 2 is
illustrated in Table 1. The majority of momentum flips in
standard HMC were eliminated by LAHMC. Note that the
acceptance rate for HMC with these hyperparameter values is reasonably close to its optimal value of 65% (Neal,
2010).
Figure 3 shows several grid searches over hyperparameters for a two dimensional ill-conditioned Gaussian,
and demonstrates that our technique outperforms standard
HMC for all explored hyperparameter settings. Due to
computational constraints, the eigenvalues of the covari-

6. Future Directions
There are many powerful variations on standard HMC that
are complementary to and could be combined naturally
with the present work. These include Riemann manifold
HMC (Girolami et al., 2011), quasi-Newton HMC (Zhang
& Sutton, 2011), Hilbert space HMC (Beskos & Pinski,
2011), shadow Hamiltonian methods (Izaguirre & Hampton, 2004), parameter adaptation techniques (Wang et al.,
2013), Hamiltonian annealed importance sampling (SohlDickstein & Culpepper, 2012), split HMC (Shahbaba et al.,
2011), and tempered trajectories (Neal, 2010).
It should be possible to further reduce random walk behavior by exploring new topologies and allowed state transitions. Two other schemes have already been explored,
though with only marginal benefit. In one scheme as many
flips as possible are replaced by identity transitions. This is
described in the note (Sohl-Dickstein, 2012). In a second
scheme, a state space is constructed with two sets of auxiliary momentum variables, and an additional momentumswap operator which switches the two momenta with each
other is included in the allowed transitions. In this scenario, in situations that would typically lead to momentum
flipping, with high probability the two sets of momenta can

Hamiltonian Monte Carlo Without Detailed Balance

instead be exchanged with each other. This leads to momentum randomization on rejection, rather than momentum reversal. Unfortunately, though this slightly improves
mixing time, it still amounts to a random walk on a similar length scale. The exploration of other topologies and
allowed transitions will likely prove fruitful.

Horowitz, AM. A generalized guided Monte Carlo algorithm. Physics Letters B, 1991.

Any deterministic, reversible, discrete stepped trajectory
through a state space can be mapped onto the ladder structure in Figure 1. The Markov transition rules presented
in this paper could therefore be applied to a wide range
of problems. All that is required in addition to the mapping is an auxiliary variable indicating direction along that
trajectory. In HMC, the momentum variable doubles as a
direction indicator, but there could just as easily be an additional variable d ‚àà {‚àí1, 1}, p (d = 1) = 21 , which indicates whether transitions are occurring up or down the
ladder. The efficiency of the exploration then depends only
on choosing a sensible, approximately energy conserving,
trajectory.

Kennedy, AD and Pendleton, B. Acceptances and autocorrelations in hybrid Monte Carlo. Nuclear Physics BProceedings Supplements, 1991.

Izaguirre, JA and Hampton, SS. Shadow hybrid Monte
Carlo: an efficient propagator in phase space of macromolecules. Journal of Computational Physics, 2004.

MacKay, DJC. Information theory, inference and learning
algorithms. 2003.
Neal, Radford M. MCMC using Hamiltonian dynamics.
Handbook of Markov Chain Monte Carlo, January 2010.
Neal, RM. Probabilistic inference using Markov chain
Monte Carlo methods. Technical Report CRG-TR-93-1,
Dept. of Computer Science, University of Toronto, 1993.

References

Neal, RM. An improved acceptance procedure for the hybrid Monte Carlo algorithm. Journal of Computational
Physics, 1994.

Beskos, A and Pinski, FJ. Hybrid monte carlo on hilbert
spaces. Stochastic Processes and their Applications,
2011.

SchuÃàtte, C and Fischer, A. A direct approach to conformational dynamics based on hybrid Monte Carlo. Journal
of Computational Physics, 1999.

Cadieu, CF and Koepsell, K. Phase coupling estimation
from multivariate phase statistics. Neural computation,
2010.

Shahbaba, B, Lan, S, Johnson, WO, and Neal, RM. Split
hamiltonian monte carlo. Statistics and Computing,
2011.

Culpepper, Benjamin J, Sohl-Dickstein, Jascha, and Olshausen, Bruno A. Building a better probabilistic model
of images by factorization. International Conference on
Computer Vision, 2011.

Sohl-Dickstein, Jascha. Hamiltonian Monte Carlo with Reduced Momentum Flips. arXiv:1205.1939v1, May 2012.

Duane, S, Kennedy, AD, Pendleton, BJ, and Roweth, D.
Hybrid monte carlo. Physics letters B, 1987.
Girolami, Mark, Calderhead, Ben, and Chin, Siu A. Riemann manifold Langevin and Hamiltonian Monte Carlo
methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2):123‚Äì214, March
2011. ISSN 13697412. doi: 10.1111/j.1467-9868.2010.
00765.x.
Hairer, E, Lubich, C, and Wanner, G. Geometric numerical integration illustrated by the Stormer-Verlet method.
Acta Numerica, 2003.
Hastings, W. Monte Carlo sampling methods using Markov
chains and their applications. Biometrika, January 1970.
Hoffman, MD and Gelman, A. The No-U-Turn Sampler:
Adaptively Setting Path Lengths in Hamiltonian Monte
Carlo. Arxiv preprint arXiv:1111.4246, pp. 1‚Äì30, 2011.

Sohl-Dickstein, Jascha and Culpepper, Benjamin J. Hamiltonian Annealed Importance Sampling for partition
function estimation. arXiv:1205.1925v1, May 2012.
Wang, Z, Mohamed, S, and Nando, D. Adaptive Hamiltonian and Riemann Manifold Monte Carlo. Proceedings
of the 30th International Conference on Machine Learning (ICML-13), 2013.
Zhang, Y and Sutton, C. Quasi-Newton Markov chain
Monte Carlo. 2011.

