Double Nyström Method:
An Efficient and Accurate Nyström Scheme for Large-Scale Data Sets

Woosang Lim
School of Computing, KAIST, Daejeon, Korea
Minhwan Kim
LG Electronics, Seoul, Korea

QUASAR 17@ KAIST. AC . KR

MINHWAN 1. KIM @ LGE . COM

Haesun Park
HPARK @ CC . GATECH . EDU
School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA
Kyomin Jung
Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea

Abstract
The Nyström method has been one of the most
effective techniques for kernel-based approach
that scales well to large data sets. Since its introduction, there has been a large body of work
that improves the approximation accuracy while
maintaining computational efficiency. In this paper, we present a novel Nyström method that
improves both accuracy and efficiency based on
a new theoretical analysis. We first provide a
generalized sampling scheme, CAPS, that minimizes a novel error bound based on the subspace
distance. We then present our double Nyström
method that reduces the size of the decomposition in two stages. We show that our method is
highly efficient and accurate compared to other
state-of-the-art Nyström methods by evaluating
them on a number of real data sets.

1. Introduction
Low-rank matrix approximation is one of the core techniques to mitigate the space requirement that arises in
large-scale machine learning and data mining. Consequently, many methods in machine learning involve a lowrank approximation of matrices that represent data, such as
manifold learning (Fowlkes et al., 2004; Talwalkar et al.,
2008), support vector machines (Fine & Scheinberg, 2002)
and kernel principal component analysis (Zhang et al.,
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

KJUNG @ SNU . AC . KR

2008). These methods typically involve spectral decomposition of a symmetric positive semi-definite (SPSD) matrix, but its exact computation is prohibitively expensive for
large data sets.
The standard Nyström method (Williams & Seeger, 2001)
is one of the popular methods for approximate spectral decomposition of a large kernel matrix K ∈ Rn×n (generally
a SPSD matrix), due to its simplicity and efficiency (Kumar et al., 2009). One of the main characteristics of the
Nyström method is that it uses samples to reduce the original problem of decomposing the given n × n kernel matrix
to the problem of decomposing a s × s matrix, where s
is the number of samples much smaller than n. The standard Nyström method has time complexity O(ksn + s3 )
for rank-k approximation and is indeed scalable. However,
the accuracy is typically its weakness, and there have been
many studies to improve its accuracy. Most of the recent
work in this line of research can be roughly categorized
into the following two types of approaches:
Refining Decomposition Exemplar works in this category
are one-shot Nyström method (Fowlkes et al., 2004), modified Nyström method (Wang & Zhang, 2013), ensemble Nyström method (Kumar et al., 2012) and standard
Nyström method using randomized SVD (Li et al., 2015).
All of these methods redefine the intersection matrix that
appears in the reconstructed form of the input matrix, of
which we provide details in Section 2.
The motivation of the one-shot Nyström method (Fowlkes
et al., 2004) is to obtain an orthonormal set of approximate
eigenvectors of the given kernel matrix via diagonalization
of the standard Nyström approximation in one-shot with
O(s2 n) running time. Because of the orthonormality, the

Double Nyström Method: An Efficient and Accurate Nyström Scheme for Large-Scale Data Sets

one-shot Nyström is widely used for kernel PCA (Zhang
et al., 2008), however there is no more elaborate analysis for it except the work of Fowlkes et al. (2004). The
modified Nyström method (Wang & Zhang, 2013) involves
multiplying both sides of kernel matrix by projection matrix consisting of orthonormal basis of subspace spanned
by samples. It solves the problem of minimizing matrix reconstruction error, which is minU kK − CUC> kF , given
the kernel matrix K and the C, where C is a submatrix
consisting of ` columns of K. Although the modified
Nyström approximation is more accurate than the standard
Nyström approximation, it is more expensive to compute
and is not able to compute a rank-k approximation. Its time
complexity is O(s2 n + sn2 ), and the latter term O(sn2 )
comes from some matrix multiplications and dominates
O(s2 n). The ensemble Nyström method (Kumar et al.,
2012) that takes a mixture of t (≥ 1) standard Nyström
approximations, and is more accurate than the standard
Nyström approximations in the empirical results. Its time
complexity is O(ksnt + s3 t + µ), where µ is the cost
of computing the mixture weights. To obtain more efficiency, we can adopt randomized SVD to approximate the
pseudo inverse of the intersection matrix of the standard
Nyström approximation (Li et al., 2015). Its time complexity is O(ksn), but it needs larger samples than other
Nyström methods due to adopting approximate SVD.
Improving Sampling The Nyström methods require column/row samples which heavily affects the accuracy.
Among many sampling strategies for Nyström methods,
uniform sampling without replacement is the most basic
sampling strategy (Williams & Seeger, 2001), of which
probabilistic error bounds for the standard Nyström method
are recently derived (Kumar et al., 2012; Gittens & Mahoney, 2013).
Recent work includes the non-uniform samplings, which
are square of diagonal sampling (Drineas & Mahoney,
2005), square of L2 column norm sampling (Drineas
& Mahoney, 2005), leverage score sampling (Mahoney
& Drineas, 2009) and approximate leverage score sampling (Drineas et al., 2012; Gittens & Mahoney, 2013).
The adaptive sampling strategies also have been studied,
e.g. (Deshpande et al., 2006; Kumar et al., 2012). Some
of the heuristic sampling strategies for Nyström methods
are utilizing normal K-means algorithm (Zhang & Kwok,
2010) with K = s, and adopting pseudo centroids of normal (weighted) K-means (Hsieh et al., 2014). Normal Kmeans sampling clusters the original data which is not applied kernel function, and uses s centroids to generate matrices consisting of kernel function values among all data
points and s centroids to perform Nyström methods. Although it has been shown to give good empirical accuracy,
its proposed analysis is quite loose and does not show any
connection to the minimum error of rank-k approximation.

1.1. Our Contributions
In this paper, we propose a novel Nyström method, Double
Nyström Method, which tightly integrates the strengths of
both types of approaches. Our contribution can be appreciated in three aspects: comprehensive analysis of the oneshot Nyström method, generalization of sampling methods,
and integration of our two results. Summary of those are
described as follows.
1.1.1. C OMPREHENSIVE A NALYSIS OF THE O NE - SHOT
N YSTR ÖM M ETHOD
In Section 3, we provide an analysis that one-shot
Nyström is quite a good compromise between the standard Nyström method and the modified Nyström method,
since it yields accurate rank-k approximations for k < s
(Thm 1). In addition, we show that it is robust (Proposition 1).
1.1.2. G ENERALIZATION OF S AMPLING M ETHODS
In Section 4, we investigate how we can improve accuracies of Nyström methods. First, we present new upper error
bounds both for the standard and one-shot Nyström methods (Thm 3), and provide a generalized view of sampling schemes which makes connection among some of the
sampling schemes and minimization of our error bounds
(Rem 1). Next, we propose Capturing Approximate Principal Subspace (CAPS) algorithm which minimizes our upper error bounds efficiently (Proposition 2).
1.1.3. T HE D OUBLE N YSTR ÖM M ETHOD
In Section 5, we propose Double Nyström Method (Alg 3)
that combines the advantages of CAPS sampling and
the one-shot Nyström method. It reduces the size of
the decomposition problem in twice, and consequently is
much more efficient than the standard Nyström method
for large data sets, but is as accurate as the one-shot
Nyström method. Its time complexity is also comparable
to the running time of standard Nyström method using randomized SVD, since it is O(`sn + m2 s) and linear for s,
where ` ≤ m  s  n.

2. Preliminaries
Given the data set X = {x1 , . . . , xn } and its corresponding
matrix X ∈ Rd0 ×n , we define the kernel function without
explicit feature mapping φ as κ(xi , xj ), where φ is a feature mapping such that φ : X → Rd . The corresponding
kernel matrix K ∈ Rn×n is a positive semi-definite (PSD)
matrix with elements κ(xi , xj ). Without loss of generality,
let Φ = φ(X) ∈ Rd×n be the matrix constructed by taking
φ(xi ) as the i-th column, so that K = Φ> Φ (Drineas &
Mahoney, 2005). We assume that rank(Φ) = r, which in

Double Nyström Method: An Efficient and Accurate Nyström Scheme for Large-Scale Data Sets

turn implies rank(K) = r.
Consider the compact singular value decomposition (com>
pact SVD) 1 Φ = UΦ,r ΣΦ,r VΦ,r
, where ΣΦ,r is the
diagonal matrix consisting of r nonzero singular values
(σ(Φ)1 , . . . , σ(Φ)r ) of Φ in decreasing order, and UΦ,r ∈
Rd×r and VΦ,r ∈ Rn×r are the matrices consisting of the
left and right singular vectors, respectively. Especially, we
simply denote compact SVD of Φ as Φ = Ur Σr Vr> , and
(σ(Φ)1 , . . . , σ(Φ)r ) as (σ1 , . . . , σr ) in this paper. Then,
we can obtain the compact SVD K = Vr Σ2r Vr> and its
>
pseudo-inverse obtained by K† = Vr Σ−2
r Vr . The best
rank-k (with k ≤ r) approximation of K can be obtained
from its SVD by
Kk = Vk Σ2k Vk> =

Pk

i=1

λi (K)vi vi> ,

where λi (K) = σi2 are the first k eigenvalues of K. Especially we simply denote again λi (K) = λi in this paper,
thus λi = σi2 .
Given a set W = {w1 , . . . , ws } of s mapped samples of
X (i.e. wi = φ(xj ) for some 1 ≤ j ≤ n), let W ∈ Rd×s
denote the matrix consisting of wi as the i-th column, and
C = Φ> W ∈ Rn×s the inner product matrix of the whole
data instances and the samples. Since the kernel matrix
KW ∈ Rs×s for the subset W is KW = W> W, we can
rearrange the rows and columns of K such that




KW
KW K>
21
K=
and C =
.
(1)
K21
K21 K22
In the later part of the paper, we will generalize the samples wi to arbitrary vectors of dimension d, not necessarily
mapped vectors of some data instances.
2.1. The Standard Nyström Method
The standard Nyström method for approximating the kernel
matrix K using the subset W of s sample data instances
yields a rank-s0 approximation matrix
†
>
K̃nys = K̃nys
s0 = CKW C ≈ K,

K†W

where
is the pseudo-inverse of KW and s0 =
rank(W). The rank-k approximation matrix (with k ≤ s0 )
is computed by
K̃nys
= CK†W,k C> ,
k
where K†W,k is the pseudo-inverse of KW,k , the best rankk approximation of KW , which can be computed from the
>
SVD KW,k = VW,k Σ2W,k VW,k
. The time complexity of
nys
3
computing K̃k is O(ksn + s ).
1
In this paper, we use compact SVD instead of full SVD unless
we give a particular mention.

The Nyström method is also used to compute the first k
2
approximate eigenvalues (Σ̃nys
k ) and the corresponding
nys
k approximate eigenvectors (Ṽk ) of the kernel matrix
>
K. Using SVD KW,k = VW,k Σ2W,k VW,k
, the eigenvalues and eigenvectors are computed by
r
n
s
nys 2
nys
2
CVW,k Σ−2
(Σ̃k ) = (ΣW,k ) and Ṽk =
W,k ,
s
n
(2)
However in general, K̃nys
is not the best rank-k approxik
mation of K̃nys , nor the eigenvectors Ṽknys are orthogonal
even though K is symmetric.
2.2. The One-Shot Nyström Method
A straightforward way to obtain the best rank-k approximation of K̃nys would be a two-stage computation, where
we first construct the full K̃nys and then reduce its rank
via SVD. This approach is costly, since its time complexity
amounts to performing SVD on the original matrix K.
The one-shot Nyström method (Fowlkes et al., 2004),
shown in Alg 1, computes the SVD in a single pass, and
thus possesses the following nice property:
Lemma 1 (Fowlkes et al., 2004) The Nyström method using a sample set W of s vectors can be decomposed as
†
>
>
K̃nys = K̃nys
s0 = CKW C = GG ,
0

n×s
with s0 = rank(KW ) and G = CVW Σ−1
.
W ∈ R
>
The one-shot Nyström method computes SVD G G =
>
, and computes s0 eigenvectors of K̃nys by
VG Σ2G VG
osn
Ṽs0 = GVG Σ−1
G . Consequently, we obtain the same
result as the Nyström method for the rank-s0 approximation
2
osn >
K̃osn = Ṽsosn
= GG> = K̃nys ,
0 ΣG (Ṽs0 )

but better yet, the best rank-k (with k < s0 ) approximation
K̃osn
= Ṽkosn Σ2G,k (Ṽkosn )> = (K̃nys )k 6= K̃nys
k
k .
The time complexity of the one-shot Nyström method is
O(s2 n) if the sample set W is a mapped samples of the
data set, i.e. W = {wi |wi = φ(xj ) for some 1 ≤ j ≤
n}. This is the case when the sample selection matrix P
is a binary matrix with a single one per column. In the
remainder of the paper, we will use the notation Σ̃osn
for
k
ΣG,k to emphasize that it is obtained from the one-shot
Nyström method.

3. The One-Shot Nyström Method: The
Optimal Sample-based KPCA
As reviewed in the previous section, the one-shot
Nyström method computes the best rank-k approximation

Double Nyström Method: An Efficient and Accurate Nyström Scheme for Large-Scale Data Sets

Algorithm 1 The One-shot Nyström method
Input: Matrix Ps ∈ Rn×s representing the composition
of s sample points such that W = ΦPs ∈ Rd×s with
rank(W) = s0 , kernel function κ
Output: Approximate kernel matrix K̃osn
k , its singular
osn
osn 2
vectors Ṽk and singular values (Σ̃k )
1: Obtain KW = W> W
>
2: Perform compact SVD KW = VW ΛW VW
=
>
VW Σ2W VW
3: Compute G> G, where G = CVW Σ−1
W
4: Compute VG,k the first k singular vectors of G> G
and corresponding singular values Σ2G,k
osn
5: Σ̃osn
= ΣG,k , Ṽkosn = GVG,k Σ−1
=
k
G,k and K̃k
>
GVG,k VG,k
G>

Definition 1 For the given samples W, sample-based
KPCA problem is defined as

of K̃nys to obtain orthonormal approximate eigenvectors
Ṽk of K for the given samples W. Here, we suggest
that the one-shot Nyström method can be used for computing optimal solutions to other closely related problems,
such as kernel principal component analysis (KPCA). In
this section, we make a formal statement that the one-shot
Nyström method provides an optimal KPCA for the given
W, which we build on in later sections.

Ũosn
= UW VG,k ,
k

minimize NRE(Ũk ) subject to Ũ>
k Ũk = Ik , Ũk = WAk .
Ak

(5)
The following lemma provides two types of the approximate principal directions Ũk , which are computed from the
standard Nyström method and one-shot Nyström method.
Lemma 2 In standard Nyström method, approximate principal directions are
Ũnys
= UW,k ,
k

(6)

>
where W = UW ΣW VW
. In the one-shot Nyström
method, approximate principal directions are

(7)

>
>
where G = Φ> WVW Σ−1
W = Φ UW and G G =
>
VG ΣG VG .

The following theorem states that the one-shot
Nyström method can be used to solve the optimization problem defined in Def 1.

We start with the observation that the most scalable KPCA
algorithms are based on a set of samples (Frieze et al.,
1998; Williams & Seeger, 2001) can be seen as computing k approximate eigenvectors of K, given by

Theorem 1 Given the s samples W ∈ Rd×s with
rank(W) = s0 , KPCA using the one-shot Nyström method
solves the optimization problem in Def 1.

Ṽk = CAk Σ̃−1
k ,

Given samples W, we proved that the one-shot Nyström
method minimizes the NRE(Ũk ) in Eqn (5) in Def 1. To
give more intuition for it, we introduce the sum of eigenvalue errors which is closely related with the NRE(Ũk ).

(3)

where C = Φ> W ∈ Rn×s is the inner product matrix
among the whole data set and the sample vectors (Eqn (1)),
Ak ∈ Rs×k is the algorithm-dependent coefficient matrix
for each pair of sample vector and principal direction, and
Σ̃k is the diagonal matrix of the first k approximate singular values. This view generalizes (Kumar et al., 2009).
To facilitate the analysis, we first reformulate Eqn (3) as
Ṽk = Φ> Ũk Σ̃−1
k ,

(4)

where Ũk = WAk ∈ Rd×k denotes k approximate principal directions in the feature space. Using the reconstruction
error (RE) and the normalized reconstruction error (NRE)
for KPCA (Günter et al., 2007)
RE(Ũk ) = kΦ − Ũk Ũ>
k ΦkF
NRE(Ũk ) =

kΦ − Ũk Ũ>
k ΦkF
kΦkF

as the objective functions, and observing that Ak determines the k approximate principal directions Ũk , we can
formulate KPCA based on samples W as an optimization
problem:

Definition 2 Given k approximate principal directions
Ũk ∈ Rd×k of Φ such that Ũ>
k Ũk = Ik , the sum of eigenvalue errors from Ũk is defined as
>
>
>
1 (Ũk ) = tr(U>
k ΦΦ Uk ) − tr(Ũk ΦΦ Ũk ),

where Uk is the matrix consisting of true principal directions as columns.
With the notion in Def 2, we can directly give a following
corollary.
Corollary 1 Minimizing the NRE(Ũk ) is equivalent to
minimizing the 1 (Ũk ) defined in Def 2, thus
Ũosn
= argmin 1 (Ũk ) s.t. Ũ>
k
k Ũk = Ik , Ũk = WAk .
Ũk

Additional to Thm 1 and Cor 1, we show that outputs of
the one-shot Nyström method depend only on subspace
spanned by input samples.

Double Nyström Method: An Efficient and Accurate Nyström Scheme for Large-Scale Data Sets

Proposition 1 Let W1 and W2 be the matrix consisting
of s1 samples and s2 samples respectively. If two column
spaces col(W1 ) and col(W2 ) are the same, then the outputs of the one-shot Nyström method are also the same regardless of difference between set of samples.
We note that the standard Nyström method does not satisfy
the robustness discussed in Proposition 1.

4. A Generalized View of Sampling Schemes
Motivated by Proposition 1, in this section, we provide new
upper error bounds of the Nyström method based on subspace distance, and suggest a generalized view of sampling
schemes for the Nyström method.
4.1. Error Analysis based on Subspace Distance
To provide new upper error bounds of the Nyström methods, our motivation is using the measure called “subspace
distance” which can evaluate the difference between two
subspaces (Wang et al., 2006; Sun et al., 2007).
Basically, the subspace distance of two subspaces depends
on the notion of projection error, hence we discuss the projection error first.
Definition 3 Given two matrices U and V consisting of
orthonormal vectors, i.e., U> U = I and V> V = I, the
projection error of U onto col(V) is defined as
PE(U, V) = kU − VV> UkF .
Since any linear subspace can be represented by its orthonormal basis, subspace distance can be defined by the
projection error between set of orthonormal vectors.
Definition 4 (Wang et al., 2006; Golub & Van Loan, 2012)
Given k dimensional subspace S1 and k dimensional subspace S2 , the subspace distance d(S1 , S2 ) is defined as
d(S1 , S2 ) = PE(Uk , Ũk ) = kUk − Ũk Ũ>
k U k kF ,
where Uk is an orthonormal basis of S1 and Ũk is an orthonormal basis of S2 .
Lemma 3 (Wang et al., 2006; Sun et al., 2007; Golub &
Van Loan, 2012) The subspace distances defined in Def 4
are invariant to the choice of orthonormal basis.
Remind that the standard Nyström and one-shot
Nyström methods satisfy K̃k
= Ṽk Σ̃2k Ṽk
=
>
>
Φ Ũk Ũk Φ, and those are characterized by Ũnys
k
as proved in Lem 2. Therefore, we provide a
and Ũosn
k
following error analysis based on the subspace distance
between col(Uk ) and col(Ũk ).

Theorem 2 Let Ũk be a matrix consisting of k approximate principal directions computed by the Nyström methods given the sample matrix W ∈ Rd×s with rank(W) ≥
k. Suppose that 0 (Ũk ) = d(col(Uk ), col(Ũk )), then the
NRE is bounded by
√
NRE(Ũk ) ≤ NRE(Uk ) + 20 ,
where NRE(Uk ) is the optimal NRE for rank-k. The error
of the approximate kernel matrix is bounded by
√
kK − K̃k kF ≤ kK − Kk kF + 20 tr(K),
where kK − Kk kF is the optimal error for rank-k.
The suggested upper error bounds in Thm 2 are applicable
both to the standard and one-shot Nyström methods.
We note that the 1 (Ũk ) is bounded on both sides by constant times of the subspace distance d(col(Uk ), col(Ũk )).
Lemma 4 Suppose that k-th eigengap is nonzero given
Gram matrix K, i.e., γk = λk − λk+1 > 0. Then, given
the Ũk ∈ Rd×k and Ṽk ∈ Rn×k such that Ũ>
k Ũk = Ik
and Ṽk> Ṽk = Ik , the subspace distance is bounded by
s

s

1 (Ũk )
≤ d(col(Uk ), col(Ũk )) ≤
λ1
2 (Ṽk )
≤ d(col(Vk ), col(Ṽk )) ≤
λ1

s

s

1 (Ũk )
,
γk
2 (Ṽk )
,
γk

where 2 (Ṽk ) = tr(Vk> Φ> ΦVk ) − tr(Ṽk> Φ> ΦṼk ).
Lem 4 tells us that the subspace distance goes to zero as
1 (Ũk ) goes to zero, and the converse is also true, like the
squeeze theorem. Thus, we can replace 0 (Ũk ) in Thm 2
with 1 (Ũk ).
We can also provide a connection between 1 (Ũk ) and
2 (Ṽk ) when we set W = ΦṼ` for the Nyström methods.
Lemma 5 Suppose that ` samples are columns of ΦṼ` ,
i.e.W = ΦṼ` , and Ṽk is a submatrix consisting of k
columns of Ṽ` , where Ṽ`> Ṽ` = I` . Then, for any k ≤
rank(W), Ũnys
and Ũosn
satisfy
k
k
nys
1 (Ũosn
k ) ≤ 1 (Ũk ) ≤ 2 (Ṽk ) ≤ 2 (Ṽ` ),

where Ũnys
and Ũosn
are defined in Lem 2.
k
k
Based on Thm 2, Lem 4 and Lem 5, we provide Thm 3
and Rem 1, that tell us how we can get sample vectors for
Nyström methods to reduce their approximation errors.

Double Nyström Method: An Efficient and Accurate Nyström Scheme for Large-Scale Data Sets

Theorem 3 Suppose that the k-th eigengap γk is nonzero
given K. If we set W = ΦṼ` with Ṽ`> Ṽ` = I` , then by
the standard and one-shot Nyström methods, the NRE and
the matrix approximation error are bounded as follows:
s
22 (Ṽk )
(8)
NRE(Ũk ) ≤ NRE(Uk ) +
γk
s
22 (Ṽk )
tr(K) (9)
kK − K̃k kF ≤ kK − Kk kF +
γk
where Ṽk is any submatrix consisting of k columns of Ṽ` .
Remark 1 By Thm 3, we suggest two kinds of strategies:
• Since 2 (Ṽk ) ≤ 2 (Ṽ` ), if we set W as ΦṼ` which
has small 2 (Ṽ` ) or minṼk 2 (Ṽk ) with constraint
Ṽ`> Ṽ` = I` , then we could get a small error induced
by Nyström methods due to a short subspace distance
to the principal subspace. The objective of the kernel K-means is minimizing the 2 (Ṽ` ) with some constraints, which will be discussed in detail in the supplementary material.
• Also, if we set W as ΦṼ` which has small
PE(Vk , Ṽ` ) or minṼk PE(Vk , Ṽk ) with constraint
Ṽ`> Ṽ` = I` , then we could get a small error induced
by Nyström methods due to a short subspace distance
to the principal subspace. The leverage score sampling reduces the expectation of minṼk PE(Vk , Ṽk ).
4.2. Capturing Approximate Principal Subspace
(CAPS)
As discussed in Rem 1, minimizing 2 (Ṽ` ) or
minṼk 2 (Ṽk ) is a key of reducing subspace distance d (col(Uk ), col(Ũk )) and can be a good objective of
sampling methods for Nyström methods. Thus, our goal
of this section is suggesting an algorithm of minimizing
2 (Ṽ` ).
Suggesting an efficient algorithm for minimizing 2 (Ṽ` ),
we introduce the notion of spanning set S defined in Def 5,
which can be utilized to approximate linear combinations
such that approximated eigenvectors lie in the col(S), e.g.
X
ṽj ≈
bij φ(xi ) for j ∈ {1, ..., `},
(10)
φ(xi )∈S

where bij is a coefficient.
Definition 5 Given n data points, let S be a spanning set
consisting of s representative points in n data points for
linear combination, S be a matrix which consists of s representative vectors as its columns, and TS be a indicator
matrix such that S = ΦTS .

Algorithm 2 Capturing Approximate Principal Subspace
(CAPS)
Input: The number of representatives s, where `  s 
n
Output: Spanning set S consisting of s representative
points, Ṽ` = TS VS,` (or Ṽ` = TS ṼS,` ), ` samples
W = SVS,` (respectively,W = SṼS,` )
1: Construct a spanning set S consisting of s representative points which are obtained by column index sampling (e.g., uniform random or approximate leverage
score etc.)
2: Obtain KS = S> S
>
3: Perform compact SVD KS = VS Σ2S VS
or approximate compact SVD (e.g. randomized SVD or the oneshot Nyström method)
4: Obtain Ṽ` as TS VS,` or TS ṼS,` , where TS is the
indicator matrix for the set S
If we express approximate ` eigenvectors as described in
Eqn (10) and set s  n for very large-scale data, then the
time complexities of computing Ṽ` will be reduced.
Here is our strategy which is called ”Capturing Approximate Principal Subspace” (CAPS).
1. For the scalability, we construct and utilize a spanning set S defined in Def 5, and set a constraint as
col(W) ⊂ col(S). Applying the constraint to our
Rem 1, then we have
W = ΦṼ` = SA` with A>
` A ` = I` .

(11)

2. Under the condition in Eqn (11), the solution of the
problem of minimizing 2 (Ṽ` ) is VS,` by the Proposition 2. Thus, we compute VS,` via SVD of KS ,
or ṼS,` through approximate SVD including randomized SVD or the one-shot Nyström method.
Consequently, CAPS aims to get a small 2 (Ṽ` ) more
directly with just O(sn) memory, where s  n. The
time complexity varies depending on step 1 and step 3
in Alg 2. For decomposing KS in step 3, the time complexity is O(s3 ) for SVD and O(m2 s) for the one-shot
Nyström method, where ` ≤ m  s.
Proposition 2 Given spanning set S consisting of s representative points, suppose that we set W = ΦṼ` and
Ṽ`> Ṽ` = I` with the constraint col(W) ⊂ col(S). Then,
under that condition, the problem of minimizing 2 (V` )
can be equivalently expressed as
minimize 2 (Ṽ` ) subject to Ṽ` = TS A` , A>
` A` = I` ,
A`

and the output of step 3 in Alg 2 with rank-` SVD minimizes
2 (V` ), i.e., VS,` = argminA` 2 (Ṽ` ) subject to Ṽ` =
2
>
TS A` , A>
` A` = I` , where KS = VS ΣS VS .

Double Nyström Method: An Efficient and Accurate Nyström Scheme for Large-Scale Data Sets

We directly provide Cor 2 which is a revised version of
Thm 3 for CAPS sampling.
Corollary 2 By standard and one-shot Nyström methods,
any set of ` samples computed by CAPS sampling satisfying
2 (Ṽ` ) error is guaranteed to satisfy Eqn (8) and Eqn (9).
Since our main concern is minimizing 2 (Ṽ` ), and the
CAPS with rank-` SVD gives the optimal solution for the
given condition in Proposition 2 as Ṽ` = TS VS,` , we can
approximate 2 (TS VS,` ) as 2 (TS ṼS,` ), where ṼS,` can
be computed by one-shot Nyström method due to its optimality discussed in Thm 1, or can be obtained from randomized SVD.

5. The Double Nyström Method
In this section, we propose a new framework of
Nyström method based on the CAPS and the oneshot Nyström method, which is called “Double
Nyström Method” and described in Alg 3. In brief,
the Double Nyström method reduces the original problem
of decomposing the given n × n kernel matrix to the
problem of decomposing a s × s matrix, and again reduces
it to the problem of decomposing a ` × ` matrix.
1. In the first part, we select the one-shot
Nyström method for step 3 in Alg 2, and run
CAPS using the one-shot Nyström method to compute VS,` . Because we can consider the problem
of computing VS,` as the KPCA problem, and
the one-shot Nyström method solves sample-based
KPCA defined in Def 1. Also, it has a small running
time complexity O(m2 s), since ` ≤ m  s  n.
2. For the second part, we consider W = ΦṼ` =
SṼS,` , and run the one-shot Nyström method. Since
computed Ṽ` = TS ṼS,` in the first part induces a
small 2 (Ṽ` ), we may have an accurate K̃k after the
second part.
Constructing a spanning set S in CAPS by using uniform random sampling, the total time complexity of double Nyström methods is O(`sn + m2 s), and O(m2 s) term
is not considerable compared to O(`sn), since ` ≤ m 
s  n.
We note that computing spanning set S is also important
for CAPS, consequently for Double Nyström method. In
Rem 2, We provide an example how we can quickly compute approximate leverage scores and construct a spanning
set S. Also, we summarize the time complexity of the
Nyström methods in Tbl 1.

Algorithm 3 The Double Nyström Method
Input: Kernel function κ, and the parameters k, `, m, s,
where k ≤ ` ≤ m  s  n
Output: Approximate kernel matrix and spectral decomposition
1: Run CAPS using the one-shot Nyström method with
m subsamples of spanning set S, and obtain Ṽ` and
W = ΦṼ` = SṼS,`
2: Compute KW = (ṼS,` )> KS ṼS,` ∈ R`×` and C =
C0 ṼS,` ∈ Rn×` by using W and C0 = Φ> S, and run
the one-shot Nyström method with parameters k and `

Remark 2 Since the computational complexity for computing the exact leverage scores is high, we can obtain approximate leverage scores by using double Nyström method
or other methods.
First, we obtain s1 instances by uniform random sampling
and construct a spanning set S1 , where s1 ≤ s. Next, we
run double Nyström method with the spanning set S1 and
approximate leverage scores in time O(`1 s1 n + m21 s1 ). We
sample additional (s−s1 ) instances to complete constructing a spanning set S by using the computed scores, where
`1 ≤ m1  s1 ≤ s  n and S1 ⊆ S. If we run double
Nyström method with the computed spanning set S, then
the total running time is O(`sn + m2 s) for `1 = Θ(`) and
m1 = Θ(m).

6. Experiments
In this section, we present experimental results that demonstrate our theoretical work and algorithms. We conduct experiments with the measure called “relative approximation error” (Relative Error): Relative Error =
kK − K̃k kF /kKkF . We report the running time as the
sum of the the sampling time and the Nyström approximation time. Every experimental instances are run on
MATLAB R2012b with Intel Xeon 2.90GHz CPUs, 96GB
RAM, and 64bit CentOS system.
We choose 5 real data sets for performance comparisons
and summarize them in Tbl 2. To construct kernel matrix K, we use radial basis function(RBF)
and

 it is defined
kx −x k2
as follows: κ(xi , xj ) = exp − i2σ2j 2 , where σ is
a kernel parameter. We set σ for 5 data sets as follows:
σ = 100.0 for Dexter, σ = 1.0 for Letter, σ = 5.0 for
MNIST, σ = 0.3 for MiniBooNE, and σ = 1.0 for Covertype. We select k = 20 and k = 50 for each data set.
We empirically compare the double Nyström method described in Alg 3 with three representative Nyström methods: the standard Nyström method (Williams & Seeger,
2001), the standard Nyström method using randomized
SVD (Li et al., 2015), and the one-shot Nyström method

Double Nyström Method: An Efficient and Accurate Nyström Scheme for Large-Scale Data Sets
MNIST

−0.08

10

−0.09

10

−0.1

Relative Error (k=20)

Relative Error (k=20)

−0.07

10

−0.3

MiniBooNE

−0.05

Covertype

10

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

10
Relative Error (k=20)

SVD (optimal)
Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

−0.4

10

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

−0.07

10

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

−0.1

Relative Error (k=20)

Letter
−0.06

10

−0.09

10

−0.11

10

10

−0.12

10

10

−0.14

10
−0.13

10
1

0

10

10

Time (s)
Letter

10

−0.14

10

−0.16

10

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

−0.3

10
Relative Error (k=50)

Relative Error (k=50)

−0.12

0

2

10

10

MNIST

SVD (optimal)
Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

−0.1

10

1

10
Time (s)

−0.4

10

1

10
Time (s)
MiniBooNE

2

1

10

10

10
Time (s)
Covertype

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

−0.1

2

10

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

−0.14

10
Relative Error (k=50)

0

10

Relative Error (k=50)

−1

10

−0.16

10

−0.18

10

−0.2

10

−0.5

−0.18

10

10

−0.22

10

−0.2

10
−1

10

0

1

10

10

0

10

Time (s)

1

0

2

10
Time (s)

10

10

1

2

10
Time (s)

1

10

2

10

10
Time (s)

Figure 1. Performance comparison both for k = 20 and k = 50 among the four methods: the standard Nyström method (Williams &
Seeger, 2001), the one-shot Nyström method (Fowlkes et al., 2004), the standard Nyström method using randomized SVD (Li et al.,
2015), and the double Nyström method (ours). We gradually increase the number of samples s as 500, 1000, 1500,..., 5000, and there
are corresponding 10 points on the each line. We perform SVD algorithm only on the Letter data set due to memory limit.
Table 1. Time complexities for the Nyström methods to obtain a rank-k approximation with spanning set S, where ` ≤ m  s  n
and CAPS(ALev) is described in Rem 2

Unif & The Standard
Unif & The One-Shot
Unif & Rand.SVD + The Standard
The Double (CAPS(Unif))
The Double (CAPS(ALev))

time complexity

linearity for s

degree for s and n

#kernel elements for computation

No
No
Yes
Yes
Yes

cubic
cubic
quadratic
quadratic
quadratic

O(sn)
O(sn)
O(sn)
O(sn)
O(sn)

3

O(ksn + s )
O(s2 n)
O(ksn)
O(`sn + m2 s)
O(`sn + m2 s)

number of instances n

dimensionality d0

Dexter
Letter
MNIST
MiniBooNE
Covertype

2600
20000
60000
130064
581012

20000
16
784
50
54

(Fowlkes et al., 2004). We run the double Nyström method
with the spanning set S constructed by uniform random
sampling (Unif) and approximate leverage scores (ALev).
There are 10 episodes for each test, and 10 points on the
each line in the figures. For example, we set s = 500t,
` = (140 + 5t), and m = (250 + 50t) when n ≥ 20000,
where t = 1, 2, ..., 10. We display the experimental results
in Fig 1 and 2. As shown in the experiments, the double
Nyström method always shows better efficiency than other
methods under the same condition of using O(sn) kernel
elements. In the experiment on the Letter data set, we can
also notice that the error of the double Nyström method
more rapidly decreases to the optimal error than the others.

SVD (optimal)
Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

−1.12

Relative Error (k=20)

data set

Dexter

Dexter

Table 2. The summary of 5 real data sets. n is the number of
instances and d0 is the dimension of the original data

10

SVD (optimal)
Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

−1.123

10
Relative Error (k=50)

The Sampling & Nyström Methods

−1.13

−1.127

10

−1.131

10

−1.135

10

10
0

10

Time (s)

1

10

0

1

10

10
Time (s)

Figure 2. Additional experiments for high dimensional data set.
We gradually increase s as 200, 400, 600,..., 2000.

7. Conclusion
In this paper, we provided a comprehensive analysis of
the one-shot Nyström method and a generalized view of
sampling strategy, and by integrating of these two results,
we proposed the “Double Nyström Method” which reduces the size of the decomposition problem to a smaller
size in two stages. Both theoretically and empirically, we
demonstrated that the double Nyström method is much
more efficient than the various Nyström methods, but is
quite accurate. Thus, we recommend using the double
Nyström method for large-scale data sets.

Double Nyström Method: An Efficient and Accurate Nyström Scheme for Large-Scale Data Sets

Acknowledgments
W. Lim acknowledges support from the KAIST Graduate
Research Fellowship via Kim-Bo-Jung Fund. K. Jung acknowledges support from the Brain Korea 21 Plus Project
in 2015.

Kumar, Sanjiv, Mohri, Mehryar, and Talwalkar, Ameet.
Sampling methods for the Nyström method. The Journal
of Machine Learning Research, 98888:981–1006, 2012.

References

Li, Mu, Bi, Wei, Kwok, James T, and Lu, B-L. Largescale Nyström kernel matrix approximation using randomized svd. Neural Networks and Learning Systems,
IEEE Transactions on, 26(1):152–165, 2015.

Deshpande, Amit, Rademacher, Luis, Vempala, Santosh,
and Wang, Grant. Matrix approximation and projective
clustering via volume sampling. Theory of Computing,
2:225–247, 2006.

Mahoney, Michael W. and Drineas, Petros. Cur matrix decompositions for improved data analysis. Proceedings
of the National Academy of Sciences, 106(3):697–702,
2009.

Drineas, Petros and Mahoney, Michael W. On the Nyström
method for approximating a gram matrix for improved
kernel-based learning. The Journal of Machine Learning
Research, 6:2153–2175, 2005.

Sun, Xichen, Wang, Liwei, and Feng, Jufu. Further results on the subspace distance. Pattern recognition, 40
(1):328–329, 2007.

Drineas, Petros, Magdon-Ismail, Malik, Mahoney,
Michael W., and Woodruff, David P. Fast approximation
of matrix coherence and statistical leverage. Journal of
Machine Learning Research, 13:3475–3506, 2012.
Fine, Shai and Scheinberg, Katya. Efficient svm training
using low-rank kernel representations. The Journal of
Machine Learning Research, 2:243–264, 2002.
Fowlkes, Charless, Belongie, Serge, Chung, Fan, and Malik, Jitendra. Spectral grouping using the Nyström
method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):214–225, 2004.
Frieze, Alan, Kannan, Ravi, and Vempala, Santosh. Fast
monte-carlo algorithms for finding low-rank approximations. In Proceedings of FOCS, pp. 370–378. IEEE,
1998.
Gittens, Alex and Mahoney, Michael W. Revisiting
the Nyström method for improved large-scale machine
learning. In Proceedings of ICML, 2013.
Golub, Gene H and Van Loan, Charles F. Matrix computations, volume 3. JHU Press, 2012.
Günter, S., Schraudolph, N., and Vishwanathan, S.V.N.
Fast iterative kernel principal component analysis. Journal of Machine Learning Research, 8, 2007.
Hsieh, Cho-Jui, Si, Si, and Dhillon, Inderjit S. Fast prediction for large-scale kernel machines. In Proceeding of
NIPS, pp. 3689–3697, 2014.
Kumar, Sanjiv, Mohri, Mehryar, and Talwalkar, Ameet. On
sampling-based approximate spectral decomposition. In
Proceedings of ICML, pp. 553–560. ACM, 2009.

Talwalkar, Ameet, Kumar, Sanjiv, and Rowley, Henry.
Large-scale manifold learning. In Proceedings of CVPR,
pp. 1–8. IEEE, 2008.
Wang, Liwei, Wang, Xiao, and Feng, Jufu. Subspace distance analysis with application to adaptive bayesian algorithm for face recognition. Pattern Recognition, 39(3):
456–464, 2006.
Wang, Shusen and Zhang, Zhihua. Improving CUR matrix decomposition and the Nyström approximation via
adaptive sampling. The Journal of Machine Learning
Research, 14(1):2729–2769, 2013.
Williams, Christopher and Seeger, Matthias. Using the
Nyström method to speed up kernel machines. In Proceedings of NIPS, 2001.
Zhang, Kai and Kwok, James T. Clustered Nyström
method for large scale manifold learning and dimension
reduction. IEEE Transactions on Neural Networks, pp.
1576–1587, 2010.
Zhang, Kai, Tsang, Ivor W, and Kwok, James T. Improved
Nyström low-rank approximation and error analysis. In
Proceedings of ICML, pp. 1232–1239. ACM, 2008.

