Double NystroÌˆm Method:
An Efficient and Accurate NystroÌˆm Scheme for Large-Scale Data Sets

Woosang Lim
School of Computing, KAIST, Daejeon, Korea
Minhwan Kim
LG Electronics, Seoul, Korea

QUASAR 17@ KAIST. AC . KR

MINHWAN 1. KIM @ LGE . COM

Haesun Park
HPARK @ CC . GATECH . EDU
School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA 30332, USA
Kyomin Jung
Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea

Abstract
The NystroÌˆm method has been one of the most
effective techniques for kernel-based approach
that scales well to large data sets. Since its introduction, there has been a large body of work
that improves the approximation accuracy while
maintaining computational efficiency. In this paper, we present a novel NystroÌˆm method that
improves both accuracy and efficiency based on
a new theoretical analysis. We first provide a
generalized sampling scheme, CAPS, that minimizes a novel error bound based on the subspace
distance. We then present our double NystroÌˆm
method that reduces the size of the decomposition in two stages. We show that our method is
highly efficient and accurate compared to other
state-of-the-art NystroÌˆm methods by evaluating
them on a number of real data sets.

1. Introduction
Low-rank matrix approximation is one of the core techniques to mitigate the space requirement that arises in
large-scale machine learning and data mining. Consequently, many methods in machine learning involve a lowrank approximation of matrices that represent data, such as
manifold learning (Fowlkes et al., 2004; Talwalkar et al.,
2008), support vector machines (Fine & Scheinberg, 2002)
and kernel principal component analysis (Zhang et al.,
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

KJUNG @ SNU . AC . KR

2008). These methods typically involve spectral decomposition of a symmetric positive semi-definite (SPSD) matrix, but its exact computation is prohibitively expensive for
large data sets.
The standard NystroÌˆm method (Williams & Seeger, 2001)
is one of the popular methods for approximate spectral decomposition of a large kernel matrix K âˆˆ RnÃ—n (generally
a SPSD matrix), due to its simplicity and efficiency (Kumar et al., 2009). One of the main characteristics of the
NystroÌˆm method is that it uses samples to reduce the original problem of decomposing the given n Ã— n kernel matrix
to the problem of decomposing a s Ã— s matrix, where s
is the number of samples much smaller than n. The standard NystroÌˆm method has time complexity O(ksn + s3 )
for rank-k approximation and is indeed scalable. However,
the accuracy is typically its weakness, and there have been
many studies to improve its accuracy. Most of the recent
work in this line of research can be roughly categorized
into the following two types of approaches:
Refining Decomposition Exemplar works in this category
are one-shot NystroÌˆm method (Fowlkes et al., 2004), modified NystroÌˆm method (Wang & Zhang, 2013), ensemble NystroÌˆm method (Kumar et al., 2012) and standard
NystroÌˆm method using randomized SVD (Li et al., 2015).
All of these methods redefine the intersection matrix that
appears in the reconstructed form of the input matrix, of
which we provide details in Section 2.
The motivation of the one-shot NystroÌˆm method (Fowlkes
et al., 2004) is to obtain an orthonormal set of approximate
eigenvectors of the given kernel matrix via diagonalization
of the standard NystroÌˆm approximation in one-shot with
O(s2 n) running time. Because of the orthonormality, the

Double NystroÌˆm Method: An Efficient and Accurate NystroÌˆm Scheme for Large-Scale Data Sets

one-shot NystroÌˆm is widely used for kernel PCA (Zhang
et al., 2008), however there is no more elaborate analysis for it except the work of Fowlkes et al. (2004). The
modified NystroÌˆm method (Wang & Zhang, 2013) involves
multiplying both sides of kernel matrix by projection matrix consisting of orthonormal basis of subspace spanned
by samples. It solves the problem of minimizing matrix reconstruction error, which is minU kK âˆ’ CUC> kF , given
the kernel matrix K and the C, where C is a submatrix
consisting of ` columns of K. Although the modified
NystroÌˆm approximation is more accurate than the standard
NystroÌˆm approximation, it is more expensive to compute
and is not able to compute a rank-k approximation. Its time
complexity is O(s2 n + sn2 ), and the latter term O(sn2 )
comes from some matrix multiplications and dominates
O(s2 n). The ensemble NystroÌˆm method (Kumar et al.,
2012) that takes a mixture of t (â‰¥ 1) standard NystroÌˆm
approximations, and is more accurate than the standard
NystroÌˆm approximations in the empirical results. Its time
complexity is O(ksnt + s3 t + Âµ), where Âµ is the cost
of computing the mixture weights. To obtain more efficiency, we can adopt randomized SVD to approximate the
pseudo inverse of the intersection matrix of the standard
NystroÌˆm approximation (Li et al., 2015). Its time complexity is O(ksn), but it needs larger samples than other
NystroÌˆm methods due to adopting approximate SVD.
Improving Sampling The NystroÌˆm methods require column/row samples which heavily affects the accuracy.
Among many sampling strategies for NystroÌˆm methods,
uniform sampling without replacement is the most basic
sampling strategy (Williams & Seeger, 2001), of which
probabilistic error bounds for the standard NystroÌˆm method
are recently derived (Kumar et al., 2012; Gittens & Mahoney, 2013).
Recent work includes the non-uniform samplings, which
are square of diagonal sampling (Drineas & Mahoney,
2005), square of L2 column norm sampling (Drineas
& Mahoney, 2005), leverage score sampling (Mahoney
& Drineas, 2009) and approximate leverage score sampling (Drineas et al., 2012; Gittens & Mahoney, 2013).
The adaptive sampling strategies also have been studied,
e.g. (Deshpande et al., 2006; Kumar et al., 2012). Some
of the heuristic sampling strategies for NystroÌˆm methods
are utilizing normal K-means algorithm (Zhang & Kwok,
2010) with K = s, and adopting pseudo centroids of normal (weighted) K-means (Hsieh et al., 2014). Normal Kmeans sampling clusters the original data which is not applied kernel function, and uses s centroids to generate matrices consisting of kernel function values among all data
points and s centroids to perform NystroÌˆm methods. Although it has been shown to give good empirical accuracy,
its proposed analysis is quite loose and does not show any
connection to the minimum error of rank-k approximation.

1.1. Our Contributions
In this paper, we propose a novel NystroÌˆm method, Double
NystroÌˆm Method, which tightly integrates the strengths of
both types of approaches. Our contribution can be appreciated in three aspects: comprehensive analysis of the oneshot NystroÌˆm method, generalization of sampling methods,
and integration of our two results. Summary of those are
described as follows.
1.1.1. C OMPREHENSIVE A NALYSIS OF THE O NE - SHOT
N YSTR OÌˆM M ETHOD
In Section 3, we provide an analysis that one-shot
NystroÌˆm is quite a good compromise between the standard NystroÌˆm method and the modified NystroÌˆm method,
since it yields accurate rank-k approximations for k < s
(Thm 1). In addition, we show that it is robust (Proposition 1).
1.1.2. G ENERALIZATION OF S AMPLING M ETHODS
In Section 4, we investigate how we can improve accuracies of NystroÌˆm methods. First, we present new upper error
bounds both for the standard and one-shot NystroÌˆm methods (Thm 3), and provide a generalized view of sampling schemes which makes connection among some of the
sampling schemes and minimization of our error bounds
(Rem 1). Next, we propose Capturing Approximate Principal Subspace (CAPS) algorithm which minimizes our upper error bounds efficiently (Proposition 2).
1.1.3. T HE D OUBLE N YSTR OÌˆM M ETHOD
In Section 5, we propose Double NystroÌˆm Method (Alg 3)
that combines the advantages of CAPS sampling and
the one-shot NystroÌˆm method. It reduces the size of
the decomposition problem in twice, and consequently is
much more efficient than the standard NystroÌˆm method
for large data sets, but is as accurate as the one-shot
NystroÌˆm method. Its time complexity is also comparable
to the running time of standard NystroÌˆm method using randomized SVD, since it is O(`sn + m2 s) and linear for s,
where ` â‰¤ m  s  n.

2. Preliminaries
Given the data set X = {x1 , . . . , xn } and its corresponding
matrix X âˆˆ Rd0 Ã—n , we define the kernel function without
explicit feature mapping Ï† as Îº(xi , xj ), where Ï† is a feature mapping such that Ï† : X â†’ Rd . The corresponding
kernel matrix K âˆˆ RnÃ—n is a positive semi-definite (PSD)
matrix with elements Îº(xi , xj ). Without loss of generality,
let Î¦ = Ï†(X) âˆˆ RdÃ—n be the matrix constructed by taking
Ï†(xi ) as the i-th column, so that K = Î¦> Î¦ (Drineas &
Mahoney, 2005). We assume that rank(Î¦) = r, which in

Double NystroÌˆm Method: An Efficient and Accurate NystroÌˆm Scheme for Large-Scale Data Sets

turn implies rank(K) = r.
Consider the compact singular value decomposition (com>
pact SVD) 1 Î¦ = UÎ¦,r Î£Î¦,r VÎ¦,r
, where Î£Î¦,r is the
diagonal matrix consisting of r nonzero singular values
(Ïƒ(Î¦)1 , . . . , Ïƒ(Î¦)r ) of Î¦ in decreasing order, and UÎ¦,r âˆˆ
RdÃ—r and VÎ¦,r âˆˆ RnÃ—r are the matrices consisting of the
left and right singular vectors, respectively. Especially, we
simply denote compact SVD of Î¦ as Î¦ = Ur Î£r Vr> , and
(Ïƒ(Î¦)1 , . . . , Ïƒ(Î¦)r ) as (Ïƒ1 , . . . , Ïƒr ) in this paper. Then,
we can obtain the compact SVD K = Vr Î£2r Vr> and its
>
pseudo-inverse obtained by Kâ€  = Vr Î£âˆ’2
r Vr . The best
rank-k (with k â‰¤ r) approximation of K can be obtained
from its SVD by
Kk = Vk Î£2k Vk> =

Pk

i=1

Î»i (K)vi vi> ,

where Î»i (K) = Ïƒi2 are the first k eigenvalues of K. Especially we simply denote again Î»i (K) = Î»i in this paper,
thus Î»i = Ïƒi2 .
Given a set W = {w1 , . . . , ws } of s mapped samples of
X (i.e. wi = Ï†(xj ) for some 1 â‰¤ j â‰¤ n), let W âˆˆ RdÃ—s
denote the matrix consisting of wi as the i-th column, and
C = Î¦> W âˆˆ RnÃ—s the inner product matrix of the whole
data instances and the samples. Since the kernel matrix
KW âˆˆ RsÃ—s for the subset W is KW = W> W, we can
rearrange the rows and columns of K such that




KW
KW K>
21
K=
and C =
.
(1)
K21
K21 K22
In the later part of the paper, we will generalize the samples wi to arbitrary vectors of dimension d, not necessarily
mapped vectors of some data instances.
2.1. The Standard NystroÌˆm Method
The standard NystroÌˆm method for approximating the kernel
matrix K using the subset W of s sample data instances
yields a rank-s0 approximation matrix
â€ 
>
KÌƒnys = KÌƒnys
s0 = CKW C â‰ˆ K,

Kâ€ W

where
is the pseudo-inverse of KW and s0 =
rank(W). The rank-k approximation matrix (with k â‰¤ s0 )
is computed by
KÌƒnys
= CKâ€ W,k C> ,
k
where Kâ€ W,k is the pseudo-inverse of KW,k , the best rankk approximation of KW , which can be computed from the
>
SVD KW,k = VW,k Î£2W,k VW,k
. The time complexity of
nys
3
computing KÌƒk is O(ksn + s ).
1
In this paper, we use compact SVD instead of full SVD unless
we give a particular mention.

The NystroÌˆm method is also used to compute the first k
2
approximate eigenvalues (Î£Ìƒnys
k ) and the corresponding
nys
k approximate eigenvectors (VÌƒk ) of the kernel matrix
>
K. Using SVD KW,k = VW,k Î£2W,k VW,k
, the eigenvalues and eigenvectors are computed by
r
n
s
nys 2
nys
2
CVW,k Î£âˆ’2
(Î£Ìƒk ) = (Î£W,k ) and VÌƒk =
W,k ,
s
n
(2)
However in general, KÌƒnys
is not the best rank-k approxik
mation of KÌƒnys , nor the eigenvectors VÌƒknys are orthogonal
even though K is symmetric.
2.2. The One-Shot NystroÌˆm Method
A straightforward way to obtain the best rank-k approximation of KÌƒnys would be a two-stage computation, where
we first construct the full KÌƒnys and then reduce its rank
via SVD. This approach is costly, since its time complexity
amounts to performing SVD on the original matrix K.
The one-shot NystroÌˆm method (Fowlkes et al., 2004),
shown in Alg 1, computes the SVD in a single pass, and
thus possesses the following nice property:
Lemma 1 (Fowlkes et al., 2004) The NystroÌˆm method using a sample set W of s vectors can be decomposed as
â€ 
>
>
KÌƒnys = KÌƒnys
s0 = CKW C = GG ,
0

nÃ—s
with s0 = rank(KW ) and G = CVW Î£âˆ’1
.
W âˆˆ R
>
The one-shot NystroÌˆm method computes SVD G G =
>
, and computes s0 eigenvectors of KÌƒnys by
VG Î£2G VG
osn
VÌƒs0 = GVG Î£âˆ’1
G . Consequently, we obtain the same
result as the NystroÌˆm method for the rank-s0 approximation
2
osn >
KÌƒosn = VÌƒsosn
= GG> = KÌƒnys ,
0 Î£G (VÌƒs0 )

but better yet, the best rank-k (with k < s0 ) approximation
KÌƒosn
= VÌƒkosn Î£2G,k (VÌƒkosn )> = (KÌƒnys )k 6= KÌƒnys
k
k .
The time complexity of the one-shot NystroÌˆm method is
O(s2 n) if the sample set W is a mapped samples of the
data set, i.e. W = {wi |wi = Ï†(xj ) for some 1 â‰¤ j â‰¤
n}. This is the case when the sample selection matrix P
is a binary matrix with a single one per column. In the
remainder of the paper, we will use the notation Î£Ìƒosn
for
k
Î£G,k to emphasize that it is obtained from the one-shot
NystroÌˆm method.

3. The One-Shot NystroÌˆm Method: The
Optimal Sample-based KPCA
As reviewed in the previous section, the one-shot
NystroÌˆm method computes the best rank-k approximation

Double NystroÌˆm Method: An Efficient and Accurate NystroÌˆm Scheme for Large-Scale Data Sets

Algorithm 1 The One-shot NystroÌˆm method
Input: Matrix Ps âˆˆ RnÃ—s representing the composition
of s sample points such that W = Î¦Ps âˆˆ RdÃ—s with
rank(W) = s0 , kernel function Îº
Output: Approximate kernel matrix KÌƒosn
k , its singular
osn
osn 2
vectors VÌƒk and singular values (Î£Ìƒk )
1: Obtain KW = W> W
>
2: Perform compact SVD KW = VW Î›W VW
=
>
VW Î£2W VW
3: Compute G> G, where G = CVW Î£âˆ’1
W
4: Compute VG,k the first k singular vectors of G> G
and corresponding singular values Î£2G,k
osn
5: Î£Ìƒosn
= Î£G,k , VÌƒkosn = GVG,k Î£âˆ’1
=
k
G,k and KÌƒk
>
GVG,k VG,k
G>

Definition 1 For the given samples W, sample-based
KPCA problem is defined as

of KÌƒnys to obtain orthonormal approximate eigenvectors
VÌƒk of K for the given samples W. Here, we suggest
that the one-shot NystroÌˆm method can be used for computing optimal solutions to other closely related problems,
such as kernel principal component analysis (KPCA). In
this section, we make a formal statement that the one-shot
NystroÌˆm method provides an optimal KPCA for the given
W, which we build on in later sections.

UÌƒosn
= UW VG,k ,
k

minimize NRE(UÌƒk ) subject to UÌƒ>
k UÌƒk = Ik , UÌƒk = WAk .
Ak

(5)
The following lemma provides two types of the approximate principal directions UÌƒk , which are computed from the
standard NystroÌˆm method and one-shot NystroÌˆm method.
Lemma 2 In standard NystroÌˆm method, approximate principal directions are
UÌƒnys
= UW,k ,
k

(6)

>
where W = UW Î£W VW
. In the one-shot NystroÌˆm
method, approximate principal directions are

(7)

>
>
where G = Î¦> WVW Î£âˆ’1
W = Î¦ UW and G G =
>
VG Î£G VG .

The following theorem states that the one-shot
NystroÌˆm method can be used to solve the optimization problem defined in Def 1.

We start with the observation that the most scalable KPCA
algorithms are based on a set of samples (Frieze et al.,
1998; Williams & Seeger, 2001) can be seen as computing k approximate eigenvectors of K, given by

Theorem 1 Given the s samples W âˆˆ RdÃ—s with
rank(W) = s0 , KPCA using the one-shot NystroÌˆm method
solves the optimization problem in Def 1.

VÌƒk = CAk Î£Ìƒâˆ’1
k ,

Given samples W, we proved that the one-shot NystroÌˆm
method minimizes the NRE(UÌƒk ) in Eqn (5) in Def 1. To
give more intuition for it, we introduce the sum of eigenvalue errors which is closely related with the NRE(UÌƒk ).

(3)

where C = Î¦> W âˆˆ RnÃ—s is the inner product matrix
among the whole data set and the sample vectors (Eqn (1)),
Ak âˆˆ RsÃ—k is the algorithm-dependent coefficient matrix
for each pair of sample vector and principal direction, and
Î£Ìƒk is the diagonal matrix of the first k approximate singular values. This view generalizes (Kumar et al., 2009).
To facilitate the analysis, we first reformulate Eqn (3) as
VÌƒk = Î¦> UÌƒk Î£Ìƒâˆ’1
k ,

(4)

where UÌƒk = WAk âˆˆ RdÃ—k denotes k approximate principal directions in the feature space. Using the reconstruction
error (RE) and the normalized reconstruction error (NRE)
for KPCA (GuÌˆnter et al., 2007)
RE(UÌƒk ) = kÎ¦ âˆ’ UÌƒk UÌƒ>
k Î¦kF
NRE(UÌƒk ) =

kÎ¦ âˆ’ UÌƒk UÌƒ>
k Î¦kF
kÎ¦kF

as the objective functions, and observing that Ak determines the k approximate principal directions UÌƒk , we can
formulate KPCA based on samples W as an optimization
problem:

Definition 2 Given k approximate principal directions
UÌƒk âˆˆ RdÃ—k of Î¦ such that UÌƒ>
k UÌƒk = Ik , the sum of eigenvalue errors from UÌƒk is defined as
>
>
>
1 (UÌƒk ) = tr(U>
k Î¦Î¦ Uk ) âˆ’ tr(UÌƒk Î¦Î¦ UÌƒk ),

where Uk is the matrix consisting of true principal directions as columns.
With the notion in Def 2, we can directly give a following
corollary.
Corollary 1 Minimizing the NRE(UÌƒk ) is equivalent to
minimizing the 1 (UÌƒk ) defined in Def 2, thus
UÌƒosn
= argmin 1 (UÌƒk ) s.t. UÌƒ>
k
k UÌƒk = Ik , UÌƒk = WAk .
UÌƒk

Additional to Thm 1 and Cor 1, we show that outputs of
the one-shot NystroÌˆm method depend only on subspace
spanned by input samples.

Double NystroÌˆm Method: An Efficient and Accurate NystroÌˆm Scheme for Large-Scale Data Sets

Proposition 1 Let W1 and W2 be the matrix consisting
of s1 samples and s2 samples respectively. If two column
spaces col(W1 ) and col(W2 ) are the same, then the outputs of the one-shot NystroÌˆm method are also the same regardless of difference between set of samples.
We note that the standard NystroÌˆm method does not satisfy
the robustness discussed in Proposition 1.

4. A Generalized View of Sampling Schemes
Motivated by Proposition 1, in this section, we provide new
upper error bounds of the NystroÌˆm method based on subspace distance, and suggest a generalized view of sampling
schemes for the NystroÌˆm method.
4.1. Error Analysis based on Subspace Distance
To provide new upper error bounds of the NystroÌˆm methods, our motivation is using the measure called â€œsubspace
distanceâ€ which can evaluate the difference between two
subspaces (Wang et al., 2006; Sun et al., 2007).
Basically, the subspace distance of two subspaces depends
on the notion of projection error, hence we discuss the projection error first.
Definition 3 Given two matrices U and V consisting of
orthonormal vectors, i.e., U> U = I and V> V = I, the
projection error of U onto col(V) is defined as
PE(U, V) = kU âˆ’ VV> UkF .
Since any linear subspace can be represented by its orthonormal basis, subspace distance can be defined by the
projection error between set of orthonormal vectors.
Definition 4 (Wang et al., 2006; Golub & Van Loan, 2012)
Given k dimensional subspace S1 and k dimensional subspace S2 , the subspace distance d(S1 , S2 ) is defined as
d(S1 , S2 ) = PE(Uk , UÌƒk ) = kUk âˆ’ UÌƒk UÌƒ>
k U k kF ,
where Uk is an orthonormal basis of S1 and UÌƒk is an orthonormal basis of S2 .
Lemma 3 (Wang et al., 2006; Sun et al., 2007; Golub &
Van Loan, 2012) The subspace distances defined in Def 4
are invariant to the choice of orthonormal basis.
Remind that the standard NystroÌˆm and one-shot
NystroÌˆm methods satisfy KÌƒk
= VÌƒk Î£Ìƒ2k VÌƒk
=
>
>
Î¦ UÌƒk UÌƒk Î¦, and those are characterized by UÌƒnys
k
as proved in Lem 2. Therefore, we provide a
and UÌƒosn
k
following error analysis based on the subspace distance
between col(Uk ) and col(UÌƒk ).

Theorem 2 Let UÌƒk be a matrix consisting of k approximate principal directions computed by the NystroÌˆm methods given the sample matrix W âˆˆ RdÃ—s with rank(W) â‰¥
k. Suppose that 0 (UÌƒk ) = d(col(Uk ), col(UÌƒk )), then the
NRE is bounded by
âˆš
NRE(UÌƒk ) â‰¤ NRE(Uk ) + 20 ,
where NRE(Uk ) is the optimal NRE for rank-k. The error
of the approximate kernel matrix is bounded by
âˆš
kK âˆ’ KÌƒk kF â‰¤ kK âˆ’ Kk kF + 20 tr(K),
where kK âˆ’ Kk kF is the optimal error for rank-k.
The suggested upper error bounds in Thm 2 are applicable
both to the standard and one-shot NystroÌˆm methods.
We note that the 1 (UÌƒk ) is bounded on both sides by constant times of the subspace distance d(col(Uk ), col(UÌƒk )).
Lemma 4 Suppose that k-th eigengap is nonzero given
Gram matrix K, i.e., Î³k = Î»k âˆ’ Î»k+1 > 0. Then, given
the UÌƒk âˆˆ RdÃ—k and VÌƒk âˆˆ RnÃ—k such that UÌƒ>
k UÌƒk = Ik
and VÌƒk> VÌƒk = Ik , the subspace distance is bounded by
s

s

1 (UÌƒk )
â‰¤ d(col(Uk ), col(UÌƒk )) â‰¤
Î»1
2 (VÌƒk )
â‰¤ d(col(Vk ), col(VÌƒk )) â‰¤
Î»1

s

s

1 (UÌƒk )
,
Î³k
2 (VÌƒk )
,
Î³k

where 2 (VÌƒk ) = tr(Vk> Î¦> Î¦Vk ) âˆ’ tr(VÌƒk> Î¦> Î¦VÌƒk ).
Lem 4 tells us that the subspace distance goes to zero as
1 (UÌƒk ) goes to zero, and the converse is also true, like the
squeeze theorem. Thus, we can replace 0 (UÌƒk ) in Thm 2
with 1 (UÌƒk ).
We can also provide a connection between 1 (UÌƒk ) and
2 (VÌƒk ) when we set W = Î¦VÌƒ` for the NystroÌˆm methods.
Lemma 5 Suppose that ` samples are columns of Î¦VÌƒ` ,
i.e.W = Î¦VÌƒ` , and VÌƒk is a submatrix consisting of k
columns of VÌƒ` , where VÌƒ`> VÌƒ` = I` . Then, for any k â‰¤
rank(W), UÌƒnys
and UÌƒosn
satisfy
k
k
nys
1 (UÌƒosn
k ) â‰¤ 1 (UÌƒk ) â‰¤ 2 (VÌƒk ) â‰¤ 2 (VÌƒ` ),

where UÌƒnys
and UÌƒosn
are defined in Lem 2.
k
k
Based on Thm 2, Lem 4 and Lem 5, we provide Thm 3
and Rem 1, that tell us how we can get sample vectors for
NystroÌˆm methods to reduce their approximation errors.

Double NystroÌˆm Method: An Efficient and Accurate NystroÌˆm Scheme for Large-Scale Data Sets

Theorem 3 Suppose that the k-th eigengap Î³k is nonzero
given K. If we set W = Î¦VÌƒ` with VÌƒ`> VÌƒ` = I` , then by
the standard and one-shot NystroÌˆm methods, the NRE and
the matrix approximation error are bounded as follows:
s
22 (VÌƒk )
(8)
NRE(UÌƒk ) â‰¤ NRE(Uk ) +
Î³k
s
22 (VÌƒk )
tr(K) (9)
kK âˆ’ KÌƒk kF â‰¤ kK âˆ’ Kk kF +
Î³k
where VÌƒk is any submatrix consisting of k columns of VÌƒ` .
Remark 1 By Thm 3, we suggest two kinds of strategies:
â€¢ Since 2 (VÌƒk ) â‰¤ 2 (VÌƒ` ), if we set W as Î¦VÌƒ` which
has small 2 (VÌƒ` ) or minVÌƒk 2 (VÌƒk ) with constraint
VÌƒ`> VÌƒ` = I` , then we could get a small error induced
by NystroÌˆm methods due to a short subspace distance
to the principal subspace. The objective of the kernel K-means is minimizing the 2 (VÌƒ` ) with some constraints, which will be discussed in detail in the supplementary material.
â€¢ Also, if we set W as Î¦VÌƒ` which has small
PE(Vk , VÌƒ` ) or minVÌƒk PE(Vk , VÌƒk ) with constraint
VÌƒ`> VÌƒ` = I` , then we could get a small error induced
by NystroÌˆm methods due to a short subspace distance
to the principal subspace. The leverage score sampling reduces the expectation of minVÌƒk PE(Vk , VÌƒk ).
4.2. Capturing Approximate Principal Subspace
(CAPS)
As discussed in Rem 1, minimizing 2 (VÌƒ` ) or
minVÌƒk 2 (VÌƒk ) is a key of reducing subspace distance d (col(Uk ), col(UÌƒk )) and can be a good objective of
sampling methods for NystroÌˆm methods. Thus, our goal
of this section is suggesting an algorithm of minimizing
2 (VÌƒ` ).
Suggesting an efficient algorithm for minimizing 2 (VÌƒ` ),
we introduce the notion of spanning set S defined in Def 5,
which can be utilized to approximate linear combinations
such that approximated eigenvectors lie in the col(S), e.g.
X
vÌƒj â‰ˆ
bij Ï†(xi ) for j âˆˆ {1, ..., `},
(10)
Ï†(xi )âˆˆS

where bij is a coefficient.
Definition 5 Given n data points, let S be a spanning set
consisting of s representative points in n data points for
linear combination, S be a matrix which consists of s representative vectors as its columns, and TS be a indicator
matrix such that S = Î¦TS .

Algorithm 2 Capturing Approximate Principal Subspace
(CAPS)
Input: The number of representatives s, where `  s 
n
Output: Spanning set S consisting of s representative
points, VÌƒ` = TS VS,` (or VÌƒ` = TS VÌƒS,` ), ` samples
W = SVS,` (respectively,W = SVÌƒS,` )
1: Construct a spanning set S consisting of s representative points which are obtained by column index sampling (e.g., uniform random or approximate leverage
score etc.)
2: Obtain KS = S> S
>
3: Perform compact SVD KS = VS Î£2S VS
or approximate compact SVD (e.g. randomized SVD or the oneshot NystroÌˆm method)
4: Obtain VÌƒ` as TS VS,` or TS VÌƒS,` , where TS is the
indicator matrix for the set S
If we express approximate ` eigenvectors as described in
Eqn (10) and set s  n for very large-scale data, then the
time complexities of computing VÌƒ` will be reduced.
Here is our strategy which is called â€Capturing Approximate Principal Subspaceâ€ (CAPS).
1. For the scalability, we construct and utilize a spanning set S defined in Def 5, and set a constraint as
col(W) âŠ‚ col(S). Applying the constraint to our
Rem 1, then we have
W = Î¦VÌƒ` = SA` with A>
` A ` = I` .

(11)

2. Under the condition in Eqn (11), the solution of the
problem of minimizing 2 (VÌƒ` ) is VS,` by the Proposition 2. Thus, we compute VS,` via SVD of KS ,
or VÌƒS,` through approximate SVD including randomized SVD or the one-shot NystroÌˆm method.
Consequently, CAPS aims to get a small 2 (VÌƒ` ) more
directly with just O(sn) memory, where s  n. The
time complexity varies depending on step 1 and step 3
in Alg 2. For decomposing KS in step 3, the time complexity is O(s3 ) for SVD and O(m2 s) for the one-shot
NystroÌˆm method, where ` â‰¤ m  s.
Proposition 2 Given spanning set S consisting of s representative points, suppose that we set W = Î¦VÌƒ` and
VÌƒ`> VÌƒ` = I` with the constraint col(W) âŠ‚ col(S). Then,
under that condition, the problem of minimizing 2 (V` )
can be equivalently expressed as
minimize 2 (VÌƒ` ) subject to VÌƒ` = TS A` , A>
` A` = I` ,
A`

and the output of step 3 in Alg 2 with rank-` SVD minimizes
2 (V` ), i.e., VS,` = argminA` 2 (VÌƒ` ) subject to VÌƒ` =
2
>
TS A` , A>
` A` = I` , where KS = VS Î£S VS .

Double NystroÌˆm Method: An Efficient and Accurate NystroÌˆm Scheme for Large-Scale Data Sets

We directly provide Cor 2 which is a revised version of
Thm 3 for CAPS sampling.
Corollary 2 By standard and one-shot NystroÌˆm methods,
any set of ` samples computed by CAPS sampling satisfying
2 (VÌƒ` ) error is guaranteed to satisfy Eqn (8) and Eqn (9).
Since our main concern is minimizing 2 (VÌƒ` ), and the
CAPS with rank-` SVD gives the optimal solution for the
given condition in Proposition 2 as VÌƒ` = TS VS,` , we can
approximate 2 (TS VS,` ) as 2 (TS VÌƒS,` ), where VÌƒS,` can
be computed by one-shot NystroÌˆm method due to its optimality discussed in Thm 1, or can be obtained from randomized SVD.

5. The Double NystroÌˆm Method
In this section, we propose a new framework of
NystroÌˆm method based on the CAPS and the oneshot NystroÌˆm method, which is called â€œDouble
NystroÌˆm Methodâ€ and described in Alg 3. In brief,
the Double NystroÌˆm method reduces the original problem
of decomposing the given n Ã— n kernel matrix to the
problem of decomposing a s Ã— s matrix, and again reduces
it to the problem of decomposing a ` Ã— ` matrix.
1. In the first part, we select the one-shot
NystroÌˆm method for step 3 in Alg 2, and run
CAPS using the one-shot NystroÌˆm method to compute VS,` . Because we can consider the problem
of computing VS,` as the KPCA problem, and
the one-shot NystroÌˆm method solves sample-based
KPCA defined in Def 1. Also, it has a small running
time complexity O(m2 s), since ` â‰¤ m  s  n.
2. For the second part, we consider W = Î¦VÌƒ` =
SVÌƒS,` , and run the one-shot NystroÌˆm method. Since
computed VÌƒ` = TS VÌƒS,` in the first part induces a
small 2 (VÌƒ` ), we may have an accurate KÌƒk after the
second part.
Constructing a spanning set S in CAPS by using uniform random sampling, the total time complexity of double NystroÌˆm methods is O(`sn + m2 s), and O(m2 s) term
is not considerable compared to O(`sn), since ` â‰¤ m 
s  n.
We note that computing spanning set S is also important
for CAPS, consequently for Double NystroÌˆm method. In
Rem 2, We provide an example how we can quickly compute approximate leverage scores and construct a spanning
set S. Also, we summarize the time complexity of the
NystroÌˆm methods in Tbl 1.

Algorithm 3 The Double NystroÌˆm Method
Input: Kernel function Îº, and the parameters k, `, m, s,
where k â‰¤ ` â‰¤ m  s  n
Output: Approximate kernel matrix and spectral decomposition
1: Run CAPS using the one-shot NystroÌˆm method with
m subsamples of spanning set S, and obtain VÌƒ` and
W = Î¦VÌƒ` = SVÌƒS,`
2: Compute KW = (VÌƒS,` )> KS VÌƒS,` âˆˆ R`Ã—` and C =
C0 VÌƒS,` âˆˆ RnÃ—` by using W and C0 = Î¦> S, and run
the one-shot NystroÌˆm method with parameters k and `

Remark 2 Since the computational complexity for computing the exact leverage scores is high, we can obtain approximate leverage scores by using double NystroÌˆm method
or other methods.
First, we obtain s1 instances by uniform random sampling
and construct a spanning set S1 , where s1 â‰¤ s. Next, we
run double NystroÌˆm method with the spanning set S1 and
approximate leverage scores in time O(`1 s1 n + m21 s1 ). We
sample additional (sâˆ’s1 ) instances to complete constructing a spanning set S by using the computed scores, where
`1 â‰¤ m1  s1 â‰¤ s  n and S1 âŠ† S. If we run double
NystroÌˆm method with the computed spanning set S, then
the total running time is O(`sn + m2 s) for `1 = Î˜(`) and
m1 = Î˜(m).

6. Experiments
In this section, we present experimental results that demonstrate our theoretical work and algorithms. We conduct experiments with the measure called â€œrelative approximation errorâ€ (Relative Error): Relative Error =
kK âˆ’ KÌƒk kF /kKkF . We report the running time as the
sum of the the sampling time and the NystroÌˆm approximation time. Every experimental instances are run on
MATLAB R2012b with Intel Xeon 2.90GHz CPUs, 96GB
RAM, and 64bit CentOS system.
We choose 5 real data sets for performance comparisons
and summarize them in Tbl 2. To construct kernel matrix K, we use radial basis function(RBF)
and

 it is defined
kx âˆ’x k2
as follows: Îº(xi , xj ) = exp âˆ’ i2Ïƒ2j 2 , where Ïƒ is
a kernel parameter. We set Ïƒ for 5 data sets as follows:
Ïƒ = 100.0 for Dexter, Ïƒ = 1.0 for Letter, Ïƒ = 5.0 for
MNIST, Ïƒ = 0.3 for MiniBooNE, and Ïƒ = 1.0 for Covertype. We select k = 20 and k = 50 for each data set.
We empirically compare the double NystroÌˆm method described in Alg 3 with three representative NystroÌˆm methods: the standard NystroÌˆm method (Williams & Seeger,
2001), the standard NystroÌˆm method using randomized
SVD (Li et al., 2015), and the one-shot NystroÌˆm method

Double NystroÌˆm Method: An Efficient and Accurate NystroÌˆm Scheme for Large-Scale Data Sets
MNIST

âˆ’0.08

10

âˆ’0.09

10

âˆ’0.1

Relative Error (k=20)

Relative Error (k=20)

âˆ’0.07

10

âˆ’0.3

MiniBooNE

âˆ’0.05

Covertype

10

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

10
Relative Error (k=20)

SVD (optimal)
Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

âˆ’0.4

10

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

âˆ’0.07

10

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

âˆ’0.1

Relative Error (k=20)

Letter
âˆ’0.06

10

âˆ’0.09

10

âˆ’0.11

10

10

âˆ’0.12

10

10

âˆ’0.14

10
âˆ’0.13

10
1

0

10

10

Time (s)
Letter

10

âˆ’0.14

10

âˆ’0.16

10

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

âˆ’0.3

10
Relative Error (k=50)

Relative Error (k=50)

âˆ’0.12

0

2

10

10

MNIST

SVD (optimal)
Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

âˆ’0.1

10

1

10
Time (s)

âˆ’0.4

10

1

10
Time (s)
MiniBooNE

2

1

10

10

10
Time (s)
Covertype

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

âˆ’0.1

2

10

Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

âˆ’0.14

10
Relative Error (k=50)

0

10

Relative Error (k=50)

âˆ’1

10

âˆ’0.16

10

âˆ’0.18

10

âˆ’0.2

10

âˆ’0.5

âˆ’0.18

10

10

âˆ’0.22

10

âˆ’0.2

10
âˆ’1

10

0

1

10

10

0

10

Time (s)

1

0

2

10
Time (s)

10

10

1

2

10
Time (s)

1

10

2

10

10
Time (s)

Figure 1. Performance comparison both for k = 20 and k = 50 among the four methods: the standard NystroÌˆm method (Williams &
Seeger, 2001), the one-shot NystroÌˆm method (Fowlkes et al., 2004), the standard NystroÌˆm method using randomized SVD (Li et al.,
2015), and the double NystroÌˆm method (ours). We gradually increase the number of samples s as 500, 1000, 1500,..., 5000, and there
are corresponding 10 points on the each line. We perform SVD algorithm only on the Letter data set due to memory limit.
Table 1. Time complexities for the NystroÌˆm methods to obtain a rank-k approximation with spanning set S, where ` â‰¤ m  s  n
and CAPS(ALev) is described in Rem 2

Unif & The Standard
Unif & The One-Shot
Unif & Rand.SVD + The Standard
The Double (CAPS(Unif))
The Double (CAPS(ALev))

time complexity

linearity for s

degree for s and n

#kernel elements for computation

No
No
Yes
Yes
Yes

cubic
cubic
quadratic
quadratic
quadratic

O(sn)
O(sn)
O(sn)
O(sn)
O(sn)

3

O(ksn + s )
O(s2 n)
O(ksn)
O(`sn + m2 s)
O(`sn + m2 s)

number of instances n

dimensionality d0

Dexter
Letter
MNIST
MiniBooNE
Covertype

2600
20000
60000
130064
581012

20000
16
784
50
54

(Fowlkes et al., 2004). We run the double NystroÌˆm method
with the spanning set S constructed by uniform random
sampling (Unif) and approximate leverage scores (ALev).
There are 10 episodes for each test, and 10 points on the
each line in the figures. For example, we set s = 500t,
` = (140 + 5t), and m = (250 + 50t) when n â‰¥ 20000,
where t = 1, 2, ..., 10. We display the experimental results
in Fig 1 and 2. As shown in the experiments, the double
NystroÌˆm method always shows better efficiency than other
methods under the same condition of using O(sn) kernel
elements. In the experiment on the Letter data set, we can
also notice that the error of the double NystroÌˆm method
more rapidly decreases to the optimal error than the others.

SVD (optimal)
Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

âˆ’1.12

Relative Error (k=20)

data set

Dexter

Dexter

Table 2. The summary of 5 real data sets. n is the number of
instances and d0 is the dimension of the original data

10

SVD (optimal)
Standard Nys.
One-shot Nys.
RandSVD + Nys.
Double Nys. (ALev)
Double Nys. (Unif)

âˆ’1.123

10
Relative Error (k=50)

The Sampling & NystroÌˆm Methods

âˆ’1.13

âˆ’1.127

10

âˆ’1.131

10

âˆ’1.135

10

10
0

10

Time (s)

1

10

0

1

10

10
Time (s)

Figure 2. Additional experiments for high dimensional data set.
We gradually increase s as 200, 400, 600,..., 2000.

7. Conclusion
In this paper, we provided a comprehensive analysis of
the one-shot NystroÌˆm method and a generalized view of
sampling strategy, and by integrating of these two results,
we proposed the â€œDouble NystroÌˆm Methodâ€ which reduces the size of the decomposition problem to a smaller
size in two stages. Both theoretically and empirically, we
demonstrated that the double NystroÌˆm method is much
more efficient than the various NystroÌˆm methods, but is
quite accurate. Thus, we recommend using the double
NystroÌˆm method for large-scale data sets.

Double NystroÌˆm Method: An Efficient and Accurate NystroÌˆm Scheme for Large-Scale Data Sets

Acknowledgments
W. Lim acknowledges support from the KAIST Graduate
Research Fellowship via Kim-Bo-Jung Fund. K. Jung acknowledges support from the Brain Korea 21 Plus Project
in 2015.

Kumar, Sanjiv, Mohri, Mehryar, and Talwalkar, Ameet.
Sampling methods for the NystroÌˆm method. The Journal
of Machine Learning Research, 98888:981â€“1006, 2012.

References

Li, Mu, Bi, Wei, Kwok, James T, and Lu, B-L. Largescale NystroÌˆm kernel matrix approximation using randomized svd. Neural Networks and Learning Systems,
IEEE Transactions on, 26(1):152â€“165, 2015.

Deshpande, Amit, Rademacher, Luis, Vempala, Santosh,
and Wang, Grant. Matrix approximation and projective
clustering via volume sampling. Theory of Computing,
2:225â€“247, 2006.

Mahoney, Michael W. and Drineas, Petros. Cur matrix decompositions for improved data analysis. Proceedings
of the National Academy of Sciences, 106(3):697â€“702,
2009.

Drineas, Petros and Mahoney, Michael W. On the NystroÌˆm
method for approximating a gram matrix for improved
kernel-based learning. The Journal of Machine Learning
Research, 6:2153â€“2175, 2005.

Sun, Xichen, Wang, Liwei, and Feng, Jufu. Further results on the subspace distance. Pattern recognition, 40
(1):328â€“329, 2007.

Drineas, Petros, Magdon-Ismail, Malik, Mahoney,
Michael W., and Woodruff, David P. Fast approximation
of matrix coherence and statistical leverage. Journal of
Machine Learning Research, 13:3475â€“3506, 2012.
Fine, Shai and Scheinberg, Katya. Efficient svm training
using low-rank kernel representations. The Journal of
Machine Learning Research, 2:243â€“264, 2002.
Fowlkes, Charless, Belongie, Serge, Chung, Fan, and Malik, Jitendra. Spectral grouping using the NystroÌˆm
method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):214â€“225, 2004.
Frieze, Alan, Kannan, Ravi, and Vempala, Santosh. Fast
monte-carlo algorithms for finding low-rank approximations. In Proceedings of FOCS, pp. 370â€“378. IEEE,
1998.
Gittens, Alex and Mahoney, Michael W. Revisiting
the NystroÌˆm method for improved large-scale machine
learning. In Proceedings of ICML, 2013.
Golub, Gene H and Van Loan, Charles F. Matrix computations, volume 3. JHU Press, 2012.
GuÌˆnter, S., Schraudolph, N., and Vishwanathan, S.V.N.
Fast iterative kernel principal component analysis. Journal of Machine Learning Research, 8, 2007.
Hsieh, Cho-Jui, Si, Si, and Dhillon, Inderjit S. Fast prediction for large-scale kernel machines. In Proceeding of
NIPS, pp. 3689â€“3697, 2014.
Kumar, Sanjiv, Mohri, Mehryar, and Talwalkar, Ameet. On
sampling-based approximate spectral decomposition. In
Proceedings of ICML, pp. 553â€“560. ACM, 2009.

Talwalkar, Ameet, Kumar, Sanjiv, and Rowley, Henry.
Large-scale manifold learning. In Proceedings of CVPR,
pp. 1â€“8. IEEE, 2008.
Wang, Liwei, Wang, Xiao, and Feng, Jufu. Subspace distance analysis with application to adaptive bayesian algorithm for face recognition. Pattern Recognition, 39(3):
456â€“464, 2006.
Wang, Shusen and Zhang, Zhihua. Improving CUR matrix decomposition and the NystroÌˆm approximation via
adaptive sampling. The Journal of Machine Learning
Research, 14(1):2729â€“2769, 2013.
Williams, Christopher and Seeger, Matthias. Using the
NystroÌˆm method to speed up kernel machines. In Proceedings of NIPS, 2001.
Zhang, Kai and Kwok, James T. Clustered NystroÌˆm
method for large scale manifold learning and dimension
reduction. IEEE Transactions on Neural Networks, pp.
1576â€“1587, 2010.
Zhang, Kai, Tsang, Ivor W, and Kwok, James T. Improved
NystroÌˆm low-rank approximation and error analysis. In
Proceedings of ICML, pp. 1232â€“1239. ACM, 2008.

