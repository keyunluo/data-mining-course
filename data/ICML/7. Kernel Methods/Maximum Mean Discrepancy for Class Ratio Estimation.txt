Maximum Mean Discrepancy for Class Ratio Estimation:
Convergence Bounds and Kernel Selection

Arun Iyer∗
Yahoo! Labs, Bangalore, Karnataka 560071 INDIA
Saketh Nath1
Sunita Sarawagi1
1
IIT Bombay, Powai, Mumbai, Maharashtra 400076 INDIA

Abstract
In recent times, many real world applications
have emerged that require estimates of class ratios in an unlabeled instance collection as opposed to labels of individual instances in the collection. In this paper we investigate the use of
maximum mean discrepancy (MMD) in a reproducing kernel Hilbert space (RKHS) for estimating such ratios.
First, we theoretically analyze the MMD-based
estimates. Our analysis establishes that, under
some mild conditions, the estimate is statistically
consistent. More importantly, it provides an upper bound on the error in the estimate in terms
of intuitive geometric quantities like class separation and data spread. Next, we use the insights obtained from the theoretical analysis, to
propose a novel convex formulation that automatically learns the kernel to be employed in the
MMD-based estimation. We design an efficient
cutting plane algorithm for solving this formulation. Finally, we empirically compare our estimator with several existing methods, and show
significantly improved performance under varying datasets, class ratios, and training sizes.

1. Introduction
The goal of this work is to estimate the ratio of classes in
any unlabeled test dataset given a labeled training dataset
with an arbitrarily different ratio of classes. The closely related problem of creating a classifier with shifted class priors in the training and test set has been extensively studied.
In contrast, our end goal is to estimate the ratio of classes
* This work was done by the author as part of the Sponsored PhD
Program with IIT Bombay.
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

ARUNIYER @ YAHOO - INC . COM

SAKETH @ CSE . IITB . AC . IN
SUNITA @ CSE . IITB . AC . IN

and not the labels of individual instances in the unlabeled
set. Many real-world applications have emerged that motivate this problem. As an example consider websites that
serve user directed content like news, videos, or reviews.
Each item (article, video, or product) is associated with
many user comments. An analyst wants to estimate the
fraction of comments that express positive/negative sentiments. The polarity of each comment is not of interest.
We now describe our problem formally. Let X = {x ∈
Rd } be the set of all instances and Y = {0, 1, . . . , c} be the
set of all labels. We are given a labeled dataset D(⊂ X ×
Y). Our goal is to design an estimator that for any given set
U (⊂ X ) can estimate the class ratios θ = [θ0 , θ1 , . . . , θc ]>
where θy denotes the fraction of instances with class label
y in U . We consider the case where the estimator will have
to handle unlabeled data with widely varying values of θ,
which might be different from those in the training data.
As in all existing work on the topic (discussed next) we
assume that the Pr(x|y) distribution remains unchanged in
the training and test distributions.
A baseline estimator is to use the labeled data D to train a
classifier C : X 7→ Y using supervised learning techniques
n̂
and estimate θy as nuy where nu is the size of U and n̂y is
the number of instances of U labeled y by C. Since most
supervised learning algorithms assume that the training and
test data follow the same distribution, this method is unlikely to perform well. Many fixes have been proposed: for
example, (Cortes & Mohri, 2004; Clémençon et al., 2009)
propose to maximize the area under the ROC to make the
classifier work for all possible class ratios, (Selvaraj et al.,
2011) proposes transductive learning, while (Elkan, 2001)
and (Lin et al., 2002) assume that the true class ratios are
known, and propose to reweight instances or rebalance the
decision cutoff. However, the primary goal of these methods is to improve per-instance accuracy, and class ratios
are estimated using the same paradigm of aggregating from
per-instance predictions.
Since we are not interested in the labels of individual in-

Maximum Mean Discrepancy for Class Ratio Estimation

stances, we explore direct methods of estimating the class
ratios. There have been three reported attempts for such direct estimation. The first is an EM-based approach (Saerens
et al., 2002) that alternates between estimating the class
ratios from per-instance predictions and ’correcting’ the
predictions using the estimated class ratios as priors. The
second approach (Plessis & Sugiyama, 2012), proposes to
minimize various f -divergence measures between the test
distribution and a model distribution parameterized on the
unknown θs and instance-level ratios of two distributions.
This method is developed in a semi-supervised setting and
involves solving an elaborate optimization problem for
each test set. The third, most recent approach (Zhang et al.,
2013), is based on minimizing the maximum mean discrepancy (MMD) over functions in a reproducing kernel
Hilbert space (RKHS) induced by a kernel K. The MMDbased approach has several advantages: it is applicable to
arbitrary domains since it does not assume any parametric
density on the data unlike conventional mixture modeling
approaches (Titterington, 1983; Woodward et al., 1984).
Because of this property, MMD has been successfully deployed in other problems including covariance shift (Gretton et al., 2009) and two-sample test (Gretton et al., 2012a).
When deployed for class ratio estimation, it gives rise to
a convex QP over a small number of variables, which is
efficiently solvable. However, the approach has not been
understood theoretically, empirical comparisons with other
class ratio estimation methods are lacking, and kernel selection has not been adequately addressed.
In this paper we address the above limitations of the MMD
approach. Specifically, we make these contributions:
We theoretically analyze the MMD-based estimator for arbitrary number of classes. Under some mild conditions, we
show that the estimator is statistically consistent. In addition to asymptotic convergence rates, we derive empirical
bounds that involve intuitive geometric quantities and motivate a kernel learning method. Analysis of the MMD-based
estimator is non-trivial and requires bounding techniques
that exploit the nature of the MMD objective in addition
to typical concentration inequalities employed in learning
theory. We are aware of no work that bounds the error of
class ratio estimates by any other method.
We use the insights obtained from the theoretical analysis,
to propose a novel convex formulation for selecting the best
kernel to be employed in the MMD-based estimation procedure. Our kernel learning formulation turns out to be an
instance of a Semi-Definite Program (SDP) with infinitely
many linear constraints.
Since it is expected that at optimality only a few of the linear constraints are active, we propose a cutting-plane based
algorithm for solving this formulation. At every iteration,
the SDP restricted to the current constraint-set is solved us-

ing a simple projected sub-gradient descent algorithm. We
are aware of no prior work on kernel selection for this problem.
We present an extensive evaluation of several existing
methods, both from the direct and per-instance aggregation
family, under varying true class ratios and training sizes.
We obtain up to 60% reduction in class ratio estimation errors over the best existing method.
Outline: In Section 2, we present an overview of the
MMD-based approach and analyze it theoretically. In Section 3, we provide our formulation for learning a kernel
function for improved estimates. In Section 4 we present
empirical comparisons and conclude in Section 5.

2. The Maximum Mean Discrepancy
approach
The core idea in maximum mean discrepancy (MMD) in a
reproducing kernel Hilbert space (RKHS) is to match two
distributions based on the mean of features in the Hilbert
space induced by a kernel K. This is justified because
when K is universal there is an injection between the space
of distributions and the space of mean feature vectors lying
in its RKHS. From a practical perspective too, the MMD
approach is appealing because unlike other parametric density estimation methods, it can be applied to arbitrary domains and to high-dimensional data, and is computationally
tractable. This approach was earlier used in the covariance
shift problem (Gretton et al., 2009), the two-sample problem (Gretton et al., 2012a), and recently in (Zhang et al.,
2013) for estimating class ratios.
Let K be a universal kernel and H denote the RKHS induced by K. Let Φ : X 7→ H denote the canonical
feature map induced by the kernel on the RKHS. We expect the test
P distribution PU (x) to match the distribution
Q(x) =
y PD (x|y)θy where θy denotes the unknown
probability of class y instances in U ’s distribution and
PD (x|y) denotes the training distribution for class y. This
holds because as mentioned earlier, we assume that
PU (x|y) = PD (x|y), ∀y ∈ Y

(A1)

Let Φ̄y and Φ̄u denote the true means of the feature vectors
of the y-th class and unlabeled data respectively. That is,
Φ̄y = EPD (x|y) Φ(x),

Φ̄U = EPU (x) Φ(x)

The true
P mean feature vector for the Q(x) distribution is
then y∈Y θy Φ̄y . To match Q(x) and PU (x), the MMD
approach minimizes the distance between the two means.
This gives rise to the following optimization problem over
the unknown θs:
X
min
k
θy Φ̄y − Φ̄U k2
Pc
(1)
θ:θy ≥0, y=0 θy =1
y∈Y

Maximum Mean Discrepancy for Class Ratio Estimation

or, after rewriting using θ0 = 1 −

Pc

y=1 θy

as


2
minc Āθ − ā

(2)

θ∈∧

where Ā = [Φ̄1 − Φ̄0 . . . Φ̄c − Φ̄0 ] and ā =P
[Φ̄U − Φ̄0 ] and
c
∧c denotes the new feasibility set {θy ≥ 0, y=1 θy ≤ 1}.
∗
Let θ denote the solution of the above, which is unique
because of the following two assumptions:
K is universal
(A2)
P
P
∀θ 6= θ 0 , y θy Pr(x|y) 6= y θy0 Pr(x|y) (A3)
A2 implies that there is an injection from the space of distributions to the space of mean feature vectors. A3 is a
standard identifiability assumption on θ without which the
class ratio estimation problem is undefined and no algorithm can identify the true class ratio.
However Equation 2 is impossible to solve as Φ̄y and Φ̄u
are unknowns. So, we approximate them by substituting
sample means from sample U of PU (x), and labeled sample D of PD (x|y) calculated as
b y (ny ) =
Φ

X
(x,y)∈D

Φ(x)
,
ny

b U (nu ) =
Φ

X Φ(x)
nu

x∈U

Here, nu denotes the number of instances in U , and ny
denotes the number of instances labeled y in D. The empirical version of the MMD above is:
2


b
(3)
−b
a(n)
minc A(n)θ

of this MMD estimate. The analysis of MMD for the covariance shift problem is very different (Yu & Szepesvari,
2012; Gretton et al., 2009; Cortes et al., 2008) from ours
because their objectives and assumptions are different.
2.1. Theoretical Analysis
b
We next discuss results that show the closeness of θ(n)
to
∗
the true θ both in the finite sample case and asymptotically. We already assumed that θ∗ is unique. For any
b is also required to be unique. Our
formal comparison, θ(n)
proof makes another mild assumption that makes the objective strongly convex:
Ā has full column rank
(A4)
b
A(n)
has full column rank (A5)
b
These assumptions imply the uniqueness of θ(n)
and θ∗ .
For example, A5 holds whenever all labeled and unlabeled
data points are distinct and K is universal.
At this point we note that typical learning theory bounds
are derived for knowing how close the objectives of (2) and
(3) are, while we desire to know how close their solutions
are. Also, (2) and (3) do not have a closed form solution1 .
Hence deriving finite sample closeness bounds as well as
asymptotic convergence between solutions of (2) and (3) is
interesting. To this end, we present the following theorem.
Theorem 1. With the notation and strong convexity assumptions (A4, A5) presented above, the following hold:
1. With probability at least 1 − δ,

θ∈∧

b
b 1 (n1 ) − Φ
b 0 (n0 ) . . . Φ
b c (nc ) − Φ
b 0 (n0 )]
where A(n)
= [Φ
b
b
and b
a(n) = [ΦU (nu ) − Φ0 (n0 )]. We call the solution to
b
this the MMD estimate θ(n).
In the paper we sometimes
b
b b
drop the (n) argument from A, Φ,
a, θb to reduce clutter.
We can apply the Kernel trick on this objective and rewrite
it in kernel form as follows:
b> A]
b θ − 2θ > [A
b> b
min θ > [A
a]

θ∈∧c

(4)

b> A]
b =Φ
b>
b
b> b b> b
where an entry yy 0 of [A
y Φy 0 − Φy Φ0 − Φ0 Φy 0 +
>
>
b Φ
b
by Φ
b y0 can be written in terms of a kernel
Φ
each Φ
0 0 and
P
1
as ny n 0 (x,y),(x0 ,y0 )∈D K(x, x0 ). Similarly an entry y of
y
b> b
b>
b
b> b
b> b
b> b
[A
a] = Φ
y ΦU − Φy Φ0 − Φ0 ΦU + Φ0 Φ0 can be written
in terms of kernel. Since the objective is convex in θ with
only the simplex constraints, algorithms such as Mirror Descent (Beck & Teboulle, 2003) can solve it efficiently.

R
∗ 2
b
kθ(n)−θ
k ≤

c2 +2c+2
nu

c
X
2
+
n
y=0 y

!

q
2

1 + log 2δ

b > A(n))
b
mineig(A(n)

(5)
n
o p
∗ 2
2
b
2. kθ(n) − θ k
→ 0 where mineig(M ) denotes the
minimum eigen value of M and R is the data-spread given
by R ≡ maxx∈X kΦ(x)k.
The proof has two key steps. The first is a result, given below as Lemma 1, that helps to bound the distance between
the optimal objectives of (2) and (3). The second is a result, given below as Lemma 2, that bounds the closeness
between the solutions of (2) and (3) in terms of closeness
between their optimal objectives. We proceed by presenting the two lemmas and defer their proofs to the end of the
section.
1

We note here that this MMD formulation is equivalent to
Equation 6 in (Zhang et al., 2013) when applied on discrete
labels and with a few other minor modifications. However,
we are aware of no prior work on the theoretical analysis

2

Problems (2) and (3) have closed form solutions (least
squares kind) only if we assume θy∗ 6= 0, θby 6= 0, ∀y. However,
in practice this is not reasonable to assume.
p
2
Also, {Xn } → X denotes that the sequence of random variables X1 , . . . , Xn , . . . converges in probability to X.

Maximum Mean Discrepancy for Class Ratio Estimation

Lemma 1.
∗
b −b
b
b θ(n)
kA(n)θ
−b
a(n)k2 − kA(n)
a(n)k2
(6)
!
!2
r
c
2
c2 + 2c + 2 X 2
≤ R2
+
1 + log
nu
n
δ
y
y=0

with at least probability 1 − δ.
Lemma 2. The following two claims hold:
b − θ ∗ k2 ≤
1. kθ(n)

∗
b
b
b
kA(n)θ
−b
a(n)k2 −kA(n)
θ(n)−b
a(n)k2
> A(n))
b
b
mineig(A(n)

b
Proof of Lemma 2 Let h(θ) ≡ kA(n)θ
−b
a(n)k2 . Since
h is quadratic in θ, we then have:
> ∗
b
b
b
h(θ∗ ) − h(θ(n))
= ∇h(θ(n))
(θ − θ(n))
> b
∗
b
b
b
+(θ∗ − θ(n))
A(n)> A(n)(θ
− θ(n))

>
b
b
Moreover, ∇h(θ(n))
(θ − θ(n))
≥ 0 for any θ ∈ ∧c .
b
This is because, θ(n) is the optimal solution of 3 and hence
b
the gradient at this point ∇h(θ(n))
should lie in the normal
c
b
cone of the feasibility set ∧ at θ(n). Hence,

2. With probability 1 − δ,
b > A(n))
b
mineig(Ā> Ā) − mineig(A(n)
v
!
u c
c
X 1
X
c2
1
1 u
t
+
log
≤ 8R2
n
n
n
δ
y
y
0
y=1
y=0

> b
∗
b
b
b
b
h(θ∗ ) − h(θ(n))
≥ (θ∗ − θ(n))
A(n)> A(n)(θ
− θ(n))
∗
2
b
b > A(n))kθ
b
≥ mineig(A(n)
− θ(n)k

This, together with assumption A5, gives Lemma 2.1. To
prove Lemma 2.2, we first prove in the supplementary that3

Now from Lemma 1 and Lemma 2.1, we obtain the first
result of the theorem. From Lemma 1, Lemma 2 and union
bound, we obtain that with probability 1 − δ, we have:
b − θ∗ k2
kθ(n)
q

2
 2
Pc
2
log 4δ
R2 c +2c+2
+
1
+
y=0 ny
nu
v
≤
!
u c
c
X
u X 1
1
c2
2
>
2
t
mineig(Ā Ā) − 8R
+
log
n
n
n0
δ
y=0 y
y=1 y
Note that the above bound holds as long as n ≡
(n0 , . . . , nc , nu ) is high enough such that the denominator
in the above expression is positive. Choosing such n is possible because of the assumption A4, mineig(
Ā> Ā) >
n
o 0.
p
∗ 2
b
From the above bound, we obtain that kθ(n) − θ k →
0 as n = (n0 , . . . , nc , nu ) → ∞. In the following we
provide a sketch of proof for the lemmas and postpone the
details to the supplementary.
∗
b
Proof of Lemma 1 First note that kA(n)θ
−b
a(n)k2 −
2
∗
2
b
b θ(n)−b
b
kA(n)
a(n)k ≤ kA(n)θ
−b
a(n)k . Now, we upper∗
b
bound the RHS. Let f (X0 , . . . , Xc , Xu ) ≡ A(n)θ
−
Pc
∗b
b
b
a(n) =
θ
Φ
(n
)
−
Φ
(n
),
where
X
,
X
dey
y
U
u
y
u
y
y=0
note independent samples of size ny , nu from PD (x|y) and
PU (x) respectively. Note that ||f (Xo , . . . , Xc , Xu )|| satisfies the bounded difference property and applying McDiarmid’s inequality we get that with probability at least 1−δ,
v
!
u
c
X
u
2
1
1
kf k − E[kf k] ≤ Rt2 log
+
(7)
δ nu y=0 ny

Next, we use E(kf k)2 ≤ E(kf k2 ) and the claim
Pc
2
E(kf k2 ) ≤ R2 ( c +2c+2
+ y=0 n2y ) to get the desired
nu
result.

b > A(n))
b
mineig(Ā> Ā) − mineig(A(n)
b > A(n)k
b
≤ kĀ> Ā − A(n)
F
b > A(n)k
b
Let g(X0 , . . . , Xc ) = kĀ> Ā − A(n)
F . It is easy
to verify that Eg = 0. Also, g satisfies the bounded difference property, hence by an application of McDiarmid’s
inequality, we get that with probability 1 − δ
v
!
u c
c
2
X
X 1
1 u
c
1
2
t
g ≤ 8R
+
log .
n
n
n
δ
y
y
0
y=0
y=1

(8)

This completes the proof for Lemma 2.
Theorem 1 is indeed interesting: first, it shows that our empirical estimate is statistically consistent. Second, it shows
that the
 convergence rate of the squared error is at least
O n1 . We observe this graphically in Figure 1(a) as we
see the error bound asymptotically going to zero with increasing training and test sizes. More importantly, in the
finite regime, the theorem provides an upper-bound on the
square error in terms of known and intuitive geometric
b > A(n))
b
quantities like mineig(A(n)
and R. In particular,
for the two-class case, this bound says that the estimate is
accurate whenever the distance between the sample means
of feature vectors of the two classes are far apart and the
overall data spread (R) is small. We illustrate the dependence graphically in Figure 1(b), where
q we plot the bounds
b> A).
b When
for increasing S/R where S = mineig(A
the positive and negative means are sufficiently separated
(S/R ≥ 0.7), the error bounds are quite tight. In the following section, we present a novel formulation for exploiting these bounds for kernel selection.
3

Here, kM kF denotes the Frobenius norm of M .

Maximum Mean Discrepancy for Class Ratio Estimation
1
0.8
0.6
0.4
0.2
0
1000

1

S/R = 0.3
0.8
S/R = 0.5
S/R = 0.7 0.6
S/R = 0.9 0.4

nu = 2000
nu = 25000
nu = 50000

0.2
0

10000

(a)

100000 1000000

0

0.5

1

1.5

2

(b)

Figure 1. Error bound computed by Theorem 1 against increasing
training and test size in log scale (Figure 1(a)), increasing separation ratio S/R (Figure 1(b)).

Comparison with other bounds We are aware of no
other work that bounds the error of class-ratio estimates
via any other method. For regression and under covariance shift, (Yu & Szepesvari, 2012) bounds
Pthe error of the
mean y in an unlabeled set U estimated as (x,y)∈D β̂(x)y
where β̂(x) is estimated using MMD for the covariance
shift problem. Even though the setting is different, it is interesting to compare their convergence rates (with 0/1 values of y) with ours for two classes.
Pc Their convergence rate
nnu
)
where
n
=
is O(log−s n+n
y=0 ny and s is a positive
u
constant. Note that these rates are much slower than ours.

3. The Kernel Learning Formulation
Our theoretical analysis shows that the universal kernel
used in the MMD estimator needs to be carefully chosen to
obtain accurate class-ratios. Here, we present a novel convex formulation for learning such a kernel. We also present
an efficient algorithm for solving this formulation.
We assume that we are given a set of base kernels
k1 , . . . , knk and
goal is to find the right conic combiPnthe
k
nation, kw = j=1
wj kj , wj ≥ 0 ∀ j, that makes the estimated class-ratios close to the true class-ratios. Posing the
problem of kernel learning as that of optimizing the kernel
weights in the conic combination is a popular strategy in
the kernel learning community (Lanckriet et al., 2002).
We first use the bounds in Theorem 1 to increase the
accuracy of class ratio estimates: we choose the kernel weights such that the upper-bound (5) is minimized.
There are two quantities in this upper-bound that deb > A(n))
b
pend on the kernel weights: i) mineig(A(n)
=
Pnk
>
bj (n) A
bj (n)), where A
bj (n) is the A
b
mineig(
wj A
j=1

term computed using the j th base kernel, and ii) R =
kwk2 . The first term needs to be maximized and the second
needs to be minimized. Since we wish to obtain a sparse set
of kernel weights, leading to kernel selection, we minimize
kwk1 instead of kwk2 .
Our goal of reducing error in the MMD-estimate may not
be adequately served by minimizing the upper bound alone
because the bound is derived without making any distribu-

tional assumptions (other than A1–A4) and may be overly
pessimistic for the given data distribution. We therefore include an empirical term in the objective that reduces the
deviation between the estimated and true class-ratios over
several datasets. We assume we have a set of datasets
{Ui }m
i=1 (each Ui is a set {xi1 , . . . , xinu }) with known
true class-ratios θ ∗i . We discuss later one way of obtaining such a dataset from the given labeled set D. Let
mmd(Ui , θ, w) denote the value of the MMD objective
( 4) at θ when applied to Ui with kernel kw . One way of
minimizing the deviation between the estimate and the true
class-ratios is by simply minimizing the deviation between
the corresponding mmd()s. We cast this goal in the maxmargin framework for structured learning (Tsochantaridis
et al., 2005): we want w to be such that for each (Ui , θ ∗i ),
mmd(Ui , θ ∗i , w) ≤ mmd(Ui , θ, w) for all θ far from θ ∗i .
We rewrite mmd(Ui , θ, w) as a linear function of w as:
mmd(Ui , θ, w) = −w> F(Ui , θ) s.t.
b> A
bj θ + 2θ > A
b> b
Fj (Ui , θ) = θ > A
j
j aj (Ui )

(9)
(10)

b j (Ui ) − Φ
b j with Φ
b j (Ui ) being the mean
where b
aj (Ui ) = Φ
0
feature map over the jth kernel calculated on sample Ui
b jy is also specialized to kernel kj . Let E(θ ∗i , θ) be a
and Φ
measure of the distance between θ ∗i and θ. We want the
margin w> F(Ui , θ ∗i ) − w> F(Ui , θ) ≡ w> δFi (Ui , θ) to
be large when E(θ ∗i , θ) is large.
Combining the two bound-related objectives and the empirical term we obtain the following convex formulation for
learning the kernel weights
min
n

w∈R+K ,ξ∈Rm
+
>

nk
X
b> A
bj )
kwk1 + Ckξk1 + B maxeig(
−wj A
j

s.t. w δFi (Ui , θ) ≥

j=1

E(θ ∗i , θ)

− ξi : ∀kθ − θ ∗i k ≥  ∀i
(11)

where ξ = {ξi }m
i=1 are the slack variables, C, B are the parameters of the optimization problem and  > 0 is a usergiven tolerance. The above is an instance of a convex program with infinitely many constraints and hence we resort
to solving it using the cutting-plane algorithm (Tsochantaridis et al., 2005). In Figure 2 we present an overview.
The input to the algorithm is a labeled dataset D and candidate kernels k1 , . . . , knK . We create the (Ui , θ ∗i ) dataset
pairs required for our training by resampling from available
labeled set D. To create m datasets we repeat this process
m times: first, sample a value of θ ∗i uniformly randomly
from the c + 1 dimensional simplex. If a prior distribution
on the test set θs is known, one can use it in place of the
uniform distribution. We did not assume any such priors
in our experiments. Second, form Ui as follows. Let T
be the expected size of the test set. For each class y, use
∗
sampling with replacement to select T θiy
instances from

Maximum Mean Discrepancy for Class Ratio Estimation

Input: D, k1 , . . . , knk
(Ui , θ ∗i ) = sampled sets from D with varying ratios θ ∗i
w = Initial parameter wj = 1
Initial constraint set S = {}
while no convergence do
ī, θ̄ = argmini,θ mmd(Ui , θ, w)−E(θ ∗i , θ) (Sec 3.1)
7:
If w> δFī (Uī , θ̄) ≥ E(θ ∗ī , θ̄) − ξī , then exit; else
add (ī, θ̄) to S.
8:
Solve (11) restricted to S and obtain w, ξ (Sec 3.2).
9: end while
1:
2:
3:
4:
5:
6:

Figure 2. Kernel selection algorithm for the MMD estimator.

this case can be written as a Semi-Definite Program (SDP):
n

For a given value of w we have to find the θ corresponding
to the most violating constraint for (Ui , θ ∗i ) by solving
min mmd(Ui , θ, w) − E(θ ∗i , θ)

θ∈∧c

(12)

This optimization problem differs from our original convex objective in only the additional E(θ ∗i , θ) term. Since
the θ corresponds to parameters of a multinomial distribution, suitable choices for E(θ ∗i , θ) are the Lγ distance
(γ ≥ 1) and KL-divergence both of which are convex
in θ. This makes objective (12) non-convex but since it
is the difference of two convex functions, algorithms like
CCCP (Yuille & Rangarajan, 2003) can give a local optimum. However, for two very apt error measures: the L∞
distance and L1 we are able to provide an optimal answer.
Both of these can be expressed as a max over a small number of linear functions of θ. For example when E(θ ∗i , θ)
∗
is the L∞ distance maxcy=0 |θiy
− θy |, we can rewrite it as
>
∗
c+1
c+1 λ (θ − θ ), where we use E
maxλ∈E∞
i
∞ to denote all
vectors with exactly one of the c + 1 positions either +1
or -1 and zero for the rest. There are 2c + 2 such vectors.
Now, we can find the most violating constraint by solving
these 2c + 2 MMD-like convex objectives:
min
minc mmd(Ui , θ) − λ> (θ − θ ∗i )
c+1

λ∈E∞

θ∈∧

(13)

Similarly, the L1 distance can be expressed as
maxλ∈E c+1 λ> (θ − θ ∗i ) where E1c consists of vec1
tors with +1 and -1 over any of the c positions.
3.2. Solving the MKL objective with finite constraints
This section focuses on solving (11) with the constraint-set
restricted to S. We begin by noting that the formulation in

kwk1 + C kξk1 + B t

s.t. w> δFi (Ui , θ) ≥ E(θ ∗i , θ) − ξi : ∀(i, θ) ∈ S,
nk
X
b>
b
tI +
wj A
(14)
j Aj  0,
j=1

where I is the identity matrix of size c × c. As long as c
and nk are not high, standard SDP solvers like Mosek or
SeDuMi can solve (14). Otherwise, we re-write the above
as the following non-differentiable convex problem:
nk
X
b>
b
−wj A
minn kwk1 + B maxeig(
j Aj )

Dy = {(x, y) ∈ D}. We now discuss the two crucial optimization problems solved within the loop of the algorithm:
(i) the selection of the most violating constraint (step 6), (ii)
solving the MKL objective with the finite set of constraints
(step 8). We elaborate on how each is solved.
3.1. Finding the most violating constraint

min

w∈R+K ,ξ∈Rm
+ ,t∈R

w∈R+K

+C

X

j=1

max

0, E(θ ∗i , θ)

− w> δFi (Ui , θ)



∀(i,θ)∈S

The above program can be solved using projected subgradient descent4 . The sub-gradient expression for the
first and third terms
is easy, for the secPnink the objective
bj ), the subgradient is5
b> A
ond term maxeig( j=1
−wj A
j
h
i>
> b> b
b> b
−e>
where ew is any
w A1 A1 ew . . . − ew Ank Ank ew
eigenvectorPcorresponding to the maximum eigenvalue of
nk
b> A
bj .
the matrix j=1
−wj A
j
Related work on kernel selection for MMD We are
aware of no other work on learning kernels in the context of
MMD-based class ratio estimation. For the two-sample test
problem, (Gretton et al., 2012b) proposed a kernel learning formulation that minimizes the asymptotic probability
of hypothesizing two distributions as same, when they are
different (Type II error) for a given bound of the Type I
error. In contrast, the novelty of our formulation is that it
includes both asymptotic terms and an empirical term that
minimizes error under finite data settings.

4. Experiments
We compare our proposed estimator with several existing
methods under various settings. First, we compare different methods under varying true class ratios. Second, we
compare them under varying training sizes. Finally, we
compare different kernel selection methods.
Our chosen kernel family KF is a conic combination of
univariate and multivariate Gaussian (RBF) kernels, a popular family in various kernel learning literature (Gretton
et al., 2012b; Vishwanathan et al., 2010). We first fix a
kernel width (σ) based on training data as per (Zhang et al.,
4

The feasibility set in this case is the first orthant and hence it
is simple to compute the projection onto this.
5
Please refer to the supplementary for the derivation.

Maximum Mean Discrepancy for Class Ratio Estimation
Dataset

Number
Features
Australian
14
Diabetes
8
German
24
Ionosphere
34
SAHeart
9
Youtube
1000
Acoustic
50
Botswana
145
Shuttle
9

Number
Instances
690
768
1000
351
462
6,431,471
78823
3248
43500

|Y|
ny
nu
c+1
2
200
100
2
200
100
2
200
100
2
200
100
2
200
100
2
250 1000
3 varying 10000
14 varying 10000
7 varying 10000

Table 1. Summary of Datasets

2013). For each feature, we have one univariate kernel with
this width. We create several multivariate kernels based on
bandwidths from this set: [2−6 2−5 . . . 26 ] ∗ σ ∗ d2 where
d is the number of features.
Methods: We compare the following methods.
SMO - MKL : As a first baseline, we estimate class ratio by
aggregating from per-instance predictions from a classifier. The classifier we used was SMO - MKL6 (Vishwanathan
et al., 2010) which trains a SVM but with the benefit of kernel selection from our kernel family KF .
PE - DR : As a member of the direct method, we use the PE
divergence based class ratio estimation method of (Plessis
& Sugiyama, 2012). We thank the authors for providing us
with the Matlab code for this method7 . We exclude results
for the method in (Saerens et al., 2002), since it offered no
benefit over the baseline SMO - MKL.
MMD : This is the MMD based approach (Section 2) with
a single best kernel chosen from our kernel family KF
through cross-validation. Recall that (Zhang et al., 2013)’s
proposal is also a MMD method — the only difference is in
how kernel parameters are chosen. We found that choosing
a single kernel via cross-validation provided much higher
accuracy than their formula of kernel width selection.
MMD - MKL : Here, we used MMD on a kernel learned as in
Section 3. The datasets i.e. {(Ui , θ ∗i )} pairs required for
this training were sampled from the training data D using
the method of Section 3 with m = 110. The class means
b y were estimated from the entire labeled data. The paΦ
rameters C and B were fixed via cross-validation.

Datasets: Table 1 summarizes the datasets we used. The
first six are binary datasets comprising five of the six UCI
datasets used in (Plessis & Sugiyama, 2012) and a dataset
based on YouTube comments that we created based on this8
6
Code
taken
from
http://research.microsoft.com/enus/um/people/manik/code/SMO-MKL/download.html
7
http://sugiyama-www.cs.titech.ac.jp/∼christo/classpriorchange
.html
8
http://mlg.ucd.ie/yt

collection. The goal in the YouTube dataset is to estimate
the fraction of comments that are spams on a YouTube
video. The dataset was crawled by tracking 6407 popular
YouTube videos over 77 days and comprises of 6,431,471
comments labeled spam or not. The feature set is a normalized TF-IDF vector over 1000 words + a comment length
feature. The next three are multi-class datasets. Acoustic is a dataset about classifying military vehicles from
geophone recordings and is used in (Plessis & Sugiyama,
2012). Botswana is a dataset about classifying spectral signatures into different land cover types and is from (Zhang
et al., 2013) and Shuttle is a UCI dataset.
We created a training set by sampling ny samples from
each class y, and series of test sets with nu points each for
a given ratio of classes θ ∗ . The default values of ny , nu are
in Table 1. For the binary datasets, all experiments are with
varying θ ∗ and for the multi-class datasets, the default θ ∗ is
the class prior skew in the entire labeled data. All numbers
are averaged over 10 random seeds. We measure error as
the L1 distance between the true and estimated class ratios
normalized by the number of classes.
Varying class ratios: We perform these experiments
on the six binary datasets by varying θ0∗ as per the
set {0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99}. In
Figure 4, we plot the estimation error (|θ0∗ − θb0 |) against
true fractions (θ0∗ ) for the six binary datasets. The following conclusions can be drawn from the plots: SMO - MKL,
the baseline that aggregates per-instance predictions, is indeed very sensitive to the changes in test prior distribution.
For many datasets, we see a ”bowl” shaped graph and usually the minimum is when test prior is close to 0.5 which is
equal to the training prior. The curves for the direct methods (PE - DR, MMD, MMD - MKL) are much flatter showing
that they are much less sensitive to the training class ratios.
Except for YouTube, both the MMD-based methods provide lower error than PE - DR. MMD - MKL is more accurate
than MMD in most cases, and on YouTube we get upto a
33% drop in error.
Running time: In this paper we skip a detailed comparison
on running time, instead we report some specific timings:
On YouTube, our largest dataset, MKL training via Matlab
takes 20 minutes whereas deployment takes 5 minutes on a
desktop class machine. On Botswana, the dataset with 14
classes, MKL training takes 6 minutes whereas deployment
takes 0.3 minutes. In comparison PE - DR took 12 minutes
on YouTube and 5 minutes on Botswana.
Increasing training size: For these experiments we vary
the value of ny (number of instances per class) in the range
[10 30 50 70 90] keeping all other values fixed to their default in Table 1. We selected the three multi-class datasets
for these experiments. We observe smooth error reduction
in MMD - MKL with increasing training size and consistent

Maximum Mean Discrepancy for Class Ratio Estimation
0.3

0.3

MMD-MKL
MMD
PE-DR
SMO-MKL

Australian

0.2
0.1
0

0.3

Diabetes

0.2

0.2

0.1

0.1

0
0.01 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.99

0.3

0
0.01 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.99

0.01 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.99

0.3

0.3

Ionosphere

0.2

0.2

0.1

0.1

0.1
0

0

0.01 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.99

0.01 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.99

0.01 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.99

Youtube

SAHeart
0.2

0

German

Figure 3. Class ratio estimation error (|θ0∗ − θb0 |) on Y-axis against varying true fractions (θ0∗ ) for the six binary datasets of Table 1. The
methods compared are same in all six datasets; the legend is present in only one of them to reduce clutter.
0.25

0.25

0.1

MMD-MKL 0.25
MMD
0.2
PE-DR
0.15
SMO-MKL
0.1

0.1

0.05

0.05

0.05

0

0

Shuttle
0.2
0.15

10

30

50

70

0.2
0.15

0
10

90

Botswana

Acoustic

30

50

70

90

10

30

50

70

90

Average Estimation Error

b + 1) on Y-axis against increasing per-class training size ny for the three multi-class
Figure 4. Class ratio estimation error (|θ∗ − θ|/c
datasets of Table 1. The methods compared are the same in all datasets; the legend is present in only one of them to reduce clutter.

0.2
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
Australian Botswana

Diabetes

MMD-MKL
MKL-EMPIRICAL

5. Conclusion

MKL-BOUND

In this paper we address a real-world motivated problem of
estimating the ratio of classes in an unlabeled set. We
investigated the use of the maximum mean discrepancy
(MMD) measure as a basis for estimating the class ratios.
We present the first ever theoretical analysis of the estimator and show that the MMD estimator is consistent under mild conditions. We provide empirical error bounds in
terms of intuitive quantities like class-separation and dataspread. Combining these bounds and empirical error we
propose a novel convex formulation for kernel learning and
also design an efficient cutting plane algorithm for solving
it. We empirically compare our estimator with many existing methods and obtain up to 60% reduction in error over
the best existing method. Further, our method of kernel
learning reduces plain MMD error by up to 40%.

German

Ionosphere SAHeart

UCI Datasets + Botswana

Figure 5. Comparing the kernel selection methods MMD - MKL,
MKL - BOUND and MKL - EMPIRICAL on various datasets

improvement over other methods. In contrast, SMO - MKL
has inconsistent behavior. The best improvement we get
is for the Acoustic dataset where MMD - MKL reduces error
from 0.12 to 0.05 with 90 instances per class.
Comparison of Kernel Selection: Our MKL attempts to
jointly minimize the theoretical error bounds and empirical
error. We evaluate if any one of them would be adequate
by comparing our joint model (MMD - MKL) with (1) MKL BOUND that minimizes only the Eigen and kwk1 terms in
(11), and (2) MKL - EMPIRICAL that drops the Eigen term.
In Figure 5 we plot error of these methods averaged over
various class ratios. We observe that the joint model provides the highest overall accuracy.

As part of future work, we wish to explore other families of
kernel selection, for example directly optimizing the width
of the RBF kernel as in (Gehler & Nowozin, 2008; Argyriou et al., 2006).

Acknowledgments
We thank the reviewers for their useful comments. This
work was partly supported by research grants from Yahoo!
Research.

Maximum Mean Discrepancy for Class Ratio Estimation

References
Argyriou, Andreas, Hauser, Raphael, Micchelli,
Charles A., and Pontil, Massimiliano.
A DCprogramming algorithm for kernel selection.
In
ICML, pp. 41–48, 2006.
Beck, Amir and Teboulle, Marc. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167 – 175,
2003.
Clémençon, Stéphan, Vayatis, Nicolas, and Depecker, Marine. AUC optimization and the two-sample problem. In
NIPS, 2009.
Cortes, Corinna and Mohri, Mehryar. AUC optimization
vs. error rate minimization. In NIPS, 2004.
Cortes, Corinna, Mohri, Mehryar, Riley, Michael, and Rostamizadeh, Afshin. Sample selection bias correction theory. In ALT, 2008.
Elkan, Charles. The foundations of cost-sensitive learning.
In IJCAI, 2001.
Gehler, Peter Vincent and Nowozin, Sebastian. Infinite kernel learning. Technical report, Max Planck Institute for
Biological Cybernetics, 2008.
Gretton, Arthur, Smola, Alexander J., Huang, Jiayuan,
Schmittfull, Marcel, Borgwardt, Karsten M., and
Schölkopf, Bernhard. Covariate shift and local learning by distribution matching, pp. 131–160. MIT Press,
Cambridge, MA, USA, 2 2009.
Gretton, Arthur, Borgwardt, Karsten M., Rasch, Malte J.,
Schölkopf, Bernhard, and Smola, Alexander J. A kernel
two-sample test. Journal of Machine Learning Research,
13:723–773, 2012a.
Gretton, Arthur, Sriperumbudur, Bharath K., Sejdinovic,
Dino, Strathmann, Heiko, Balakrishnan, Sivaraman,
Pontil, Massimiliano, and Fukumizu, Kenji. Optimal
kernel choice for large-scale two-sample tests. In NIPS,
2012b.
Lanckriet, Gert, Cristianini, Nello, Bartlett, Peter, and
Ghaoui, Laurent El. Learning the kernel matrix with
semi-definite programming. JMLR, 2002.
Lin, Yi, Lee, Yoonkyung, and Wahba, Grace. Support vector machines for classification in nonstandard situations.
Machine Learning Journal, 2002.
Plessis, Marthinus D. and Sugiyama, Masashi. Semisupervised learning of class balance under class-prior
change by distribution matching. In ICML, 2012.

Saerens, Marco, Latinne, Patrice, and Decaestecker, Christine. Adjusting the outputs of a classifier to new a priori
probabilities: a simple procedure. Neural Computation,
2002.
Selvaraj, Sathiya Keerthi, Bhar, Bigyan, Sellamanickam,
Sundararajan, and Shevade, Shirish. Semi-supervised
SVMs for classification with unknown class proportions
and a small labeled dataset. In CIKM, 2011.
Sriperumbudur, Bharath K., Fukumizu, Kenji, Gretton,
Arthur, Lanckriet, Gert R. G., and Schölkopf, Bernhard.
Kernel choice and classifiability for RKHS embeddings
of probability distributions. In NIPS, pp. 1750–1758,
2009.
Titterington, D. M. Minimum distance non-parametric estimation of mixture proportions. Journal of the Royal
Statistical Society. Series B (Methodological), 45(1):pp.
37–46, 1983.
Tsochantaridis, I., Joachims, T., Hofmann, T., and Altun, Y.
Large margin methods for structured and interdependent
output variables. JMLR, 6:1453–1484, 2005.
Vishwanathan, S. V. N., Sun, Zhaonan, TheeraAmpornpunt, Nawanol, and Varma, Manik. Multiple
kernel learning and the SMO algorithm. In Advances
in Neural Information Processing Systems, December
2010.
Woodward, Wayne A., Parr, William C., Schucany,
William R., and Lindsey, Hildegard. A comparison of
minimum distance and maximum likelihood estimation
of a mixture proportion. Journal of the American Statistical Association, 79(387):590–598, 1984.
Yu, Yaoliang and Szepesvari, Csaba. Analysis of kernel
mean matching under covariate shift. In ICML, 2012.
Yuille, A. L. and Rangarajan, Anand. The concave-convex
procedure. Neural Computation, 15(4):915936, 2003.
Zhang, Kun, Schölkopf, Bernhard, Muandet, Krikamol,
and Wang, Zhikun. Domain adaptation under target and
conditional shift. In ICML, 2013.

