A Kernel Independence Test for Random Processes

Kacper Chwialkowski
University College London, Computer Science Department

KACPER . CHWIALKOWSKI @ GMAIL . COM

Arthur Gretton
University College London, Gatsby Computational Neuroscience Unit

Abstract
A non-parametric approach to the problem of
testing the independence of two random processes is developed. The test statistic is the
Hilbert-Schmidt Independence Criterion (HSIC),
which was used previously in testing independence for i.i.d. pairs of variables. The asymptotic
behaviour of HSIC is established when computed
from samples drawn from random processes. It
is shown that earlier bootstrap procedures which
worked in the i.i.d. case will fail for random
processes, and an alternative consistent estimate
of the p-values is proposed. Tests on artificial
data and real-world forex data indicate that the
new test procedure discovers dependence which
is missed by linear approaches, while the earlier
bootstrap procedure returns an elevated number
of false positives.

1. Introduction
Measures of statistical dependence between pairs of random variables (X, Y ) are well established, and have been
applied in a wide variety of areas, including fitting causal
networks (Pearl, 2000), discovering features which have
significant dependence on a label set (Song et al., 2012),
and independent component analysis (Hyvärinen et al.,
2004). Where pairs of observations are independent and
identically distributed, a number of non-parametric tests
of independence have been developed (Feuerverger, 1993;
Gretton et al., 2007; Székely et al., 2009; Gretton & Györfi,
2010), which determine whether the dependence measure
value is statistically significant. These non-parametric tests
are consistent against any fixed alternative - they make no
assumptions as to the nature of the dependence.
For many data analysis tasks, however, the observations beProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

ARTHUR . GRETTON @ GMAIL . COM

ing tested are drawn from a time series: each observation
is dependent on its past values. Examples include audio
signals, financial data, and brain activity. Given two such
random processes, we propose a hypothesis test of instantaneous dependence, of whether the two signals are dependent at a particular time t. Our test satisfies two important
properties: it is consistent against any fixed alternatives,
and it is non-parametric - we do not assume the dependence takes a particular form (such as linear correlation),
nor do we require parametric models of the time series. We
further avoid making use of a density estimate as an intermediate step, so as to avoid the assumption that the distributions have densities (for instance, when dealing with text
or other structured data).
We use as our test statistic the Hilbert-Schmidt Independence Criterion (HSIC) (Gretton et al., 2005; 2007), which
can be interpreted as the distance between embeddings of
the joint distribution and the product of the marginals in a
reproducing kernel Hilbert space (RKHS) (Gretton et al.,
2012, Section 7). When characteristic RKHSs are used, the
HSIC is zero iff the variables are independent (Sriperumbudur et al., 2010). Under the null hypothesis of independence, PXY = PX PY , the minimum variance estimate of
HSIC is a degenerate U-statistic. The distribution of the
empirical HSIC under the null is an infinite sum of independent χ2 variables (Gretton et al., 2007), which follows
directly from e.g. (Serfling, 2009, Ch. 5). In practice, given
a sample (xi , yi )ni=1 of pairs of variables drawn from PXY ,
the null distribution is approximated by a bootstrap procedure, where a histogram is obtained by computing the test
statistic on many different permutations {xi , yπ(i) }ni=1 , to
decouple X and Y .
In the case where the samples Zt = (Xt , Yt ) are drawn
from a random process, the analysis of the asymptotic behaviour of HSIC requires substantially more effort than in
the i.i.d. case. As our main contribution, we obtain both the
null and alternative distributions of HSIC for random processes, where the null distribution is defined as Xt being
independent of Yt at time t. Such a test may be used for re-

A Kernel Independence Test for Random Processes

jecting causal effects (i.e., whether one signal is not dependent on the values of another signal at a particular delay) or
instant coupling (see our first experiment in Section 4.2).1
The null distribution is again an infinite weighted sum of
χ2 variables, however these are now correlated, rather than
independent. Under the alternative hypothesis, the statistic
has an asymptotically normal distribution.
For the test to be used in practice, we require an empirical estimate of the null distribution, which gives the correct
test threshold when Zt = (Xt , Yt ) is a random process.
Evidently, the bootstrap procedure used in the i.i.d. case is
incorrect, as the temporal dependence structure within the
Yt will be removed. This turns out to cause severe problems in practice, since the permutation procedure will give
an increasing rate of false positives as the temporal dependence of the Yt increases (i.e., dependence will be detected
between Xt and Yt , even though none exists, this is also
known as a Type I error). Instead, our null estimate is obtained by making shifts of one signal relative to the other,
so as to retain the dependence structure within each signal.
Consequently, we are able to keep the Type I error at the
designed level α = 0.05. In our experiments, we address
three examples: one artificial case consisting of two signals
which are dependent but have no correlation, and two realworld examples on forex data. HSIC for random processes
reveals dependencies that classical approaches fail to detect. Moreover, our new approach gives the correct Type
I error rate, whereas a bootstrap-based approach designed
for i.i.d. signals returns too many false positives.
Related work Prior work on testing independence in
time series may be categorized in two branches: testing
serial dependence within a single time series, and testing
dependence between one time series and another. The case
of serial dependence turns out to be relatively straightforward, as under the null hypothesis, the samples become independent: thus, the analysis reduces to the i.i.d.
case. Pinkse (1998); Diks & Panchenko (2005) provide
a quadratic forms function-based serial dependence test
which employs the same statistic as HSIC. Due to the simple form of the null hypothesis, the analysis of (Serfling,
2009, Ch. 5) applies. Further work in the context of the serial dependency testing includes simple approaches based
on rank statistics e.g. Spearman’s correlation or Kendall’s
tau, correlation integrals e.g. (Broock et al., 1996); criteria
based on integrated squared distance between densities e.g
(Rosenblatt & Wahlen, 1992); KL-divergence based criteria e.g. (Robinson, 1991; Hong & White, 2005); and generalizations of KL-divergence to so called q-class entropies
e.g. (Granger et al., 2004; Racine & Maasoumi, 2007).
1

We distinguish our case from the problem of ensuring time
series are independent simultaneously across all time lags, e.g the
null will hold even if Xt = Yt−1 where Yt is white noise.

In most of the tests of independence of two time series, specific conditions have been enforced, e.g that processes follow a moving average specification or the dependence is linear. Prior work in the context of dependency
tests of two time series includes cross covariance based
tests e.g. (Haugh, 1976; Hong, 1996; Shao, 2009); and
a Generalized Association Measure based criterion (Fadlallah et al., 2012). Some work has been undertaken in
the non-parametric case, however. A non-parametric measure of independence for time series, based on the HilbertSchmidt Independence Criterion, was proposed by Zhang
et al. (2008). While this work established the convergence in probability of the statistic to its population value,
no asymptotic distributions were obtained, and the statistic was not used in hypothesis testing. To our knowledge,
the only non-parametric independence test for pairs of time
series is due to Besserve et al. (2013), which addresses
the harder problem of testing independence across all time
lags simultaneously. 2 The procedure is to compute the
Hilbert-Schmidt norm of a cross-spectral density operator
(the Fourier transform of the covariance operator at each
time lag). The resulting statistic is a function of frequency,
and must be zero at all frequencies for independence, so a
correction for multiple hypothesis testing is required. It is
not clear how the asymptotic analysis used in the present
work would apply to this statistic, and this remains an interesting topic of future study.
The remaining material is organized as follows. In Section 2 we provide a brief introduction to random processes
and various mixing conditions, and an expression for our
independence statistic, HSIC. In Section 3, we characterize the asymptotic behaviour of HSIC for random variables
with temporal dependence, under the null and alternative
hypotheses, and establish the test consistency. We propose
an empirical procedure for constructing a statistical test,
and demonstrate that the earlier bootstrap approach will not
work for our case. Section 4 provides experiments on synthetic and real data.

2. Background
In this section we introduce necessary definitions referring
to random processes. We then go on to define a V-statistic
estimate of the Hilbert-Schmidt Independence Criterion,
which applies in the i.i.d. case.
Random process. First, we introduce the probabilistic
tools needed for pairs of time series. Let (Zt , Ft )t∈N be
a strictly stationary sequence of random variables defined
on a probability space Ω with a probability measure P and
natural filtration Ft . Assume that Zt denotes a pair of ran2

Let Xt follow a MA(2) model and put Yt = Xt−20 . This is
a case addressed by Besserve et al. (2013), who will reject their
null hypothesis, whereas our null is accepted

A Kernel Independence Test for Random Processes

dom variables i.e. Zt = (Xt , Yt ), where Xt is defined on
X , and Yt on Y. Each Zt takes values in a measurable Polish space (Z, B(Z), PZ ). The space Z is a Cartesian product of two Polish spaces X and Y, endowed with a natural
Borel sigma field and a probability measure.
We introduce a sequence of independent copies of Z0 , i.e.,
(Zt∗ )t∈N . Since Zt is stationary, Zt∗ retains the dependence
between random variables Xt and Yt , but breaks the temporal dependence.
Next, we formalize a concept of memory of a process. A
process is called absolutely regular (β-mixing) if β(m) →
0, where
β(m) =

I X
J
X
1
sup sup
|P (Ai ∩ Bj ) − P (Ai )P (Bj )|.
2 n
i=1 j=1

The second supremum in the β(m) definition is taken
over all pairs of finite partitions {A1 , · · · , AI } and
{B1 , · · · , BJ } of the sample space such that Ai ∈ An1 and
c
Bj ∈ A ∞
n+m , and Ab is a sigma field spanned by a subc
sequence, Ab = σ(Zb , Zb+1 , ..., Zc ). A process is called
uniform mixing (φ-mixing) if φ(m) → 0, where
φ(m) = sup sup
n

A∈An
1

sup

|P (B|A) − P (B)|.

B∈A∞
n+m

Uniform mixing implies absolute regularity, i.e. β(m) ≤
φ(m) (Bradley et al., 2005). Under technical assumptions,
Autoregressive Moving Average processes — or more generally Markov Chains — are absolutely regular or uniformly mixing (Doukhan, 1994).
Hilbert-Schmidt Independence Criterion Let k, l be
positive definite kernels associated with respective reproducing kernel Hilbert spaces HX on X , and HY on Y.
We assume that k and l are bounded and continuous. We
associate to the random variable X a mean embedding
µX (x) := EX k(X, x), such that ∀f ∈ HX , hf, µX iHX =
EX (f (X)) (Berlinet & Thomas-Agnan, 2004; Smola et al.,
2007). We assume k, l are characteristic kernels, meaning the mappings µX and µY (y) := EY l(Y, y) are injective embeddings of the probability measures to the corresponding RKHSs; i.e., distributions have unique embeddings (Fukumizu et al., 2008; Sriperumbudur et al., 2010).
We next recall a measure of statistical dependence, the
Hilbert-Schmidt Independence Criterion (HSIC), which
can be expressed in terms of expectations of RKHS kernels
(Gretton et al., 2005; 2007). Denote a group of permutations over 4 elements by S4 , with π one of its elements,
i.e., a permutation of four elements. We define
1 X
k(xπ(1) , xπ(2) )[l(yπ(1) , yπ(2) )+
h(z1 , z2 , z3 , z4 ) =
4!
π∈S4

+ l(yπ(3) , yπ(4) ) − 2l(yπ(2) , yπ(3) )].

Lemma 1. Let γ be an expected value of the function h,
γ = Eh(Z1∗ , Z2∗ , Z3∗ , Z4∗ ). This expectation corresponds
to HSIC, computed using a function symmetric in its arguments. For k and l characteristic, continuous, translation
invariant, and vanishing at infinity, γ is equal to zero if and
only if the null hypothesis holds (see (Lyons, 2013, Lemma
3.8), applying (Sriperumbudur et al., 2011, Proposition 2),
and the note at the end of Section 5).
The value of γ corresponds to a distance between embeddings of (X1∗ , Y2∗ ) and (X1∗ , Y1∗ ) to an RKHS with the
product kernel κ = k · l (Gretton et al., 2012, Section 7).
A biased empirical estimate of the Hilbert-Schmidt Independence Criterion can be expressed as a V -statistic (the
unbiased estimate is a U-statistic, however the difference
will be accounted for when constructing a hypothesis test,
through an appropriate null distribution).
V statistics. A V -statistic of a k-argument, symmetric
function f is written
X
1
f (Zi1 , ..., Zik ).
(1)
V (f, Z) = k
n
1≤i1 ,··· ,ik ≤n

Gretton et al. (2005) show that the biased estimator of
γ is V (h, Z). The asymptotic behaviour of this statistic depends on the degeneracy of the function that defines
it. We say that a k-argument, symmetric function f is jdegenerate (j < k) if for each z1 , · · · , zj ∈ Z,
∗
Ef (z1 , · · · , zj , Zj+1
, · · · , Zk∗ ) = 0.

If j = k − 1 we say that the function is canonical. We refer
to a normalized V statistic as a V -statistic multiplied by the
sample size, n · V .

3. HSIC for random processes
In this section we construct the Hilbert-Schmidt Independence Criterion for random processes, and define its
asymptotic behaviour. We then introduce an independence
testing procedure for time series.
We introduce two hypotheses: the null hypothesis H0 that
Xt and Yt are independent, and the alternative hypothesis
H1 that they are dependent. To build a statistical test based
on n · V (h, Z) we need two main results. First, if null hypothesis holds, we show n · V (h, Z) converges to a random
variable. Second, if the null hypothesis does not hold, the
n · V (h, Z) estimator diverges to infinity. Following these
results, the Type I error (the probability of mistakenly rejecting the null hypothesis) will stabilize at the design parameter α, and the Type II error (the probability of mistakenly accepting the null hypothesis when the variables are
dependent) will drop to zero, as the sample size increases.
We begin by introducing an auxiliary kernel function s, and
characterize the normalized V -statistic distribution of s us-

A Kernel Independence Test for Random Processes

ing a CLT introduced by (Borisov & Volodko, 2008). We
then show that the normalized V -statistic associated with
the function s has the same asymptotic distribution as the
n · V (h, Z) distribution.
Let s be an auxiliary
k̃(x1 , x2 )˜l(y1 , y2 ), where

function

s(z1 , z2 )

=

We now characterize the asymptotics of V (h, Z).

− Ek(X1∗ , x2 ) + Ek(X1∗ , X2∗ ),
and ˜l is defined similarly.
Both k̃ and ˜l are kernels, meaning that they are dot products between features centred in their respective RKHSs
(Berlinet & Thomas-Agnan, 2004). Therefore s = k̃ · ˜l
defines a kernel on a product space of pairs Zt . Using Mercer’s Theorem we obtain an expansion for s.
Statement 1. By Steinwart & Scovel (2012) Corollary 3.5,
the bounded, continuous kernel s has a representation3
∞
X

Eτa τb = Eea (Z1 )eb (Z1 )+
∞
X
+
[Eea (Z1 )eb (Zj+1 ) + Eeb (Z1 )ea (Zj+1 )] .
j=1

k̃(x1 , x2 ) =k(x1 , x2 ) − Ek(x1 , X2 )

s(za , zb ) =

where τj is a centred Gaussian sequence with the covariance matrix

λi ei (za )ei (zb ),

(2)

i=1

where (ei )i∈N+ denotes an orthonormal basis of
PN
L2 (Z, B(Z), PZ ). The series ( i=1 λi ei (za )ei (zb ))
converges absolutely and uniformly. ei are eigenfunctions
of s and λi are eigenvalues of s.
We will henceforth assume that for every collection of
pairwise distinct subscripts (t1 , t2 ), the distribution of
(Zt1 , Zt2 ) is absolutely continuous with respect to the
(Zt∗1 , Zt∗2 ) distribution. This assumption prevents the occurrence of degenerate cases, such that all Zt being the
same. The following results are proved in Section 5.1.
Lemma 2. Let the process Zt have a mixing coefficient
smaller than m−3 (β(m), φ(m) ≤ m−3 ) and satisfy either
of the following conditions:

Theorem 1. Under assumptions of Lemma 2, if H0 holds,
then the asymptotic distribution of the empirical HSIC
(with scaling n) is the same as that of n · V (s, Z),
D

lim n · V (h, Z) = lim n · V (s, Z).

n→∞

Theorem 2. Under assumptions
of the Lemma 2, if H1
√
holds, then γ > 0 and n(V (h, Z)−γ) has asymptotically
normal distribution with mean zero and finite variance.
Consequently, if the null hypothesis does not hold then
P (n · V (h, Z) > C) = P (V (h, Z) > C
n ) → 1 for any
fixed C. Finally, we show that the γ estimator is easy
to compute. According to Gretton et al. (2007, equation
4), V (h, Z) = n−2 trHKHL, where Kab = k(Xa , Xb ),
Lab = l(Ya , Yb ) ,Hij = δij − n−1 and n is a sample size.
Testing procedure We begin by showing that the H0 distribution of the γ estimator obtained via the bootstrap approach of (Diks & Panchenko, 2005; Gretton et al., 2007)
gives an incorrect p-value estimate when used with independent random processes. In fact, the null hypothesis obtained by permutation corresponds to the processes being
both i.i.d. and independent from each other. Recall the
covariance structure of the γ estimator from Theorem 1,
Eτa τb = Eea (Z1 )eb (Z1 )+
∞
X
+
[Eea (Z1 )eb (Zj+1 ) + Eeb (Z1 )ea (Zj+1 )] .

A Zt is φ-mixing.

j=1

B Zt is β-mixing. For some  > 0 and for an even number
c ≥ 2, the following holds
1. supi E|ei (X1 )|2+ ≤ ∞, where ei is the basis
introduced in the Statement 1 and | · | denotes an
absolute value.
P∞
/(2+)
2.
(m) < ∞.
m=1 β
If the null hypothesis holds, then s is a canonical function
and a kernel. What is more,
D

lim n · V (s, Z) =

n→∞
3

n→∞

∞
X

(3)
Y
We can represent ea and eb as ea (z) = eX
u (x)eo (y),
X
Y
eb (z) = ei (x)ep (y), as a decomposition of the Z basis
into bases of X and Y, respectively. Consider a partial sum
Tn of series from the above equation (3), with Xt replaced
with its permutation Xπ(t) ,

Tn =

n
X

X
Y
Y
EeX
u (Xπ(1) )ei (Xπ(j+1) )Eeo (Y1 )ep (Yj+1 ).

j=1

(4)

λj τj2 ,

j

A bounded kernel is compactly embedded
L2 (Z, B(Z), PZ ) (Steinwart & Scovel, 2012).

into

Using covariance inequalities from (Doukhan, 1994,
Section 1.2.2) we conclude that EeYo (Y1 )eYp (Yj+1 ) =
1
X
O(Λ(j) 2 ) and EeX
u (Xπ(1) )ei (Xπ(j+1) ) = O(Λ(|π(j) −

A Kernel Independence Test for Random Processes
1

π(1)|) 2 ) where Λ is an appropriate mixing coefficient (β
or φ). Recall that 0 < Λ(j) < Cj −3 .
We can therefore reduce the problem to the convergence of
a random variable
n
X
1
1
Sn =
(5)
Λ(j) 2 Λ(|π(j) − π(1)|) 2 ,
j=1

where π is a random permutation drawn from the uniform
distribution over the set of n-element permutations. In the
supplementary material we show that this sum converges in
probability to zero.
Since Sn > Tn > 0, then Tn converges to zero in probability, and consequently the covariance matrix entry Eτa τb
converges to unity for a = b, and to zero otherwise. Indeed,
the expected value Eea ((Xπ(1) , Y1 ))eb ((Xπ(1) , Y1 )) = 0 if
a 6= b and is equal to one otherwise. Note that this is the
covariance matrix described by Gretton et al. (2007).
A correct approach to approximating the asymptotic null
distribution of n · V (h, Z) under H0 is by shifting of one
time series relative to the other. Define the shifted process
Stc = Yt+c mod n for an integer c, 0 ≤ c ≤ n and 0 ≤ t ≤ n.
If we let c vary over 0 ≤ A ≤ B ≤ n for A such that
the dependence between Yt+A and Xt is negligible, then
we can approximate the null distribution with an empirical
distribution calculated on points (V (h, Z k ))A≤k≤B , where
Ztk = (Xt , Stk ). This is due to the fact that the shifted
process Stc retains most of the dependence, since it does
not scramble the time index.4 We call this method Shift
HSIC. In the supplementary material we show that Shift
HSIC samples from the correct null distribution.

4. Experiments
In the experiments we compare Shift HSIC with the Bootstrap HSIC of Gretton et al. (2007). We investigate three
cases: an artificial dataset, where two time series are coupled non-linearly; and two forex datasets, where in one
case we seek residual dependence after one time series has
been used to linearly predict another, and in the other case,
we reveal strong dependencies between signals that are not
seen via linear correlation.
4.1. Artificial data
Non-linear dependence. We investigate two dependent,
autoregressive random processes Xt ,Yt , specified by
Xt = aXt−1 + t

Yt = aYt−1 + ηt ,

(6)

with an autoregressive component a. The coupling of the
processes is a result of the dependence in the innovations
4

As a illustration, consider Wt = Yt−10 . If Yt is stationary
then the dependence structure of (Wt1 , Wt2 ) and (Yt1 , Yt2 ) is
the same. If we set Wt = Yπ(t) this property does not hold.

Algorithm 1 Generate innovations
Input: extinction rate 0 ≤ p ≤ 1, radius r.
repeat
Initialize ηt , t to N (0, 1) and d to a number uniformly
distributed on [0, 1] .
if ηt2 + 2t > r2 or d > p then
return ηt , t
end if
until true

t , ηt . These t , ηt are drawn from an Extinct Gaussian distribution, defined in Algorithm 1. The parameter p (called
extinction rate) controls how often a point drawn form a
ball B(0, r) dies off. According to Algorithm 1, the probability of seeing a point inside the ball B(0, r) is different than for a two dimensional Gaussian N (0, Id). On the
other hand, as p goes to zero, the Extinct Gaussian converges in distribution to N (0, Id). Figure 1 illustrates the
joint distribution of Xt , Yt . The left scatter plot in Figure 1 presents Xt and Yt generated with an extinction rate
of 50%, while the right hand plot is generated with an extinction rate of 99.87%. Processes used in this experiment
had an autoregressive component of 0.2, and the radius of
the innovation process was 1.
Figure 2 compares the power of the Shift HSIC test and the
correlation test. The X axis represents an extinction rate,
while the Y axis shows the true positive rate. Shift HSIC
is capable of detecting non-linear dependence between Xt
and Yt , which is missed by linear correlation. The red star
depicts performance of the KCSD algorithm developed by
Besserve et al. (2013), with parameters tuned by its authors:
note that this result required using four times as many data
points as HSIC.
False positive rates. We next investigate the rate of false
positives for Shift HSIC and Bootstrap HSIC on independent copies of the AR(1) processes used in the previous
experiment. To generate independent processes, we first
sampled two pairs (Xt , Yt ), (Xt0 , Yt0 ) of time series using
(6), and then constructed Z by taking X from the first pair
and Y from the second, i.e., Zt = (Xt , Yt0 ). We set an
extinction rate to 50%. 5 The AR component a in the
model (6) controls the memory of a processes - the larger
this component, the longer the memory. We performed the
Shift HSIC and the Bootstrap HSIC tests on Zt generated
under H0 with different AR components. Figure 3 illustrates the results of this experiment. The X axis is indexed
by the AR component and Y axis shows the FP rate. As the
temporal dependence increases, the Bootstrap HSIC incorrectly gives an increasing number of false positives: thus,
5

As a reviewer pointed out, the example for the FP rates can be
simplified, however we decided to be consistent with the marginal
distribution of Xt ,Yt across the experiments.

A Kernel Independence Test for Random Processes
1

t

X

False positive

Y

Y

t

0.8

X

t

t

Figure 1. Xt and Yt , described in the Experiment 4.1, with extinction rates 50% (left) and 99.8% (right), respectively.

0.6

0.4

0.2

0
0.3
1
0.9
0.8

Covariance
Bootstrap
Shift
KCSD

0.4

0.5

0.6
0.7
AR component

0.8

0.9

1

Figure 3. False positive rate for the Shift HSIC and the Bootstrap
HSIC. The sample size was 1200, and results were averaged over
300 repetitions.

0.7
True positive

Bootstrap
Shift
5% p−value

0.6
0.5
0.4
0.3
0.2
0.1
0

0.4

0.5

0.6
0.7
Extinction rate

0.8

0.9

1

Figure 2. True positive rate for the Shift HSIC, the Bootstrap
HSIC and correlation based test: sample size 1200, results averaged over 300 repetitions. The red star shows KCSD performance
at 4× the HSIC sample size; see Section 4.1 for details.

Instantaneous coupling and causal effect. Having one
Australian dollar we may obtain a quantity of Yen in two
ways, either by using AUD/JPY exchange rate explicitly
or by buying Canadian dollars and then selling them at the
CAD/JPY rate. Let Xt be a differentiated AUD/JPY exchange rate and Yt be a differentiated product of exchange
rates AUD/CAD×CAD/JPY. We will investigate the relation between these two. Common sense dictates that Yt
should behave similarly to Xt . After examining the crosscorrelation of Xt and Yt , we propose a simple regression
model to describe the interaction between the signals,
Ŷt = a0 Xt + a1 Xt−1 + · · · + a6 Xt−6 .

it cannot be relied on to detect dependence in time series.
The Shift HSIC false positive rate remains at the targeted
5% p-value level.
4.2. Forex data
We use Foreign Exchange Market quotes to evaluate Shift
HSIC performance on the real life data. Practitioners point
out that forex time series are noisy and hard to handle,
especially at low granulations (smaller then 15 minutes).
We decided to work with forex time series to show that
Shift HSIC can detect dependence even on such a difficult
dataset. The forex time series were granulated to obtain
two minute sampling (the granulation function returned the
last price in the two minute window). Using the test of Diks
& Panchenko (2005), we checked that serial dependence of
the differentiated time series decays fast enough to satisfy
the assumed mixing conditions (by a differentiated time series, we refer to (Xt − Xt−1 )t∈N ). The choice of the pairs
and trading day (21st January 2013) were arbitrary.

We fit the model and see that a0 = 0.97, and the remaining coefficients are not bigger then 0.06 in absolute value.
This suggest that most of the dependence is explained by an
instantaneous coupling. We further investigate the crosscorrelation between residuals Rt = Yt − Ŷt and Xt . We
observe no significant correlations in the first 30 lags.
Next we investigate dependence of residuals with lagged
values of the explanatory variables, i.e., Rt with Xt−k for
k ∈ (0, · · · , 30). After calculating p-values using the Bootstrap HSIC and the Shift HSIC, we discover dependence
only at lags 4, 5, 9, 13 and 29, as presented in the Figure 4.
Lack of the dependence at lag zero suggests that the linear
model for coupling is reasonable. However, both the Bootstrap HSIC and the Shift HSIC support the hypothesis that
there is a strong relation at lag 5, which is not explained
well by the linear model.
The questions remains whether test statistics at lags 4, 9, 13
and 29 indicate further model misspecification. Under H0 ,
at a significance level 94%, we expect 1.8 out of 30 statis-

A Kernel Independence Test for Random Processes

0.9

Gamma estimator
Bootstrap p value
Shift p value

0.85
0.8

XAU/
USD

XAU/
USD

USD/
JPY

HKD/
JPY

USD/
JPY

HKD/
JPY

EUR/
RUB

AUD/
CHF

EUR/
RUB

AUD/
CHF

0.75
0.7
0.65
0.6

Shift HSIC

0.55
0.5

lag 4

lag 5

lag 9

lag 13

lag 29

Figure 4. Instantaneous coupling. Results for 720 samples, null
threshold of Shift HSIC used 300 lags in range 100 − 400.

tics to be higher than the 94th percentile. Excluding the
statistic at lag 5, the Shift HSIC test reports two statistics
above this percentile, while Bootstrap HSIC reports four.
Should the statistics at the different lags be independent
from each other, the probabilities of seeing two and four
statistics above the percentile are respectively 25% and 6%.
Shift HSIC indicates that the model fits the data well, while
the Bootstrap HSIC suggests that some non-linear dependencies remain unexplained.
Dependence structure. The data are five currency pairs.
A correlation based independence test, and the Shift HSIC
test, were performed on each pair of currencies. The dependencies revealed by these tests are depicted in Figure 5
- nodes represent the time series and edges represent dependence. Shift HSIC reveals a strong coupling between
EUR/RUB and USD/JPY, HKD/JPY and XAU/USD that
was not found by simple correlation. All edges revealed
by Shift HSIC have p-values at most at level 0.03 - clearly,
the Shift HSIC managed to find a strong non-linear dependence. Note that the obtained graphs are cliques.

5. Proofs
A U -statistic of a k-argument, symmetric function f , is
written
 −1
X
n
U (f, Z) =
f (Zi1 , ..., Zik ).
k
1≤i1 <···<ik ≤n

A decomposition due to Hoeffding allows us to decompose
this U-statistic intoPa sum of U -statistics of canonical funcl
tions, U (h, Z) = k=1 kl U (hk , Z), where hk (z1 , ..., zl )
are components of the decomposition. According to Serfling (2009, section 5.1.5), each of h1 ,h2 ,h3 ,h4 is symmetric and canonical. Note that hk is defined using indepen-

Correlation

Figure 5. Differences between the dependence structure on the
forex revealed by the Shift HSIC and covariance. Parameter settings are as in Figure 4.

dent samples Z ∗ - this is because the CLT or LLN state that
U-statistics or V-statistics of mixing processes converge
to their expected value taken with respect to independent
copies, i.e., Z ∗ . Under H0 , h1 is equal to zero everywhere
and h2 = 16 s, where these results were obtained by Gretton
et al. (2007).6 See supplementary material for details.
In order to characterize U (h, Z), we show that under null
hypothesis U (h2 , Z) converges to a random variable, and
both U (h3 , Z),U (h4 , Z) converge to zero in a probability
(the latter proof can be found in the supplementary material). Bellow we characterise U (h2 , Z) convergence.
Lemma 3. Under assumptions of Lemma 2,
∞

D

lim n · U (h2 , Z) =

n→∞

1X
λi1 (τi21 − 1).
6 i
1

Proof. First recall that under null hypothesis h2 = 61 s. We
will check the conditions of (Borisov & Volodko, 2008,
Theorem 1) (also available in the supplementary).
First, from Mercer’s Theorem (Steinwart & Scovel, 2012,
Corollary 3.5), we deduce that the h2 coefficients in
L2 (Z, BZ , PZ ) are absolutely summable. In the supplementary material we show that Eei (Z1∗ ) = 0.
Recall
of Lemma 2. If A holds then
P∞ the assumptions
1
2 < ∞ and sup E|e (X )|2 = 1 < ∞ (e
φ(k)
i
1
i
i
k=1
is an orthonormal eigenfunction). Finally, if B holds then
the process Zt is α-mixing. The remaining assumptions
concerning uniform mixing in Borisov & Volodko (2008)
are exactly the same as in this lemma.
5.1. Main body proofs
Proof. (Lemma 2) We use the fact that h2 is equal to s
up to scaling (6U (h2 , Z) = U (s, Z)), and Lemma 3, to
6
The second result is hard to locate - it is in appendix A.2, text
between equations 12 and 13

A Kernel Independence Test for Random Processes
D P∞
seePthat nU (s, Z) → Pi λi (τi2 − 1). Since Es(Zt , Zt ) =
∞
∞
E i=1 λi ei (Zt )2 = i=1 λi , then by the LLN for mixing
processes,
∞

n

n

X
1X
P
s(Zi , Zi ) =
λi .
n→∞ n
i
i=1
lim

(7)

We use a relationship between U and V statistics,
1
n→∞ n

D

lim nV (s, Z) = lim nU (s, Z) + lim

n→∞

n→∞

D

=

∞
X

λi +

∞
X

i=1

D

λi (τi2 − 1) =

∞
X

i

On the other hand, via the relation h2 = 16 s and the h2 definition, we get Es(Z1∗ , Z1∗ ) = 6Eh(Z1∗ , Z1∗ , Z2∗ , Z3∗ ), and
therefore

n
X

1
1X
P
Sn = lim
s(Zi , Zi ). (9)
n→∞ n(n − 1)(n − 2)
n→∞ n
i
lim

Finally, we rewrite Sn as
X

s(Zi , Zi )

i

λi τi2 .

h(Zi1 , ..., Zi4 ) = Sn +

1≤i1 ,i2 ,i3 ,i4 ≤n

We normalize by

i

X

h(Zi1 , ..., Zi4 ).

i∈C4
1
n(n−1)(n−2) ,

and take the limit in n,

n4
D
V (h, Z) =
n→∞ n(n − 1)(n − 2)


1
Sn + (n − 4)U (h, Z) .
= lim
n→∞ n(n − 1)(n − 2)
lim

Proof. (Theorem 1) We operate under the null hypothesis. Recall
 that U (h, Z) can be decomposed as U (h, Z) =
P4
4
k=1 k U (hk , Z). Here h1 ≡ 0. We show in the supplementary material that U (h3 , Z) and U (h4 , Z) tend to zero
in probability. From Lemma 3,
D

D

lim nU (h, Z) = lim nU (s, Z) =

n→∞

n→∞

∞
X

We substitute (9) and (8) on the right hand side,
and use equation
(7) fromP Lemma 2 to replace
Pn
∞
limn→∞ n1 i s(Zi , Zi ) with i=1 λi , yielding

λi (τi2 − 1). (8)

i

D

lim n · V (h, Z) =

n→∞

We define an auxiliary symmetric function w,
w(z1 , z2 , z3 ) = h(z1 , z1 , z2 , z3 ) + h(z1 , z2 , z2 , z3 )
+ h(z1 , z2 , z3 , z3 ) + h(z1 , z1 , z3 , z2 )
+ h(z3 , z2 , z2 , z1 ) + h(z2 , z1 , z3 , z3 ).

n

D

=

∞
X
i=1

It is obvious that Ew(Z1∗ , Z2∗ , Z3∗ ) = 6Eh(Z1∗ , Z1∗ , Z2∗ , Z3∗ ).
We consider the difference between the unnormalized V
and U statistics,
X
X
Sn =
h(Zi1 , ..., Zi4 ) −
h(Zi1 , ..., Zi4 ),
1≤i1 ,i2 ,i3 ,i4 ≤n

i∈C4


P
n
where i∈Cm denotes summation over all m
combinations of m distinct elements {i1 , · · · , im } from {1, · · · , n}.
The difference is equal to the sum over 4-tuples with at least
one
 pair of equal elements. We can choose such tuples in
4
2 = 6 ways. Observe that w covers the choice of all these
six tuples. Since for any z1 , z2 ∈ Z, h(z1 , z1 , z1 , z2 ) = 0,
then w is zero whenever more than two indices are equal.
Therefore we can sum w over distinct indices z1 , z2 , z3 ,
X
Sn =
w(Zi1 , Zi2 , Zi3 ).
i∈C3

We see that Sn is almost a U -statistic (U (w, Z)). By the
CLT for U -statistics from Denker & Keller (1983), Theorem 1(c), we obtain
1
P
Sn = 6Eh(Z1∗ , Z1∗ , Z2∗ , Z3∗ ).
lim
n→∞ n(n − 1)(n − 2)

n

1X
1X
D
s(Zi , Zi ) + lim
s(Zi , Zj ) =
= lim
n→∞ n
n→∞ n
i
i,j
D

λi +

∞
X
i

D

λi (τi2 − 1) =

∞
X

λi τi2 .

i

Proof. (Theorem 2) If the null hypothesis does not hold,
then γ > 0 (Gretton et al., 2005). In this case h is nondegenerate, and we √
can use Denker & Keller (1983, Theorem
1(c)) to see that 4 √nσ (V (h, Z) − γ) ∼ N (0, 1), where σ is
finite (see the note below Theorem 1 of (Denker & Keller,
1983), stating that in case (c) σ 2 is finite, and the note above
Theorem 1 stating that σ 2 = limn→∞ n−1 σn2 ).

Proof. (Lemma 1) We use Lemma 1 and Theorem 4 from
Gretton et al. (2005) to show that Eh(Z1∗ , Z2∗ , Z3∗ , Z4∗ ) = 0
D
iff (X1∗ , Y1∗ ) has a product distribution. Since Z1∗ = Z1
D
and Zt = Z1 , we infer that Xt is independent from Yt iff
Eh(Z1∗ , Z2∗ , Z3∗ , Z4∗ ) = 0.
Acknowledgements The authors thank the reviewers and
colleagues for helpful feedback, especially M. Skomra, D.
Toczydlowska, and A. Zaremba.

A Kernel Independence Test for Random Processes

References
Berlinet, A. and Thomas-Agnan, C. Reproducing Kernel Hilbert
Spaces in Probability and Statistics. Kluwer, 2004.
Besserve, M., Logothetis, N., and Schölkopf, B. Statistical analysis of coupled time series with kernel cross-spectral density
operators. In NIPS, pp. 2535–2543, 2013.
Borisov, I. and Volodko, N. Orthogonal series and limit theorems
for canonical U- and V-statistics of stationary connected observations. Siberian Advances in Mathematics, 18(4):242–257,
2008.
Bradley, R. et al. Basic properties of strong mixing conditions. a
survey and some open questions. Probability surveys, 2(10744):37, 2005.
Broock, W., Scheinkman, J., Dechert, D., and LeBaron, B. A test
for independence based on the correlation dimension. Econometric Reviews, 15(3):197–235, 1996.
Denker, M. and Keller, G. On U-statistics and v. Mises statistics
for weakly dependent processes. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete, 64(4):505–522, 1983.
Diks, C. and Panchenko, V. Nonparametric tests for serial independence based on quadratic forms. Technical report, Tinbergen Institute Discussion Paper, 2005.
Doukhan, P. Mixing. properties and examples. In Mixing, number 85 in Lect. Notes in Stat., pp. 87–109. Springer, January
1994.
Fadlallah, B., Brockmeier, A., Seth, S., Li, L., Keil, A., and
Principe, J. An association framework to analyze dependence
structure in time series. In Engineering in Medicine and Biology Society (EMBC), 2012 Annual International Conference of
the IEEE, pp. 6176–6179. IEEE, 2012.
Feuerverger, A. A consistent test for bivariate dependence. International Statistical Review/Revue Internationale de Statistique, pp. 419–433, 1993.
Fukumizu, K., Gretton, A., Sun, X., and Schölkopf, B. Kernel
measures of conditional dependence. In NIPS 20, pp. 489–496,
2008.
Granger, C., Maasoumi, E., and Racine, J. A dependence metric for possibly nonlinear processes. Journal of Time Series
Analysis, 25(5):649–669, 2004.

Hong, Y. Testing for independence between two covariance stationary time series. Biometrika, 83(3):615–625, 1996.
Hong, Y. and White, H. Asymptotic distribution theory for nonparametric entropy measures of serial dependence. Econometrica, 73(3):837–901, 2005.
Hyvärinen, A., Karhunen, J., and Oja, E. Independent component
analysis, volume 46. John Wiley & Sons, 2004.
Lyons, R. Distance covariance in metric spaces. The Annals of
Probability, 41(5):3051–3696, 2013.
Pearl, J. Causality: models, reasoning and inference, volume 29.
Cambridge Univ Press, 2000.
Pinkse, J. A consistent nonparametric test for serial independence.
Journal of Econometrics, 84(2):205–231, 1998.
Racine, J. and Maasoumi, E. A versatile and robust metric entropy test of time-reversibility, and other hypotheses. Journal
of Econometrics, 138(2):547–567, 2007.
Robinson, P. Consistent nonparametric entropy-based testing. The
Review of Economic Studies, 58(3):437–453, 1991.
Rosenblatt, M. and Wahlen, B. A nonparametric measure of independence under a hypothesis of independent components.
Statistics & probability letters, 15(3):245–252, 1992.
Serfling, R. Approximation theorems of mathematical statistics,
volume 162. John Wiley & Sons, 2009.
Shao, X. A generalized portmanteau test for independence between two stationary time series. Econometric Theory, 25(01):
195–210, 2009.
Smola, A. J, Gretton, A., Song, L., and Schölkopf, B. A Hilbert
space embedding for distributions. In Algorithmic Learning Theory, volume LNAI4754, pp. 13–31, Berlin/Heidelberg,
2007. Springer-Verlag.
Song, L, Smola, A., Gretton, A., Bedo, J., and Borgwardt, K.
Feature selection via dependence maximization. JMLR, 13:
1393–1434, 2012.
Sriperumbudur, B., Gretton, A., Fukumizu, K., Lanckriet, G., and
Schölkopf, B. Hilbert space embeddings and metrics on probability measures. JMLR, 11:1517–1561, 2010.

Gretton, A. and Györfi, L. Consistent nonparametric tests of independence. JMLR, 11:1391–1423, 2010.

Sriperumbudur, B., Fukumizu, K., and Lanckriet, G. Universality, characteristic kernels and RKHS embedding of measures.
JMLR, 12:2389–2410, 2011.

Gretton, A., Bousquet, O., Smola, A., and Schölkopf, B. Measuring statistical dependence with hilbert-schmidt norms. In
Algorithmic learning theory, pp. 63–77. Springer, 2005.

Steinwart, I. and Scovel, C. Mercers theorem on general domains:
on the interaction between measures, kernels, and rkhss. Constructive Approximation, 35(3):363–417, 2012.

Gretton, A., Fukumizu, K., Teo, C, Song, L., Schölkopf, B., and
Smola, A. A kernel statistical test of independence. In NIPS,
volume 20, pp. 585–592, 2007.

Székely, G. J, Rizzo, M. L, et al. Brownian distance covariance.
The annals of applied statistics, 3(4):1236–1265, 2009.

Gretton, A., Borgwardt, K., Rasch, M., Schölkopf, B., and Smola,
A. A kernel two-sample test. JMLR, 13:723–773, 2012.
Haugh, L. Checking the independence of two covariancestationary time series: a univariate residual cross-correlation
approach. Journal of the American Statistical Association, 71
(354):378–385, 1976.

Zhang, X., Song, L., Gretton, A., and Smola, A. Kernel measures
of independence for non-iid data. In NIPS, volume 22, 2008.

