The Kendall and Mallows Kernels for Permutations
Yunlong Jiao
YUNLONG . JIAO @ MINES - PARISTECH . FR
Jean-Philippe Vert
JEAN - PHILIPPE . VERT @ MINES - PARISTECH . FR
MINES ParisTech – CBIO, PSL Research University, Institut Curie, INSERM U900, Paris, France

Abstract
We show that the widely used Kendall tau correlation coefficient, and the related Mallows kernel, are positive definite kernels for permutations. They offer computationally attractive
alternatives to more complex kernels on the
symmetric group to learn from rankings, or to
learn to rank. We show how to extend the
Kendall kernel to partial rankings or rankings
with uncertainty, and demonstrate promising results on high-dimensional classification problems in biomedical applications.

1. Introduction
Kernel-based algorithms have been proved successful in
numerous applications and enjoy great popularity in the
machine learning community (Cortes & Vapnik, 1995;
Vapnik, 1998; Schölkopf & Smola, 2002; Shawe-Taylor &
Cristianini, 2004). The essential idea behind these methods is to define a positive definite kernel K : X × X → R
over an input space X , which can often be thought of as a
measure of similarity, and which implicitly defines an embedding Φ : X → F of the input space X to a Hilbert
space F in which the kernel becomes an inner product:
∀x, x0 ∈ X ,

K(x, x0 ) = hΦ(x), Φ(x0 )iF .

Kernel methods operate implicitly in the Hilbert space F,
which can be high-dimensional, by only manipulating the
kernel function between data. This kernel trick is particularly interesting when K(x, x0 ) is inexpensive to evaluate,
compared to Φ(x) and Φ(x0 ). In particular, kernel methods
have found many applications where the input data are discrete or structured, such as strings or graphs, thanks to the
development of numerous kernels for these data (Haussler,
1999; Kashima et al., 2003; Gärtner et al., 2004; ShaweTaylor & Cristianini, 2004; Schölkopf et al., 2004; Vishwanathan et al., 2009).
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

In this paper, we are interested in developing and studying positive definite kernels for a particular type of discrete
data, namely, permutations. A permutation is a 1-to-1 mapping from a finite set into itself. Permutations are ubiquitous in many applications involving rankings or partial
rankings, such as analyzing data describing the preferences
or votes of a population (Diaconis, 1988), learning or tracking correspondances between sets of objects (Huang et al.,
2009), or estimating a single ranking that best represents a
collection of individual rankings (Ailon et al., 2008). Another potentially rich source of ranking data comes from
real-valued vectors in which the relative order of the values
of multiple features is more important than their absolute
magnitude. For example, in the case of high-dimensional
gene expression data, Geman et al. (2004) showed that simple classifiers based on binary comparisons between the
expression of different genes in a sample show competitive prediction accuracy with much more complex classifiers built on the quantitative gene expression levels, a line
of thoughts that have been further investigated by Tan et al.
(2005); Xu et al. (2005); Lin et al. (2009). In these approaches, a n-dimensional vector is thus first transformed
into a permutation by sorting its entries, and a classifier is
trained on the resulting permutations.
Working with permutations is, however, computationally
challenging. There are n! permutations of n items, suggesting that various simplifications or approximations are
necessary in pursuit of efficient algorithms to analyze or
learn permutations. Such simplifications include for example, reducing ranks to a series of binary decisions (Ailon
et al., 2008; Balcan et al., 2008), or estimating a parametric distribution over permutations (Lebanon & Mao, 2008;
Helmbold & Warmuth, 2009; Huang et al., 2009).
In this context, it is surprising that relatively little attention has been paid to the problem of defining positive definite kernels between permutations, which could pave the
way to the use of computationally efficient kernel methods in problems involving permutations. A notable exception is the work of Kondor (2008); Kondor & Barbosa
(2010), who exploit the fact that the set of permutations
endowed with the composition operation forms a group,

The Kendall and Mallows Kernels for Permutations

called the symmetric group (Diaconis, 1988; Huang et al.,
2009), on which right-invariant positive definite kernels are
fully characterized by Bochner’s theorem (Kondor, 2008;
Fukumizu et al., 2008). They derive interesting kernels,
such as a diffusion kernel for rankings or partial rankings,
which however remains prohibitive to compute when the
number of ranked items is large.
In this paper we take one step further towards the development of computationally attractive kernels for permutations, by noticing that two widely-used and computationally efficient measures of similarity between permutations,
the Kendall tau correlation coefficient and the Mallows kernel, are positive definite. These kernels
compare two per
mutations of n items in terms of n2 pairwise comparisons,
but can be computed in O(n log n), paving the way to the
use of kernel methods for problems involving rankings or
permutations of a large number of items. In particular, the
feature space of the Kendall and Mallows kernels is precisely the space of binary pairwise comparisons defined by
Geman et al. (2004), and we show that instead of selecting
a few features in this space as the Top Scoring Pairs (TSP)family classifiers do (Geman et al., 2004; Tan et al., 2005;
Xu et al., 2005; Lin et al., 2009), one can simply work with
all pairs with the kernel trick. We further study how the
new kernels can be extended to partial rankings, and to
uncertain rankings which is particularly relevant when the
rankings are obtained by sorting a real-valued vector where
ties or near-ties occur. Finally, we demonstrate promising
results of the underlying kernels on a large benchmark of
high-dimensional biomedical data classification problems.

2. The Kendall and Mallows Kernel for Total
Rankings
Let us first fix some notations. Given a list of n items
{x1 , x2 , . . . , xn }, a total ranking is a strict ordering of the
n items of the form
xi1  xi2  · · ·  xin ,

(1)

where {i1 , . . . , in } are distinct indices in {1, 2, . . . , n} =:
[1, n]. Each total ranking can equivalently be represented
by a permutation σ : [1, n] → [1, n] such that σ(i) 6= σ(j)
for i 6= j, and σ(i) = j indicates that a ranker assigns rank
j to item i. For example, the ranking x
x1
2  x4  x3  
1 2 3 4
is associated to the permutation σ =
,
1 4 2 3
meaning σ(1) = 1, σ(2) = 4, etc.. There are n! different total rankings, and we denote by Sn the set of all permutations over n items. Endowed with the composition
operation σ1 σ2 (i) = σ1 (σ2 (i)), Sn is a group called the
symmetric group.
Given two permutations σ, σ 0 ∈ Sn , the number of concor-

dant and discordant pairs between σ and σ 0 are respectively
X
nc (σ, σ 0 ) =
1{σ(i)<σ(j)} 1{σ0 (i)<σ0 (j)}
i<j


+ 1{σ(i)>σ(j)} 1{σ0 (i)>σ0 (j)} ,
X
nd (σ, σ 0 ) =
1{σ(i)<σ(j)} 1{σ0 (i)>σ0 (j)}
i<j


+ 1{σ(i)>σ(j)} 1{σ0 (i)<σ0 (j)} .
As their names suggest, nc (σ, σ 0 ) and nd (σ, σ 0 ) count how
many pairs of items are respectively in the same or opposite
order in the two rankings σ and σ 0 . nd is frequently used
as a distance between permutations, often under the name
Kendall tau distance, and underlies two popular similarity
measures between permutations:
• The Mallows kernel defined for any λ ≥ 0 by
0

λ
KM
(σ, σ 0 ) = e−λnd (σ,σ ) ,

(2)

• The Kendall kernel defined as
nc (σ, σ 0 ) − nd (σ, σ 0 )

.
Kτ (σ, σ 0 ) =
n

(3)

2

The Mallows kernel plays a role on the symmetric group
similar to the Gaussian kernel on Euclidean space, for example for statistical modeling of permutations (Mallows,
1957; Critchlow, 1985; Fligner & Verducci, 1986; Meilă
et al., 2007) or nonparametric smoothing (Lebanon & Mao,
2008), and the Kendall kernel (Kendall, 1938; 1948) is
probably the most widely used measure of rank correlation
coefficient. In spite of their pervasiveness, to the best of our
knowledge the following property has been overlooked:
λ
Theorem 1. The Mallows kernel KM
, for any λ ≥ 0, and
the Kendall kernel Kτ are positive definite.
n
Proof. Consider the mapping Φ : Sn → R( 2 ) defined by


1
Φ(σ) = q  (1{σ(i)>σ(j)} − 1{σ(i)<σ(j)} )
.

n
2

1≤i<j≤n

Then one immediately sees that, for any σ, σ 0 ∈ Sn ,
Kτ (σ, σ 0 ) = Φ(σ)> Φ(σ 0 ) ,
showing that Kτ is positive definite, and that
kΦ(σ) − Φ(σ 0 )k2 = Kτ (σ, σ) + Kτ (σ 0 , σ 0 ) − 2Kτ (σ, σ 0 )
 n (σ, σ 0 ) − n (σ, σ 0 ) 
c
 d
=1+1−2
n
2

4
= n nd (σ, σ 0 ) ,
2

showing that nd is conditionally positive definite (Schoenλ
berg, 1938) and therefore that KM
is positive definite for
all λ ≥ 0.

The Kendall and Mallows Kernels for Permutations

Although the Kendall and Mallows kernels correspond

respectively to a linear and Gaussian kernel on a n2 dimensional embedding of Sn such that they can in particular be computed in O(n2 ) time by a naive implementation of pair-by-pair comparison, it is interesting to notice
that more efficient algorithms based on divide-and-conquer
strategy can significantly speed up the computation, up to
O(n log n) using a technique based on Merge Sort algorithm (Knight, 1966). Computing in O(n log n) a kernel
corresponding to a O(n2 )-dimensional embedding of Sn is
a typical example of the kernel trick, which allows to scale
kernel methods to larger values of n than what would be
possible for methods working with the explicit embedding.

In this section we show how the Kendall kernel Kτ can
efficiently be adapted to partial rankings, a situation frequently encountered in practice. For example, in a movie
recommendation system, each user only grades a subset of
movies that he has watched according to personal preference. As another example, in a chess tournament, each
match results in an relative ordering between two contestants, and one would like to find globally a single ranking
that best represents the large collection of binary outcomes.
As opposed to a total ranking (1), a partial ranking is in general of the form X1  X2  · · ·  Xk , where X1 , . . . , Xk
are k disjoint subsets of n items {x1 , . . . , xn }. For example, {x2 , x4 }  x6  {x3 , x8 } in a social survey could
represent the fact that items 2 and 4 are ranked higher by
an interviewee than item 6, which itself is ranked higher
than items 3 and 8. Note that it is uninformative of the
relative order of items 2 and 4, nor of how item 1 is rated.
To extend any kernel K over Sn to a kernel over the set
of partial rankings, we represent a partial ranking by the
set R ⊂ Sn of permutations which are compatible with all
partial orders described by the partial ranking, and adopt
the convolution kernel between two partial rankings R and
R0 as
X X
1
K(σ, σ 0 ).
0
|R||R |
0
0

xi1  xi2  · · ·  xik ,

k ≤ n,

where we have a total ranking for k out of n items. This
type of partial ranking is frequently encountered in real
life, for example if each person is able to vote for only a
few candidates in an election example, or in case there exist interleaved inaccessible values. The interleaving partial
ranking corresponds to the set of permutations compatible
with it:
Ai1 ,...,ik = {σ ∈ Sn |σ(ia ) > σ(ib ) if a < b, a, b ∈ [1, k]}.
(5)
b) A top-k partial ranking is of the form

3. Extension to Partial Rankings

K(R, R0 ) =

a) An interleaving partial ranking is of the form

(4)

σ∈R σ ∈R

As a convolution kernel, it is positive definite as long as
K is positive definite (Haussler, 1999). However, a naive
implementation to compute (4) typically requires O((n −
k)!(n−k 0 )!) operations when the number of observed items
in R, R0 is respectively k and k 0 , which can quickly become
prohibitive.
We now show that we can circumvent the computational
burden of naively implementing (4) with the Kendall kernel
on at least two particular cases of partial rankings:

xi1  xi2  · · ·  xik  Xrest ,

k ≤ n,

where we have a total ranking for k out of n items and also
know that these k items are ranked higher than all the other
items. For example, the top k hits returned by a search
engine leads to a top k partial ranking, or so does a survey
on top k favorite flavors of ice cream. The top-k partial
ranking corresponds to the set of compatible permutations
Bi1 ,...,ik = {σ ∈ Sn |σ(ia ) = n + 1 − a, a ∈ [1, k]}. (6)
Indeed, the following holds:
Theorem 2. The Kendall kernel between two interleaving
partial rankings of respectively k and m observed items, or
between a top-k partial ranking and a top-m partial ranking, of form (4) can be computed in O(k log k + m log m)
operations.
Proof. We prove this theorem by showing explicitly how
to compute the Kendall kernel between two partial rankings of type (5) or (6) in supplementary material. Checking
the correctness of the algorithms is tedious but easy once
we notice that most of the additive terms in the Kendall
kernel for partial rankings cancel each other out because
of the symmetry of the compatible set of full permutations. This also means that such a fast algorithm is not
likely to exist for the Mallows kernel over partial rankings taking form (4). Note that in both algorithms, the first
step is the computationally most expensive one, where we
need to identify the total ranking restricted to the observed
items in partial rankings. This can be achieved by any sorting algorithm, leading the algorithms to time complexity
O(k log k + m log m) overall.

4. The Kendall Kernel for Quantitative
Vectors
When data to analyze are n-dimensional real-valued quantitative vectors, converting them to permutations in Sn by

The Kendall and Mallows Kernels for Permutations

ranking their entries can be beneficial in cases where we
trust more the relative ordering of the values than their absolute magnitudes. For example, an interesting line of work
in the analysis of gene expression data promotes the development of classifiers built upon relative reversals of pairwise feature comparison, based on the observations that
gene expression measurements are subject to various measurement errors such as technological biases and normalization issues, while assessing whether a gene is more expressed than another gene is generally a more robust task
(Geman et al., 2004; Tan et al., 2005; Xu et al., 2005; Lin
et al., 2009). This suggests that the Kendall kernel can be
relevant for analyzing quantitative vectors as well. It now
takes the form of dot product as

Depending on the noise distribution, various kernels are obtained. In particular, assuming that  ∼ (U[− a2 , a2 ])n the
n-dimensional uniform noise of window size a centered at
0, the (i, j)-th entry of Ψ(x) for all i < j becomes

Kτ (x, x0 ) = Φ(x)> Φ(x0 ) ,

ga is odd, continuous, piecewise quadratic between [−a, a]
and constant elsewhere at ±1, and thus can be viewed as
smoothed version of the Heaviside step function to compare any two entries xi and xj from their difference xi −xj .

(7)

n
>
where Φ : Rn → R( 2 ) is defined for x = (x1 , . . . , xn ) ∈
n
R by


1
. (8)
Φ(x) = q  (1{xi >xj } − 1{xi <xj } )

n
2

1≤i<j≤n

In this case, the interpretation of the Kendall kernel in terms
of concordant and discordant pairs (3) is obviously still
valid, with the caveats that in the presence of ties between
entries of x, say two coordinates i and j such that xi = xj ,
the tied pair {xi , xj } will be neither concordant nor discordant. This implies in particular that if x has ties or so does
x0 , then |Kτ (x, x0 )| < 1 strictly. As for permutations, the
fast implementation of Kendall kernel also applies to quantitative vectors in O(n log n) time, even in the presence of
ties (Knight, 1966).
Feature mapping (8) is by construction very sensitive to the
presence of entry pairs that are “almost ties” but not “sheer
ties”. In fact, each entry of Φ(x) is, up to a normalization
constant, the Heaviside step function which takes discrete
values in {−1, 0, +1}, and thus can change abruptly even
when x changes slightly but reverses the order of two entries whose values are close. We propose to make the mapping more robust by assuming a random noise  added to x
and checking where Φ(x + ) is, on average (similarly to,
e.g., Muandet et al., 2012). In other words, we consider a
n
smoother mapping Ψ : Rn → R( 2 ) defined by
Ψ(x) = EΦ(x + ),

(9)

where  is a n-dimensional random vector, and the corresponding kernel
G (x, x0 ) = Ψ(x)> Ψ(x0 ) .

(10)

Denoting x̃ := x +  the randomly jittered vector, we deduce from (8) that Ψ is equivalently written as


1
Ψ(x) = q  (P (x̃i > x̃j ) − P (x̃i < x̃j ))
.
n
2

1≤i<j≤n

1
Ψij (x) = q  ga (xi − xj ) ,
n

(11)

2

where

1



2( at ) − ( at )2
ga (t) :=
t 2
t

 2( a ) + ( a )

−1

t≥a
0≤t≤a
.
−a ≤ t ≤ 0
t ≤ −a

(12)

Although the kernel (10) can be an interesting alternative to the Kendall kernel (7), we unfortunately lose for
G the computational trick that allows to compute Kτ in
O(n log n). Specifically, we have two ways to compute G:
• Exact evaluation. The first alternative is to compute
explicitly the n2 -vector representation Ψ(x) in the
feature space by (11) and (12), and then take the dot
product to obtain G. The computational cost is therefore linear with the dimension of the feature space, i.e.
O(n2 ).
• Monte Carlo approximation. The second alternative requires the observation that G(x, x0 ) =
EΦ(x̃)> EΦ(x̃0 ) = EKτ (x̃, x̃0 ), where x̃ and x̃0 are
independently noise-perturbed versions of x and x0 ,
and we can thus approximate G by a D2 -sample
mean:
GD (x, x0 ) =

D D

1 XX
Kτ x̃i , x̃0j ,
2
D i=1 j=1

(13)

where x̃1 , . . . , x̃D are i.i.d. noisy versions of x, and
the same for x0 . Since computing Kτ has complexity O(n log n), the computational cost of GD is
O(D2 n log n)
We note that the second alternative is faster to compute than
the first one as long as, up to constants, D2 < n/ log n,
and small values of D are thus favored. In this case, however, the approximation performance can be unappealing.
To better understand the trade-off between the two alternatives, there is therefore a pressing need to understand how
large D should be to ensure that the approximation error
is not detrimental to learning with the approximate kernel
GD instead of G.

The Kendall and Mallows Kernels for Permutations

For this purpose, let us consider for example the case where
the smoothed kernel G is used to train a Support Vector
Machine (SVM) from a training set D = {(xi , yi )}m
i=1 ⊂
(Rn × {−1, +1})m , specifically to estimate a function
h(x) = w> Ψ(x) by solving
min F (w) =
w

λ
b
kwk2 + R(w),
2

(14)

Pm
1
>
b
where R(w)
= m
i=1 `(yi w Ψ(xi )) is the empirical
>
loss, with `(yi w Ψ(xi )) = max(0, 1 − yi w> Ψ(xi )) the
hinge loss associated to the i-th point, λ the regularization parameter. Now suppose that instead of training the
SVM with smoothed feature mapping on the original points
{Ψ(xi )}i=1,...,m , we first randomly jitter {xi }i=1,...,m at
each point D times, resulting in {x̃ji }i=1,...,m;j=1,...,D , and
then replace each Ψ(xi ) by the D-sample empirical average of jittered points mapped by Φ into the feature space,
that is
D
1 X
Φ(x̃ji ) .
ΨD (xi ) :=
D j=1
Note that ΨD (xi )> ΨD (xj ) = GD (xi , xj ), hence training a SVM with the Monte Carlo approximate GD instead of exact version G is equivalent to solving (14)
with {ΨD (xi )}i=1,...,m in the hinge loss instead of
{Ψ(xi )}i=1,...,m . The following theorem quantifies the approximation performance in terms of objective function F .
b D of the
Theorem 3. For any 0 ≤ δ ≤ 1, the solution w
SVM trained with the Monte Carlo approximation (13) with
D random jittered samples for each training point satisfies,
with probability greater than 1 − δ,
r
b D ) ≤ min F (w) +
F (w
w

8
λD

r


2+

m
8 log
δ

5. Relationship to the Diffusion Kernel on Sn
It is interesting to relate the Mallows kernel (2) to the diffusion kernel on symmetric group proposed by Kondor &
Barbosa (2010), which is the diffusion kernel (Kondor &
Lafferty, 2002) on the Cayley graph of Sn generated by adjacent transpositions with left-multiplication. This graph,
illustrated for a specific case n = 4 in Figure 1, is defined by G = (V, E) with V = Sn as vertices, and
	 undirected edge set E = {σ, πσ} : σ ∈ Sn , π ∈ Q , where
Q = {(i, i + 1)|i = 1, . . . , n − 1} the set of all adjacent transpositions. Note Q is symmetric in the sense that
π ∈ Q ⇔ π −1 ∈ Q, and the graph adjacency relation is a
right-invariant relation, that is σ ∼ σ 0 ⇔ σ 0 σ −1 ∈ Q. The
corresponding graph Laplacian is the matrix ∆ with

if σ ∼ σ 0
 1
−(n − 1) if σ = σ 0 ,
∆σ,σ0 =

0
otherwise
where n − 1 is the degree of vertex σ (number of edges
connected with vertex σ), and the diffusion kernel on Sn is
finally defined as
β
Kdif
(σ, σ 0 ) = [eβ∆ ]σ,σ0

(15)
β∆

for some diffusion parameter β ∈ R, where e
is the
β
matrix exponential. Kdif
is a right-invariant kernel on
the symmetric group (Kondor & Barbosa, 2010, Proposition 2), and we denote by κβdif the positive definite funcβ
β
tion induced by Kdif
such that Kdif
(σ, σ 0 ) = κβdif (σ 0 σ −1 ).
λ
Since it is straightforward that the Mallows kernel KM
is
λ
also right-invariant, we denote by κM the positive defiλ
such that
nite function induced by the Mallows kernel KM
λ
(σ, σ 0 ) = κλM (σ 0 σ −1 ).
KM


.

The proof is left in supplementary material. It is known
that compared to the exact solution of (14), an O(m−1/2 )approximate solution is sufficient to reach the optimal statistical accuracy (Bottou & Bousquet, 2008). This accuracy can be attained in our analysis when D = O(m/λ),
and since typically λ ∼ m−1/2 (Steinwart, 2005), this suggests that it is sufficient to take D of order m3/2 . Going
back to the comparison strategy of the two alternatives G
and GD , we see that the computational cost of computing the full m × m Gram matrix with the exact evaluation is O(m2 n2 ), while the cost of computing the approximate Gram matrix with D = O(m3/2 ) random samples is
O(m2 D2 n log n) = O(m5 n log n). This shows that, up to
constants and logarithmic terms, the Monte Carlo approach
is interesting when m = o(n1/3 ), otherwise the exact evaluation using explicit computation in the feature space is
preferable.

Figure 1. Cayley graph of S4 , generated by the transpositions (1
2) in blue, (2 3) in green, and (3 4) in red.

Interestingly, the Mallows kernel has a similar interpretation. Indeed, it is well-known that the Kendall tau distance

The Kendall and Mallows Kernels for Permutations

nd (σ, σ 0 ) is the minimum number of adjacent swaps required to bring σ to σ 0 , i.e. nd (σ, σ 0 ) equals to the shortest
path distance on the Cayley graph, or simply written
nd (σ, σ 0 ) = dG (σ, σ 0 ).

(16)

Different from the diffusion kernel for which communication between permutations is a diffusion process over the
λ
graph, the Mallows kernel KM
= e−λnd = e−λdG considers exclusively the shortest path over the graph when
expressing the similarity between permutations.
A notable advantage of the Mallows kernel over the diffusion kernel is that the Mallows kernel enjoys faster evaluation. On one hand if data examples are total rankings,
β
i.e. σ, σ 0 ∈ Sn , evaluating Kdif
(σ, σ 0 ) would require exponentiating a n!-dimensional Laplacian matrix by naive
implementation, and can reduce to exponentiating matrices
of smaller sizes by careful analysis in the Fourier space,
which still remains problematic if working dimension n
is large (Kondor & Barbosa, 2010). However, evaluating
λ
(σ, σ 0 ) only takes O(n log n) time. On the other hand
KM
if data examples are partial ranking of size k  n, i.e.
R, R0 ⊂ Sn , and we take convolution kernel (4), the analysis of exploring the sparsity of the Fourier coefficients of
the group algebra of partial rankings R, R0 of size k reduces the evaluation of both the diffusion kernel and the
Mallows kernel to O((2k)2k+3 ) time, provided that the exponential kernel Fourier matrices [κ̂(µ)]≥[... ]n−k are precomputed before any kernel evaluations take place (Kondor
& Barbosa, 2010, Theorem 13). To avoid notation overflow, we simply point out that the complexity bound should
be further refined if we additionally consider the sparsity of
the Fourier coefficients κ̂M (µ) for the Mallows kernel. In
fact, since κM (σ) depends only on the destination of the
ordered item pairs {(i, j)}i<j sent by permutation σ, the
Fourier coefficient κ̂M (µ) is zero whenever µC(n−2, 1, 1)
with respect to dominance order indexed by integer partition (Huang et al., 2009), regardless of k, which renders a
huge interest in terms of computational issue.

6. Experimental Results
Datasets. We investigate the performance of classifying
high-dimensional biomedical data, motivated by previous
work demonstrating the relevance of replacing numerical
features by pairwise comparisons in this context (Geman
et al., 2004; Tan et al., 2005; Xu et al., 2005; Lin et al.,
2009). For that purpose, we collected 10 datasets related to
human cancer research publicly available online (Li et al.,
2003; Schroeder et al., 2011; Shi et al., 2011), as summarized in Table 1. The features are proteomic spectra relative intensities for the Ovarian Cancer dataset and gene
expression levels for all the others. The contrasting classes
are typically “Non-relapse v.s. Relapse” in terms of cancer prognosis, or “Normal v.s. Tumor” in terms of cancer

identification. The datasets have no missing values, except
the Breast Cancer 1 dataset for which we performed additional preprocessing to remove missing values as follows:
first we removed two samples (both labeled “relapse”) from
the training set that have around 10% and 45% of missing
gene values; next we discarded any gene whose value was
missing in at least one sample, amounting to a total of 3.5%
of all genes.
Methods. We compare the Kendall kernel to other standard kernels (linear, homogeneous 2nd-order polynomial
and Gaussian RBF with bandwidth set with “median
trick”), using SVM (with regularization parameter C) and
Kernel Fisher Discriminant (KFD, without tuning parameter) as classifiers. In addition, we include in the benchmark classifiers based on Top Scoring Pairs (TSP) (Geman
1
et al., 2004), namely (1-)TSP, k-TSP (Tan
 et al., 2005) and
n
APMV (all-pairs majority votes, i.e. 2 -TSP). Finally we
also test SVM with various kernels using as input only top
features selected by TSP (Shi et al., 2011).
In all experiments, each kernel is centered (on the training set) and scaled to unit norm in the feature space. For
KFD-based models, we add 10−3 on the diagonal of the
centered and scaled kernel matrix, as suggested by (Mika
et al., 1999). The Kendall kernel we use in practice is a soft
version to (7) in the sense that the extremes ±1 can still be
attained in the presence of ties, specifically we use
nc (x, x0 ) − nd (x, x0 )
,
Kτ (x, x0 ) = p
(n0 − n1 )(n0 − n2 )

where n0 = n2 and n1 , n2 are the number of tied pairs in
x, x0 respectively.
Except for three datasets that are split into training and test
sets, in which case we report the performance on the test
set, we perform a 5-fold cross-validation repeated 10 times
and report the mean performance over the 5 × 10 = 50
splits to evaluate the performance of the different methods. In addition, on each training set, an internal 5-fold
cross-validation is performed to tune parameters, namely
the C parameter of SVM-based models optimized over a
grid ranging from 10−2 to 103 in log scale, and the number k of TSP in case of feature selection (ranging from 1 to
5000 in log scale).
Results. Table 2 and Figure 2 (Left) summarize the performance of each model across the datasets. A SVM with
the Kendall kernel achieves the highest average prediction accuracy overall (79.39%), followed by a linear SVM
1
While the original k-TSP algorithm selects only top k disjoint
pairs with the constraint that k is less than 10, we do not restrict
ourselves to any of these two conditions since we consider k-TSP
in this study essentially a feature pair scoring algorithm.

The Kendall and Mallows Kernels for Permutations
Table 1. Information of biomedial datasets.
Dataset

No. of features

Breast Cancer 1
Breast Cancer 2
Breast Cancer 3
Colon Tumor
Lung Adenocarcinoma 1
Lung Cancer 2
Medulloblastoma
Ovarian Cancer
Prostate Cancer 1
Prostate Cancer 2

23624
22283
22283
2000
7129
12533
7129
15154
12600
12600

No. of samples (training/test)
C1
C2
44/7 (Non-relapse)
32/12 (Relapse)
142 (Non-relapse)
56 (Relapse)
71 (Poor Prognosis) 138 (Good Prognosis)
40 (Tumor)
22 (Normal)
24 (Poor Prognosis)
62 (Good Prognosis)
16/134 (ADCA)
16/15 (MPM)
39 (Failure)
21 (Survivor)
162 (Cancer)
91 (Normal)
50/9 (Normal)
52/25 (Tumor)
13 (Non-relapse)
8 (Relapse)

Reference
(van ’t Veer et al., 2002)
(Desmedt et al., 2007)
(Wang et al., 2005)
(Alon et al., 1999)
(Beer et al., 2002)
(Gordon et al., 2002)
(Pomeroy et al., 2002)
(Petricoin et al., 2002)
(Singh et al., 2002)
(Singh et al., 2002)

Table 2. Prediction accuracy (%) of different models across datasets.
SVMkdtALL
SVMlinearTOP
SVMlinearALL
SVMkdtTOP
SVMpolyALL
KFDkdtALL
kTSP
SVMpolyTOP
KFDlinearALL
KFDpolyALL
TSP
SVMrbfALL
KFDrbfALL
APMV

Average
79.39
77.16
76.09
75.5
74.54
74.33
74.03
73.99
71.81
71.39
69.71
69.31
66.39
61.91

BC1
78.95
84.21
78.95
52.63
68.42
63.16
57.89
63.16
63.16
63.16
68.42
63.16
63.16
84.21

BC2
71.31
69.29
71.67
70.61
71.62
59.41
58.22
69.44
60.43
60.48
49.58
71.41
60.48
65.98

BC3
67.34
67.11
64.27
65.81
63.66
67.22
64.47
66.26
67.52
67.38
57.8
65.87
66.03
33.96

trained on a subset of features selected from the top scoring
pairs (77.16%) and a standard linear SVM (76.09%). The
SVM with Kendall kernel outperforms all the other methods at a P-value of 0.07 according to a Wilcoxon rank test.
We note that even though models based on KFD generally
are less accurate than those based on SVM, the relative order of the different kernels is consistent between KFD and
SVM, adding evidence that the Kendall kernel provides an
interesting alternative to other kernels in this context. The
performance of TSP and k-TSP, based on majority vote
rules, are comparatively worse than those of SVM using
the same features, as already observed by Shi et al. (2011).
Figure 2 further shows how the performance of different
kernels depends on the choice of the C parameter or the
SVM (Middle), and on the number of features used (Right),
on some representative datasets. We observe that compared
to other kernels, a SVM with the Kendall kernel is relatively insensitive to hyper-parameter C especially when C
is large, which corresponds to a hard-margin SVM. This
may explain in part the success of SVM in this setting,
since the risk of choosing a bad C during training is reduced. Regarding the number of features used in case of
feature selection, we notice that it does not seem to be beneficial to perform feature selection in this problem, explain-

CT
85.78
84.19
86.73
85.46
78.43
85.46
87.23
79.14
77.26
75.1
85.61
81.18
83.71
64.49

LA1
70.98
63.92
70.23
67.7
70.53
59.08
61.7
65.98
57.24
58.52
58.96
70.84
58.73
33.6

LC2
97.99
97.32
97.99
97.99
98.66
99.33
97.99
99.33
97.99
97.99
95.97
93.96
97.32
89.93

MB
63.67
65.17
62.67
58.33
61.17
59.33
56
60
59.5
60.33
52.67
63.83
59.67
42.17

OC
99.48
99.41
99.64
99.92
99.28
98.73
99.92
99.21
100
100
99.8
98.85
98.46
85.19

PC1
100
85.29
73.53
97.06
79.41
97.06
100
88.24
73.53
73.53
76.47
26.47
26.47
73.53

PC2
58.4
55.7
55.17
59.47
54.23
54.57
56.83
49.1
61.43
57.43
51.83
57.5
49.87
46

ing why the Kendall kernel which uses all pairwise comparisons between features outperforms other kernels restricted
to a subset of these pairs.
Finally, as a proof of concept we empirically compare on
one dataset the smooth alternative (10) and its Monte Carlo
approximate (13) with the original Kendall kernel. Figure
3 shows how the performance varies with the amount of
noise added to the samples (Left), and how the performance
varies with the number of samples in the Monte Carlo
scheme for a given amount of noise (Right). It confirms that
the smooth alternative (10) can improve the performance of
the Kendall kernel, and that the amount of noise (window
size) should be considered as a parameter of the kernel to
be optimized. Although the D2 -sample Monte Carlo approximate kernel (13) mainly serves as a fast estimate to the
exact evaluation of (10), it shows that the idea of jittered input with specific noise can also bring a tempting benefit for
data analysis with Kendall kernel, even when D is small.
This also justifies the motivation of our proposed smooth
alternative (10). Last but not least, despite the fact that the
convergence rate of D2 -sample Monte Carlo approximate
to the exact kernel evaluation is guaranteed by Theorem 3,
experiments show that the convergence in practice is typically faster than the theoretical bound, and even faster in

The Kendall and Mallows Kernels for Permutations

1.0

BC1

0.9

0.8

●

0.7

0.8

●

0.5

0.6

0.6

acc

0.7

0.6
acc

acc

0.8

1.0

PC1

0.9

●
●

1e−02

1e+00

0.4

0.5

SVMlinearTOP
SVMkdtTOP
SVMpolyTOP
kTSP
SVMlinearALL
SVMkdtALL
SVMpolyALL
TSP
APMV

0.3

0.3

APMV

KFDrbfALL

TSP

SVMrbfALL

KFDpolyALL

KFDlinearALL

kTSP

SVMpolyTOP

KFDkdtALL

SVMkdtTOP

SVMpolyALL

SVMlinearALL

SVMkdtALL

●

SVMlinearTOP

SVMlinearALL
SVMkdtALL
SVMpolyALL
SVMrbfALL
KFDlinearALL
KFDkdtALL
KFDpolyALL
KFDrbfALL

0.4

0.4

1e+02

1

5

C parameter

10

50

500

5000

Number k of top gene pairs

Figure 2. Left: Model performance comparison (ordered by decreasing average accuracy across datasets). Middle: Sensitivity of kernel
SVMs to C parameter on the Breast Cancer 1 dataset. Right: Impact of TSP feature selection on the Prostate Cancer 1 dataset. (Special
marks are returned by cross-validation.)

0.66

●

cvacc

●

●

●
●
●

●

●

0.62

●

●

●

0.60

●

●

1e+02

0.70

●

●

1e+01

0.68
●

●

1e+03

1e+04

0.62

●

● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●

SVMkdtALLalt−−exact (a=10204)
SVMkdtALLalt−−MCapprox (a=10204)
SVMkdtALL

0.66

●

●

0.64

SVMkdtALLalt−−exact
SVMkdtALLalt−−MCapprox (D=1)
SVMkdtALLalt−−MCapprox (D=3)
SVMkdtALLalt−−MCapprox (D=5)
SVMkdtALLalt−−MCapprox (D=7)
SVMkdtALLalt−−MCapprox (D=9)
SVMkdtALL

●

0.64

cvacc

MB

1e+05

Noise window size a

0.60

●

0.68

0.70

MB

0

5

10

15

20

25

30

35

Number D of random jitter

Figure 3. Left: Empirical performance of smoothed alternative to Kendall kernel on the Medulloblastoma dataset. Right: Empirical
convergence of Monte Carlo approximate at the fixed window size attaining maximum underlying accuracy from the left plot.

case that the window size a is small. This is due to the fact
that the convergence rate is also dependent of the observed
data distribution in the input space, for which we have not
made any specific assumption in our analysis.

7. Conclusion
Based on the observation that the classical Kendall tau correlation between total rankings is a positive definite kernel,
we presented some extensions and applications pertaining
to learning with the Kendall kernel and the related Mallows kernel. We showed that both kernels can be evaluated
efficiently in O(n log n) time, and that the Kendall kernel
can be extended to partial rankings containing k items out
of n in O(k log k) time. When permutations are obtained

by sorting real-valued vectors, we proposed an extension
of the Kendall kernel based on random perturbations of
the input vector to increase its robustness to small variations, and discussed two possible algorithms to compute it.
We further highlighted a connection between the fast Mallow kernel and the diffusion kernel of Kondor & Barbosa
(2010). We also reported promising experimental results on
biomedical data demonstrating that for highly noisy data,
the Kendall kernel is competitive or even outperforms other
state-of-the-art kernels. We leave for future work further
applications of kernel methods to permutations with these
kernels, such as clustering of rankings with kernel k-means
as an alternative to existing techniques based on mixtures
of Mallows models.

The Kendall and Mallows Kernels for Permutations

Acknowledgments
This work was supported by the European Union 7th
Framework Program through the Marie Curie ITN MLPM
grant No 316861, and by the European Research Council
grant ERC-SMAC-280032.

References
Ailon, N., Charikar, M., and Newman, A. Aggregating inconsistent information: ranking and clustering. J. ACM,
55(5):23:1–23:27, 2008.
Alon, U., Barkai, N., Notterman, D. A., et al. Broad patterns of gene expression revealed by clustering analysis
of tumor and normal colon tissues probed by oligonucleotide arrays. Proc. Natl. Acad. Sci. U. S. A., 96(12):
6745–6750, 1999.
Balcan, M.-F., Bansal, N., Beygelzimer, A., et al. Robust
reductions from ranking to classification. Mach. Learn.,
72(1–2):139–153, 2008.
Beer, D. G., Kardia, S. L. R., Huang, C.-C., et al. Geneexpression profiles predict survival of patients with lung
adenocarcinoma. Nat. Med., 8(8):816–824, Aug 2002.
Bottou, L. and Bousquet, O. The tradeoffs of large scale
learning. In Platt, J.C., Koller, D., Singer, Y., and
Roweis, S. (eds.), Adv. Neural. Inform. Process Syst.,
volume 20, pp. 161–168. Curran Associates, Inc., 2008.
Cortes, C. and Vapnik, V. Support-vector networks. Mach.
Learn., 20(3):273–297, 1995.
Critchlow, Douglas E. Metric methods for analyzing partially ranked data, volume 34 of Lecture Notes in Statistics. Springer New York, 1985.
Desmedt, C., Piette, F., Loi, S., et al. Strong time dependence of the 76-gene prognostic signature for nodenegative breast cancer patients in the TRANSBIG multicenter independent validation series. Clin. Cancer Res.,
13(11):3207–3214, 2007.
Diaconis, P. Group representations in probability and
Statistics, volume 11 of Lecture Notes–Monograph Series. Institut of Mathematical Statistics, Hayward, CA,
1988.

Gärtner, T., Lloyd, J.W., and Flach, P.A. Kernels and distances for structured data. Mach. Learn., 57(3):205–232,
2004.
Geman, D., d’Avignon, C., Naiman, D. Q., and Winslow,
R. L. Classifying gene expression profiles from pairwise
mRNA comparisons. Stat. Appl. Genet. Mol. Biol., 3(1):
Article19, 2004.
Gordon, G. J., Jensen, R. V., Hsiao, L.-L., et al. Translation of microarray data into clinically relevant cancer
diagnostic tests using gene expression ratios in lung cancer and mesothelioma. Cancer Res., 62(17):4963–4967,
2002.
Haussler, D. Convolution kernels on discrete structures.
Technical Report UCSC-CRL-99-10, UC Santa Cruz,
1999.
Helmbold, D. P. and Warmuth, M. K. Learning permutations with exponential weights. J. Mach. Learn. Res., 10:
1705–1736, 2009.
Huang, J., Guestrin, C., and Guibas, L. Fourier theoretic probabilistic inference over permutations. J. Mach.
Learn. Res., 10:997–1070, 2009.
Kashima, H., Tsuda, K., and Inokuchi, A. Marginalized
Kernels between Labeled Graphs. In Faucett, T. and
Mishra, N. (eds.), Proceedings of the Twentieth International Conference on Machine Learning, pp. 321–328,
New York, NY, USA, 2003. AAAI Press.
Kendall, M. G. A new measure of rank correlation.
Biometrika, 30(1/2):81–93, 1938.
Kendall, M. G. Rank correlation methods. Griffin, 1948.
Knight, W. R.
A computer method for calculating
Kendall’s tau with ungrouped data. J. Am. Stat. Assoc.,
61(314):436–439, 1966.
Kondor, I. R. Group theoretical methods in machine learning. PhD thesis, Columbia University, 2008.

Fligner, M. A. and Verducci, J. S. Distance based ranking
models. J. R. Stat. Soc. Ser. B, 48(3):359–369, 1986.

Kondor, I. R. and Lafferty, J. Diffusion kernels on graphs
and other discrete input spaces. In Proceedings of the
Nineteenth International Conference on Machine Learning, volume 2, pp. 315–322, San Francisco, CA, USA,
2002. Morgan Kaufmann Publishers Inc.

Fukumizu, K., Gretton, A., Schölkopf, B., and Sriperumbudur, B. K. Characteristic kernels on groups and semigroups. In Koller, D., Schuurmans, D., Bengio, Y., and
Bottou, L. (eds.), Adv. Neural. Inform. Process Syst., volume 21, pp. 473–480. 2008.

Kondor, R. I. and Barbosa, M. S. Ranking with kernels
in fourier space. In Kalai, A. T. and Mohri, M. (eds.),
COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pp. 451–463. Omnipress, 2010.

The Kendall and Mallows Kernels for Permutations

Lebanon, G. and Mao, Y. Non-parametric modeling of partially ranked data. J. Mach. Learn. Res., 9:2401–2429,
2008.
Li, J., Liu, H., and Wong, L. Mean-entropy discretized
features are effective for classifying high-dimensional
biomedical data. In Zaki, M. J., Wang, J. T.-L.,
and Toivonen, H. (eds.), Proceedings of the 3nd ACM
SIGKDD Workshop on Data Mining in Bioinformatics
(BIOKDD 2003), August 27th, 2003, Washington, DC,
USA, pp. 17–24, 2003.
Lin, X., Afsari, B., Marchionni, L., et al. The ordering of
expression among a few genes can provide simple cancer
biomarkers and signal BRCA1 mutations. BMC bioinformatics, 10:256, 2009.
Mallows, C. L. Non-null ranking models. i. Biometrika, 44
(1/2):114–130, 1957.
Meilă, M., Phadnis, K., Patterson, A., and Bilmes, J. Consensus ranking under the exponential model. In Proceedings of the Twenty-Third Conference Annual Conference
on Uncertainty in Artificial Intelligence (UAI-07), pp.
285–294, Corvallis, Oregon, 2007. AUAI Press.
Mika, S., Rätsch, G., Weston, J., Schölkopf, B., and Müller,
K.R. Fisher discriminant analysis with kernels. In Hu,
Y.-H., Larsen, J., Wilson, E., and Douglas, S. (eds.),
Neural Networks for Signal Processing IX, pp. 41–48.
IEEE, 1999.
Muandet, K., Fukumizu, K., Dinuzzo, F., and Schölkopf,
B. Learning from distributions via support measure machines. In Pereira, F., Burges, C. J. C., Bottou, L., and
Weinberger, K. Q. (eds.), Adv. Neural. Inform. Process
Syst., volume 25, pp. 10–18. Curran Associates, Inc.,
2012.

Schroeder, M., Haibe-Kains, B., Culhane, A., et al. breastCancerTRANSBIG: Gene expression dataset published
by Desmedt et al. [2007] (TRANSBIG)., 2011. URL
http://compbio.dfci.harvard.edu/.
R
package version 1.2.0.
Shawe-Taylor, J. and Cristianini, N. Kernel Methods for
Pattern Analysis. Cambridge University Press, New
York, NY, USA, 2004.
Shi, P., Ray, S., Zhu, Q., and Kon, M. A. Top scoring
pairs for feature selection in machine learning and applications to cancer outcome prediction. BMC Bioinformatics, 12:375, 2011.
Singh, D., Febbo, P. G., Ross, K., et al. Gene expression
correlates of clinical prostate cancer behavior. Cancer
cell, 1(2):203–209, 2002.
Steinwart, I. Consistency of support vector machines and
other regularized kernel classifiers. IEEE Trans. Inform.
Theory, 51(1):128–142, 2005.
Tan, A. C., Naiman, D. Q., Xu, L., Winslow, R. L., and
Geman, D. Simple decision rules for classifying human
cancers from gene expression profiles. Bioinformatics,
21(20):3896–3904, 2005.
van ’t Veer, L. J., Dai, H., van de Vijver, M. J., et al. Gene
expression profiling predicts clinical outcome of breast
cancer. Nature, 415(6871):530–536, 2002.
Vapnik, V. N. Statistical Learning Theory. Wiley, NewYork, 1998.
Vishwanathan, S. V. N., Schraudolph, N. N., Kondor, R.,
and Borgwardt, K. M. Graph kernels. J. Mach. Learn.
Res., 10:1–41, 2009.

Petricoin, E. F., Ardekani, A. M., Hitt, B. A., et al. Use of
proteomic patterns in serum to identify ovarian cancer.
Lancet, 359(9306):572–577, 2002.

Wang, Y., Makedon, F. S., Ford, J. C., and Pearlman, J.
HykGene: a hybrid approach for selecting marker genes
for phenotype classification using microarray gene expression data. Bioinformatics, 21(8):1530–1537, 2005.

Pomeroy, S. L., Tamayo, P., Gaasenbeek, M., et al. Prediction of central nervous system embryonal tumour outcome based on gene expression. Nature, 415(6870):436–
442, 2002.

Xu, L., Tan, A. C., Naiman, D. Q., Geman, D., and
Winslow, R. L. Robust prostate cancer marker genes
emerge from direct integration of inter-study microarray
data. Bioinformatics, 21(20):3905–3911, 2005.

Schoenberg, I. J. Metric spaces and positive definite functions. Trans. Am. Math. Soc., 44(3):522–536, 1938.
Schölkopf, B. and Smola, A. J. Learning with Kernels:
Support Vector Machines, Regularization, Optimization,
and Beyond. MIT Press, Cambridge, MA, 2002.
Schölkopf, B., Tsuda, K., and Vert, J.-P. Kernel Methods
in Computational Biology. MIT Press, The MIT Press,
Cambridge, Massachussetts, 2004.

