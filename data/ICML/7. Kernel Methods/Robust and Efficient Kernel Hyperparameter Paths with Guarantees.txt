Robust and Efficient Kernel Hyperparameter Paths with Guarantees

Joachim Giesen
Sören Laue
Patrick Wieschollek
Friedrich-Schiller-Universität Jena, Germany

Abstract
Algorithmically, many machine learning tasks
boil down to solving parameterized optimization
problems. The choice of the parameter values
in these problems can have a significant influence on the statistical performance of the corresponding methods. Thus, algorithmic support
for choosing good parameter values has received
quite some attention recently, especially algorithms for computing the whole solution path of
a parameterized optimization problem. These algorithms can be used, for instance, to track the
solution of a regularized learning problem along
the regularization parameter path, or for tracking
the solution of kernelized problems along a kernel hyperparameter path. Since exact path following algorithms can be numerically unstable,
robust and efficient approximate path tracking algorithms have gained in popularity for regularized learning problems. By now algorithms with
optimal path complexity in terms of a guaranteed
approximation error are known for many regularized learning problems. That is not the case for
kernel hyperparameter path tracking algorithms,
where the exact path tracking algorithms can also
suffer from numerical problems. Here we address this problem by devising a robust and efficient path tracking algorithm that can also handle
kernel hyperparameter paths. The algorithm has
asymptotically optimal complexity. We use this
algorithm to compute approximate kernel hyperparamter solution paths for support vector machines and robust kernel regression. Experimental results for these problems applied to various
data sets confirm the theoretical complexity analysis.

JOACHIM . GIESEN @ UNI - JENA . DE
SOEREN . LAUE @ UNI - JENA . DE
PATRICK @ WIESCHOLLEK . INFO

1. Introduction
Parameterized optimization problems of the form
min ft (x)

x∈Ft

are abundant in machine learning. Here t ∈ R is a parameter, ft : Rd → R is some function depending on t, and
Ft ⊆ Rd is the feasible region of the optimization problem
at parameter value t.
The solution path problem is to compute an optimal or approximate solution xt ∈ Ft of the parameterized problem
along some parameter interval I ⊆ R. From the solution
path a good parameter value t and a corresponding solution xt can be chosen by some optimization criterion that
should not be confused with the objective of the parameterized optimization problem. In a machine learning context
the parameter t is typically optimized using some measure
for the generalization error on test data while xt is computed from training data.
An important example of the abstract parameterized optimization problem is
ft (x) = r(x) + t · l(x),
where l : Rd → R is a loss function and r : Rd → R
is some regularizer, e.g. Euclidean regularization r(x) =
kxk22 that enables the so-called kernel trick, or r(x) = kxk1
that encourages sparse solutions. This case, namely efficiently computing robust regularization paths, has received
considerable attention and can be considered solved for the
relevant problems in machine learning even when optimizing over positive-semidefinite matrices. Another important
example that has received less attention is when ft is given
as a function f : Rd → R that is parameterized by a positive kernel function
kt : Ω × Ω → R
that itself is parameterized by t ∈ R on some set Ω.

st

Proceedings of the 31 International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

Here we study a fairly general class of parameterized convex optimization problems that contains most of the regularization path and kernel hyperparameter path problems.

Robust and Efficient Kernel Hyperparameter Paths with Guarantees

min

x∈Rd

s.t.

ft (x)

(1)

ct (x) ≤ 0,

where ft : Rd → R is convex and ct : Rd → Rn is convex
in every component
cit : Rd → R, i = 1, . . . , m
for all values of t. We assume that ft (x) and ct (x) are
Lipschitz continuous in t at any feasible point x, but we do
not require convexity (or concavity) of these functions in t.
The feasible region at t is given as

	
Ft = x ∈ Rd | ct (x) ≤ 0 ,
with componentwise inequalities. Our goal in this paper
is to devise a robust and efficient algorithm for computing an ε-approximate solution path for Problem (1), i.e.,
in contrast to the exact solution path problem we only aim
for an ε-approximate solution along the parameter interval
instead of an exact solution. Turning to approximate solutions leads to much more efficient and robust algorithms
than the known exact solution paths algorithms.
Related work and contributions. Regularized optimization methods are in widespread use throughout machine
learning. Thus, computing regularization paths has received considerable attention over the last years. The
work on regularization paths started with the seminal work
by (Efron et al., 2004) who observed that the regularization
path of the LASSO is piecewise linear. In (Rosset & Zhu,
2007) a fairly general theory of piecewise linear regularization paths has been developed and exact path following
algorithm have been devised. Important special cases are
support vector machines whose regularization paths have
been studied in (Zhu et al., 2003; Hastie et al., 2004), support vector regression (Wang et al., 2006b), where also the
loss-sensitivity parameter can be tracked, and the generalized LASSO (Tibshirani & Taylor, 2011). From the beginning it was known, see for example (Allgower & Georg,
1993; Hastie et al., 2004; Bach et al., 2004), that exact regularization path following algorithms suffer from numerical instabilities as they repeatedly need to invert a matrix
whose condition number can be poor, especially when using kernels. It also turned out (Gärtner et al., 2012; Mairal
& Yu, 2012) that the combinatorial (and thus also computational) complexity of exact regularization paths can
be exponential in the number of data points. This triggered the interest in approximate path algorithms (Rosset,
2004; Friedman et al., 2007). By now numerically robust,
approximate regularization path following algorithms are
known for many problems including support vector machines (Giesen et al., 2012b;c), the LASSO (Mairal & Yu,

2012), and regularized matrix factorization and completion
problems (Giesen et al., 2012a;c). These algorithms com√ 
pute a piecewise constant approximation with O 1/ ε
segments, where ε > 0 is the guaranteed approximation
error. Notably, the complexity is independent of the number of data points and even matching lower bounds are
known (Giesen et al., 2012c).
The situation is still different for kernel hyperparameter path tracking. Exact kernel path tracking algorithms
are known for kernelized support vector machines (Wang
et al., 2007b), the kernelized LASSO (Wang et al.,
2007a), and Laplacian-regularized semi-supervised classification (Wang et al., 2006a; 2012). The exact kernel
path tracking algorithms are even more prone to numerical problems than regularization path tracking algorithms
since they repeatedly need to invert a kernel matrix whose
condition number tends to be poor (large), see Figure 1.

1012

average condition number

1010
condition number

We consider problems of the form

108
106
104
102
100

2−11 2−9 2−7 2−5 2−3 2−1 21 23 25
kernel hyperparameter t

27

29

211

Figure 1. The
 condition number of the
 Gaussian kernel matrix
kt (xi , xj ) = exp(−tkxi − xj k22 ) of 100 data points drawn
uniformly at random from [0, 1]10 and for various values of t.

Here we address this problem by devising a numerically
stable approximate solution path algorithm for parameterized problems of the Form (1). The algorithm can be used
to compute approximate regularization paths as well as approximate kernel hyperparameter paths. We prove that the
resulting path complexity is in O(1/ε), where ε > 0 is
again the guaranteed approximation error. This complexity
might look disappointing considering
√  that ε-approximation
paths with complexity in O 1/ ε are known for many
regularization path problems. Still, this is best possible.
A matching lower bound of Ω(1/ε) has been first proved
in (Giesen et al., 2010) for the class of Problems (1). This
problem class includes problems, for instance kernel hyperparameter path problems, whose exact solution path is not
piecewise linear as it is the case for the regularization path

Robust and Efficient Kernel Hyperparameter Paths with Guarantees

problems that exhibit a better approximation path complexity. We observed the Θ(1/ε) complexity bound also in experiments on various data sets for support vector machines
and robust kernel regression that have been kernelized with
a Gaussian kernel.

for all x ∈ Rd , and finally
max
min `t (x0 , α) ≤ min max
`t (x0 , α0 ),
0
0
α ≥0 x0 ∈Rd

x0 ∈Rd α ≥0

which implies
ϕ̂t (α) ≤ max
ϕ̂t (α0 )
0
α ≥0

2. Duality and approximate solution paths
Since our approximate solution path algorithm is based on
duality we review here some basic facts of duality theory
for parameterized optimization problems. We also introduce our notation and define approximate solution paths
for parameterized optimization problems and bound their
complexity.
Lagrangian duality. The Lagrangian of the parameterized convex optimization problem (1) is the following function
`t : Rd × Rn≥0 → R, (x, α) 7→ ft (x) + αT ct (x).
From the Lagrangian we can derive a dual optimization
problem as
maxn

α∈R

s.t.

min `t (x, α)

x∈Rd

α≥0

We call
ϕ̂t : Rn → R, α 7→ min `t (x, α).
x∈Rd

the dual objective function. From the Lagrangian we can
also derive an alternative expression for the primal objective function, namely

= max
min `t (x0 , α0 )
0
α ≥0 x0 ∈Rd

`t (x0 , α0 )
≤ min max
0
x0 ∈Rd α ≥0

= min ϕt (x0 )
x0 ∈Rd

≤ ϕt (x).
In particular, we have ϕ̂t (αt∗ ) ≤ ϕ(x∗t ), where
αt∗ = argmaxα≥0 ϕ̂t (α) and x∗t = argminx∈Ft ϕt (x)
are the dual and primal optimal solutions, respectively. We
say that strong duality holds if ϕ̂t (αt∗ ) = ϕt (x∗t ).
Duality gap and approximate solution.
value t we call
gt (x, α) = ϕt (x) − ϕ̂t (α)
the duality gap at (x, α) ∈ Ft × Rn≥0 . For ε > 0, we
call x ∈ Ft an ε-approximate solution of the parameterized
optimization problem (1) at parameter value t, if
ft (x) − ft (x∗t ) ≤ ε.
Assume that gt (x, α) ≤ ε, then we have
ft (x) − ft (x∗t ) = ϕt (x) − ϕt (x∗t )
= ϕt (x) − ϕ̂t (α) + ϕ̂t (α) − ϕt (x∗t )

= gt (x, α) − ϕt (x∗t ) − ϕ̂t (α)

ϕt : Rd → R, x 7→ max `t (x, α)

≤ gt (x, α) ≤ ε

α≥0

Note that
ft (x) = ϕt (x) for all x ∈ Ft
since αT ct (x) ≤ 0 and thus maxα≥0 αT ct (x) = 0 (which
can always be achieved by setting α = 0) for all x ∈ Ft .
Weak and strong duality. At a fixed parameter value t
we have the following well known weak duality property
ϕ̂t (α) ≤ ϕt (x)
for any x ∈ Rd and any α ∈ R≥0 . To see this note that
min `t (x0 , α) ≤ `t (x, α)

x0 ∈Rd

for all x ∈ Rd and all α ∈ Rn≥0 . Thus,
max
min `t (x0 , α0 ) ≤ max
`t (x, α0 )
0
0
α ≥0 x0 ∈Rd

α ≥0

At parameter

Approximate solution path. Let [tmin , tmax ] ⊂ R be a
compact parameter interval and ε > 0. We call a function
x : [tmin , tmax ] → Rd , t 7→ xt
an ε-approximate solution path of the parameterized optimization problem (1), if for all t ∈ [tmin , tmax ]
1. xt ∈ Ft and
2. ft (xt ) − ft (x∗t ) ≤ ε.
We say that the path x : [tmin , tmax ] → Rd has complexity
k ∈ N, if x can be computed from k primal-dual optimal
pairs (x∗ti , αt∗i ) with ti ∈ [tmin , tmax ), i = 1, . . . , k.
We can bound the complexity of approximate solution
paths as follows.

Robust and Efficient Kernel Hyperparameter Paths with Guarantees

Theorem 1. Given a parameterized convex optimization
problem (1), a parameter interval [tmin , tmax ], and ε > 0.
If the following conditions
1. the feasible region Ft has a nonempty interior, and
2. the problem has an optimal solution

x∗t

∈ Ft , and

3. ϕ̂t (α) is Lipschitz continuous in t for any α ≥ 0, and
4. there exists a function

such that x̃t (τ ) is feasible at parameter value τ , and
kx̃t (τ ) − x∗t k2 ≤ L|τ − t|
for some constant L > 0,
are satisfied for all t ∈ [tmin , tmax ], then there exists an
ε-approximate solution path for the interval [tmin , tmax ]
whose complexity is in O(1/ε). The constants in the big-O
notation depend only on the functions ft and ct and on the
interval [tmin , tmax ].
Proof. Let
max

t∈[tmin ,tmax ]

gτ (x̃t (τ ), αt∗ )
= fτ (x̃t (τ )) − ϕ̂τ (αt∗ )
≤ fτ (x∗t ) + L0 kx̃t (τ ) − x∗t k − ϕ̂τ (αt∗ )
≤ fτ (x∗t ) + L · L0 |τ − t| − ϕ̂τ (αt∗ )
≤ ft (x∗t ) − ϕ̂t (αt∗ ) + 2M |t − τ | + L · L0 |t − τ |
= gt (x∗t , αt∗ ) + (2M + L · L0 )|t − τ |
= (2M + L · L0 )|t − τ |,

x̃t : [tmin , tmax ] → Rd

r =

Combining these properties we obtain a bound for the following duality gap

kx∗t k2 .

Note that r < ∞ since x∗t exists for all t in the compact interval [tmin , tmax ]. Thus we can impose the additional constraint kxk2 ≤ r in the parameterized convex optimization
problem (1) without changing its solutions. Since the function ft is convex on Rd it is also Lipschitz continuous with
respect to it argument x for some constant L0 > 0 on the
set {x ∈ Ft | kxk2 ≤ r}.
By our assumptions both ft (x) and ϕ̂t (α) are Lipschitz
continuous with respect to t for any feasible x and α ≥ 0,
respectively, i.e., there exists a constant M > 0 such that
|ft (x) − fτ (x)| ≤ M |t − τ |
and
|ϕ̂t (α) − ϕ̂τ (α)| ≤ M |t − τ |
for all t, τ ∈ [tmin , tmax ].
Note that x̃t (τ ) is a feasible solution for the primal problem at τ and αt∗ is a feasible solution for the dual problem at τ , because the feasible region of the dual problem
does not depend on the parameter t. By Slater’s Condition,
see for example (Boyd & Vandenberghe, 2004), strong duality holds since Ft has a nonempty interior, and ft and
the components of ct are convex functions, i.e., we have
gt (x∗t , αt∗ ) = 0 for the duality gap.

where the first inequality follows from the Lipschitz continuity of fτ with respect to x, the second inequality follows from the Lipschitz continuity of the function x̃t with
respect to τ , and the third inequality follows from the Lipschitz continuity of ft (x) and ϕ̂t (α) with respect to t for
any feasible x and α ≥ 0, respectively. Hence, a primaldual solution pair (x̃t (τ ), αt∗ ) is a feasible primal-dual εapproximate solution pair for all τ with
|t − τ | ≤

ε
.
2M + L · L0

It follows that there exists an ε-approximate solution path
whose complexity can be bounded by
2M + L · L0
(tmax − tmin ) ∈ O(1/ε).
ε

The problem dependent constant in Theorem 1,
(2M + L · L0 )(tmax − tmin ),
might look huge at a first glance, but note that for an interval with tmin = 2−10 and tmax = 210 it turns out that
the value of this constant is at most 20 on the data sets that
we have tried in our experiments for kernelized SVMs, see
Section 6.
Lower bound The parameterized optimization problems
in the lower bound construction in (Giesen et al., 2010) satisfy the conditions of Theorem 1. This gives us a lower
bound in Ω(1/ε) on the path complexity for the class of
Problems (1) and shows that the complexity analysis in
Theorem 1 is asymptotically tight.

3. Approximate path tracking algorithm
In the following we assume that strong duality holds for
all parameter values in the interval [tmin , tmax ] ⊂ R. The
simple idea for computing an ε-approximate solution path
makes use of duality and works as follows:
1. Compute the primal-dual pair (x∗t , αt∗ ) for t = tmin .

Robust and Efficient Kernel Hyperparameter Paths with Guarantees

2. Determine x̃t : [tmin , tmax ] → Rd and t0 ∈
[tmin , tmax ] such that
gτ (x̃t (τ ), αt∗ ) ≤ ε.
for all τ ∈ [t, t0 ].
3. At t0 compute a new optimal primal-dual pair
(x∗t0 , αt∗0 ) and iterate Steps 2 and 3 until the whole interval [tmin , tmax ] has been covered.
Let tmin = t1 , . . . , tk be the points in [tmin , tmax ) at which
an optimal primal-dual pair is computed (Step 3 of the algorithm). The path
x : [tmin , tmax ] → Rd , t 7→

k
X

1[t1 ,ti+1 ) (t) x̃ti (t),

i=1

where tk+1 = tmax and
1[t1 ,ti+1 ) (t) =



1, if t ∈ [t1 , ti+1 )
0, if t ∈
/ [t1 , ti+1 )

is an ε-approximate solution path of complexity k.

4. Application: Kernelized SVM
Here we specialize Theorem 1 and the approximate path algorithm to the standard hinge loss support vector machine
(SVM) that has been kernelized with a Gaussian kernel matrix


Kt = kt (x, x0 ) = exp(−tkx − x0 k22 )
with bandwidth parameter t > 0. That is, we need to make
sure that this SVM meets the necessary conditions of Theorem 1. The primal SVM problem is given as
min

w∈Rd ,b∈R,ξ∈Rd

s.t.

1 T
w Kt w + c · kξk1
2
y  (Kt w + b) ≥ 1 − ξ
ξ ≥ 0,

where c is a regularization parameter, y ∈ Rd is a label
vector with entries in {−1, +1}, and  is the element-wise
multiplication.
The dual SVM problem can be written as

1
max − αT Kt  yy T α + kαk1
α∈∆c
2


= ϕ̂(α) ,

where
∆c =


	
α ∈ Rd | 0 ≤ α ≤ c, αT y = 0 .

It is straightforward to see that Assumptions 1.-3. of Theorem 1 are satisfied for the SVM problem. It remains to
ensure that also Assumption 4. holds true.

To ensure Assumption 4. we need to find a function as required in this assumption. Here we discuss two functions
that satisfy the requirements. In the first function the bias b
is fixed and in the second function it depends on the bandwidth parameter t0 . We call the first case the fixed bias
update rule and the second the dynamic bias update rule.
1. Fixed bias updates. For an optimal primal solution
(wt∗ , b∗t , ξt∗ ) at parameter value t we need to derive
a solution (w̃t , b̃t , ξ˜t )(t0 ) that is feasible at parameter
value t0 . Setting


w̃t , b̃t , ξ˜t (t0 )

= wt∗ , b∗t , max{1 − y  (Kt0 wt∗ + b∗t ), 0}
satisfies Assumption 4, i.e., the values of this function
are by construction feasible for the primal SVM at any
admissible value for t0 and it is Lipschitz continuous
in t0 since the dependence of Kt0 on t0 is differentiable. Note that b̃t (t0 ) = b∗t , which justifies the name
fixed bias update rule.
2. Dynamic bias updates. We can also adapt the bias b
for each t0 instead of only adapting ξ. This can be
done by setting b̃t (t0 ) to the median (for robustness
reasons) of the expressions (y−Kt0 wt∗ )i , i ∈ I, where
the index set
I = {i | 0 < eTi αt∗ < c},
with ei the i-th standard basis vector of Rd , is the set
of all support vectors that are exactly on the margin
for the optimal dual SVM solution αt∗ at parameter
value t. As for the fixed bias update rule, the entries
of ξ˜t (t0 ) are chosen such that all inequality constraints
of the primal SVM problem are satisfied, i.e.,


w̃t , b̃t , ξ˜t (t0 )

= wt∗ , b̃t (t0 ), max{1 − y  (Kt0 wt∗ + b̃t (t0 )), 0} .
Obviously, the dynamic bias update rule improves the
primal objective function value over the fixed bias update rule and hence Theorem 1 applies here as well.
Asymptotically the complexity of the SVM kernel path is
in O(1/ε) in both cases since Theorem 1 applies. In practice, however, it makes a difference which of the two update
rules is used, see Section 6. Although the asymptotic behavior is the same, the constants are much smaller for the
dynamic bias update rule than for the fixed bias update rule.

5. Application: Robust Kernel Regression
Robust regression is an alternative to least squares regression that uses an `1 -loss function instead of an `2 -loss function to become more robust against outliers. Robust kernel

Robust and Efficient Kernel Hyperparameter Paths with Guarantees
Table 1. Number of updates (complexity) of ε-approximate hyperparamter kernel path for some data sets and various values for ε for the
fixed bias update rule (on the left) and the dynamic bias update rule (on the right).
F IXED B IAS U PDATES
DATA SET
A1A
A2A
A3A
A4A
DIABETES
HEART
IONOSPHERE

SIZE

ε= 4

2

1

0.5

0.25

0.125

4

2

1

0.5

0.25

0.125

1605
2265
3185
4781
768
270
351

4
5
6
8
11
1
10

8
9
11
14
18
2
18

15
16
19
24
28
6
31

26
29
33
40
43
10
49

47
51
59
71
64
16
81

89
95
108
126
95
25
132

2
3
4
6
3
1
2

5
6
7
11
5
2
3

9
10
13
18
8
3
7

17
19
22
30
11
5
12

32
35
40
52
19
8
20

61
65
74
79
29
11
33

regression is an extension of robust regression that accommodates the use of kernels for nonlinear regression. Here
we even consider sparse robust kernel regression by adding
an additional `1 -regularizer that favors sparse solutions.
The sparse robust kernel regression problem is given as the
following minimization problem
min

β∈Rd

ky − Kt βk1 + λ · kβk1 ,

where y ∈ Rd is the output vector, and Kt ∈ Rd×d is
the kernel matrix that is determined by the parameterized
Gaussian kernel function kt and d data points x1 , . . . , xd .
The regression function is then given as
f (x) =

d
X

βi kt (xi , x).

i=1

The dual problem of the sparse robust kernel regression
problem is the following maximization problem
max

u∈Rd

s.t.

− yT u
kuk∞ ≤ 1
kKtT uk∞ ≤ λ.

To apply Theorem 1 we interchange the role of the primal
and the dual problem, i.e., we consider
min

u∈Rd

s.t.

yT u
kuk∞ ≤ 1
kKtT uk∞ ≤ λ

as the primal problem, whose dual is given as
max

β∈Rd

DYNAMIC B IAS U PDATES

− ky − Kt βk1 − λ · kβk1 .

Obviously, all four conditions of Theorem 1 are met, since
the primal problem has a nonempty interior, the problem

is bounded (and hence a primal optimum exists), the dual
function is Lipschitz continuous with respect to t, and for
any optimal dual solution u∗t we can find a feasible solution
ũt (τ ) by projecting u∗t onto the feasible region at parameter value τ . Since Kt is differentiable in t (for a Gaussian
kernel matrix) the projection itself is Lipschitz continuous.
Hence, we can apply Theorem 1 that guarantees the existence of an ε-approximate hyperparameter solution path of
complexity O(1/ε).

6. Experiments
To validate our theoretical finding, in particular the dependence of the path complexity on the guaranteed approximation error ε, we have conducted experiments for the kernelized SVM and also for the robust kernel regression.
6.1. Kernelized SVM
We have implemented the approximate path tracking algorithm for the kernelized SVM. LIBSVM Version 3.17,
whose implementation is described in (Fan et al., 2005),
has been used to compute primal-dual optimal pairs. LIBSVM actually solves the dual problem. If αt∗ is the optimal
dual solution at parameter value t, then the optimal primal
solution can be reconstructed by setting wt∗ = y  αt∗ and
b∗t to the median of the expressions
(y − Kt wt∗ )i , i ∈ {j | 0 < eTj αt∗ < c}.
It remains to describe the implementation of the second
step of the algorithm. Since we can compute the value of
the primal objective function for every value of t0 , see Section 4, we can also compute the duality gap



gτ w̃t , b̃t , ξ˜t (τ ), αt∗ .
The largest t0 > t for which the duality gap gt0 is still at
most ε can be simply found by binary search.
As test environment we used MATLAB, and all data sets
that have been used in our experiments were retrieved from

Robust and Efficient Kernel Hyperparameter Paths with Guarantees

26
26
24

20

exact path
+ε
−ε
upper bound
lower bound

18
16

20

1

2−10

2−8

2−6

2−4

2−2
20
22
kernel parameter t

24

26

28

0

210

objective value

objective value

2−10

2−8

2−6

2−4

2−2
20
22
kernel parameter t

24

26

28

210

2−4

2−2
20
22
kernel parameter t

24

26

28

210

80

75
exact path
+ε
−ε
upper bound
lower bound

70

75
exact path
+ε
−ε
upper bound
lower bound

70

65

65

1

1

gap
ε=1

0.5
2−10

2−8

gap

gap

gap
ε=1

0.5

80

0

exact path
+ε
−ε
upper bound
lower bound

18

gap
ε=1

0.5
0

22

16

1
gap

objective value

22

gap

objective value

24

2−6

2−4

2−2
20
22
kernel parameter t

24

26

28

gap
ε=1

0.5
0

210

2−10

2−8

2−6

Figure 2. The kernel hyperparameter path for the IONOSPHERE data set with fixed bias updates (top, left) and dynamic bias updates (top,
right), and the kernel hyperparameter path for the A 1 A data set with fixed bias updates (bottom, left) and dynamic bias updates (bottom,
right).

Dependence on ε Our theoretical finding that O(1/ε)
optimal primal-dual pairs are sufficient to approximate the
whole kernel hyperparameter solution path was confirmed
in our experiments. Figure 3 indicates that the number of
optimal primal-dual pairs computed by the algorithm depends linearly on 1/ε.

a1a
a2a
a3a
a4a
ionosphere
diabetes
heart

80
number of optimization calls

the LIBSVM Website, see (Lin). The data sets and results
are summarized in Table 1. The regularization parameter c
was set to 0.1 in all the experiments.

60

40

20

0
0

Choice of bias update rule The experiments also show
that the choice of the bias update rule has a significant influence on the approximation path complexity. As expected
the dynamic bias update rule leads to a lower path complexity than the fixed bias update rule, since it improves
the value of the primal objective function over the fixed
bias update rule and thus needs fewer updates to maintain

1

2

3

4

5

6

7

8

ε−1

Figure 3. Number of optimal primal-dual pairs computed by the
path tracking algorithm (path complexity) for various data sets at
several values for 1/ε.

Robust and Efficient Kernel Hyperparameter Paths with Guarantees

1.5

test MAE
0.3

true signal
LASSO
robust regression
data

0.5

0.25
MAE

y

1

0.2

0
0.15

−0.5
−4

−3

−2

−1

0
x

1

2

3

4

2−11 2−9

2−7

2−5

2−3

2−1

21

23

25

27

29

211

t

Figure 4. Synthetic data set for regression sampled with noise and outliers (on the left), and the mean absolute error (MAE) for the test
data set (on the right).

the approximation guarantee. Figure 2 directly compares
the two update rules and shows that indeed the dynamic
bias update rule performs better. The main difference is
that the upper bound, i.e., the approximation of the primal
optimum, is much better for the dynamic bias update rule.
Figure 2 also shows that our path tracking algorithm, in
contrast to a simple grid search, adapts well to regions of
interest (especially for the dynamic bias update rule), i.e.,
the solution is only updated frequently in these regions.

It is well known that the choice of the bandwidth parameter
in the Gaussian kernel has a significant influence on the performance of kernel regression methods. This can be seen
also in Figure 4 (on the right), where we show the mean
absolute error (MAE) on a set of test data points tracked
along the kernel hyperparameter path (i.e., the bandwidth
path). Note that the test error path in Figure 4 (on the right)
has many local minima which is typical for this type of
problems.

6.2. Robust Kernel Regression

7. Conclusions

We have also implemented the approximate path tracking
algorithm for robust kernel regression. The optimal primaldual pairs at a fixed parameter value t have been computed
using the SeDuMi solver (Sturm, 1999). The implementation of the second step of the algorithm is analogous to the
implementation for the kernelized SVM since also here we
can compute the value of the primal objective function for

every value of t0 and thus the duality gap gτ ũt (τ ), βt∗ .
The largest t0 > t for which the duality gap gt0 is still at
most ε can be found by binary search.

We have presented an algorithmic framework for tracking
approximate solutions for a large class of parameterized
optimization problems. In particular, the framework allows
to track kernel hyperparameter paths and even has the optimal path complexity of O(1/ε) in terms of the prescribed
approximation error ε for this type of problems, for which
no efficient approximation schemes had been devised before. The framework also allows to compute approximate
regularization paths, but it is not optimal for this easier
class of problems (whose exact solution path is piecewise
linear which is not true for hyperparameter paths).

As test environment we used again MATLAB and following the example of (Wang et al., 2007a) we generated a
synthetic data set by randomly sampling 100 points from
the following target function
f (x) =

sin(πx)
πx

in the interval [−4, 4] and by adding Gaussian noise. Additionally, we also added 10% outliers to the data set. The
data set, i.e., the sample points, and the target function are
shown in Figure 4 (on the left). In this figure we also show
that, as expected, robust regression performs better in the
presence of outliers than for instance the LASSO (Tibshirani, 1994). The regularization parameter λ was set to 0.1
in the experiments.

We have instantiated the algorithmic framework for computing approximate kernel hyperparameter paths for SVMs
and the robust kernel regression problem, both with Gaussian kernel. Our experiments for these applications, in contrast to exact path algorithms, did not suffer from numerical
problems, and confirmed our optimal theoretical complexity bounds.

Acknowledgments
This work has been supported by a grant of the Deutsche
Forschungsgemeinschaft (GI-711/3-2).

Robust and Efficient Kernel Hyperparameter Paths with Guarantees

References
Allgower, Eugene and Georg, Kurt. Continuation and path
following. Acta Numerica, 2:1–64, 1993.
Bach, Francis R., Thibaux, Romain, and Jordan, Michael I.
Computing regularization paths for learning multiple
kernels. In Advances in Neural Information Processing
Systems (NIPS), 2004.

Mairal, Julien and Yu, Bin. Complexity analysis of the
lasso regularization path. In International Conference
on Machine Learning (ICML), 2012.
Rosset, Saharon. Following curved regularized optimization solution paths. In Advances in Neural Information
Processing Systems (NIPS), 2004.

Boyd, Stephen and Vandenberghe, Lieven. Convex Optimization. Cambridge University Press, 2004.

Rosset, Saharon and Zhu, Ji. Piecewise linear regularized
solution paths. The Annals of Statistics, 35(3):1012–
1030, 2007.

Efron, Bradley, Hastie, Trevor, Johnstone, Iain, and Tibshirani, Robert. Least angle regression. The Annals of
Statistics, 32(2):407–499, 2004.

Sturm, Jos F. Using SeDuMi 1.02, a MATLAB toolbox for
optimization over symmetric cones. Optimization Methods and Software, 11-12:625–653, 1999.

Fan, Rong-En, Chen, Pai-Hsuen, and Lin, Chih-Jen. Working Set Selection Using Second Order Information for
Training Support Vector Machines. Journal of Machine
Learning Research, 6:1889–1918, 2005.

Tibshirani, Robert. Regression Shrinkage and Selection
Via the Lasso. Journal of the Royal Statistical Society,
Series B, 58:267–288, 1994.

Friedman, Jerome, Hastie, Trevor, Höfling, Holger, and
Tibshirani, Robert. Pathwise Coordinate Optimization.
The Annals of Applied Statistics, 1(2):302–332, 2007.

Tibshirani, Ryan and Taylor, Jonathan. The solution path
of the generalized lasso. The Annals of Statistics, 39(3):
1335–1371, 2011.

Gärtner, Bernd, Jaggi, Martin, and Maria, Clément. An
Exponential Lower Bound on the Complexity of Regularization Paths. Journal of Computational Geometry
(JoCG), 3(1):168–195, 2012.

Wang, Gang, Chen, Tao, Yeung, Dit-Yan, and Lochovsky,
Frederick H. Solution path for semi-supervised classification with manifold regularization. In IEEE International Conference on Data Mining (ICDM), pp. 1124–
1129, 2006a.

Giesen, Joachim, Jaggi, Martin, and Laue, Sören. Approximating Parameterized Convex Optimization Problems.
In European Symposium on Algorithms (ESA), pp. 524–
535, 2010.

Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H.
Two-dimensional solution path for support vector regression. In International Conference on Machine Learning
(ICML), pp. 993–1000, 2006b.

Giesen, Joachim, Jaggi, Martin, and Laue, Sören. Regularization Paths with Guarantees for Convex Semidefinite Optimization. In International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 432–439,
2012a.

Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H.
The Kernel Path in Kernelized LASSO. In International
Conference on Artificial Intelligence and Statistics (AISTATS), pp. 580–587, 2007a.

Giesen, Joachim, Jaggi, Martin, and Laue, Sören. Approximating parameterized convex optimization problems.
ACM Transactions on Algorithms, 9(1):10, 2012b.

Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H.
A kernel path algorithm for support vector machines. In
International Conference on Machine Learning (ICML),
pp. 951–958, 2007b.

Giesen, Joachim, Müller, Jens K., Laue, Sören, and
Swiercy, Sascha. Approximating Concavely Parameterized Optimization Problems. In Advances in Neural Information Processing Systems (NIPS), pp. 2114–2122,
2012c.
Hastie, Trevor, Rosset, Saharon, Tibshirani, Robert, and
Zhu, Ji. The Entire Regularization Path for the Support Vector Machine. In Advances in Neural Information
Processing Systems (NIPS), 2004.
Lin, Chih-Jen.
LIBSVM Tools.
Data sets available
at
www.csie.ntu.edu.tw/˜cjlin/
libsvmtools/datasets/.

Wang, Gang, Wang, Fei, Chen, Tao, Yeung, Dit-Yan, and
Lochovsky, Frederick H. Solution Path for Manifold
Regularized Semisupervised Classification. IEEE Transactions on Systems, Man, and Cybernetics, Part B, 42(2):
308–319, 2012.
Zhu, Ji, Rosset, Saharon, Hastie, Trevor, and Tibshirani,
Robert. 1-norm Support Vector Machines. In Advances
in Neural Information Processing Systems (NIPS), 2003.

