Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian
Processes
Yves-Laurent Kom Samo
Stephen Roberts
Deparment of Engineering Science and Oxford-Man Institute, University of Oxford

Abstract
In this paper we propose an efficient, scalable
non-parametric Gaussian process model for inference on Poisson point processes. Our model
does not resort to gridding the domain or to introducing latent thinning points. Unlike competing
models that scale as O(n3 ) over n data points,
our model has a complexity O(nk 2 ) where k 
n. We propose a MCMC sampler and show that
the model obtained is faster, more accurate and
generates less correlated samples than competing
approaches on both synthetic and real-life data.
Finally, we show that our model easily handles
data sizes not considered thus far by alternate approaches.

1. Introduction
Point processes are a standard model when the objects of
study are the number and repartition of otherwise identical
points on a domain, usually time or space. The Poisson
point process is probably the most commonly used point
process. It is fully characterised by an intensity function
that is inferred from the data. Gaussian processes have been
successfully used to form a prior over the (log-) intensity
function for applications such as astronomy (Gregory &
Loredo, 1992), forestry (Heikkinen & Arjas, 1999), finance
(Basu & Dassios, 2002), and neuroscience (Cunningham
et al., 2008b). We offer extensions to existing work as follows: we develop an exact non-parametric Bayesian model
that enables inference on Poisson processes. Our method
scales linearly with the number of data points and does not
resort to gridding the domain. We derive a MCMC sampler for core components of the model and show that our
approach offers a faster and more accurate solution, as well
as producing less correlated samples, compared to other approaches on both real-life and synthetic data.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

YLKS @ ROBOTS . OX . AC . UK
SJROB @ ROBOTS . OX . AC . UK

2. Related Work
Non-parametric inference on point processes has been extensively studied in the literature. Rathbum & Cressie
(1994) and Moeller et al. (1998) used a finite-dimensional
piecewise constant log-Gaussian for the intensity function.
Such approximations are limited in that the choice of the
grid on which to represent the intensity function is arbitrary
and one has to trade-off precision with computational complexity and numerical accuracy, with the complexity being
cubic in the precision and exponential in the dimension of
the input space. Kottas (2006) and Kottas & Sanso (2007)
used a Dirichlet process mixture of Beta distributions as
prior for the normalised intensity function of a Poisson
process. Cunningham et al. (2008a) proposed a model
using Gaussian processes evaluated on a fixed grid for
the estimation of intensity functions of renewal processes
with log-concave renewal distributions. They turned hyperparameters inference into an iterative series of convex optimization problems, where ordinarily cubic complexity operations such as Cholesky decompositions are evaluated
in O(n log n) leveraging the uniformity of the grid and
the log-concavity of the renewal distribution. Adams et
al. (2009) proposed an exact Markov Chain Monte Carlo
(MCMC) inference scheme for the posterior intensity function of a Poisson process with a sigmoid Gaussian prior
intensity, or equivalently a Cox process (Cox, 1955) with
sigmoid Gaussian stochastic intensity. The authors simplified the likelihood of a Cox process by introducing latent
thinning points. The proposed scheme has a complexity exponential in the dimension of the input space, cubic in the
number of data and thinning points, and performs particularly poorly when the data are sparse. Gunter et al. (2014)
extended this model to structured point processes. Rao &
Teh (2011) used uniformization to produce exact samples
from a non-stationary renewal process whose hazard function is modulated by a Gaussian process, and consequently
proposed a MCMC sampler to sample from the posterior
intensity of a unidimensional point process. Although the
authors have illustrated that their model is faster than that
of Adams et al. (2009) on some synthetic and real-life data,

Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes

their method still scales cubically in the number of thinned
and data points, and is not applicable to data in dimension
higher than 1, such as spatial point processes.

fact, for every finite-dimensional prior over the vector in
Equation (2), there exists a Cox process with an a.s. C ∞
intensity process that coincides with the postulated prior
(see appendix for the proof).

3. Model

This approach is similar
R to that of (Kottas, 2006). The author regarded I = S λ(s)ds as a random variable and
can be regarded as a pdf
noted that p(s) = R λ(s)
λ(s)ds
S
whose support is the domain S. He then made inference on (I, p(s1 ), ..., p(sn )), postulating as prior that I and
(p(s1 ), ..., p(sn )) are independent, I has a Jeffrey’s prior
and (s1 , ..., sn ) are i.i.d. draws from a Dirichlet process
mixture of Beta with pdf p.

3.1. Setup
We are tasked with making non-parametric Bayesian inference on the intensity function of a Poisson point process
assumed to have generated a dataset D = {s1 , ..., sn }. To
simplify the discourse without loss of generality, we will
assume that data points take values in Rd .
Firstly, let us recall that a Poisson point process (PPP) on
a bounded domain S ⊂ Rd with non-negative intensity
function λ is a locally finite random collection of points
in S such that the numbers of points occurring in disjoint
parts Bi of S are independent
and each follows a Poisson
R
distribution with mean Bi λ(s)ds.

The model we present in the following section
puts an appropriate finite-dimensional
prior on
R
(λ(s1 ), ..., λ(sn ), λ(s01 ), ..., λ(s0k ), S λ(s)ds) for some
inducing points s0j rather than putting a functional prior on
the intensity function directly.

The likelihood of a PPP is given by:
 Z
Y
n
λ(si ).
L(λ|s1 , ..., sn ) = exp − λ(s)ds

3.3. Our model

S

3.3.1. I NTUITION
(1)

i=1

3.2. Tractability discussion
The approach adopted thus far in the literature to make
non-parametric Bayesian inference on point process using Gaussian processes (GPs) (Rasmussen & Williams,
2006) consists of putting a functional prior on the intensity function in the form of a positive function of a GP:
λ(s) = f (g(s)) where g is drawn from a GP and f is a positive function. Examples of such f include the exponential
function and a scaled sigmoid function (Adams et al., 2009;
Rao & Teh, 2011). This approach can be seen as a Cox
process where the stochastic intensity follows the same dynamics as the functional prior. When the Gaussian process
used has almost surely continuous paths, the random vector
Z
(λ(s1 ), ..., λ(sn ), λ(s)ds)
(2)
S

provably admits a probability density function (pdf). Moreover, we note that any piece of information not contained in
the implied pdf over the vector in Equation (2) will be lost
as the likelihood only depends on those variables. Hence,
given a functional prior postulated on the intensity function, the only necessary piece of information to be able to
make a full Bayesian treatment is the implied joint pdf over
the vector in Equation (2).
For many useful transformations f and covariance structures for the GP, the aforementioned implied pdf might not
be available analytically. We note however that there is no
need to put a functional prior on the intensity function. In

The intuition behind our model is that the data are not
a ‘natural grid’ at which to infer the value of the intensity function. For instance, if the data consists of 200,000
points on the interval [0, 24] as in one of our experiments, it might not be necessary to infer the value of a
function at 200,000 points to characterise it on [0, 24].
Instead, we find a small set of inducing points D0 =
{s01 , ..., s0k }, k  n on our domain, through which we will
define the prior over the vector in Equation (2) augmented
with λ(s01 ), ..., λ(s0k ). The set of inducing points will be
chosen so that knowing λ(s01 ), ..., λ(s0k ) would result in
knowing the values of the intensity function elsewhere on
the domain, in particular λ(s1 ), ..., λ(sn ), with ‘arbitrary
certainty’. We will then analytically integrate out the dependency in λ(s1 ), ..., λ(sn ) from the posterior, thereby reducing the complexity from cubic to linear in the number of
data points without ‘loss of information’, and reformulating
our problem as that of making exact Bayesian inference on
the value of the intensity function at the inducing points.
We will then describe how to obtain predictive mean and
variance of the intensity function elsewhere on the domain
from training.
3.3.2. M ODEL SPECIFICATION
Let us denote by λ∗ a positive stochastic process on S
such that log λ∗ is a stationary Gaussian process with covariance kernel γ ∗ : (s1 , s2 ) → γ ∗ (s1 , s2 ) and constant
mean m∗ . Let us further denote by λ̂ a positive stochastic process on S such that log λ̂ is a conditional Gaussian process coinciding with log λ∗ at k inducing points
D0 = {s01 , ..., s0k }, k  n. That is, log λ̂ is the non-

Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes

stationary Gaussian process whose mean function m is defined by

It follows from the fifth assertion in our prior specification
µ2
σ2
that αI = σI2 and βI = µII . We also note that
I

∗

m(s) = m +

Σ∗sD0 Σ∗−1
D0 D0 G

(3)


where G = log λ∗ (s01 ) − m∗ , ..., log λ∗ (s0k ) − m∗ and
Σ∗XY is the covariance matrix between the vectors X and
Y under the covariance kernel γ ∗ . Moreover, log λ̂ is such
that for every vector S1 of points in S, the auto-covariance
matrix ΣS1 S1 of the values of process at S1 reads1
∗T
ΣS1 S1 = Σ∗S1 S1 − Σ∗S1 D0 Σ∗−1
D 0 D 0 ΣS 1 D 0 .

(4)

The prior distribution in our model is constructed as follows:
1. {log λ(s0i )}ki=1 are samples from the stationary GP
#D
,
log λ∗ at {s0i }ki=1 respectively, with m∗ = log µ(S)
where µ(S) is the size of the domain.
R
2. I = S λ(s)ds and {log λ(sj )}nj=1 are conditionally
independent given {log λ(s0i )}ki=1 .
3. Conditional on {log λ(s0i )}ki=1 , {log λ(sj )}nj=1 are independent, and for each j ∈ [1..n] log λ(sj ) follows
the same distribution as log λ̂(sj ).
{log λ(s0i )}ki=1 ,

4. Conditional on
I follows a Gamma
distribution with shape αI and scale βI .
5. The mean µI = αI βI and variance σI2 = αI βI2 of I
R
are that of S λ̂(s)ds.
Assertion 3. above is somewhat similar to the FITC model
of (Quinonero & Rasmussen, 2005).
This construction yields a prior pdf of the form:

p log λ(s1 ), ..., log λ(sn ), log λ(s01 ), ..., log λ(s0k ), I, θ

= N log λ(s01 ), ..., log λ(s0k )|m∗ 1k , Σ∗D0 D0

× N log λ(s1 ), ..., log λ(sn )|M, diag(ΣDD )

× γd I|αI , βI × p(θ)
(5)

Z


µI = E
λ̂(s)ds
Z S

=
E exp(log λ̂(s)) ds
ZS
1
=
exp(m(s) + γ(s, s))ds
2
S

Z
:=

f (s)ds

(6)

S

and
 Z

2
σI2 = E
λ̂(s)ds
− µ2I
 Z SZ

=E
exp(log λ̂(s1 ) + log λ̂(s2 ))ds1 ds2 − µ2I
S S

Z Z 

=
E exp log λ̂(s1 ) + log λ̂(s2 ) ds1 ds2 − µ2I
ZS ZS
1
=
exp m(s1 ) + m(s2 ) + γ(s1 , s2 ) + γ(s1 , s1 )
2
S S

1
+ γ(s2 , s2 ) ds1 ds2 − µ2I
2Z Z
g(s1 , s2 )ds1 ds2 − µ2I .

:=
S

(7)

S

The integrals in Equations (6) and (7) can be easily evaluated with numerical methods such as Gauss-Legendre
quadrature (Hildebrand, 2003).
In particular, when S = [a, b],
p

b−aX
µI ≈
ωi f
2 i=1



b−a
b+a
xi +
2
2


(8)

and
σI2 ≈
b+a
2

p
p
b − a
(b − a)2 X X
b+a b−a
ωi ωj g
xi +
,
xj +
4
2
2
2
i=1 j=1
!

− µ2I

(9)

where N (.|X, C) is the multivariate Gaussian pdf
with mean X and covariance matrix C, M =
(m(s1 ), ..., m(sn )), 1k is the vector with length k and elements 1, diag(ΣDD ) is the diagonal matrix whose diagonal
is that of ΣDD , γd x|α, β) is the pdf of the gamma distribution with shape α and scale β, and where θ denotes the
hyper-parameters of the covariance kernel γ ∗ .

where the roots xi of the Legendre polynomial of order p and the weights ωi are readily available from standard textbooks on numerical analysis such as (Hildebrand,
2003) and scientific programming packages (R, Matlab and
Scipy). Extensions to rectangles in higher dimensions are
straightforward. Moreover, the complexity of such approximations only depends on the number of inducing points
and p (see Equations (3) and (4)), and hence scales well
with the data size.

1
The positive definitiveness of the induced covariance kernel
γ is a direct consequence of the positive definitiveness of γ ∗ .

A critical step in the derivation of our model is to analytically integrate out log λ(s1 ), ..., log λ(sn ) in the posterior,

Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes

to eliminate the cubic complexity in the number of data
points. To do so, we note that:
Z

n
Y

posterior variance we would have obtained, had we chosen
inducing points in those parts of the domain.
Intuitively, a good algorithm to find inducing points should
leverage prior knowledge about the smoothness, periodicity, amplitude and length scale(s) of the intensity function
to optimize for the quality of (post-training) predictions
while minimising the number of inducing points.

λ(si )N log λ(s1 ), ..., log λ(sn )|M,

(Rd )n i=1


diag(ΣDD ) d log λ(s1 )...d log λ(sn )
!!
n
X
= E exp
log λ(si )

We use as utility function for the choice of inducing points:

i=1

1
= exp(1Tn M + Tr(ΣDD ))
2

(10)

where the second equality results from the moment generating function of a multivariate Gaussian.
Thus, putting together the likelihood of Equation (1) and Equation
 (5), and integrating out
log λ(s1 ), ..., log λ(sn ) , we get:

p log λ(s01 ), ..., log λ(s0k ), I, θD)

∼ p(θ)N log λ(s01 ), ..., log λ(s0k )|M ∗ , Σ∗D0 D0

1
× exp(1Tn M + Tr(ΣDD )) exp(−I)γd I|αI , βI (11)
2

∗T
U(D0 ) = Eθ (Tr(Σ∗DD0 (θ)Σ∗−1
D 0 D 0 (θ)ΣDD 0 (θ)))

(13)

where θ is the vector of hyper-parameters of the covariance kernel γ ∗ , and the expectation is taken with respect to
the prior distribution over θ. In other words, the utility of
a set of inducing points is the expected total reduction of
the (predictive) variances of log λ(s1 ), ... log λ(sn ) resulting from knowing log λ(s01 ), ..., log λ(s0k ).
In practice, the expectation in Equation (13) might not
be available analytically. We can however use the Monte
Carlo estimate:
Ũ(D0 ) =

N
1 X
∗T
Tr(Σ∗DD0 (θ̃i )Σ∗−1
D 0 D 0 (θ̃i )ΣDD 0 (θ̃i )). (14)
N i=1

Finally, although our model allows for joint inference on
the intensity function and its integral, we restrict our attention to making inference on the intensity function for
brevity. By integrating out I from Equation (11), we get
the new posterior:

(12)
p(λ, θ|D) := p log λ(s01 ), ..., log λ(s0k ), θD)
1
∼ p(θ) exp(1Tn M + Tr(ΣDD ))(1 + βI )−αI
2

× N log λ(s01 ), ..., log λ(s0k )|M ∗ , Σ∗D0 D0

The algorithm proceeds as follows. We sample (θ̃i )N
i=1
from the prior. Initially we set k = 0, D0 = ∅ and u0 = 0.
We increment k by one, and consider adding an inducing
point. We then find the point s0k that maximises Ũ(D0 ∪{s})

where we noted that the dependencies of Equation (11) in I
is of the form exp(−x)γd (x|α, β) which can be integrated
out as the moment generating function of the gamma distribution evaluated at −1, that is (1 + β)−α .

we update D0 = D0 ∪ {sk } and stop when

3.3.3. S ELECTION OF INDUCING POINTS
Inferring the number k and positions of the inducing points
s0i is critical to our model, as k directly affects the complexity of our scheme and the positions of the inducing points
affect the quality of our prediction. Too large a k will lead
to an unduly large complexity. Too small a k will lead to
loss of information (and subsequently excessively uncertain predictions from training), and might make assertion 2
of our prior specification inappropriate. For a given k, if
the inducing points are not carefully chosen, the coverage
of the domain will not be adapted to changes in the intensity function and as a result, the predictive variance in certain parts of the domain might considerably differ from the

s0k := argmax Ũ(D0 ∪ {s})

(15)

s∈S

using Bayesian optimisation (Mockus, 2013). We compute
the utility of having k inducing points as
uk = Ũ(D0 ∪ {s0k }),

uk − uk−1
< α,
uk
where 0 < α  1 is a convergence threshold. Proposition
(a) For any D, α, N and pθ Algorithm 1 stops in finite time
and the sequence (uk )k∈N converges at least linearly with
1
.
rate 1 − #D
(b) Moreover, the maximum utility uf (α) returned by Algorithm 1 converges
to the average total unconditional variPN
ance w∞ := N1 i=1 Tr(Σ∗DD (θ̃i )) as α goes to 0.
The idea behind the proof of this proposition is that the
sequence of maximum utilities uk is positive, increasing2 , and upper-bounded by the total unconditional vari2
Intuitively, conditioning on a new point increases the reduction of variance from the unconditional variance.

Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes

Algorithm 1 Selection of inducing points
Inputs:0 < α  1, N , pθ
Output: uf , D0
k = 0, u0 = 0, D0 = ∅, e = 1;
Sample (θ̃i )N
i=1 from p(θ);
while e > α do
k = k + 1;
s0k = argmax Ũ(D0 ∪ {s});
s∈S

uk = Ũ(D0 ∪ {s0k });
D0 = D0 ∪ {s0k };
k−1
;
e = uk −u
uk
end while

ance w∞ 3 . Hence, the sequence uk converges to a strictly
positive limit, which implies that the stopping condition of
the while loop will be met in finite time regardless of D,
α, N and pθ . Finally, we construct a sequence wk upperbounded by the sequence uk and that converges linearly
to the average total unconditional variance w∞ with rate
1
1 − #D
. As the sequence uk converges and is itself upperbounded by w∞ , its limit is w∞ as well, and it converges
at least as fast as wk . (See appendix for the full proof)
Our algorithm is particularly suitable to Poisson point processes as it prioritises sampling inducing points in parts of
the domain where the data are denser. This corresponds
to regions where the intensity function will be higher, thus
where the local random counts of the underlying PPP will
vary more4 and subsequently where the posterior variance
of the intensity is expected to be higher. Moreover, it leverages prior smoothness assumptions on the intensity function to limit the number of inducing points and to appropriately and sequentially improve coverage of the domain.
Algorithm 1 is illustrated on a variety of real life and synthetic data sets in section 5.

4. Inference
We use a squared exponential kernel for γ ∗ and scaled sigmoid Gaussian priors for the kernel hyper-parameters; that
θimax
is θi = 1+exp(−x
where xi are i.i.d standard Normal. The
i)
problem-specific scales, θimax , restrict the supports of those
distributions using prior knowledge to avoid unlikely extreme values and to improve conditioning.
We use a Block Gibbs Sampler (Geman & Geman, 1984)
to sample from the posterior. We sample the hyperparameters using the Metropolis-Hastings (Hastings, 1970)
algorithm taking as proposal distribution the prior of the
3

The variance cannot be reduced by more than the total unconditional variance.
4
The variance of the Poisson distribution is its mean.

variable of interest. We sample the log-intensities at the
inducing points using Elliptical Slice Sampling (Murray
et al., 2010) with the pdf in Equation (12).
Prediction from training
To predict the posterior mean at the data points we note
from the law of total expectation that
∀si ∈ D, E(log λ(si )|D)
 
= E E log λ(si )|{log λ∗ (s0j )}kj=1 , D |D .

(16)

Also, we note from Equations (1) and (5) that the dependency of the posterior of log λ(si ) conditional on
{log λ∗ (s0j )}kj=1 is of the form
exp(log λ(si )) × N (log λ(si )|m(si ), γ(si , si )),
where we recall that m(si ) is the i-th element of the vector
M and γ(si , si ) is the i-th diagonal element of the matrix
ΣDD . Hence, the posterior distribution of log λ(si ) conditional on {log λ∗ (s0j )}kj=1 is Gaussian with mean

E log λ(si )|{log λ∗ (s0j )}kj=1 , D = M [i] + ΣDD [i, i]
(17)
and variance

Var log λ(si )|{log λ∗ (s0j )}kj=1 , D = ΣDD [i, i].

(18)

Finally, it follows from Equation (16) that E(log λ(si )|D)
is obtained by averaging out M [i] + ΣDD [i, i] over MCMC
samples after burn-in.
Similarly, the law of total variance implies that
Var(log λ(si )|D)
 
= E Var log λ(si )|{log λ∗ (s0j )}kj=1 , D |D
 
+ Var E log λ(si )|{log λ∗ (s0j )}kj=1 , D |D .

(19)

Hence, it follows from Equations (17) and (18) that the posterior variance at a data point si is obtained by summing up
the sample mean of ΣDD [i, i] with the sample variance of
M [i] + ΣDD [i, i], where sample mean and sample variance
are taken over MCMC samples after burn-in.

5. Experiments
We selected four data sets to illustrate the performance of
our model. We restricted ourselves to one synthetic data
set for brevity. We chose the most challenging of the
synthetic intensity functions of (Adams et al., 2009) and
t
2
) + exp(−( t−25
(Rao & Teh, 2011), λ(t) = 2 exp(− 15
10 ) ),
to thoroughly compare our model with competing methods. We also ran our model on a standard 1 dimensional
real-life data set (the coal mine disasters dataset used in

Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes
Table 1. Maximum output (resp. input) scale hmax (resp. lmax )
used for each data set to select inducing points.

1.0
0.9

hMAX
lMAX

SYNTHETIC

COAL MINE

BRAMBLE

10.0
25.0

10.0
50.0

10.0
0.25

T WITTER
10.0
5.0

0.8
0.7
0.6

Table 2. Number of inducing points produced by Algorithm 1 required to achieve some critical normalised utility values on the 4
data sets.

0.5
0.4

eg
cm
t
b

0.3
0.2

k
uk
u∞

SYNTHETIC

COAL MINE

BRAMBLE

0.75
0.90
0.95

2
3
4

2
4
5

8
17
28

T WITTER
3
5
8

(Jarrett, 1979); 191 points) and a standard real-life 2 dimensional data (spatial location of bramble canes (Diggle,
1983); 823 points). Finally we ran our model on a reallife data set large enough to cause problems to competing
models. This data set consists of the UTC timestamps (expressed in hours in the day) of Twitter updates in English
published in the (Twitter sample stream, 2014) on September 1st 2014 (188544 points).
5.1. Inducing points selection
Figure 1 illustrates convergence of the selection of inducing points on the 4 data sets. We ran the algorithm 10 times
with N = 20, and plotted the average normalised utility
uk
u∞ ± 1 std as a function of the number of inducing points.
Table 1 contains the maximum hyper-parameters that were
used for each data set. Table 2 contains the number of inducing points required to achieve some critical normalised
utility values for each of the 4 data sets. We note that just 8
inducing points were required to achieve a 95% utility for
the Twitter data set (188544 points). In regards to the positions of sampled inducing points, we note from Figures 2
and 3 that when the intensity function was bimodal, the first
inducing point was sampled around the argument of the
highest mode, and the second inducing point was sampled
around the argument of the second highest mode. More
generally, the algorithm sampled inducing points where the
latent intensity function varies the most, as expected.
5.2. Intensity function
In each experiment we generated 5000 samples after burnin (1000 samples). For each data set we used the set of
inducing points that yielded a 95% normalized utility. The
exact numbers are detailed in Table 2.
We ran a Monte Carlo simulation for the stochastic pro-

0.1

0

5

10

15

20

25

30

k
Figure 1. Average normalised utility uu∞
of choosing k inducing
points using Algorithm 1 ± 1 standard deviation as a function of
k on the synthetic data set (eg), the coal mine data set (cm), the
Twitter data set (t) and the bramble canes data set (b). The average
was taken over 10 runs.

cesses considered herein and found that the Legendre polynomial order p = 10 was sufficient to yield a Quadrature
estimate for the standard deviation of the integral less than
1% away from the Monte Carlo estimate (using the trapezoidal rule), and a Quadrature estimate for the mean of the
integral less than a standard error away from the Monte
Carlo average. We took a more conservative stand and used
p = 20.
Inference on synthetic data
We generated a draw from a Poisson point process with the
t
2
intensity function λ(t) = 2 exp(− 15
) + exp(−( t−25
10 ) ) of
(Adams et al., 2009) and (Rao & Teh, 2011). The draw consisted of 41 points (blue sticks in Figure 2). We compared
our model to (Adams et al., 2009) (SGCP) and (Rao & Teh,
2011) (RMP). We ran the RMP model with the renewal parameter γ set to 1 (RMP 1), which corresponds to an exponential renewal distribution or equivalently an inhomogeneous Poisson process. We also ran the RMP model with a
uniform prior on [1, 5] over the renewal parameter γ (RMP
full). Figure 2 illustrates the posterior mean intensity function under each model. Finally we ran the Dirichlet process
Mixture of Beta model of (Kottas, 2006) (DPMB). As detailed in Table 3, our model outperformed that of (Adams
et al., 2009), (Rao & Teh, 2011) and (Kottas, 2006) in terms
of accuracy and speed.
Inference on real-life data
Figure 3 shows the posterior mean intensity functions of
the coal mine data set, the Twitter data set and the bramble
canes data set under our model.

Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes

3.5

s4

s1

s8

s3

s6

s2

s5 s7

5

s3′

s1′

s4′

s5′

s2′

3.0
4

2.5

us
Real
SGCP
RMP full
RMP 1
DPMB

2.0

1.5

3

2

1.0

1

0.5
0
1860

0.0
0

10

20

30

40

1880

1900

1920

1940

1960

(a)

50

40000

s7s3 s1

s8

Figure 2. Inference on a draw (blue sticks) from a Poisson point
t
process with intensity λ(t) = 2 exp(− 15
) + exp(−( t−25
)2 )
10
(black line). The red dots are the inducing points generated by
our algorithm, labelled in the order they were selected. The solid
blue line and the grey shaded area are the posterior mean ± 1
posterior standard deviation under our model. SGCP is the posterior mean under (Adams et al., 2009). RMP full and RMP 1
are the posterior mean intensities under (Rao & Teh, 2011) with γ
inferred and set to 1 respectively. DPMB is the Dirichlet process
mixture of Beta (Kottas, 2006)

s4 s2s6

s5

35000

30000

25000

20000

15000

10000

5000

0
0

Scalability: We note that it took only 240s on average
to generate 1000 samples on the Twitter data set (188544
points). As a comparison, this is the amount of time that
would be required to generate as many samples on a data
set that has 50 points (resp. 100 points) under the models
of (Adams et al., 2009) (resp. (Rao & Teh, 2011)). More
importantly, it was not possible to run either of those two
competing models on the Twitter data set. Doing so would
require computing 17×1010 covariance coefficients to evaluate a single auto-covariance matrix of the log-intensity at
the data points, which a typical personal computer cannot
handle.

5

10

15

20

25

(b)
1.0

2200

1900
0.8
1600

0.6
1300

1000
0.4

700
0.2
400

Table 3. Some statistics on the MCMC runs of Figure 2. RMSE
and MAE denote the Root Mean Square Error and the Mean Absolute Error, expressed as a proportion of the average of the true
intensity function over the domain. LP denotes the log mean predictive probability on 10 held out PPP draws from the true intensity ± 1 std. t(s) is the average time in seconds it took to generate
1000 samples ± 1 std and ESS denotes the average effective sample size (Gelman et al., 2013) per 1000 samples.

SGCP
RMP 1
RMP FULL
DPMB
US

MAE
0.31
0.32
0.25
0.23
0.19

RMSE
0.37
0.38
0.31
0.32
0.27

LP
-45.07 ± 1.64
-45.24 ± 1.41
-43.51 ± 2.15
-42.95 ± 3.58
-42.84 ± 3.07

T (S)

257.72 ± 16.29
110.19 ± 7.37
139.64 ± 5.24
23.27 ± 0.94
4.35 ± 0.12

ESS
6
23
6
47
38

0.0
0.0

100
0.2

0.4

0.6

0.8

1.0

(c)
Figure 3. Inference on the intensity functions of the coal mine
data set (top), the Twitter data set (middle), and the bramble canes
data set (bottom). Blue dots are data points, red dots are inducing
points (labelled in the upper panels in the order they were selected), the grey area is the 1 standard deviation confidence band.

6. Discussion
Scalability of the selection of inducing points

Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes

The computational bottleneck of the selection of inducing
points is in the evaluation of
∗T
Tr(Σ∗DD0 (θ̃i )Σ∗−1
D 0 D 0 (θ̃i )ΣDD 0 (θ̃i )).

Hence, the complexity and the memory requirement of the
selection of inducing points are both linear in the number
of data points n := #D.
The number of inducing points generated by our algorithm
does not increase with the size of the data, but rather as a
function of the size of the domain and the resolution implied by the prior over the hyper-parameters.
Comparison with competing models
We note that the computational bottleneck of our MCMC
inference is in the evaluation of
∗T
Tr(ΣDD ) = Tr(Σ∗DD ) − Tr(Σ∗DD0 Σ∗−1
D 0 D 0 ΣDD 0 ).

Hence, inferring the intensity function under our model
scales computationally in O(nk 2 ) and has a memory requirement O(nk), where the number of inducing points k
is negligible. This is considerably better than alternative
methods using Gaussian processes (Adams et al., 2009;
Rao & Teh, 2011) whose complexities are cubic in the
number of data points and whose memory requirement is
squared in the number of data points. Moreover, the superior accuracy of our model compared to (Adams et al.,
2009) and (Rao & Teh, 2011) is due to our use of the exponential transformation rather than the scaled sigmoid one.
In effect, unlike the inverse scaled sigmoid function that
tends to amplify variations, the logarithm tends to smooth
out variations. Hence, when the true intensity is uneven,
the log-intensity is more likely to resemble a draw from a
stationary GP than the inverse scaled sigmoid of the true
intensity function, and subsequently a stationary GP prior
in the inverse domain is more suitable to the exponential
transformation than to the scaled sigmoid transformation.
Our model is also more suitable than that of (Cunningham
et al., 2008a) when confidence bounds are needed for the
intensity function, or when the input space is of dimension
higher than 1. The model is a useful alternative to that of
(Kottas, 2006), whose complexity is also linear. In effect,
Gaussian processes (GP) are more flexible than a Dirichlet
process (DP) mixture of Beta distributions. This is the result of the large number of known covariance kernels available in the literature and the state-of-the-art understanding
of how well a given kernel can approximate an arbitrary
function (Micchelli et al., 2006; Pillai et al., 2007). Moreover, unlike a Dirichlet process mixture of Beta distributions, Gaussian processes allow directly expressing practical prior features such as smoothness, amplitude, length
scale(s) (memory), and periodicity.
As our model relies on the Gauss-Legendre quadrature, we
would not recommend it for applications with a large input

space dimension. However, most interesting point process
applications involve modelling temporal, spatial or spatiotemporal events, for which our model scales considerably
better with the data size than competing approaches. In effect, the models proposed by (Kottas, 2006; Cunningham
et al., 2008a;b; Rao & Teh, 2011) are all specific to unidimensional input data, whereas the model introduced by
(Kottas & Sanso, 2007) is specific to spatial data. As for
the model of (Adams et al., 2009), it scales very poorly
with the input space dimension for its complexity is cubic
in the sum of the number of data points and the number of
latent thinning points, and the number of thinning points
grows exponentially with the input space dimension5 .
Extension of our model
Although the covariance kernel γ ∗ was assumed stationary,
no result in this paper relied on that assumption. We solely
needed to evaluate covariance matrices under γ ∗ . Hence,
the proposed model and algorithm can also be used to account for known non-stationarities. More generally, the
model presented in this paper can serve as foundation to
make inference on the stochastic dependency between multiple point processes when the intensities are assumed to be
driven by known exogenous factors, hidden common factor, and latent idiosyncratic factors.

7. Summary
In this paper we propose a novel exact non-parametric
model to make inference on Poisson point processes using
Gaussian processes. We derive a robust MCMC scheme to
sample from the posterior intensity function. Our model
outperforms competing benchmarks in terms of speed and
accuracy as well as in the decorrelation of MCMC samples. A critical advantage of our approach is that it has
a numerical complexity and a memory requirement linear
in the data size n (O(nk 2 ), and O(nk) respectively, with
k  n). Competing models using Gaussian processes have
a cubic numerical complexity and squared memory requirement. We show that our model readily handles data sizes
not yet considered in the literature.

Acknowledgments
Yves-Laurent Kom Samo is supported by the Oxford-Man
Institute of Quantitative Finance.
5
The expected number of thinning points grows proportionally with the volume of the domain, which is exponential in the
dimension of the input space when the domain is a hypercube with
a given edge length.

Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes

References
Adams, R.P., Murray, I., and MacKay, D.J.C. Tractable
nonparametric Bayesian inference in Poisson processes
with Gaussian process intensities. pp. 9–16, 2009.
Basu, S.and Dassios, A. (2002) A Cox process with lognormal intensity. Insurance: mathematics and economics, 31 (2). pp. 297–302. ISSN 0167-6687
Cox, D.R. Some statistical methods connected with series
of events. Journal of the Royal Statistical Society, 17:
129–164, 1955.
Cox, D.R., Isham, V. (eds.). Point Processes. Chapman
Hall/CRC, 1980.
Cunningham, J.P., Shenoy, K.V., and Sahani, M. Fast
Gaussian process methods for point process intensity estimation. Appearing in Proceedings of the 25 th International Conference on Machine Learning, Helsinki, Finland, 2008.
Cunningham, J.P., Yu, B., Shenoy, K.V., and Sahani, M. Inferring neural firing rates from spike trains using Gaussian Processes. Advances in Neural Information Processing Systems 20 pp. 329–336.
Daley, D.J. and Vere-Jones, D. An introduction to the theory of point processes. Springer-Verlag, 2008.

Heikkinen, J., Arjas, E. Modeling a Poisson forest in variable elevations: a nonparametric Bayesian approach.
Biometrics, 55, 738–745, 1999.
Hildebrand, F. B, Introduction to numerical analysis: second edition, Chap. 8. Dover Publications, Inc., 2003.
Jarrett, R.G. A note on the intervals between coal-mining
disasters. Biometrika, 66, 191–193.
Kingman, J.F.C. Poisson Processes. Oxford Science Publications, 1992.
Kottas, A. Dirichlet process mixtures of beta distributions,
with applications to density and intensity estimation. In
Proceedings of the Workshop on Learning with Nonparametric Bayesian Methods, 23rd ICML, Pittsburgh,
PA, 2006.
Kottas, A., and Sanso, B. Bayesian mixture modeling for
spatial Poisson process intensities, with applications to
extreme value analysis. Journal of Statistical Planning
and Inference. Journal of Statistical Planning and Inference, 137, 3151–3163, 2007.
Micchelli, C.A., Xu, Y., Zhang, H. Universal kernels. Journal of Machine Learning Research, 7 (2006) 2651-2667.

Diggle, P.J. Statistical analysis of spatial point patterns.
Academic Press.

Metropolis, N., Rosenbluth, A.W, Rosenbluth, M.N.,
Teller, A.H., and Teller, E. Equations of state calculations by fast computing machines. Journal of Chemical
Physics, 24:1087–1092, 1953.

Diggle, P.J. A kernel method for smoothing point process
data. Applied Statistics, 34:138–147, 1985.

Mockus, J. Bayesian approach to global optimization: theory and applications. Kluwer Academic, 2013.

Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., and Rubin, D.B. (eds.). Bayesian data analysis
thrid edition. CRC Press, 2013.

Moeller, J., Syversveen, A., and Waagepetersen, R. LogGaussian Cox processes. Scandinavian Journal of
Statistics, 1998.

Geman, S. and Geman, D. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 6:721–741, 1984.

Murray, I., Adams, R.P., and MacKay, D.J.C. Elliptical
slice sampling. pp. 9–16. Appearing in Proceedings of
the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), 2010.

Gregory, P. C., Loredo, T. J. A new method for the detection
of a periodic signal of unknown shape and period. The
Astrophysical Journal, The Astrophysical Journal, 398,
146–168, 1992.

Pillai, N.S., Wu, Q., Liang, F., Mukherjee, S., and Wolpert,
R.L. Characterizing the function space for Bayesian kernel models. Journal of Machine Learning Research, 8:
1769–1797, 2007.

Gunter, T., Lloyd, C., Osborne, M.A., Roberts, S.J. Efficient Bayesian nonparametric modelling of structured
point processes. Uncertainty in Artificial Intelligence
(UAI), 2014.

Quinonero-Candela, J. and Rasmussen C.E. A unifying
view of sparse approximate Gaussian process regression. Journal of Machine Learning Research, 6 (2005)
1939–1959.

Hastings, W.K. Monte Carlo sampling methods using
Markov chains and their applications. Biometrika, 24:
97–109, 1970.

Rao, V. A. and Teh, Y. W. Gaussian process modulated
renewal processes. Neural Information Processing Systems (NIPS), 2011.

Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes

Rasmussen, Carl E. and Williams, Christopher K.I. (eds.).
Gaussian processes for machine learning. The MIT
Press, 2006.
Rathbum, S.L. and Cressie, N.A.C. Asymptotic properties of estimators for the parameters of spatial inhomogeneous Poisson point processes. Advances in Applied
Probability, 26:122–154, 1994.
Twitter Inc.
Twitter sample stream
https://dev.twitter.com/streaming/reference/get/
tuses/sample

API.
sta-

