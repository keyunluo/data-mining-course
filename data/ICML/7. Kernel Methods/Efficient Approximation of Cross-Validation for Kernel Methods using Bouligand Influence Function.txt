Efficient Approximation of Cross-Validation for Kernel Methods using
Bouligand Influence Function

Yong Liu
Shali Jiang
Shizhong Liao
School of Computer Science and Technology, Tianjin University, Tianjin 300072, P. R. China

Abstract
Model selection is one of the key issues both in
recent research and application of kernel methods. Cross-validation is a commonly employed
and widely accepted model selection criterion.
However, it requires multiple times of training the algorithm under consideration, which is
computationally intensive. In this paper, we
present a novel strategy for approximating the
cross-validation based on the Bouligand influence function (BIF), which only requires the solution of the algorithm once. The BIF measures
the impact of an infinitesimal small amount of
contamination of the original distribution. We
first establish the link between the concept of BIF
and the concept of cross-validation. The BIF is
related to the first order term of a Taylor expansion. Then, we calculate the BIF and higher order BIFs, and apply these theoretical results to
approximate the cross-validation error in practice. Experimental results demonstrate that our
approximate cross-validation criterion is sound
and efficient.

1. Introduction
Kernel methods, such as SVM (Steinwart & Christmann,
2008; Vapnik, 2000), least squares support vector machine
(LSSVM) (Suykens & Vandewalle, 1999) and support vector regression (SVR) (Shawe-Taylor & Cristianini, 2000),
have been widely used in data mining and machine learning. The performance of these kernel methods greatly depends on the choice of some hyper-parameters (such as the
kernel parameter and regularization parameter), therefore
the model selection problem becomes an important topic
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

YONGLIU @ TJU . EDU . CN
SLJIANG @ TJU . EDU . CN
SZLIAO @ TJU . EDU . CN

in kernel methods. A related problem is the evaluation of
the generalization ability of the learning algorithms. In fact, it is common to select the optimal hyper-parameters by
choosing the ones with the lowest generalization error.
Obviously, the generalization error is not directly computable, as the probability distribution generating the data
is unknown, therefore it is necessary to resort to estimates
of its value. This error can be estimated either via testing on
some data which has not been used for learning (hold-out
validation or cross-validation techniques) or via a bound
given by theoretical analysis (Chapelle et al., 2002). To establish the upper bounds of the generalization error, some
measures are introduced: such as VC dimension (Vapnik,
2000), Rademacher complexity (Bartlett & Mendelson,
2002), maximal discrepancy (Bartlett et al., 2002), regularized risk (Schölkopf & Smola, 2002), radius-margin bound
(Vapnik, 2000), compression coefficient (Luxburg et al.,
2004) and eigenvalues perturbation (Liu et al., 2013).
While there have been many interesting attempts to use
the above bounds or other techniques to pick the hyperparameters, the most commonly used and widely accepted methods for selecting the hyper-parameters are still
the k-fold cross-validation (KCV) and leave-one-out crossvalidation (LOO). However, KCV and LOO requires the
solution of the algorithm under consideration several times,
which are computationally expensive. For the sake of efficiency, some approximate LOO criteria for some specific algorithms are given: such as generalized crossvalidation (GCV) (Golub et al., 1979), influence function
(Debruyne et al., 2008), generalized approximate crossvalidation (GACV) (Wahba et al., 1999) and span bound
(Chapelle et al., 2002).
In this paper, we will present a novel strategy for approximating the k-fold cross-validation based on the Bouligand
influence function (BIF) (Christmann & Messem, 2008).
To our knowledge, an effective strategy for approximating the k-fold cross-validation error (for all k) for kernel
methods has never been given before. We establish the link

Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function

between the concept of BIF and the concept of KCV, and
present a novel method to calculate the BIF and higher order BIFs at the continuous distribution. Furthermore, we
evaluate these BIFs at the sample distribution and use these
BIFs to obtain an approximation of KCV. Our method requires the solution of the algorithm only once, which can
dramatically improve the efficiency. Experimental results demonstrate that our BIF criterion is a good choice for
model selection.
Related Work
In recent years, some researchers study the robustness of
the kernel methods. In the field of robust statistics the influence function (IF) (Hampel et al., 1986) is introduced in order to analyze the effects of outliers on the algorithm. This
influence function is defined for continuous distributions that are slightly perturbed by adding a small amount of
probability mass at a certain place. Christmann and Steinwart (Christmann & Steinwart, 2004; 2007), Steinwart and
Christmann (Steinwart & Christmann, 2008), Christmann
et al (Christmann et al., 2009), and Messem and Christmann (Messem & Christmann, 2010) show that SVMs for
classification and regression have a bounded influence
function under some assumptions of the loss function. Debruyne et al (Debruyne et al., 2008) presented a method to
estimate the LOO via the influence function. Christmann
and Messem (Christmann & Messem, 2008) generalize the
notion of influence function, and introduce a new notion
from Bouligand-derivatives (Robinson, 1991) called Bouligand influence function (BIF), which measures the impact
of an infinitesimal small amount of contamination of the original distribution. They show that SVMs have a bounded
BIF with some weaker assumptions of loss function.
For kernel methods, such as SVM, LSSVM
∑ and SVR, the
form of the decision function is f (x) = i αi K(x, xi ) +
b. The above work about the robust statistics of kernel
methods all ignore the bias b. However, sometimes the
bias b plays an important role in the performance of kernel
methods. In this paper, we consider the b, and present a theoretical result to calculate the BIF at the continuous distribution. This result generalizes the result of Christmann and
Messem (Christmann & Messem, 2008) with a much simpler proof. Debruyne et al (Debruyne et al., 2008) present
a method to calculate the higher order IFs, and apply these
results to approximate the LOO. We generalize the results
of IFs to BIFs, and apply these results of BIFs to approximate the cross-validation error.
The rest of the paper is organized as follows. In Section
2, we introduce some elementary facts. In Section 3, we
introduce the concept of BIF, and give a novel strategy for
approximating the cross-validation error. A method to calculate the BIF and higher order BIFs is proposed in Section

4. In Section 5, we show how to use these BIFs to approximate the cross-validation estimator in practice. We empirically analyze the performance of our proposed approximate
cross-validation criterion in Section 6. We end in Section 7
with conclusion.

2. Preliminaries
Let S = {(xi , yi )}ni=1 be a sample set of size n drawn
identically and independently from a fixed, but unknown
probability measure P on Z = X × Y, X ⊆ Rd , Y ⊆ R
for regression, and Y ⊆ {+1, −1} for classification. Let
K : X × X → R be a kernel, that is, K is symmetric
and for any finite set of points {x1 , . . . , xn } ⊂ X , the kerm
nel matrix K = [K(xi , xj )]i,j=1 is positive semidefinite.
The reproducing kernel Hilbert space (RKHS) H associated with the kernel K is defined to be the completion of
the linear span of the set of functions {Φ(x) = K(x, ·) :
x ∈ X } with the inner product denoted as ⟨·, ·⟩K satisfying
⟨Φ(x), Φ(x′ )⟩K = K(x, x′ ) (Aronszajn, 1950).
The operator fλ,K +bλ,K : P → fλ,K,P +bλ,K,P is defined
by fλ,K,P + bλ,K,P =
arg min EP V (y − f (x) − b) + λ∥f ∥2K ,

f ∈H,b∈R

where V (·) is a loss function and λ is the regularization
parameter. When the sample distribution Pn is used, one
has that fλ,K,Pn + bλ,K,Pn =
1∑
V (yi − f (xi ) − b) + λ∥f ∥2K .
f ∈H,b∈R n i=1
n

arg min

Such estimators have been studied in detail, see for example (Wahba, 1990; Vapnik, 2000).
LSSVM (Suykens & Vandewalle, 1999; Cawley & Talbot,
2007), ϵ-insensitive support vector regression (ϵ-SVR)
(Shawe-Taylor & Cristianini, 2000) and quadratic ϵinsensitive support vector regression (quadratic ϵ-SVR)
(Shawe-Taylor & Cristianini, 2000) are only different in
the choice of the loss function. For LSSVM, V (r) = r2 ,
for ϵ-SVR, V (r) = max{|r| − ϵ, 0}, and for quadratic ϵSVR, V (r) = (max{|r| − ϵ, 0})2 .
Unless specially stated, we respectively write fλ,K,P and
bλ,K,P as fP and bP in the following.

3. A Strategy for Fast Approximation of
Cross Validation
In this section, we introduce the Bouligand influence function (BIF) (Christmann & Messem, 2008) and higher order
BIFs, and show how to use these BIFs to approximate the
k-fold cross-validation (KCV).

Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function

3.1. Bouligand Influence Function
Definition 1. Let P be a distribution and T be an operator
T : P → T (P ). Then the Bouligand influence function
(BIF) of T at P in the direction of a distribution Q ̸= P is
defined as

The right hand side now only depends on the full sample
Pn and ∆Si . Given the BIFj (∆Si ; fλ,K + bλ,K , Pn ), the
k-fold cross validation error can be written as
k-CV =

T ((1 − ϵ)P + ϵQ) − T (P )
.
ϵ→0
ϵ

BIF (Q; T, P ) = lim

The BIF measures the impact of an infinitesimal small amount of contamination of the original distribution P
in the direction of Q on the quantity of T (P ).
Denote Pϵ,Q = (1 − ϵ)P + ϵQ. One can see that the BIF
is a first order derivative of T (Pϵ,Q ) at ϵ = 0. Higher order
BIFs can be defined too:
Definition 2. Let P be a distribution and T be an operator
T : P → T (P ). Then the kth order BIF of T at P in the
direction of a distribution Q is defined as
BIFk (Q; T, P ) =

∂
T (Pϵ,Q )|ϵ=0 .
∂kϵ

If all BIFs exist then the following Taylor expansion holds:
T (Pϵ,Q ) = T (P ) +

∞ i
∑
ϵ
i=1

i!

BIFi (Q; T, P ).

(1)

k
1∑ ∑ (
ℓ yj , fPn + bPn +
n i=1
xj ∈Si
)p
∞ (
∑
−M
BIFp (∆Si ; fλ,K + bλ,K , Pn ) )
,
n−M
p!
p=1

where ℓ(·, ·) is an appropriate loss function. It only requires
the solution of the algorithm once.


 (−1)p 
Note that −M/(n − M ) = −1/(k − 1), the  (k−1)
p p!  is
very small for some large p. Thus, we can take the low
order approximation of the Taylor expansion to effectively
approximate the k-fold cross-validation:
k-CV ≈

k
1∑ ∑ (
ℓ yj , fPn + bPn +
n i=1
xj ∈Si
)p
r (
∑
BIFp (∆Si ; fλ,K + bλ,K , Pn ) )
−M
.
n−M
p!
p=1

Remark 1. In our experiments, when the order of Taylor
expansion r ≥ 3, we find that the value of the approximate
cross-validation error is almost the same as original one.

3.2. A Strategy for Approximating the KCV using BIF

4. The Calculation of BIFs

Assume the sample set S = {(xi , yi )}ni=1 is divided into
k disjoint parts {Si }ki=1 . Let Pn−Si be the empirical distribution of the sample S without the observations Si , that is
1
Pn−Si (x) = n−M
if x ∈ S \ Si , otherwise 0, where M is
the size of Si .

In this section, we first provide a novel method to calculate
the BIF and higher order BIFs at the continuous distribution P , and then estimate these BIFs at the specific sample
distribution Pn .

For k-fold cross-validation, the T (Pn−Si ) should be computed for every i. This means that the algorithm under
consideration has to be executed k times, which is computationally intensive.
If the BIFs of T can be calculated, we can provide a fast
alternative. First note that
))
(
(
−M
−M
−Si
Pn +
∆Si ,
Pn = 1 −
n−M
n−M
where ∆Si is the sample distribution corresponding to the
1
sample Si , that is, ∆Si (x) = M
if x ∈ Si , otherwise
M
0. Thus, taking Q = ∆Si , ϵ = − n−M
, Pϵ,Q = Pn−Si ,
P = Pn and T = fλ,K + bλ,K , Equation (1) gives
fP −Si + bP −Si = fPn + bPn +
n
n
)j
∞ (
∑
BIFj (∆Si ; fλ,K + bλ,K , Pn )
−M
.
n
−
M
j!
j=1

(2)

4.1. The Calculation of BIFs at Continuous
Distribution
By the definition of the k-th order BIF of fλ,K + bλ,K ,
k = 1, 2, . . ., it is easy to verify that
BIFk (Q; fλ,K + bλ,K , P ) =

∂
∂
fPϵ,Q |ϵ=0 + k bPϵ,Q |ϵ=0 .
k
∂ ϵ
∂ ϵ

Let VP = V (y − fP (x) − bP )), the first order BIF at the
P will be given in the following theorem.
Theorem 1. Let H be the RKHS of a bounded continuous
kernel K on X . Furthermore, let P be a distribution on
X × Y, then the BIF of fλ,K + bλ,K in the direction of a
distribution Q ̸= P is
∂
∂
fP |ϵ=0 , bPϵ,Q |ϵ=0 ] =
∂ϵ ϵ,Q
∂ϵ
L−1 [−2λfP + EQ (VP′ Φ(x)), EQ VP′ ],
[

Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function

where the operator L : (H, R) → (H, R) is defined by
[
L(f, b) = 2λf + EP (VP′′ f (x)Φ(x)) + bEP (VP′′ Φ(x)),
]
EP (VP′′ f (x)) + bEP (VP′′ ) .
The proof is given in Appendix A.
Remark 2. The first order BIF of the decision function
without the bias term (bλ,K = 0) has been given in
(Christmann & Messem, 2008). Our above theorem generalizes their result. Moreover, our proof is much simpler.
The higher order BIF is given in the following theorem:
Theorem 2. Let H be the RKHS of a bounded continuous
kernel K on X . Let V be a convex loss function such that
the third derivative is 0. Furthermore, let P be a distribution on X × Y, then the (k + 1) order BIF of fλ,K + bλ,K
in the direction of a distribution Q ̸= P is
[
]
∂
∂
fP |ϵ=0 , k+1 bPϵ,Q |ϵ=0 =
∂ k+1 ϵ ϵ,Q
∂
ϵ
[
−1
(k + 1)L
2EP (BIFk (Q; fλ,K + bλ,K , P )VP′′ (Φ(x)))−
EQ (BIFk (Q; fλ,K + bλ,K , P )VP′′ Φ(x)),

where the operator L : (H, R) → (H, R) is defined by
[
L(f, b) = 2λf + EP (VP′′ f (x)Φ(x)) + bEP (VP′′ Φ(x)),
]
EP (VP′′ f (x)) + bEP (VP′′ ) .
The proof is given in Appendix B.
Remark 3. For the common loss function V , such as
V (r) = r2 and V (r) = (max(|r| − ϵ, 0)2 , the third derivative is 0. Thus, the assumption of the above Theorem is
feasible.
4.2. The Calculation of BIFs at the Sample Distribution
In this subsection, we will estimate the BIF at the sample
distribution Pn to obtain BIFj (∆Si ; fλ,K + bλ,K , Pn ).
4.2.1. LSSVM
2

First consider taking the least squares loss V (r) = r .
From Theorem 1, the operator L at Pn maps any (f, b) ∈
(H, R) to
n
n
[
2∑
2b ∑
L(f, b) = 2λf +
f (xj )Φ(xj ) +
Φ(xj ),
n j=1
n j=1
n




L(f, b)(x1 )
[
λIn + n1 K


..

=2
1 T
.
n1
L(f, b)(xn )
which means that the matrix
[
λIn + n1 K
2Ln := 2
1 T
n1

1
n K1

1

1
n K1

][

f
b

]
,

]

1

is the finite sample version of the operator L at Pn . Denote
∂
fP
|ϵ=0
∂ k ϵ ϵ,∆Si
∂
∂
= ( k fPϵ,∆S (x1 )|ϵ=0 , . . . , k fPϵ,∆S (xn )|ϵ=0 )T .
i
i
∂ ϵ
∂ ϵ
From Theorem 1, it is now clear that
[ ∂
]
]
[ 1
[K • Si ]g − λfPn
∂ϵ fPϵ,∆Si |ϵ=0
−1
M
= Ln
1 T
∂
M gSi 1
∂ϵ bPϵ,∆S |ϵ=0
i

EP (BIFk (Q; fλ,K + bλ,K , P )VP′′ )−
]
EQ (BIFk (Q; fλ,K + bλ,K , P )VP′′ ) .

]
2∑
f (xj ) + 2b .
n j=1

Denote f = (f (x1 ), . . . , f (xn ))T , 1 = (1, . . . , 1)T , kernel matrix K = [K(xi , xj )]ni,j=1 . Note that

where g = (g1 , . . . , gn )T , gi = yi − fPn (xi ) − bPn , gSi =
(gSi ,1 , . . . , gSi ,n )T , gSi ,j = gj if xj ∈ Si , otherwise 0,
fPn = (fPn (x1 ), . . . , fPn (xn ))T , Si denote the n × n
matrix as [Si ]j,k = 1 if xk ∈ Si , otherwise 0, and • is
the entrywise matrix product (also known as the Hadamard
product).
From Theorem 2, one sees similarly that the higher order
terms can be computed
[ ∂
]
f
|
∂ k+1 ϵ Pϵ,∆Si ϵ=0
=
∂
b
|
∂ k+1 ϵ Pϵ,∆Si ϵ=0
]
[ 1
1
Kbk − M
K • Si bk
−1
n
,
(k + 1)Ln
1 T
1 T
n 1 bk − M 1 bk,Si
where

(
bk = BIFk (∆Si ; fλ,K + bλ,K , Pn )(x1 )), . . . ,
)T
BIFk (∆Si ; fλ,K + bλ,K , Pn )(xn ) ,

BIFk (∆Si ; fλ,K + bλ,K , Pn )(xj ) =
∂
∂
fϵ,∆Si (xj )|ϵ=0 + k bϵ,∆Si (xj )|ϵ=0 ,
∂kϵ
∂ ϵ
bk,Si = (bk,Si ,1 , . . . , bk,Si ,n )T , bk,Si ,j = bk,j if xj ∈ Si ,
otherwise 0.
For the k-fold cross-validation, define [BIF M LSSV Mt ]
the k × n matrix with
[BIF M LSSV Mt ]i,j = BIFt (∆Si ; fλ,K + bλ,K , Pn )(xj ).

Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function

According to Equation (2), by cutting it off at some step r,
we have
fP −Si (xj ) + bP −Si ≈ fPn (xj ) + bPn +
n
n
)s
r (
∑
−1
1
[BIF M LSSV Ms ]i,j .
k
−
1
s!
s=1

From Equation (2), we have
fP −Si (xj ) + bP −Si ≈ fPn (xj ) + bPn +
n
n
)s
r (
∑
1
−1
[BIF M SV Rs ]i,j .
k−1
s!
s=1

(3)

(4)

5. Approximate KCV Criteria
4.2.2. Q UADRATIC ϵ-SVR

The traditional k-fold cross-validation error is given by

For the quadratic ϵ-insensitive loss we have that
{
0,
if |r| ≤ ϵ
V (r) =
2
(r − ϵ) , if |r| > ϵ

kCV =

k
1∑ ∑
ℓ(yj , fP −Si (xj ) + bP −Si ),
n
n
n i=1
xj ∈Si

{

0,
if |r| < ϵ
and thus V ′ (r) =
, V ′′ (r) =
2(r − ϵ), if |r| > ϵ
{
0, if |r| < ϵ
Note that the derivatives in r = ϵ do not
2, if |r| > ϵ.
exist, but in practice the probability that r = ϵ is 0, so we
can ignore this possibility.

where ℓ(·, ·) is an appropriate loss function. The idea we investigate is to replace the explicit k-fold cross-validation by
the approximation in (3) for LSSVM and (4) for quadratic
ϵ-SVR.
The t-th order BIF criterion of the approximate k-fold
cross-validation error for LSSVM is defined as
BIFkt =

k
1∑ ∑ (
ℓ yj , fλ,K,Pn (xj ) + bλ,K,Pn +
n i=1
xj ∈Si

Similar with the least squares loss, it is easy to verify that
[
]
2λIn + n1 [K • B] [K • B]1
Ln :=
1 T
vT 1
nv

)s
t (
)
∑
−1
1
[BIF M LSSV Ms ]i,j .
k−1
s!
s=1

For quadratic ϵ-SVR:
is the finite sample version of the operator L at sample Pn , where B denote the matrix containing V ′′ (yi −
fPn (xi ) − bPn ) at every entry in the i-th column, and
v = (v1 , . . . , vn )T , vi = V ′′ (yi − fPn (xi ) − bPn ).
From Theorem 1, we have
[ ∂
]
[
∂ϵ fPϵ,∆Si |ϵ=0
−1
=
L
∂
n
∂ϵ bPϵ,∆S |ϵ=0
i

1
MK

• Si u − λfPn
1 T
M uSi 1

ϵ-BIFkt =

xj ∈Si

t (
∑

]

where u = (u1 , . . . , un ), ui = V ′ (yi − fPn (xi ) −
bPn ), uSi = (uSi ,1 , . . . , uSi ,n ), uSi ,j = uj if xj ∈
Si , otherwise 0. By Theorem 2, the higher order terms can
be computed,
[ ∂
]
f
|
∂ k+1 ϵ Pϵ,∆Si ϵ=0
=
∂
b
|
∂ k+1 ϵ Pϵ,∆Si ϵ=0
]
[ 1
1
[K • B]bk − M
K • B • Si bk
−1
n
.
(k + 1)Sn
1 T
1 T
n v bk − M vSi bk

k
1∑ ∑ (
ℓ yj , fλ,K,Pn (xj ) + bλ,K,Pn +
n i=1

s=1

−1
k−1

)s

)
1
[BIF M SV Rs ]i,j .
s!

5.1. Time Complexity Analysis
To compute BIFkt and ϵ-BIFkt , we need O(n3 ) to calculate the inversion of Ln 1 , and O(kn2 + tn2 ) to calculate
the BIF matrices, where n is size of the training set, k is
the fold of cross-validation and t is the order of the Taylor
expansion. Thus, the overall time complexity of BIFkt and
ϵ-BIFkt are both O(n3 + kn2 + tn2 ).
For the traditional k-fold cross-validation method, the algorithm under consideration need to be executed k times,
thus for LSSVM and quadratic ϵ-SVR the time complexity
are both O(kn3 ).

where v = (v1 , . . . , vn )T , vi = V ′′ (yi − fPn (xi ) − bPn ),
vSi ,j = vj if x ∈ Si , otherwise 0.

6. Experiments

For the k-fold cross-validation, let [BIF M SV Rt ] be the
k × n matrix with

In this section, we will empirically analyze the performance of our proposed approximate k-fold cross-validation
criterion (BIF-kCV).

[BIF M SV Rt ]i,j = BIFt (∆Si ; fλ,K + bλ,K , Pn )(xj ).

1

If Ln is not invertible, we can use the pseudo-inverse of Ln .

Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function

Table 1. The average testing errors (%) on the classification data sets and the testing mean square error (MSE) on regression

data sets, the order of Taylor expansion t = 3.
Classification

EP

ELOO

5CV

BIF-5CV

10CV

BIF-10CV

20CV

BIF-20CV

ionosphere
breast
diabetes
fourclass
australian
heart
german
liver
sonar
a2a

14.74 ± 3.97
3.58 ± 0.38
24.22 ± 1.67
22.87 ± 0.98
13.51 ± 1.38
18.96 ± 3.08
25.84 ± 2.84
39.42 ± 4.06
17.12 ± 2.39
20.38 ± 1.68

6.65 ± 1.47
3.07 ± 0.59
23.83 ± 1.69
19.49 ± 2.03
14.29 ± 1.81
19.70 ± 4.19
26.38 ± 2.31
31.39 ± 3.71
16.15 ± 3.65
18.90 ± 1.01

7.16 ± 1.54
3.45 ± 0.81
22.24 ± 2.47
18.19 ± 3.32
15.19 ± 2.18
16.56 ± 3.35
25.52 ± 1.45
29.71 ± 1.86
16.92 ± 4.49
18.98 ± 0.95

8.18 ± 1.54
3.45 ± 0.81
22.24 ± 2.47
18.19 ± 3.32
15.19 ± 2.18
17.41 ± 1.69
25.52 ± 1.45
29.71 ± 1.86
17.88 ± 2.08
18.98 ± 0.95

7.16 ± 2.07
3.45 ± 0.81
22.66 ± 2.23
18.19 ± 3.32
14.09 ± 1.96
16.15 ± 3.33
25.28 ± 1.38
29.25 ± 2.73
17.12 ± 4.58
19.10 ± 0.96

7.61 ± 1.69
3.45 ± 0.81
22.66 ± 2.23
18.19 ± 3.32
14.09 ± 1.96
17.85 ± 2.25
25.28 ± 1.38
31.21 ± 1.29
18.62 ± 2.45
19.10 ± 0.96

8.18 ± 1.54
3.45 ± 0.81
22.50 ± 2.18
17.12 ± 2.28
14.49 ± 2.35
16.15 ± 2.98
25.28 ± 1.38
31.10 ± 3.43
16.92 ± 4.69
19.10 ± 1.05

8.07 ± 1.68
3.45 ± 0.81
22.50 ± 2.18
17.12 ± 2.28
14.49 ± 2.35
17.59 ± 3.07
25.28 ± 1.38
31.10 ± 3.43
17.32 ± 2.45
19.10 ± 1.05

Regression

EP

ELOO

5CV

BIF-5CV

10CV

BIF-10CV

20CV

BIF-20CV

bodyfat
housing
mpg
pyrim
triazines
eunite
space-ga
cpusmall
mg
abalone

5.1e-5± 3.1e-5
31.3 ± 6.4
12.4 ± 2.2
1.2e-2 ± 4.0e-3
2.0e-2 ± 2.9e-3
700.4 ± 118.4
2.7e-2 ± 3.9e-3
42.0 ± 13.1
1.6e-2 ± 3.3e-4
6.4 ± 0.5

3.9e-5 ± 9.9e-6
24.3 ± 3.4
9.6 ± 1.5
1.4e-2 ± 4.2e-3
2.2e-2 ± 3.3e-3
625.8 ± 62.1
1.9e-2 ± 2.0e-3
44.5 ± 4.4
1.5e-2 ± 7.6e-4
5.7 ± 0.5

4.5e-5 ± 1.4e-5
23.9 ± 3.8
8.7 ± 0.8
1.0e-2 ± 2.9e-3
2.3e-2 ± 3.6e-3
593.1 ± 95.0
1.9e-2 ± 2.0e-3
42.9 ± 5.9
1.5e-2 ± 9.7e-4
5.5 ± 0.3

4.5e-5 ± 1.4e-5
23.9 ± 3.8
8.7 ± 0.8
1.1e-2 ± 2.4e-3
2.3e-2 ± 4.4e-3
592.5 ± 95.0
1.9e-2 ± 2.0e-3
42.9 ± 5.9
1.5e-2 ± 9.7e-4
5.5 ± 0.3

4.5e-5 ± 1.4e-5
23.98 ± 3.8
8.7 ± 0.8
1.0e-2 ± 2.9e-3
2.2e-2 ± 3.2e-3
596.9 ± 95.8
1.9e-2 ± 2.0e-3
42.9 ± 5.9
1.5e-2 ± 9.7e-4
5.5 ± 0.3

4.5e-5 ± 1.4e-5
23.9 ± 3.8
8.6 ± 0.8
1.1e-2 ± 2.4e-3
2.2e-2 ± 3.7e-3
594.6 ± 96.3
1.9e-2 ± 2.0e-3
42.91 ± 5.9
1.5e-2 ± 9.7e-4
5.5 ± 0.3

4.5e-5 ± 1.4e-5
23.9 ± 3.8
8.6 ± 0.8
1.0e-2 ± 2.9e-3
2.3e-2 ± 3.1e-3
596.9 ± 95.8
1.9e-2 ± 2.0e-3
42.9 ± 5.9
1.5e-2 ± 9.7e-4
5.5 ± 0.3

4.5e-5 ± 1.4e-5
23.9 ± 3.8
8.6 ± 0.8
1.1e-2 ± 2.1e-3
2.3e-2 ± 4.4e-3
594.6 ± 96.2
1.9e-2 ± 2.0e-3
42.9 ± 5.9
1.5e-2 ± 9.7e-4
5.5 ± 0.3

2
0

2
1
0

2
4
6
Taylor expansion order: t

3
2
1
0

2
4
6
Taylor expansion order: t

(a) heart

10

5

0

(c) breast-cancer

0.02
0.01
0
2
4
6
Taylor expansion order: t

(f) housing

5−fold
10−fold
20−fold

10
5
0

2
4
6
Taylor expansion order: t

(g) mpg

5−fold
10−fold
20−fold

1

0.5

0
2
4
6
Taylor expansion order: t

(h) eunite2001

MSE of cv and bifcv

0.03

4
2
0

2
4
6
Taylor expansion order: t

2
4
6
Taylor expansion order: t

(d) diabetes

(e) fourclass

x 10

x 10

MSE of cv and bifcv

0.04

15
MSE of cv and bifcv

MSE of cv and bifcv

5−fold
10−fold
20−fold

5−fold
10−fold
20−fold

6

−8

−3

0.05

5−fold
10−fold
20−fold

2
4
6
Taylor expansion order: t

(b) ionosphere

−6

x 10

−6

x 10

5−fold
10−fold
20−fold

−7

x 10

5−fold
10−fold
20−fold

6
4
2
0

2
4
6
Taylor expansion order: t

(i) space ga

MSE of cv and bifcv

4

5−fold
10−fold
20−fold

MSE of cv and bifcv

6

MSE of cv and bifcv

MSE of cv and bifcv

−4

x 10

3

MSE of cv and bifcv

−3

x 10
5−fold
10−fold
20−fold

MSE of cv and bifcv

−5

x 10

5−fold
10−fold
20−fold

10

5

0
2
4
6
Taylor expansion order: t

(j) mg

Figure 1. The mean square discrepancies between 5CV and BIF-5CV, 10CV and BIF-10CV, 20CV and BIF-20CV with different t, where
t is the order of Taylor expansion.

The evaluation is made on 20 publicly available data sets
from LIBSVM Data: 10 data sets for classification and 10
data sets for regression seen in Table 1. Experiments are
performed on a Dell Vestro PC with 3.4-GHz 8-core CPU
and 8-GB memory.
We use K(x, x′ ) = exp(−∥x − x′ ∥22 /2τ ) as our candidate
kernels, τ ∈ {2i , i = −6, −5, . . . , 7, 8} 2 . The regularization parameter λ ∈ {2i , i = −7, −6, . . . , 2}. The learning
algorithm considered in our experiments is LSSVM. For
each data set, we have run all the methods 10 times with
Note for LSSVM, when τ is too small (e.g. 2−6 ), our approximation would probably fail due to (near) identity kernel matrix.
But we can easily exclude such small τ (which is unlikely to be
an optimal parameter) by setting our approximate criterion to ∞
if the kernel matrix is near identity.
2

training and testing data sets be split randomly (50% of all
the examples for training and the other 50% for testing).
Accuracy. We will compare our proposed BIF-kCV with
the traditional k-fold cross-validation (kCV), the efficient leave-one-out cross-validation (ELOO) (Cawley, 2006;
Cawley & Talbot, 2007) and the latest proposed eigenvalues perturbation criterion (EP) (Liu et al., 2013).
In our first experiment, we set the order of Taylor expansion t = 3. The average testing errors for classification
and testing mean square error for regression are reported in
Table 1. For each training set, we choose the kernel parameter τ and regularization parameter λ by each criterion on
the training set, and evaluate the testing error for the chosen
parameters on the testing set.

Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function

Table 2. The average computational time (second), the order of Taylor expansion t = 3
Classification

EP

ELOO

5CV

BIF-5CV

10CV

BIF-10CV

20CV

BIF-20CV

ionosphere
breast
diabetes
fourclass
australian
heart
german
liver
sonar
a2a

0.91 ± 0.01
2.99 ± 0.05
3.57 ± 0.04
4.23 ± 0.02
2.82 ± 0.17
0.58 ± 0.01
7.02 ± 0.06
1.04 ± 0.04
0.54 ± 0.01
58.44 ± 0.21

0.43 ± 0.02
1.42 ± 0.11
2.10 ± 0.09
2.50 ± 0.08
1.45 ± 0.23
0.30 ± 0.01
3.85 ± 0.13
0.42 ± 0.02
0.25 ± 0.01
31.87 ± 0.20

0.87 ± 0.02
2.88 ± 0.06
3.30 ± 0.04
4.40 ± 0.17
2.70 ± 0.09
0.58 ± 0.01
6.78 ± 0.12
0.81 ± 0.01
0.48 ± 0.01
52.66 ± 0.15

0.47 ± 0.01
1.70 ± 0.04
2.57 ± 0.03
3.31 ± 0.09
1.71 ± 0.03
0.32 ± 0.02
4.65 ± 0.10
0.46 ± 0.01
0.23 ± 0.00
37.50 ± 0.39

2.02 ± 0.03
6.83 ± 0.15
8.17 ± 0.21
11.64 ± 0.25
6.81 ± 0.04
1.31 ± 0.02
16.89 ± 0.18
1.96 ± 0.02
1.05 ± 0.02
142.92 ± 0.70

0.66 ± 0.01
2.20 ± 0.05
3.46 ± 0.03
4.39 ± 0.18
2.19 ± 0.01
0.45 ± 0.01
6.10 ± 0.11
0.62 ± 0.02
0.36 ± 0.02
46.57 ± 0.27

4.60 ± 0.03
14.03 ± 0.23
21.63 ± 0.11
26.05 ± 0.48
13.76 ± 0.04
2.79 ± 0.03
38.88 ± 0.34
4.02 ± 0.08
2.24 ± 0.03
308.06 ± 0.96

1.02 ± 0.01
3.21 ± 0.08
5.15 ± 0.04
6.52 ± 0.27
3.17 ± 0.08
0.73 ± 0.02
8.99 ± 0.07
0.97 ± 0.02
0.57 ± 0.01
64.85 ± 0.38

Regression

EP

ELOO

5CV

BIF-5CV

10CV

BIF-10CV

20CV

BIF-20CV

bodyfat
housing
mpg
pyrim
triazines
eunite
space-ga
cpusmall
mg
abalone

0.76 ± 0.01
1.78 ± 0.01
1.01 ± 0.02
0.23 ± 0.01
0.39 ± 0.03
0.17 ± 0.07
97.77 ± 0.15
73.65 ± 0.03
16.25± 0.05
275.5± 3.52

0.30 ± 0.04
0.86 ± 0.03
0.52 ± 0.04
0.09 ± 0.01
0.22 ± 0.01
0.42 ± 0.03
64.89 ± 6.29
41.69 ± 0.25
8.49 ± 0.17
152.8 ± 3.45

0.59 ± 0.03
1.60 ± 0.04
0.99 ± 0.01
0.20 ± 0.01
0.46 ± 0.01
0.83 ± 0.07
93.65 ± 0.45
68.49 ± 2.48
13.36 ± 0.47
253.2 ± 2.66

0.32 ± 0.02
0.91 ± 0.02
0.57 ± 0.01
0.09 ± 0.01
0.21 ± 0.00
0.43 ± 0.02
69.84 ± 0.62
48.38 ± 1.28
8.99 ± 0.07
168.7 ± 1.92

1.28 ± 0.06
3.63 ± 0.12
2.31 ± 0.00
0.40 ± 0.01
0.94 ± 0.02
1.75 ± 0.08
252.3 ± 0.77
172.2 ± 5.66
37.17 ± 0.46
730.4 ± 3.62

0.45 ± 0.02
1.22 ± 0.02
0.77 ± 0.01
0.15 ± 0.01
0.30 ± 0.00
0.61 ± 0.02
85.49 ± 0.28
60.21 ± 0.82
13.00 ± 0.04
196.7 ± 1.56

2.61 ± 0.16
8.38 ± 0.20
4.94 ± 0.02
0.78 ± 0.01
2.00 ± 0.03
3.83 ± 0.20
600.1 ± 0.42
395.8 ± 11.9
81.72 ± 0.73
1760.9 ± 8.05

0.72 ± 0.04
1.85 ± 0.01
1.19 ± 0.01
0.24 ± 0.01
0.50 ± 0.01
0.94 ± 0.04
117.8 ± 0.2
85.01 ± 1.33
19.15 ± 0.02
255.1 ± 3.41

The results in Table 1 can be summarized as follows: (a)
On most data sets, BIF-kCV gives almost the same testing
errors as the traditional kCV, k = 5, 10, 20. On breast, diabetes, australian, fourclass, german, a2a, bodyfat, housing,
eunite, space-ga, mg and abalone, BIF-kCV gives the same
testing errors as kCV. On the remaining data sets, both BIFkCV and kCV give the similar results. Thus, it implicates
that the quality of our approximation based on the Bouligand influence function is quite good. (b) BIF-kCV gives
much better results than EP on most data sets. In particular,
BIF-CV outperforms EP on 16 out of 20 data sets, and also
give results close to results of EP on the remaining 4 sets.
(c) For classification, BIF-kCV and ELOO give comparable results. However, for regression, BIF-kCV outperforms
ELOO on 8 out of 10 data sets.
In the second experiment, we will explore the effect of the
parameter t (the order of Taylor expansion). The discrepancies between kCV and BIF-kCV with different k are given
in Figure 1 (due to space limit, we randomly select 5 classification data sets and 5 regression data sets). For each
training set, we choose the τ and λ by cross validation
on the training set. Plotted are the mean square error of
the approximate fP −Si (x)’s (computed by BIF-kCV) and
n
fP −Si (x)’s (computed by kCV). for the chosen parameters
n
on the validation sample Si , x ∈ Si , i = 1, . . . , k. We can
find that, on most data sets, the discrepancies between kCV
and BIF-kCV is equal 0 when t ≥ 3. Thus, we can select
t = 3 in practice without sacrificing accuracy.
Efficiency. The running time are reported in Table 2. The
results in Table 2 can be summarized as follows: (a) The
time cost of BIF-kCV is much lower than that of kCV.
Thus, BIF-kCV significantly improves the efficiency of
kCV. (b) BIF-5CV and BIF-10CV are faster than EP, BIF20CV and EP are comparable in computing time. (c) BIF5CV and ELOO give the similar results.

7. Conclusion
We propose a novel strategy for approximating the k-fold
cross-validation error based on the Bouligand influence
function (BIF), which can be computed efficiently. Link
between the concept of BIF and concept of cross-validation
is considered. The calculation of the higher order BIFs and
a recursive relation are proposed. It is shown that these theoretical results can be applied in practice to approximate
the cross-validation error. Experiments indicate that our
proposed criterion based on BIF is a good choice for model
selection.
Future work will extend our method to other kernel based
methods, such as kernel-based logistic regression and
SVM.

Acknowledgments
The work is supported in part by the National Natural Science Foundation of China under grant No. 61170019, the
Natural Science Foundation of Tianjin under grant No. 11JCYBJC00700, and Tianjin Key Laboratory of Cognitive
Computing and Application.
Appendix A: Proof of Theorem 1
Proof. From Theorem 2 in (Vito et al., 2004), we have
2λfP = EP [VP′ Φ(x)], 0 = EP VP′ .

(5)

Let fϵ = fPϵ,Q and bϵ = bPϵ,Q . Note that Pϵ,Q = (1 −
ϵ)P + ϵQ, thus we can obtain that
2λfϵ = (1 − ϵ)EP [Vϵ′ Φ(x)] + ϵEQ [Vϵ′ Φ(x)]
0 = (1 − ϵ)EP Vϵ′ + ϵEQ Vϵ′ ,
where Vϵ = V (y − fϵ (x) − bϵ ).

(6)
(7)

Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function

Taking the first derivative on both sides of (6) with respect
to ϵ yields

Taking the derivatives of both sides in (12),
2λ

2λ

∂
fϵ =
∂ϵ

∂
∂
fϵ (x) + bϵ )Vϵ′′ Φ(x)] − EP (Vϵ′ Φ(x))
∂ϵ
∂ϵ
∂
∂
+ ϵEQ [−( fϵ (x) + bϵ )Vϵ′′ Φ(x)] + EQ (Vϵ′ Φ(x)).
∂ϵ
∂ϵ
(8)

= (1 − ϵ)EP [−(

(1 − ϵ)EP [−(

∂
∂
∂
fϵ |ϵ=0 + EP [( fϵ (x)|ϵ=0 + bϵ |ϵ=0 )VP′′ Φ(x)]
∂ϵ
∂ϵ
∂ϵ
= −2λfP + EQ (VP′ Φ(x)).
(9)

Taking the first derivative on both sides of (7) with respect
to ϵ yields
∂
∂
fϵ (x) − bϵ )Vϵ′′ ] − EP Vϵ′
∂ϵ
∂ϵ
(10)
∂
∂
+ ϵEQ [(− fϵ (x) − bϵ )Vϵ′′ ] + EQ Vϵ′ .
∂ϵ
∂ϵ

0 = (1 − ϵ)EP [(−

∂
∂
fϵ (x)|ϵ=0 + bϵ |ϵ=0 )VP′′ ] = EQ VP′ .
∂ϵ
∂ϵ

+

∂
bϵ )Vϵ′′ Φ(x)]
k+1
∂
ϵ

∂
∂
fϵ (x) + k bϵ )Vϵ′′ Φ(x)]
k
∂ ϵ
∂ ϵ
∂
∂
− (k + 1)EQ ( k fϵ (x) + k bϵ )Vϵ′′ Φ(x)
∂ ϵ
∂ ϵ
∂
∂
− ϵEQ ( k+1 fϵ (X) + k+1 bϵ )Vϵ′′ Φ(x)
∂
ϵ
∂
ϵ
from which it follows that (12) holds for k + 1 indeed. Set
ϵ = 0:
∂
fϵ |ϵ=0 +
∂ k+1 ϵ
∂
∂
EP [( k+1 fϵ (x)|ϵ=0 + k+1 bϵ |ϵ=0 )VP′′ Φ(x)] =
∂
ϵ
∂
ϵ
∂
∂
(k + 1)EP [( k fϵ (x)|ϵ=0 + k bϵ |ϵ=0 )VP′′ Φ(x)]−
∂ ϵ
∂ ϵ
∂
∂
(k + 1)EQ ( k fϵ (x)|ϵ=0 + k bϵ |ϵ=0 )VP′′ Φ(x).
∂ ϵ
∂ ϵ

2λ

Taking the derivative on both sides of (10) and setting ϵ =
0, we have

Set ϵ = 0 and according to (5),
EP [(

∂

fϵ (x)
∂ k+1 ϵ

+ (k + 1)EP [(

Set ϵ = 0 and according to (5), we have
2λ

∂

fϵ
∂ k+1 ϵ

∂
∂
fϵ (x)|ϵ=0 − 2 bϵ |ϵ=0 )VP′′ ] =
2
∂ ϵ
∂ ϵ
∂
∂
EP ( fϵ (x)|ϵ=0 + bϵ |ϵ )2 VP′′′ +
∂ϵ
∂ϵ
∂
∂
2EP [( fϵ (x)|ϵ=0 + bϵ |ϵ=0 )VP′′ ]−
∂ϵ
∂ϵ
∂
∂
2EQ [( fϵ (x)|ϵ=0 + bϵ |ϵ=0 )Vϵ′′ ].
∂ϵ
∂ϵ

EP [−(
(11)

By the definition of the operator L, the system
of linear [equations, (9) and
] (11), [can be writ∂
∂
ten as L ∂ϵ fϵ |ϵ=0 , ∂ϵ bϵ |ϵ=0
=
− 2λfP +
]
′
′
EQ (VP Φ(x)), EQ (VP ) .
Appendix B: Proof of the Theorem 2
Proof. First we prove the following for all 2 ≤ k ∈ N:
∂
∂
∂
2λ k fϵ =(1 − ϵ)EP [−( k fϵ (x) + k bϵ )Vϵ′′ Φ(x)]+
∂ ϵ
∂ ϵ
∂ ϵ
∂
∂
kEP [( k−1 fϵ (x) + k−1 bϵ )Vϵ′′ Φ(x)]−
∂
ϵ
∂
ϵ
∂
∂
kEQ [( k−1 fϵ (x) + k−1 bϵ )Vϵ′′ Φ(x)]−
∂
ϵ
∂
ϵ
∂
∂
ϵEQ [( k fϵ (x) + k bϵ )Vϵ′′ Φ(x)].
∂ ϵ
∂ ϵ
(12)
Taking the derivative on both sides of (8) with respect to ϵ yields 2λ ∂∂2 ϵ fϵ = (1 − ϵ)EP [−( ∂∂2 ϵ fϵ (x) +
[ ∂
]
∂
∂
′′
′′
+
∂ 2 ϵ bϵ )Vϵ Φ(x)] + 2EP ( ∂ϵ fϵ (x) + ∂ϵ bϵ )Vϵ Φ(x)
∂
ϵEQ [−( ∂∂2 ϵ fϵ (x) + ∂∂2 ϵ bϵ )Vϵ′′ Φ(x)] + 2EQ Vϵ′′ ( ∂ϵ
fϵ (x) +
∂
∂ϵ bϵ )Φ(x). Thus for k = 2, the Equation (12) is satisfied.

(13)

Similar to the above proof, it is easy to verify that
EP [(

∂

fϵ (X)|ϵ=0
∂ k+1 ϵ

+

∂
bϵ |ϵ=0 )VP′′ ]
k+1
∂
ϵ

=

∂
∂
fϵ (X)|ϵ=0 + k bϵ |ϵ=0 )VP′′ ]−
k
∂ ϵ
∂ ϵ
∂
∂
(k + 1)EQ ( k fϵ (X)|ϵ=0 + k bϵ |ϵ=0 )VP′′ .
∂ ϵ
∂ ϵ

(k + 1)EP [(

Thus, we have
[
]
∂
∂
L k+1 fϵ |ϵ=0 , k+1 bϵ |ϵ=0
∂
ϵ
∂
ϵ
[
= (k + 1) EP (BIFk (Q; (fλ,K ), P ))VP′′ (Φ(x))
− EQ (BIFk (Q; (fλ,K ), P )VP′′ )Φ(x),
+ EP (BIFk (Q; (fλ,K ), P ))VP′′
]
− EQ (BIFk (Q; (fλ,K ), P )VP′′ ) .

Efficient Approximation of Cross-Validation for Kernel Methods using Bouligand Influence Function

References
Aronszajn, Nachman. Theory of reproducing kernels.
Transactions of the American Mathematical Society, 68:
337–404, 1950.
Bartlett, Peter L. and Mendelson, Shahar. Rademacher and
gaussian complexities: Risk bounds and structural results. The Journal of Machine Learning Research, 3:
463–482, 2002.
Bartlett, Peter L., Boucheron, Stéphane, and Lugosi,
Gábor. Model selection and error estimation. Machine
Learning, 48:85–113, 2002.
Cawley, Gavin C. Leave-one-out cross-validation based
model selection criteria for weighted ls-svms. In Proceeding of the International Joint Conference on Neural
Networks (IJCNN 2006), pp. 1661–1668, 2006.
Cawley, Gavin C. and Talbot, Nicola L. C. Preventing overfitting during model selection via bayesian regularisation
of the hyper-parameters. Journal of Machine Learning
Research, 8:841–861, 2007.
Chapelle, Olivier, Vapnik, Vladimir, Bousquet, Olivier, and
Mukherjee, Sayan. Choosing multiple parameters for
support vector machines. Machine Learning, 46(1-3):
131–159, 2002.
Christmann, Andreas and Messem, Arnout Van. Bouligand
derivatives and robustness of support vector machines
for regression. Journal of Machine Learning Research,
9:915–936, 2008.
Christmann, Andreas and Steinwart, Ingo. On robustness
properties of convex risk minimization methods for pattern recognition. Journal of Machine Learning Research,
5:1007–1034, 2004.
Christmann, Andreas and Steinwart, Ingo. Consistency and
robustness of kernel based regression. Bernoulli, 13:
799–819, 2007.
Christmann, Andreas, Messem, Arnout Van, and Steinwart,
Ingo. On consistency and robustness properties of support vector machines for heavy-tailed distributions. Statistics and Its Interface, 2:311–327, 2009.
Debruyne, Michiel, Hubert, Mia, and Suykens, Johan A.K.
Model selection in kernel based regression using the
influence function. Journal of Machine Learning Research, 9:2377–2400, 2008.
Golub, Gene H., Heath, Michael, and Wahba, Grace. Generalized cross-validation as a method for choosing a
good ridge parameter. Technometrics, 21(2):215–223,
1979.

Hampel, Frank R, Ronchetti, Elvezio M, Rousseeuw, Peter J, and Stahel, Werner A. Robust Statistics: The Approach Based on Influence Functions. Wiley, New York,
1986.
Liu, Yong, Jiang, Shali, and Liao, Shizhong. Eigenvalues perturbation of integral operator for kernel selection.
In Proceedings of the 22th ACM International Conference on Information and Knowledge Management (CIKM 2013), 2013.
Luxburg, Ulrike Von, Bousquet, Olivier, and Schölkopf,
Bernhard. A compression approach to Support Vector
model selection. Journal of Machine Learning Research,
5:293–323, 2004.
Messem, Arnout Van and Christmann, Andreas. A review
on consistency and robustness properties of support vector machines for heavy-tailed distributions. Advances in
Data Analysis and Classification, 4(2-3):199–220, 2010.
Robinson, Stephen M. An implicit-function theorem for a
class of nonsmooth functions. Mathematics of Operations Research, 16:292–309, 1991.
Schölkopf, Bernhard and Smola, Alexander J. Learning
with kernels. MIT Press, Cambridge, MA, 2002.
Shawe-Taylor, John and Cristianini, Nello. An introduction to support Vector Machines and other kernel-based
learning methods. Cambridge University Press, 2000.
ISBN 0521780195.
Steinwart, Ingo and Christmann, Andreas. Support vector
machines. Springer Verlag, New York, 2008.
Suykens, Johan A. K. and Vandewalle, Joos. Least squares
Support Vector Machine classifiers. Neural Processing
Letters, 9(3):293–300, 1999.
Vapnik, Vladimir. The nature of statistical learning theory.
Springer Verlag, 2000.
Vito, Ernesto De, Rosasco, Lorenzo, Caponnetto, Andrea,
Piana, Michele, and Verri, Alessandro. Some properties of regularized kernel methods. Journal of Machine
Learning Research, 5:1363–1390, 2004.
Wahba, Grace. Spline Models for Observational Data. CBMS-NSF Regional Conference Series in Applied
Mathematics, SIAM, 1990.
Wahba, Grace, Lin, Yi, and Zhang, Hao. GACV for support
vector machines. In Advances in Large Margin Classifiers. MIT Press, Cambridge,, 1999.

