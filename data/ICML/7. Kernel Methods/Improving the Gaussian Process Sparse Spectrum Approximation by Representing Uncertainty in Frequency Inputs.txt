Improving the Gaussian Process Sparse Spectrum Approximation by
Representing Uncertainty in Frequency Inputs
Yarin Gal
Richard Turner
University of Cambridge

Abstract
Standard sparse pseudo-input approximations to
the Gaussian process (GP) cannot handle complex functions well. Sparse spectrum alternatives
attempt to answer this but are known to over-fit.
We suggest the use of variational inference for
the sparse spectrum approximation to avoid both
issues. We model the covariance function with
a finite Fourier series approximation and treat it
as a random variable. The random covariance
function has a posterior, on which a variational
distribution is placed. The variational distribution transforms the random covariance function
to fit the data. We study the properties of our
approximate inference, compare it to alternative
ones, and extend it to the distributed and stochastic domains. Our approximation captures complex functions better than standard approaches
and avoids over-fitting.

1 Introduction
The Gaussian process (GP, Rasmussen & Williams, 2006)
is a powerful tool for modelling distributions over nonlinear functions. It offers robustness to over-fitting, a
principled way to tune hyper-parameters, and uncertainty
bounds over the outputs. These properties are critical for
tasks including non-linear function regression, reinforcement learning, density estimation, and more (Brochu et al.,
2010; Rasmussen et al., 2003; Engel et al., 2005; Titsias &
Lawrence, 2010). But the advantages of the Gaussian process come with a great computational cost. Evaluating the
GP posterior involves a large matrix inversion – for N data
points the model requires O(N 3 ) time complexity.
Many approximations to the GP have been proposed to reduce the model’s time complexity. Quiñonero-Candela &
Rasmussen (2005) survey approaches relying on “sparse
pseudo-input” approximations. In these, a small number of
points in the input space with corresponding outputs (“inducing inputs and outputs”) are used to define a new GausProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

YG 279@ CAM . AC . UK
RET 26@ CAM . AC . UK

sian process. The new GP is desired to be as close as possible to the GP defined on the entire dataset, and the matrix
inversion is now done with respect to the inducing points
alone. These approaches are suitable for locally complex
functions. The approximate model would place most of the
inducing points in regions where the function is complex,
and only a small number of points would be placed in regions where the function is not. Highly complex functions
cannot be modelled well with this approach.
Lázaro-Gredilla et al. (2010) suggested an alternative approximation to the GP model. In their paper they suggest the decomposition of the GP’s stationary covariance
function into its Fourier series. The infinite series is then
approximated with a finite one. They optimise over the
frequencies of the series to minimise some divergence
from the full Gaussian process. This approach was named
a “sparse spectrum” approximation. This approach is
closely related to the one suggested by Rahimi & Recht
(2007) in the randomised methods community (random
projections). In Rahimi & Recht (2007)’s approach, the
frequencies are randomised (sampled from some distribution rather than optimised) and the Fourier coefficients are
computed analytically. Both approaches capture globally
complex behaviour, but the direct optimisation of the different quantities often leads to some form of over-fitting (as
reported in (Wilson et al., 2014) for the SSGP and shown
below for random projections). Similar over-fitting problems observed with the sparse pseudo-input approximation
were answered with variational inference (Titsias, 2009).
We suggest the use of variational inference for the sparse
spectrum approximation. This allows us to avoid overfitting while efficiently capturing globally complex behaviour. We replace the stationary covariance function with
a finite approximation obtained from Monte Carlo integration. This finite approximation is a random variable,
and conditioned on a dataset this random variable has an
intractable posterior. We approximate this posterior with
variational inference, resulting in a non-stationary finite
rank covariance function. The approximating variational
distribution transforms the covariance function to fit the
data well. The prior from the GP model keeps the approximating distribution from over-fitting to the data.

Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs

Like in (Lázaro-Gredilla et al., 2010), we can marginalise
over the Fourier coefficients. This results in approximate
inference with O(N K 2 + K 3 ) time complexity with N
data points and K inducing frequencies (components in the
Fourier expansion). This is the same as that of the sparse
pseudo-input and sparse spectrum approximations. We can
further optimise a variational distribution over the frequencies reducing the time complexity to O(N K 2 ). This factorises the lower bound and allows us to perform distributed
inference, resulting in O(K) time complexity given a sufficient number of nodes in a distributed framework. We can
approximate the latter lower bound and use random subsets
of the data (mini batches) employing stochastic variational
inference (Hoffman et al., 2013). This results in O(SK 2 )
time complexity with S << N the size of the mini-batch1 .
In the experiments section we demonstrate the properties of
our GP approximation and compare it to alternative approximations. We describe qualitative properties of the approximation and discuss how the approximation can be used
to learn the covariance function by fitting to the data. We
compare the approximation to the full Gaussian process,
sparse spectrum GP, sparse pseudo-input GP, and random
projections. We show that alternative approximations either over-fit or under-fit even on simple datasets. We empirically demonstrate the advantages of the variational inference in avoiding over-fitting by comparing the approximation to the sparse spectrum one on audio data from the
TIMIT dataset. We compare the stochastic optimisation to
the non-stochastic one, and compare the performance to the
sparse pseudo-input SVI. Finally, we inspect the model’s
time accuracy trade-off and show that it avoids over-fitting
as the number of parameters increases.

2 Sparse Spectrum Approximation in Gaussian Process Regression
We use Bochner’s theorem (Bochner, 1959) to reformulate
the covariance function in terms of its frequencies. Since
our covariance function K(x, y) is stationary, it can be
represented as K(x − y) for all x, y ∈ RQ . Following Bochner’s theorem, K(x − y) can be represented as
the Fourier transform of some finite measure σ 2 p(w) with
p(w) a probability density,
Z
T
K(x − y) =
σ 2 p(w)e−2πiw (x−y) dw
Q
ZR
=
σ 2 p(w) cos(2πwT (x − y))dw (1)

using Monte Carlo integration,
K(x − y) ≈

k=1

with wk ∼ p(w) and zk some Q dimensional vectors for
k = 1, ..., K. The points zk act as inducing inputs, and
will have corresponding inducing frequencies in our approximation. For the sparse spectrum GP, these points take
value 0. These will be explained in detail in a later section.
Using identity 1 proved in the appendix we rewrite the
terms above for every k as

cos 2πwkT (x − zk ) − (y − zk )
Z 2π

1 √
=
2 cos 2πwkT (x − zk ) + b
2π
0
√

· 2 cos 2πwkT (y − zk ) + b db.
This integral can again be approximated as a finite sum using Monte Carlo integration. To keep the notation simple,
we approximate the integral with a single sample2 for every
k,
K
σ2 X √
K(x − y) ≈
2 cos(2πwkT (x − zk ) + bk )
K
k=1
√
· 2 cos(2πwkT (y − zk ) + bk )
b − y)
=: K(x
with bk ∼ Unif[0, 2π], defining the approximate covariance
b We refer to (wk )K as inducing frequencies
function K.
k=1
K
and to (bk )K
k=1 as phases, and denote ω = (wk , bk )k=1 .
Note that this integral could be approximated with any arbitrary number of samples instead.
We denote X ∈ RN ×Q the inputs and Y ∈ RN ×D the outputs of a real-valued dataset with N data points. In Gaussian process regression we find the probability P (Y|X)
with the assumption that the function generating Y is
drawn from a Gaussian process. The full GP model is defined as (assuming stationary covariance function K(·, ·)):
F | X ∼ N (0, K(X, X))
Y | F ∼ N (F, τ −1 IN )
with some precision hyper-parameter τ .
b instead as the covariance function of the Gaussian
Using K
process yields the following generative model:
wk ∼ p(w), bk ∼ Unif[0, 2π],

RQ

ω = (wk , bk )K
k=1

since the covariance function is real-valued.
This can be approximated as a finite sum with K terms
1
Python code for all inference algorithms is available at
http://github.com/yaringal/VSSGP

K

σ2 X
cos 2πwkT (x − zk ) − (y − zk )
K

2

The above transformation and approximate integration are
used in the randomised methods literature (“Random projections”, Rahimi & Recht, 2007). It was shown to give better approximation than Monte Carlo integration of eq. 1. Intuitively it
is equivalent to a random phase shift for each basis function.

Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs
K

σ2 X √
b
K(x,
y) =
2 cos 2πwkT (x − zk ) + bk
K
k=1
√

· 2 cos 2πwkT (y − zk ) + bk
b
F | X, ω ∼ N (0, K(X,
X))

We refer to A ∈ RK×D as the Fourier coefficients.
Regarding ω as parameters and optimising these values (integrating over A) results in the sparse spectrum approximation (Lázaro-Gredilla et al., 2010). Regarding A as
parameters and optimising these values (leaving ω constant) results in a method known as “random projections”
(Rahimi & Recht, 2007). Related work to random projections variationally integrates over the hyper-parameters
while leaving ω constant (Tan et al., 2013).

Y | F ∼ N (F, τ −1 IN ).

3 Random Covariance Functions
b is
K is a deterministic covariance function of its inputs; K
a random finite rank covariance function. As such, we can
find the conditional distribution of the covariance function
given a dataset (more precisely, the conditional distribution
of ω). This is a powerful view of this approximation – it
allows us to transform the covariance function to fit the data
well, while the prior keeps it from over-fitting to the data.
b as our Gaussian process covariance function
We will use K
from now on, replacing K. This results in the following
predictive distribution:
Z
p(Y|X) = p(Y|F)p(F|ω, X)p(ω)dωdF.
We can integrate this analytically for F and obtain
Z
b
p(Y|X) = N (Y; 0, K(X,
X) + τ −1 IN )p(ω)dω

We can extend the above to sums of covariance functions as
well. Following proposition 1 in the appendix, given a sum
of covariance functions with L components (with each corresponding to Φi an N × K matrix) we have Φ = [Φi ]L
i=1
an N × LK matrix. We will continue the development of
our method using weighted sums of automatic relevance
determination (ARD) squared exponential (SE) covariance
functions. Note however that our method is general and can
be extended to other covariance functions as well. A sum
of SE covariance functions with L components is given by


Q
L
X
1 X (xq − yq )2
σi2 exp −
K(x, y) =
2
2 q=1
liq
i=1
with weights σi2 and length-scales liq . We write Li =
diag([2πlqi ]Q
q=1 ).

b
but this involves the inversion of K(X,
X)+τ −1 IN , which
does not allow us to integrate over ω (even variationally!).
Instead, we introduce an auxiliary random variable.
Denoting the 1 × K row vector

r 2
 K
2σ
T
cos 2πwk (x − zk ) + bk
φ(x, ω) =
K
k=1
and the N × K feature matrix
Φ = [φ(xn , ω)]N
n=1 ,
b
we have K(X,
X) = ΦΦT . We rewrite p(Y|X) as
Z
p(Y|X) = N (Y; 0, ΦΦT + τ −1 IN )p(ω)dω.

For p(w) composed of a single SE component, we follow
proposition 2 in the appendix and perform a change of variables, resulting in p(w) a standard normal distribution with
the parameters of p(w) now expressed in Φ instead. For
p(w) composed of several components, for each component i we get Φi is an N × K matrix with elements
r

2σi2
T
cos 2π(L−1
i wk ) (x − zk ) + bk ,
K
where for simplicity, we index wk and bk with k =
1, ..., LK as a function of the component i.

4 Variational Inference

Following an identity proved in (Bishop, 2006, page 93)
we introduce a K × 1 auxiliary random variable ad ∼
N (0, IK ) to the distribution inside the integral above,
N (yd ; 0, ΦΦT + τ −1 IN )
Z
= N (yd ; Φad , τ −1 IN )N (ad ; 0, IK )dad ,

The predictive distribution for an input point x∗ is given by
Z
p(y∗ |x∗ , X, Y) = p(y∗ |x∗ , A, ω)p(A, ω|X, Y)dAdω,
(3)
∗

1×D

where yd is the d’th column of the N × D matrix Y.

with y ∈ R
. The distribution p(A, ω|X, Y) cannot
be evaluated analytically. Instead we define an approximating variational distribution q(A, ω), whose structure is
easy to evaluate.

Writing A = [ad ]D
, the above is equivalent to3
Zd=1
p(Y|X) = p(Y|A, X, ω)p(A)p(ω)dAdω.

We would like our approximating distribution to be as close
as possible to the posterior distribution obtained from the
full GP. We thus minimise the Kullback–Leibler divergence

3

(2)

This is equivalent to the weighted basis function interpretation of the Gaussian process (Rasmussen & Williams, 2006).

KL(q(A, ω) | p(A, ω|X, Y)),

Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs

resulting in the approximate predictive distribution
Z
q(y∗ |x∗ ) = p(y∗ |x∗ , A, ω)q(A, ω)dAdω.

(4)

Minimising the Kullback–Leibler divergence is equivalent
to maximising the log evidence lower bound
Z
L := q(A, ω) log p(Y|A, X, ω)dAdω
− KL(q(A, ω)||p(A)p(ω))

(5)

with respect to the variational parameters defining q(A, ω).
We define a factorised variational distribution q(A, ω) =
q(A)q(ω). We define q(ω) with ω = (wk , bk )K
k=1 to be a
joint Gaussian distribution and a uniform distribution,
wk ∼ N (µk , Σk ),
bk ∼ Unif(αk , βk ),

k = 1, ..., LK
k = 1, ..., LK

with Σk diagonal, 0 ≤ αk ≤ βk ≤ 2π, and define q(A) =
QD
LK×1
) by
d=1 q(ad ) (with ad ∈ R
ad ∼ N (md , sd ),

d = 1, ..., D

with sd diagonal.
We evaluate the log evidence
lower bound and optimise over {µk , Σk , αk , βk }LK
k=1 ,
L
{md , sd }D
,
and
{σ
,
L
}
to
maximise
Eq.
5.
i
i
i=1
d=1
4.1

Evaluating the Log Evidence Lower Bound

Given A and ω, we evaluate the probability of the d’th
column of Y, yd , as
log p(yd |A, X, ω) =
τ
N
log(2πτ −1 ) − (yd − Φad )T (yd − Φad ).
−
2
2
Note that yd is an N × 1 vector, Φ is an N × LK matrix,
and ad is an LK × 1 vector.
We need to evaluate the expectations of ydT Φad and
aTd ΦT Φad (both scalar values) under q(A)q(ω):



Eq(A)q(ω) ydT Φad = ydT Eq(ω) Φ Eq(A) ad ,
and





Eq(A)q(ω) aTd ΦT Φad = tr Eq(ω) ΦT Φ Eq(A) ad aTd .
The values Eq(A) (ad ) and Eq(A) (ad aTd ) are evaluated as
Eq(A) (ad ) = md , Eq(A) (ad aTd ) = sd + md mTd .
Next we evaluate Eq(ω) Φ). Remember that Φ depends on
ω and that q(ω) = q((wk , bk )LK
k=1 ). We write as shorthand
xnk := 2πL−1
(x
−
z
)
with
component i appropriate to
n
k
i
k. Following identity 2 proved in the appendix, we have
that the expectation of a single cosine in Φ with respect to
q(w) is


1 T
Eq(w) cos wT xn + b = e− 2 xn Σxn cos µT xn + b
where µ is the mean of q(w) and Σ is its covariance (note

that we omitted the index k). We get
r



2σi2 − 1 xTnk Σk xnk
Eq(ω) Φ
=
e 2
K
n,k

· Eq(bk ) cos(µTk xnk + bk )
with the integration with respect to q(bk ) trivial.

Next we evaluate Eq(ω) ΦT Φ , an LK × LK matrix:


N
X

2σi2
T
Eq(ω) Φ Φ
E cos(wiT xni + bi )
=
LK
i,j
n=1
· cos(wjT xnj + bj )



where the expectation is over q(wi , bi , wj , bj ) for i, j ≤
LK.
For i 6= j, from independence we can break the expectation
of each term into a product of Eq(wi ,bi ) cos(wiT xni + bi )
terms, and for i = j we follow identity 3 to obtain

Eq(wi ,bi ) cos(wiT xni + bi )2

T
1 1
= + e−2xni Σi xni Eq(bi ) cos(2µTi xni + 2bi ) .
2 2
In conclusion, we obtained our optimisation objective:
D 
X
τ
N
log(2πτ −1 ) − ydT yd
L=
−
2
2
d=1

+ τ ydT Eq(ω) Φ md


τ
− tr Eq(ω) (ΦT Φ)(sd + md mTd )
2
− KL(q(A)||p(A)) − KL(q(ω)||p(ω)).
(6)
The KL divergence terms can be evaluated analytically for
the Gaussian and uniform distributions.
4.2

Optimal variational distribution over A

In the above we optimise over the variational parameters
for A, namely md and sd for d ≤ D. This allows us to
attain a reduction in time complexity compared to previous approaches and use stochastic inference, as will be explained below. This comes with a cost, as the dependence
between ω and A can render the optimisation hard.
We can find the optimal variational distribution q(A) analytically, which allows us to optimise q(ω) and the hyperparameters alone. In proposition 3 in the appendix we show
that the optimal variational distribution is given by
q(ad ) = N (ΣEq(ω) (ΦT )yd , τ −1 Σ)
with Σ = (Eq(ω) (ΦT Φ) + τ −1 I)−1 .
The lower bound to optimise then reduces to
D 
X
N
τ
1
L=
−
log(2πτ −1 ) − ydT yd + log(|τ −1 Σ|)
2
2
2
d=1

Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs

1
+ τ ydT Eq(ω) (Φ)ΣEq(ω) (ΦT )yd
2
− KL(q(ω)||p(ω)).



6 Predictive Distribution
(7)

5 Distributed Inference and Stochastic Inference
Evaluating L in equation 6 requires O(N K 2 ) time complexity (for fixed Q, D, diagonal sd , and covariance function with one component L = 1). This stems from the term
Eq(ω) ΦT Φ – a K × K matrix, where each element is
composed of a sum over N .
Following the ideas of (Gal et al., 2014) we show that the
approximation can be implemented in a distributed framework. Write

1
τ
Lnd = − log(2πτ −1 ) − ynd ynd + τ ynd Eq(ω) φn md
2
2

τ
− tr Eq(ω) (φTn φn )(sd + md mTd )
2
with φn = φ(xn , ω). We can break the optimisation objective in equation 6 into a sum over N ,
L=

D X
N
X

Lnd − KL(q(A)||p(A)) − KL(q(ω)||p(ω)).

d=1 n=1

(8)
These terms can be computed concurrently on T different
nodes in a distributed framework, requiring O(N K 2 /T )
time complexity in each iteration. Scaling the number of
nodes with dataset size gives a practical time complexity
of O(K 2 ). We can further break the computation of Lnd
into a sum over K as well, thus reducing the time complexity to O(K) with K inducing points. This is in comparison to distributed inference with sparse pseudo-input
GPs which takes O(K 3 ) time complexity in practice with
K inducing points, resulting from the covariance matrix
inversion. This is a major advantage, as empirical results
suggest that in many real-world applications the number of
inducing points should scale with the data.

The approximate predictive distribution for a point x∗ is
given by equation 4. Denoting M = [md ]D
, we have
d=1
∗
Eq(y∗ |x∗ ) (y ) = Eq(ω) φ∗ M
(10)
following proposition 4 in the appendix.
The variance of the predictive distribution is given by
Varq(y∗ |x∗ ) (y∗ ) = τ −1 ID + Ψ
(11)



T
+ MT Eq(ω) φT∗ φ∗ − Eq(ω) φ∗ Eq(ω) φ∗ M


with Ψi,j = tr Eq(ω) φT∗ φ∗ · si · 1[i = j], following
proposition 5 in the appendix (1 is the indicator function).
When the optimal variational distribution over A is used,
we have M = ΣEq(ω) (ΦT )Y and si = τ −1 Σ for all i.

7 Properties of the Approximate Model
We have presented a variational sparse spectrum approximation to the Gaussian process (VSSGP in short). We gave
3 approximate models with different lower bounds: an approximate model with an optimal variational distribution
over A (equation 7, referred to as VSSGP), an approximate model with a factorised lower bound over the data
points (equations 6, 8, referred to as factorised VSSGP –
fVSSGP), and an approximation to the lower bound of the
factorised VSSGP for use in stochastic optimisation over
subsets of the data (equation 9, referred to as stochastic
factorised VSSGP – sfVSSGP).
The VSSGP model generalises on some of the GP approximations brought in the introduction. Fixing Σk at zero in
our approximate model (as well as αk and βk at 0 and 2π)
and optimising only µk results in the sparse spectrum approximation. Randomising µk , we obtain the random projections approximation (Rahimi & Recht, 2007). Indeed,

for Σk = 0 and fixed phases we have
that Eq(ω) ΦT Φ =

Eq(ω) ΦT Eq(ω) Φ and Eq(ω) Φ = Φ, and equation 7
recovers equation 8 in (Lázaro-Gredilla et al., 2010).

(9)

The points zk act as inducing inputs with wk and bk acting
as inducing frequencies and phases at these inputs. This
is similar to the sparse pseudo-input approximation, but
instead of having inducing values in the output space, we
have the inducing values in the frequency domain. These
are necessary for the approximation. Without these points
(or equivalently, setting these to 0), the features would decay quickly for data points far from the origin (the fixed
point 0).

with a mini-batch S of randomly selected points. This is an
unbiased estimator to the lower bound. The time complexity of each iteration is O(SK 2 ) with S << N the size of
the random subset, compared to O(SK 2 + K 3 ) of GP SVI
using sparse pseudo-input approximation (Hensman et al.,
2013).

The distribution over the frequencies is optimised to fit the
data well. The prior is used to regulate the fit and avoid
over-fitting to the data. This approximation can be used to
learn covariance functions by fitting them to the data. This
is similar to the ideas brought in (Duvenaud et al., 2013)
where the structure of a covariance function is sought by

We can exploit the above representation and perform
stochastic variational inference (SVI) by approximating the
objective with a subset of the data, resulting in noisy gradients (Hoffman et al., 2013). Here we use as our objective
D

L≈

N XX
Lnd
|S|
d=1 n∈S

− KL(q(A)||p(A)) − KL(q(ω)||p(ω))

Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs

looking at possible compositions of these. This can give additional insight into the data. In (Duvenaud et al., 2013) the
structure of the covariance composition is used to explain
the data. In the approximation presented here the spectrum
of the covariance function can be used to explain the data.
It is interesting to note that although the approximated covariance function K(x, y) has to be stationary (i.e. it is
represented as K(x, y) = K(x − y)), the approximate
posterior is not. This is in contrast to the SSGP that results in a stationary approximation. Furthermore, unlike
the SSGP, our approximation is not periodic. This is one of
the theoretical limitations of the sparse spectrum approximation. The limitation arises from the fact that the covariance is represented as a weighted sum of cosines in the
SSGP. In the our approximation this is avoided by decaying the cosines to zero. This and other properties of the
approximation are discussed further in discussion 1 in the
appendix.

Figure 1. Predictive mean and uncertainty on the Mauna Loa CO2
concentrations dataset. In red is the observed function; in blue is
the predictive mean plus/minus two standard deviations. In this
example the approximating distribution is used with a sum of
squared exponential covariance functions with two components
(L = 2, K = 10).

[0.37, 5.18], correspondingly. The frequency with the
smallest standard-deviation (highest confidence) for the
first component is 3.27 (corresponding to a period of 1 year,
capturing the short term behaviour). For the second component it is 0.58 (corresponding to a period of 475 years
capturing the long term behaviour).

8 Experiments

8.2

We study the properties of VSSGP and compare it to alternative approximations, showing its advantages. We compare the VSSGP to the full Gaussian process (denoted Full
GP), sparse spectrum GP approximation (denoted SSGP),
sparse pseudo-input GP approximation (denoted SPGP),
and the random projections approximation (denoted RP).
We compare the VSSGP to the fVSSGP and sfVSSGP
that offer improved time complexity. We further compare
sfVSSGP to the existing sparse pseudo-input GP approach
used with SVI (denoted sSPGP, Hensman et al., 2013). We
inspect the model’s time accuracy trade-off and show that it
avoids over-fitting as the number of parameters increases.

We compare various GP approximations on the solar irradiance dataset (Lean, 2004). We scaled the dataset dividing
by the data standard deviation, and removed 5 segments of
length 20. We followed the experiment set-up of the previous section and used the same initial parameters for all
approximate models. We use a single SE setting its lengthscale l = 1, and used 50 inducing inputs. LBFGS was used
for 1000 iterations. The RP model was run twice with two
different settings: once following the same set-up of the
other models, optimising over the model hyper-parameters
(RP1 ), and once keeping all hyper-parameters fixed and
setting the observation noise precision to τ = 100 with
K = 500 inducing inputs5 (RP2 ).

8.1 VSSGP Properties
We evaluate the predictive mean and uncertainty of the
VSSGP on the atmospheric CO2 concentrations dataset derived from in situ air samples collected at Mauna Loa Observatory, Hawaii (Keeling et al., 2004). We fit the approximate model using a sum of squared exponential covariance functions with two components initialised with initial
length-scales [0.1, 1000]. We randomised the phases following the Monte Carlo integration (instead of optimising
a variational distribution on these4 ) and initialise the frequencies at random. We use 10 inducing inputs for each
component (K = 10), set the observation noise precision
to τ = 10, and covariance noise to σ 2 = 1. LBFGS (Zhu
et al., 1997) was used to optimise the objective given in
equation 7, and was run for 500 iterations.
Figure 1 shows the predictive mean with the predictive uncertainty increasing far from the data. This is a property
shared with the SE GP. The covariance hyper-parameters
optimise to length-scales [0.52, 43.83] and covariance noise
4

This seems to work better in practice.

Comparison to Existing GP Approximations

Figure 2 shows qualitatively the predictive mean and uncertainty of the various approaches. SSGP and RP seem
to over-fit the function using high frequencies with high
confidence. SPGP seems to under-fit the function, but has
accurate predictive mean and uncertainty at points where
many inducing inputs lie (such as the flat region). VSSGP’s
predictive mean resembles that of the full GP, but with increased uncertainty throughout the space. Further, its uncertainty on the missing segments is smaller than that of the
full GP (some frequencies have low uncertainty, thus used
near the data). The full GP learnt length-scale is 4. VSSGP
learnt a length-scale of 3, and SPGP learnt a length-scale
of 5. SSGP and RP1 learnt length-scales of 0.97, 1.66, i.e.
the hyper-parameter optimisation found a local minimum.
Table 1 gives a quantitative comparison of the different approximations for the task of imputation. RMSE (root mean
square error) of the approximate predictive mean on the
5

This follows the usual use of the model in the randomised
methods community. We experimented with various values of τ
and decided to use 100.

Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs

the data better. It is interesting to note that the average
standard deviation on the test set for the full GP is 0.63,
whereas for the VSSGP it is 0.25, i.e. the model is more
certain about its predictions.
8.3

From SSGP to Variational SSGP

(a) Sparse pseudo-input GP

We use variational inference in the VSSGP to avoid overfitting to the data, a behaviour that is often observed with
SSGP. To test this we perform a direct comparison of the
proposed approximate model to SSGP on the task of audio signal imputation. For this experiment we used a short
speech signal with 1000 samples taken from the TIMIT
dataset (Garofolo et al., 1993). We removed 5 segments
of length 40 from the signal, and evaluated the imputation
error (RMSE) of the predictive mean with K = 100 inducing points. We used the same experiment set-up as before with a sum of 2 SE covariance functions with lengthscales l = [2, 10] and observation noise precision τ = 1000
matching the signal magnitude. LBFGS was run for 1000
iterations. The experiment was repeated 5 times and the
results averaged.

(b) Sparse Spectrum GP

(c) Random Projections (RP2 , K = 500)

Audio 1K
Train
Test

Table 3 shows the RMSE of the training set and test set
for the audio data. SSGP seems to achieve a small training
error but cannot generalise well to unseen audio segments.
VSSGP attains a slightly lower training error, and is able to
impute unseen audio segments with better accuracy.

(e) Variational Sparse Spectrum GP
Figure 2. Predictive mean and uncertainty on the reconstructed
solar irradiance dataset with missing segments, for the GP and
various GP approximations. In red is the observed function and in
green are the missing segments. In blue is the predictive mean
plus/minus two standard deviations of the various approximations. All tests were done with the SE covariance function, and all
sparse approximations use K = 50 inducing inputs (apart from
RP2 with K = 500).

SPGP
0.23
0.61

SSGP
0.15
0.63

RP1
0.32
0.65

VSSGP
0.0062 ± 0.00048
0.034 ± 0.0043

Table 3. Imputation RMSE on both train and test sets, for a speech
signal segment of length 1K (K = 100).

(d) Full GP

Solar
Train
Test

SSGP
0.0091 ± 0.0042
0.088 ± 0.033

RP2
0.04
0.76

GP
0.08
0.50

VSSGP
0.13
0.41

Table 1. Imputation RMSE on both train and test sets, for the reconstructed solar irradiance dataset. All tests were done with the
SE covariance function, and all sparse approximations use 50 inducing inputs (apart from RP2 that uses K = 500).

missing segments was computed (test error), as well as the
RMSE on the observed function (training error). Note that
the full GP seems to get worse results than VSSGP. This
might be because the learnt covariance function captures

It is interesting to note that using the RMSE of the shorttime Fourier transform of the original signal and the predicted mean (STFT, the common metric for audio imputation, with 25ms frame size and a hop size of 12ms), the
SSGP model attains a training error of 0.094 ± 0.05 and a
test error of 0.55 ± 0.41. The VSSGP attains a training error of 0.067 ± 0.0067 with a test error of 0.17 ± 0.022. For
comparison, baseline performance of predicting 0 attains a
training error of 0.44 and an error of 0.38 on the test set.
GPs for audio data are examined further in (Turner, 2010).
8.4

VSSGP, factorised VSSGP, and stochastic factorised VSSGP

VSSGP, fVSSGP, and sfVSSGP all rely on different lower
bounds to the same approximate model. Whereas VSSGP
solves for the variational distribution over the Fourier coefficients analytically, fVSSGP optimises over these quantities. This reduces the time complexity, but with the price of
potentially worsened performance. sfVSSGP further employs an approximation to the lower bound using random
subsets of the data – following the idea that not all data

Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs

Audio 1K
Train
Test

VSSGP
0.0062 ± 0.00048 (0.063±0.0068)
0.034 ± 0.0043 (0.17±0.022)

fVSSGP
0.0054 ± 0.00083 (0.055±0.0088)
0.038 ± 0.0049 (0.22±0.028)

sfVSSGP
0.005 ± 0.003 (0.052±0.031)
0.04 ± 0.0066 (0.24±0.0089)

Table 2. Imputation RMSE (and in smaller font STFT RMSE) on train and test sets, for a speech signal segment of length 1K (K = 100).

points have to be observed for a good fit to be found. This
assumption has the potential to hinder performance even
further. We next assess these trade-offs.
We repeated the experimental set-up of the previous section (and use the same RMSE for VSSGP). We optimise
both fVSSGP and sfVSSGP for 5000 iterations instead of
the 1000 of VSSGP. This is because the improved time
complexity allows us to perform more function evaluations
within the same time-frame. We optimise the fVSSGP
lower bound with LBFGS, and the sfVSSGP lower bound
with RMSPROP (Tieleman & Hinton, 2012). RMSPROP
performs stochastic optimisation with no need for learningrate tuning – the learning rate changes adaptively based on
the directions of the last two gradients.
Table 2 shows the RMSE for the train and test sets. Both
fVSSGP and sfVSSGP effectively achieve the same test set
accuracy (taking the standard deviation into account). We
also see a slight decrease in train set RMSE.
8.5 Stochastic Variational Inference
We compared sfVSSGP to the SPGP approximation with
stochastic variational inference (sSPGP, Hensman et al.,
2013). We used the same audio experiment as above, but
with a signal of length 16000. 25 random segments of
length 80 were removed from the signal. sSPGP’s time
complexity (O(SK 2 + K 3 ) with mini-batch of size S and
K inducing points) prohibits it from being used with a large
number of inducing points. We therefore used 800 inducing
points for sSPGP and 400 inducing inputs for each component in the covariance function of sfVSSGP (K = 400).
The RMSE of sSPGP for the training set is 0.043 and for
the test set is 0.034 (with a training time of 133 minutes
using GPy (authors, 2012–2014)). The RMSE of sfVSSGP
for the training set is 0.016 and for the test set is 0.034
(with a training time of 48 minutes). Using the same audio imputation metric as in the previous section, the STFT
RMSE for sSPGP on the training set is 0.54 and on the test

set is 0.43. The STFT RMSE for VSSGP on the training
set is 0.18 with a test error 0.3. For comparison, baseline
performance of predicting 0 attains an error of 0.52 on the
training set and an error of 0.62 on the test set.
8.6

Speed-Accuracy Trade-off

We inspect the speed-accuracy trade-off of the approximation (RMSE as a function of the number of inducing points)
for the sfVSSGP approximation. We repeat the same audio
experiment set-up with a speech signal with 4000 samples
and evaluate the imputation error (RMSE) of the predictive
mean with various numbers of inducing point. RMSPROP
was run for 500 iterations. The experiment was repeated 5
times and the results averaged.
Figure 3 shows that the approximation offers increased accuracy with an increasing number of inducing points. No
further improvement is achieved with more than 400 inducing points. The time scales quadratically with the number
of inducing points. Note that the approximation does not
over-fit to the data as the number of parameters increases.

9 Discussion
Our approximate inference relates to the Bayesian neural
network (Bayesian NN, Mackay, 1992; MacKay, 1992).
In the Bayesian NN a prior distribution is placed over
the weights of an NN, and a posterior distribution (over
the weights and outputs) is sought. The model offers a
Bayesian interpretation to the classic NN, with the desired
property of uncertainty estimates on the outputs. Inference
in Bayesian NNs is generally hard, and approximations to
the model are often used (Bishop, 2006, pp 277-290). Our
GP approximate inference relates Bayesian NNs and GPs,
and can be seen as a method for tractable variational inference in Bayesian NNs with a single hidden layer.
Future research includes the extension of our approximation to deep GPs (Damianou & Lawrence, 2012). We also
aim to use the approximate model as a method for adding
and removing units in an NN in a principled way. Lastly,
we aim to replace the cosines in the Fourier expansion with
alternative basis functions and study the resulting approximate model.

Acknowledgements
(a) Train error

(b) Test error

(c) Running time

Figure 3. Mean and standard deviation for train error, test error,
and running time, all as functions of the number of inducing
points (K) for a speech signal segment of length 4K.

The authors would like to thank James Robert Lloyd for
some helpful discussions. YG is supported by the Google
European Fellowship in Machine Learning. Funding was
provided by the EPSRC (grant numbers EP/G050821/1 and
EP/L000776/1) and Google (R.E.T.).

Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs

References
authors, The GPy. GPy: A Gaussian process framework
in python. http://github.com/SheffieldML/
GPy, 2012–2014.
Bishop, Christopher M. Pattern Recognition and Machine
Learning (Information Science and Statistics). SpringerVerlag New York, Inc., Secaucus, NJ, USA, 2006. ISBN
0387310738.
Bochner, Salomon. Lectures on Fourier integrals. Number 42. Princeton University Press, 1959.
Brochu, Eric, Cora, Vlad M, and de Freitas, Nando.
A tutorial on Bayesian optimization of expensive cost
functions, with application to active user modeling and
hierarchical reinforcement learning. arXiv preprint
arXiv:1012.2599, 2010.

Observatory, Hawaii. Scripps Institution of Oceanography (SIO), University of California, La Jolla, California
USA 92093-0444, June 2004.
Lázaro-Gredilla, Miguel, Quiñonero-Candela, Joaquin,
Rasmussen, Carl Edward, and Figueiras-Vidal,
Anı́bal R.
Sparse spectrum Gaussian process regression. The Journal of Machine Learning Research,
11:1865–1881, 2010.
Lean, J. Solar irradiance reconstruction. NOAA/NGDC
Paleoclimatology Program, Boulder CO, USA, 2004.
IGBP PAGES/World Data Center for Paleoclimatology.
Data Contribution Series 2004-035.
Mackay, David. The evidence framework applied to classification networks. Neural computation, 4(5):720–736,
1992.

Damianou, Andreas C and Lawrence, Neil D. Deep Gaussian processes. arXiv preprint arXiv:1211.0358, 2012.

MacKay, David JC. A practical Bayesian framework for
backpropagation networks. Neural computation, 4(3):
448–472, 1992.

Duvenaud, David, Lloyd, James Robert, Grosse, Roger,
Tenenbaum, Joshua B., and Ghahramani, Zoubin. Structure discovery in nonparametric regression through compositional kernel search. In Proceedings of the 30th
International Conference on Machine Learning, June
2013.

Quiñonero-Candela, Joaquin and Rasmussen, Carl Edward. A unifying view of sparse approximate Gaussian
process regression. Journal of Machine Learning Research, 6:2005, 2005.

Engel, Yaakov, Mannor, Shie, and Meir, Ron. Reinforcement learning with Gaussian processes. In Proceedings
of the 22nd international conference on Machine learning, pp. 201–208. ACM, 2005.
Gal, Yarin, van der Wilk, Mark, and Rasmussen, Carl. Distributed variational inference in sparse Gaussian process
regression and latent variable models. In Ghahramani,
Z., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, K.Q. (eds.), Advances in Neural Information Processing Systems 27, pp. 3257–3265. Curran Associates,
Inc., 2014.
Garofolo, John S, Consortium, Linguistic Data, et al.
TIMIT: acoustic-phonetic continuous speech corpus.
Linguistic Data Consortium, 1993.
Hensman, James, Fusi, Nicolo, and Lawrence, Neil D.
Gaussian processes for big data. In Nicholson, Ann and
Smyth, Padhraic (eds.), UAI. AUAI Press, 2013.
Hoffman, Matthew D., Blei, David M., Wang, Chong, and
Paisley, John. Stochastic Variational Inference. Journal
Of Machine Learning Research, 14:1303–1347, MAY
2013. ISSN 1532-4435.
Keeling, C.D., Whorf, T.P., and the Carbon Dioxide Research Group. Atmospheric CO2 concentrations (ppmv)
derived from in situ air samples collected at Mauna Loa

Rahimi, Ali and Recht, Benjamin. Random features for
large-scale kernel machines. In Advances in neural information processing systems, pp. 1177–1184, 2007.
Rasmussen, Carl Edward and Williams, Christopher K. I.
Gaussian Processes for Machine Learning (Adaptive
Computation and Machine Learning). The MIT Press,
2006. ISBN 026218253X.
Rasmussen, Carl Edward, Kuss, Malte, et al. Gaussian processes in reinforcement learning. In NIPS, volume 4, pp.
1, 2003.
Tan, Linda S. L., Ong, Victor M. H., Nott, David J., and
Jasra, Ajay. Variational inference for sparse spectrum
Gaussian process regression. arXiv:1306.1999, 2013.
Tieleman, T. and Hinton, G. Lecture 6.5 - rmsprop,
COURSERA: Neural networks for machine learning,
2012.
Titsias, Michalis and Lawrence, Neil. Bayesian Gaussian
process latent variable model. Thirteenth International
Conference on Artificial Intelligence and Statistics (AISTATS), 6:844–851, 2010.
Titsias, Michalis K. Variational learning of inducing variables in sparse Gaussian processes. In International
Conference on Artificial Intelligence and Statistics, pp.
567–574, 2009.

Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs

Turner, Richard E. Statistical Models for Natural Sounds.
PhD thesis, Gatsby Computational Neuroscience Unit,
UCL, 2010.
Wilson, Andrew, Gilboa, Elad, Cunningham, John P, and
Nehorai, Arye. Fast kernel learning for multidimensional pattern extrapolation. In Advances in Neural Information Processing Systems, pp. 3626–3634, 2014.
Zhu, Ciyou, Byrd, Richard H, Lu, Peihuang, and Nocedal,
Jorge. Algorithm 778: L-BFGS-B: Fortran subroutines
for large-scale bound-constrained optimization. ACM
Transactions on Mathematical Software (TOMS), 23(4):
550–560, 1997.

