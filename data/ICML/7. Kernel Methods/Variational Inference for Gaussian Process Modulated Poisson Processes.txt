Variational Inference for Gaussian Process Modulated Poisson Processes
Chris Lloyd*
Tom Gunter*
Michael A. Osborne
Stephen J. Roberts
Department of Engineering Science, University of Oxford

Abstract
We present the first fully variational Bayesian
inference scheme for continuous Gaussianprocess-modulated Poisson processes. Such
point processes are used in a variety of domains,
including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our
scheme: requires no discretisation of the domain;
scales linearly in the number of observed events;
and is many orders of magnitude faster than previous sampling based approaches. The resulting
algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster
data and in the prediction of Malaria incidences
in Kenya.

1. Introduction
Sparse events defined over a continuous domain arise in a
variety of real-world applications. The geospatial spread
of disease through time, for example, may be viewed as a
set of infections which occur in three dimensional spacetime. In this work, we will consider data where the intensity (or average incidence rate) of the event generating
process is assumed to vary smoothly over the domain. A
popular model for such data is the inhomogenous Poisson
process with a Gaussian process model for the smoothlyvarying intensity function. This flexible approach has
been adopted for applications in neuroscience (Cunningham et al., 2008b), finance (Basu & Dassios, 2002) and
forestry (Heikkinen & Arjas, 1999).
However, existing inference schemes for such models scale
poorly with the number of data, preventing them from finding greater use. The use of a full Gaussian process in modelling the intensity (Adams et al., 2009) incurs prohibitive
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

CLLOYD @ ROBOTS . OX . AC . UK
TGUNTER @ ROBOTS . OX . AC . UK
MOSB @ ROBOTS . OX . AC . UK
SJROB @ ROBOTS . OX . AC . UK

O(N 3 ) computational scaling in the number of data points,
N . To tackle this problem in practice, many approaches
(Rathbun & Cressie, 1994; Mller et al., 1998) discretise
the domain, binning counts within each segment. This
approach enabled Cunningham et al. (2008a) to achieve
O(N log N ) performance. However, the discretisation approach suffers from poor scaling with the dimension of the
domain and sensitivity to the choice of discretisation.
We introduce a new model for Gaussian-processmodulated Poisson processes that eliminates the requirement for discretisation, while simultaneously delivering
O(N ) scaling. We further introduce the first fully variational Bayesian inference scheme for such models, allowing computation many orders of magnitude faster than existing schemes. This approach, which we term Variational
Bayes for Point Processes (VBPP), is shown to provide
more accurate prediction than benchmarks on held-out data
from datasets including synthetic examples, coal mining
disaster data and Malaria incidences in Kenya. The power
of our approach suggests many future applications: in particular, our fully generative model will permit the joint inference of real-valued covariates (such as log-rainfall) and
a point process (such as disease outbreaks).

2. Cox Processes
Formally a Cox process‚Äîa particular type of inhomogenous Poisson process‚Äîis defined via a stochastic intensity
function Œª(x) : X ‚Üí R+ . For a domain X = RR of arbitrary dimension R, the number of points, N (T ), found in
a subregion
R T ‚äÇ X is Poisson distributed with parameter
ŒªT = T Œª(x) dx‚Äîwhere dx indicates integration with
respect to the Lebesgue measure over the domain‚Äîand for
disjoint subsets Ti of X , the counts N (Ti ) are independent.
This independence is due to the completely independent
nature of points in a Poisson process (Kingman, 1993).
If we restrict our consideration to some bounded region,
T , the probability density of a set of N observed points,
* Corresponding authors.

Variational Inference for Gaussian Process Modulated Poisson Processes

D = {x(n) ‚àà T }N
n=1 , conditioned on the rate function
Œª(x) is
 Z
p(D | Œª) = exp ‚àí

Œª(x) dx
T

Y
N

Œª(x(n) ).

(1)

n=1

We use |T | to denote the measure of the continuous domain
T . In this work we will assume T is a hyper-rectangular
sub-set of RR with boundaries Trmin and Trmax in each dimension r and
Z
R
Y
|T | =
(Trmax ‚àí Trmin ).
(2)
1 dx =
T

r=1

Using Bayes‚Äô rule, the posterior distribution of the rate
function conditioned on the data, p(Œª|D), is
	 QN
 R
(n)
)
p(Œª) exp ‚àí T Œª(x) dx
n=1 Œª(x
(3)
	 QN
 R
R
(n)
)dŒª
p(Œª) exp ‚àí T Œª(x) dx
n=1 Œª(x
which is often described as ‚Äúdoubly-intractable‚Äù because of
the nested integral in the denominator.

higher dimensional point processes. The authors term this
approach ‚Äúadaptive thinning‚Äù.
In Lasko (2014) the author performs renewal process inference without thinning the domain, by making use of a positively transformed intensity function. The intractability of
their chosen approach forces them to resort to numerical
integration techniques, however, and Bayesian inference is
still performed using computationally expensive sampling.

3. Model
We construct our prior over the rate function using a Gaussian process. Rather than using a squashing function, we
will assume1 the intensity function is simply defined as
Œª(x) = f 2 (x), x ‚àà T , where f is a Gaussian process
distributed random function achieving a non-negative prior
(Gunter et al., 2014b). Furthermore we will assume that
f is dependent on a set of inducing points Z = {z(m) ‚àà
T }M
m=1 . We denote the evaluation of f at these points u,
and note u has distribution u ‚àº N (~1uÃÑ,
 Kzz ). Using this
formulation f |u ‚àº GP ¬µ(x), Œ£(x, x‚Ä≤ ) has mean and covariance functions

2.1. Inferring Intensity Functions
To overcome the challenges posed by the doubly intractable
integral Adams et al. (2009) propose the Sigmoidal Gaussian Cox Process (SGCP). In the SGCP, a Gaussian process (Rasmussen & Williams, 2006) is used to construct
an intensity function prior by passing a random function,
f ‚àº GP, through a sigmoid transformation and scaling it
with a maximum intensity Œª‚àó . The intensity function is
therefore Œª(x) = Œª‚àó œÉ f (x) , where œÉ(¬∑) is the logistic
sigmoid (squashing) function
1
.
œÉ(x) =
1 + exp(‚àíx)

¬µ(x) = kxz K‚àí1
zz u,
‚Ä≤

Œ£(x, x ) = Kxx‚Ä≤ ‚àí

In Gunter et al. (2014a), the authors go some way towards
improving the scalability of the SGCP, by introducing a further set of latent variables such that the entire space need no
longer be thinned uniformly. Instead, they thin to a piecewise uniform Poisson process, maintaining the tractability
of the inner integral, and allowing the model to scale to

(6)

where kxz , Kxx‚Ä≤ , Kzz are matrices evaluated at x, x‚Ä≤ and
Z using an appropriate kernel. We use the exponentiated
quadratic (also known as the ‚Äúsquared exponential‚Äù) ARD
kernel
R
Y

(xr ‚àí x‚Ä≤r )2
exp ‚àí
K(x, x ) = Œ≥
2Œ±r
r=1
‚Ä≤

(4)

To remove the inner intractable integral, the authors augment the variable set to include latent data, such that the
joint distribution of the latent and observed data is uniform
Poisson over the region T . While this model works well
in practice on small, sparse event data in one dimension,
in reality, it scales poorly with both the dimensionality of
the domain and the maximum observed density of points.
This is due to: the incorporation of latent, or thinned, data,
whose number grows exponentially with the dimensionality of the space; and an O(N 3 ) cost in the number N of all
data (thinned or otherwise).

(5)

kxz K‚àí1
zz kzx‚Ä≤ ,





.

(7)

With this hierarchical formulation the joint distribution
over D, f , u and Œò is
p(D, f, u, Œò) = p(D|Œª = f 2 )p(f |u, Œò)p(u|Œò)p(Œò), (8)
where p(Œò) is an optional prior on the set of model parameters Œò = {Œ≥, Œ±1 , . . . , Œ±R , uÃÑ}. For notational convenience
we will often omit conditioning on Œò.

4. Inference
We will use variational inference to obtain a bound on the
model evidence p(D). To achieve this we must integrate
out f and u, but we must also integrate f 2 over the region
T due to the integral embedded in the likelihood, Equation
1.
1

See Section 5 for a detailed motivation of this choice.

Variational Inference for Gaussian Process Modulated Poisson Processes

4.1. Variational Bound

4.2. Integrating Over The Region T

We begin by integrating out the inducing variables u, using a variational distribution q(u) = N (u; m, S) over the
inducing points. We now multiply and divide the joint by
q(u) and lower bound using Jensen‚Äôs inequality to obtain a
lower bound on the model evidence:

Z Z
q(u)
du df
log p(D|Œò) = log
p(D|f )p(f |u)p(u)
q(u)
ZZ
‚â•
p(f |u)q(u) du log[p(D|f )] df


ZZ
p(u)
du
+
p(f |u)q(u) df log
q(u)

= Eq(f ) [log p(D|f )] ‚àí KL q(u)||p(u)

This lower bound has the desirable property that we can
take expectations under q(f ) at any specific point, x, of the
function value, f (x), since q(f (x)) is Gaussian. It is only
possible to take useful expectations because: a) we used
the conditional GP formulation, allowing tractable expectations to be taken w.r.t. q(f ); b), we have already integrated
out the inducing variables u; and c) we chose a suitable
transformation, i.e. Œª(x) = f 2 (x).

,L

(9)

Since p(f |u) is conjugate to q(u), we can write down in
closed-form the resulting integral:
Z
q(f ) = p(f |u)q(u)du = GP(f ; ¬µÃÉ, Œ£ÃÉ),
(10)
¬µÃÉ(x) = kxz K‚àí1
zz m,

‚àí1
‚àí1
Œ£ÃÉ(x, x‚Ä≤ ) = Kxx‚Ä≤ ‚àí kxz K‚àí1
zz kzx‚Ä≤ + kxz Kzz SKzz kzx‚Ä≤ .

KL q(u)||p(u) is simply the KL-divergence between two
Gaussians

 1

|Kzz |
KL q(u)||p(u) =
tr K‚àí1
‚àíM
zz S ‚àí log
2
|S|
i
~
+ (~1uÃÑ ‚àí m)‚ä§ K‚àí1
(11)
zz (1uÃÑ ‚àí m) .

We can now take expectations of the data log-likelihood
under q(f ):

L = Eq(f ) [log p(D|f )] ‚àí KL q(u)||p(u)
" Z
#
N
X
2
2
= Eq(f ) ‚àí
log fn
fx dx +
T

n=1


‚àí KL q(u)||p(u)
Z

	
=‚àí
Eq(f ) [fx ]2 + Varq(f ) [fx ] dx
+

n=1




Eq(f ) log fn2 ‚àí KL q(u)||p(u) ,

(12)

where to keep the notation concise we have introduced the
following identities:
fx , f (x),
fn , f (x(n) ),

¬µÃÉx , ¬µÃÉ(x),
¬µÃÉn , ¬µÃÉ(x(n) ),

‚àí1
Eq(f ) [fx ]2 = ¬µÃÉ2x = m‚ä§ K‚àí1
zz kzx kxz Kzz m,

Varq(f ) [fx ] =

œÉÃÉx2

= kxx ‚àí Tr(K‚àí1
zz kzx kxz )
‚àí1
‚àí1
+ Tr(Kzz SKzz kzx kxz ).

œÉÃÉx2

, Œ£ÃÉ(x, x),
œÉÃÉn2 , Œ£ÃÉ(x(n) , x(n) ).

Note we have used Tonelli‚Äôs Theorem to reverse the ordering of the integrations over the positive integrand fx2 q(f ).
We now have two tasks remaining: we must compute the
integral
 over the region T and calculate the expectations
Eq(f ) log fn2 at the data points.

(13)
(14)

It is now easy to calculate the integral since only kzx = k‚ä§
xz
is a function of x, leading to the following terms:
Z
‚àí1
Eq(f ) [fx ]2 dx = m‚ä§ K‚àí1
(15)
zz Œ®Kzz m,
T
Z
Varq(f ) [fx ]dx = Œ≥|T | ‚àí Tr(K‚àí1
zz Œ®)
T

‚àí1
+ Tr(K‚àí1
zz SKzz Œ®).

(16)

For the exponentiated quadratic ARD kernel, the matrix
Z
Œ® = K(z, x)K(x, z‚Ä≤ ) dx
(17)

can be calculated by re-arranging the product as a single
exponentiated quadratic in x and zÃÑ as follows:


Z
R
Y
(xr ‚àí zÃÑr )2
(zr ‚àí zr‚Ä≤ )2
dx
‚àí
Œ®(z, z‚Ä≤ ) =
Œ≥2
exp ‚àí
4Œ±r
Œ±r
T
r=1


‚àö
R
Y
œÄŒ±r
(zr ‚àí zr‚Ä≤ )2
2
exp ‚àí
=Œ≥
‚àí
2
4Œ±r
r=1



 
zÃÑr ‚àí Trmin
zÃÑr ‚àí Trmax )
‚àí erf
,
√ó erf
‚àö
‚àö
Œ±r
Œ±r
where zÃÑ = [zÃÑ1 , . . . , zÃÑR ]‚ä§ has elements zÃÑr =

T

N
X

The required statistics for Equation 12 are:

zr +zr‚Ä≤
2 .

In addition to the exponentiated quadratic ARD kernel, the
matrix Œ® can be computed in closed-form for other kernels,
including polynomial and periodic kernels, as well as sum
and product combinations of kernels.
4.3. Expectations At The Data Points
The expectation Eq(f ) [log fn2 ] has an analytical‚Äîalbeit
complicated‚Äîsolution expressed as
Z ‚àû
2
log(fn2 ) N (fn , ¬µÃÉn , œÉÃÉn2 ) dfn (18)
Eq(f ) [log fn ] =
‚àí‚àû

 2

œÉÃÉn
¬µÃÉ2n
‚àí C, (19)
= ‚àíGÃÉ ‚àí 2 + log
2œÉÃÉn
2

Variational Inference for Gaussian Process Modulated Poisson Processes

where C ‚âà 0.57721566 is the Euler-Mascheroni constant
and GÃÉ is defined via the confluent hyper-geometric function
1 F1 (a, b, z)

=

‚àû
X
(a)k z k

k=0

(b)k k!

,

(20)

4.5. Locating The Inducing Points

Specifically GÃÉ is a specialised version of the partial derivative of 1 F1 with respect to its first argument and can
be computed using the method of Ancarani & Gasaneo
(2008), which has a particular solution at a = 0, leading
to the following definition of GÃÉ:
GÃÉ(z) = 1 F1



1
0, , z
2



= 2z

‚àû
X
j=0

j! z j
. (21)
(2)j (1 12 )j

Naive implementation of Equation 21 has poor numerical
stability, although this can be improved somewhat using an
iterative scheme, in practice we therefore use a large multiresolution look-up table of precomputed values obtained
from a numerical-package. As shown in Figure 1, this
function decreases very slowly as its argument becomes
increasingly negative, so we can easily compute accurate
evaluations of GÃÉ(z) for any z by linear interpolation of our
lookup table and, as a by-product, we also obtain GÃÉ‚Ä≤ .
0
‚àí1
‚àí2

GÃÉ(z)

‚àí3
‚àí4
‚àí5
‚àí6
‚àí7
‚àí8
‚àí9
‚àí1000 ‚àí900

‚àí800

‚àí700

‚àí600

‚àí500

z

‚àí400

‚àí300


Trmin + Trmax Trmin ‚àí Trmax
‚àí
sin œâr(m) , (22)
2
2
(m)

(a)k = a(a + 1)(a + 2) . . . (a + k ‚àí 1).

(1,0,0)

zr(m) =

and optimise in œâr ‚àà [‚àíœÄ, œÄ], which ensures the inducing points always remain within the region T .

where (¬∑)k denotes the rising Pochhammer series
(a)0 = 1,

To optimise the inducing point locations we use the change
of variables

‚àí200

‚àí100

0

Figure 1. Accurate evaluations of GÃÉ can be obtained from a precomputed look-up table.

4.4. Optimising The Bound
To perform inference we find variational parameters m‚àó ,
S‚àó and the model parameters Œò‚àó that maximise L. To
optimise these simultaneously we construct an augmented
vector y = [Œò‚ä§ , m‚ä§ , vech(L)‚ä§ ]‚ä§ ‚Äîwhere vech(L) is the
vectorisation of the lower triangular elements of L, such
that S = LL‚ä§ .
As well as the maximum-likelihood solution, we can also
compute the the maximum-a-posterior (MAP) estimate by
maximising L(D; y) + log p(Œò).

One final part of our model we have so far left unspecified is the number and location of inducing points. In
principle, for a given set of parameters Œò, we will obtain
a lower bound for the true GP likelihood for any number of
inducing points in any configuration of locations. We consider two possible approaches: firstly, treating the inducing points as optimisation parameters and, secondly, fixing
them on a regular grid.
If the locations of the inducing points are optimised, this
suggests‚Äîin common with other sparse GP models‚Äîthat
we can achieve good performance using a small number of
well-placed inducing points. This is achieved at the cost of
an additional set of optimisation parameters, although the
size of m and S are commensurately reduced. Optimisation of the inducing points is particularly computationally
expensive, however, because of the necessity to recompute
K‚àí1
zz for each dimension of each inducing point (M √ó R in
total) and since Kzz affects every term in the bound.
Regular grids, on the other hand, also have several advantages. Consider firstly that‚Äîin contrast to standard sparse
GP regression‚Äîthe accuracy of our solution is not only
governed by the distance between the inducing points and
the data points. The variance of f (x) increases as x becomes further from the inducing points. However, the rate
function, Œª, is a function of both the mean and the variance
of f . Since we are integrating Œª over T , we need inducing
points distributed across T and not just in regions close to
the data. An evenly-spaced grid is one way to ensure this
is achieved.
Regularly sampled grids also afford potential computational advantages. When the grid points are evenly spaced,
the kernel matrix has Toeplitz structure, and hence allows matrix inversion (and linear solving) in O(M log2 M )
time, a fact previously utilised for efficient point processes
by Cunningham et al. (2008a). Furthermore, when the kernel function is separable across the dimensions (as specified by Equation (7)), the kernel matrix has Kronecker
structure which can further reduce the cost of matrix inversion (Osborne et al., 2012). The latter is relevant to all
sparse GP applications based on inducing points, however,
it is particularly relevant for this application as we are motivated by the doubly intractable nature of Equation 3. In
our implementation, we use naƒ±Ãàve inversion of the induc-

Variational Inference for Gaussian Process Modulated Poisson Processes

ing point kernel matrix, Kzz , resulting in computational
complexity of O(N M 3 ). Hence the computation times
reported below could be improved with a relatively small
amount of additional implementation effort.
4.6. Predictive Distribution
To form the predictive distribution we assume our optimised variational distribution q ‚àó (u) = N (u; m‚àó , S‚àó ) approximates the posterior p(u|D). Analogously to Equation
10 we next compute q ‚àó (f ) ‚âà p(f |D). We can now derive a
lower bound of the (approximate) predictive log-likelihood
on a held-out test dataset H :
log p(H |D, Œò‚àó ) = log Ep(f |D) [p(H |f )]

‚âà log Eq‚àó (f ) [p(H |f )] , Mp
‚â• Eq‚àó (f ) [log p(H |f )] , Lp .

(23)

The derivation of Lp follows Equations 12-19. The resulting bound is the same as L except m, S are replaced with
m‚àó and S‚àó , and there is no KL divergence term. Kernel
matrices are computed using Œò‚àó .
The tightness of this final bounding step will be a function
of how well the inducing variables u define the function
f . Intuitively, when the variance at the inducing points is
large, the entropy of f will be large, as it is unrestricted.
From an information theoretic perspective, we can say that
the tightness of the bound will be a function of the entropy
of f : H(f ) = 21 log |Œ£ÃÉ| + const.
Given this knowledge we define a second, tightened, lower
bound L0 , where we allow the variance of function values at the inducing points to collapse to zero: p(u|D) ‚âà
Œ¥(m). This reduces the conditional entropy of f given u by
shrinking the final term in the definition of Œ£ÃÉ (Equation 10),
resulting in a tighter bound for a slightly restricted class of
models. As it is a slightly more restricted model, we expect the ground truth M0 = log Eq(f |u=m) [p(H|f )] to
be lower than the ground truth Mp . In practice, however,
because the variational L0 is so much tighter, we also use
this to give results for approximate predictive likelihood
when comparing against other approaches. In Figure 7 we
demonstrate this empirically for the coal mining dataset by
evaluating the true predictive bounds on a held-out 50% of
the data via 10, 000 MCMC samples, and shading between
these and the variational approximations. We do this for a
range of inducing point grid densities, both with and without optimisation of the inducing point locations. In Figures
6, 5, and 7, we plot all of the bounds described above as the
number of inducing points increase. We note that for the
relatively smooth coal mining data, (Figure 7), all bounds
do not benefit from more than about 10 inducing points,
while in the case of the twitter data, the faster dynamics
call for increased numbers. In all Figures the tightness of
L0 is evident as compared to Lp .

5. Alternative GP Transformations
At this point it is worth considering why we have chosen
the function transformation Œª(x) = f 2 (x) in preference
to other alternatives we might have used. An obvious first
choice would be

Œª(x) = exp f (x) .
(24)
This transformation is undesirable for two reasons. The
more obvious of these is that after taking expectations under q(f ) we are left with the integral


Z
œÉÃÉ 2
(25)
‚àí
exp ¬µÃÉx + x dx,
2
T

which cannot be computed in closed form. We could approximate the integral using a series expansion, however
this would be difficult with more than a couple of terms and
furthermore, since the function is concave, this approximation would not be a lower bound.
The second‚Äîand more subtle‚Äîreason is that in using this
transformation, when we take expectations under q(f ) of
the data, we obtain
Eq(f ) [log {exp(fn )}] = ¬µÃÉn .

(26)

Since the mean, ¬µÃÉn , is not a function of S, the variance
of the variational distribution q(u), we have effectively decoupled the data from the uncertainty on our variational
approximation; this is clearly undesirable.
Another possible candidate is the probit function, Œª(x) =
Œ¶(f (x)). This can be integrated analytically against the
GP prior, however we are again left with a difficult integral
over T which is
!
Z
¬µÃÉx
dx.
(27)
‚àí
Œ¶ p
1 + œÉÃÉx2
T

As the range of this transformation is (0, 1) we would also
require additional machinery to infer a scaling variable.

In contrast the square transform presented allows the integral over the region T to be computed in closed form and
Eq(f ) [log fn2 ] can be computed quickly and accurately. Importantly this transformation also maintains the connection
between the data and the variational uncertainty.
Although the square transform is not a one-to-one
function‚Äîany rate function Œª may have been generated
by f 2 or (‚àíf )2 ‚Äîthis sign ambiguity is integrated out in
a Bayesian sense, Equation 18.

6. Relationship to Sparse GP Models
The use of inducing points in our model relates it to a wide
range of sparse Gaussian process models, e.g. SPGP (Snelson & Ghahramani, 2005). The variational sparse Gaussian

Variational Inference for Gaussian Process Modulated Poisson Processes

process framework was introduced by Titsias (2009), however the bound we develop is more akin to the ‚ÄúBig-Data‚Äù
GP bound (Hensman et al., 2013), since we explicitly maintain the variational distribution q(u). The variable Œ® that
results from integrating the kernel over the input domain is
similar to the so-called ‚ÄúŒ®-statistic‚Äù which arises when integrating out the uncertainty of latent variables in the variational GPLVM (Titsias & Lawrence, 2010).

Our SGCP sampler is based on Adams et al. (2009). Our
implementation differs by using elliptical slice sampling to
infer the latent function f and we perform hyper-parameter
inference using Hybrid Monte-Carlo (HMC). We also use
the ‚Äúadaptive-thinning‚Äù method described in Gunter et al.
(2014a), to reduce the number of thinning points required.
7.2. Synthetic Data

7. Experiments
To evaluate our algorithm, we benchmarked against our frequentist kernel smoothing approach, described below, both
with without end correction (KS+EC and KS‚ÄìEC respectively), and a fully Bayesian SGCP MCMC sampler. Our
test data sets are generative data from the SGCP model and
several real-world data sets.
7.1. Benchmarks
Our kernel smoothing method is similar to standard kernel
density estimation except we use truncated normal kernels
to account for our explicit knowledge of the domain‚Äîthe
latter is referred to as ‚Äúend-correction‚Äù in some literature
(Diggle, 1985). The kernel smoother optimises a diagonal
covariance, Œ£‚àó , by maximising the leave-one-out training
objective
Œ£‚àó = argmax
Œ£

N
X
i=1

log

N
X

j6=i=1

NT (x(i) ; x(j) , Œ£).

(28)

We can construct the predictive distribution by combining
the maximum-likelihood estimates of the size and spatial
location of the data. For a test data set H (with K! permutations) this distribution is
p(H |D) = K! p(K|D)

K
Y

k=1

p(xÃÉ(k) |D),

(29)

Figure 2. 2D Synthetic Data. Clockwise from top left: Ground
truth, VBPP, KS+EC, SGCP.

For the synthetic data sets, we first generate a 2D function
from a high resolution grid using a Gaussian process and
sigmoid link function, and then, conditioned on that function, we draw a training dataset and multiple test datasets.
We give average performances results for these test datasets
in Table 1. Figure 2 visualises an inferred 2D intensity conditioned on ‚àº 500 observations. Although the SGCP sampler gives better predictive performance than the VBPP L0
bound, it should be noted that the sampler uses well tuned
hyper-parameters, uses the same link function as the generative process and is much more computationally expensive.
VBPP outperforms kernel smoothing in terms of both predictive likelihood and RMS error.

where the location density,
p(xÃÉ

(k)

N
1 X
NT (xÃÉ(k) ; x(n) , Œ£‚àó ),
|D) =
N n=1

7.3. Real Data
(30)

is computed using using the previously described method
and the distribution of the number of points,
p(K|D) =

NK
exp(‚àíN ),
K!

(31)

is simply a Poisson distribution with parameter N . It is
straight forward to show that Equation 29 is equivalent
to Equation
interpret the rate
R function as
PN1 since we can
(n)
‚àó
Œª(x)dx =
Œª(x) =
N
(x;
x
,
Œ£
)
and
since
n=1 T
T
N.

We next investigate three real world data sets. For these
data sets we create training and test subsets by allocating
each point to either subset with probability 0.5. Since true
rates are unknown for these datasets we rely on held-out
predictive likelihood as the only performance metric.
7.3.1. C OAL M INING D ISASTER DATA
Our first real dataset comprises 190 events recorded from
March 15, 1851 to March 22, 1962; each represents a coalmining disaster that killed at least ten people in the United
Kingdom. These data, first analysed in this form in 1979
(Jarrett, 1979), have often been tackled with nonhomoge-

Variational Inference for Gaussian Process Modulated Poisson Processes

Table 1. Results for 2D synthetic data (drawn from the SGCP generative process).

Avg. LL
446.1
-61.1
122.4
175.8
446.1

SGCP
RMS
1.37
0.38
0.88
1.71
2.94

Time(s)
7547.83
1039.65
3173.91
3773.75
6368.44

3.5
3
2.5
2
1.5
1
0.5
0
1860

1900
1920
1940
Year
Figure 3. Coal mining disaster data.

KS+EC
RMS Time(s)
1.48
0.34
0.46
0.02
1.04
0.12
1.26
0.05
2.02
0.21

35
30
Data
25
VB (Lp )
20
VB (L0 )
15
KS+EC
10
KS-EC
SGCP
5
VB 25-75%
0
11 Sept

1880

1960

160

250

140

200

120
100

L
006
005
004 L ‚Üí M
p
p
000
001
002
003

80

L0 ‚Üí M0

60

KS+EC
SGCP

Log Likelihood

Log Likelihood

Avg. LL
389.8
-78.3
84.3
147.0
413.6

VBPP (L0 )
Avg. LL RMS Time(s)
392.9
1.21
3.26
-76.1
0.38
2.00
92.6
0.81
2.44
148.3
1.14
2.58
415.5
1.81
2.83

Tweets per day

Deaths per year

Function
1
2
3
4
5

21 Sept
Date
Figure 4. Twitter data

16 Sept

26 Sept

150
100
50
0

40
15
25
30
20
35
Number of Inducing Points
Figure 5. Twitter data (Z on a fixed grid).
10

40

neous Poisson processes, (Adams et al., 2009), as the rate
of such disasters is expected to vary according to known
historical developments. The events are indicated by the
rug plot along the axis of Figure 3.
Our inferred intensity of disasters correlates with the historical introduction of safety regulation, as noted in previous
work on this data (Fearnhead, 2006; Carlin et al., 1992).
Firstly, our results depict a decline in the rate of such disasters throughout 1870‚Äì1890, a period that saw the UK
parliament passing several acts with the aim of improving
safety for mine workers, including the Coal Mines Regulation Acts of 1872 and 1887. Our inferred intensity also declines after 1950, likely related to the imposition of further
safety regulation with the Mines and Quarries Act, 1954.
Predictive log-likelihood values on held out data (Figure 7)
are also encouraging. VBPP outperforms kernel smoothing
and SGCP with as few as 10 inducing points; more inducing points yielding no further benefit.

15
20
30
35
25
Number of Inducing Points
Figure 6. Twitter data (Z optimised).

10

40

7.3.2. T WITTER DATA
Next, we ran the models on the tweet profile of the chairman of the ‚ÄòBetter Together Campaign‚Äô, Alistair Darling,
one week either side of the Scottish independence election
(189 tweets). Results are shown in Figure 4 and Table 2,
where half the data was held out and a regular 31 point grid
was used. Figures 5 and 6 compare the performance of
regularly spaced and optimised inducing points, and show
optimisation yields considerably improved performance on
this dataset. The L0 and Lp bounds become less tight as the
number of inducing points is increased, suggesting there is
less uncertainty represented in the variational parameter S
and more uncertainty captured by a reduced kernel length
scale. This transition is observed for fewer inducing points
when inducing point optimisation is employed. Both with
and without inducing point optimisation, VBPP M0 and
Mp outperform both the SGCP and kernel smoothing by
a wide margin, suggesting the square link function is an
appropriate model for this data.

Variational Inference for Gaussian Process Modulated Poisson Processes
‚àí90

Table 2. Run times for 1D data sets.
Method Coal Mining Twitter
VBPP
0.7
0.5
KS+EC
0.0
0.3
KS-EC
0.0
0.2
SGCP
417.6
230.0

Log Likelihood

‚àí92
‚àí94
‚àí96
‚àí98

Table 3. Test log-likelihood for 2D Malaria data.
KS-EC KS+EC VBPP(Mp ) VBPP(L0 )
855.0
867.2
869.7
855.9

‚àí100
‚àí102
14 16 18 20 22 24 26 28 30
Number of Inducing Points
Figure 7. Coal Mining Data set: The difference between sampling
M0 and Mp , and the corresponding lower bounds, L0 and Lp
(see Section 4.6). The figure clearly demonstrates the tightness of
the L0 bound. Legend as in Figure 5.
8

10

12

Figure 8. A sample of 741 malaria incidences in Kenya, which
occured over the course of 1985-2010, and the associated VBPP
intensity function. 20 √ó 20 inducing points.

8. Further Work
Although the performance of the variational Bayesian point
process inference algorithm described in this paper improves upon standard methods when used in isolation, it
is in its extensions that its utility will be fully realised.
Previous work (Gunter et al., 2014a) has shown that hierarchical modelling of point processes‚Äîstructured point
processes‚Äîcan significantly improve predictive accuracy.
In these multi-output models, statistical strength is shared
across multiple rate processes via shared latent processes.
The method presented here provides a likelihood model for
point-process data that can be incorporated as a probabilistic building-block into these larger interconnected models. That is, our fully generative model can readily be extended to additionally incorporate other observation modalities. For example, real-valued observations such as (log-)
household income could be modelled along with the intensity function over crime incidents using a variational
multi-output GP framework. Future work will be aimed towards developing these variational structured point process
algorithms and integration of these techniques to popular
Gaussian process tool-kits, such as GPy (The GPy authors,
2014).

9. Conclusion
7.3.3. M ALARIA DATA
We expect that a major application of the contributions presented in this paper is the joint modelling of disease incidence with correlating factors, in a fully Bayesian, scalable framework. For example, those studying the spread of
malaria often wish to use continuous rainfall measurements
to better inform their epidemiological models. We use examples from the Malaria Atlas Project (2014) to test our
scheme. We extracted 741 incidences of malaria outbreak
documented in Kenya between 1985 and 2010, and ran our
VBPP algorithm and kernel smoothing on approximately
half of the resulting dataset, holding out the remainder for
testing. Test log-likelihood results, given in Table 3, show
VBPP performs comparably to kernel smoothing.

Point process models have hitherto been hindered by their
scaling with the number of data. To address this problem,
we propose a new model, accommodating non-discretised
intensity functions, that permits linear scaling. We additionally contribute a variational Bayesian inference scheme
that delivers rapid and accurate prediction. The scheme is
validated on real datasets including the canonical coal mining disaster data set and malaria incidence in Kenya data.
ACKNOWLEDGEMENTS
The authors would like to thank James Hensman and Neil
Lawrence for helpful discussions. Chris Lloyd is funded by
a DSTL National PhD Scheme Studentship. Tom Gunter is
supported by UK Research Councils.

Variational Inference for Gaussian Process Modulated Poisson Processes

References
Adams, R. P., Murray, I., and MacKay, D. J. C. Tractable
Nonparametric Bayesian Inference in Poisson Processes
with Gaussian Process Intensities. In Proceedings of
the 26th Annual International Conference on Machine
Learning, ICML ‚Äô09, pp. 9‚Äì16, New York, NY, USA,
2009. ACM.
Ancarani, L. U. and Gasaneo, G. Derivatives of any order
of the confluent hypergeometric function 1 F1 (a, b, z)
with respect to the parameter a or b. Journal of Mathematical Physics, 49(6), 2008.
Basu, S. and Dassios, A. A Cox process with log-normal
intensity. Insurance: Mathematics and Economics, 31
(2):297302, Oct 2002.
Carlin, B. P., Gelfand, A. E., and Smith, A. F. M. Hierarchical Bayesian analysis of changepoint problems. Journal
of the Royal Statistical Society. Series C (Applied Statistics), 41(2):389405, Jan 1992.
Cunningham, J. P., Shenoy, K. V., and Sahani, M. Fast
Gaussian process methods for point process intensity
estimation. In Proceedings of the 25th international
conference on Machine learning, pp. 192‚Äì199. ACM,
2008a.
Cunningham, J. P., Yu, B. M., Shenoy, K. V., and Sahani,
M. Inferring neural firing rates from spike trains using
Gaussian processes. In Advances in Neural Information
Processing Systems 20, 2008b.
Diggle, P. A kernel method for smoothing point process
data. Journal of the Royal Statistical Society. Series
C (Applied Statistics), 34(2):pp. 138‚Äì147, 1985. ISSN
00359254.
Fearnhead, P. Exact and efficient Bayesian inference for
multiple changepoint problems. Statistics and Computing, 16(2):203213, Jun 2006.
Gunter, T., Lloyd, C., Osborne, M. A., and Roberts, S. J.
Efficient Bayesian nonparametric modelling of structured point processes. In Uncertainty in Artificial Intelligence (UAI), 2014a.
Gunter, T., Osborne, M. A., Garnett, R., Hennig, P., and
Roberts, S. Sampling for inference in probabilistic models with fast Bayesian quadrature. In Cortes, C. and
Lawrence, N. (eds.), Advances in Neural Information
Processing Systems (NIPS), 2014b.
Heikkinen, J. and Arjas, E. Modeling a Poisson forest
in variable elevations: A nonparametric Bayesian approach. Biometrics, 55(3):738745, Sep 1999.

Hensman, J., Fusi, N., and Lawrence, N. D. Gaussian Processes for Big Data. CoRR, abs/1309.6835, 2013.
Jarrett, R. G. A note on the intervals between coal-mining
disasters. Biometrika, 66(1):191193, 1979.
Kingman, J. F. C. Poisson Processes (Oxford Studies in
Probability). Oxford University Press, jan 1993. ISBN
0198536933.
Lasko, T. A. Efficient Inference of Gaussian Process Modulated Renewal Processes with Application to Medical
Event Data. In Uncertainty in Artificial Intelligence
(UAI), 2014.
Malaria Atlas Project. http://www.map.ox.ac.uk/
explore/data-modelling/, 2014. [Online; accessed 24-October-2014].
Mller, J., Syversveen, A. R., and Waagepetersen, R. P.
Log Gaussian Cox Processes. Scandinavian Journal of
Statistics, 25(3):451‚Äì482, 1998. ISSN 1467-9469.
Osborne, M. A., Roberts, S. J., Rogers, A., and Jennings,
N. R. Real-time information processing of environmental sensor network data. ACM Transactions on Sensor
Networks, 9(1), 2012.
Rasmussen, C. E. and Williams, C. K. I. Gaussian Processes for Machine Learning. The MIT Press, 2nd edition 2006 edition, 2006. ISBN 0-262-18253-X.
Rathbun, S. L. and Cressie, N. Asymptotic properties of
estimators for the parameters of spatial inhomogeneous
Poisson point processes. Advances in Applied Probability, pp. 122154, 1994.
Snelson, E. and Ghahramani, Z. Sparse Gaussian Processes
using Pseudo-inputs. In Advances in Neural Information
Processing Systems (NIPS), 2005.
The GPy authors. GPy: A Gaussian process framework in
Python. https://github.com/SheffieldML/
GPy, 2014.
Titsias, M. K. Variational learning of inducing variables in
sparse Gaussian processes. In In Artificial Intelligence
and Statistics 12, pp. 567‚Äì574, 2009.
Titsias, M. K. and Lawrence, N. D. Bayesian Gaussian
process latent variable model. In Artificial Intelligence
and Statistics 10, 2010.

