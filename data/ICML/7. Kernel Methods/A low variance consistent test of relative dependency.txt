A low variance consistent test of relative dependency

Wacha Bounliphone
WACHA . BOUNLIPHONE @ CENTRALESUPELEC . FR
CentraleSupélec & Inria, Grande Voie des Vignes, 92295 Châtenay-Malabry, France
Arthur Gretton
ARTHUR . GRETTON @ GMAIL . COM
Gatsby Computational Neuroscience Unit, University College London, United Kingdom
Arthur Tenenhaus
CentraleSupélec, 3 rue Joliot-Curie, 91192 Gif-Sur-Yvette, France

ARTHUR . TENENHAUS @ CENTRALESUPELEC . FR

Matthew B. Blaschko
MATTHEW. BLASCHKO @ INRIA . FR
Inria & CentraleSupélec, Grande Voie des Vignes, 92295 Châtenay-Malabry, France

Abstract
We describe a novel non-parametric statistical
hypothesis test of relative dependence between
a source variable and two candidate target variables. Such a test enables us to determine
whether one source variable is significantly more
dependent on a first target variable or a second. Dependence is measured via the HilbertSchmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). We test
whether the first dependence measure is significantly larger than the second. Modeling the
covariance between these HSIC statistics leads
to a provably more powerful test than the construction of independent HSIC statistics by subsampling. The resulting test is consistent and
unbiased, and (being based on U-statistics) has
favorable convergence properties. The test can
be computed in quadratic time, matching the
computational complexity of standard empirical HSIC estimators. The effectiveness of the
test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression
than chromosomal imbalances. Source code is
available for download at https://github.
com/wbounliphone/reldep.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

1. Introduction
Tests of dependence are important tools in statistical analysis, and are widely applied in many data analysis contexts. Classical criteria include Spearman’s ρ and Kendall’s
τ , which can detect non-linear monotonic dependencies.
More recent research on dependence measurement has focused on non-parametric measures of dependence, which
apply even when the dependence is nonlinear, or the variables are multivariate or non-euclidean (for instance images, strings, and graphs). The statistics for such tests are
diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois & Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Székely
et al., 2007; Sejdinovic et al., 2013b), kernel regression
tests (Cortes et al., 2009; Gunn & Kandola, 2002), rankings (Heller et al., 2013), and space partitioning approaches
(Gretton & Gyorfi, 2010; Reshef et al., 2011; Kinney & Atwal, 2014). Specialization of such methods to univariate
linear dependence can yield similar tests to classical approaches such as Darlington (1968); Bring (1996).
For many problems in data analysis, however, the question
of whether dependence exists is secondary: there may be
multiple dependencies, and the question becomes which
dependence is the strongest. For instance, in neuroscience,
multiple stimuli may be present (e.g. visual and audio), and
it is of interest to determine which of the two has a stronger
influence on brain activity (Trommershauser et al., 2011).
In automated translation (Peters et al., 2012), it is of interest to determine whether documents in a source language
are a significantly better match to those in one target language than to another target language, either as a measure
of difficulty of the respective learning tasks, or as a basic
tool for comparative linguistics.

A low variance consistent test of relative dependency

We present a statistical test which determines whether two
target variables have a significant difference in their dependence on a third, source variable. The dependence between each of the target variables and the source is computed using the Hilbert-Schmidt Independence Criterion
(Gretton et al., 2005; 2008).1 Care must be taken in analyzing the asymptotic behavior of the test statistics, since
the two measures of dependence will themselves be correlated: they are both computed with respect to the same
source. Thus, we derive the joint asymptotic distribution
of both dependencies. The derivation of our test utilizes
classical results of U -statistics (Hoeffding, 1963; Serfling,
1981; Arcones & Gine, 1993). In particular, we make use
of results by Hoeffding (1963) and Serfling (1981) to determine the asymptotic joint distributions of the statistics (see
Theorem 4). Consequently, we derive the lowest variance
unbiased estimator of the test statistic.
We prove our approach to have greater statistical power
than constructing two uncorrelated statistics on the same
data by subsampling, and testing on these. In experiments,
we are able to successfully test which of two variables is
most strongly related to a third, in synthetic examples, in a
language group identification task, and in a task for identifying the relative strength of factors for Glioma type in a
pediatric patient population.
To our knowledge, there do not exist competing nonparametric tests to determine which of two dependencies
is strongest. One related area is that of multiple regression
analysis (e.g. (Sen & Srivastava, 2011)). In this case a linear model is assumed, and it is determined whether individual inputs have a statistically significant effect on an output
variable. The procedure does not address the question of
whether the influence of one variable is higher than that of
another to a statistically significant degree. The problem of
variable selection has also been investigated in the case of
nonlinear relations between the inputs and outputs (Cortes
et al., 2009; 2012; Song et al., 2012), however this again
does not address which of two variables most strongly influences a third. A less closely related area is that of detecting three-variable interactions (Sejdinovic et al., 2013a),
where it is determined whether there exists any factorization of the joint distribution over three variables. This test
again does not address the issue of finding which connections are strongest, however.
1

Dependency can also be tested with the correlation operator.
However, Fukumizu et al., (2007) show that unlike the covariance
operator, the asymptotic distribution of the norm of the correlation
operator is unknown, so the construction of a computationally efficient test of relative dependence remains an open problem.

2. Definitions and description of HSIC
We base our underlying notion of dependence on the
Hilbert-Schmidt Independence Criterion (Gretton et al.,
2005; 2008; Song et al., 2012). All results in this section
except for Problem 1 can be found in these previous works.
Definition 1. (Gretton et al., 2005, Definition 1,Lemma 1:
Hilbert-Schmidt Independence Criterion)
Let Pxy be a Borel probability measure over over (X ×
Y, Γ × Λ) with Γ and Λ the respective Borel sets on X and
Y, and Px and Py the marginal distributions on domains
X and Y. Given separable RKHSs F and G, the HilbertSchmidt Independence Criterion (HSIC) is defined as the
squared HS-norm of the associated cross-covariance operator Cxy . When the kernels k, l are associated uniquely
withs respective RKHSs F and G and bounded, HSIC can
be expressed in terms of expectations of kernel functions
HSIC(F, G, Pxy ) := kCxy k2HS
= Exx0 yy0 [k(x, x0 )l(y, y 0 )] + Exx0 [k(x, x0 )] Eyy0 [l(y, y 0 )]
− 2Exy [Ex0 [k(x, x0 )]Ey0 [l(y, y 0 )]] .

(1)

HSIC determines independence: HSIC = 0 iff Pxy = Px Py
when kernels k and l are characteristic on their respective
marginal domains (Gretton, 2015).
With this choice, the problem we would like to solve is
described as follows:
Problem 1. Given separable RKHSs F, G, and H with
HSIC(F, G, Pxy ) > 0 and HSIC(F, H, Pxz ) > 0,
we test the null hypothesis H0 : HSIC(F, G, Pxy ) ≤
HSIC(F, H, Pxz ) versus the alternative hypothesis H1 :
HSIC(F, G, Pxy ) > HSIC(F, H, Pxz ) at a given significance level α.
We now describe the asymptotic behavior of the HSIC for
dependent variables.
Theorem 1. (Song et al., 2012, Theorem 2: Unbiased
estimator for HSIC(F, G, Pxy )) We denote by S the set
of observations {(x1 , y1 ), ..., (xm , ym )} of size m drawn
i.i.d. from Pxy . The unbiased estimator HSICm (F, G, S)
is given by
1
×
(2)
HSICm (F, G, S) =
m(m − 3)
"
#
10 K̃110 L̃1
2
Tr(K̃L̃) +
−
10 K̃L̃1
(m − 1)(m − 2) m − 2
where K̃ and L̃ are related to K and L by K̃ij = (1 −
δij )K̃ij and L̃ij = (1 − δij )L̃ij .
Theorem 2. (Song et al., 2012, Theorem 3: U-statistic of
XY
HSIC) This finite sample unbiased estimator of HSICm

A low variance consistent test of relative dependency

can be written as a U-statistic,
XY
HSICm
= (m)−1
4

X

hijqr

(3)

(i,j,q,r)∈im
4

m!
, the index set im
4 denotes the
(m − 4)!
set of all 4−tuples drawn without replacement from the set
{1, . . . m}, and the kernel h of the U-statistic is defined as
where (m)4 :=

XY
XZ
dij = d(zi , zj ). Let HSICm
and HSICm
be respectively the unbiased estimators of HSIC(F, G, Pxy ) and
HSIC(F, H, Pxz ), written as a sum of U-statistics with
respective kernels hijqr and gijqr as described in (4),

hijqr =

gijqr =
hijqr =

1
24

(i,j,q,r)

X

kst (lst + luv − 2lsu )

(4)

(s,t,u,v)

where the kernels k and l are associated uniquely with respective reproducing kernel Hilbert spaces F and G.
Theorem 3. (Gretton et al., 2008, Theorem 1: Asymptotic
distribution of HSICm ) If E[h2 ] < ∞, and source and
targets are not independent, then, under H1 , as m → ∞,

1
24
1
24

(i,j,q,r)

X

kst (lst + luv − 2lsu ),

(s,t,u,v)
(i,j,q,r)

X

kst (dst + duv − 2dsu ).

(6)

(s,t,u,v)

Theorem 4. (Joint asymptotic distribution of HSIC) If
E[h2 ] < ∞ and E[g 2 ] < ∞, then

 

XY
√
HSIC(F, G, Pxy )
HSICm
m
−
XZ
HSIC(F, H, Pxz )
HSICm
  

2
d
σXY
σXY XZ
0 ,
−→ N
,
(7)
2
0
σXY XZ
σXZ

2
2
are as in Theorem 3.
The
and σXZ
where σXY
empirical estimate of σXY XZ is
σ̂XY XZ
=
− HSIC(F, G, Pxy )) −→
)

(5)  16 R
XZ
XY

, where
XY XZ − HSICm HSICm
2
2
m
where σXY
= 16 Ei (Ej,q,r hijqr ) − HSIC(F, G, Pxy ))


m
with Ej,q,r := ESj ,Sq ,Sr .
Its empirical
estiX
X

1
XY 2
(m − 1)−2
RXY XZ =
hijqr gijqr  .
mate is σ̂XY = 16 RXY − (HSICm
)
where
3
m

2
m
i=1
(j,q,r)∈i3 \{i}
m
X
1 X
(8)
−1

RXY =
(m − 1)3
hijqr
and
m
i=1
(j,q,r)∈im
3 \{i}
Proof. Eq. (8) is constructed with the definition of varim
the index set i3 \ {i} denotes the set of all 3−tuples drawn
ance of a U-statistic as given by Serfling, Ch. 5 (1981),
without replacement from the set {1, . . . m} \ {i}.
where one variable is fixed. Eq. (7) follows from the application of Hoeffding, Theorem 7.1 (1963), which gives the
3. A test of relative dependence
joint asymptotic distribution of U-statistics.

√

XY
m(HSICm

d

2
N (0, σXY

In this section we calculate two dependent HSIC statistics
and derive the joint asymptotic distribution of these dependent quantities, which is used to construct a consistent test
for Problem 1. We next construct a simpler consistent test,
by computing two independent HSIC statistics on sample
subsets. While the simpler strategy is superficially attractive and less effort to implement, we prove the dependent
strategy is strictly more powerful.
3.1. Joint asymptotic distribution of HSIC and test
In the present section, we compute each HSIC estimate
on the full dataset, and explicitly obtain the correlations
between the resulting empirical dependence measurements
XY
XZ
HSICm
and HSICm
. We denote by S1 = (X, Y, Z)
the joint sample of observations which are drawn i.i.d. with
respective Borel probability measure Pxyz defined on the
domain X × Y × Z. The kernels k, l and d are associated
uniquely with respective reproducing kernel Hilbert spaces
F, G and H. Moreover, K, L and D ∈ Rm×m are kernel
matrices containing kij = k(xi , xj ), lij = l(yi , yj ) and

Based on the joint asymptotic distribution of HSIC described in Theorem 4, we can now describe a statistical
test to solve Problem 1: given a sample S1 as described
in Section 3.1, T (S1 ) : {(X × Y × Z)m } → {0, 1} is
used to test the null hypothesis H0 : HSIC(F, G, Pxy ) ≤
HSIC(F, H, Pxz ) versus the alternative hypothesis H1 :
HSIC(F, G, Pxy ) > HSIC(F, H, Pxz ) at a given significance level α. This is achieved by projecting the distriXY
XZ
bution to 1D using the statistic HSICm
− HSICm
,
and determining where the statistic falls relative to a
conservative estimate of the the 1 − α quantile of the
null. We now derive this conservative estimate. A simple way of achieving this is to rotate the distribution by
π
4 counter-clockwise about the origin, and to integrate
the resulting distribution projected onto the first axis (cf.
Fig.
the asymptotically normal distribution of
√ 3). Denote
XY
XZ T
m[HSICm
HSICm
] as N (µ, Σ). The distribution
resulting from rotation and projection is

N [Qµ]1 , [QΣQT ]11 ,
(9)

A low variance consistent test of relative dependency

√ 
2 1 −1
where Q =
is the rotation matrix by π4 and
2 1 1
√
2
[Qµ]1 =
(HSIC(F, G, Pxy ) − HSIC(F, H, Pxz )) ,
2
(10)
1
2
2
[QΣQT ]11 = (σXY
+ σXZ
− 2σXY XZ ).
(11)
2
Following the empirical distribution from Eq. (9), a test
XY
XZ
with statistic HSICm
− HSICm
has p-value
!
XY
XZ
(HSICm
− HSICm
)
,
(12)
p≤1−Φ p 2
2
σXY + σXZ
− 2σXY XZ
where Φ is the CDF of a standard normal distribution, and
we have made the most conservative possible assumption
that HSIC(F, G, Pxy ) − HSIC(F, H, Pxz ) = 0 under
the null (the null also allows for the difference in population
dependence measures to be negative).
To implement the test in practice, the variances of
2
2
2
and σXY
, σXZ
σXY
XZ may be replaced by their empirical estimates. The test will still be consistent for a large
enough sample size, since the estimates will be sufficiently
well converged to ensure the test is calibrated. Eq. (8) is
expensive to compute naı̈vely, because even computing the
kernels hijqr and gijqr of the U -statistic itself is a non
trivial task. Following (Song et al., 2012, Section 2.5),
we
P first form a vector hXY with entries corresponding to
hijqr , and a vector hXZ with entries cor(j,q,r)∈im
3 \{i}P
responding to (j,q,r)∈im \{i} gijqr . Collecting terms in

Then for m > 1 and all δ > 0 with probability at least
1 − δ, for all pxyz , the generalization bound on the difference of empirical HSIC statistics is
| {HSIC(F, G, Pxy ) − HSIC(F, H, Pxz )}

	
XY
XZ
− HSICm
− HSICm
|
(r
)
log(6/δ)
C
≤2
+
(14)
α2 m
m
where α > 0.24 and C are constants.
Proof. In Gretton et al., (2005) a finite sample bound is
given for a single HSIC statistic. Eq. (14) is proved by
using a union bound.
XY
XZ
Corollary 1. HSICm
−HSIC
m converges to the pop√
ulation statistic at rate O( m).

3.2. A simple consistent test via uncorrelated HSICs
From the result in Eq. (5), a simple, consistent test of
relative dependence can be constructed as follows: split
the samples from Px into two equal sized sets denoted
by X 0 and X 00 , and drop the second half of the sample pairs with Y and the first half of the sample pairs
with Z. We will denote the remaining samples as Y 0
and Z 00 . We can now estimate the joint distribution of
√
X0Y 0
X 00 Z 00 T
m[HSICm/2
, HSICm/2
] as

N

3

Eq. (4) related to kernel matrices K̃ and L̃, hXY can be
written as


hXY = (m − 2)2 K̃  L̃ 1 − m(K̃1)  (L̃1) (13)


+ (m − 2) (Tr(K̃L̃))1 − K̃(L̃1) − L̃(K̃1)
+ (1T L̃1)K̃1 + (1T K̃1)L̃1 − ((1T K̃)(L̃1))1
where  denotes the Hadamard product. Then RXY XZ
in Eq. (8) can be computed as RXY XZ = (4m)−1 (m −
T
1)−2
3 hXY hXZ . Using the order of operations implied by
the parentheses in Eq. (13), the computational cost of the
cross covariance term is O(m2 ). Combining this with the
unbiased estimator of HSIC in Eq. (2) leads to a final computational complexity of O(m2 ).
In addition to the asymptotic consistency result, we provide
a finite sample bound on the deviation between the difference of two population HSIC statistics and the difference
of two empirical HSIC estimates.
Theorem 5 (Generalization bound on the difference of
empirical HSIC statistics). Assume that k, l, and d are
bounded almost everywhere by 1, and are non-negative.

 
σ2
HSIC(F, G, Pxy )
, X0Y 0
HSIC(F, H, Pxz )
0

0
2
σX
00 Z 00


, (15)

which we will write as N (µ0 , Σ0 ). Given this joint
distribution, we need to determine the distribution over
the half space defined by
HSIC(F, G, Pxy ) <
HSIC(F, H, Pxz ). As in the previous section, we achieve
this by rotating the distribution by π4 counter-clockwise
about the origin, and integrating the resulting distribution
projected onto the first axis (cf. Fig. 3). The resulting projection of the rotated distribution onto the primary axis is

 
N [Qµ0 ]1 , QΣ0 QT 11
(16)
where
√

2
(HSIC(F, G, Pxy ) − HSIC(F, H, Pxz )) ,
2
(17)
1
2
2
(18)
[QΣ0 QT ]11 = (σX
0 Y 0 + σX 00 Z 00 ).
2
0

[Qµ ]1 =

From this empirically estimated distribution, it is straightforward to construct a consistent test (cf. Eq. (12)). The
power of this test varies inversely with the variance of the
distribution in Eq. (16).

A low variance consistent test of relative dependency

3.3. The dependent test is more powerful
While discarding half the samples leads to a consistent test,
we might expect some loss of power over the approach in
Section 3.1, due to the increase in variance with lower sample size. In this section, we prove the Section 3.1 test is
more powerful than that of Section 3.2, regardless of Pxy
and Pxz .

where we again make the most conservative possible assumption that HSIC(F, G, Pxy ) − HSIC(F, H, Pxz ) =
0 under the null. The Type II error probability of the dependent test at level α is

m−1/2 HSIC(F, G, Pxy )
 −1
−HSIC(F, H, Pxz ) 

Φ
Φ (1 − α) − pσ 2 + σ 2 − 2σ



XY

We call the simple and consistent approach in Section 3.2,
the independent approach, and the lower variance approach
in Section 3.1, the dependent approach. The following theorem compares these approaches.
Theorem 6. The asymptotic relative efficiency (ARE) of the
independent approach relative to the dependent approach
is always greater than 1.
Remark 1. The asymptotic relative efficiency (ARE) is defined in e.g. Serfling (1981, Chap.5, Section 1.15.4). If mA
and mB are the sample sizes at which tests ”perform equivmA
reprealently” (i.e. have equal power), then the ratio m
B
sents the relative efficiency. When mA and mB tend to +∞
mA
and the ratio m
→ L (at equivalent performance), then
B
the value L represents the asymptotic relative efficiency of
procedure B relative to procedure A. This example is relevant to our case since we are comparing two test statistics
with different asymptotically Normal distributions.
The following lemma is used for the proof of Theorem 6.
Lemma 1. (Lower Variance) The variance of the dependent test statistic is smaller than the variance of the independent test statistic.
Proof. From the convergence of moments in the application of the central limit theorem (von Bahr, 1965), we
2
2
have that σX
0 Y 0 = 2σXY . Then the variance summary
2
2
in Eq. (11) is 12 (σXY
+ σXZ
− 2σXY XZ ) and the variance
2
2
summary in Equation (18) is 21 (2σXY
√ + 2σXZ ) where in
both cases the statistic is scaled by m. We have that the
variance of the independent test statistic is smaller than the
variance of the dependent test statistic when
1 2
1
2
2
2
(σXY + σXZ
− 2σXY XZ ) < (2σXY
+ 2σXZ
)
2
2
2
2
⇐⇒ −2σXY XZ < σXY
+ σXZ
(19)

XZ

(21)

XY XZ

where Φ is the CDF of the standard normal distribution.
The numerator in Eq. (20) is the same as the numerator in
Eq. (21), and the denominator in Eq. (21) is smaller due to
Lemma 1. The lower variance dependent test therefore has
higher ARE, i.e., for a sufficient sample size m > τ for
some distribution dependent τ ∈ N+ , the dependent test
will be more powerful than the independent test.

4. Generalizing to more than two HSIC
statistics
The generalization of the dependence test to more
than three random variables follows from the earlier
derivation by applying successive rotations to a higher
dimensional joint Gaussian distribution over multiple
HSIC statistics. We assume a sample S of size m
over n domains with kernels k1 , . . . , kn associated
uniquely with respective reproducing kernel Hilbert
spaces F1 , . . . , Fn . We define a generalized statistical test, P
Tg (S) → {0, 1} to test the null hypothesis
H0 :
≤
(x,y)∈{1,...,n}2 v(x,y) HSIC(Fx , Fy , Pxy )
0P versus the alternative hypothesis Hm
:
v
HSIC(F
,
F
,
P
)
>
0,
where
2
x
y
xy
(x,y)
(x,y)∈{1,...,n}
v is a vector of weights on each HSIC statistic. We
may recover the test in the previous section by setting v(1,2) = +1 v(1,3) = −1 and v(i,j) = 0 for all
(i, j) ∈ {1, 2, 3}2 \ {(1, 2), (1, 3)}.
The derivation of the test follows the general strategy used
in the previous section: we construct a rotation matrix so
as to project the joint Gaussian distribution onto the first
axis, and read the p-value from a standard normal table. To
construct the rotation matrix, we simply need to rotate v
such that it is aligned with the first axis. Such a rotation
can be computed by composing n 2-dimensional rotation
matrices as in Algorithm 1.

which is implied by the positive definiteness of Σ.
Proof of Theorem 6. The Type II error probability of the
independent test at level α is

m−1/2 HSIC(F, G, Pxy )
 −1
−HSIC(F, H, Pxz ) 
 , (20)
p
Φ

Φ (1 − α) −
σ2
+ σ2


X0Y 0

X 00 Z 00

5. Experiments
We apply our estimates of statistical dependence to three
challenging problems. The first is a synthetic data experiment, in which we can directly control the relative degree
of functional dependence between variates. The second experiment uses a multilingual corpus to determine the relative relations between European languages. The last exper-

A low variance consistent test of relative dependency
Power of the tests for m=500 and 100 repeated tests

Algorithm 1 Successive rotation for generalized highdimensional relative tests of dependency (cf. Section 4)
Require: v ∈ Rn
Ensure: [Qv]i = 0 ∀i 6= 1, QT Q = I
Q=I
for i = 2 to n do
vi
Qi = I; θ = − tan−1 [Qv]
1
[Qi ]11 = cos(θ); [Qi ]1i = − sin(θ)
[Qi ]i1 = sin(θ); [Qi ]ii = cos(θ)
Q = Qi Q
end for

0.5

0

−0.5

−1

−1.5

−2
−1

0

1

2

3

4

5

6

7

0

−5

−10

−15
−10

Power of the tests

0.8

0.6

0.4

0.2
dependent tests
independent tests
0

0.5

1

1.5

γ3

2

2.5

3

3.5

10

5

−5

0

5

10

15

t cos(t) + γ3 N (0, 1)

10

1

t sin(t) + γ2 N (0, 1)

sin(t) + γ1 N (0, 1)

2

1.5

1

5

0

−5

−10

−15
−15

−10

−5

0

5

10

t + γ1 N (0, 1)

t cos(t) + γ2 N (0, 1)

t cos(t) + γ3 N (0, 1)

(a) γ1 = 0.3

(b) γ2 = 0.3

(c) γ3 = 0.6

Figure 1. Illustration of a synthetic dataset sampled from the distribution in Eq. (22).

iment is a 3-block dataset which combines gene expression,
comparative genomic hybridization, and a qualitative phenotype measured on a sample of Glioma patients.
5.1. Synthetic experiment
We constructed 3 distributions as defined in Eq. (22) and
illustrated in Figure 1.
Let t ∼ U[(0, 2π)],

(22)

(a) x1 ∼ t + γ1 N (0, 1) y1 ∼ sin(t) + γ1 N (0, 1)
(b) x2 ∼ t cos(t) + γ2 N (0, 1) y2 ∼ t sin(t) + γ2 N (0, 1)
(c) x3 ∼ t cos(t) + γ3 N (0, 1) y3 ∼ t sin(t) + γ3 N (0, 1)
These distributions are specified so that we can control the
relative degree of functional dependence between the variates by varying the relative size of noise scaling parameters
γ1 , γ2 and γ3 . The question is then whether the dependence
between (a) and (b) is larger than the dependence between
(a) and (c). In these experiments, we fixed γ1 = γ2 = 0.3,
while we varied γ3 , and used a Gaussian kernel with bandwidth σ selected as the median pairwise distance between
data points. This kernel is sufficient to obtain good performance, although others choices exist (Gretton et al., 2012).
Figure 2 shows the power of the dependent and the independent tests as we vary γ3 . It is clear from these results
that the dependent test is far more powerful than the independent test over the great majority of γ3 values considered. Figure 3 demonstrates that this superior test power
arises due to the tighter and more concentrated distribution
of the dependent statistic.

15

Figure 2. Power of the dependent and independent test as a function of γ3 on the synthetic data described in Section 5.1. For values of γ3 > 0.3 the distribution in Fig. 1(a) is closer to 1(b) than
to 1(c). The problem becomes difficult as γ3 → 0.3. As predicted
by theory, the dependent test is significantly more powerful over
almost all values of γ3 by a substantial margin.

5.2. Multilingual data
In this section, we demonstrate dependence testing to predict the relative similarity of different languages. We use
a real world dataset taken from the parallel European Parliament corpus (Koehn, 2005). We choose 3000 random
documents in common written in: Finnish (fi), Italian (it),
French (fr), Spanish (es), Portuguese (pt), English (en),
Dutch (nl), German (de), Danish (da) and Swedish (sv).
These languages can be broadly categorized into either the
Romance, Germanic or Uralic groups (Gray & Atkinson,
2003). In this dataset, we considered each language as a
random variable and each document as an observation.
Our first goal is to test if the statistical dependence between
two languages in the same group is greater than the statistical dependence between languages in different groups.
For pre-processing, we removed stop-words (http://
www.nltk.org) and performed stemming (http://
snowball.tartarus.org). We applied the TF-IDF
model as a feature representation and used a Gaussian kernel with the bandwidth σ set per language as the median
pairwise distance between documents.
In Table 1, a selection of tests between language groups
(Germanic, Romance, and Uralic) is given: all p-values
strongly support that our relative dependence test finds the
different language groups with very high significance.
Further, if we focus on the Romance family, our test enables one to answer more fine-grained questions about the
relative similarity of languages within the same group. As
before, we determine the ground truth similarities from the
topology of the tree of European languages determined by
the linguistics community (Gray & Atkinson, 2003; Bouckaert et al., 2012) as illustrated in Fig. 4 for the Romance

A low variance consistent test of relative dependency

0.025

0.025

0.025

HSICXZ

0.03

HSICXZ

0.02

0.02

0.02

0.015

0.015

0.015

0.02

0.025
HSICXY

0.03

0.01
0.01

0.035

(a) m=500, γ3 = 0.7
pdep = 0.0189, pindep = 0.3492

0.015

0.02

0.025
HSICXY

0.01
0.01

0.035

(b) m=1000, γ3 = 0.7
pdep = 10−4 , pindep = 0.3690
0.035

0.035

0.03

0.03

0.03

0.025

0.025

0.025

HSICXZ

0.03

0.02

0.02

0.015

0.015

0.01
0.01

0.015

0.02

0.025
HSICXY

0.03

(d) m=500, γ3 = 1.7
pdep = 10−9 , pindep = 0.982

0.035

0.01
0.01

0.02

0.025
HSICXY

0.035

independent tests
dependent tests

independent tests
dependent tests

0.015

0.03

0.035

(c) m=3000, γ3 = 0.7
pdep = 10−6 , pindep = 0.2876

HSICXZ

HSICXZ

0.03

0.015

independent tests
dependent tests

independent tests
dependent tests

independent tests
dependent tests

0.03

0.01
0.01

HSICXZ

0.035

0.035

0.035

independent tests
dependent tests

0.02

0.015

0.015

0.02

0.025
HSICXY

0.03

0.035

(e) m=1000, γ3 = 1.7
pdep = 10−10 , pindep = 0.0326

0.01
0.01

0.015

0.02

0.025
HSICXY

0.03

0.035

(f) m=3000, γ3 = 1.7
pdep = 10−13 , pindep = 0.005

Figure 3. For the synthetic experiments described in Section 5.1, we plot empirical HSIC values for dependent and independent tests for
100 repeated draws with different sample sizes. Empirical p-values for each test show that the dependent distribution converges faster
than the independent distribution even at low sample size, resulting in a more powerful statistical test.

Source Target 1 Target 2
es
pt
fi
fr
it
da
it
es
fi
pt
es
da
de
nl
fi
nl
en
es
da
sv
fr
sv
en
it
en
de
es

p-value
0.0066
0.0418
0.0169
0.0173
< 10−4
< 10−4
< 10−6
< 10−4
< 10−4

Table 1. A selection of relative dependency tests between two
pairs of HSIC statistics for the multilingual corpus data. Low pvalues indicate a source is closer to target 1 than to target 2. In all
cases, the test correctly identifies that languages within the same
group are more strongly related than those in different groups.

group. We have run the test on all triplets from the corpus for which the topology of the tree specifies a correct
ordering of the dependencies. In a fraction of a second (excluding kernel computation), we are able to recover certain
features of the subtree of relationships between languages
present in the Romance language group (Table 2). The test
always indicates the correct relative similarity of languages
when nearby languages (pt,es) are compared with those further away (ft,it), however errors are made when comparing
triplets of languages for which the nearest common ancestor is more than one link removed.

Figure 4. Partial tree of Romance languages adapted from (Gray
& Atkinson, 2003).

Source Target 1 Target 2
fr
es
it
fr
pt
it
es
fr
it
es
pt
it
es
pt
fr
pt
fr
it
pt
es
it
pt
es
fr

p-value
0.0157
0.1882
0.2147
< 10−4
< 10−4
0.7649
0.0011
< 10−8

Table 2. Relative dependency tests between Romance languages.
The tests are ordered such that a low p-value corresponds with a
confirmation of the topology of the tree of Romance languages determined by the linguistics community (Gray & Atkinson, 2003).

A low variance consistent test of relative dependency
×10−3
6

4
3
2
1
0
−1
−2

Source
da
da
de
fr
es

Targets
de sv fi
sv en fr
sv en it
it es sv
fr pt nl

p-value
< 10−9
< 10−9
< 10−5
< 10−5
0.0175

Table 3. Relative dependency test between four pairs of HSIC
statistics for the multilingual corpus data. These tests show the
ability of the relative dependence test to generalize to arbitrary
numbers of HSIC statistics by constructing a rotation matrix using Algorithm 1. In all cases v = [1 1 −2].

5.3. Pediatric glioma data
Brain tumors are the most common solid tumors in children
and have the highest mortality rate of all pediatric cancers.
Despite advances in multimodality therapy, children with
pediatric high-grade gliomas (pHGG) invariably have an
overall survival of around 20% at 5 years. Depending on
their location (e.g. brainstem, central nuclei, or supratentorial), pHGG present different characteristics in terms of
radiological appearance, histology, and prognosis. The hypothesis is that pHGG have different genetic origins and
oncogenic pathways depending on their location. Thus, the
biological processes involved in the development of the tumor may be different from one location to another.
In order to evaluate such hypotheses, pre-treatment frozen
tumor samples were obtained from 53 children with newly
diagnosed pHGG from Necker Enfants Malades (Paris,
France) from Puget et al, (2012). The 53 tumors are divided into 3 locations: supratentorial (HEMI), central nuclei (MIDL), and brain stem (DIPG). The final dataset is organized in 3 blocks of variables defined for the 53 tumors:
X is a block of indicator variables describing the location
category, the second data matrix Y provides the expression
of 15 702 genes (GE). The third data matrix Z contains the
imbalances of 1229 segments (CGH) of chromosomes.
For X, we use a linear kernel, which is characteristic for
indicator variables, and for Y and Z, the kernel was chosen to be the Gaussian kernel with σ selected as the median
of pairwise distances. The p-value of our relative dependency test is < 10−5 . This shows that the tumor location
in the brain is more dependent on gene expression than on
chromosomal imbalances. By contrast with Section 5.1,
the independent test was also able to find the same order-

dependent test
independent test

5

HSIC XZ

In our next tests, we evaluate our more general framework
for testing relative dependencies with more than two HSIC
statistics. We chose four languages, and tested whether the
average dependence between languages in the same group
is higher than the dependence between groups. The results
of these tests are in Table 3. As before, our test is able to
distinguish language groups with high significance.

0

1

2

3

4

5

6 ×10−3

HSIC XY

Figure 5. 2σ iso-curves of the Gaussian distributions estimated
from the pediatric Glioma data. As before, the dependent test
has a much lower variance than the independent test. The tests
support the stronger dependence on the tumor location to gene
expression than chromosomal imbalances.

ing of dependence, but with a p-value that is three orders of
magnitude larger (p = 0.005). Figure 5 shows iso-curves
of the Gaussian distributions estimated in the independent
and dependent tests. The empirical relative dependency is
consistent with findings in the medical literature, and provides additional statistical support for the importance of
tumor location in Glioma (Gilbertson & Gutmann, 2007;
Palm et al., 2009; Puget et al., 2012).

6. Conclusions
We have described a novel statistical test that determines
whether a source random variable is more strongly dependent on one target random variable or another. This
test, built on the Hilbert-Schmidt Independence Criterion,
is low variance, consistent, and unbiased. We have shown
that our test is strictly more powerful than a test that does
not exploit the covariance between HSIC statistics, and
empirically achieves p-values several orders of magnitude
smaller. We have empirically demonstrated the test performance on synthetic data, where the degree of dependence
could be controlled; on the challenging problem of identifying language groups from a multilingual corpus; and
for finding the most important determinant of Glioma type.
The computation and memory requirements of the test are
quadratic in the sample size, matching the performance of
HSIC and related tests for dependence between two random variables. The test is therefore scalable to the wide
range of problem instances where non-parametric dependency tests are currently applied. We have generalized the
test framework to more than two HSIC statistics, and have
given an algorithm to construct a consistent, low-variance,
unbiased test in this setting.
Acknowledgements
We thank Ioannis Antonoglou for helpful discussions. The
first author is supported by a fellowship from Centrale-

A low variance consistent test of relative dependency

Supélec. This work is partially funded by the European Commission through ERC Grant 259112 and FP7MCCIG334380.

Gretton, A. and Gyorfi, L. Consistent nonparametric tests
of independence. Journal of Machine Learning Research, 11:1391–1423, 2010.

References

Gretton, A., Bousquet, O., Smola, A. J., and Schölkopf, B.
Measuring statistical dependence with Hilbert-Schmidt
norms. In Algorithmic Learning Theory, pp. 63–77,
2005.

Arcones, M. A. and Gine, E. Limit theorems for Uprocesses. The Annals of Probability, pp. 1494–1542,
1993.
Bouckaert, R., Lemey, P., Dunn, M., Greenhill, S. J., Alekseyenko, A. V., Drummond, A. J., Gray, R. D., Suchard,
M. A., and Atkinson, Q. D. Mapping the origins and expansion of the Indo-European language family. Science,
337(6097):957–960, 2012.
Bring, J. A geometric approach to compare variables in
a regression model. The American Statistician, 50(1):
57–62, 1996.
Cortes, C., Mohri, M., and Rostamizadeh, A. Learning
non-linear combinations of kernels. In Neural Information Processing Systems, 2009.
Cortes, C., Mohri, M., and Rostamizadeh, A. Algorithms
for learning kernels based on centered alignment. Journal of Machine Learning Research, 13:795–828, 2012.
Darlington, Richard B. Multiple regression in psychological research and practice. Psychological bulletin, 69(3):
161, 1968.
Dauxois, J. and Nkiet, G. M. Nonlinear canonical analysis and independence tests. Annals of Statistics, 26(4):
1254–1278, 1998.
Fukumizu, K., Bach, F. R., and Gretton, A. Statistical consistency of kernel canonical correlation analysis.
The Journal of Machine Learning Research, 8:361–383,
2007.

Gretton, A., Fukumizu, K., Teo, C.-H., Song, L.,
Schölkopf, B., and Smola, A. J. A kernel statistical test
of independence. In Neural Information Processing Systems, pp. 585–592, 2008.
Gretton, A., Sejdinovic, D., Strathmann, H.and Balakrishnan, S., Pontil, M., Fukumizu, K., and Sriperumbudur,
B. K. Optimal kernel choice for large-scale two-sample
tests. In Advances in Neural Information Processing Systems, pp. 1205–1213, 2012.
Gunn, S. R. and Kandola, J. S. Structural modelling with
sparse kernels. Machine Learning, 48(1):137–163, 2002.
Heller, R., Heller, Y., and Gorfine, M. A consistent multivariate test of association based on ranks of distances.
Biometrika, 100(2):503–510, 2013.
Hoeffding, W. Probability inequalities for sums of bounded
random variables. Journal of the American statistical
association, 58(301):13–30, 1963.
Kinney, J. B. and Atwal, G. S. Equitability, mutual information, and the maximal information coefficient. Proceedings of the National Academy of Sciences, 2014.
Koehn, P. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5, pp. 79–86,
2005.

Fukumizu, K., Gretton, A., Sun, X., and Schölkopf, B. Kernel measures of conditional dependence. In Advances
in Neural Information Processing Systems, pp. 489–496.
MIT Press, 2008.

Palm, T., Figarella-Branger, D., Chapon, F., Lacroix, C.,
Gray, F., Scaravilli, F., Ellison, D. W., Salmon, I.,
Vikkula, M., and Godfraind, C. Expression profiling
of ependymomas unravels localization and tumor gradespecific tumorigenesis. Cancer, 115(17):3955–3968,
2009.

Gilbertson, R. J. and Gutmann, D. H. Tumorigenesis in the
brain: location, location, location. Cancer research, 67
(12):5579–5582, 2007.

Peters, C., Braschler, M., and Clough, P. Multilingual Information Retrieval: From Research to Practice.
Springer, 2012.

Gray, R. D. and Atkinson, Q. D. Language-tree divergence
times support the Anatolian theory of Indo-European
origin. Nature, 426(6965):435–439, 2003.

Puget, S., Philippe, C., Bax, D., Job, B., Varlet, P., Junier, M. P., Andreiuolo, F., Carvalho, D., Reis, R.,
and Guerrini-Rousseau, L. Mesenchymal transition
and PDGFRA amplification/mutation are key distinct
oncogenic events in pediatric diffuse intrinsic pontine
gliomas. PloS one, 7(2):e30313, 2012.

Gretton, A. A simpler condition for consistency of a kernel
independence test. arXiv:1501.06103, 2015.

A low variance consistent test of relative dependency

Reshef, D., Reshef, Y., Finucane, H., Grossman, S.,
McVean, G., Turnbaugh, P., Lander, E., Mitzenmacher,
M., and Sabeti, P. Detecting novel associations in large
datasets. Science, 334(6062), 2011.
Sejdinovic, D., Gretton, A., and Bergsma, W. A kernel
test for three-variable interactions. In Neural Information Processing Systems, 2013a.
Sejdinovic, D., Sriperumbudur, B., Gretton, A., and Fukumizu, K. Equivalence of distance-based and RKHSbased statistics in hypothesis testing. Annals of Statistics, 41(5):2263–2702, 2013b.
Sen, A. and Srivastava, M. Regression Analysis – Theory,
Methods, and Applications. Springer-Verlag, 2011.
Serfling, R. J. Approximation theorems of mathematical
statistics. Wiley Series in Probability and Statistics. Wiley, 1981.
Song, L., Smola, A., Gretton, A., Bedo, J., and Borgwardt, K. Feature selection via dependence maximization. Journal of Machine Learning Research, 13:1393–
1434, 2012.
Székely, G., Rizzo, M., and Bakirov, N. Measuring and
testing dependence by correlation of distances. Annals
of Statistics, 35(6):2769–2794, 2007.
Trommershauser, J., Kording, K., and Landy, M. S. Sensory Cue Integration. Oxford University Press, 2011.
von Bahr, Bengt. On the convergence of moments in
the central limit theorem. The Annals of Mathematical
Statistics, 36(3):808–818, 06 1965.
Zhang, K., Peters, J., Janzing, D., B., and Schölkopf, B.
Kernel-based conditional independence test and application in causal discovery. In 27th Conference on Uncertainty in Artificial Intelligence, pp. 804–813, 2011.

