Context-based Unsupervised Data Fusion for Decision Making

Erfan Soltanmohammadi
Marvell Semiconductor, Inc., Santa Clara, CA, USA.

ERFAN @ MARVELL . COM

Mort Naraghi-Pour
Louisiana State University, Baton Rouge, LA, USA.

NARAGHI @ LSU . EDU

Mihaela van der Schaar
University of California, Los Angeles, CA, USA.

Abstract
Big Data received from sources such as social
media, in-stream monitoring systems, networks,
and markets is often mined for discovering patterns, detecting anomalies, and making decisions
or predictions. In distributed learning and realtime processing of Big Data, ensemble-based
systems in which a fusion center (FC) is used
to combine the local decisions of several classifiers, have shown to be superior to single expert systems. However, optimal design of the FC
requires knowledge of the accuracy of the individual classifiers which, in many cases, is not
available. Moreover, in many applications supervised training of the FC is not feasible since
the true labels of the data set are not available.
In this paper, we propose an unsupervised joint
estimation-detection scheme to estimate the accuracies of the local classifiers as functions of
data context and to fuse the local decisions of
the classifiers. Numerical results show the dramatic improvement of the proposed method as
compared with the state of the art approaches.

1. Introduction
Ensemble-based approaches have proven to be more accurate than single-classifier systems for many applications involving decision making, prediction, classification, or detection (Kuncheva & Whitaker, 2003). Furthermore, for problems with high computational complexity, ensemble-based approaches allow for distributed processing which results in load sharing among the individProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

MIHAELA @ EE . UCLA . EDU

ual classifiers. Due to their high computational complexity, Big Data applications including data mining, decision making, and prediction demand parallel processing for
which ensemble learning is well-suited (Zhang et al., 2013;
Tekin & van der Schaar, 2013).
An ensemble system is comprised of a set of (possibly heterogeneous1 ) classifiers and a combining rule for fusing the
classifiers’ outputs. Individual classifiers may be trained
with different data sets and by judiciously combining their
outputs we can achieve a more accurate decision; a set of
linear (or nonlinear) classifiers may be used to span the data
space to a complicated nonlinear boundary.
The fusion center (FC) which combines the local decisions of the classifiers plays the key role in the performance of the overall system. Several different fusion rules
have been proposed in the literature. The majority rule
may be employed when no information on the performance
of the classifiers is available (Kuncheva, 2004). On the
other hand, weighted majority rule can be used when the
performances of individual classifiers are known a priori. Another approach is to construct a look-up table during the test and validation procedure including the output patterns of the classifiers and their corresponding labels. This approach, known as Behavior Knowledge Space
(BKS), actually estimates the densities of the classifier outputs and requires large training and validation data sets
(Huang & Suen, 1995). For some ensemble systems the
classifiers and the FC are trained together using a joint procedure such as stacked generalization or mixture of classifiers (Wolpert, 1992; Jacobs et al., 1991).
Optimal fusion of local decisions requires the a priori
knowledge of the accuracy of the classifiers which, in many
applications, may not be available. For example, the data
may have an extremely large dimension which makes it im1

Here heterogeneity of classifiers implies that they have different error rates in classifying the data (Webb & Copsey, 2011).

Context-based Unsupervised Data Fusion for Decision Making

precise to evaluate the accuracy of the classifiers based on
the training and validation sets; or the data stream may be
time-varying for which accurate evaluation of the classifiers’ performance is impractical.
In many applications the data stream is received along with
its own context. The context may be a small side information such as a description of the way the data is acquired,
(Tekin & van der Schaar, 2013), or it may be a small dimensional portion of the actual high dimensional data representing one of its features or attributes. Since the accuracies of the classifiers vary with the data context, optimal fusion rule requires knowledge of the accuracies of the classifiers for every arriving context resulting in prohibitively
high costs in processing, communication and storage requirements.
In this paper we assume that no prior information regarding
the classifiers’ performance is available. The details on the
working of each classifier, and how they receive their data
is also unknown. Each classifier may work with a different
part of the Big Data, the preprocessed data, or even different correlated data from distributed multiple sources. We
propose an unsupervised method based on the ExpectationMaximization (EM) algorithm, (Dempster et al., 1977), for
evaluating the accuracies of the classifiers as functions
of context as well as fusing the decisions of individual
classifiers. To this end, we introduce a model for estimation of the classifiers’ accuracies in terms of probabilities of false alarm and detection. As such the proposed approach allows for maximum likelihood estimation of the classifier parameters based on unlabeled data.
This model is different from other typical models in which
the probability of correct decision is used to evaluate the
performance of classifiers, (Tekin & van der Schaar, 2013;
Canzian & van der Schaar, 2014). Our approach is also different from that in (Platanios et al., 2014) where the accuracies of classifiers are estimated for unlabeled data. The
authors assume that several (at least three) classifiers operate on the same data set and the classifiers make independent errors. By calculating the agreement rates of the
classifiers, the authors are able to estimate the error-rate of
each classifier. This method does not work if only a single
classifier operates on each data set.

2. Problem Formulation and Notations
We consider an ensemble learning system with K classifiers each classifying an input data stream characterized by
its context. Every classifier makes a local decision which
it delivers to the FC for the final decision. Since multiplechoice decision making can be divided into a set of binary
decision problems, (Lienhart et al., 2003), without loss of
generality we consider the binary decision problem here.

Let the portion of data available for the kth classifier be
denoted by sk (t) ∈ Sk , and let X(t) ∈ X be the context
of the received data where t is the integer-valued time index 2 . The context, which may be a vector in general, may
represent a side information about the data or it may be a
subset of the features (attributes) of the data. For instance,
in the case of image labeling, the context may be the camera resolution. The set X is assumed to be a (subset of a)
metric space with the metric dX (x1 , x2 ) that represents the
distance between x1 and x2 . Let y(t) ∈ Y , {0, 1} denote
the true label at time t. In the proposed approach, the true
label y(t) is not available for training. Moreover, we are
not concerned about how the classifiers classify the data.
However, the accuracy of each classifier is estimated as a
function of the context X(t).
Let X(t0 ) , [X(t0 ), X(t0 + 1), · · · , X(t0 + T − 1)] and
y(t0 ) , [y(t0 ), y(t0 +1), · · · , y(t0 +T −1)] denote the observed vector of contexts and the unobserved vector of true
labels, respectively, for a duration T starting at t0 . As mentioned previously, y(t0 ) is not available and its detection is
also a part of the proposed approach. Note that in this and
subsequent sections, t is in the range t0 to t0 + T − 1, k
goes from 1 to K, and i goes from 0 to 1. We define the
label matrix, by ∆(t0 ) = [δi (t)]2×T , where column t of
∆ corresponds to the true label y(t), and at each time t,
one of the elements in column t is 1 and the other is 0. If
δ0 (t) = 0, then δ1 (t) = 1, indicating that at time t we have
y(t) = 1; similarly, if δ0 (t) = 1, then δ1 (t) = 0, indicating
that at time t we have y(t) = 0.
Let ŷk (t) be the local decision of the kth classifier at time
†
t and let ŷ(t) = [ŷ1 (t) ŷ2 (t) . . . ŷK (t)] denote the vector of K local decisions at time t, where † represents the
transpose operation. Finally, let Ŷ (t0 ) = [ŷk (t)]K×T denote the collection of local decisions of all classifiers for
duration T . The FC receives the decisions of all the classifiers, Ŷ (t0 ), (as well as the context X(t0 )) and needs to
fuse them to get an estimate of the true labels. However,
for judicial fusing of the received decisions, the FC must
estimate the accuracy of each classifier.
To model the accuracy of the classifiers, we associate a
probability of detection and a probability of false alarm
with each classifier.
Since the performance of a classifier depends on the context of the data it receives, these probabilities are assumed
to be functions of the context. For a fixed context, however, a classifier has fixed probabilities of detection and
false alarm. Therefore, for context x and for classifier k,
we define the probability of detection, denoted by p1k (x),
2

For other applications such as processing a database, time can
be replaced by the index of the data sample.

Context-based Unsupervised Data Fusion for Decision Making

and the probability of false alarm, denoted by p0k (x) as
pik (x) , p(ŷk (t) = 1 | δi (t) = 1; x), i = 0, 1

(1)

We assume that the probability pik (x) is Lipschitz continuous with Lipschitz constants cik , i.e.,
|pik (x1 ) − pik (x2 )| ≤ cik dX (x1 , x2 )

(2)

This assumption which imposes a constraint on how fast
a classifier’s accuracy can change with context is clearly
valid in most practical situations, (Kleinberg et al., 2008;
Tekin & van der Schaar, 2013). For instance, in the case of
image labeling, where the context may be the camera resolution, it is not expected that the accuracy of a classifier can
change sharply with a small change in the resolution of the
images. We arrange these probabilities for all the classifiers
into a matrix P (x) , [pik (x)], i = 0, 1, k = 1, 2, . . . , K.
Note that the FC does not know P (x) and one of the goals
of our proposed method is to estimate it. In addition, in the
formulation above and the proposed solution, the context
variable x may be vector-valued. For example, if several
features are considered as part of the context, then x will
be a vector. In this case, the metric dX (x1 , x2 ) may be
chosen to be an Lµ norm for some µ ≥ 1.
In order to facilitate the detection of the true labels we
assign probabilities φ0 (t) and φ1 (t) to label y(t) and arrange them in a matrix Φ(t0 ) = [φi (t)]2×T , where φi (t) =
p(δi (t) = 1) and φ0 (t) + φ1 (t) = 1. We should point out
that the probabilities φi (t) do not represent a prior probability of the true labels. They are introduced in order to
convert the problem of detection of the true labels into the
problem of estimation of the φi (t)’s which is then solved
using the EM algorithm. Also, please note that neither
∆(t0 ) nor Φ(t0 ) are available to the FC. They are assumed
to be unknown parameters which are evaluated in the proposed method in order to estimate P (x) and to detect
Ŷ (t0 ). To summarize, the two-tuple, Θ = {P (x), Φ(t0 )}
is defined as the unknown parameter set which the FC tries
to estimate based on the local decisions of the classifier,
Ŷ (t0 ), and context of the data, X(t0 ). After estimating the
parameter set Θ, the FC detects the true labels y(t0 ). In
the next section, we propose an approach based on the EM
algorithm for the FC to achieve these goals.
Remark: One may ask whether, instead of the detection
and false alarm probabilities of the classifiers, their accuracies (i.e., the error probabilities) can be estimated. The
problem is that for the case of unsupervised learning being
considered here, we do not know how to solve the problem
in terms of the error probabilities of the classifiers. Moreover, it is clear that given the detection and false alarm
probabilities of individual classifiers, we can implement the
maximum likelihood (ML) fusion rule. However, given the
accuracies, we cannot formulate an ML fusion rule.

3. Estimation of the Classifiers’ Accuracies
and Decision Making
In this section, given the local decisions, Ŷ (t0 ), and the
observed vector of contexts, X(t0 ), we first develop an estimation method for Θ. We then use the estimated Φ(t0 ) to
detect the true labels y(t0 ).
3.1. Estimation Procedure
The maximum likelihood estimate of Θ given Ŷ (t0 ) and
X(t0 ) is given by
X
Θ̂ = arg max
p(Ŷ (t0 ), ∆(t0 ) | Θ, X(t0 )) (3)
Θ

∆

By considering ∆(t0 ) as a latent variable, the mixture
model in (3) can be iteratively solved using the EM algorithm. First, we evaluate p(Ŷ (t0 ), ∆(t0 )|Θ, X(t0 )) from
YYY
p(Ŷ (t0 ), ∆(t0 )|Θ, X(t0 )) =
(4)
t

h

ŷ (t)

pikk

(X(t)) (1 − pik (X(t)))

The log-likelihood function,
Θ, X(t0 )), is obtained as

k

i

1−ŷk (t)

1

φiK (t)

iδi (t)

log p(Ŷ (t0 ), ∆(t0 )

|

L(Θ; Ŷ (t0 ), ∆(t0 ), X(t0 ))
h
XXX
=
δi (t) ŷk (t) log pik (X(t))
t

k

i

i
1
log φi (t) (5)
K
The two steps of EM algorithm are described below.
+ (1 − ŷk (t)) log (1 − pik (X(t))) +

Expectation step: In this step, the expectation of the
log-likelihood function, denoted by Q(Θ; Θold ) is evaluated with respect to the conditional distribution p(∆(t0 ) |
Ŷ (t0 ); Θold ) of the latent variable ∆(t0 ), where Θold is the
previous estimate for Θ. That is,
Q(Θ; Θold )

(6)
h

= E∆(t0 )|Ŷ (t0 );Θold L(Θ; Ŷ (t0 ), ∆(t0 ), X(t0 ))
h
XXX
=
γ(i, t) ŷk (t) log pik (X(t))
k

t

i

i

i
1
log φi (t)
K
where EA|C,D,... denotes expectation with respect to A
given the variables C and D, . . . , and where
+ (1 − ŷk (t)) log (1 − pik (X(t))) +

γ(i, t) = EH|Y ;Θold [δit ] = p(δit = 1 | Y ; Θold , X(t))
= p(δit = 1 | ŷ(t); Θold , X(t)) =
(7)


Q
yk (t)
1−yk (t)
old
φold
1 − pold
i (t)
ik (X(t))
k pik (X(t))
1
yk (t)
1−yk (t)
Q
P
old
old
φold
1 − pjk
(X(t))
j (t)
k pjk (X(t))

j=0

Context-based Unsupervised Data Fusion for Decision Making

Maximization step: In this step, Q(Θ; Θold ) is maximized
with respect to Θ. In maximizing Q(Θ; Θold ) with respect
P1
to φi (t) we must consider the constraint i=0 φi (t) = 1.
Using the Lagrange multiplier method, we get
γ(i, t)
= γ(i, t)
φnew
i (t) = P1
j=0 γ(j, t)

(8)

We would like to note that since Q(Θ; Θold ) is a concave
function of φi (t), and the constraint is linear, the above Lagrangian method results in the optimal solution for Φ(t0 ).

that the objective function in (14) is concave and the constraints are linear; Therefore, (14) can be solved using the
interior point methods, (Boyd & Vandenberghe, 2004).
By iterating between the expectation and the maximization
steps, until a stopping criterion is satisfied3 , we find an estimation of the parameter set4 . We denote the final estimates of the parameter set by Θ̃, and the final estimates of
˜ = [p̃ik (x)], and
P (x) = [pik (x)] and Φ = [φi (t)] by P (x)
Φ̃ = [φ̃i (t)], respectively.

1
2
3

pnew
ik (X(t))

old

= arg max Q(Θ; Θ )

(9)

4

..
.

pik (X(t))

subject to:

2T −1

|pik (x1 ) − pik (x2 )| ≤ cik dX (x1 , x2 ) , ∀x1 , x2 ∈ X
0 ≤ pik (x) ≤ 1 for i = 0, 1, k = 1, 2, . . . , K, ∀x ∈ X

2T
2T +1

Λ,

old

It can be shown that Q(Θ; Θ ) is a concave function
of pik (X(t)). Therefore we can use convex optimization
methods to solve (9). Towards this let

2T +2
2T +3
2T +4

..
.
4T −1
4T −2

̺ik (l, j) , cik dX (x(l), x(j))

(10)

pik (t0 ) ,

(11)

..
.
2

T −T −1
T 2 −T

†

[pik (X(t0 )), pik (X(t0 + 1)), . . . , pik (X(t0 + T − 1))]

X
ψ(pik (t0 )) ,
γ(i, t) ŷk (t) log pik (X(t))+
(12)

2

3

4

···

T −1

T

−1
1
0
0

0
0
−1
1

0
0
0
0

···
···
···
···

0
0
0
0

0
0
0
0

0
0
1
−1
1
−1

0
0
−1
1
0
1

0
0
0
0
−1
1

···
···
···
···
···
···

0
0
0
0
0
0

−1
1
0
0
0
0

0
0

0
0

···
···

0
0

−1
1

0
0

0
0

0
0
..
.
0
0

···
···

1
−1

−1
1

1

Maximization of Q(Θ; Θold ) with respect to pik (X(t)) is
also a constraint optimization problem given by

 1
 −1
 1

 −1




 1

 −1

 0

 0

 0

 0




 1

 −1




0
0

1



2
3
4

Then, to maximize Q(Θ; Θold ) with respect to pik (x) subject to the Lipschitz continuity constraint in (2), we can
solve the constrained optimization problem given by

..
.
2T −1
2T

pnew
ik (t0 )

= arg max ψ(pik (t0 ))

2T +1

(13)

̺ik ,

pik (t0 )

subject to:
|pik (x(t0 + l)) − pik (x(t0 + j))| ≤ ̺ik (l, j) ∀ l, j,

2T +2
2T +3
2T +4

..
.

0 ≤ pik (X(t)) ≤ 1 for i = 0, 1, k = 1, 2, . . . , K,
t = t0 , t0 + 1, . . . , t0 + T − 1

4T −1
4T −2

..
.

We can rewrite (13) as

T 2 −T −1
T 2 −T

pnew
ik (t0 )

= arg max ψ(pik (t0 ))
































and

t

(1 − ŷk (t)) log (1 − pik (X(t)))



































̺ik (1, 2)
̺ik (1, 2)
̺ik (1, 3)
̺ik (1, 3)
..
.
̺ik (1, T )
̺ik (1, T )
̺ik (2, 3)
̺ik (2, 3)
̺ik (2, 4)
̺ik (2, 4)
..
.
̺ik (2, T )
̺ik (2, T )
..
.
̺ik ((T − 2), T )
̺ik ((T − 1), T )

































(14)

pik (t0 )

subject to: Λpik (t0 ) ≤ ̺ik , 0 ≤ pik (t0 ) ≤ 1
where the inequalities are component-wise, and 0 and 1 are
the all-zero and the all-one column vectors of length T , respectively, and where Λ and ̺ik are defined below. Note

In order to evaluate pik (x) for all x ∈ X , we note that for
3

A stopping criterion could be a selected number of iterations
or a threshold on the percentage difference between the last two
estimations.
4
For a discussion of the convergence properties of the EM algorithm we refer to (Dempster et al., 1977; Bishop, 2006).

Context-based Unsupervised Data Fusion for Decision Making

any j = t0 , t0 + 1, · · · , t0 + T − 1, i = 0, 1 and k =
1, 2, · · · , K,
p̃ik (x(j)) − cik dX (x, x(j)) ≤ p̃ik (x)

(15)

p̃ik (x(j)) + cik dX (x, x(j)) ≥ p̃ik (x)

(16)

Therefore, we can interpolate the values of pik (x(t0 + l)),
l = 0, 1, · · · , T − 1 to obtain pik (x) for any x ∈ X . Let
p1ik (x) =

max

t0 ≤j≤t0 +T −1

{p̃ik (x(j)) − cik dX (x, x(j))}
(17)

p2ik (x) =

min

t0 ≤j≤t0 +T −1

{p̃ik (x(j)) + cik dX (x, x(j))}
(18)

We then set5
p̃ik (x) = min {p1ik (x), p2ik (x)}

(19)

Remark: The Lipschitz constants cik affect the performance of the algorithm in estimating the parameters pik (x)
as functions of x. As evident from (2), (15) and (19),
smaller values of cik result in a smoother estimate for
pik (x), while larger values of cik allow for larger variations in the estimates. Therefore the Lipschitz constants
cik must be selected in accordance with the performance of
the classifiers vs. the context variables. In particular, if for
example the detection performance p1k (x) of the kth classifier is believed to be very sensitive to the context variable
x, i.e., small changes in x result in large changes in p1k (x),
then the value of c1k must be chosen to be large. On the
other if the detection performance of the kth classifier is
not very sensitive to the context variable x, then a smaller
value should be assigned to c1k . That said, we would like
to also point out that the Lipschitz condition in (2) is only
introduced to enable the estimation of the functions pik (x).
If the selected parameters cik do not satisfy (2) for the true
functions pik (x), then our algorithm still works. However,
in this case our estimates of pik (x) will not be very accurate. In Fig. 2 of Section 4 we present results of the estimations for different values of the Lipschitz constants which
verify this statement.
3.2. Fusion Center’s Decisions
In the previous section, we evaluated the estimates of probabilities of false alarm and detection for all the classifiers as
well as the prior probabilities of the true labels Φ̃. To detect
the true labels of the classifiers we use the maximum likelihood detection of y(t) given the probabilities, Φ̃, namely
5

The minimum in (19) provides a maxmin approximation for
the values of detection (false alarm) probabilities that have been
calculated. This is an interpolation problem and our approach is
admittedly heuristic. Another approach is to select the median or
mean.

Algorithm 1 Estimation of the parameter set and FC’s decisions
Input: The local decisions of K classifiers from t0 to t0 +
T − 1, Ŷ (t0 ) and the corresponding contexts, X(t0 )
Output: The estimation of the probabilities of false alarm
and detention for all of the classifiers, P̃ , and the made
decisions, ỹ
Assume an initial estimation for Θnew
while Stopping criterion is not satisfied do
new
pold
ik (X(t)) ← pik (X(t))
old
new
φi (t) ← φi (t)
Find γ(i, t) using (7)
new
Find φnew
i (t) and pik (X(t)) using (8) and (14)
end while
For all x ∈ X , interpolate the values of pnew
ik (x(t0 + l))
using (17)-(19)
Θ̃ ← Θnew
Make decisions using (20)

ỹEM (t) = arg maxy(t) p(y(t) | Θ̃) which is given by

1, φ̃1 (t) > φ̃0 (t)
(20)
ỹ(t) =
0, otherwise.
We denote the final detected labels by ỹ = [ỹ(t0 ), ỹ(t0 +
1), . . . , ỹ(t0 +T −1)]. The entire procedure of estimating
the parameter set and making decisions is summarized in
Algorithm 1.

4. Numerical Results
In this section, first we use a system with up to 8 classifiers
to evaluate the performance of the proposed approach. The
probabilities of false alarm and detection of these classifiers
as a function of the context x are shown in Table 1. These
probabilities are selected so as to represent a variety of behaviors. In particular, the classifiers are not very accurate,
and for many values of the context, their false alarm and detection probabilities are close to 0.5. The L1 norm is used
as the distance measure, i.e., dX (x1 , x2 ) = kx1 − x2 k1 .
The values of the Lipschitz constants are also shown in Table 1. These values are selected so as to satisfy the condition in (2).
In Fig. 1 we show the performance of the proposed
method in estimating the probabilities of false alarm and
detection of the individual classifiers. To show the convergence speed of the proposed approach, we use a system with 4 classifiers; namely Classifiers 1 − 4 from
Table 1. We initialize the EM algorithm with all the
probabilities of false alarm equal to 0.2, all the probabilities of detection equal to 0.8, and φ1 (t) = 0.6 for
t = t0 , t0 + 1, · · · , T + t0 − 1. The parameter T is
chosen to be 100. The estimated probabilities of false

Context-based Unsupervised Data Fusion for Decision Making

Table 1. The probabilities of false alarm and detection of the classifiers.
p0k (x)
Classifier 1
Classifier 2
Classifier 3
Classifier 4

2

−2x + 2x
2(x − .5)2
.5 |sin(2πx)|
.1

c0k
2.0
2.0
3.1
0.1

p1k (x)
.5 + .5 |sin(2πx)|
.9
1 − 2(x − .5)2
.5 + 2(x − .5)2

p0k (x)

c1k
3.1
0.1
2.0
2.0

Classifier 5
Classifier 6
Classifier 7
Classifier 8

.5x
.25 + 2(x − .5)3
.5(1 − x)
.25 + 2(x − .5)3

c0k
0.5
1.5
0.5
1.5

p1k (x)

c1k
3

.75 + 2(x − .5)
.75 − 2(x − .5)3
.75 + .5(x − .5)
.5(2 − x)

1.5
1.5
0.5
0.5

Figure 1. Comparison of the proposed method and the majority rule.

alarm and detection for the four classifiers are shown in
Fig. 1 for 1, 2 and 5 iterations of the EM algorithm. In
Fig. 1 we also compare the performance of the proposed
method with that of the majority rule which is the most
widely used unsupervised fusion rule for ensemble learning
(Breiman, 1996; Schapire, 1990; Freund & Schapire, 1997;
Herbster & Warmuth, 1998; Canzian et al., 2013). It can be
seen that the difference between the estimations after the
2nd and the 5th iterations are very small indicating the fast
convergence of the proposed approach. On the other hand
for the majority rule, the final estimated probabilities are
very spiky, and in all of the cases, the proposed approach
significantly outperforms the majority rule. In the rest of
this section, we set the number of iterations to 5.
In Fig. 2 we show the effect of the Lipschitz constants
on the final estimations. Here we set cik = c for all
i = 0, 1 and k = 1, 2, · · · , K. Three different values
of c = 0.2, 1.7, 3.2 are used. It is evident that for small
value of c = 0.2, the estimated detection and false alarm
probabilities are a very smooth function of the context x.
However, the estimations do not closely follow the actual
functions. On the other hand, for c = 3.2, the estimation
can better follow the rapid variations of pik (x) vs. x, but in
this case the estimations are somewhat spiky.
In order to quantify the improvement of the proposed

method over the majority rule we define a reliability metric,
denoted by DP where
1 R
K
1 X X x |pik (x) − p̂ik (x)| dx
R
(21)
DP ,
2K
p (x)dx
x ik
i=0
k=1

The reliability metric in (21) measures the normalizederror in the estimation of the detection and false alarm
probabilities of all the classifiers6 . Clearly a smaller value
of DP indicates a better performance for the estimator.

In Fig. 3, we show the value of DP vs. T for different number of classifiers, where for K = ℓ, Classifiers 1, 2, . . . , ℓ
are used. The values of cik are given in Table 1. As shown,
the performance of our method improves with the number
of classifiers and T and the proposed approach outperforms
the majority rule in all cases.
In Fig. 4 we show the probability of error for the proposed
method vs. the majority rule for the case presented in Fig.
1. It can be seen that the proposed method significantly
outperforms the majority rule.
In order to evaluate the performance of the proposed approach for real data, we used the Wisconsin breast cancer
data set (Murphy & Aha, 1994). The goal is to classify
6
Please note that if the set of context values is discrete, then
the integrals in (21) are replaced with summations.

Context-based Unsupervised Data Fusion for Decision Making

Figure 2. Estimations of the probabilities of false alarm and detection vs. context for K = 4 different experts (Expert1-4 from Table 1),
T = 100 using the proposed approach for c = 0.2, 1.7, 3.2.

Figure 3. Reliability, DP versus T for K = 2, 4, 8 classifiers.
The values of cik are given in Table 1.

Figure 4. The probability of error for the fusion center vs. T for
the case in Fig. 1.

Remark: We should point out that this data set comes with
true labels. As discussed previously, our algorithm does
not require the true labels and does not utilize the labels in
order to estimate the performance of the classifiers and for
fusing the decisions of the individual classifiers. However,
the labels are used in order to evaluate the performance
of our algorithm in terms of correct decisions (see Fig.
6) as well as to compare our results with other methods.
The labels are also used for training the supervised optimal fusion rule (SOFR),(Chair & Varshney, 1986), which
provides a lower bound to the performance of any unsupervised technique (see Fig. 6)

shape, 4) marginal adhesion, 5) single epithelial, 6) bare
nucleoli, 7) bland chromatin, 8) normal nucleoli, and 9)
mitoses. All the features are in the interval [1, 10]. We
used DecisionStump (one-level decision tree), KNN (knearest neighbor classifier), k-Star (instance classifier using entropy as distance), LogitBoost+ZeroR (ZeroR classifier uses mode), Multilayer Perceptron, and NaiveBayes
(naive Bayes classifier) as classifiers and trained them with
a subset of the data7 . Each of these features is considered
as context separately, but due to space limitations, in Fig.
5 we show the performance for clump thickness, uniformity of cell size, bland chromatin, normal nucleoli, and
mitoses. We implemented our approach for each of the contexts where for each i and k we set the Lipschitz constants

Each point in the data set has 9 different features: 1) clump
thickness, 2) uniformity of cell size, 3) uniformity of cell

7
We used machine learning classifiers from Weka. Detailed
description of each classifier can be found in (Witten et al., 2011).

each data point as benign or malignant.

Context-based Unsupervised Data Fusion for Decision Making

Figure 5. The proposed approach is used in order to evaluate the performance of different classifiers identified at the top of each subfigure as a function of the context x.

cik = 0.058 , and the final results in terms of probabilities
of false alarm and detection versus context are shown in
Fig. 5. As shown, the NaiveBayes has the worst performance. When the context is set to be x = clump thickness,
the performance of k-Star deteriorates with increasing x, in
the sense that the probability of false alarm increases while
the probability of detection does not change. Therefore, if
one wishes to use one of the classifiers, it can be suggested
that for larger values of clump thickness, it is better to use
Multilayer Perceptron than k-Star. Therefore the proposed
method can be used in this way to determine the efficacy of
the individual classifiers.
To evaluate the performance of the fusion rule in making the right decision about benign or malignant samples, we define the probability of fusion error as pe =
p(ỹ(t) 6= y(t)). In Fig. 6, we compare the results of
our unsupervised method with the supervised and unsupervised versions of the method of tracking the best classifier (MTBE), (Herbster & Warmuth, 1998), adaptive Perceptron weighted majority rule (APMR), (Canzian et al.,
2013), and the SOFR, in term of pe vs. T . The parameters
for our method are the same as those in Fig. 5 with each
feature used as a context with a value between 1 and 10. It
can be seen that the proposed approach works better than
MTBE and APMR and even the supervised MTBE. APMR
and MTBE do not fuse the data optimally. Moreover, in its
modeling APMR does not “reward” or “punish” the classifiers who make decisions similar to or different from the
FC even when the FC correctly detects the true label. Another fundamental problem with the unsupervised MTBE
and APMR is that since these methods are only concerned
8
For this data set and as shown n Fig. 5, the variations of
false alarm and detection probabilities with respect to the context
variable are very small.

Figure 6. Comparison of our approach with the method of tracking the best classifier (MTBE), adaptive Perceptron weighted majority rule (APMR), and supervised optimal fusion rule (SOFR).

with correct detection, they do not properly characterize a
classifier which has a very high false alarm probability.

5. Conclusion
Ensemble-based systems have proven to be superior to
single-expert systems for Big Data analytics. In many applications prior information about the accuracies of the individual classifiers is not available and the true label of the
data is never observed. In this paper, we propose an unsupervised method to estimate the accuracies of the experts
and to fuse their local decisions to obtain a final decision.
The results show the superior performance of the proposed
approach as compared with the state of the art approaches.

Context-based Unsupervised Data Fusion for Decision Making

References
Bishop, Christopher M. Pattern Recognition and Machine
Learning (Information Science and Statistics). SpringerVerlag New York, Inc., Secaucus, NJ, USA, 2006.
Boyd, Stephen and Vandenberghe, Lieven. Convex Optimization. Cambridge University Press, New York, NY,
USA, 2004. ISBN 0521833787.
Breiman, Leo. Bagging predictors. Mach. Learn., 24(2):
123–140, August 1996. ISSN 0885-6125. doi: 10.1023/
A:1018054314350.
Canzian, Luca and van der Schaar, Mihaela. A network of
cooperative learners for data-driven stream mining. In
Acoustics, Speech and Signal Processing, 2014. ICASSP
2014 Proceedings. 2014 IEEE International Conference
on, To appear 2014.
Canzian, Luca, Zhang, Yu, and van der Schaar, Mihaela. Ensemble of distributed learners for online classification of dynamic data streams. arXiv preprint
arXiv:1308.5281, 2013.
Chair, Z. and Varshney, P.K. Optimal data fusion in multiple sensor detection systems. Aerospace and Electronic Systems, IEEE Transactions on, AES-22(1):98 –
101, jan. 1986. ISSN 0018-9251. doi: 10.1109/TAES.
1986.310699.
Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum
likelihood from incomplete data via the em algorithm.
Journal of the Royal Statistical Society, Series B, 39(1):
1–38, 1977.
Freund, Yoav and Schapire, Robert E. A decision-theoretic
generalization of on-line learning and an application to
boosting. J. Comput. Syst. Sci., 55(1):119–139, August
1997. ISSN 0022-0000. doi: 10.1006/jcss.1997.1504.
Herbster, Mark and Warmuth, Manfred K. Tracking the
best expert. Machine Learning, 32(2):151–178, 1998.
Huang, Y.S. and Suen, C.Y. A method of combining multiple experts for the recognition of unconstrained handwritten numerals. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 17(1):90–94, 1995. ISSN
0162-8828. doi: 10.1109/34.368145.
Jacobs, Robert A, Jordan, Michael I, Nowlan, Steven J, and
Hinton, Geoffrey E. Adaptive mixtures of local experts.
Neural computation, 3(1):79–87, 1991.
Kleinberg, Robert, Slivkins, Aleksandrs, and Upfal, Eli.
Multi-armed bandits in metric spaces. In Proceedings
of the 40th annual ACM symposium on Theory of computing, pp. 681–690. ACM, 2008.

Kuncheva, L.I. Combining Pattern Classifiers: Methods
and Algorithms. Wiley, 2004. ISBN 9780471660255.
Kuncheva, Ludmila I. and Whitaker, Christopher J. Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. Mach. Learn.,
51(2):181–207, May 2003. ISSN 0885-6125. doi:
10.1023/A:1022859003006.
Lienhart, Rainer, Liang, Luhong, and Kuranov, Er. A detector tree of boosted classifiers for real-time object detection and tracking. In IEEE Int. Conf. on Multimedia
and Systems (ICME2003), 2003.
Murphy, P.M. and Aha, D.W. UCI repository of machine
learning databases: Machine readable data repository.
Univ. of California at Irvine, 1994.
Platanios, Emmanouil Antonios, Blum, Avrim, and
Mitchell, Tom M. Estimating Accuracy from Unlabeled
Data. In Conference on Uncertainty in Artificial Intelligence, pp. 1–10, 2014.
Schapire, Robert E. The strength of weak learnability.
Mach. Learn., 5(2):197–227, July 1990. ISSN 08856125. doi: 10.1023/A:1022648800760.
Tekin, Cem and van der Schaar, Mihaela. Distributed online big data classification using context information.
arXiv preprint arXiv:1307.0781, 2013.
Webb, A.R. and Copsey, K.D. Statistical Pattern Recognition. Wiley, 2011. ISBN 9781119952961.
Witten, I.H., Frank, E., and Hall, M.A. Data Mining: Practical Machine Learning Tools and Techniques: Practical
Machine Learning Tools and Techniques. The Morgan
Kaufmann Series in Data Management Systems. Elsevier Science, 2011. ISBN 9780080890364.
Wolpert, David H. Stacked generalization. Neural Networks, 5(2):241 – 259, 1992. ISSN 0893-6080. doi:
http://dx.doi.org/10.1016/S0893-6080(05)80023-1.
Zhang, DTY, Sow, Daby, and van der Schaar, M. A fast
online learning algorithm for distributed mining of bigdata. In the Big Data Analytics workshop at SIGMETRICS, volume 2013, 2013.

