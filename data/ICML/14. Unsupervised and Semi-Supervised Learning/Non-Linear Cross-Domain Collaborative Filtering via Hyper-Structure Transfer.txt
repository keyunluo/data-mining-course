Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure
Transfer
Yan-Fu Liu
National Tsing Hua University, ROC

YFLIU @ NETDB . CS . NTHU . EDU . TW

Cheng-Yu Hsu
National Tsing Hua University, ROC

CYHSU @ NETDB . CS . NTHU . EDU . TW

Shan-Hung Wu
National Tsing Hua University, ROC

SHWU @ CS . NTHU . EDU . TW

Abstract
The Cross Domain Collaborative Filtering
(CDCF) exploits the rating matrices from multiple domains to make better recommendations.
Existing CDCF methods adopt the sub-structure
sharing technique that can only transfer linearly
correlated knowledge between domains. In this
paper, we propose the notion of Hyper-Structure
Transfer (HST) that requires the rating matrices
to be explained by the projections of some more
complex structure, called the hyper-structure,
shared by all domains, and thus allows the nonlinearly correlated knowledge between domains
to be identified and transferred. Extensive experiments are conducted and the results demonstrate
the effectiveness of our HST models empirically.

1. Introduction
Collaborative Filtering (CF) (Hofmann, 2004; Hu et al.,
2008; Koren et al., 2009) is a major technique used by the
recommender systems to find out items interesting to a user.
The problem of CF can be defined as follows: given a rating matrix X âˆˆ RnÃ—m whose each entry X i,j denotes
either the rating (or other similar feedback) from the user
i to the item j if i has interacted with j or blank otherwise, fill the blanks in X so that a recommender systems
can recommend to the user i those items having the top
filled/predicted values in the row X i,: . One common approach to CF is to approximate X by a dense matrix Y
whose entries can be explained by some common knowledge such as latent factors. Traditionally, CF methods foProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

cus on items in one domain. For example, a video-rental
recommender system like Netflix may regard items as all
its available videos. In practice, the rating matrix in a single domain can be too sparse to learn satisfactory latent
factors (Su & Khoshgoftaar, 2009). Furthermore, with the
popularity of e-commerce systems (e.g., Amazon, TripAdvisor, etc.) and social media (e.g., Facebook, Twitter, etc.),
users can provide feedbacks to items in multiple domains
(e.g., books, DVDs, music, electronics, etc. in Amazon).
It is desirable to have a cross-domain recommender system
that is able to recommend items in different domains to a
user, helping cross-selling the products and/or discovering
new customers.
The above motivates recent studies on the Cross Domain
Collaborative Filtering (CDCF) (Li, 2011) problem: given
(k)
(k)
d rating matrices X (k) âˆˆ Rn Ã—m , 1 â‰¤ k â‰¤ d, each
corresponding to n(k) users1 and m(k) items in a specific
domain k, fill the blanks in X (k) â€™s jointly. The CDCF
methods are expected to improve the quality of recommendations because blanks in each X (k) can be predicted based
on not only the knowledge/latent factors in that domain,
but also the correlated knowledge transferred from other
domains. The correlation is less obscured by the sparsity
since it can be inferred from multiple X (k) â€™s. Furthermore, with plenty domains available in an e-commerce system and social media, CDCF methods are also expected to
further improve the quality of recommendations as d increases.
However, we find that in practice, existing CDCF methods usually give limited improvement in performance, and
sometimes, adding new domains even results in worse performance. This is mainly due to how the correlation between domains is captured. To transfer knowledge between domains, most existing CDCF methods employ a
1

Users in different domains can overlap.

Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer

technique called the sub-structure sharing which factorizes
each Y (k) into multiple matrices and in the meanwhile requires some of them, denoted as S (k) , to be shared by all
domains. For example, it is common to tri-factorize each
(k)
(k)
Y (k) âˆˆ Rn Ã—m into Y (k) = U (k) B (k) V (k)> , where
(k)
(k)
U (k) âˆˆ Rn Ã—p is a user-cluster matrix whose each
(k)
entry U i,s denotes how much the user i belongs to the
(k)

(k)

user-cluster/latent factor s, V (k) âˆˆ Rm Ã—q is an item(k)
cluster matrix whose each entry V j,t denotes how much
the item j is assigned to the item-cluster t, and B (k) âˆˆ
(k)
(k)
Rp Ã—q a rating pattern matrix (or codebook) describing the linear relationship between clusters in U (k) and
V (k) . Then, to transfer knowledge, one can either share
the S (k) = {U , V } (Pan et al., 2010) or S (k) = B (Gao
et al., 2013b;a; Li et al., 2009; Long et al., 2012) across all
domains. The shared S (k) captures the correlation between
domains and is used as a bridge via which the knowledge
in Y (k) \S (k) can be transferred. But sharing S (k) also
create linear dependency between all Y (k) \S (k) â€™s. Consider S (k) = {U , V }. For any two domains k and l,
we have Y (k) = U B (k) V > and Y (l) = U B (l) V > .
By Y (k) V (B (k) )âˆ’1 = U = Y (l) V (B (l) )âˆ’1 , we obtain
B (k) = LB (l) for some L âˆˆ RpÃ—p . That is, Y (k) \S (k) =
B (k) and Y (l) \S (l) = B (l) , which denote the knowledge
to be transferred, are linearly dependent with each other.
This linear dependency exists in most current CDCF methods. Therefore, only the linearly correlated knowledge between domains can be identified and transferred.
In the real world, the correlation of knowledge between domains is usually non-linear. For example, consider two
domains â€œbooksâ€ and â€œmovies.â€ Suppose the ratings in
X (movies) in the â€œmoviesâ€ domain can be explained by
(movies)
(but not limited to) a latent factor fhit
denoting the
â€œbox office hit,â€ and the ratings in X (books) in the â€œbooksâ€
(books)
domain can be explained by two latent factors fvisibility
(books)

and fcuriosity representing the â€œvisibility of the bookâ€
and â€œuserâ€™s curiosity about the storyâ€ respectively. The
(movies)
(books)
fhit
and fvisibility may be linearly correlated, as the
more a movie adapted from a book is played, the more the
original book is visible to users. If so, these two latent
factors and their correlation will be captured by Y (movies)
and Y (books) using existing CDCF methods. Neverthe(movies)
(books)
less, it is likely that the fhit
and fcuriosity are nonlinearly correlated, as users may be highly curious about
the story in the original book either when the box office
record of the adapted movie is high due to the public praise
of screenplay, or when the box office record is low due to
the criticism of unfaithful adaptation. In this case, the fac(books)
tor fcuriosity will be missed by existing CDCF methods,
resulting in degraded Y (books) .

Figure 1. Different Y (k) â€™s share the projections of a hyperstructure H. So, a pattern (e.g., the blue line) in some projection
can be non-linearly dependent with that of another (e.g., the green
curve).

In this paper, we propose a new knowledge transfer
technique for CDCF, called the Hyper-Structure Transfer
(HST), that captures the non-linear correlation of knowledge between domains. As in the sub-structure sharing,
HST factorizes each Y (k) into multiple matrices. But instead of fixing S (k) across domains, HST requires S (k)
in each domain to be a projection of some more complex
structure H, called the hyper-structure, shared between domains. HST enables the knowledge transfer by using H as a
bridge. Furthermore, since S (k) â€™s of different domains are
projections of a more complex structure, they can be nonlinearly dependent with each other, and so are the domainspecific matrices Y (k) \S (k) â€™s. Figure 1 shows an example.
Hence, HST allows the non-linearly correlated knowledge
between domains to be identified and transferred.
HST brings new challenges: how to define H and the projections? And how to solve Y (k) â€™s (and H) using the
sparse ratings in X (k) â€™s? We study these problems in depth
and propose an HST model, called the Minimal Orthogonal Tensor Approximation with Residuals (MOTAR) that
can accommodate arbitrarily complex hyper-structure in
the tensor form. We then show that the MOTAR objective
can be transformed into a Canonical Polyadic (CP) decomposition problem (Kiers et al., 1999; Kolda & Bader, 2009;
Bader & Kolda, 2006) of tensors and solved using existing
techniques. Extensive experiments are conducted on both
real and synthetic datasets and the results show that MOTAR can achieve up to 50% improvement in prediction accuracy as compared with the state-of-the-art CDCF methods. In addition, MOTAR always give better performance
when taking into account new domains (i.e., increasing d).
Further Related Work. In addition to the studies mentioned above, there are CDCF methods (Cao et al., 2010;
Shi et al., 2011; Zhang et al., 2012) that model Y (k) using
random variables. In these models, some random variables
are assumed to be identical in distribution across domains,
thus creating linear dependency, in probability, between the
rest variables.

Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer

To exploit the data in large recommender systems (such
as Amazon and Facebook) where users are allowed to
rate different items in different domains/categories, some
extensions of the above factorization methods are proposed, including the TrAnsfer Learning for MUltiple Domains (TALMUD) (Moreno et al., 2012), the extended
Cluster-level Latent Factor Model (CLFM) (G AO ET AL .,
2013 B ; A ) and the Cross-Domain Triadic Factorization
(CDTF) (Hu et al., 2013). These work target d > 2.
In this paper, we focus on CDCF across multiple domains
(d > 2). We do not assume the availability of any side information (e.g., tags (Chen et al., 2013), implicit feedbacks
(Moreno et al., 2012), etc.). We also make no assumption
about the distributions of latent representation of users and
items as in (Cao et al., 2010; Shi et al., 2011; Zhang et al.,
2012).

2. Challenges and Evidence
In this section, we give evidence of limitations of existing
CDCF methods.
2.1. Linear Knowledge Transfer
Despite the correlation of knowledge between domains is
usually non-linear, existing CDCF methods can only transfer the linearly correlated knowledge between domains.
Thus, they may give little help or even result in negative
transfer (Rosenstein et al., 2005), where the availability
of new domains degrades performance. Traditional studies (Argyriou et al., 2008; Bakker & Heskes, 2003; BenDavid & Schuller, 2003) aim to lower the chance of negative transfer by 1) grouping the domains such that the
domains in a group are more likely to have transferable
knowledge, and then 2) performing separate learning task
for each group. However, this significantly limits the data
available in each group, and the transferability of knowledge inside each group can still be limited by the linearity.
To give a clear case of the above limitation, we conduction experiments using DBLP citation dataset (Tang
et al., 2008) and MovieLens rating dataset2 to demonstrate the non-linearity of the correlation between domains.
We measure the non-linearity by (1) tri-factorizing each
Y (k) into Y (k) = U (k) B (k) V (k)> independently to obPd
tain the overall approximation score a = k=1 kX (k) âˆ’
U (k) B (k) V (k)> k2F where k Â· kF denotes the Frobenius
norm, and (2) tri-factorizing Y (k) â€™s jointly by sharing
B (k) = B to obtain another approximation score b, and
(3) derive a ratio, named Non-Linearity Ratio (NLR), by
(b âˆ’ a)/b. The larger the NLR, the more approximation
error is introduced due to the shared linear explanation B
2

http://grouplens.org/datasets/movielens/.

in step (2), implying that the correlation between datasets
tends to be zero or non-linear.
The NLRs of domain 1 over different domain pairs3 in the
DBLP and MovieLens datasets are summarized in Table
1. As we can see, the NLRs are approximately between
[0.34 âˆ¼ 0.5] in all domain pairs, which are very high.
2.2. Maladaptive Bridge
The above limitation can be amplified by a suboptimal
bridge. Existing CDCF methods across multiple domains
(d > 2) rely on either the pairwise or all-intersectional
extensions of the sub-structure sharing technique. In the
former (e.g, TALMUD (Moreno et al., 2012)), each pair
(Y (k) , Y (l) ) shares a bridge that can be different from that
of the other pairs; while in the latter (e.g., CDTF (Hu et al.,
2013) and extended CLFM (Gao et al., 2013a)), all Y (k) â€™s
share the same bridge. We argue that neither of the bridges
in these two extensions is good enough to capture the correlation between multiple domains. Consider four domains
â€œbooks,â€ â€œmovies,â€ â€œmusic,â€ and â€œelectronics.â€ Suppose
there is a pattern that users who have watched a movie and
listened to its soundtrack (showing interests in related products) are usually welling to buy its original book also. Note
that this pattern involves only partial domains (no â€œelectronicsâ€), and it cannot be captured by the above bridges,
no matter how dense the rating matrices X (k) â€™s are, because the bridges of pairwise extensions are too weak to
capture any correlation between more than two domains,
yet the bridge of all-intersectional extensions are too strong
to capture any correlation between partial domains. The
strength of the bridge should be adaptive to capture the correlation between any domain combination.
To show the impacts of maladaptive bridges to performance, we test the error rates of existing CDCF extensions to multiples domains against different dâ€™s, as shown
in Figures 4 and 5. In TALMUD, the decrease in error
rate marginalizes quickly as d grows. Since only the correlation between domain pairs can be captured, this shows
3

The DBLP citation dataset contains 180,640 authors (users),
141,507 papers (items) and 1,495,081 citation relations. Each paper is associated with authors, year, venue, and citations. We
use the categories listed in Microsoft Academic Search to divide
the venues into different domains: Algorithm & Theory (A&T),
Data Mining (DM), Databases (DB), Distribution & Parallel
Computing (D&PC), Human-Computer Inter- action (HCI), Machine Learning & Pattern Recognition (ML&PR), Artificial Intelligence (AI), Natural Language & Speech (NL&S), Programming
Language (PL), Software Engineering (SE), World Wide Web
(WWW). The MovieLens rating dataset contains 69,878 users,
10,677 movies (items), and 10,000,054 ratings. We categorize
the movies into the following domains: Action (ACT), Adventure (ADV), Animation (ANI), Children (CHI), Comedy (COM),
Crime (CRI), Drama (DRA), Horror (HOR), Sci-Fi (S&F), and
Thriller (THR).

Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer

that there are many patterns that involve more than two domains, and these patterns are missed during the knowledge
transfer. We can also see that the all-intersectional methods (CDTF and extended CLFM) may lead to the negative transfer when adding new domains. Since these methods only capture the correlation between all domains, this
shows that a domain can be correlated to only parts of the
rest domains. So, adding a new domain prevents the allintersectional correlation between the rest domains from
being captured.
Given the above limitations, it is crucial to have a new
CDCF technique that is able to transfer non-linearly correlated knowledge between domains, via an adaptive bridge.

3. Hyper-Structure Transfer
In this section, we propose the notion of Hyper-Structure
Transfer (HST) and its models that allows the non-linearly
correlated knowledge between domains to be transferred.
3.1. General Model
A general model of Hyper-Structure Transfer (HST) can be
written as:
L=



arg min (k) (k)
L U (k) , V (k) , H ,
U
,V
,H

2



(k)
âˆ’ U (k) Ã— proj(k) (H) Ã— V (k)> â—¦ W (k)  ,
k=1  X

Pd

(k)

(k)

subject to constraints U
â‰¥ O, U 1 = 1, V
â‰¥ O,
and V (k) 1 = 1, where d is the number of participating
(k)
(k)
domains, X (k) âˆˆ Rn Ã—m is the rating matrix to ap(k)
(k)
proximate, U (k) âˆˆ Rn Ã—p is the user-cluster matrix
(k)
whose each entry ui,s denote how much the user i belong
(k)

The idea of learning from data in a space more complex
than where the original data reside has appeared in the
machine learning field. For example, a kernel machine
(SchÃ¶lkopf & Smola, 2002) employs a kernel function to
lift the observed data instances (users/items) to a highorder space (called the feature space), and then performs
the learning task in that space. However, HST differs from
the kernel machines in that the target of the lifting is not
observed (it is the unknown correlation between rating patterns to be lifted). Therefore, we cannot simply pass the
correlation into the kernel function to obtain H. The precise definitions of H and the projection function remains
open.

(1)

F

(k)

bridge. Furthermore, the B (k) â€™s can capture the nonlinearly correlation between domain. For example, suppose
in domain k there is a rating pattern that can be depicted as
a blue line in Figure 1. Suppose there is another pattern, a
curve, in domain l. Traditional CDCF methods cannot capture these patterns due to their non-linear correlation, so
any user/item clusters that can only be linked through these
patterns will be missed. On the other hand, the correlation
between these two patterns can be captured in HST by a
more complex structureâ€”a high-order curve. Therefore,
HST can achieve better approximation.

(k)

to the cluster/latent factor s, V (k) âˆˆ Rm Ã—q is the itemcluster matrix, H is a high-order structure, called the hyperstructure, whose dimension is higher than p(k) Ã—q (k) for all
(k)
(k)
k, and proj(k) (Â·) is some projection map onto Rp Ã—q ,
(k)
(k)
â—¦ is the entry-wise product, and W (k) âˆˆ Rn Ã—m is the
indicator matrix of X (k) whose each entry is defined as:
(
(k)
1, if X ij 6= 0,
(k)
W ij =
0, otherwise,
and k Â· kF is the Frobenius norm. Note that the hyperstructure H is fixed across domains. Basically, HST
approximates each X (k) by a dense Y (k) = U (k) Ã—
proj(k) (H) Ã— V (k)> such that they look similar at the positions where the ratings are available. Similar to the existing methods we have seen in Section 1, each Y (k) is
tri-factorized into U (k) B (k) V (k)> for some rating pattern
(k)
(k)
matrix B (k) âˆˆ Rp Ã—q . But instead of fixing B (k) = B
across domains, HST allows B (k) â€™s to be different projections of H.
HST enables the knowledge transfer by using H as a

3.2. MOTAR
The H and the projection function proj(Â·) in Eq. (1) can
be specified by domain experts if some priors knowledge
about the correlation is available. However, it is hard to
know all priors. Next, we present the Minimal Orthogonal
Tensor Approximation with Residuals (MOTAR), a special
case of HST that allows H to capture arbitrarily complex
correlation in the tensor form without the need for priors.
In MOTAR, the H and proj(Â·) are defined as
(1)

H = B âˆˆ Rp

Ã—q (1) Ã—Â·Â·Â·Ã—p(d) Ã—q (d)

(2)

and
(k)

proj

(B)s,t =

p(l)
,q (l)
X

Bi(1) ,j (1) ,Â·Â·Â· ,s,t,Â·Â·Â· ,i(d) ,j (d)

i(l) =1,j (l) =1,âˆ€l6=k

(3)
respectively. Basically, MOTAR assumes that the hyperstructure H is a tensor B with 2d modes, and each
(k)
(k)
proj(k) (B) âˆˆ Rp Ã—q is a projection of B onto some
two modes. The projections are orthogonal to each other as
they target distinct modes. By definition of proj(Â·) in Eq.
(3), 2d is the minimum number of modes that allows B to
be orthogonally projected onto the rating pattern matrices
in different domains, yet B is large enough to capture any
correlation (in tensor form) of arbitrarily complexity:
Theorem 1. Let C be a tensor with c modes, c > 2d, that
denotes the correlation between the ratting patterns of dif-

Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer

ferent domains. MOTAR can capture C by letting
Bs(1) ,t(1) ,Â·Â·Â·
P,s(d) ,t(d) =

i(1) ,Â·Â·Â· ,i(câˆ’2d)

C s(1) ,t(1) ,Â·Â·Â· ,s(d) ,t(d) ,i(1) ,Â·Â·Â· ,i(câˆ’2d) .

The above theorem can be easily proved by showing that
proj(k) (B) = proj(k) (C).
Figure 2. An example cubicization of B over domain 1.

3.3. Residuals for Adaptive Bridge
When multiple domains are available (d > 2), a correlation pattern may involve only partial domains (an example
is given in Section 2.2). The bridge for knowledge transfer needs to be adaptive to capture the patterns involving
arbitrary domain combinations.
MOTAR has another advantage in that the bridge B defined
in Eq. (2) can be readily extended to be adaptive, by creating one extra dimension in each mode, i.e.,
B âˆˆ R(p

(1)

+1)Ã—(q (1) +1)Ã—Â·Â·Â·Ã—(p(d) +1)Ã—(q (d) +1)

,

(4)

such that each the rating pattern matrix proj(k) (B) âˆˆ
(k)
(k)
R(p +1)Ã—(q +1) relates one extra user-cluster and one
extra item-cluster, called the residual clusters. MOTAR requires the columns corresponding to the residual clusters in
the user-cluster and item-cluster matrices to be 0, i.e.,
h
i
h
i>
Y (k) = U (k) , 0 proj(k) (B) V (k) , 0 ,

in space and time, and may even be infeasible) due to the
huge number of variables to solve in B in each iteration.
Instead of updating B directly in the above iterating process, we propose updating proj(k) (B) for each domain alternately to improve the overall approximation. This can
significantly reduces the number of variables to solve for
each update (from Î l p(l) q (l) to p(k) q (k) ). However, it creates another challenge: after updating proj(k) (B) in the
current iteration, how to obtain proj(l) (B) to be updated
in the next iteration? We link proj(k) (B) and proj(l) (B)
based on a key observation, as described below.
Letâ€™s first define a cubicization of B over some domain k,
(k)
(k)
(k)
(l) (l)
denoted by BÌƒ âˆˆ Rp Ã—q Ã—Î l6=k p q , a 3-mode tensor
whose the first and second modes are the modes of domain
k in B and the third mode is the concatenation of the rest
modes in B, as sown in Figure 2. Note that
X (k)
proj(k) (B) =
BÌƒ:,:,h ,
h

(k)

so the values of Y
will not be affected by the extension
of proj(k) (B). This allows the new values in proj(k) (B) to
reflect some partial structure of B that only affects the other
domains. In other words, the residual clusters in each domain denote â€œthe latent factors which cannot be explained
by X (k) in this domain.â€ Therefore, the bridge B, which
links any combination of the user-clusters, item-clusters,
and their negation across different domains, is adaptive.
Finally, we obtain the objective of MOTAR as follows:


L=

(k)

(k)

(k)

(k)

where BÌƒ:,:,h âˆˆ Rp Ã—q denotes the h-th slice of BÌƒ
along the third mode. By CP decomposition, we have
Pz
(k)
(k)
BÌƒ
= r=1 ar  er  cr for some ar âˆˆ Rp , er âˆˆ
(k)
(l) (l)
Rq , cr âˆˆ RÎ l6=k p q and hyperparameter z, as shown
(k)
in Figure 3. Let A(k) = [a1 , Â· Â· Â· , az ] âˆˆ Rn Ã—z , E (k) =
(k)
[e1 , e2 , Â· Â· Â· ] âˆˆ Rm Ã—z , and C (k) = [c1 , c2 , Â· Â· Â· ] âˆˆ
Pz
(k)
(l) (l)
(k) (k) (k)
RÎ l6=k p q Ã—z , we have BÌƒi,j,h =
r=1 Ai,r E j,r C h,r ,
(k)



arg min (k) (k)
L U (k) , V (k) , B ,
U
,V
,B

2
h
i
h
i> 


(k)

âˆ’ U (k) , 0 proj(k) (B) V (k) , 0
â—¦ W (k) 
 ,
k=1  X

Pd

and we can write the h-th slice of BÌƒ along the third mode
as
(k)
(k)
BÌƒ:,:,h = A(k) Î¦h E (k)> ,
(6)

F

(5)

(k)

(k)

(k)

subject to constraints U
â‰¥ O, U 1 = 1, V
â‰¥ O,
and V (k) 1 = 1. Note that the l1 normalization constraints
on each row of U (k) and V (k) keeps the objective welldefined (Gu et al., 2011).

4. Objective Solving
One way to solve Eq. (5) is to randomly initialize B, and
repeat until convergence the process of updating B for each
domain k such that the proj(k) (B) lead to a better approximation. However, this approach is very inefficient (either

(k)

(k)

where Î¦h = diag(C h,: ) âˆˆ RzÃ—z is a diagonal matrix.
This leads to
P (k)
proj(k) (B) = h BÌƒ:,:,h
P
(k)
(7)
= h A(k) Î¦h E (k)>
(k) (k) (k)>
=A Î¨ E
,
P
(k)
where Î¨(k) = h Î¦h is diagonal.
Eq. (7) implies that, during each iteration, we can simply
update proj(k) (B) by updating A(k) and E (k) . Further(k)
more, if we fix Î¨(k) (and Î¦h , âˆ€h), we can use Eq. (6) to

Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer

Figure 3. The CP decomposition of a three-order tensor

reconstruct B and then obtain the proj(l) (B) to be updated
in the next iteration by using Eq. (7) again. The number of
variables to solve in A(k) and E (k) is much smaller than
that in B.
With the above observation, we can employ the multiplicative gradient descent method (Ding et al., 2006) to update

Mean Absolute Error (MAE)

0.7
0.6

MOTAR
TALMUD
CDTF
CLFM

0.5
0.4
0.3
0.2
0.1
0

1

2

3

4

Number of Domains

the UÌƒ
= [U (k) , 0], VÌƒ
= [V (k) , 0], A(k) , and E (k) in
each iteration using the update rules described in Section 1
of the supplementary material.

Figure 4. Performance comparison over multiple domains in the
DBLP dataset.

5. Experiment

DBLP Citation Dataset (Tang et al., 2008). The DBLP
citation dataset contains 180,640 authors (users), 141,507
papers (items) from the year 1960 to 2010. We regard
(k)
each rating X i,j as 1 if the user i has ever cited the paper j. Based on this definition, there are 1,495,081 ratings
in this datasets. We then partition these ratings into different X (k) â€™s by using the categories listed in the Microsoft
Academic Search website.

(k)

(k)

In this section, we conduct experiments over both synthetic
and real datasets to compare the performance of MOTAR
with that of the state-of-the-art CDCF techniques.
Metric. We adopt a widely used metric for the CF
problemsâ€”the Mean Absolute Error (MAE) (Su & Khoshgoftaar, 2009)â€”to evaluate the prediction quality of Y (k)
(k)

X

M AE(Y (k) ; Z (k) ) =
(k)
Z i,j
(k)

is not blank

(k)

|Z i,j âˆ’ Y i,j |
kZ (k) k

,

(k)

where Z (k) âˆˆ Rn Ã—m denotes the ratings that are held
out from X (k) during the training process, and kZ (k) k is
the number of ratings in Z (k) . The smaller the MAE, the
higher the accuracy. We hold out data by time. Specifically,
we cut the elder 90% ratings to X (k) and the later 10% to
Z (k) . And we tune the parameters by randomly selecting
some ratings from X (k) as the validation set.
Baselines. We compare MOTAR with the TALMUD
(Moreno et al., 2012), CDTF (Hu et al., 2013) and CLFM
(Gao et al., 2013a). Note that the CDTF assumes the
users in different domains are identical, so we preprocess
the datasets accordingly to allow the comparison. Generally, the objectives of in existing CDCF methods (including MOTAR) are not convex thus the solutions are subject
to local minimum. We overcome this by randomly initializing the variables to solve 20 times and pick the resulting
Y (k) â€™s that achieve the lowest objective score. These strategy works well for all the methods considered.
We employ two real datasets: the DBLP citation dataset
and the MovieLens movie rating dataset.

MovieLens Rating Dataset.4 The MovieLens rating
dataset contains 69,878 users, 10,677 movies (items), and
10,000,054 ratings from the year 1970 to 2009. We use categories listed in its official website to divide the ratings into
different domains. Some statistics of the above datasets are
given in Table 1 of the supplementary material.
Note that some baseline methods such as CDTF require
the users from different domains to be identical. So we
prune users that give ratings in only a single domain (this
setting is in favor of baselines). Also, we observe that
users who have few publications/reviews tend to give ratings to random items. With these users, all algorithms give
very poor performance because the test set contains random
pulls mostly. Therefore, we also prune users who have few
publications/reviews.
5.1. Results over Two Domains
Table 1 shows the MAEs of different CDCF methods
over different domain pairs in the DBLP and MovieLens
datasets. We randomly select 9 pairs of domains from
each of the datasets respectively and report the MAEs in
domain 1. The results show that MOTAR outperforms
the baselines in almost all cases. The last column of
4

http://grouplens.org/datasets/movielens/.

MovieLens

DBLP

Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer

Domain 1
A&T
A&T
DP&C
D&PC
HCI
HCI
NL&S
NL&S
NL&S
ACT
ADV
ANI
CHI
COM
CRI
DRA
HOR
S&F

Domain 2
DM
NL&S
DB
AI
DM
WWW
AI
ML&PR
DB
COM
THR
COM
ACT
ACT
THR
THR
THR
THR

NLR of domain 1
0.430629
0.439931
0.411229
0.394680
0.494192
0.540556
0.393302
0.379950
0.413516
0.364349
0.464158
0.516600
0.540419
0.518781
0.344379
0.430730
0.445017
0.450324

MOTAR
0.118268
0.132746
0.116443
0.114169
0.130390
0.125405
0.104989
0.090102
0.088783
0.316164
0.288821
0.306506
0.304316
0.380733
0.275506
0.327201
0.254903
0.293761

TALMUD
0.174754
0.252289
0.388081
0.428092
0.437423
0.436462
0.333125
0.291277
0.356582
0.327645
0.337397
0.395720
0.440897
0.402398
0.332996
0.386229
0.385203
0.397893

CDTF
0.181608
0.242860
0.244012
0.259215
0.316056
0.280759
0.189784
0.185565
0.202629
0.494919
0.501841
0.516925
0.593867
0.638214
0.480467
0.591035
0.552813
0.579108

CLFM
0.479545
0.488257
0.537190
0.541288
0.693805
0.706178
0.493210
0.491606
0.498284
0.597450
0.622482
0.623446
0.635377
0.616576
0.536740
0.628440
0.590041
0.628371

Table 1. Performance comparison over two domains.

5.2. Results over Multiple Domains
Now, we evaluate the performance of MOTAR over multiple domains (d > 2). We randomly select some pairs of
domains in the DBLP and MovieLens datasets, then we run
different algorithms and show their average MAEs as well
as the standard deviation in Figures 4 and 5.
From the results, we can see that MOTAR significantly outperforms the other baselines in all cases due to the nonlinear knowledge transfer. Note that when the number of
domain is 1, all algorithms are reduced to the standalone
tri-factorization, so all of them have the same error. Furthermore, we can also see that MOTAR gives more performance gain as the number d of domains increases. The
more domains available, the more complex their correlation patterns can be. Thus, MOTAR has a higher chance to
capture those patterns than the others when d is large.
Another important observation is that MOTAR does not

0.8

Mean Absolute Error (MAE)

Table 1 shows the degrees of non-linearity (mentioned
in Section 2) between different domain pairs. In general, the degree of non-linearity between each pair of domains are about 0.35~0.5, and we can see how the nonlinearity affects the performanceâ€”the higher the degree of
non-linearity, the more the MAE in the baseline methods.
This shows that the performance of existing CDCF methods are largely limited by the sub-structure sharing technique, which can only transfer linearly correlated knowledge across domains. MOTAR, on the other hand, avoid
this problem by sharing the hyper-structure.

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

MOTAR
TALMUD
CDTF
CLFM
1

2

3

4

Number of Domains

Figure 5. Performance comparison over multiple domains in the
MovieLens dataset.

tend to result in the negative knowledge transfer as we
have discussed in Section 2.2. We can see from the figures
that the MAE of MOTAR decreases monotonously as d increases. This is because that MOTAR employs an adaptive
bridge that can capture the correlation patterns involving
any domain combination.
5.3. Effects of Non-linearity
To validate that MOTAR can capture arbitrarily complex
correlations between domains, we create a synthetic dataset
with known B in the ground truth to simulate the non-linear

Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer

Figure 6. The effects of non-linearity.

condition. We then obtain the MAEs of MOTAR over this
datatset by varying the number of modes of B in the ground
truth. We focus on two domains only to set aside the problem of maladaptive bridges (to be discussed in the supplementary material).
To create the dataset, we first define the number of users
n(k) (resp. items m(k) ), the number of user-clusters p(k)
(resp. item-clusters q (k) ) in k domains. And to control
the non-linear correlation, we introduce extra modes in B.
The number of the extra modes is denoted by exM and the
number of dimensions in each extra modes is denoted by
r(l) , where 1 â‰¤ l â‰¤ exM . We define the latent factors of
users (resp. items) for each domain in ground truth, and
randomly assign the latent factors to each user (resp. item)
in the same domain.
(k)

(1)

(2)

Here we fix each n
= 20, m = 40, m = 50 and
each p(k) = q (k) = r(k) = 6, and varies exM to simulate the non-linear complexity. And we generate the rating
matrices by using X (k) = U (k) proj(k) (B) V (k) . Figure.
6 shows the accuracy achieved by MOTAR model under
different degree of the non-linearity. As we can see, the
performance of MOTAR is unaffected by the extra modes
in B. This justifies that MOTAR can capture arbitrarily
complex correlations between domains.

6. Conclusions and Future Work
We propose the notion of Hyper-Structure Transfer (HST)
and its model called the Minimal Orthogonal Tensor Approximation with Residuals (MOTAR) that transfers nonlinearly correlated knowledge between domains in the tasks
of Cross-Domain Collaborative Filtering (CDCF). To the
best of our knowledge, this is the first work that enables
non-linear knowledge transfer. Empirical results show that
MOTAR can achieve up to 50% reduction in prediction error rate as compared with the state-of-the-art CDCF methods. In addition, MOTAR always give better performance
when taking into account new domains.
HST opens up numerous research directions in the future.
First, there are many other ways to select H and the projection function proj(Â·) in Eq. (1). For example, H could
be some manifold in a high-order geometric space and the

proj(k) (H)â€™s could be its local patches (overlapping with
each other). Second, one can develop new regularizers
for HST/MOTAR that best suite the targeted recommender
systems where some particular side information (e.g., tags,
implicit feedbacks, etc.) is available. Third, the in-depth
studies about the non-linear correlation between domain
knowledge will be valuable. Theoretical guarantee about
the non-linear transferability wold be important too. Last
but not the least, we find that the CP decomposition is the
major performance bottleneck when solving the MOTAR
objectives (despite that the training Algorithm shown in the
supplementary material converges after only tens of outeriterations in most cases). Alternatives to CP decomposition
will be valuable for large-scale settings.

References
Argyriou, A., Maurer, A., and Pontil, M. An algorithm
for transfer learning in a heterogeneous environment. In
Proc. of ECML PKDD, 2008.
Bader, Brett W. and Kolda, Tamara G. Algorithm 862:
MATLAB tensor classes for fast algorithm prototyping. ACM Trans. on Mathematical Software, 32:635â€“
653, 2006.
Bakker, Bart and Heskes, Tom. Task clustering and gating
for bayesian multitask learning. The Journal of Machine
Learning Research, 4:83â€“99, 2003.
Ben-David, S. and Schuller, R. Exploiting task relatedness for multiple task learning. In Proc. of COLT/Kernel,
2003.
Cao, Bin, Liu, Nathan N, and Yang, Qiang. Transfer learning for collective link prediction in multiple heterogenous domains. In Proc. of ICML, 2010.
Chen, Wei, Hsu, Wynne, and Lee, Mong Li. Making recommendations from multiple domains. In Proc. of KDD,
2013.
Ding, Chris, Li, Tao, Peng, Wei, and Park, Haesun. Orthogonal nonnegative matrix t-factorizations for clustering. In Proc. of KDD, 2006.
Gao, Sheng, Luo, Hao, Chen, Da, Li, Shantao, Gallinari,
Patrick, and Guo, Jun. Cross-domain recommendation
via cluster-level latent factor model. In Proc. of ECML
PKDD, 2013a.
Gao, Sheng, Luo, Hao, Chen, Da, Li, Shantao, Gallinari,
Patrick, Ma, Zhanyu, and Guo, Jun. A cross-domain recommendation model for cyber-physical systems. IEEE
Trans. on Emerging Topics in Computing, 1:384â€“393,
2013b.

Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer

Gu, Quanquan, Ding, Chris, and Han, Jiawei. On trivial
solution and scale transfer problems in graph regularized
nmf. In Proc. of IJCAI, 2011.

Su, Xiaoyuan and Khoshgoftaar, Taghi M. A survey of
collaborative filtering techniques. Advances in artificial
intelligence, 2009:4, 2009.

Hofmann, Thomas. Latent semantic models for collaborative filtering. ACM Trans. on Information Systems, 22:
89â€“115, 2004.

Tang, Jie, Zhang, Jing, Yao, Limin, Li, Juanzi, Zhang, Li,
and Su, Zhong. Arnetminer: extraction and mining of
academic social networks. In Proc. of KDD, 2008.

Hu, Liang, Cao, Jian, Xu, Guandong, Cao, Longbing, Gu,
Zhiping, and Zhu, Can. Personalized recommendation
via cross-domain triadic factorization. In Proc. of WWW,
2013.

Zhang, Yu, Cao, Bin, and Yeung, Dit-Yan. Multi-domain
collaborative filtering. arXiv preprint arXiv:1203.3535,
2012.

Hu, Yifan, Koren, Yehuda, and Volinsky, Chris. Collaborative filtering for implicit feedback datasets. In Proc. of
ICDM, 2008.
Kiers, Henk AL, Ten Berge, Jos MF, and Bro, Rasmus.
Parafac2-part i. a direct fitting algorithm for the parafac2
model. Journal of Chemometrics, 13:275â€“294, 1999.
Kolda, Tamara G and Bader, Brett W. Tensor decompositions and applications. SIAM review, 51:455â€“500, 2009.
Koren, Yehuda, Bell, Robert, and Volinsky, Chris. Matrix factorization techniques for recommender systems.
Computer, 42:30â€“37, 2009.
Li, Bin. Cross-domain collaborative filtering: A brief survey. In Proc. of ICTAI, 2011.
Li, Bin, Yang, Qiang, and Xue, Xiangyang. Can movies
and books collaborate? cross-domain collaborative filtering for sparsity reduction. In Proc. of IJCAI, 2009.
Long, Mingsheng, Wang, Jianmin, Ding, Guiguang, Shen,
Dou, and Yang, Qiang. Transfer learning with graph coregularization. In Proc. of AAAI, 2012.
Moreno, Orly, Shapira, Bracha, Rokach, Lior, and Shani,
Guy. Talmud: transfer learning for multiple domains. In
Proc. of CIKM, 2012.
Pan, Weike, Xiang, Evan Wei, Liu, Nathan Nan, and Yang,
Qiang. Transfer learning in collaborative filtering for
sparsity reduction. In Proc. of AAAI, 2010.
Rosenstein, Michael T, Marx, Zvika, Kaelbling,
Leslie Pack, and Dietterich, Thomas G. To transfer or not to transfer. In Proc. of NIPS, 2005.
SchÃ¶lkopf, Bernhard and Smola, Alexander J. Learning
with kernels: support vector machines, regularization,
optimization, and beyond. MIT press, 2002.
Shi, Yue, Larson, Martha, and Hanjalic, Alan. Tags as
bridges between domains: Improving recommendation
with tag-induced cross-domain collaborative filtering.
User Modeling, Adaption and Personalization, 6787:
305â€“316, 2011.

