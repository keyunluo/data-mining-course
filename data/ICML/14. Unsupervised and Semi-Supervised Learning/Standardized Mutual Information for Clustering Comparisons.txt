Standardized Mutual Information for Clustering Comparisons:
One Step Further in Adjustment for Chance

Simone Romano
SIMONE . ROMANO @ UNIMELB . EDU . AU
James Bailey
BAILEYJ @ UNIMELB . EDU . AU
Nguyen Xuan Vinh
VINH . NGUYEN @ UNIMELB . EDU . AU
Karin Verspoor
KARIN . VERSPOOR @ UNIMELB . EDU . AU
Department of Computing and Information Systems, The University of Melbourne, Victoria, Australia

Abstract
Mutual information is a very popular measure
for comparing clusterings. Previous work has
shown that it is beneficial to make an adjustment for chance to this measure, by subtracting
an expected value and normalizing via an upper
bound. This yields the constant baseline property that enhances intuitiveness. In this paper,
we argue that a further type of statistical adjustment for the mutual information is also beneficial
– an adjustment to correct selection bias. This
type of adjustment is useful when carrying out
many clustering comparisons, to select one or
more preferred clusterings. It reduces the tendency for the mutual information to choose clustering solutions i) with more clusters, or ii) induced on fewer data points, when compared to a
reference one. We term our new adjusted measure the standardized mutual information. It requires computation of the variance of mutual information under a hypergeometric model of randomness, which is technically challenging. We
derive an analytical formula for this variance and
analyze its complexity. We then experimentally
assess how our new measure can address selection bias and also increase interpretability. We
recommend using the standardized mutual information when making multiple clustering comparisons in situations where the number of records is
small compared to the number of clusters considered.

1. Introduction
Clustering techniques aim at partitioning data by grouping
objects with similar characteristics into homogeneous clusters (Aggarwal & Reddy, 2013). External validation techProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

niques, which compare a clustering solution to a reference
clustering, are therefore extremely important to assess clustering quality. However, there is no acknowledged measure
of choice to compare partitions and in practice many measures are used (Wu et al., 2009). Among those, there exist pair-counting based measures such as the Rand Index
(RI) (Rand, 1971) and Jaccard coefficient (Ben-Hur et al.,
2001), as well as information theoretic measures such as
the Mutual Information (MI) (Cover & Thomas, 2012) and
Variation of Information (VI) (Meilă, 2007). We provide a
glossary of acronyms used in this paper in Table 1.
A desirable property of clustering comparison measures is
to have a constant baseline in the case of random independent partitions. Adopting a probabilistic interpretation of
the partition problem, an expected value can be computed
under the assumption of random and independent clusterings and then subtracted from the measure. Adjusted for
chance measures include the Adjusted Rand Index (ARI)
(Hubert & Arabie, 1985) and the Adjusted Mutual Information (AMI) (Vinh et al., 2009). They take a 0 expectation
value when partitions are independent, and are bounded
above by 1 via the use of a normalization factor (an upper
bound of the measure).
In this paper, our focus is on information theoretic measures, in particular the mutual information. Our first key
observation is that employing a baseline adjustment to the
mutual information does not guarantee that it is bias free.
In fact, it is still susceptible to what we term selection bias,
a tendency to choose inappropriate clustering solutions i)
with more clusters, or ii) induced on fewer data points,
when compared to a reference one. To illustrate, consider
the following experiment on a dataset of 500 records: a reference clustering of 10 equal-size clusters is compared in
turn with 6 clustering solutions. Each solution is randomly
generated with equal-size clusters and the number of clusters in each is 2, 6, 10, 14, 18 and 22 respectively. We
then use the AMI measure to select the most similar clustering solution to the reference and then repeat the whole
procedure 5,000 times. Figure 1 shows the probability of
selecting a clustering solution with c clusters. We see that a

Prob. of selection

Standardized Mutual Information for Clustering Comparisons

0.2

0.1

0

2

6

10

14

18

22

Number of clusters c

Figure 1. Probability AMI measure selects a random clustering
solution with c clusters as the most similar solution to the reference clustering which has 10 clusters. AMI is biased towards
selecting clustering solutions with a higher number of clusters.

clustering with 22 clusters will be selected more often than
one with 2 clusters, even though we expect the former solution should be no more similar to the reference than the
latter solution, due to the randomness of generation. So,
although the AMI may have a constant baseline, it is still
biased towards selecting clusterings containing more clusters.
To address this issue of selection bias, we go one step further in the direction of probabilistic adjustment for chance
on information theoretic measures: standardizing mutual
information. Using both the expected value (E[MI]) and
the variance (Var) obtained under an assumption of random
and independent clusterings, we propose the Standardized
Mutual Information (SMI) measure as follows:
MI − E[MI]
SMI = p
Var(MI)

compared to the number of clusters considered. Practically,
this is the situation commonly encountered in life science
and medical research. In cancer study via microarray data
analysis for example, there might be up to thirteen subtypes
of cancer on a data set of as few as 90 microarray samples
(Monti et al., 2003). There also exist a number of important application areas such as external clustering validation,
generation of alternative or multi-view clusterings (Müller
et al., 2013), categorical feature selection (each feature can
be seen as a clustering), and the exploration of the clustering space using results from the Meta-Clustering algorithm (Caruana et al., 2006) when the task it to find similar/dissimilar clustering from a query one.
Overall, the contributions of this paper are as follows: i)
We identify new biases of the mutual information when
comparing clusterings: selection bias towards clusterings
with more clusters and selection bias towards clusterings
induced on fewer records; ii) We propose the standardized
mutual information measure (SMI) to address these bias issues; iii) To compute the SMI, we provide an analytical
formula for calculating the variance of mutual information
and analyze its complexity.
Acronym
MI
NMI
AMI
SMI
VI
RI
ARI

Full name
Mutual Information
Normalized Mutual Information
Adjusted Mutual Information
Standardized Mutual Information
Variation of Information
Rand Index
Adjusted Rand Index

(1)

The SMI value is the number of standard deviations the
mutual information is away from the mean value. This provides insight into the statistical significance of the mutual
information between the clusterings. However, a key technical challenge is how to compute the variance term in the
formula. We will argue that SMI has a number of desirable
properties:
• When looking for the most similar clusterings to a reference one or performing external validation against a
reference clustering, it reduces the bias towards selecting clusterings with more clusters or induced on fewer
data points;
• The value of SMI has good interpretability, being a
count of the number of standard deviations the mutual
information is from the mean, under a null distribution
of random clustering solutions with fixed marginals.
We advocate the use of the standardized mutual information especially when making multiple clustering comparisons in situations where the number of records is small

Table 1. Acronyms used in this paper.

2. Background and Related Work
We start by reviewing the literature on related work. Next,
we introduce some notation and specifically focus on information theoretic measures and literature on adjustment for
chance.
2.1. Partition Comparison Measures and their Bias
Measures of agreement between two clusterings might
be unfairly inflated just because of statistical fluctuations.
Commonly used measures for clustering comparisons, such
as the RI, show an increasing trend when the number of
clusters increases even if clusterings are random and independent. In (Hubert & Arabie, 1985) it was proposed to
adjust a measure M for chance as follows:
M − E[M ]
max M − E[M ]

(2)

An analytical formula for the expected value is used to remove the baseline component of the measure. The expected

Standardized Mutual Information for Clustering Comparisons

value for measure M is computed under the null hypothesis of random and independent clusterings and max M is
an upper bound for M that acts as normalization factor.
The model of randomness adopted to compute the expected
value is the permutation model, also called the hypergeometric model: fixing the number of points for each cluster,
partitions are generated uniformly and randomly via permutations of records. Under this assumption, the distribution of measure M is known and thus so is the expected
value. It has been shown that this hypothesis behaves well
in practical scenarios and it was recently employed to compute the expected value of MI in (Vinh et al., 2009).
The problem of exaggerated agreement between partitions
due to chance has been also extensively studied in the decision tree literature. For each internal node in a decision tree, the best partitioning according to feature values,
termed split, is selected in accordance with a splitting criterion that quantifies its predictiveness towards the target
classification. Splitting criteria are in fact clustering comparison measures that aim at comparing the clustering induced by the split and the one induced by the target classification. Since the very first implementation of decision
trees, ad hoc methods to reduce the bias toward selection of
splits with many values have involved normalization (Quinlan, 1993). However, even with normalization, partitions
with higher cardinality are more preferred (White & Liu,
1994). A key pitfall pointed out for splitting criteria is the
lack of statistical significance concepts. This was formalized in (Dobra & Gehrke, 2001) where it has been proven
that “a p-value of any split criterion is a nearly unbiased
criterion”. On the other hand, the use of the p-value is
controversial. It has indeed been claimed that the p-value
based on the chi-square distribution for splitting criteria
such as the G-statistic “is not able to distinguish between
more and less informative attributes” (Kononenko, 1995).
This behaviour is mainly due to computer precision limitations when computing the p-value for informative features
to the class.
The distribution of MI could be approximated by considering the G-statistic used in goodness-of-fit tests, since the
G-statistic is a scaled version of the MI. The G-statistic distribution can be approximated by a chi-square distribution,
but it is well known that this approximation becomes poor
when the number of objects is small in regards to the number of clusters of the clusterings compared. In particular, it
is inappropriate when there exists a cluster of one clustering that shares less than 5 records with any of the clusters
of the clustering compared (Agresti, 2002). Thus, its applicability for clustering comparisons is limited. Alternatively, one might attempt a brute force exact computation
of the distribution of MI under the hypergeometric model
of randomness, to obtain a p-value. However, this rapidly
becomes infeasible, even for modest sized cases. Indeed,
it is as hard as computing the p-value for the Fisher’s ex-

act test, which is not used in clustering comparison due to
its computational demand. Although there exist methods to
speed up Fisher’s exact test using graph-based algorithms,
it is still asymptotically and practically slow (Mehta & Patel, 1983). An exact p-value for MI suffers from the same
problems. A common workaround is to estimate the MI
distribution via Monte Carlo simulations (Frank & Witten,
1998). However, this method is still time consuming when
a given degree of accuracy is required and it does not provide an exact (analytical) result for the value of the variance.
In contrast, we will shortly see that, it is possible to analytically compute the variance for mutual information under
the hypergeometric model of randomness to standardize it,
and this computation is orders of magnitude faster than a
brute force computation of the full distribution. Moreover,
a standardized measure can discriminate between clustering solutions that show high agreement with the reference
clustering better than a p-value because is less prone to
computer precision errors. Therefore, we use the variance
and the expected value to standardize the mutual information (the SMI measure) and experimentally demonstrate
that employing SMI can decrease the bias towards selecting
clusterings with more clusters and towards selecting clusterings estimated on fewer data points.
2.2. Notation and Information Theoretic Measures
Let A and B be two clusterings of a dataset consisting of
N records. Let A cluster the data in r clusters and define
ai as the size of cluster i = 1, . . . , r, and let B cluster the
data in c clusters
Pr of size
Pbcj for each cluster j = 1, . . . , c.
Naturally, i=1 ai = j=1 bj = N . Given that A and
B are partitions of the same data it is possible to count
the elements that belong both to cluster i and j. Let nij
denote the number of records shared between cluster i and
j. The overlap between two clusterings can be represented
in matrix form by a r × c contingencyPtable M such as
the one in Table 2. We refer to ai =
j nij as the row
P
marginals and to bj = i nij as the column marginals.

A

a1
..
.

b1
n11
..
.

ai
..
.

·
..
.

ar

nr1

···
···

B
bj
·
..
.

···
···

·
..
.

nij
..
.
···

·

bc
n1c
..
.

···

nrc

Table 2. r × cP
contingency table M related to two clusterings
A
P
and B. ai = j nij are the row marginals and bj = i nij are
the column marginals.

In order to employ information theory to measure the

Standardized Mutual Information for Clustering Comparisons

agreement between partitions, we have to treat the clusterings as random variables. Using the maximum likelihood
estimation method, we estimate the empirical joint probab
n
bility distribution of clusterings A and B as aNi , Nj , and Nij
for the probability that an element falls in cluster i, cluster
j, and both cluster i and j respectively. The entropy for a
clustering is defined as the expected value of its information content if it is seen as a random variable. We can therefore define entropy for clustering A and B as follows1 :
Pr
Pc b
b
H(A) , − i=1 aNi log aNi , H(B) , − j=1 Nj log Nj .
Mutual information MI(A, B) quantifies the value of information shared between the two random variables and
can be defined using the entropy definitions: MI(A, B) ,
Pr Pc n
n N
H(A) + H(B) − H(A, B) = i=1 j=1 Nij log aiji bj
where H(A, B) is the joint entropy between clusterings.
Intuitively, it computes the total amount of uncertainty of
each variable independently minus the uncertainty when
put together. Naturally, mutual information between two
clusterings can also be computed from their associated contingency table, that is MI(M) = MI(A, B). When it is
obvious, we drop the arguments and simply write MI for
mutual information computed between two clusterings.
The mutual information has many possible upper bounds
that might be used to obtain the Normalized Mutual Information (NMI): MI(A, B) ≤ min {H(A), H(B)} ≤
p
1
H(A) · H(B)
≤
≤
2 (H(A) + H(B))
max {H(A), H(B)} ≤ H(A, B).
Depending on
the chosen upper bound, it is possible to obtain information theoretic distance measures with metric properties
(Vinh et al., 2010). A distance measure with metric
properties is indeed useful for designing efficient algorithms that exploit the nice geometric properties of metric
spaces (Meilă, 2012). An example of a true metric is the
variation of information (VI), defined in (Meilă, 2007):
VI(A, B) , H(A) + H(B) − 2MI(A, B).
In order to adjust the MI for chance as in equation (2) we
have to compute the expected value over all possible contingency tables M with fixed number of points and fixed
marginals and this is extremely time expensive. It has also
been shown that the mere counting of such contingency
tables with fixed marginals is #P-complete (Dyer et al.,
1997). In (Vinh et al., 2009) the complexity of the problem
has been dramatically reduced by reordering the sums in
E[MI]:

geometric distribution and reduces the complexity to linear in the number of records, as we will show in Section 3.3. According to the adopted permutation model,
Nij is a hypergeometric distribution that models the sampling without replacement of ai records among N possible ones where the number of total successes is bj :
−bj
(nbj )(aN−n
)
Nij ∼ Hyp(ai , bj , N ), P (nij ) = ij Ni ij , nij ∈
(ai )
[max {0, ai + bj − N }, min {ai , bj }]. Note that if we
swap ai with bj we obtain the same probability distribution, i.e. Hyp(ai , bj , N ) = Hyp(bj , ai , N ).
AMI2 is computed according equation (2):
AMI = p

MI − E[MI]
H(A) · H(B) − E[MI]

(3)

3. Standardization of Information Theoretic
Measures
In Section 3.1 we provide an analytical formula for the
variance of mutual information under the hypergeometric
model of randomness. We use the variance to standardize
information theoretic measures in Section 3.2. The computational complexity of SMI is derived in Section 3.3.
3.1. Variance of Mutual Information
In order to compute the variance of MI we need to compute
its second moment E[MI2 ]. For this purpose, we first set
up an additional pair of indexes i0 and j 0 in order to take
care of all possible cross-products between cells:

E[MI2 ] =

X
M


2
c
r X
X
n
n
N
ij
ij

 P (M) =
log
N
a
b
i
j
i=1 j=1

(4)

X X nij
nij N ni0 j 0
ni0 j 0 N
log
log
P (M) =
·
N
a i bj
N
ai0 bj 0
0 0
M i,j,i ,j

X X X nij
nij N ni0 j 0
n i0 j 0 N
log
·
log
P (nij , ni0 j 0 )
N
ai bj
N
ai0 bj 0
0 0 n
n

i,j,i ,j

ij

i0 j 0

As in the mean value computation, we can swap the
outer summation across all contingency tables (with fixed
marginals) and sum over all possible values for cells (i, j)
and (i0 , j 0 ). Yet, it is difficult to compute the joint probability distribution P (nij , ni0 j 0 ) for two general cells in the
X
X X nij
nij N
log
P (M) contingency table and it is necessary to treat cells differE[MI] =
MI(M)P (M) =
N
a
b
i
j
ently according to their positions. Moreover, two cells beM
M i,j
longing to two different rows and columns are inherently
X X nij
nij N
=
log
P (nij )
interacting through the remaining cells that their rows and
N
ai bj
i,j n
ij

The inner summation varies over the support of a hyper1

All logarithms are considered in base 2, log ≡ log2

p
We choose to normalize AMI with
H(A) · H(B)
(AMIsqrt ) here and in the rest of the paper as proposed in (Vinh
et al., 2010) given that the experimental results discussed in Section 4 are similar with other normalization factors.
2

Standardized Mutual Information for Clustering Comparisons

able Ni0 j 0 |Nij distributes differently depending on the possible combinations of indexes i, i0 , j, j 0 :

N
red

Case 1: i0 = i ∧ j 0 = j
n

bj

i0 j

nij

ai0

This is the simplest case, in which P (ni0 j 0 |nij ) = 1 if and
only if ni0 j 0 = nij and 0 otherwise. This case produces the
n
n N
first term Nij log aiji bj enclosed in square brackets.

ni0 j 0

Case 2: i0 = i ∧ j 0 6= j
nij 0

ai

bj 0

Figure 2 comes in help when we focus on Nij 0 |Nij . In this
case, the possible successes are the bj 0 blue marbles. We
have already sampled nij red marbles and we are not interested in red marbles any more, thus the total ones available
are N − bj . Thus, Nij 0 |Nij ∼ Hyp(ai − nij , bj 0 , N − bj ).

blue

Figure 2. Urn containing N marbles among which bj are red, and
bj 0 are blue, and N − bj − bj 0 are white.

columns have in common. This means when we consider
cells (i, j) and (i0 , j 0 ), we also have to take care of cells
(i, j 0 ) and (i0 , j).
Theorem 1. The variance of MI under the hypergeometric
hypothesis is Var(MI) = E[MI2 ] − E[MI]2 where
"
c X
r X
X
n
n
N
ij
ij
E[MI2 ] =
log
P (nij )
×
N
ai bj
i=1 j=1 n
ij

nij N X X ni0 j
n i0 j N
nij
log
+
log
P (ni0 j |nij )
N
ai bj
N
ai0 bj
0
n
i 6=i

XX
j 0 6=j nij 0

P (nij 0 |nij )

nij 0
nij 0 N
log
N
ai bj 0

X X n i0 j 0
ni0 j 0 N
log
P (ni0 j 0 |nij 0 , nij )
N
ai0 bj 0
0
n

i 6=i

+

i0 j

+
!

#

i0 j 0

with Nij ∼ Hyp(ai , bj , N )
Ni0 j |Nij ∼ Hyp(bj − nij , ai0 , N − ai )
Nij 0 |Nij ∼ Hyp(ai − nij , bj 0 , N − bj )
Ni0 j 0 |Nij 0 , Nij ∼ Hyp(ai0 , bj 0 − nij 0 , N − ai )
Proof. In order to make the derivation easier to follow, we
employ an urn model with red, blue, and white marbles as
illustrative example. Figure 2 represents such urn containing N total marbles among which bj are red, bj 0 are blue,
and N − bj − bj 0 are white. This urn is used to simulate
the sampling experiment without replacement modelled by
the hypergeometric distribution. For example, it is easy to
see that the random variable Nij defined above models the
probability of obtaining nij red marbles among ai drawn
from an urn of N marbles among which bj are red.
We rewrite the joint probability as a product of conditional probabilities P (nij )P (ni0 j 0 |nij ). The random vari-

Case 3: i0 6= i ∧ j 0 = j
This case is symmetric to the previous one where ai0 is now
the possible number of successes. Therefore Ni0 j |Nij ∼
Hyp(bj − nij , ai0 , N − ai ).
Case 4: i0 6= i ∧ j 0 6= j
This is the most complicated case. When all indexes are
different we cannot write Ni0 j 0 |Nij as a single hypergeometric distribution. We might think about this scenario as
the second draw from the urn in Figure 2. We have already
sampled ai marbles focusing on the red ones as successes.
We are now going to sample other ai0 marbles but focusing
on blue ones as successes. Just knowing that nij red ones
have already been sampled does not allow us to know how
many blue ones remain in the urn. Indeed, only with that information we can obtain the hypergeometric distribution. If
we know that nij 0 blue marbles have already been sampled
we know there are bj 0 − nij 0 possible successes and thus
Ni0 j 0 |Nij 0 , Nij ∼ Hyp(ai0 , bj 0 − nij 0 , N − ai ). Finally, by
the
P law of total probability we can obtain P (ni0 j 0 |nij ) =
0 0
0
0
nij 0 P (ni j |nij , nij )P (nij |nij ). Note that we could
have conditioned on ni0 j and obtained the symmetric version of the above probability. The result follows from algebraic manipulations of equation (4).
3.2. Standardized Mutual Information
We can obtain standardized versions of information theoretic measures knowing the mean value and standard deviation of the MI as per Eq. (1). An interesting point to
note is that, standardization unifies several existing measures for clustering. To see this, let us define the Standardized Variation of Information (SVI) and Standardized Gstatistic (SG):
E[VI] − VI

SVI = p

Var(VI)

,

G − E[G]
SG = p
Var(G)

Theorem 2. Standardization unifies the mutual information MI, variation of information VI and the G-statistic:
SMI = SVI = SG.

Standardized Mutual Information for Clustering Comparisons

Proof. H(A) and H(B) are constant under the fixed
marginal assumption, thus the VI is a linear function of the
MI under the hypergeometric hypothesis. The G-statistic is
equal to a linear scaling of the MI (G = 2N · loge (2) · MI).
The standardized version of a linear function of MI is equal
to SMI because of the properties of expected value and
variance.
This ‘unification’ property is useful, recalling that for the
normalized mutual information NMI and adjusted mutual information AMI, depending on the upperbound used,
there can be as many as 5 different variants for each measure (Vinh et al., 2010).

to reduce the bias toward clusterings with more clusters and
towards those estimated on fewer data points.
4.1. Interpretability
We provide an example that strongly highlights the improved interpretability of SMI in comparison to the AMI.
We are aiming to determine whether a clustering solution
(Clustering B) is significant compared to a random solution when performing external validation against a reference clustering (Clustering A). Both clusterings consist
of 2 clusters of equal size 50 and their cluster overlap is
represented in the contingency table in Table 3. The agreeB

3.3. Computational Complexity
Significant computational speedups might be obtained in
computing the expected value of mutual information if
probabilities are computed iteratively as: P (nij + 1) =
(ai −nij )(bj −nij )
P (nij ) (nij +1)(N
−ai −bj +nij +1) . Here, we give a novel result characterizing the complexity of computing the expected MI:
Theorem 3. The computational complexity for E[MI] is
O(max {rN, cN }).
The proof is in the supplementary material. The computational complexity of computing the mean value is linear
in the number of records and symmetric in c and r. This
does not happen for the variance of mutual information,
where we have to choose whether to condition on either
nij 0 or ni0 j and this leads to an asymmetric computational
complexity in regards to the number of rows and columns.
Choosing to condition on nij 0 as in Theorem 1, the computational complexity for SMI is dominated by the computational complexity of E[MI2 ]:
Theorem 4. The computational complexity for E[MI2 ] is
O(max {rcN 3 , c2 N 3 }).
The proof is in the supplementary material. If the number
of columns c in the contingency table is greater than the
number of rows r, a longer computational time is incurred.
For example, if we fix the number of records N , the computation time for the variance of MI for a contingency table
with r = 6 rows and c = 2 columns is bounded above by
12N 3 . Yet, for the same but transposed table (with r = 2
rows and c = 6 columns), the time is bounded by 36N 3 .
Given that we can transpose a contingency table and obtain
identical variance results, we can always transpose to tables
where the number of rows r is higher than the number of
columns c, thus making the computation faster.

4. Experiments
In this Section, we carry out several experiments to demonstrate how SMI improves interpretability as well as helping

A

50
50

50
47
3

50
3
47

Table 3. Contingency table related to clusterings A and B that
show high agreement. Nonetheless, the AMI measure is just equal
to 0.67 that is apparently far from the maximum achievable 1.
SMI value is 64.22 which highlights that the clustering solution
B is significantly better than a random clustering solution.

ment between the two clusterings is very high, indeed cluster A1 in A shares 47 elements with cluster B1 in B, and
47 elements are also shared between cluster A2 in A and
B2 in B. Nonetheless, we obtain a modest score of 0.67
for AMI, that seems apparently far from 1, the maximum
achievable. It seems that there may be plenty of clusterings
B that could show more agreement with A given that 33%
of the total range of values to the maximum is still possibly
achievable. However, if we randomly assign records to the
clusters in B, fixing the size of each cluster to 50, we notice that it is very difficult to find a clustering whose AMI
with A is more than 0.67. In fact, 95% of such clusterings
have AMI less than 0.03. This characteristic is highlighted
by SMI, which has a value of 64.22. It means that the MI
between A and B is 64 standard deviations away from the
mean value under the hypothesis of random and independent clusterings and therefore highly significant.
In order to achieve even more interpretability, a p-value for
mutual information might be obtained by fitting a distribution parametrized on the mean and the standard deviation.
Good candidates might be the Gamma and the Normal distributions (Dobra & Gehrke, 2001; Vinh et al., 2009). However, there are no theoretical proofs about the quality of
these approximations available in literature. A conservative approach we can take is to use Cantelli’s inequality,
1
which holds for all distributions: P (SMI ≥ λ) ≤ 1+λ
2.
This inequality states that if SMI is greater than 4.36, then
the upper bound for the p-value under the hypergeometric
null hypothesis is 0.05. In the above example, we get a
p-value of ∼ 0.0002, which again is highly significant.

Standardized Mutual Information for Clustering Comparisons

Probability of selection

using the full distribution of MI.

SMI

0.2
0.1
0

0.2
0.1
0

2

6

10

2

6

10

2

6

10

1

AMI

MI

14

18

22

14

18

22

14

18

22

0.5
0

Number of clusters c in B

Figure 3. Estimated selection probability of a random clustering
solution B with c clusters when compared with a reference one
with 10 clusters. MI is strongly biased towards selecting clustering solutions with 22 clusters, and so is AMI, despite being baseline adjusted. SMI shows close to constant selection probability
across the different solutions.

4.2. Bias Towards Clusterings with More Clusters
Consider the scenario where the user has to compare some
clustering solutions to a reference one and select the one
that agrees with it the most using information theoretic
measures. Each solution might have been obtained using
different clustering algorithms, or setting different parameters for a single algorithm of choice, e.g. varying k in kMeans. Using MI and AMI, clustering solutions with more
clusters have more chances to be selected. We have observed that although the AMI has a constant baseline of 0,
its variance increases as the number of clusters increases,
thus creating a bias towards clustering with more clusters.
Let A be a reference clustering of N = 500 records with
10 equal size clusters. If we randomly generate clusterings
B with different number of clusters c independently from
the reference one, we do not expect any clustering solution to outperform the others in terms of agreement with
A. We carry out an experiment as follows: we generate a
pool of 6 random clusterings B with different numbers of
clusters c = 2, 6, 10, 14, 18, 22 and give a win to the solution that obtains the higher value for respectively SMI,
AMI, and MI against the reference clustering A. If a measure is unbiased, we expect that each clustering is selected
as often as the others, that is 16.7% of the time. Figure 3
shows the estimated ‘winning’ frequencies obtained from
5,000 trials. We can see that random clusterings B with 22
clusters are selected more than 90% of the time if we use
the MI. Even if we use the adjusted-for-chance AMI, such
clusterings are selected 24% of the time versus the 8% for
the random ones with 2 clusters. As observed, SMI helps
to decrease this bias significantly. SMI shows close to constant probability of selection across different solutions but
negligible differences might still exists because we are not

4.3. Bias Towards Clusterings Estimated on Fewer
Data Points
Clustering solutions might also be induced on different
numbers of data points. This is the application scenario
commonly encountered in modern data processing tasks,
such as streaming or distributed data. In streaming, the initial snapshots of the data often contain fewer data. Similarly, in distributed data processing, each node might have
limited access to a small part of the whole data set, due to
scale or privacy requirements. On the same data, one can
still encounter this situation, as in the following scenario:
recall that a discrete feature can be interpreted as a clustering, in which each cluster contains data points having the
same feature value. Suppose we have a number of features
(clusterings) and wish to compare the similarity of each
against a reference clustering (class label), then choosing
the feature with highest similarity to the class. If the features have missing values, then the respective clustering solutions will contain varying numbers of data points.
In these situations, there is selection bias if one uses MI
and AMI as the clustering comparison measure. To demonstrate this point, we generate a random reference clustering with 4 clusters and 100 data points and then generate 5 random clustering solutions with 4 clusters, each
induced using a different number of data points (20, 40,
60, 80 and 100). Each of the 5 clusterings is compared
against the reference clustering (discarding from the reference any points not present in the candidate clustering
solution). Even though each solution is random and independent from the reference clustering, MI and AMI select
the one with 20 records significantly more often than the
one with 100. Figure 4 shows the ‘winning’ probabilities
estimated from 10,000 trials. As observed, SMI helps to
decrease the bias significantly.
4.4. SMI Running Time
We compare the execution time of SMI implemented in
Matlab3 and the Fisher’s exact test available in R implemented as discussed in (Mehta & Patel, 1983). We make
this comparison, since Fisher’s test is a very popular, yet
expensive exact method and makes a good benchmark for
assessing the relative runtime performance of SMI given
that its computational effort is the same as for an exact
p-value for MI. On a quadcore Intel Core-i7 2.9GHz PC
with 16Gb of RAM, the average running time for 10 random clusterings is provided in Table 4. Each two compared clusterings were generated by assigning randomly
each record to one cluster with equal probability and independently from the others. Even with a carefully-tuned
3
The code is available at https://sites.google.
com/site/icml2014smi

Standardized Mutual Information for Clustering Comparisons

5. Discussion and Conclusion

Probability of selection

SMI
0.2
0.1
0

20

40

20

40

0.4

60

80

100

60

80

100

60

80

100

AMI

0.2
0
1

MI

0.5
0

20

40

Number of records N

Figure 4. Estimated selection probability of a random clustering
solution varying the number of points used to induce it. MI and
AMI show strong bias toward selection of solutions with fewer
data points. SMI shows close to constant selection probability
across the solutions.

implementation, Fisher’s exact test is extremely computationally expensive: it becomes intractable for a fairly small
number of records N and when either the number of rows
r or columns c increases. We note that computing the
Fisher’s exact test with the network algorithm implemented
in R also requires significant memory, i.e. ∼ 1Gb of RAM
for the data used herein.
It is worth noting that computing the SMI is highly
amenable to parallelization and it is easy to implement a
parallel version using modern programming languages. For
example, Matlab provides the parfor construct that splits
the load of a for loop on different CPUs. We can choose
to parallelize the outer loops in i and j to exploits better
parallelism even on 2 × 2 tables. It is achievable by iterating on another variable u from 1 to rc and using i and j as
follows: i ← du/ce, j ← (u − 1) mod c + 1. We indeed
obtain almost linear speedup when r > 2 or c > 2. For
example, for r = 5 and c = 5, the speedups for two and
four CPU cores are 1.96 and 3.64 folds on average.

SMI
SMI parallel
Fisher’s

3×3
0.30
0.15
0.01

N = 100 records in r × c tables
4×4
5×5
6×6
7×7
0.64
1.12
1.72
2.47
0.27
0.40
0.55
0.80
0.61
67.06
857.11
N/A

8×8
3.30
1.01
N/A

SMI
SMI parallel
Fisher’s

100
0.65
0.30
0.65

4 × 4 tables with N records
150
200
250
300
1.53
2.94
5.00
7.59
0.51
0.97
1.52
2.33
11.32 242.67 844.62
N/A

350
11.00
3.35
N/A

In this paper, we have introduced a further degree of adjustment for chance for information theoretic measures: the
standardization of mutual information. We showed that
standardization unifies several well known measures, including the mutual information, the variation of information, and the G-statistic. We have provided an analytical
formula to compute the variance of MI under the hypothesis of random and independent clusterings. We also analysed its computational complexity and provided running
time comparisons against the Fisher’s exact test as a benchmark. We experimentally demonstrated that Standardized
Mutual Information (SMI) reduces the bias towards selecting clusterings with more clusters and clusterings induced
on fewer data points, and arguably provides better interpretability and comparability compared to using the AMI
for clustering comparison.
To conclude, we provide a radar chart to highlight the relative utility information theoretic measures in Figure 5.
Each axis assesses the capability with respect to a particular clustering comparison scenario. In some situations,
the user might be interested to know how far a solution is
from the maximum agreement achievable with the reference clustering. In this case, VI, NMI and AMI are good
choices. On the other hand, SMI is particularly useful
when the task is selection of a clustering based on multiple clustering comparisons against a reference and when
there are clusterings induced on data sets where the number of records is small relative to the number of clusters.
As a rule of thumb, SMI should definitely be employed if
N
r·c < 5 and more than three clustering solutions have to
be compared. Lastly, we remark this paper is focused on
MI for clustering comparisons but we know that all results
are applicable when using MI in arbitrary size contingency
tables.

Maximum agreement achievable

VI
NMI

AMI
Multiple clustering comparisons

MI
SMI

Table 4. Running times in seconds for SMI and Fisher’s exact
test. Fisher’s exact test becomes intractable when the number of
records N is large or the number of rows r or columns c is large.

Few data points

Figure 5. Relative utility of SMI, AMI, VI, NMI, MI on different
clustering comparison scenario (best viewed in colors).

Standardized Mutual Information for Clustering Comparisons

References
Aggarwal, C. C. and Reddy, C. K. Data Clustering: Algorithms and Applications. CRC Press, 2013.
Agresti, A. Categorical data analysis, volume 359. John
Wiley & Sons, 2002.
Ben-Hur, A., Elisseeff, A., and Guyon, I. A stability based
method for discovering structure in clustered data. In
Pacific symposium on biocomputing, volume 7, pp. 6–
17, 2001.
Caruana, R., Elhaway, M., Nguyen, N., and Smith, C. Meta
clustering. In Data Mining, 2006. ICDM’06. Sixth International Conference on, pp. 107–118. IEEE, 2006.
Cover, T. M. and Thomas, J. A. Elements of information
theory. John Wiley & Sons, 2012.
Dobra, A. and Gehrke, J. Bias correction in classification
tree construction. In ICML, pp. 90–97, 2001.
Dyer, M., Kannan, R., and Mount, J. Sampling contingency
tables. Random Structures and Algorithms, 10(4):487–
506, 1997.
Frank, E. and Witten, I. H. Using a permutation test for
attribute selection in decision trees. In ICML, pp. 152–
160, 1998.
Hubert, L. and Arabie, P. Comparing partitions. Journal of
Classification, 2:193–218, 1985.
Kononenko, I. On biases in estimating multi-valued attributes. In International Joint Conferences on Artificial
Intelligence, pp. 1034–1040, 1995.
Mehta, C. R. and Patel, N. R. A network algorithm for
performing fisher’s exact test in r × c contingency tables. Journal of the American Statistical Association, 78
(382):427–434, 1983.
Meilă, M. Comparing clusteringsan information based distance. Journal of Multivariate Analysis, 98(5):873–895,
2007.
Meilă, M. Local equivalences of distances between clusteringsa geometric perspective. Machine learning, 86
(3):369–389, 2012.
Monti, S., Tamayo, P., Mesirov, J., and Golub, T. Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray
data. Machine Learning, 52(1-2):91–118, 2003. ISSN
0885-6125.
Müller, E., Günnemann, S., Färber, I., and Seidl, T. Discovering multiple clustering solutions: Grouping objects
in different views of the data. Tutorial at ICML, 2013.
URL http://dme.rwth-aachen.de/en/DMCS.

Quinlan, J. R. C4.5: Programs for Machine Learning.
Morgan Kaufmann, 1993. ISBN 1-55860-238-0.
Rand, W. M. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association, 66(336):846–850, 1971.
Vinh, N. X., Epps, J., and Bailey, J. Information theoretic measures for clusterings comparison: is a correction for chance necessary? In ICML, pp. 1073–1080.
ACM, 2009.
Vinh, N. X., Epps, J., and Bailey, J. Information theoretic
measures for clusterings comparison: Variants, properties, normalization and correction for chance. Journal of
Machine Learning Research, 11:2837–2854, 2010.
White, A. P. and Liu, W. Z. Bias in information-based measures in decision tree induction. Machine Learning, pp.
321–329, 1994.
Wu, J., Xiong, H., and Chen, J. Adapting the right measures for k-means clustering. In Knowledge Discovery
and Data Mining, pp. 877–886, 2009.

