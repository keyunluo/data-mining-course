A Probabilistic Model for Dirty Multi-task Feature Selection

Daniel HernaÃÅndez-Lobato
DANIEL . HERNANDEZ @ UAM . ES
Universidad AutoÃÅnoma de Madrid, Computer Science Department, Madrid, 28049, Spain
JoseÃÅ Miguel HernaÃÅndez-Lobato
JMH @ SEAS . HARVARD . EDU
Harvard University, School of Engineering and Applied Sciences, Cambridge, MA 02138, USA
Zoubin Ghahramani
University of Cambridge, Department of Engineering, Cambridge CB2 1PZ, UK

Abstract
Multi-task feature selection methods often make
the hypothesis that learning tasks share relevant
and irrelevant features. However, this hypothesis may be too restrictive in practice. For example, there may be a few tasks with specific relevant and irrelevant features (outlier tasks). Similarly, a few of the features may be relevant for
only some of the tasks (outlier features). To account for this, we propose a model for multi-task
feature selection based on a robust prior distribution that introduces a set of binary latent variables to identify outlier tasks and outlier features.
Expectation propagation can be used for efficient
approximate inference under the proposed prior.
Several experiments show that a model based on
the new robust prior provides better predictive
performance than other benchmark methods.

1. Introduction
When the number of samples is smaller or equal to the
number of attributes or features, regression problems are
under-determined. In this case, a linear model is too complex to explain the observed data since an infinite number
of model coefficients perfectly fit the data. In this context,
sparsity, i.e., the assumption of zeros in the model coefficients, plays a strong regularization role that can be useful to obtain estimates with good generalization properties.
Sparsity can be favored by using sparsity enforcing priors
in probabilistic models or by optimizing a loss function penalized by a sparsity inducing norm (Carvalho et al., 2009;
Jalali et al., 2010; Vogt & Roth, 2010). The assumption of
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

ZOUBIN @ ENG . CAM . AC . UK

zeros in the model coefficients is equivalent to the assumption of only a few of relevant features for prediction.
Multi-task feature selection methods are used to improve
the process of inferring the model coefficients from the observed data under the sparsity assumption (Vogt & Roth,
2010; HernaÃÅndez-Lobato et al., 2010; Obozinski et al.,
2009; Xiong et al., 2007; Zhang et al., 2008). In these
methods several tasks that have a common feature space are
solved simultaneously, often under the assumption that the
tasks share relevant and irrelevant features, as illustrated by
Figure 2 (top). However, in some situations this hypothesis
may be too restrictive (Jalali et al., 2010). As illustrated by
Figure 2 (bottom), a few of the tasks may have specific relevant / irrelevant features (outlier tasks), and a few of the
features may be arbitrarily relevant / irrelevant across tasks
(outlier features). In this situation, traditional multi-task
feature selection methods are expected to perform poorly.
In this paper we propose a multi-task feature selection
model, based on a robust prior distribution, that is expected
to have better properties in the presence of diverse tasks,
i.e., data with the properties described above. Exact inference is intractable in such a model. However, expectation
propagation can be used for efficient approximate inference
(Minka, 2001). Several experiments involving the reconstruction of gene regulatory networks, the denoising of natural images and the prediction of drug sensitivity from microarray data illustrate the benefits of the model proposed.
Specifically, it has better prediction properties than other
methods from the literature and it can be used to successfully identify relevant attributes for prediction, alongside
with outlier tasks and features, which may be useful to better understand the characteristics of the observed data.

2. Dirty Multi-task Feature Selection
Assume K regression tasks with data {X(k) , y(k) }K
k=1 ,
where X(k) and y(k) are the design matrix and the vector

A Probabilistic Model for Dirty Multi-task Feature Selection

of targets for task k, respectively. All tasks share the same
d attributes or features, but feature values can be different
across tasks. A linear model is considered for each task,
i.e., y(k) = X(k) w(k) + (k) , where w(k) ‚àà Rd is the vec2
tor of model coefficients for task k and (k) ‚àº N (0, IœÉ(k)
)
2
is Gaussian noise with variance œÉ(k) . Let W be a K √ó d
matrix whose k-th row is w(k) and Y a matrix whose k-th
2
2
K
row is y(k) . Define X = {X(k) }K
k=1 and œÉ = {œÉ(k) }k=1 .
The likelihood for W and œÉ 2 is:
p(Y|X , W, œÉ 2 ) =

K
Y

2
N (y(k) |X(k) w(k) , IœÉ(k)
) . (1)

k=1

Moreover, feature selection for each task, or equivalently,
sparsity in each w(k) is expected to be beneficial. We also
assume that the K tasks share relevant and irrelevant features, but we allow for small deviations from this hypothesis. All this prior knowledge is introduced in the model by
a robust prior for W described in the next section.
2.1. Robust prior distribution
The prior considered is based on the discrete mixture prior
introduced in (Carvalho et al., 2009). Thus, we first describe and motivate the use of that prior to favor sparse solutions. Then, we show how it can be extended to perform
feature selection across several tasks in a robust way.
2.1.1. D ISCRETE MIXTURE PRIOR
This is a spike and slab prior in which the i-th coefficient
(k)
(k)
of task k satisfies wi ‚àº (1 ‚àí œÅ)Œ¥0 + œÅœÄ(wi ), where œÅ is
the prior inclusion probability, Œ¥0 is a point of probability
mass at zero, and œÄ(¬∑) is a density that specifies the distri(k)
bution of the coefficients that are not zero. Each wi is a
priori zero with probability (1 ‚àí œÅ). In (Carvalho et al.,
2009) it is suggested for œÄ(¬∑) the Strawderman-Berger distribution (Strawderman, 1971; Berger, 1980), which has
Cauchy-like tails and yet allows for a closed form convolution with a Gaussian likelihood. This distribution is a scale
mixture of Gaussians (Armagan et al., 2011) defined as:

 Z
Œªi
(k)
(k)
œÄ wi
= N (wi |0, Œª2i ) 2
3 dŒªi
(Œªi + 1) 2
!
(k)
1
(k) Œ¶(‚àí|wi |)
=‚àö
1 ‚àí |wi |
, (2)
(k)
2œÄ
N (wi |0, 1)
where | ¬∑ | denotes absolute value, Œ¶(¬∑) is the cdf of a standard Gaussian distribution, N (¬∑|0, 1) is the standard Gaus3
sian density, and Œªi /(Œª2i + 1) 2 is the density assumed for
Œªi . Figure 1 (left and middle) compares the discrete mixture prior with other priors from the literature (an arrow
means a point of probability mass). We observe that the
discrete mixture has heavy tails to explain coefficients that

significantly differ from zero. It also has a point mass at
zero that allows for exact zeros in the coefficients.
2
Let œÉ(k)
= 1 and X(k) = I, and define Œ∫i = 1/(1 + Œª2i ).
Carvalho et al. (2009) shows that the posterior mean for
(k)
(k)
wi is in this case (1 ‚àí Œ∫i )yi , where Œ∫i is a random
shrinkage coefficient. Figure 1 (right) displays the prior
(k)
density for Œ∫i that results from each prior for wi . The
prior for Œ∫i is obtained by applying the change of variables
Œ∫i = 1/(1 + Œª2i ) to the prior for Œªi , which in the case of the
discrete-mixture prior is a mixture between the distribution
for Œªi assumed in (2) and a point mass at zero. Figure 1
(right) shows that under the discrete-mixture prior a priori
we expect to observe Œ∫i = 1 as a consequence of the point
mass at one in the prior for Œ∫i . Furthermore, we also expect
to observe Œ∫i ‚âà 0 as a consequence of the density tending
to infinity at zero. These two values for Œ∫i correspond respectively to total shrinkage (zero values) and no shrinkage
(k)
at all (non-zero values) for wi . By contrast, under the
(k)
other priors for wi the density for Œ∫i tends to zero at zero
(Laplace) or tends to zero at one (Student‚Äôs T). This means
that these priors will shrink relevant coefficients and will
not shrink irrelevant coefficients, respectively. Thus, the
discrete mixture prior can be considered as a golden standard for learning under sparsity (Carvalho et al., 2009).

2.1.2. E XTENSION OF THE DISCRETE - MIXTURE PRIOR
The previous prior is extended to perform feature selection
across several tasks. We assume that the tasks share in general relevant and irrelevant features, but we consider a few
outlier tasks with specific relevant / irrelevant features and
a few outlier features that may be arbitrarily relevant / irrelevant for each task. This is illustrated in Figure 2 (bottom).
Tasks 4 and 8 are outlier tasks and features 19 and 21 are
outlier features. All other tasks and features share the hypothesis of jointly relevant / irrelevant features across tasks.
To model this type of prior knowledge we introduce the
following binary latent variables:
zi Indicates whether feature i is an outlier (zi = 1) or not
(zi = 0). If it is an outlier it can be independently relevant
or irrelevant for each task.
œâk Indicates whether task k is an outlier (œâk = 1) or not
(œâk = 0). If it is an outlier it can have specific relevant
and irrelevant features for prediction.
Œ≥i Indicates whether the non-outlier feature i is relevant (Œ≥i =
1) for prediction or not (Œ≥i = 0) in all tasks that are not
outliers, i.e., those tasks for which œâk = 0.
(k)

Indicates whether, given that task k is an outlier task, i.e.,
(k)
œâk = 1, feature i for that task is relevant (œÑi = 1) or
(k)
irrelevant (œÑi = 0) for prediction.

(k)

Indicates whether, given that feature i is an outlier feature,

œÑi

Œ∑i

‚àí2

‚àí1

0

1

2

4

3

5
4
3

Probability Density

1
0

0.000

0.1
0.0

‚àí3

Discrete Mixture
Student‚àít(df=1)
Laplace

2

0.025
0.015

Probability Density

0.020

Discrete Mixture
Gaussian
Student‚àít(df=1)
Laplace

0.005

0.2

0.3

0.4

Probability Density

0.5

0.6

Discrete Mixture
Gaussian
Student‚àít(df=1)
Laplace

0.010

0.7

A Probabilistic Model for Dirty Multi-task Feature Selection

5

6

7

0.0

0.2

0.4

0.6

0.8

1.0

Figure 1. (left) Density of different priors. Note the spike of the discrete mixture at the origin. (middle) Tails of the different priors.
(right) Prior density of the shrinkage parameter Œ∫i for the discrete mixture prior and for other priors from the literature.

1

2

3

4

5

6

7

Task k

8

9 10 11 12

(k)

to an outlier feature (zi = 1) relevant for task k (Œ∑i = 1);
or (ii) it does not correspond to an outlier feature (zi =
0), but it corresponds to an outlier task (œâk = 1) and the
(k)
feature is relevant for that task (œÑi = 1); or (iii) it does
not correspond to an outlier feature (zi = 0), nor an outlier
task (œâk = 0), but the feature is relevant for prediction
across tasks (Œ≥i = 1). Otherwise, the coefficient is zero.

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26

1

2

3

4

5

6

7

Task k

8

9 10 11 12

Dimension i

Dimension i

Figure 2. (top) Traditional multi-task feature selection: All tasks
share relevant and irrelevant features (model coefficients). (bottom) Dirty multi-task feature selection: Most tasks share relevant
and irrelevant features, but we allow for outlier tasks (tasks 4 and
8) and for outlier features (dimensions 19 and 21). White squares
represent irrelevant coefficients that are equal to zero. Colored
squares represent relevant coefficients with non-zero values.
that particular feature is relevant for prediction in task k
(k)
(k)
(Œ∑i = 1) or not (Œ∑i = 0).

Let ‚Ñ¶ be the collection of all these latent variables, i.e.
(k) K
‚Ñ¶ = {z, œâ, Œ≥, {œÑ (k) }K
}k=1 }. Given the latent
k=1 , {Œ∑
variables we can specify the prior distribution for W:
p(W|‚Ñ¶) =

d Y
K
Y

(k)

(k)

(k)

(k)

1‚àíŒ∑i

where p(wi |‚Ñ¶) = {œÄ(wi )Œ∑i Œ¥0
(k)
1‚àíœÑi

Last, we set the hyper-prior for the noise of each task
to be an inverse gamma distribution, i.e., p(œÉ 2 ) =
QK
2
k=1 InvGam(œÉ(k) |Œ±0 , Œ≤0 ), where we specify Œ±0 = 5 and
Œ≤0 = 5. These parameter values are equivalent to the prior
observation in each task of 10 data instances with noise
variance equal to 1. Furthermore, they also produce high
variance in the prior distribution which allows for the identification of the correct level of noise of each task using the
training data only. An alternative formulation of the prior
that assumes the same level of noise for each task is also
considered. Namely, we set p(œÉ 2 ) = InvGam(œÉ 2 |Œ±0 , Œ≤0 ),
where each entry in œÉ 2 is constrained to be equal to œÉ 2 .
2.2. Prediction and identification of relevant features

(k)
p(wi |‚Ñ¶) ,

(3)

i=1 k=1
(k)

The hyper-priors for the latent variables are Bernoullis with
parameters œÅz , œÅœâ , œÅŒ≥ , œÅœÑ and œÅŒ∑ , i.e., we set p(z|œÅz ) =
Qd
QK
=
z ), p(œâ|œÅœâ )
i=1 Bern(zi |œÅQ
k=1 Bern(œâk |œÅœâ ),
d
(k) K
p(Œ≥|œÅŒ≥ ) =
}k=1 |œÅœÑ ) =
i=1 Bern(Œ≥k |œÅŒ≥ ), p({œÑ
QK Qd
(k)
(k) K
Bern(œÑ
|œÅ
)
and
finally
p({Œ∑
}k=1 |œÅŒ∑ ) =
œÑ
i
i=1
Qk=1
(k)
K Qd
k=1
i=1 Bern(Œ∑i |œÅŒ∑ ). The hyper-prior for each œÅz ,
œÅœâ , œÅŒ≥ , œÅœÑ and œÅŒ∑ is a beta distribution with parameters a0
and b0 , e.g., p(œÅz ) = Beta(œÅz |a0 , b0 ) for the case of œÅz .
Furthermore, we set a0 = 1 and b0 = 1 which leads to a
uniform distribution so that no particular hyper-parameter
value is favored a priori. These uniform priors allow to
identify each hyper-parameter value from the training data.

(k)

Define œÅ = {œÅz , œÅœâ , œÅŒ≥ , œÅœÑ , œÅŒ∑ }. The joint probability of
the targets Y and the latent variables W, ‚Ñ¶, œÅ and œÉ 2 is:

(k)

}zi {[œÄ(wi )œÑi

Œ¥0
]œâk [œÄ(wi )Œ≥i Œ¥01‚àíŒ≥i ]1‚àíœâk }1‚àízi . Under this prior a
(k)
coefficient wi is different from zero if (i) it corresponds

p(Y, W, ‚Ñ¶, œÅ, œÉ 2 |X ) = p(Y|X , W, œÉ 2 )p(W|‚Ñ¶)√ó
√ó p(‚Ñ¶|œÅ)p(œÅ)p(œÉ 2 ) ,

(4)

A Probabilistic Model for Dirty Multi-task Feature Selection

where p(Y|X , W, œÉ 2 ) is given by (1), p(W|‚Ñ¶) is
given by (3), p(‚Ñ¶|œÅ) = p(z|œÅz )p(œâ|œÅœâ )p(Œ≥|œÅŒ≥ )
(k) K
p({œÑ (k) }K
}k=1 |œÅŒ∑ ) and p(œÅ) = p(œÅz )
k=1 |œÅœÑ )p({Œ∑
p(œÅœâ )p(œÅŒ≥ )p(œÅœÑ )p(œÅŒ∑ ). This joint distribution is normalized with respect to W, ‚Ñ¶, œÅ and œÉ 2 to get a posterior:
p(Y, W, ‚Ñ¶, œÅ, œÉ 2 |X )
.
p(W, ‚Ñ¶, œÅ, œÉ |Y, X ) =
p(Y|X )
2

(5)

The posterior is used to compute predictions for
the target value ynew of a new un-observed instance
|xnew ) =
P R xnew of T task(k)k. 2 Namely, p(ynew
, œÉ(k) )p(W, ‚Ñ¶, œÅ, œÉ 2 |Y, X )dW
‚Ñ¶ N (ynew |xnew w
(k)
wi

dœÅdœÉ 2 . The probability that a particular
is different from zero can be computed similarly. For this, we
eliminate variables in (5) and sum the posterior probabilities of the three events described in Section 2.1.2, i.e.,
(k)
(k)
p(wi 6= 0|Y, X ) = p({zi = 1 ‚à© Œ∑i = 1} ‚à™ {zi =
(k)
0 ‚à© œâk = 1 ‚à© œÑi = 1} ‚à™ {zi = 0 ‚à© œâk = 0 ‚à© Œ≥i =
1}|Y, X ). Finally, the probability that task k is an outlier,
p(œâk = 1|Y, X ), or the probability that feature i is an
outlier, p(zi = 1|Y, X ), are computed in a similar way.
The computation of all the expressions described in this
section, except (4), is intractable for typical problems.
Thus, we have to resort to approximate inference methods.

3. Expectation propagation (EP)
EP is an efficient mechanism for approximate inference
(Minka, 2001). EP approximates each factor in (4) that
is not inside a particular exponential family F of distributions with an un-normalized factor that is inside F. We
set F to be the product of Gaussian distributions on W,
Bernoulli distributions on ‚Ñ¶, beta distributions on œÅ and inverse gamma distributions on œÉ 2 . F is closed under product and division operations. The only factors in (4) that
are not in F are those of the likelihood (1), p(W|‚Ñ¶) and
p(‚Ñ¶|œÅ). The hyper-prior for œÅ is beta, and the hyper-prior
for œÉ 2 is inverse gamma so they need not be approximated.
In EP each likelihood factor corresponding to the
n-th instance of the k-th task (xn(k) , yn(k) ) is ap(k)
(k)
(k)
2
proximated as p(yn |w(k) , xn , œÉ(k)
) = N (yn |
(k)
(k)
(k)
(xn )T w(k) , œÉ 2 )
‚âà
fÀún (w(k) , œÉ 2 )
=
cÃÉn
(k)

(k)

(k)

(k)

(k)
(k)

(k)

2
N ((xn )T w(k) |mÃÉn , vÃÉn )InvGam(œÉ(k)
|aÃÉn , bÃÉn ).
(k)

The approximation of each factor p(wi |‚Ñ¶)
(k)
that appears in p(W|‚Ñ¶) in (3) is p(wi |‚Ñ¶) ‚âà
(k)
(k) (k)
(k)
(k)
2
gÃÉi (wi , zi , œâk , Œ≥i , œÑi , Œ∑i ) = sÃÉi N (wi |mÃÉi , œÉÃÉ(i,k)
)
(i,k)

(i,k)

(i,k)

(k)

Bern(zi |pÃÉz )Bern(œâk |pÃÉœâ )Bern(Œ≥i |pÃÉŒ≥ )Bern(œÑi
(i,k)
(k) (i,k)
|pÃÉœÑ )Bern(Œ∑i |pÃÉŒ∑ ).
Finally, each factor in
p(‚Ñ¶|œÅ) is approximated following a similar principle. For example, for p(zi |œÅz ) the approximation is

(i)

(i)

(i)

Bern(zi |œÅz ) ‚âà hÃÉz (zi , œÅz ) = Œ∫ÃÉz Bern(zi |pÃÉz )Beta(œÅz |
(i) (i)
aÃÉz , bÃÉz ). The approximation of the other factors in
p(‚Ñ¶|œÅ) is equivalent to this one. All the parameters with
the superscriptÀúare adjusted by EP, as described below.
The EP approximation of the joint distribution (4) replaces
each exact factor by the corresponding approximate one.
Denote by qÃÉ this approximation. After normalization, the
joint distribution (4) becomes the exact posterior (5). Similarly, after normalization qÃÉ becomes the EP posterior approximation q:
q(W, ‚Ñ¶, œÅ, œÉ 2 ) =

qÃÉ(W, ‚Ñ¶, œÅ, œÉ 2 )
,
Zq

(6)

which is inside of F because F is closed under the product
operation. The parameters of q are obtained from the product of all the factors in qÃÉ and Zq can be readily computed
because qÃÉ is an un-normalized parametric distribution inside of F. Given q, all the expressions in Section 2.2 can
be approximated by replacing the exact posterior with q.
EP refines until convergence each approximate factor fÀú.
For this, q old ‚àù q/fÀú is computed. q old has the same form as
q because q, fÀú ‚àà F. Then, an updated posterior approximation q new is obtained by minimizing the Kullback-Leibler
divergence between f q old and q new , KL(f q old ||q new ), where
f denotes the exact factor associated to fÀú. The updated
approximate factor is fÀú = Zf q new /q old , where Zf is the
normalization constant of f q old . This guarantees that fÀú
is similar to the exact factor in regions of high posterior
probability, as estimated by q old . The minimization of
KL(f q old ||q new ) with respect to q new has a global optimum
found by matching expected sufficient statistics between
f q old and q new (Bishop, 2006). These expectations can be
obtained from the derivatives of log Zf with respect to the
(natural) parameters of q old , as indicated by Seeger (2006).
A contribution of this paper is the computation of Zf for
the factors in p(W|‚Ñ¶). In that case, Zf is a probabilistic mixture of the convolution of the Strawderman-Berger
prior œÄ(¬∑) with a Gaussian distribution, i.e., the posterior
(k)
distribution for wi under q old , and the convolution of
the point probability mass at the origin, Œ¥0 , with the same
Gaussian. Fortunately, the Strawderman-Berger prior has
a closed form convolution with the Gaussian distribution.
Johnstone & Silverman (2005) provide the analytic solution when the variance of the Gaussian is one. We provide
the solution when this is not the case. The complete details
about EP are found in the supplementary material, alongside with an R implementation of the proposed method.
When d > Nk , where Nk is the number of instances of task
k and d is the number of features, the covariance matrix of
the likelihood of each task in (1) is low rank. P
EP is able to
K
exploit this and it has a cost that scales like O( k=1 Nk2 d).

A Probabilistic Model for Dirty Multi-task Feature Selection

4. Related work
There are several works in the literature focusing on feature
selection within a multi-task learning setting. In this section they are described. For example, HernaÃÅndez-Lobato
et al. (2010) propose a model based on the spike-and-slab
prior (Mitchell & Beauchamp, 1988) to determine whether
a feature is either relevant or irrelevant across all tasks. The
spike-and-slab prior is also used in (Jebara, 2004), where
a multi-task feature selection method is derived using the
maximum entropy discrimination formalism. In (Obozinski et al., 2009; Vogt & Roth, 2010) the group LASSO is
considered as an efficient estimator that selects common
features across tasks by penalizing a mixed norm of the
model coefficients. The work presented by Argyriou et al.
(2007) is based on a similar approach. Finally, in (Xiong
et al., 2007) a set of common relevant features across tasks
is found using the automatic relevance determination principle. In summary, all these works assume jointly relevant
and irrelevant features across tasks, as e.g. in Figure 2 (top),
and are hence expected to perform poorly when this hypothesis is not fully satisfied, as e.g. in Figure 2 (bottom).
There are other methods that have been proposed to relax the hypothesis of jointly relevant and irrelevant features
across all tasks. In (Jalali et al., 2010) a dirty model considers a mixed norm to penalize the model coefficients of several tasks. Specifically, W = P + Q where P is penalized
with the `1 norm and Q with the the `1,‚àû norm. A similar model can be derived using the `1,2 norm instead (Vogt
& Roth, 2010). The estimator employed selects a common
subset of relevant features for all the tasks, but it allows for
tasks with additional specific relevant features. The result
is a generalization of the group LASSO (Obozinski et al.,
2009; Vogt & Roth, 2010), which is expected to be more
robust to outlier tasks in the learning process, but not to
be as flexible as the model proposed in this paper. Another
method introduced for this purpose is found in (Gong et al.,
2012). This robust multi-task feature learning model also
defines W = P + Q and estimates the model coefficients
W by penalizing both P and QT with the `1,2 norm. The
intuition behind this idea is that if the k-th row of Q is not
zero after the estimation, task k is an outlier task with all
features relevant for prediction. However, again this a less
flexible assumption than the one we make. In particular,
the two works just described assume that all tasks share a
few relevant features, although they allow for some arbitrary tasks to have extra relevant features. This means that
they cannot model outlier tasks, (e.g., tasks 4 and 8 in Figure 2 (bottom)), unlike the approach proposed in this work.
HernaÃÅndez-Lobato & HernaÃÅndez-Lobato (2013) consider
that tasks do not share common relevant and irrelevant features for prediction, but common dependencies in the feature selection process. These dependencies are induced

from the data using a generalization of the horseshoe prior
for feature selection (Carvalho et al., 2009). The principle they follow is hence more flexible than the assumption
made by the models described in the first paragraph of this
section. However, such an approach is expected to be suboptimal when most tasks actually share relevant and irrelevant features for prediction, which is the hypothesis we
assume and the one that is displayed in Figure 2 (bottom).
Finally, some works in the literature also consider modeling
outlier tasks in multi-task learning, e.g., (Xue et al., 2007;
Passos et al., 2012). However, they do not consider sparsity
in the model coefficients and are hence expected to perform
poorly when this hypothesis is actually satisfied in practice.

5. Experiments
We compare the proposed model for dirty multi-task feature selection (DMFS) with single task learning (STL) and
with a model for multi-task feature selection (MFS) that
assumes relevant and irrelevant features shared across all
tasks. STL and MFS are particular cases of DMFS with all
tasks being outliers (STL) and with no outlier tasks nor outlier features (MFS). We also compare results with the methods described in Section 4. That is, the dirty model (DM)
of Jalali et al. (2010), the robust multi-task feature learning method (RMFL) of Gong et al. (2012) and the model
for learning feature selection dependencies (MFSDep ) of
HernaÃÅndez-Lobato & HernaÃÅndez-Lobato (2013). In DM
and RMFL we choose hyper-parameters using a grid search
guided by an inner cross-validation method. In MFSDep we
use type-II maximum likelihood for this (Bishop, 2006).
DMFS, STL and MFS need not fix any hyper-parameters
since they infer them from the data using hyper-priors. Unless stated differently, in all probabilistic models we assume different levels of noise for each task when training.
All methods described are implemented in the R language.
5.1. Experiments with synthetic data
We generate 12 tasks where the model coefficients are sampled from a Student‚Äôs distribution with 5 degrees of freedom. Each task k has d = 2000 attributes and Nk = 150
samples. The sparsity pattern employed for the model coefficients across tasks is displayed in Figure 2 (bottom). All
model coefficients above dimension 26 are equal to zero.
The targets are added Gaussian noise with variance 1/2 and
each entry of the design matrix X(k) of task k is standard
Gaussian. We use 90% of the instances for training and
10% for testing. The reported estimates are averaged over
100 repetitions. We report the test root mean squared error
(RMSE) and the average
PKreconstruction error of the model
coefficients, i.e., 1/K k=1 ||w(k) ‚àí wÃÇ(k) ||2 , where wÃÇ(k)
is either the posterior mean (only in the probabilistic models), or a point estimate of w(k) (only in DM and RMFL).

A Probabilistic Model for Dirty Multi-task Feature Selection

1
0.5
1

2

3

4

5

6

7

8

9

10

11

9 10 11 12
8

9 10 11 12

Task k

8

5

6

6

7

7

Task k

4

5

3

4

2

3

1

2
1
4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26

Dimension i

8
6

7

Task k

5
4
3
2
9 10 11 12

Dimension i

Task k

8

8

9 10 11 12

Dimension i

1

1

2

3

4

5

6

7

Task k

8

9 10 11 12

Dimension i

9 10 11 12

3

7
6

7

5

6

Dimension i

4
3
2
1

Dimension i

12

0.5

1

5.2. Reconstruction of gene regulatory networks

0

Probability

2

Figure 4. Average probability for each method across the 100 repetitions that each of the 26 first model coefficients of each task is
different from zero in a gray scale (0 = white and 1 = black).

0

Probability

The better results obtained by DMFS are also explained
by Figure 3, which shows the average posterior probability
that each task and each feature is an outlier, as estimated
by DMFS. DMFS successfully identifies tasks 4 and 8 as
outlier tasks and features 19 and 21 as outlier features. We
note that features 1, 3 and 24 have also a small probability
of being outliers. This makes sense because according to
Figure 2 (bottom) they are relevant only for a few tasks.

1

Task k

The results obtained are displayed in Table 1. The best
method in terms of the reconstruction error and the RMSE
is DMFS, followed by MFSDep and STL. MFS performs
worse than STL. DM and RMFL perform poorly. The
differences of DMFS with respect to the other methods
are statistically significant (p-value < 5% using a paired
Student‚Äôs T test). In terms of training time, the fastest
method is STL closely followed by MFS and DMFS. DM
and RMFL take longer training times due to the expensive
grid search procedure that is used to fix their two hyperparameters. This process demands re-training each method
many times. If the optimal hyper-parameters were given,
they would be the fastest methods. The training time of
MFSDep is very high for the same reason. By contrast, unlike these methods, DMFS uses Bayesian inference to infer
hyper-parameters and does not require any re-training.

5

Training Time
6.41¬±1.57
21.87¬±0.18
150.35¬±10.0
2 ¬∑ 103 ¬±4 ¬∑ 102
95.42¬±5.0
5.14¬±0.39

4

Rec. Error
0.37¬±0.04
0.22¬±0.02
0.50¬±0.03
0.32¬±0.04
0.56¬±0.03
0.33¬±0.06

3

Test RMSE
0.81¬±0.06
0.73 ¬±0.04
0.86¬±0.05
0.77¬±0.06
0.90¬±0.05
0.78¬±0.08

2

Method
MFS
DMFS
DM
MFS D EP
RMFL
STL

terns that agree the most with those of Figure 2 (bottom).
However, STL does not exploit the multi-task setting and
is less confident about the non-zeroness of coefficients 6 to
16 for non-outlier tasks. DMFS is also more confident than
MFSDep about irrelevant coefficients. On the other hand,
MFS, RMFL and DM give high probability of being different from zero to coefficients 6 to 16 for tasks 4 and 8. The
reason is that they assume a few relevant features shared
across all tasks and cannot model outlier tasks. Furthermore, they also find to be non-zero across all tasks some
coefficients corresponding to features that are in fact only
relevant for a few tasks (e.g., the coefficients corresponding
to features 1, 3, 19, 21 and 24). DM and RMFL can in principle model some of these coefficients (note that they give
higher probabilities of being not zero to some of them), but
to avoid their joint selection, these methods would have to
shrink non-zero coefficients shared by all non-outlier tasks,
producing worse results. In particular, the norms that they
use cannot provide very sparse solutions and not shrink relevant coefficients (HernaÃÅndez-Lobato et al., 2013).

1

Table 1. Avg. test RMSE, reconstruction error and running time
in minutes of each method on the synthetic experiments.

1

8

16

24

32

Figure 3. Avg. posterior probability for œâk = 1 (task k is an outlier) and zi = 1 (feature i is an outlier) in DMFS, in the synthetic
data. The last prob. is only displayed for the first 32 features.

Figure 4 also sheds light on the better performance of
DMFS. It shows in a gray scale the average probability
that each of the 26 first model coefficients of each task is
zero, as estimated by each method. These probabilities are
obtained from the approximate posterior in the probabilistic models. In DM and RMFL we report the fraction of
times that a coefficient is different from zero across experiments. We note that DMFS, MFSDep and STL find pat-

Assume X is a N √ód matrix with columns denoting d genes
and rows containing N measurements of log mRNA concentration obtained under different steady state conditions.
Consider that X is contaminated with additive Gaussian
noise. Then, X ‚âà XWT + œÉ 2 E, where the entries in E
are standard Gaussian, œÉ 2 is the variance of the noise and
W is a sparse d √ó d regression matrix with zero diagonal
entries that links the expression level of each gene with that
of its transcriptional regulators (HernaÃÅndez-Lobato et al.,
2015). When an entry of W is non-zero there is a regulatory dependency between the pair of genes it refers to.
These dependencies are described by gene regulatory networks in which there is a node per gene and two nodes are
connected with a directed edge if the first gene regulates

A Probabilistic Model for Dirty Multi-task Feature Selection

the second. These networks are sparse (with many missing edges) and have hub nodes (transcription factors) that
regulate several genes. Figure 5 shows a sample network.

Figure 5. Sample gene regulatory network used in the experiments. Nodes that are potential hubs have a diamond shape.
Table 2. Avg. area under the ROC curve for the network reconstruction experiments and RMSE for the anticancer drug sensitivity experiments, for each of the different methods considered.
Method
MFS
DMFS
DM
MFS D EP
RMFL
STL

AUROC
0.73¬±0.05
0.84¬±0.05
0.76¬±0.06
0.79¬±0.05
0.79¬±0.05
0.70¬±0.04

RMSE
0.733¬±0.053
0.717¬±0.050
0.703¬±0.050
0.704¬±0.051
0.703¬±0.050
0.730¬±0.049

We formulate the problem of inducing W given X as a
multi-task problem with d tasks where the model coefficients of task k correspond to the k-th row of W. The
design matrix X(k) is given by the matrix X with column
k set to zero. The targets of task k are the entries in the
k-th column of X. To favor sparse networks we use the
proposed prior for W. This prior models the hub nodes in
the network by considering jointly relevant features across
tasks, but it allows for small deviations to consider genes
regulated, in addition, by a few extra genes (outlier features), or genes regulated by very specific transcription factors (outlier tasks). The regulatory network can be induced
by computing the posterior probability pij that each entry
(j)
wi in W is non-zero. The corresponding directed edge
j ‚Üí i is predicted when pij exceeds a threshold Œ∂ ‚àà [0, 1].
Thus, these experiments evaluate the ability of each method
to discriminate between relevant and irrelevant coefficients.
We evaluate the different methods in the task of inferring
gene regulatory networks. The experimental protocol follows the DREAM 4 in silico challenge 2009. We generate
100 networks with 100 genes and sample 90 steady-state
measurements from each network using GeneNetWeaver
(Schaffter et al., 2011). The reconstruction performance
is measured in terms of the area under the ROC curve (AUROC) (Fawcett, 2006), obtained when Œ∂ varies between 0
and 1. In MFS, DM and RMFL, to induce the network, we

use the absolute values of the estimated entries of W normalized to sum to one, instead of posterior probabilities.
Table 2 shows the average AUROC for each method.
The best method (higher is better) is DMFS followed by
MFSDep , RMFL, DM and MFS. The method with the lowest performance is STL. The differences are statistically
significant (p-value < 5% using a paired Student‚Äôs T test).
This result shows that multi-task methods are beneficial in
this problem and that the hypothesis made by DMFS is
more adequate, probably because it is more flexible. In
DMFS several tasks have a significantly higher probability
of being outlier tasks, and the same is observed for several
features (results not shown). We have also evaluated here
the winning solution of the DREAM 4 challenge (HuynhThu et al., 2010). This method uses tree-ensembles to identify relevant features, but does not exploit task relations.
The average AUROC obtained is 0.75, which is below the
one shown in Table 2 for DMFS, MFSDep and RMFL.
5.3. Denoising of natural images
We consider the problem of denoising the 256 √ó 256 house
image used in (Titsias & LaÃÅzaro-Gredilla, 2011) when it
has been contaminated by Gaussian noise. Three levels
are considered for the standard deviation of the noise œÉ(k) .
Namely, 25, 50 and 75, ‚àÄk. Following that work, we partition the noisy image in 62, 001 overlapping blocks of 8 √ó 8
pixels and regard each block as a different task. These tasks
are then grouped forming 64 groups of non-overlapping
blocks (i.e., one group of 32√ó32 blocks, 7 groups of 32√ó31
blocks, 7 groups of 31√ó32 blocks and 49 groups of 31√ó31
blocks) which are solved in parallel in a cluster using each
multi-task method (see the supplementary material). To denoise the image we set y(k) equal to each block and each
X(k) equal to an orthonormal basis corresponding to the
Haar wavelet. Thus, d = 64 and Nk = 64 for each task k.
It is well known that natural images have sparse representations under a wavelet basis. Thus, the learning process
involves finding the wavelet coefficients corresponding to
each block from the noisy observations. Using these coefficients the original image can be reconstructed by obtaining
their projection under the wavelet basis. We assume in all
probabilistic methods the same level of noise for each task.
Table 3 shows the peak-signal-to-noise ratio obtained by
each method (higher is better) in the denoising process.
The best results are obtained by the proposed approach
DMFS, which improves the results of the other methods,
especially for high levels of noise, where multi-task methods show a clear advantage over single-task learning. As in
the previous experiments, DMFS also identifies here several outlier tasks and features (results not shown). Figure
6 shows the original noisy images and the corresponding
denoised images obtained by DMFS for each value of œÉ(k) .

A Probabilistic Model for Dirty Multi-task Feature Selection

1

ZD‚àí6474

TKI258

Topotecan

TAE684

RAF265

Sorafenib

Panobinostat

PLX4720

Paclitaxel

PF2341066

PHA‚àí665752

PD‚àí0332991

PD‚àí0325901

Nilotinib

Nutlin‚àí3

LBW242

Lapatinib

L‚àí685458

Erlotinib

Irinotecan

AZD6244

AEW541

AZD0530

17‚àíAAG

Probability

We observe that several drugs are marked as outlier tasks.

0.5

œÉ(k) = 75

0

œÉ(k) = 50

Denoised Image

œÉ(k) = 25

Noisy Image

Figure 7. Avg. posterior probability that each drug is an outlier
task, as estimated by DMFS, in the drug sensitivity experiments.

Figure 6. Noisy images and corresponding denoised images obtained by DMFS, for each different value of œÉ(k) considered ‚àÄk.
Table 3. Peak-signal-to-noise ratio for each method.
Method œÉ(k) = 25 œÉ(k) = 50 œÉ(k) = 75
MFS
25.90
23.89
23.87
DMFS
30.67
27.25
25.22
DM
28.50
25.91
24.24
MFS D EP
30.46
25.74
23.65
RMFL
28.35
25.56
24.09
STL
30.58
26.37
23.35

5.4. Anticancer drug sensitivity prediction
We consider the dataset described in (Barretina et al.,
2012). This dataset contains microarray gene expression
data from 479 human cancer cell lines with pharmacological profiles for 24 anticancer drugs. After removing missing values 294 cell lines remain. We filter the data and
consider only the 1, 000 genes with the largest interquartile
distance. The task of interest is to predict each drug sensitivity (measured in terms of the area over the dose-response
curve) from the microarray data for each cell line. Thus, in
these experiments d = 1, 000, K = 24 and Nk = 294
‚àÄk. We use 90% of the data for training and 10% for testing. The reported estimates are averaged over 100 repetitions. We report the test root mean squared error (RMSE).
In these experiments assuming in all probabilistic methods
the same level of noise for each task also improves results.
Table 2 shows the results of the experiments. The best
methods are DM, RMFL and MFSDep . The solution of DM
reduces to the one of the group LASSO (i.e., one regularization parameters is set always to zero). We believe the
better performance obtained by DM and RMFL is a consequence of shrinking too much relevant coefficients, which
may be useful here to alleviate over-fitting since microarray
data is notoriously very noisy. DMFS performs worse than
these three methods, and the differences are statistically
significant according to a paired Wilcoxon test (p-value
< 5%). Nevertheless, DMFS improves over the baselines
STL and MFS, and the differences are also statistically significant. Finally, Figure 7 shows the average probability
that each drug is an outlier task, as estimated by DMFS.

Last, we compare here the utility of DMFS to identify outlier tasks with that of RMFL. For this, we train the group
LASSO on the data when the tasks identified as outliers
by each method are removed (recall that the group LASSO
performs best). In DMFS we remove those tasks whose
probability of being an outlier is above 1%. In RMFL we
remove those tasks whose rows in Q are not zero. The results obtained indicate that when DMFS is used to remove
outlier tasks the RMSE of the group LASSO on the nonoutlier tasks is 0.667 ¬± 0.061, when the outlier tasks are
removed, and 0.671 ¬± 0.059 otherwise. This improvement
is statistically significant according to a paired Wilcoxon
test (p-value < 5%). By contrast, when RMFL is used to
remove outlier tasks the RMSE of the group LASSO on the
non-outlier tasks is 0.684¬±0.074, when the outlier tasks are
removed, and 0.686¬±0.069 otherwise. This other improvement is not statistically significant (p-value > 5%), which
shows that DMFS is better for identifying outlier tasks.

6. Conclusions
Most methods for multi-task feature selection in the literature assume jointly relevant and irrelevant features across
tasks. This hypothesis may be too restrictive in practice.
In this paper, we have proposed a new prior distribution
that considers that most tasks share relevant and irrelevant
features, but that allows for some tasks to have different
relevant and irrelevant coefficients (outlier tasks), and for
some features to be arbitrarily relevant or irrelevant for
each task (outlier features). This is a more flexible assumption. Unfortunately, exact inference is infeasible under
such a prior. Nevertheless, a quadrature-free expectation
propagation method can be used for approximate inference.
A model using our prior has been evaluated in several experiments involving the reconstruction of gene regulatory
networks, the denoising of natural images and the prediction of drug sensitivity from microarray data. These experiments show gains in the prediction performance and in the
identification of relevant features. Such a prior is also useful to better understand the data, since it allows to identify
outlier tasks and features. When outlier tasks are removed
from the training set, traditional multi-task feature selection methods obtain better results in the non-outlier tasks.
This confirms that removed tasks were indeed outlier tasks.

A Probabilistic Model for Dirty Multi-task Feature Selection

Acknowledgements
Daniel HernaÃÅndez-Lobato gratefully acknowledges the use
of the facilities of Centro de Computacin Cientfica (CCC)
at Universidad AutoÃÅnoma de Madrid. This author also acknowledges financial support from Spanish Plan Nacional
I+D+i, Grant TIN2013-42351-P, and from Comunidad
de Madrid, Grant S2013/ICE-2845 CASI-CAM-CM. JoseÃÅ
Miguel HernaÃÅndez-Lobato acknowledges financial support
from the Rafael del Pino Fundation.

References
Argyriou, A., Evgeniou, T., and Pontil, M. Multi-task feature learning. In Neural Information Processing Systems,
pp. 41‚Äì48. 2007.
Armagan, A., Dunson, D., and Clyde, M. Generalized beta
mixtures of Gaussians. In Neural Information Processing Systems, pp. 523‚Äì531. 2011.
Barretina et al. The cancer cell line encyclopedia enables
predictive modelling of anticancer drug sensitivity. Nature, 483:603‚Äì307, 2012.
Berger, J. A robust generalized Bayes estimator and confidence region for a multivariate normal mean. The Annals
of Statistics, 8:716‚Äì761, 1980.
Bishop, C. M. Pattern Recognition and Machine Learning.
Springer, 2006.
Carvalho, C.M., Polson, N.G., and Scott, J.G. Handling
sparsity via the horseshoe. J. Mach. Learn. Res. W&CP,
5:73‚Äì80, 2009.
Fawcett, T. An introduction to ROC analysis. Pattern
recognition letters, 27:861‚Äì874, 2006.
Gong, P., Ye, J., and Zhang, C. Robust multi-task feature learning. In International Conference on Knowledge
Discovery and Data Mining, pp. 895‚Äì903, 2012.
HernaÃÅndez-Lobato, D. and HernaÃÅndez-Lobato, J. M.
Learning feature selection dependencies in multi-task
learning. In Neural Information Processing Systems, pp.
746‚Äì754. 2013.
HernaÃÅndez-Lobato, D., HernaÃÅndez-Lobato, J. M.,
Helleputte, T., and Dupont, P. Expectation propagation for Bayesian multi-task feature selection.
In European Conference on Machine Learning, pp.
522‚Äì537, 2010.
HernaÃÅndez-Lobato, D., HernaÃÅndez-Lobato, J. M., and
Dupont, P.
Generalized spike-and-slab priors for
Bayesian group feature selection using expectation propagation. J. Mach. Learn. Res., 14:1891‚Äì1945, 2013.

HernaÃÅndez-Lobato, J. M., HernaÃÅndez-Lobato, D., and
SuaÃÅrez, A. Expectation propagation in linear regression
models with spike-and-slab priors. Machine Learning,
99:437‚Äì487, 2015.
Huynh-Thu, V. A., Irrthum, A., Wehenkel, L., and Geurts,
P. Inferring regulatory networks from expression data
using tree-based methods. PLoS ONE, 5:e12776, 2010.
Jalali, A., Ravikumar, P., Sanghavi, S., and Ruan, C. A
dirty model for multi-task learning. In Neural Information Processing Systems, pp. 964‚Äì972. 2010.
Jebara, T. Multi-task feature and kernel selection for
SVMs. In International Conference on Machine Learning, pp. 55‚Äì62, 2004.
Johnstone, I. M. and Silverman, B. W. Empirical Bayes
selection of wavelet thresholds. Annals of Statistics, 33:
1700‚Äì1752, 2005.
Minka, T.
Expectation propagation for approximate
Bayesian inference. In Annual Conference on Uncertainty in Artificial Intelligence, pp. 362‚Äì36, 2001.
Mitchell, T. J. and Beauchamp, J. J. Bayesian variable selection in linear regression. Journal of the American Statistical Association, 83:1023‚Äì1032, 1988.
Obozinski, G., Taskar, B., and Jordan, M.I. Joint covariate
selection and joint subspace selection for multiple classification problems. Statistics and Computing, pp. 1‚Äì22,
2009.
Passos, A., Rai, P., Wainer, J., and DaumeÃÅ III, H. Flexible
modeling of latent task structures in multitask learning.
In International Conference on Machine Learning, 2012.
Schaffter, T., Marbach, D., and Floreano, D.
Genenetweaver: In silico benchmark generation and
performance profiling of network inference methods.
Bioinformatics, 27:2263‚Äì2270, 2011.
Seeger, M. Expectation propagation for exponential families. Technical report, UC, Berkeley, 2006.
Strawderman, W. E. Proper Bayes minimax estimators of
the multivariate normal mean. The Annals of Mathematical Statistics, 42:385‚Äì388, 1971.
Titsias, M. and LaÃÅzaro-Gredilla, M. Spike and slab variational inference for multi-task and multiple kernel learning. In Neural Information Processing Systems, pp.
2339‚Äì2347, 2011.
Vogt, J. E. and Roth, V. The group-lasso: `1,‚àû regularization versus `1,2 regularization. In 32nd Anual Symposium of the German Association for Pattern Recognition,
volume 6376, pp. 252‚Äì261, 2010.

A Probabilistic Model for Dirty Multi-task Feature Selection

Xiong, T., Bi, J., Rao, B., and Cherkassky, V. Probabilistic
joint feature selection for multi-task learning. In Seventh SIAM International Conference on Data Mining,
pp. 332‚Äì342, 2007.
Xue, Y., Liao, X., Carin, L., and Krishnapuram, B. Multitask learning for classification with Dirichlet process priors. J. Mach. Learn. Res., 8:35‚Äì63, 2007.
Zhang, J., Ghahramani, Z., and Yang, Y. Flexible latent
variable models for multi-task learning. Machine Learning, 73:221‚Äì242, 2008.

