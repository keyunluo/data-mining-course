Subsampling Methods for Persistent Homology
Frédéric Chazal
INRIA Saclay, Palaiseau, 91120, France

FREDERIC . CHAZAL @ INRIA . FR

Brittany Terese Fasy
Computer Science Department, Tulane University, New Orleans, LA 70118

BRITTANY @ FASY. US

Fabrizio Lecci
Department of Statistics, Carnegie Mellon University, Pittsburgh, PA 15213

LECCI @ CMU . EDU

Bertrand Michel
LSTA, Université Pierre et Marie Curie (UPMC), Paris, 75005, France

BERTRAND . MICHEL @ UPMC . FR

Alessandro Rinaldo
Department of Statistics, Carnegie Mellon University, Pittsburgh, PA 15213

ARINALDO @ CMU . EDU

Larry Wasserman
Department of Statistics, Carnegie Mellon University, Pittsburgh, PA 15213

LARRY @ CMU . EDU

Abstract
Persistent homology is a multiscale method for
analyzing the shape of sets and functions from
point cloud data arising from an unknown distribution supported on those sets. When the size of
the sample is large, direct computation of the persistent homology is prohibitive due to the combinatorial nature of the existing algorithms. We
propose to compute the persistent homology of
several subsamples of the data and then combine the resulting estimates. We study the risk
of two estimators and we prove that the subsampling approach carries stable topological information while achieving a great reduction in computational complexity.

1. Introduction
Topological Data Analysis (TDA) refers to a collection of
methods for finding topological structure in data (Carlsson,
2009). The input is a dataset drawn from a probability measure supported on an unknown set X. The output is a collection of data summaries that are used to describe the topological features of X. Comparisons between the datasets
can then be done on these data summaries.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

Homology, or more precisely persistent homology, appears
as a fundamental tool for TDA. Homology associates to
any topological space X, a family of vector spaces (the
so-called homology groups) Hk (X), k = 0, 1, . . ., each of
them encoding topological features of X. The k th Betti
number of X, denoted k , is the rank of Hk (X) and represents the number of k-dimensional features of X: for example, 0 is the number of connected components of M,
1 the number of independent cycles or “tunnels”, 2 the
number of “voids”, etc. (see Hatcher, 2001). Persistent homology (Edelsbrunner et al., 2002; Zomorodian & Carlsson, 2005) provides a framework and efficient algorithms
to encode the evolution of the homology of families of
nested topological spaces indexed by a set of real numbers
that may often be seen as scales, such as the the union of
growing balls, or a nested family of simplicial complexes
built on top of the data. The obtained multiscale topological information is then represented in a simple way as a
barcode or persistence diagram, providing relevant information about the data (Cohen-Steiner et al., 2007; Chazal
et al., 2009; 2012a;b). The persistence diagram can be converted into a summary function called a persistence landscape (Bubenik, 2012); see Section 2. These landscapes
are the data summaries that we focus on in this paper.
Contribution and Related Work. The time and space
complexity of persistent homology algorithms is one of
the main obstacles in applying TDA techniques to highdimensional problems. To overcome the problem of computational costs, we propose the following strategy: given

Subsampling Methods for Persistent Homology

a large point cloud, take several subsamples, compute the
landscape for each subsample, and then combine the information. More precisely, let be a random persistence
landscape from m
µ , a measure on the space of landscape
functions induced by a sample of size m from a metric
measure space (X, ⇢, µ). We show that the average landscape is stable with respect to perturbations of the underlying measure µ in the Wasserstein metric; see Theorem 5. TheP
empirical counterpart of the average landscape
n
1
m
is m
n = n
µ . The empiri=1 i , where 1 , . . . , n ⇠
ical average landscape can be used as an unbiased estimator
of E m
[ ] and as a biased estimator of Xµ , the computaµ
tionally expensive persistence landscape associated to the
support of the measure µ. Unlike Xµ , the estimator m
n
is robust to the presence of outliers. In the same spirit,
we propose a different estimator constructed by choosing
a sample of m points of X as close as possible to Xµ ,
and then computing its persistent homology to approximate Xµ . See Section 3 for more details.
Closely related to our approach, the distribution of persistence diagrams associated to subsamples of fixed size was
studied in Blumberg et al. (2014). There, the authors show
that the distribution of persistence diagrams associated to
subsamples of fixed size is stable with respect to perturbations of the underlying measure in the Gromov-Prohorov
metric. Though similar in spirit, our approach relies on
different techniques and, in particular, leads to easily computable summaries of the persistent homology of a given
space. These summaries are particularly useful when the
exact computation of the persistent homology is infeasible,
as in the case of large point clouds or when the data are
noisy or contain outliers.
Software. The computations in this paper were done using the R package TDA (Fasy et al., 2014a). The package
includes a series of tools for the statistical analysis of persistent homology, including the methods described in Fasy
et al. (2014b), Chazal et al. (2014b), Chazal et al. (2014a),
and this paper.
Outline. Background on persistent homology is presented
in Section 2. Our approach is introduced in Section 3, with
a formal definition of the estimators briefly described in this
introduction. Section 4 contains the stability result of the
average landscape. Section 5 is devoted to the risk analysis of the proposed estimators. In Section 6, we apply
our methods to two examples. We conclude with some remarks in Section 7 and defer proofs and technical details to
the appendices.

2. Background
In this section, we briefly introduce the basics of persistence used in this paper. We refer the reader to Edelsbrun-

ner & Harer (2010); Chazal et al. (2012b); Bubenik (2012)
for more details.
2.1. Geometric Complexes
To compute the persistent homology from a set of data, we
need to construct a set of structures called simplicial complexes. A simplicial complex C is a set of simplices (points,
segments, triangles, etc) such that any face from a simplex
in C is also in C and the intersection of any two simplices
of C is a (possibly empty) face of these simplices.

Figure 1. Top left: The ↵ sublevel set of the distance function
to a point set X in R2 . Top right: the ↵-complex. Bottom left:
Cech↵ (X). Bottom right: Rips2↵ (X). The last two complexes
include a tetrahedron.

Given a metric space X, we define three simplicial complexes whose vertex set is X; see Figure 1 for illustrations.
The Vietoris-Rips complex Rips↵ (X) is the set of simplices [x0 , . . . , xk ] such that dX (xi , xj )  ↵ for all (i, j).
The Čech complex Cech↵ (X) is similarly defined as the
set of simplices [x0 , . . . , xk ] such that there exists a point
x 2 X for which dX (x, xi )  ↵ for all i. Note that these
two complexes are related by Rips↵ (X) ✓ Cech↵ (X) ✓
Rips2↵ (X) and that their definition does not require X to
be finite. When X ⇢ Rd , we also define the ↵-complex as
the set of simplices [x0 , . . . , xk ] such that there exists a ball
of radius at most ↵ containing x0 , . . . , xk on its boundary
and whose interior does not intersect X.
Each family described above is non-decreasing with ↵: for
any ↵  , there is an inclusion of Rips↵ (X) in Rips (X),
and similarly for the Čech and Alpha complexes. These
sequences of inclusions are called filtrations. In the following, we let Filt(X) := (Filt↵ (X))↵2A denote a filtration corresponding to one of the parameterized complexes
defined above.
2.2. Persistence Diagrams
The topology of Filt↵ (X) changes as ↵ increases: new connected components can appear, existing connected components can merge, cycles and cavities can appear or be filled,
etc. Persistent homology tracks these changes, identifies

Subsampling Methods for Persistent Homology

features and associates an interval or lifetime (from b to d)
to them. For instance, a connected component is a feature
that is born at the smallest ↵ such that the component is
present in Filt↵ (X), and dies when it merges with an older
connected component. Intuitively, the longer a feature persists, the more relevant it is. The lifetime of a feature can be
represented as a point in the plane with coordinates (b, d).
The obtained set of points (with multiplicity) is called the
persistence diagram D(Filt(X)) (and we will abuse terminology slightly by denoting it DX ). Note that the diagram is entirely contained in the half-plane above the diagonal defined by y = x, since death always occurs after
birth. Chazal et al. (2012a) shows that this diagram is still
well-defined under very weak hypotheses, and in particular
D(Filt(X)) is well-defined for any compact metric space.
The most persistent features (supposedly the most important) are those represented by the points furthest from the
diagonal in the diagram; whereas, points close to the diagonal can be interpreted as (topological) noise, as they are
indistinguishable from features that are born and die at the
same value of ↵.
To avoid (minor) technical difficulties, we restrict our attention to diagrams D such that (b, d) 2 [0, T ] ⇥ [0, T ] for
all (b, d) 2 D, for some fixed T > 0. Note that, in our setting, DX satisfies this property as soon as T is larger than
the diameter of X. We denote by DT the space of all such
(restricted) persistence diagrams and we endow it with a
metric called the bottleneck distance db . Given two persistence diagrams, the bottleneck distance is defined as the
infimum of the for which we can find a matching between
the diagrams, such that two points can only be matched if
their distance is less than and all points at distance more
than from the diagonal must be matched.
A fundamental property of persistence diagrams, proven
in Chazal et al. (2012a), is their stability.
Recall
that the Hausdorff distance between two compact subsets n
X, Y of a metric space (X, ⇢) is oH(X, Y ) =
e
max max min ⇢(x, y), max min ⇢(x, y) . If X and X
x2X y2Y

y2Y x2X

are two compact metric spaces, then one has

e
db (DX , D e )  2dGH (X, X),
X

(1)

e denotes the Gromov-Hausdorff distance,
where dGH (X, X)
e over
i.e., the infimum Hausdorff distance between X and X
all possible isometric embeddings into a common metric
e are already embedded in the same metric
space. If X and X
space then (1) holds for H(·, ·) in place of dGH (·, ·).
2.3. Persistence Landscapes
The persistence landscape, introduced in Bubenik (2012),
is a collection of continuous, piecewise linear functions : Z+ ⇥ R ! R that summarizes a persistence dia-

gram. To define the landscape, consider the set of functions
d b
created by tenting each point p = (x, y) = b+d
rep2 , 2
resenting a birth-death pair (b, d) 2 D as follows:
8
>
<t x + y t 2 [x y, x]
⇤p (t) = x + y t t 2 (x, x + y]
>
:
0
otherwise
8
b+d
>
<t b t 2 [b, 2 ]
b+d
= d t t 2 ( 2 , d]
(2)
>
:
0
otherwise.

Figure 2. We use the rotated axes to represent a persistence diagram D. A feature (b, d) 2 D is represented by the point
( b+d
, d 2 b ) (pink). In words, the x-coordinate is the average pa2
rameter value over which the feature exists, and the y-coordinate
is the half-life of the feature. The cyan curve is the landscape (1, ·).

We obtain an arrangement of piecewise linear curves by
overlaying the graphs of the functions {⇤p }p ; see Figure 2.
The persistence landscape of D is a summary of this arrangement. Formally, the persistence landscape of D is the
collection of functions
D (k, t)

= kmax ⇤p (t),
p

t 2 [0, T ], k 2 N,

(3)

where kmax is the kth largest value in the set; in particular,
1max is the usual maximum function. We set D (k, t) = 0
if the set {⇤p (t)}p contains less than k points. For simplicity of exposition, if DX is the persistence diagram of some
metric space X, then we use X to denote D .
X
We denote by LT the space of persistence landscapes corresponding to DT . From the definition of persistence
landscape, we immediately observe that D (k, ·) is oneLipschitz. The following additional properties are proven
in Bubenik (2012).
Lemma 1. Let D, D0 be persistence diagrams. We have
the following for any t 2 R and any k 2 N:
(i) D (k, t)
0.
D (k + 1, t)
0
(ii) | D (k, t)
D 0 (k, t)|  db (D, D ).
For ease of exposition, we focus on the case k = 1, and set
D (t) = D (1, t). However, the results we present hold

Subsampling Methods for Persistent Homology

for k > 1. In fact, the results hold for more general summaries of persistence landscapes, including the silhouette
defined in Chazal et al. (2014b).

3. The Multiple Samples Approach
Let (X, ⇢) be a metric space of diameter at most T /2 and
let P(X) be the space of probability measures on X, such
that, for any measure µ 2 P(X), its support Xµ is a compact set. The space Xµ is a natural object of interest in
computational topology. Its persistent homology is usually
approximated by the persistent homology of the distance
function to a sample XN = {x1 , . . . , xN } ⇢ Xµ . Fasy
et al. (2014b) propose several methods for the construction
of confidence sets for the persistence diagram of Xµ , while
Chazal et al. (2014c) establish optimal convergence rates
for db (DXµ , DXN ).
When N is too large, the computation of the persistent
homology of XN is prohibitive, due to the combinatorial
complexity of the computation. Our aim is to study topological signatures of the data that can be efficiently computed in a reasonable time. We define such quantities by
repeatedly sampling m points of X according to µ.
For any positive integer m, let X = {x1 , · · · , xm } ⇢ X
be a sample of m points from the measure µ 2 P(X). The
corresponding persistence landscape is X and we denote
⌦m
by m
on LT . Note that
µ the measure induced by µ
the persistence landscape X can be seen as a single draw
from the measure m
µ . We consider the point-wise expectations of the (random) persistence landscape under this measure: E m
[ X (t)], t 2 [0, T ]. The main result of this paper
µ
establishes the stability of this quantity under perturbation
of µ, making it relevant from a topological point of view
(see next section).
The average landscape E m
[ X ] has a natural empirical
µ
counterpart, which can be used as its unbiased estimator. Let S1m , . . . , Snm be n independent samples of size m
from µ. We define the empirical average landscape as

n independent samples of size m from µ⌦m . The closest
sample is
m
d
C
n = arg

min

m}
S2{S1m ,...,Sn

H(S, Xµ )

(5)

m =
and the corresponding landscape function is c
m.
d
n
C
n
Of course, the method requires the support of µ to be a
known quantity.
Remark 2. Computing the persistent homology of XN is
O(exp(N )), whereas computing the average landscape is
O(n exp(m)) and the persistent homology of the closest
sample is O(nmN + exp(m)).
Remark 3. The general framework described above is
valid for the case in which µ is a discrete measure with
support Xµ = {x1 , . . . , xN } ⇢ RD . For example, the following situation is very common in practice. Let XN =
{x1 , . . . , xN } be a given point cloud, for large but fixed
N 2 N. When N is large, the computation of the persistent homology of XN is infeasible. Instead, we consider
the discrete uniform measure µ that puts mass 1/N on each
point of XN , and we propose to estimate Xu by repeatedly
subsampling m ⌧ N points of XN according to µ.

We study the `1 -risk hof the proposed
estimators,
i
⇥
⇤
m
m
c
E k Xµ
n k1 and E k Xµ
n k1 , under the following assumption on the underlying measure µ, which we
refer to as the (a, b, r0 )-standard assumption: there exist
positive constants a, b and r0 0 such that
8r > r0 , 8x 2 Xµ , µ(B(x, r))

1 ^ arb .

(6)

For r0 = 0, this is known as the (a, b)-standard assumption
and has been widely used in the literature of set estimation under Hausdorff distance (Cuevas & Rodrı́guez-Casal,
2004; Cuevas, 2009; Singh et al., 2009) and more recently
in the statistical analysis of persistence diagrams (Chazal
et al., 2014c; Fasy et al., 2014b). We use the generalized
version with r0 > 0 to take into account the case in which µ
is a discrete measure (in which case r0 depends on N ); see
Appendix C for more details.

n

m (t)
n

=

1X
n i=1

Sim (t),

for all t 2 [0, T ],

(4)

and propose to use m
n to estimate Xµ . The variance of
this estimator under the `1 -distance was studied in detail
in Chazal et al. (2014b). Here instead we are concerned
with the quantity k Xµ E m
[ X ]k1 , which can be seen
µ
as the bias component (see Section 5).
In addition to the average, we also consider using the closest sample to Xµ in Hausdorff distance. The closest sample
method consists in choosing a sample of m points of X, as
close as possible to Xµ , and then use this sample to build
a landscape that approximates Xµ . Let S1m , . . . , Snm be

4. Stability of the Average Landscape
Consider the framework described in Section 3: m points
are repeatedly sampled from the space X according to a
measure µ 2 P(X). In this section, we show that the
average landscape E m
[ X ] is an interesting quantity on
µ
its own, since it carries some stable topological information about the underlying measure µ, from which the data
are generated.
Chazal et al. (2014b) provide a way to construct confidence
bands for E m
[ X ]. Here, we compare the average landµ
scapes corresponding to two measures that are close to each
other in the Wasserstein metric.

Subsampling Methods for Persistent Homology

Definition 4. Given a metric space (X, ⇢), the pth Wasserstein distance between two measures µ, ⌫ 2 P(X) is
⇣
⌘ p1
R
W⇢,p (µ, ⌫) = inf ⇧ X⇥X [⇢(x, y)]p d⇧(x, y) , where
the infimum is taken over all measures on X ⇥ X with
marginals µ and ⌫.
Next, we show that the average behavior of the landscapes
of sets of m points sampled according to any measure µ is
stable with respect to the Wasserstein distance.
Theorem 5. Let (X, ⇢) be a metric space of diameter
bounded by T /2. Let X ⇠ µ⌦m and Y ⇠ ⌫ ⌦m , where
µ, ⌫ 2 P(X) are two probability measures. For any p 1
we have
E

m
µ

[

E

X]

m
⌫

[

Y

]

1

1

 2 m p W⇢,p (µ, ⌫).

Remark 6. For measures that are not defined on
the same metric space, the inequality of Theorem 5
can be extended to Gromov-Wasserstein
metric:
1
E m
[ X] E m
[ Y]
 2m p GW⇢,p (µ, ⌫).
µ
⌫
1

The result of Theorem 5 is useful for two reasons. First, it
tells us that for a fixed m, the expected “topological behavior” of a set of m points carries some stable information
about the underlying measure from which the data are generated. Second, it provides a lower bound for the Wasserstein distance between two measures, based on the topological signature of samples of m points.
The dependence on m of the upper bound of Theorem 5
seems to be necessary in this setting: intuitively, when m
grows, the samples of m points converge to the support
of µ and ⌫ w.r.t. the Hausdorff distance. Therefore the expected landscapes should converge to the landscapes of the
support of the measures. But, in general, two measures
that are close in the Wasserstein metric can have support
that have very different and unrelated topologies. Indeed,
a similar dependence was also obtained in Blumberg et al.
(2014) when considering the Gromov-Prohorov metric.
Note that in Theorem 5 we do not make any assumption on
the measures µ and ⌫. If we assume that they both satisfy
the (a, b, r0 )-standard assumption we can provide a different bound on the difference of the expected landscapes,
based on the Hausdorff distance between the support of the
two measures.
Theorem 7. Let (X, ⇢) be a metric space of diameter
bounded by T /2. Let X ⇠ µ⌦m and Y ⇠ ⌫ ⌦m , where
µ, ⌫ 2 P(X) satisfy the (a, b, r0 )-standard assumption
⇣
⌘1/b
m
on X. Define rm = 2 log
. Then
am
kE

m
µ

(

X)

E

m
⌫

+ 4rm

(

Y

)k1  2H(Xµ , X⌫ ) + 4r0 +

(r0 ,1) (rm )

+ 4 C1 (a, b) rm

1
,
(log m)2

where C1 (a, b) is a constant depending on a and b.
The following result follows from Theorems 5 and 7.
Corollary 8. Under the same assumptions of Theorem 7,
we have that
n 1
m ( Y )k1  2 min
kE m
(
)
E
m p Wp (µ, ⌫),
X
µ
⌫
H(Xµ , X⌫ ) + 2r0 + 2rm (r0 ,1) (rm ) +
o
1
+ 2 C1 (a, b) rm
.
(log m)2

5. Risk Analysis
In this section, we study the performance of the average
landscape m
n and of the landscape of the closest sample
m , as estimators of
c
Xµ . We start by decomposing the `1 n
risk of the average landscape as follows. Set 1 = S1m ,
with S1m a sample of size m from µ. Then,
E

m
n 1

Xµ



E

Xµ

1 1

+E

where the expectation of m
n is w.r.t. (
pectation of 1 is w.r.t. m
µ.
For the bias term Xµ E
erty to go back into Rd :
E

E

1 1

m
n

m ⌦n
µ)

E

,
(7)
and the ex1 1

we use the stability prop-

 2Eµ⌦m H(Xµ , X),
(8)
where X is a sample of size m from µ. Note that, if calculating H(Xµ , X) is computationally feasible, then, in practice, Eµ⌦m H(Xµ , X) can be approximated by the average
of a large number B of values of H(Xµ , X), for B different
draws of subsamples X ⇠ µ⌦m .
Xµ

1 1

m
µ

Xµ

1 1

To give an explicit bound on the bias, we assume that µ
satisfies the (a, b, r0 )-standard assumption.
⇣
⌘1/b
m
. If µ satisfies the
Theorem 9. Let rm = 2 log
am
(a, b, r0 )-standard assumption, then
Xµ

E

1 1

2r0 + 2rm

(r0 ,1) (rm ) +

+ 2C1 (a, b) rm

1
,
(log m)2

where C1 (a, b) is a constant that depends on a and b.
Chazal et al. (2014b)
control the variance term, which is of
p
the order of 1/ n. Therefore, if r0 is negligible, we see
2/b
that n should be taken of the order of (m/log m) .
bn and invesWe now turn to the closest
sample estimator
h
i
m
c
tigate its `1 risk E k Xµ
n k1 , where the expectation is with respect to (

m ⌦n
.
µ)

As before, in our analyi
h
mk
sis, we rely on the stability property E k Xµ c

1
n

Subsampling Methods for Persistent Homology

h

i

m
d
2E H(Xµ , C
n ) , where the second expectation is with re-

point clouds and compare the corresponding average landscapes and closest subsample landscapes, induced by the
persistent homology of the VR filtrations built on top of
the subsamples.

fies the (a, b, r0 )-standard assumption, then
h
i
mk
E k Xµ c
n 1  2r0 + 2rm (r0 ,1) (rm ) +

Example 13 (3D Shapes). We use the publicly available database of triangulated shapes (Sumner & Popović,
2004). We select a single pose (#2) of four different classes:
camel, elephant, flamingo, lion. The four shapes are represented in Figure 4. In practice, each shape consists of a 3D
point cloud embedded in Euclidean space, with a number of
vertices that ranges from 7K to 40K. The data are normalized, so that the diameter of each shape is one. For n = 100
times we subsample m = 300 points from each shape, then
we select the closest subsample to the corresponding original point cloud and compute 4 ⇥ n persistence diagrams
(dimension one), one for each subsample. See Figure 5:
the plot on the left shows the landscapes corresponding to
the closest subsamples of m points among the n different
subsamples from each shape; the plot in the middle shows
the empirical average landscapes within each class, computed as the pointwise average of n landscapes, with a
95% uniform confidence band for the true average landscape, constructed using the method described in Chazal
et al. (2014b); the dissimilarity matrix on the right shows
the pairwise `1 distances between the average landscapes
(scale from yellow to red), which, according to Theorem 5,
represent a lower bound for the pairwise Wasserstein distances of the discrete uniform measures on the four different shapes.

spect to (µ

⌦m ⌦n

)

.

⇣
⌘1
b
m) b
Theorem 10. Let rm = 2 log(2
. If µ 2 P(X) satisam

+ 2C2 (a, b) rm

1
,
n [ log(2b m)]n+1

where C2 (a, b) is a constant that depends on a and b.
Remark 11. The risk of the closest subsample method can
in principle be smaller than the average landscape method.
In Appendix C, we show that if µ is the discrete uniform
measure on a point cloud of size N , sampled from a measure satisfying the (a, b, 0)-standard assumption, then r0 is
of the order of ( logNN )1/b . When r0 is negligible, the rates
of theorems 9 and 10 are comparable, both of the order
of O( logmm )1/b . However, the average method has another
advantage: it is robust to outliers. This point is discussed
in detail in Appendix D.

6. Experiments
Since computing the persistent homology of the VietorisRips (VR) filtrations built on top of a large samples is infeasible, we resort to the subsampling strategy described
in Section 3. More formally, let XN = {x1 , . . . , xN }
be a large point cloud. We draw n subsamples, each of
size m ⌧ N points, from µ, the discrete uniform measure on XN . First, we use a toy example to compare the
time complexity of computing the persistent homology of
the entire point cloud, with the complexity of the subsampling approach.
Example 12 (Toy). Let XN be the sample of N = 500
points depicted in the left plot of Figure 3. The VR filtration built on top of the sample consists of 20,833,750 simplices and computing the persistence diagram, and hence
the 1st persistence landscape of one-dimensional features
in the middle plot, required 28.34 seconds on a Macbook
Pro with 2.8 GHz processor and 16 GB RAM. The average landscape on the right plot is computed using n = 10
subsamples of size m = 100, each resulting in a VR filtration of 166,750 simplices, whose persistent homology
was computed on average in 0.14 seconds. The 95% confidence band for the true average landscape is constructed
using the multiplier bootstrap described in Chazal et al.
(2014b). Both landscapes (the landscape of the full sample
and the average landscape) show two peaks, corresponding to the two loops of the circles from which the data were
sampled. However computing the the average landscape
was 20 times faster.
In each of the following two examples, we consider four

Example 14 (Magnetometer Data). For the second example, we consider the problem of distinguishing human activities performed while wearing inertial and magnetic sensor units. The dataset is publicly available at the UCI Machine Learning Repository1 and is described in Barshan
& Yüksek (2013), where it is used to classify 19 activities performed by eight people wearing sensor units on the
chest, arms, and legs. For ease of illustration, we report
here the results on four activities (walking, stepper, cross
trainer, jumping) performed by a single person (#1). We
use the data from the magnetometer of a single sensor (left
leg), which measures the direction of the magnetic field in
the space at a frequency of 25Hz. For each activity there
are 7,500 consecutive measurements that we treat as a 3D
point cloud in the Euclidean space. As an example, Figure 4 shows 500 points at random for two activities (walking and using a cross trainer). As in the previous example,
for n = 80 times, we subsample m = 200 points from the
point cloud of each activity, then construct the landscapes
of the closest subsamples, the average landscapes (dimension one), and the dissimilarity matrix based on the `1 distances of the average landscapes. See Figure 6. To the
1
http://archive.ics.uci.edu/ml/datasets/Daily+and+
Sports+Activities

Subsampling Methods for Persistent Homology

●
●●

●●
●
●

●●●
●
●●
●

●
●●

●

●●● ● ●
●●
●

●
●●●●
●●
●
●
●

Mean Landscape (m=100)
with 95% band

●●
●
●●
●●
●
●
●
●
●
●

●
●
●

●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●

1.5

●

●
●
●
●
●
●
●
●

1.5

●
●●
●●

2.0

Landscape (N=500)
2.0

Sample (N=500)

1.0

λm
n

λXN

●
●
●●
●●
●●
●
●●●●
●●

●
●
●
● ●
●
●
●● ●●● ●

●● ●
●

●
●●
●
●●
●
●
●
●
●
●●
●●●

0.0

●●●●● ●
●
●
● ●
●
●
●●●
●
●
●●
●●
●
●●
●●
●
●
●●
●
●●
●●
●
●
●●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●●●
●●
●●●●
●
●●
●●
●
●

●
●
●
●●
●
●
●
●
●

0.0

●
●
●
●
●●
●●
●
●
●●
●●
●
●

0.5

●
●
●
●
●
●
●
●
●
●
●

0.5

●
●
●
●
●
●
●

●
●
●

1.0

●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●

0

1

2

3

4

5

0

1

2

t

3

4

5

t

Figure 3. Left: 500 points from two circles of different radii. Middle: 1st landscape of one-dimensional features using the entire sample.
Right: average landscape with 95% confidence band, constructed using n = 10 subsamples, each of size m = 100.

Figure 4. Left: Four 3D shapes. Middle and Left: 500 random points from the magnetometer data of the second experiment.
Lands. of Closest Subsamples

1

ele.

0.03

camel

camel
ele.
flam.
lion

lion

0.01

flam.

0.02

0.03
0.02
0.01

−1

Dissimilarity Matrix

0
0.00

0.7

−1

1

0.00

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●● ●
●
●
●
●
●
●
●
●
●
●
●●
●
●
● ●
●●
●
●●
●
●
●
●
●●●
●●
●
● ●●● ● ●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●●●
●●
●●●●● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●● ●
●
●
●
●
●
●
●
●
● ●
● ● ●
●
●
●
●
●
●
●
● ●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●●
● ●
●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●●●
●
●
● ●
●
●●
● ●
●
●
●
●●●●●
●
●
●
●
●
●
●
●
●●
●
●●
●●●
●
●●●●
●
●
●●
●
●
● ●
●
●
●
●
●●
●
●
●
● ●●
●
● ●●
●●
●●●
●●
●●
●
●
●●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●●
●
●
●●●●
●
●
●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●●
●
●
● ●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●● ●
●
●●
●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●●
●
●
●
●●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●●
●
● ●●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●

Average Landscapes
0.04

camel
ele.
flam.
lion

0.04

Shapes

0.00

0.10

0.20

0.30

camel
0.00

0.10

0.20

ele.

flam.

lion

0.30

Figure 5. Subsampling methods applied to 3D shapes. For n = 100 subsamples of size m = 300, for each shape, we constructed the
landscapes of the closest subsample (left), the average landscape with 95% confidence band (middle) and the dissimilarity matrix of the
pairwise `1 distance between average landscapes.

best of our knowledge, persistent homology has never been
used to study data from accelerometers or magnetometers
before. A remarkable advantage is that the methods of persistent homology are insensitive to the orientation of the
input data, as opposed to other methods that require the
exact calibration of the sensor units; see, for example, Altun et al. (2010) and Barshan & Yüksek (2013).

7. Conclusion
We presented a framework for approximating the persistent
homology of a set using subsamples. The method is simple
and computationally fast. Moreover, we provided stability
results for the new summaries and bounds on the risk of the
proposed estimators. In the future, we plan to investigate
methods for further speeding up the computations.

Subsampling Methods for Persistent Homology

step.
cross.
jump

0.010
0.000

0.010
0.000
0.00

0.05

0.10

0.15

Dissimilarity Matrix
walk

walking
stepper
cross tr.
jumping

0.020

walking
stepper
cross tr.
jumping

0.020

Average Landscapes
0.030

0.030

Lands. of Closest Subsamples

walk
0.00

0.05

0.10

step.

cross.

jump

0.15

Figure 6. Subsampling methods applied to magnetometer data. For n = 80 subsamples of size m = 200, for each activity, we
constructed the landscapes of the closest subsample (left), the average landscape with 95% confidence band (middle) and the dissimilarity
matrix of the pairwise `1 distance between average landscapes.

Acknowledgments
Research supported by ANR-13-BS01-0008, NSF
CAREER Grant DMS 1149677, Air Force Grant
FA95500910373 and NSF Grant DMS-0806009.

References
Alexander, Kenneth S. Rates of growth for weighted empirical processes. In Proc. of Berkeley Conference in Honor
of Jerzy Neyman and Jack Kiefer, volume 2, pp. 475–
493, 1985.
Alexander, Kenneth S.˙ The central limit theorem for
weighted empirical processes indexed by sets. J. Multivar. Anal., 22(2):313–339, 1987a.
Alexander, Kenneth S.˙ Rates of growth and sample moduli for weighted empirical processes indexed by sets.
Probab. Theory Related Fields, 75(3):379–423, 1987b.
Altun, Kerem, Barshan, Billur, and Tunçel, Orkun. Comparative study on classifying human activities with
miniature inertial and magnetic sensors. Pattern Recognition, 43(10):3605–3620, 2010.
Barshan, Billur and Yüksek, Murat Cihan. Recognizing
daily and sports activities in two open source machine
learning environments using body-worn sensor units.
The Computer Journal, pp. bxt075, 2013.
Blumberg, Andrew J. Gal, Itamar, Mandell, Michael A.
and Pancia, Matthew. Persistent homology for metric
measure spaces, and robust statistics for hypothesis testing and confidence intervals. Found. Comput. Math., pp.
1–45, May 2014.
Bubenik, Peter. Statistical topological data analysis using
persistence landscapes. arXiv preprint 1207.6437, 2012.
Carlsson, Gunnar. Topology and data. Bull. Amer. Math.
Soc., 46(2):255–308, 2009.

Chazal, F., Fasy, B.T., Lecci, F., Michel, B., Rinaldo,
A., and Wasserman, L. Robust topological inference: Distance-to-a-measure and kernel distance. arXiv
preprint arXiv: 1412.7197, 2014a.
Chazal, Frédéric, Cohen-Steiner, David, Glisse, Marc,
Guibas, Leonidas J. and Oudot, Steve Y. Proximity of
persistence modules and their diagrams. In Proc. 25th
Annu. Symp. Comp. Geom, pp. 237–246. ACM, 2009.
Chazal, Frédéric, de Silva, Vin, Glisse, Marc, and Oudot,
Steve. The structure and stability of persistence modules.
arXiv preprint 1207.3674, 2012a.
Chazal, Frédéric, De Silva, Vin, and Oudot, Steve. Persistence stability for geometric complexes. Geom. Dedicata, pp. 1–22, 2012b.
Chazal, Frédéric, Fasy, Brittany Terese, Lecci, Fabrizio,
Rinaldo, Alessandro, and Wasserman, Larry. Stochastic
convergence of persistence landscapes and silhouettes.
In Proc. 30th Annu. Sympos. Comput. Geom, 2014b.
Chazal, Frédéric, Glisse, Marc, Labruère, Catherine, and
Michel, Bertrand. Convergence rates for persistence diagram estimation in topological data analysis. In Proc.
31st Int. Conf. Mach. Learn., volume 32, pp. 10–18.
JMLR W&CP, 2014c. arXiv preprint 1305.6239.
Cohen-Steiner, David, Edelsbrunner, Herbert, and Harer,
John. Stability of persistence diagrams. Discrete Comput. Geom., 37(1):103–120, 2007.
Cuevas, Antonio. Set estimation: another bridge between
statistics and geometry. Bol. Estad. Investig. Oper., 25
(2):71–85, 2009. ISSN 1889-3805.
Cuevas, Antonio and Rodrı́guez-Casal, Alberto. On boundary estimation. Advances in Applied Probability, pp.
340–354, 2004.

Subsampling Methods for Persistent Homology

Edelsbrunner, H., Letscher, D., and Zomorodian, A. Topological persistence and simplification. Discrete Comput.
Geom., 28:511–533, 2002.
Edelsbrunner, Herbert and Harer, John. Computational
Topology: An Introduction. AMS, 2010.
Fasy, Brittany Terese, Kim, Jisu, Lecci, Fabrizio, and
Maria, Clement. Introduction to the R package TDA.
arXiv preprint arXiv: 1411.1830, 2014a.
Fasy, Brittany Terese, Lecci, Fabrizio, Rinaldo, Alessandro, Wasserman, Larry, Balakrishnan, Sivaraman, and
Singh, Aarti. Confidence sets for persistence diagrams.
The Annals of Statistics, 42(6):2301–2339, 2014b.
Giné, Evarist and Koltchinskii, Vladimir.
Concentration inequalities and asymptotic results for ratio
type empirical processes. Ann. Probab., 34(3):1143–
1216, 2006.
ISSN 0091-1798.
doi: 10.1214/
009117906000000070. URL http://dx.doi.org/
10.1214/009117906000000070.
Giné, Evarist, Koltchinskii, Vladimir, and Wellner, Jon A.
Ratio limit theorems for empirical processes. In Stochastic inequalities and applications, volume 56 of Progr.
Probab., pp. 249–278. Birkhäuser, Basel, 2003.
Hatcher, A. Algebraic Topology. Cambridge Univ. Press,
2001.
Singh, Aarti, Scott, Clayton, and Nowak, Robert. Adaptive
Hausdorff estimation of density level sets. Ann. Statist.,
37(5B):2760–2782, 2009.
Sumner, Robert W and Popović, Jovan. Deformation transfer for triangle meshes. In ACM Transactions on Graphics (TOG), volume 23, pp. 399–405. ACM, 2004.
Zomorodian, A. and Carlsson, G. Computing persistent homology. Discrete Comput. Geom., 33(2):249–274, 2005.

