Hidden Markov Anomaly Detection

Nico Görnitz
Berlin Institute of Technology, 10587 Berlin, Germany

NICO . GOERNITZ @ TU - BERLIN . DE

Mikio Braun
Berlin Institute of Technology, 10587 Berlin, Germany

MIKIO . BRAUN @ TU - BERLIN . DE

Marius Kloft
Humboldt University of Berlin, 10099 Berlin, Germany

Abstract
We introduce a new anomaly detection methodology for data with latent dependency structure.
As a particular instantiation, we derive a hidden
Markov anomaly detector that extends the regular one-class support vector machine. We optimize the approach, which is non-convex, via a
DC (difference of convex functions) algorithm,
and show that the parameter ν can be conveniently used to control the number of outliers in
the model. The empirical evaluation on artificial
and real data from the domains of computational
biology and computational sustainability shows
that the approach can achieve significantly higher
anomaly detection performance than the regular
one-class SVM.

1. Introduction
In the age of big data, effective filtering methodologies for
unlabeled data such as the framework of learning-based
anomaly detection (Markou & Singh, 2003; Chandola
et al., 2009) are gaining increasing interest by the machine
learning community (Blanchard et al., 2010; Saligrama &
Zhao, 2012; Kloft & Laskov, 2012; Görnitz et al., 2014).
Learning-based anomaly detection methods are at the heart
of several important applications areas, including, in computer security, the detection of yet unsignatured attacks
and novel intrusions in computer networks (Jyothsna et al.,
2011) and, in computational biology, the characterization
of systematic anomalies in microarray analysis (Noto et al.,
2014) and deep sequencing data (Kukita et al., 2013).
Prominent approaches to learning-based anomaly detection include prevalent kernel-based approaches such as the
one-class support vector machine (OC-SVM) (Schölkopf
et al., 2001) or support vector data description (Tax & Duin,
2004). Such methods use a kernel approach to learn a nonProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

KLOFT @ HU - BERLIN . DE

linear representation of the class membership that can be
used to predict the anomaly score of new and yet unseen inputs. These methods are based on the fundamental assumption that the nominal inputs xi are realized independently
from a common probability distribution P , without exploiting any potential patterns contained in the outputs yi . However, in many real-world applications, the data is associated
with an inherently underlying output structure: e.g., in intrusion detection or speech and text recognition the data
naturally admits a language and grammar structure (Rieck
et al., 2010; Joachims et al., 2009); in bioinformatics, the
annotation into exonic, intronic, and intergenic regions inherently underlies genomic data (Schweikert et al., 2009).
It has been shown that methodologies exploiting such potential structure such as the structured support vector machine (SSVM) (Tsochantaridis et al., 2005a) can substantially help performance in such contexts (Rätsch & Sonnenburg, 2007; Joachims et al., 2009; Rieck et al., 2010).
In this paper, we propose a novel kernel-based framework
for the detection of anomalies with underlying sequence
structure, called hidden Markov anomaly detection. This
approach can be introduced in a general way for data with
latent dependency structure, and, for a specific choice of
loss function and joint feature map, we obtain a hidden
Markov analogue to the one-class SVM. Just like the original OC-SVM, the method has a parameter ν that controls the fraction of the outliers, and enjoys deep theoretical
guarantees: we prove that a large deviation bound holds on
the generalization error (measured with respect to the loss
function).
The presented approach is general enough to work with
many problems that can be addressed within the structured
output prediction framework. In this paper though, we focus on using hidden Markov chain type structured output
models, as common in label sequence learning, to facilitate
the detection of anomalous sequences where changes follow a hidden Markov model. That way, we achieve much
better performance compared to the standard OC-SVM,
which treats each position independently, as our empirical
evaluation shows.

Hidden Markov Anomaly Detection
z:

z1

z2

z3

z T-1

zT

x:

x1

x2

x3

xT-1

xT

Figure 1. Factorization of hidden Markov models: the latent variables (z, green) can not be observed directly, instead, noisy observations (x, blue) and bindings between consecutive latent variables give rise to their current state.

The remainder of the paper is structured as follows: in Section 2 we describe the problem setting. In Section 3 we
introduce the novel latent anomaly detection framework,
leading to hidden Markov anomaly detection (Section 4.2),
for which we develop an effective optimization algorithm.
Our method is evaluated on controlled artificial data and
two real-world data sets from bioinformatics and computational sustainable energy applications (Section 5). Section 6 concludes. Additional run time experiments and
mathematical proofs are presented in the supplementary
material.

2. Anomaly Detection
In anomaly detection (Chandola et al., 2009) we are given
a set of input instances x1 , . . . , xn ∈ X , which are commonly assumed to be realized from independent and identically distributed (i.i.d) random variables X1 , . . . , Xn ∼ P ,
where P is a potentially unknown measure of probability.
The aim is to find a set containing the most typical instances
under the measure P , and instances lying outside of the set
are declared as anomalies. The task of anomaly detection
can be formally phrased within the framework of density
level set estimation (Tsybakov, 1997) as follows. Denoting
by X another i.i.d. copy according to P , the theoretically
optimal nominal set is Lν := {x ∈ X : p(x) ≥ bν } for
ν ∈]0, 1[ and bν such that P (X ∈
/ Lν ) = ν, which is
called the ν density level set and can be interpreted as follows: Lν contains the most likely inputs under the density
p, while rare or untypical data (“anomalies”) are modeled
to lie outside of Lν . The parameter ν indicates the fraction
of outliers in the model.
The aim is to compute, based on the data x1 , . . . , xn ∈ X ,
a good approximation of Lν , that is, to determine a function f : X → R giving rise to an estimated density level
set L̂ν := {x ∈ X : f (x) ≥ 0} . It is desirable that L̂ν
closely approximates the true density level set Lν , i.e., L̂ν
converges to Lν in probability, that is,
P (L̂ν \Lν ∪ Lν \L̂ν ) → 0 for n → ∞.
This implies that L̂ν has asymptotically probability mass ν,
that is, P (X ∈
/ L̂ν ) → ν for n → ∞. Classic approaches
to anomaly detection include kernel-based ones (Müller
et al., 2001) such as the one-class support vector machine
(Schölkopf et al., 2001) (OC-SVM). The OC-SVM is one
of the most prominent and successful anomaly detectors
and employs linear models fw,ρ (x) = hw, φ(x)i − ρ,

where the data is mapped into a reproducing kernel Hilbert
space (RKHS) H via a feature map φ : X → H. It subsequently separates a fraction of 1 − ν many inputs from the
origin with maximum margin:
n
1 X
2
max kwk − ρ +
ξi
(OC-SVM)
w,ρ,ξ≥0
νn i=1
s.t.

ξi ≥ −fw,ρ (xi ) ∀ i = 1, . . . , n.

However, this approach does not exploit latent dependency
structure of the data. Latent dependencies, however, are
prevalent in many real-world applications, e.g., network
intrusion detection (Rieck et al., 2010; Kloft & Laskov,
2010; Görnitz et al., 2009a;b; Görnitz et al., 2013; Kloft
et al., 2008), speech and text recognition (Joachims et al.,
2009), and gene finding (Rätsch & Sonnenburg, 2007). In
this case, the i.i.d. postulate no longer holds: having a latent dependency structure means that there are unobserved
latent variables Zi such that, only when conditioned on Zi ,
the Xi become conditionally independent. In other words,
if the latent structures Zi are unknown, the input variables
Xi are dependent.
Many real-world problems are sequential by nature, with
observations stemming from probability distributions according to a corresponding hidden state sequence. E.g. the
goal in gene finding is the segmentation of the DNA input
sequence into genic and intergenic regions where the observed sequence of nucleotides (A,C,G,T) changes its distribution whenever we enter or leave a genic region. In this
paper, we focus on problems that exhibit sequence structure where observations change their distribution according
to a corresponding latent state sequence and can be tackled with hidden Markov models (see Figure 1). We show
that exploiting latent structure directly, leads to significant
improvements over state-of-the-art methods. This problem
can be solved in a generic way for latent dependencies, as
we show in the next section.

3. Latent Anomaly Detection
In the problem setting of latent anomaly detection,
we extend the expressiveness of the model given in
Eqn. (OC-SVM) by considering models of the form
fw,ρ (x) = maxz∈Z hw, Ψ(x, z)i + δ(z) − ρ, where Ψ :
X × Z → H is a joint feature map into a reproducing
kernel Hilbert space H that corresponds to a kernel function k : (X × Z) × (X × Z) → R , and δ : Z → R
is a prior weight function of the instances z ∈ Z. This
is a principled way of approaching the encoding problem for arbitrary dependencies between x and z as it is
common in the structured output literature (Tsochantaridis
et al., 2005b). Albeit, it has been already used to encode
hidden Markov and hidden semi-Markov models (Görnitz
et al., 2011; Rätsch & Sonnenburg, 2007), it is not restricted
to those and has been applied to Markov random fields
(Nowozin & Lampert, 2010), weighted context-free grammars and taxonomies (Tsochantaridis et al., 2005b). Here,
the maximization step for the latent variable z acts as a fre-

Hidden Markov Anomaly Detection

quentist’s equivalent to marginalization in basic probability
theory (Nowozin & Lampert, 2010).
Employing the above notation, we phrase the primal optimization problem of latent anomaly detection as follows:
Problem 1 (P RIMAL LATENT ANOMALY DETECTION
OPTIMIZATION PROBLEM ). Given a monotonically nondecreasing loss function l : R → R, minimize, with respect
to w ∈ H and ρ ∈ R,

Eq. (P) ≥ max

min

α:α≥0 w∈H,ρ∈R,ξ∈Rn

−

max

α:α≥0

L(w, ρ, ξ, α) =

n


1 X
max αi νnξi − l(ξi )
νn i=1 ξi ∈R

n


X
+ min ρ − 1 +
αi
ρ∈R

i=1
n
X

 1
2
− max
αi hw, Ψ(xi , zi )i + δ(zi ) − kwk
w∈H,zi ∈Z
2
i=1

!

i=1,...,n

n
|
{z
}

1
1 X 
2
(∗)
kwk − ρ +
l ρ − max hw, Ψ(xi , z)i + δ(z) .
z∈Z
2
νn i=1
Let wα and (ziα )i=1,...,n be the maximizing ar(P)
guments in (∗).
Thus maxzi ∈Z hwα , Ψ(xi , zi )i +
α
δ(zi ) = hw , Ψ(xi , ziα )i + δ(ziα ), and, moreover,
maxzi ∈Z hw, Ψ(xi , zi )i+δ(zi ) ≥ hw, Ψ(xi , ziα )i+δ(ziα )
for all w ∈ H and i = 1, . . . , n. Hence, for all α ∈ Rn+ ,
The interpretation of the above formulation is as follows.
The loss function could be, e.g., l(t) = max(0, t), in which
n
X
 1
case the above detection method extends the one-class sup2
αi hw, Ψ(xi , ziα )i + δ(ziα ) − kwk ,
(∗) = max
port vector machine (Schölkopf et al., 2001) to the latent
w∈H
2
i=1
domain (this is extensively discussed in the upcoming SecPn
tion 4.2). Variants of this detection method can be obtained
from which it follows wα = i=1 αi Ψ(xi , ziα ), and thus
from the above general formulation by employing different
n
n

 X
1 X
loss functions, e.g., of logistic or exponential type (l(t) =
(∗) = max
αi δ(zi ).
αi αj k (xi , zi ), (xj , zj ) +
log(1 + exp(t)) and l(t) = exp(t), respectively). It is imzi ∈Z 2
i=1
i,j=1
i=1,...,n
portant to note that, when contrasted to the classical kernelbased hypothesis model fw,ρ (φ(x)) = hw, φ(x)i − ρ, the
Hence,
above detection method employs a latent hypothesis model
max
min
L(w, ρ, ξ, α)
of the form fw,ρ (x) = maxz∈Z hw, Ψ(x, z)i + δ(z) − ρ,
α:α≥0 w∈H,ρ∈R,ξ∈Rn
which allows for additional flexibility.
n


1 X
3.1. Dual Optimization Problem
= max
−
max αi νnξi − l(ξi )
α:α≥0
νn i=1 ξi ∈R
To obtain a dual representation of the Problem 1, we start
n


X
by equivalently re-writing (P) as
αi
+ min ρ − 1 +
ρ∈R

min

w∈H,ρ∈R,ξ∈Rn

1
1
2
kwk − ρ +
2
νn

n
X


l(ξi )

i=1


s.t. ξi ≥ ρ − max hw, Ψ(xi , z)i + δ(z) , ∀i
z∈Z

− max

zi ∈Z
i=1,...,n

i=1

!
n
n

 X
1 X
αi αj k (xi , zi ), (xj , zj ) +
αi δ(zi )
2 i,j=1
i=1
n

(†)

=

max
Pn

α:α≥0,

Denote, for all α ∈ Rn with α ≥ 0,1 the Lagrangian by
n
n
X
1
1 X
2
kwk − ρ +
l(ξi ) +
αi
2
νn i=1
i=1


ρ − ξi − max hw, Ψ(xi , z)i + δ(z) .

L(w, ρ, ξ, α) :=

z∈Z

By weak duality (e.g., Boyd & Vandenberghe, 2004, Chapter 5),
1
For vectors x ∈ Rn , we denote by x ≥ 0 as the componentwise inequalities xi ≥ 0, i = 1, . . . , n.


− max

zi ∈Z
i=1,...,n

i=1

−
αi =1

1 X ∗
l (αi νn)
νn i=1

!
n
n

 X
1 X
αi αj k (xi , zi ), (xj , zj ) +
αi δ(zi )
2 i,j=1
i=1

where for (†) we employ the notion of the FenchelLegendre convex conjugate function f ∗ (a)
:=
supb ha, bi − f (b) (Rifkin & Lippert, 2007) and ex2
ploit that the function w 7→ 12 k·k is self-conjugated;
 as
Pn
well as we observe that minρ∈R ρ − 1 + i=1 αi = 0
Pn
if i=1 αiP= 1 and −∞ else-wise, which enforces the
n
constraint i=1 αi = 1 when maximizing with respect
to α. Thus we obtain the following dual optimization
problem of (P).

Hidden Markov Anomaly Detection

Problem 2 (D UAL LATENT ANOMALY DETECTION OP TIMIZATION PROBLEM ). Given a monotonically nondecreasing loss function l : R → R, and denoting by the
l∗ : R → R the dual loss function, maximize,
Pn with respect
to α ∈ Rn and subject to α ≥ 0 and
i=1 αi = 1,
n

1 X
− min
αi αj k (xi , zi ), (xj , zj )
zi ∈Z
2 i,j=1
i=1,...,n
(D)
!
n
n
X
1 X ∗
+
αi δ(zi ) −
l (αi νn) .
νn i=1
i=1
Remark 1 (Dual complexity). The minimization over z ∈
Z can be expanded into slack variables, so the above dual
becomes a quadratically constrained program (QCQP) with
n · |Z| many quadratic constraints.
Remark 2 (Prediction function f (x) and estimated density
level set L̂ν ). By the above dualization the prediction function can be written as

X
n

αi k (xi , ziα ), (x, z) + δ(z) − ρ.
f (x) = max
z∈Z

i=1

where ρ can be calibrated by line search such that exactly
a fraction of 1 − ν training points satisfy f (xi ) ≥ 0. The
corresponding estimated density-level set is given by L̂ν :=
{x ∈ X : f (x) ≥ 0}.
3.2. Theoretical Analysis
For the theoretical analysis, we consider a slight variation
of latent anomaly detection,
n

1X 
l 1 − max hw, Ψ(xi , z)i + δ(z)
min
z∈Z
w∈H n
(1)
i=1
s.t.

kwk ≤ C .

For the important choice of l(t) = max(0, t) studied in
Section 4.2, the above reformulation is equivalent to the
original problem (P), in the sense for any choice of ν in
(P), there exists a choice of C > 0 in (1) such that both
problems have the same solution in the variable w. This is
shown in Supplementary Material D.
To analyze (1) theoretically, note that (1) corresponds to
performing empirical
risk minimization (ERM), fˆ :=
Pn
1
argminf ∈F n i=1 l(f (xi )) over the class F := {fw =

x 7→ 1−maxz∈Z (hw, Ψ(x, z)i+δ(z)) : kwk ≤ C} . In
the following theorem, we show that the solution of (1) has
asymptotically the same loss as the theoretically optimal
quantity f ∗ := argminf ∈F E l(f (X)).
Theorem 3 (L ATENT ANOMALY DETECTION GENERAL IZATION BOUND ). The following generalization bound
holds for the latent anomaly detection method (D.1). Let
l : R → R be a non-negative and L-Lipschitz continuous loss function. Denote A := maxz∈Z |δ(z)| and
B := maxx∈X ,z∈Z kΨ(x, z)k. With probability at least
1 −  over the draw of the sample, the generalization error
is bounded as:

1 + A + BC |Z|
√
E l(fˆ) − E l(f ∗ ) ≤ 8L
n
r
2 log(2/)
+ L(1 + A + BC)
.
n
Proof. The full proof is shown in supplemental material
D.
Remark 3. While the present analysis considers a worstcase bound that is independent of the structure of the latent
space Z, it would be interesting to analyze the bound also
for special choices of the joint feature map and discrete loss
functions. Such an analysis was presented in Mcallester &
Keshet (2011), who showed asymptotic consistency of the
update direction of a perception-like structured prediction
algorithm.
Remark 4. Note that the requirements on the loss function
are, in particular, fulfilled by the loss l(t) = max(0, t),
which is employed both by the one-class SVM and by the
proposed hidden Markov anomaly detector that is introduced in Section 4.2 below. Indeed in that case, l is nonnegative and Lipschitz continuous with constant L = 1.

4. Hidden Markov Anomaly Detection
In this section, we derive the proposed hidden Markov
anomaly detection (HMAD) methodology that is capable
of dealing with sequence data that exhibits latent state
structure. We therefore need to settle for an appropriate
loss function l and a joint feature map Ψ(x, z).
4.1. Latent One-class SVM
Setting l(t) := max(0, t), we can derive a latent version of the one-class support vector machine (OC-SVM)
(Schölkopf et al., 2001). Contrary to (Lampert & Blaschko,
2009), structures need not to be known. We derive the latent version of the OC-SVM as follows.
Problem 4 (P RIMAL LATENT OC-SVM OPTIMIZATION
PROBLEM ). Given the monotonically non-decreasing
hinge loss function l : R → R, l(t) = max(0, t), minimize, with respect to w ∈ H and ρ ∈ R,
n

1
1 X
2
kwk − ρ +
max 0, ρ
(P0 )
2
νn i=1

− max hw, Ψ(xi , z)i + δ(z) .
z∈Z

It is easy to check that the dual loss of l(t) = max(0, t) is
the function l∗ (t) = 0 if 0 ≤ t ≤ 1 and ∞ else, and thus
the corresponding dual optimization problem is as follows.
Problem 5 (D UAL LATENT ONE - CLASS SVM OPTI MIZATION PROBLEM ). Given the monotonically nondecreasing hinge loss function l : R → R, l(t) =
max(0, t), and denoting by l∗ : R → R the dual hinge loss
n
function, maximize, with
Pnrespect to α ∈ R and subject
1
to 0 ≤ α ≤ νn
and
α
=
1,
i=1 i

Hidden Markov Anomaly Detection

−

min

zi ∈ Z
i = 1, . . . , n

n

1 X
αi αj k (xi , ziα ), (xj , zjα )
2 i,j=1

−

n
X

!

(D0 )

αi δ(ziα )

i=1

4.2. Hidden Markov Anomaly Detection (HMAD)
In hidden Markov anomaly detection, we are interested in
inferring the hidden state sequence z = (z 1 , . . . , z T ) ∈ Z,
with single entries z t ∈ Y, associated with an observed
feature sequence x = (x1 , . . . , xT ), i.e., each element of
the sequence is a feature vector xt = (xtl )l=1,...,d ∈ Rd .
Hidden Markov models have been introduced as a certain
class of probability density functions P with chain-like factorization (Rabiner, 1989) and parameters w:
T
Y

P (x, z|w) = π(x1 , z 1 |w)
P (z t |z t−1 , w)P (xt |z t , w) .
t=2

(2)
Based on the corresponding log-probability and conditioned on the inputs, logP (z|x) = log π(z 1 , x1 |w) +
PT
t t−1
, w) + log P (z t |xt , w), we introduce
t=2 log P (z |z
the matching scoring function G : X × Z × H → R that
decomposes into Gtrans : Y × Y × H → R and Gem : X ×
Y × H → R:
log P (z|x) = G(x, z, w) =
T
X

Gtrans (z t , z t−1 , w) +

T
X

Gem (xt , z t , w),

(3)

t=1

t=2

such that G(x, z, w) ∝ hw, Ψ(x, z)i. This motivates defining a joint feature map as follows:
Definition 1 (H IDDEN M ARKOV JOINT FEATURE MAP).
Given a feature map φ : X → F, define the Hidden
Markov joint feature map Ψ : X × Z → H as
 PT

( t=2 1[z t = i ∧ z t−1 = j])i,j∈Y ,
PT
Ψ(x, z) =
.
( t=1 1[z t = i] φ(xt ))i∈Y
To better understand the above feature map, observe that
the weight vector w = (wem , wtrans ) decomposes into a
transition vector wtrans = (wtrans
i,j )i,j∈Y and an emission
vector wem = (wem
)
,
so
the
linear model becomes
i i∈Y
T X
X
hw, Ψ(x, z)i =
1[z t = i ∧ z t−1 = j] wtrans
i,j
t=2 i,j∈Y

+

T X
X

t
1[z t = i]hwem
i , φ(x )i ,

t=1 i∈Y

which is reminiscent of the log probability associated with
HMMs and given by (3).
Definition 2 (H IDDEN M ARKOV ANOMALY DETECTION
(HMAD)). Hidden Markov anomaly detection (HMAD)
is defined as the latent OC-SVM (Problem 4 and 5) together
with the hidden Markov joint feature map (Definition 1).

Note that thus, because of the specific form of the joint feature map occuring in HMAD, the problem of maximizing
over the latent variables in Eqn. (P0 ) can be solved by finding the most probable state sequence of the corresponding
hidden Markov model, which can be efficiently computed
using, e.g., Viterbi’s algorithm (Rabiner, 1989).
4.3. Properties
Similar to its non-structured counterpart, the structured
one-class SVM enjoys interesting properties, as we show
below. Recall that for an input x and prediction function f
the following cases can occur:
1. f (x) > 0 (then x is strictly inside the density level
set)
2. f (x) = 0 (then x is right at the boundary of the set)
3. f (x) < 0 (then x is outside of the density level set,
i.e., x is an outlier)
The following theorem shows that the parameter ν controls
the number of outliers.
Theorem 6. The following statements hold for the structured one-class SVM and the induced decision function f :
(a) The fraction of outliers (inputs xi with f (xi ) < 0) is
upper bounded by ν.
(b) The fraction of inputs lying strictly inside the density
level set (inputs xi with f (xi ) > 0) is upper bounded
by 1 − ν.
The theorem is proven in Appendix E and shows that the
quantity ν can be interpreted as the fraction of outliers
predicted by the learning algorithm. In particular this
shows, together with theoretical analysis of Section 3.2,
that for well behaved problems (where there is no probability mass exactly on the decision region and where the
true decision boundary is contained in the hypothesis set,
e.g., via the use of universal kernels (Steinwart & Christmann, 2008)), the estimated density level set L̂ν asymptotically equals the truly underlying density level set Lν :
P (L̂ν \Lν ∪ Lν \L̂ν ) → 0 for n → ∞ .
4.4. Optimization Algorithm
A first difficulty occurring when trying to solve the optimization problem (P0 ) consists in the function g :
(w, ρ) 7→ ρ − maxz∈Z hw, Ψ(xi , z)i + δ(z)), which
is concave and thus renders the optimization problem
non-convex. However, note that any concave function
h : R → R can be decomposed into convex and
concave parts, max(0, h(x)) = max(0, −h(x)) + h(x).
Hence, putting g(w, ρ) = ρ − maxz∈Z hw, Ψ(xi , z)i +
2
1
δ(z), wecan write Eq. (P0 )
=
2 kwk − ρ +

P
n
1
i=1 max 0, −g(w, ρ) + g(w, ρ) . The above deνn
composition consists of a convex term followed by a concave term, which admits the optimization framework of DC
programming (difference of convex functions) (Tao & An,
1998). Although the function −g is not differentiable, it
admits, at any point (w0 , ρ0 ) ∈ H × R, a subdifferential

Hidden Markov Anomaly Detection

∂(w0 ,ρ0 ) g(w0 , ρ0 ) := {v ∈ H × R : g(w, ρ) − g(w0 , ρ0 )
≥ hv, (w, ρ) − (w0 , ρ0 )i , ∀(w, ρ) ∈ H × R} .
One can verify—using the sub-differentiability of the
maximum operator—that, for any z ∈ Z, the
point (Ψ(xi , z), −1) is contained in the subdifferential
∂(w0 ,ρ0 ) g(w0 , ρ0 ). Thus, we can linearly approximate,
for any z ∈ Z, via g(x) ≈ hw, Ψ(xi , z)i + δ(z) −
ρ. In the optimization algorithm we will thus construct
a sequence of variables (wt , ρt , z t ), t = 1, 2, 3, . . .,
where we use
approximation
with z chosen as z t =

 this

t−1
argmaxz∈Z w , Ψ(xi , z) + δ(z) , where wt−1 is conveniently computed by solving a regular one-class SVM
problem. The resulting optimization algorithm is described
in Algorithm 1.
Algorithm 1 Hidden Markov Anomaly Detection
input data x1 , . . . , xn
put t = 0 and initialize wt (e.g., randomly)
repeat
t:=t+1
for i = 1, . . . , n do
zit := argmaxz∈Z hwt−1 , Ψ(xi , z)i + δ(z)
(i.e. use Viterbi algorithm)
end for
let (wt , ρt ) be the optimal arguments when solving
one-class SVM with φ(xi ) := Ψ(xi , zit )
until ∀ i = 1, . . . , n : zit = zit−1
Return optimal model parameters w := wt , ρ = ρt ,
and zi := zit ∀ i = 1, . . . , N
Despite the non-convex nature of the optimization problem,
we found in our experiments that the algorithm tends to
converge often faster than the standard column-generation
approach of the supervised structured SVM (Tsochantaridis et al., 2005a), since no storage of constraints is necessary, which in turn leads to constant time and space complexity for each iteration of Algorithm 1.

5. Empirical Analysis
We conducted experiments for the scenario of label sequence learning where we have full access to the ground
truth as well as two real-world scenarios from computational biology and computational sustainability. Our interest is to assess the anomaly detection performance of our
hidden Markov anomaly detection (HMAD) method. As
baseline methods that excel in one-class classification settings, we chose one-class support vector machines (OCSVM) with appropriate kernels. For initialization, we randomly choose a vector w0 for each run of our algorithm
which is sufficient, since no initialization of structures is
needed, as those are deduced from the parameter vector.
5.1. Controlled Experiment
For the controlled experiments, we aim to gain insights into
the behavior of our method. We investigate the anomaly de-

tection performance for low to very high (up to 30%) fraction of anomalies. Furthermore, we are interested in the
anomaly detection performance for an increasing amount
of disorganization in the input sequences. Since HMAD
exploits latent structure, it is not clear how it performs
when less structure is present. Vanilla OC-SVMs does not
exploit latent dependencies and should be unaffected by
this. Additionally, we are interested in the runtime behavior
for various training set sizes.
We generated Gaussian noise sequences of length 600 with
unit variance for the nominal bulk of the data. Non-trivial
anomalies (see Fig. 3) were induced as blocks of Gaussian
noise with non-zero mean and a total, cumulative length of
120 per anomalous example. We vary either the fraction of
anomalies in the training data set or the number of blocks,
depending on the amount of structure that is modeled into
the data (see Figure 3: from 120 sub-blocks of length 1
(100% disorganization) to a single block of length 120 (0%
disorganization). We employ a binary state model consisting of 2 states and 4 possible transitions with an constant
prior δ(·). We report on the average area under the ROC
curve (AUC) for the anomaly detection performance over
50 repetitions of the experiment. Since we know the underlying ground truth we can exactly compute the Bayes
classifier,2 which in our case lies within the set of linear
classifiers, and serves as a hypothetical upper performance
bound for the maximal achievable detection performance.
We compare the detection performance of our method to
the one achieved by OC-SVMs with RBF kernels, histogram kernels, and linear kernels using l1 - and l2 -feature
normalization, and optimal kernel parameters (1.0 for the
RBF kernel, 8 for the histogram kernel, and l1 for the linear kernel). The results of the anomaly detection experiment are shown in Figure 2 (left and center). As can
be seen in the figure, our method achieves tremendously
higher detection rates than the OC-SVMs using linear or
RBF kernel, which perform similar bad as random guessing. Most competitive baseline methods are OC-SVMs
with histogram kernels and optimal bin size (8 bins). There
exists a strong relation between our method HMAD and
Fisher kernels (Jebara et al., 2004) in the sense, that the
same representation is used. Unlike Fisher kernels, our
methodology includes the parameter optimization procedure, and therefore, given the same model parameters both
methods are on par. For a more detailed comparison we
refer the reader to Appendix B. Remarkably, our method
achieves stable on-par performance with the Bayes classifier for all levels of disorganization, even when there is no
structure to be exploited in the data (see Figure 2 center)
and outperforms significantly all competitors for varying
fraction of anomalies (see Figure 2 left).
2
For data that is i.i.d. realized from a distribution (which is
the case in our synthetic experiment), the Bayes classifier is defined as the classifier achieving the maximal accuracy among all
measurable functions.

0.8

0.6

0.4

0.2

2.5% 5%

10%

15%

20%

30%

3

10

1.0

0.8

Time in [sec]

1.0

Detection accuracy [in AUC]

Detection accuracy [in AUC]

Hidden Markov Anomaly Detection

0.6

0.4

0

10

-2

10

Bayes (Linear)
HMAD
OC-SVM (RBF 1.0)
OC-SVM (Hist 8)
OC-SVM (Linear)

-3

10

0.2
-5

10
0%

Percentage of anomalous data

2%

5%

10%

20%

40%60% 100%

100 200

Percentage of disorganization

400

600

800

1000

Number of training examples

Percentage of disorganization

Figure 2. Results for the controlled experiment: (left) anomaly detection performance for various fractions of anomalies in the training
set, (center) anomaly detection performance for increasing amount of disorganization, and (right) runtime behavior. All settings show
results for our hidden Markov anomaly detection (HMAD) as well as a set of competitors (using optimal kernel parameters). Noticeable,
the detection performance of HMAD is not affected by increasing amounts of disorganization in the input data (center).
Noisy observations
True state sequence
0%

100%

to RNA and then translated into a protein. Since genes
are separated from one another by intergenic regions, the
problem of identifying genes can be posed as a label sequence learning task, were one assigns a label (out of intergenic, start, stop, exonic) to each position in the genome
(Schweikert et al., 2009).
Intergenic

0

300

Start

Exonic

Exemplary, we depict two typical anomalous observation
sequences of length 600 and anomalous block length 120
of the experiment in Fig. 3 for the 0% (top) and 100% disorganization (bottom) settings. As can be seen, anomalies are
not trivially detectable. We also conducted runtime experiments (Fig. 2 right) to compare the runtime of our method
HMAD against that of the baseline methods. We used the
same two-state model as in the previous controlled experiment, but with training set size varying from 100 to 1000
examples. We used a fraction 10% of anomalies to ensure
there is a sufficient number of anomalies in the data. As expected, absolute computational runtime is higher than for
vanilla OC-SVMs. This is due to the iterative approach
that includes Viterbi decoding of the sequences and solving a vanilla OC-SVM in each step. However, computational complexity grows with increasing number of examples comparable to OC-SVM which gives a total complexity of O(OC-SVM) + O(c), where c is a constant.
5.2. Bioinformatics Application: Procaryotic Gene
Prediction
In prokaryotes (mostly bacteria and archaea) gene structures consist of the protein coding region that starts by
a start codon (one out of three specific 3-mers in many
prokaryotes) followed by a number of codon triplets (of
three nucleotides each) and is terminated by a stop codon
(one out of five specific 3-mers in many prokaryotes) (Alberts et al., 2002). Genic regions are first transcribed

Intergenic

Ex3

600

Sequence position

Figure 3. Examples of observation sequences for two extreme
cases of our controlled experiments: even in the easy setting (top),
the true state sequence is barely visible to the naked eye in the
noisy observed sequence, while in the challenging setting (bottom) it is almost impossible for humans to extrapolate the truly
underlying state sequence.

Stop

IGE

Start

Ex2

Stop
Ex1

Figure 4. State model of prokayotic gene finding.

We downloaded the genome of the widely studied escherichia coli bacteria, which is publicly available.3 Genomic sequences were cut between neighboring genes
(splitting intergenic regions equally), such that a minimum
distance of 6 nucleotides between genes was maintained.
Intergenic regions have a minimum distance of 50 nucleotides to genic regions. Features were derived from the
nucleotide sequence by transcoding it to a numerical representation of triplets. All examples have a minimum length
of 500 nucleotides and do not exceed 1200 nucleotides.
For the OC-SVM we use matching spectrum kernels of order 1,2, and 3 (resp. 64, 4160, and 266.304 dimensions),
while the SSVM and HMAD obtain a sequence of binary
entries as input data. A description of the used state model,
which is based on Görnitz et al. (2011), is given in Figure 4. Start and stop states use corresponding features that
encode start and stop codons. Any other states is using
all 64 binary input features. Furthermore, we choose δ(z)
to have a slightly higher probability towards the intergenic
state. For a more fair comparison, OC-SVM and HMAD
are given the true fraction of anomalies which varies from
2.5% up to 30%. The training set contained 200 examples of intergenic and genic examples with a total length of
>170.000 nucleotides, while the testing set contained 350
intergenic and 50 genic examples of length >330.000 nucleotides, rending this a computationally challenging experiment. The experiment was repeated 20 times where
training and test set are drawn randomly.
3

http://www.sanger.ac.uk...
.../resources/downloads/bacteria/escherichia-coli.html

Hidden Markov Anomaly Detection

The results in Figure 5 show a vastly superior performance
of our method (HMAD) in terms of the detection accuracy:
HMAD achieves a perfect AUC of 1.00 (which means: it
exactly identifies every sequence containing a gene with
zero error) for all outlier fractions, while the classical oneclass SVM shows much worse performance with an AUC
of 0.85 at best and 0.66 in the worst case. Using higher
order spectrum kernels increases the detection performance
only marginally. This result is remarkable as it has been
reported that string kernels such as spectrum kernel achieve
state of the art performance in this application (Schweikert
et al., 2009).
Detection accuracy [in AUC]

1.0

1.0

Detection accuracy [in AUC]

We further employ a simple feature selection procedure
where the 8 most distinctive genic- and intergenic features
are selected on a comparable labeled procaryote (e. fergusonii), which increased performance for OC-SVM by more
than 10%. While performance for our HMAD remained
unchanged, training and prediction times dropped down to
15% when compared to the full model.

0.9

0.8

OC-SVM (Hist 4)
OC-SVM (Hist 8)
OC-SVM (Hist 16)
HMAD

0.7

0.6
2.5% 5%

10%

15%

20%

30%

Percentage of anomalous data

Figure 6. Detection performance for various fractions of outliers
in terms of AUC for the computational sustainability experiment.
Clearly, the accuracy of our hidden Markov anomaly detection
exceeds the vanilla one-class SVM performance with histogram
kernels albeit detection performance deteriorates with increasing
amount of outliers in the training set.

vastly outperforms all OC-SVMs, which is the previously
best known performing method on this data. Detection performances for all methods decrease with increasing amount
of anomalies.

6. Conclusion
0.9

0.8

0.7

0.6
2.5% 5%

OC-SVM Spectrum
OC-SVM Spectrum
OC-SVM Spectrum
OC-SVM Spectrum
HMAD (FS)
HMAD
10%

15%

(1)
(2)
(3)
(FS)

20%

30%

Percentage of anomalous data

Figure 5. Detection performance for various fractions of outliers
in terms of AUC for the procaryotic gene finding experiment.
Clearly, the accuracy of our hidden Markov anomaly detection
exceeds the vanilla one-class SVM performance even when using
higher order (1,2 & 3 codons = 64, 4160 and 266.304 dimensions)
spectrum kernels.

5.3. Computational Sustainability Application:
Anomalous State Detection in Wind Turbines
In this anomaly detection task, the target objective is to
discriminate between two different wind turbine states depending on the weather conditions. Such applications are
important, for example, to monitor machines for failures
or changes in underlying system state (Zaher et al., 2009).
We used the wind turbine simulator FAST (Jonkman et al.,
2005) to generate simulated sensor readings. The weather
conditions, i.e., wind speed and turbulence are modeled by
the wind turbulence simulator TurbSim (Jonkman & Buhl,
2012). We used 200 nominal and anomalous sequences of
length 800, consisting of 5 time series of sensor data each.
Nominal data consisted of a single wind speed and perturbation class setting, while the anomalous data contained a
block of differing wind speed and perturbation class. From
this data we selected half for training with various anomalous data fraction and the remaining for testing. The OCSVM employs histogram kernels with 4, 8, and 16 bins and
all methods are given the true fraction of outliers. As can
be seen in Fig. 6 the detection performance of our method

We proposed a novel methodology for latent anomaly detection on hidden Markov models, which combines ideas
from structured output learning and kernel-based anomaly
detection. Theoretical guarantees in the form of generalization error bounds underlie the proposed general latent
anomaly detection framework, which we optimized using
a DC approach. We empirically analyzed a specific instantiation of our approach, hidden Markov anomaly detection
(HMAD), on controlled artificial and real data from the domains of bioinformatics and computational sustainability.
The results show that the proposed HMAD significantly
outperforms the original one-class SVM on real output
sequence data. For gene finding, an increasingly important application where a large amount of pre-knowledge
is incorporated, we showed that we can achieve a perfect detection rate (1.00 AUC), substantially outperforming the vanilla one-class SVM (0.66 AUC at 30%). Similar, for the studied computational energy sustainability application, the proposed method achieved almost optimal
accuracy (>0.99 AUC), while the regular one-class SVM
achieved only 0.92 AUC at best.
Finally and importantly, note that our approach is neither
restricted to hidden Markov models nor to the setting of
anomaly detection; it can be extended to tree- or graphstructured joint feature maps and to clustering and dimensionality reduction (e.g., hidden Markov PCA). A principal analysis of this general framework will be presented in
forthcoming publications.

7. Acknowledgments
MK acknowledges support by the German Research Foundation through the grant KL 2698/2-1. NG was supported
by BMBF ALICE II grant 01IB15001B.

Hidden Markov Anomaly Detection

References
Alberts, B., Bray, D., Lewis, J., Raff, M., Roberts, K., and
Watson, J.D. Molecular Biology of the Cell. Garland,
4th edition, 2002.
Bartlett, P.L. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, November 2002.
Blanchard, Gilles, Lee, Gyemin, and Scott, Clayton. Semisupervised novelty detection. JMLR, 11:2973–3009,
2010.

Jonkman, B. J. and Buhl, M. L. Turbsim User’s Guide: Version 1. 50. Nat. Renew. Energy Laboratory, 2012. ISBN
9781234541484. URL http://books.google.
de/books?id=NAfOygAACAAJ.
Jonkman, J. M., Buhl, M. L., of Energy. Office of
Energy Efficiency, United States. Dept., and Energy,
Renewable. FAST User’s Guide: Updated August
2005. NREL/TP. National Renewable Energy Laboratory, 2005. URL http://books.google.de/
books?id=V_9pNwAACAAJ.

Boyd, Stephan and Vandenberghe, Lieven. Convex Optimization. Cambrigde University Press, 2004.

Jyothsna, V., Prasad, V. V. Rama, and Prasad, K. Munivara. Article: A review of anomaly based intrusion detection systems. International Journal of Computer Applications, 28(7):26–35, August 2011.

Chandola, Varun, Banerjee, Arindam, and Kumar, Vipin.
Anomaly detection: A survey. ACM Comput. Surv., 41
(3):1–58, 2009.

Kloft, M., Brefeld, U., Sonnenburg, S., and Zien, A. `p norm multiple kernel learning. Journal of Machine
Learning Research, 12:953–997, Mar 2011.

Görnitz, Nico, Kloft, Marius, and Brefeld, Ulf. Active and
semi-supervised data domain description. In Machine
Learning and Knowledge Discovery in Databases, pp.
407–422. Springer Berlin Heidelberg, 2009a.

Kloft, Marius and Laskov, Pavel. Online anomaly detection
under adversarial impact. AISTATS, pp. 405–412, 2010.

Görnitz, Nico, Kloft, Marius, Rieck, Konrad, and Brefeld,
Ulf. Active learning for network intrusion detection. In
Proceedings of the 2nd ACM workshop on Security and
artificial intelligence, pp. 47–54. ACM, 2009b.
Görnitz, Nico, Widmer, Christian, Zeller, Georg, Kahles,
Andre, Sonnenburg, Soeren, and Raetsch, Gunnar. Hierarchical multitask structured output learning for largescale sequence segmentation. In NIPS, pp. 1–10, 2011.
Görnitz, Nico, Widmer, Christian, Zeller, Georg, Kahles,
André, Sonnenburg, Sören, and Rätsch, Gunnar. Hierarchical multitask structured output learning for largescale sequence segmentation. In NIPS, 2011.
Görnitz, Nico, Kloft, Marius, Rieck, Konrad, and Brefeld,
Ulf. Toward supervised anomaly detection. Journal
of Artificial Intelligence Research (JAIR), 46:235–262,
2013.
Görnitz, Nico, Porbadnigk, Anne K, Binder, Alexander,
Sannelli, Claudia, Braun, Mikio, Müller, Klaus-Robert,
and Kloft, Marius. Learning and evaluation in presence
of non-iid label noise. In AISTATS 2014, pp. 293–302,
2014.
Jebara, Tony, Kondor, Risi, and Howard, Andrew. Probability product kernels. JMLR, 2004.
Joachims, Thorsten, Hofmann, Thomas, Yue, Yisong, and
Yu, Chun-Nam. Predicting structured objects with support vector machines. Communications of the ACM, 52
(11):97–104, 2009.

Kloft, Marius and Laskov, Pavel. Security analysis of
online centroid anomaly detection. J. Mach. Learn.
Res., 13(1):3681–3724, December 2012. ISSN 15324435. URL http://dl.acm.org/citation.
cfm?id=2503308.2503359.
Kloft, Marius, Brefeld, Ulf, Düessel, Patrick, Gehl, Christian, and Laskov, Pavel. Automatic feature selection for
anomaly detection. In Proceedings of the 1st ACM workshop on Workshop on AISec, pp. 71–76. ACM, 2008.
Kukita, Yoji, Uchida, Junji, Oba, Shigeyuki, Nishino,
Kazumi, Kumagai, Toru, Taniguchi, Kazuya, Okuyama,
Takako, Imamura, Fumio, and Kato, Kikuya. Quantitative identification of mutant alleles derived from lung
cancer in plasma cell-free dna via anomaly detection using deep sequencing data. PLoS One, 8(11):e81468,
2013.
Lampert, Christoph H. and Blaschko, Matthew B. Structured prediction by joint kernel support estimation. Machine Learning, 77(2-3):249–269, 4 2009. doi: 10.1007/
s10994-009-5111-0.
Markou, M. and Singh, S. Novelty detection: a review
– part 1: statistical approaches. Signal Processing, 83:
2481–2497, 2003.
Mcallester, David and Keshet, Joseph. Generalization
bounds and consistency for latent structural probit and
ramp loss. Nips, pp. 1–8, 2011.
Mohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar,
Ameet. Foundations of Machine Learning. The MIT
Press, 2012. ISBN 026201825X, 9780262018258.

Hidden Markov Anomaly Detection

Müller, K.-R., Mika, S., Rätsch, G., Tsuda, K., and
Schölkopf, B. An introduction to kernel-based learning algorithms. IEEE Neural Networks, 12(2):181–201,
May 2001.
Müller, K R, Mika, S, Rätsch, G, Tsuda, K, and Schölkopf,
B. An introduction to kernel-based learning algorithms.
IEEE transactions on neural networks / a publication of
the IEEE Neural Networks Council, 12(2):181–201, 1
2001. doi: 10.1109/72.914517.
Noto, Keith, Brodley, Carla E., Majidi, Saeed, Bianchi, Diana W., and Slonim, Donna K. Csax: Characterizing
systematic anomalies in expression data. In RECOMB
18, pp. 222–236, 2014.
Nowozin, Sebastian and Lampert, Christoph H. Structured
learning and prediction in computer vision. Foundations
and Trends in Computer Graphics and Vision, 6(3-4):
185–365, 2010. doi: 10.1561/0600000033.
Rabiner, Lawrence R. A tutorial on hidden markov models
and selected appications in speech recognition. Proceedings of the IEEE, 77(2), 1989.
Rätsch, Gunnar and Sonnenburg, Sören. Large scale hidden
semi-markov svms. In Schölkopf, B, Platt, J, and Hoffman, T (eds.), NIPS, pp. 1161–1168. MIT Press, 2007.
Rieck, Konrad, Krueger, Tammo, Brefeld, Ulf, and Müller,
Klaus-Robert. Approximate tree kernels. JMLR, 11:
555–580, 2010.
Rifkin, Ryan M. and Lippert, Ross A. Value regularization
and fenchel duality. JMLR, 8:441–479, 2007.
Saligrama, Venkatesh and Zhao, Manqi. Local anomaly
detection. In AISTATS 2012, pp. 969–983, 2012.
Schölkopf, B. and Smola, A.J. Learning with Kernels. MIT
Press, Cambridge, MA, 2002.
Schölkopf, B., Platt, J., Shawe-Taylor, J., Smola, A.J., and
Williamson, R.C. Estimating the support of a highdimensional distribution. Neural Computation, 13(7):
1443–1471, 2001.
Schweikert, Gabriele, Zien, Alexander, Zeller, Georg,
Behr, Jonas, Dieterich, Christoph, Ong, Cheng Soon,
Philips, Petra, De Bona, Fabio, Hartmann, Lisa, Bohlen,
Anja, et al. mgene: accurate svm-based gene finding
with an application to nematode genomes. Genome research, 2009.
Steinwart, Ingo and Christmann, Andreas. Support Vector Machines. Springer, 1st edition, 2008. ISBN
0387772413.
Tao, Pham Dinh and An, Le Thi Hoai. A DC optimization
algorithm for solving the trust-region subproblem. SIAM
Journal on Optimization, 8(2):476–505, 1998.

Tax, David M.J. and Duin, Robert P.W. Support vector data
description. Machine Learning, 54:45–66, 2004.
Tsochantaridis, Ioannis, Joachims, Thorsten, Hofmann,
Thomas, and Altun, Yasemin. Large margin methods for structured and interdependent output variables.
J. Mach. Learn. Res., 6:1453–1484, December 2005a.
ISSN 1532-4435.
URL http://dl.acm.org/
citation.cfm?id=1046920.1088722.
Tsochantaridis, Ioannis, Joachims, Thorsten, Hofmann,
Thomas, and Altun, Yasemin. Large margin methods for
structured and interdependent output variables. Journal
of Machine Learning Research, 6:1453–1484, 2005b.
Tsybakov, A.B. On nonparametric estimation of density
level sets. Annals of Statistics, 25:948–969, 1997.
Zaher, ASAE, McArthur, SDJ, Infield, DG, and Patel, Y.
Online wind turbine fault detection through automated
scada data analysis. Wind Energy, 12(6):574–593, 2009.

