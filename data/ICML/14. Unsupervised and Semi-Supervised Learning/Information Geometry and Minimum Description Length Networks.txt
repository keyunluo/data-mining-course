Information Geometry and Minimum Description Length Networks

Ke Sun
SUNK . EDU @ GMAIL . COM
Viper Group, Computer Vision and Multimedia Laboratory, University of Geneva, Switzerland
Jun Wang
Expedia, Switzerland

JWANG 1@ EXPEDIA . COM

Alexandros Kalousis
A LEXANDROS .K ALOUSIS @ HESGE . CH
Business Informatics Department, University of Applied Sciences, Western Switzerland
Department of Computer Science, University of Geneva, Switzerland
Stéphane Marchand-Maillet
S TEPHANE .M ARCHAND -M AILLET @ UNIGE . CH
Viper Group, Computer Vision and Multimedia Laboratory, University of Geneva, Switzerland

Abstract
We study parametric unsupervised mixture learning. We measure the loss of intrinsic information from the observations to complex mixture
models, and then to simple mixture models. We
present a geometric picture, where all these representations are regarded as free points in the
space of probability distributions. Based on minimum description length, we derive a simple geometric principle to learn all these models together. We present a new learning machine with
theories, algorithms, and simulations.

1. Introduction
Knowledge is often gradually perceived from simple to
complex. In the realm of mixture modeling (Hinton et al.,
1995; Rasmussen, 2000; Vincent & Bengio, 2003), a simple representation with a few components and a complex
representation with many components are both meaningful in the learning path. For example, the point cloud in
fig. 1 (a) can either be perceived as 3 blobs, or as 9 blobs,
which are both “correct”.
A stack of mixture models, or clustering schemes, with
a different number of components at each layer can be
learned (Ward, 1963; Heller & Ghahramani, 2005; Goldberger & Roweis, 2005; Garcia et al., 2010; Liu et al., 2012;
Schwander & Nielsen, 2012; Krishnamurthy et al., 2012).
Often, such learning is split into stages, and the learners
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

(a)

(b)

(c)

(d)

Figure 1. (a) a point cloud; (b) a simple perception (colors and
circles denote mixture components); (c) a complex perception;
(d) a stacked mixture model.

at different stages are not fully communicating with each
other. For example, while successively merging small clusters (Ward, 1963), these clusters are often not re-adjusted
based on the latter-learned hierarchy. As another example,
consider a stage-À learner for complex mixture modeling
using many components, and a stage-Á learner for simplification (Goldberger & Roweis, 2005; Schwander & Nielsen,
2012). The communication from À to Á is unidirectional.
What are the intrinsic principles of stacked mixture learning, and how to implement full communication within this
stack, so that different representation layers help each other
to learn, are still open problems.
This paper studies such a stacked mixture model as in
fig. 1 (d). This model fits in a minimum description length
(MDL, Rissanen, 1978; 1989) network, where all mixture
components at different layers are regarded as free points
in the space of probability distributions. Based on a global
cost function, these points are learned so that the whole
network is compact in the geometric sense. Under this concept, we implement one specific method, where the mixture
components are Gaussian distributions. We show empirical
results on the effectiveness of the network-structured regularization.

Information Geometry and Minimum Description Length Networks

The proposed MDL networks extend to stacked mixtures
of any distribution in the exponential family. It unifies
the concepts of geometric compactness and description
conciseness. It provides new insights on the connections
among machine learning, information geometry, and MDL.
In the following, section 2 recalls useful concepts of information geometry. Sections 3 and 4 present the method and
how to implement it, respectively. Section 5 shows density
estimation simulations. Section 6 presents an analysis on
the learning theory. Section 7 concludes.

ψ ? (negative entropy)

η2
D(η1 k η2 )

ψ?

η2

η1

η

η1

D(η1 k η) + D(η k η2 ) ≤ D(η1 k η2 )

Figure 2. (left) D(η 1 k η 2 ) as a Bregman divergence. (right) An
intuitive presentation of the “gain”, to be used in section 6.

ψ ? 1 have the fundamental relationship

2. Information Geometry
This section introduces basic known facts about information geometry to make the manuscript self-contained.
Readers are referred to other materials (Amari & Nagaoka,
2000; Nielsen, 2013) for a more complete understanding.
2.1. Exponential Family Statistical Manifolds
In an exponential family S dominated by certain measure
σ(x), any probability distribution can be written in the
canonical form


p(x | θ) = exp θ T t(x) − ψ(θ) ,

(1)

where θ is the canonical parameter, t(x) is a vector of
sufficient statistics, and ψ(θ) is a strictly-convex potendef

tial function. The Hessian of ψ(θ), given by g(θ) =
∂ 2 ψ/∂θ 2 , must be positive definite and can be regarded
as a Riemannian metric (Jost, 2011) of S, known as the
Fisher Information Metric (FIM) (Rao, 1945). Using Riemannian geometry as a tool, and based on the fundamentals defined by FIM, the discipline of information geometry (Rao, 1945; Čencov, 2000; Efron, 1975; Amari & Nagaoka, 2000; Nielsen & Nock, 2009) studies the measurements and dynamics of intrinsic information on S as a Riemannian manifold.

ψ ? − η T θ + ψ = 0.

(2)

We give without proof that, g(η), the FIM in the ηcoordinates, is the Hessian of ψ ? , and is equivalent to
g(θ) under coordinate transformation. Therefore, g(η) =
∂ 2 ψ ? /∂η 2 = ∂θ/∂η, where ∂θ/∂η denotes the Jacobi
matrix of the mapping η → θ.
2.3. Divergence
By Riemannian geometry (Jost, 2011),
p the infinitesimal
distance between η and η + dη is dη T g(η)dη. The
macroscopic distance, i.e., the length of the shortest path,
between two points η 1 and η 2 on S does not have a closed
form in general. As a practical way to measure their dissimilarity, the Bregman divergence (Bregman, 1967; Nielsen
et al., 2010) induced by the function ψ ? is 2
def

D(η 1 k η 2 ) = ψ ? (η 1 ) − ψ ? (η 2 ) − θ T2 (η 1 − η 2 )
= ψ ? (η 1 ) − η T1 θ 2 + ψ(θ 2 ),

(by eq. (2))
(3)

which is illustrated by fig. 2 (left) and turns out to be
the Kullback-Leibler divergence. By a Taylor expansion of ψ ? (η) at η 2 up to the second order, we see that
D(η 1 k η 2 ) ≈ (η 1 − η 2 )T g(η 2 )(η 1 − η 2 )/2 is half of the
square distance between η 1 and η 2 w. r. t. the FIM at η 2 .
This approximation is accurate when η 1 and η 2 are close
enough.

2.2. Expectation Parameters
Because of the strict convexity of ψ(θ), there is a one-todef

one mapping between
R θ and η = ∂ψ/∂θ. Differentiating both sides of p(x
R | θ)dσ(x) = 1 w. r. t. θ, by
eq. (1), we get η = p(x | θ)t(x)dσ(x). Therefore η
is called the “expectation parameter”, i.e., the expectation
of t(x). Both θ and η play the role of a global coordinate
system of S. They are connected by the Legendre transformations (Amari & Nagaoka, 2000) η = ∂ψ/∂θ and
def R
θ = ∂ψ ? /∂η, where ψ ? =
p(x | θ) ln p(x | θ)dσ(x)
is the negative entropy, the dual potential function which is
convex w. r. t. η. By eq. (1) and the definition of ψ ? , ψ and

2.4. The Gaussian Manifold
A multivariate Gaussian density with mean µ and covariance
matrix Σ can be written

 as G(x
| µ, Σ) =
(2)
(1)
(2)
T (1)
T
exp x θ + tr(θ xx ) − ψ θ , θ
, where
θ (1) = Σ−1 µ, θ (2) = −Σ−1 /2, and tr(·) is the trace.
In this paper, functions defined on S, like ψ(θ) or ψ ? (η),
and coordinate transformations of S, like η(θ) or θ(η), can be
notated without their arguments.
2
To avoid confusion, ψ ? in this paper is denoted as ϕ in the
textbook (Amari & Nagaoka, 2000), and D(· k ·) in this paper is
denoted as D(−1) (· k ·) or D? (· k ·).
1

Information Geometry and Minimum Description Length Networks

Therefore, the Gaussian manifold, i.e., the parameter space
of all such G(x | µ, Σ), is in the exponential family. Its
dimensionality is dim(S) = dim(x)(dim(x) + 3)/2. The
expectation parameters are given by η (1) = E(x) = µ
and η (2) = E(xxT ) = Σ + µµT . The Legendre
transformations θ ↔ η involve matrix inversions and can
have a cubic complexity in dim(x). All diagonal Gaussian
distributions with the same dim(x) form an embedded
sub-manifold. There, the computational complexity of the
Legendre transformations is linear in dim(x).

3. Minimum Description Length Networks
3.1. Divergence-induced Priors
For developing the proposed method, we introduce a new
tool called divergence-induced priors. Using a reference
set B = {η 1 , . . . , η m } on an exponential family S, and
usingPsome non-negative weights α = (α1 , . . . , αm ) so
m
that i=1 αi = 1, we define a distribution of a random
η ∈ S as
p(η | B, α) =

1
N (B, α)

m
X

αi exp (−D(η k η i )) , (4)

i=1

R
Pm
where N (B, α) = η∈S i=1 αi exp (−D(η k η i )) dη.
The intuition is that any η close to some η i ∈ B has a
high probability, where closeness is defined by the information divergence. Although the η-coordinates are used
here, p(η | B, α) is a density function defined on S and is
invariant to coordinate transformations. This p(η | B, α) is
an informative prior, as compared to the non-informative
prior (Jeffreys, 1946) or weakly informative priors used in
Bayesian learning (Ghahramani & Beal, 2000).
Using a divergence-induced prior, we can describe any
η ∈ S with a code. This description will be used later to
measure the cost to describe the data. We first discretize S
into tiny hyper-cubes with edge-length δ. By Riemannian
geometry
2011), the volume3 of the cube containing
p (Jost,dim
η is |g(η)|δ S . By the divergence-induced
prior, the
p
probability mass of this cube is p(η | B, α) |g(η)|δ dim S .
The optimal code length (Shannon, 1948) of η in nats is
h
i
p
− ln p(η | B, α) |g(η)|δ dim S
!
m
X
= − ln
αi exp (−D(η k η i )) + ln N (B, α)
i=1

1
− ln |g(η)| − dim S ln δ.
2

(5)

We refer the reader to a good introduction of MDL (Hansen
& Yu, 2001) for a related analysis.
In the four-part code on the right-hand-side of eq. (5), the
first term measures the novelty of η or its difference to the

prior knowledge B and α; the second term measures the
strength of the prior knowledge or its similarity with S as
a whole; the last two terms measure the description accuracy w. r. t. the discretization. Therefore, longer codes are
assigned to accurate description of new knowledge.
By approximating D(η k η i ) to a square distance4 ,
ln N (B, α) ≈ dim S ln(2π)/2. By discretizing over a coordinate system ν with more uniform |g(ν)| on different
ν ∈ S, − 12 ln |g(ν)| can be regarded as constant. Therefore, it is reasonable to use only the first novelty term in
eq. (5) for parameter learning, and regard the rest terms as
constant, because they are less sensitive to the variation of
the parameters.
We build an equivalence between a mixture model in the
observation space and a divergence-induced prior defined
on S. Given an x, its associated t(x) in the η-coordinates
is not on S but on its boundary ∂S. This is because a single observation is a deteriorated distribution with zero variance. The image of the observation space under the mapdef

ping t(·) is O = {t(x)}, It is embedded in ∂S, where
FIM degenerates (Amari et al., 2006). We can approach O
from inside S using a series of equal-entropy surfaces, or
level sets of ψ ? , with reducing entropy levels.
Proposition 1. Given B and α,
m
X
i=1

αi p(x | η i ) ∝ lim p(η̃ | B, α)

def

= p(t(x) | B, α) ,

η̃→t(x)

(6)
where the limit is taken through equal-entropy surfaces.
The left-hand-side of eq. (6) is a function in the observation space. On the right-hand-side, p(η̃ | B, α) is given by
eq. (4), constrained on a level set of ψ ? . By proposition 1,
when this level set is close enough to ∂S, these two functions become proportional. The proof is straightforward
from eqs. (1), (3) and (4). The likelihood of a sample x can
be measured by giving x a small variance, regarding it as a
point on S, and computing a divergence-induced prior.
3.2. MDL Networks
An unsupervised mixture modeling MDL Network N =
{L0 , . . . , LL } on a statistical manifold is a collection of
points (distributions) on S ∪ O, referred to as “cells” and
organized in pyramid-shaped layers, as shown in fig. 3.
Each layer Ll = {η l1 , . . . , η lnl } consists of nl cells, where
n = n0 ≥ · · · ≥ nL . The ground layer L0 = {η 0i : η 0i =
t(xi )} ⊂ O ⊂ ∂S is fixed by the input samples {xi }ni=1 .
Any other layer Ll ⊂ S (1 ≤ l ≤ L) consists of free points
3

In this paper, depending on the context, “| · |” denotes either
the determinant or the cardinality of a set.
4
Analyses and proofs are in the supplementary material.

Information Geometry and Minimum Description Length Networks
boundary of S

L0 ⊂ O
L1
L2

S

Figure 3. An MDL network. The black dots denote the cells, i.e.
points on S and O. The links mean that attraction exists between
the endpoints. The thick colored lines represent different layers.

on S, which are to be learned. The size of N is denoted as
n1 : · · · : nL with the sample size n0 omitted.
Cells of consecutive layers interrelate with each other
through pair-wise attraction forces, shown by the links in
fig. 3. Using each layer Ll (1 ≤ l ≤ L) as a reference set,
and using some mixture weights αl = (αl1 , . . . , αlnl ), we
can define a divergence-induced prior p(η | Ll , αl ). However, this “prior” is not known a priori but is to be learned
from data. The learning goal is to reduce the description
length, given by the first novelty term in eq. (5) after abandoning constants, of all the cells in N using a one-levelhigher model by minimizing
E(N , A)
=−

nl
L−1
XX
l=0 i=1



nl+1

ln 

X

Figure 4. A toy example to show how an MDL network of size 31 reduces over-fitting on a dataset of 10 samples (the stars). The
red arrows show the regularization strengths by the top-layer (the
dashed circle) upon the 3 mixture components (the solid circles).

1999), using LL to generate LL−1 , and using LL−1 to
generate LL−2 , and so on. Minimizing E(N , A) implements a maximum a posteriori (MAP) estimator. In the
sum in eq. (7), the term corresponding to l = 0 measures
the fitness of L1 to the input data in L0 . The rest terms
(l = 1, . . . , L − 1) perform network-structured regularization on L1 .
An immediate advantage of MDL networks is to avoid singular mixture components, which is a known problem of
maximum likelihood mixture learning. For example, in
fig. 4, a zig-zag pattern over-fits a “line structure”. By
adding an upper layer with one parent cell, the three mixture components are pulled toward this parent on S, and
thus can avoid the boundary ∂S, where singularity occurs.




αl+1,j exp −D(η li k η l+1,j )  ,

j=1

4. Implementations
4.1. HARDN

(7)
where A = (α1 , . . . , αL ) consists of all the mixture
weights. The learning result is a mixture model in the input
space, and a stack of higher-level mixture models.
A first justification of the cost function E(N , A) is the theory of MDL (Rissanen, 1978; 1989), or minimum message
length (Wallace & Boulton, 1968). After reasonable simplifications as discussed in subsection 3.1, E(N , A) measures the cost to gradually describe the data in a simpleto-complex manner. If a person would like to communicate the data, he or she can start from some vague common
knowledge LL , and then tell the codes of all the cells in
LL−1 , which in turn give another coding scheme to describe LL−2 , and so on.
For simplicity, the cost to describe the top-most layer LL
and the mixture weights A is not measured by E(N , A).
This is because the high-entropy LL is close to some common knowledge, e.g., uniform distribution, and the scalars
{αli } are negligible in storage compared to the often highdimensional vectors {η li }.
An MDL network corresponds to a spawning process or a
directed graphical model (Heckerman, 1995; Jordan et al.,

To tackle the log-sum terms in eq. (7), a simple way is to
relax min E(N , A) to min Ê(N , A), where Ê(N , A) is an
upper bound of E(N , A) given by

Ê(N , A) =

nl
L−1
XX
l=0 i=1


min − ln αl+1,j + D(η li k η l+1,j ) .
j

(8)

This is exactly the strategy used in “hard” assignment mixture learning (Bishop, 2006; Nielsen, 2012). The algorithm is therefore named HARDN, whose outline is given
by alg. 1. Each cell η li in the layers L0 , . . . , LL−1 is associated with a unique parent cell η l+1,j , which minimizes

− ln αl+1,j + D(η li k η l+1,j ) . Unlike fig. 3, the links
are sparse in HARDN. The algorithm alternates between updating the child-parent associations, and updating all cells
in N with these associations fixed. Its advantage is being
simple and efficient. P
The computational complexity w. r. t
L−1
the size of N is O( l=0 nl nl+1 ), which is O(n) if the
scale of L1 , . . . , LL can be neglected as compared to the
sample size n. The main memory cost is on storing N .

Information Geometry and Minimum Description Length Networks

Alg. 1: N , A = HARDN ({xi }ni=1 , n1 , . . . , nL , γ)
1

2

Fix L0 = {t(xi )}ni=1 ; randomly initialize A and
{Ll : 1 ≤ l ≤ L}; store the result into N (0) ; t ← 0;
repeat
// Establish the connections of the network N (t)

4
5

Create the link η li → η l+1,j ? ;

6

// Adjust the cells in N (t) based on the connections

for l ← 1 to L do
foreach cell η li in Ll do
η li ←
relocate(η li , pred(η li ), succ(η li ), γ);

7
8
9

::::::::::::::::::::::::::::::::

// γ is a learning rate

αli ← percentage of pred(η li ) in Ll−1 ;

10
11

12

1
2
3
4

for l ← 0 to L − 1 do
foreach cell η li in Ll do
j? ←

arg minj − ln αl+1,j + D(η li k η l+1,j ) ;

3

R
R
L
Alg. 2: relocate η, {(η L
i , wi )}, {(η i , wi )}, γ

Copy all cells (without connections) from N (t)
to N (t+1) ; set t → t + 1; optionally update γ;
until convergence;

pred(·) denotes the predecessors
succ(·) denotes the successors

5
6
7

If {wiL } and {wjR } are missing, set all of them to 1;
L
η L ← weighted arithmetic average of {(η L
i , wi )};
R
Obtain
θR
by :::::::::
Legrendre :::::::::::::
transformations;
1 , θ 2 , . . .:::
:::::::::::::::
R
R
θ ← weighted arithmetic average of {(θ R
i , wi )};
Compute
the wavy underlined term in eq. (12);
:::::::::::::::::::::::::::::::::::::
Compute gradC = ∆Tη ∂/∂η by proposition 2;
η ← η − γ∆η ;

The underlined procedures are relatively expensive.

relatively small. Storing {βlij }, a real vector of size nl+1
for each cell η li , adds a memory overhead as compared to
HARDN.
4.3. A Key Procedure
Both HARDN and SOFTN minimize a sum of divergences.
This is a key procedure to implement MDL networks. If we
see Ê(N , A) in eq. (8) or Ē(N , A, B) in eq. (9) as a function of a single cell η, while fixing all the other parameters,
the problem reduces to
min C(η),
X
X
C(η) =
wiL D(η L
wjR D(η k η R
i k η) +
j )
i

4.2. SOFTN
In analogy to “soft” v.s. “hard” assignment (Bishop, 2006),
an alternative implementation called SOFTN is to minimize
a variational upper bound (Jordan et al., 1999) of E(N , A)
given by
Ē(N , A, B)
=

nl n
L−1
l+1
XX
X
l=0 i=1 j=1

βlij

ln

βlij
αl+1,j

!
+ D(η li k η l+1,j ) , (9)



(11)

j

L
R
R
+
w. r. t. some given {(η L
i , wi )}, {(η j , wj )} ⊂ S × < .
Minimizing the first term on the right-hand-side of eq. (11)
leads η to a linear combination of {η L
This interi }.
prets the M-step in Estimation-Maximization (EM, Amari,
1995) mixture learning. Minimizing the second term alone
leads θ(η) to a linear combination of {θ R
j }. Minimizing
both terms introduces non-linearity, resulting in a weighted
symmetrized Bregman centroid, and forming a key difference
EM.
P with
P AR binary searching algorithm for the case
L
w
=
i i
j wj was given (Nielsen & Nock, 2009). We
seek more general, albeit slower, solutions.

Pnl+1 j
where B = {βlij }, ∀l, i, j, βlij ≥ 0 and j=1
βli = 1.
nl+1 
1
This βli , · · · , βli
models a random parent cell of η li in
Ll+1 . By simple derivations, a minimizer of Ē(N , A, B)
must satisfy

αl+1,j exp −D(η li k η l+1,j )
j
.
βli = Pnl+1
(10)
j=1 αl+1,j exp −D(η li k η l+1,j )

We use natural gradient (Amari, 1998), which defines
w. r. t. a cost function an intrinsic gradient flow on S,
and shows good properties in machine learning optimization (Amari et al., 2006). The natural gradient of C(η)
T
def 
is gradC = g −1 (η)∂C/∂η ∂/∂η, where ∂/∂η denotes the local velocities, i.e., tangent vectors (Jost, 2011),
along the η-coordinate curves. By subsection 2.2, g(η) =
T
∂θ/∂η. Therefore, gradC = (∂C/∂θ) ∂/∂η, leading to

The corresponding algorithm is omitted due to space limit.
Basically, it alternates among updating βlij , αl+1,j and η li .
Because updating each cell η li requires all the cells in Ll−1
and Ll+1 , the computational complexity w. r. t. the size
PL−1
of N is O( l=1 nl (nl−1 + nl+1 )), which is slower than
HARDN but still approximates to O(n) if n1 , · · · , nL are

Proposition 2.
#T
∂η
∂
R
gradC = wL (η − η L ) + wR
(θ − θ )
,
∂θ
∂η
::::::::::
"

(12)

Information Geometry and Minimum Description Length Networks

P R
P L
R
=
w , ηL
where wL =
i wi , w
P R R Rj j
P L L L
R
j wj θ j /w .
i wi η i /w , and θ =

=

The proof is straightforward from eqs. (3) and (11). Assume the model is represented and updated in the ηcoordinates during learning. The underlined term in
eq. (12) means to push-forward an increment in the θcoordinates to the η-coordinates. By eq. (3), this term can
also be written as ∂D(η k η R )/∂θ. A simpler expression
can be derived based on the choice of S. Alg. 2 gives one
gradient descent step in an iterative procedure to minimize
C(η).

5. Simulations on Gaussian MDL Networks
We implement an MDL network N , where S is the Gaussian manifold 5 . The purpose is to show how higher levels
in N help regularize a mixture model in L1 . This demonstrates a key advantage of a fully communicating hierarchy
(see section 1) over the bottom-up approach, where L1 is
learned from L0 but not from the higher levels.
MDL networks are a family of methods. How the other
family members perform, how they compare to other hierarchical modeling approaches, and how an MDL network
with descent scale and depth can be useful in real learning
tasks, are beyond the scope of this paper.
For simplicity, we fix αl (2 ≤ l ≤ L) to be uniform. Only
α1 consisting of the weights of L1 is to be learned. Each
cell η 0i in L0 is fixed to be a small spherical Gaussian
G(· | xi , I), where  = 10−3 . This means that, in practice, η 0i is placed near the boundary ∂S rather than on ∂S.
This blurring trick is known to give better empirical results.
In alg. 1, the µ’s in Ll (1 ≤ l ≤ L) are initialized (line 1)
by the k-means (Arthur & Vassilvitskii, 2007) centroids of
the µ’s in Ll−1 ; the Σ’s are either initialized by the covariance of the k-means clusters, or the global data covariance.
The learning rate γ is adjusted (line 11) online by a bold
driver (Battiti, 1989).
In alg. 2, because the µ of the weighted centroid in eq. (11)
can be solved in closed form, the lines 5 ∼ 7 are adapted to
H ← Σ(ΣR )−1 ;
µ ← wL I + wR H

−1


wL µL + wR HµR ;

Σ ← Σ + γwL (ΣL + (µL − µ)(µL − µ)T − Σ)
+ γwR (Σ − HΣ).
This is based on the same natural gradient as in proposition 2 (derivations omitted). If all cells in N are full Gaussians, alg. 2 has a cubic complexity in dim(x) due to the
5
The codes are at https://git.unige.ch/gitweb/
marchand/mdlnetworks

Table 1. Datasets used. The columns, in order, are data name,
number of samples, dimensionality, the number of instance
datasets, and the size of the MDL network.
Name # samples dim(x) # datasets size of N
faithful
272
2
105
2 : 1
Old Faithful Geyser Data
2moons
104
2
105
8 : 2 : 1
From scikit-learn library
9blobs
104
2
105
9 : 3 : 1
Synthesized data similar to fig. 1
iris
150
4
105
3 : 1
wine
178
13
105
3 : 1
Both from the UCI repository 6
digit1
7877
784
10
n1 : 1
Hand-written digits of “1” in MNIST 7

matrix inversions. If the cells are diagonal Gaussians, alg. 2
has a linear complexity in dim(x).
The methods compared, in order, are GMM — a vanilla
Gaussian mixture model by the scikit-learn machine learning library (Pedregosa et al., 2011); DP — a Dirichlet process Gaussian mixture model (Blei & Jordan, 2006) implemented by Haines (Haines & Xiang, 2014), which does
not need a pre-specified number of components; HARD1
— a flat Gaussian mixture model based on the same implementation as HARDN, with only L0 and L1 but no higher
levels; HARDN; SOFT1 — SOFTN with only L0 and L1 ;
SOFTN. Among these methods, GMM and SOFT1 are both
variations of EM with the following difference. The E-step
in SOFT1 is based on eq. (10), or the Gaussian-to-Gaussian
assignment of η 0i (with a small variance I) to η 1j . The
E-step in GMM is based on the sample-to-Gaussian assignment. A similar blurring trick is used by GMM by adding I
( = 10−3 ) to the learned covariances in each iteration.
The datasets used are listed in table 1. For each dataset, a
large number of instance datasets are generated based on
different random seeds and different splits of training and
testing data. The size of the network, given by the last column, is chosen empirically based on prior knowledge. This
is only to show the effect of regularization by N on L1 ,
under similar configurations with a flat mixture model with
only L1 . The model selection will be discussed in section 6.
Figure 5 shows the testing errors measured by the average
negative log-likelihood. For each dataset, a small training
size and a big training size are used. The results are stable
based on the large number of instance datasets. The key
observation is that, in all cases, HARDN (resp. SOFTN) performs better than HARD1 (resp. SOFT1), meaning that the
regularization by N is effective. This improvement is more
obvious on a smaller training size. HARD1 and HARDN, although being inconsistent (Bishop, 2006), can gain better
performance than SOFT1 and SOFTN if the dataset has explicit clustering structures. HARDN is recommended for its

Information Geometry and Minimum Description Length Networks
GMM

DP

HARD1
1.3

1.05
1.00
0.95
0.90
0.85
0.80
0.75
faithful-0.050 faithful-0.100
(6.62)
(4.60)

blobs-0.005
(9.50)

blobs-0.010
(6.42)

2.5

HARDN

moons-0.005 moons-0.010
(1.49)
(0.76)

1.3

iris-0.300
(2.69)

SOFT1

iris-0.700
(1.73)

SOFTN

0.22

wine-0.300
(18.44)

wine-0.700
(3.23)

Figure 5. Average testing error over all instance datasets. The labels on the x-axis are in the format “data name–ratio of training set”
followed by “(testing error of GMM)”. The y-axis shows the testing error of all methods divided by the testing error of GMM.

6. Analysis
training and testing errors

−1400

We investigate the basic properties of an MDL network as
a parameter estimator. The following theorem shows that it
uncovers certain “truth” given enough samples.

−1450

−1500

Theorem 3. If the true distribution is a finite mixture model
with the components {η ti }, then as n → ∞, L?1 is exactly {η ti } in an optimal MDL network N ? which minimizes eq. (7).

−1550

−1600

MDL
BIC
GMM_train
GMM_test
SOFTN_train
SOFTN_test

−1650

−1700

−1750

10

20

30

40

50

60

n1 =jL1 j

Figure 6. Average training (resp. testing) errors over the training
(resp. testing) samples on digit1 against n1 (n2 is fixed to 1)
with a training : testing ratio of 1 : 9. BIC and MDL are also
normalized to be an average value over the training samples.

The structural regularization by an MDL network biases
the cells in L1 towards the cells in L2 . This strength increases as the size of L2 decreases. If L2 does not correspond to the data, the performance will go down. This
effect is reduced by MDL networks, because the whole hierarchy adapts to the data. This is different from Bayesian
learning, where some vague priors should be fixed manually.
Why a hierarchy leads to a better description as compared
to a flat model? It is clear from eq. (7) that
nl
L−1
XX

good performance and its simplicity. GMM performs a bit
worse than SOFT1 due to the implementation difference as
discussed earlier. The advantage of DP is that, it does not
need to be told a “correct” number of components, and the
results are more stable across different configurations. In
several cases with small training sizes, it performs significantly better. When the training size scales up, it does not
catch up with the other methods, because it has less information and it is biased by its priors.
The dataset digit1 has a large dimensionality and an unknown number of clusters. To reduce the model flexibility, we constrain the network cells to be diagonal Guassians. Figure 6 shows the training and testing errors by
GMM and SOFTN against n1 , the size of L1 . The testing
errors of SOFTN are consistently smaller than GMM. When
n1 is large, SOFTN is much less affected by over-fitting as
compared to GMM. The effective regularization means less
dependence on choosing a “correct” model scale.

l=0 i=1

min D(η li k η l+1,j ) ≤ E(N , A)
j

≤

nl n
L−1
l+1
XX
X

αl+1,j D(η li k η l+1,j ).

(13)

l=0 i=1 j=1

By eqs. (7) to (9) and (13), E(N , A) is essentially a nonlinear integration of the divergences from each η li to a set
Ll+1 , by converting those divergences to similarities on S
and then converting back. Then, how a “routing network”
helps save the total divergence from L0 to LL ?
Horizontally, a representation layer Ll could have clustering structures on S. If {η li } ⊂ Ll are clustered around a
centroid η̄, it is more economical to route from {η li } to η̄,
then from η̄ to higher layers. It “saves words” to describe
the common η̄ first, then describe the difference of each
individual η li .
Vertically, it is a fundamental property of information divergence to favor more representation layers. Consider a

Information Geometry and Minimum Description Length Networks

simplified scenario to route from η 1 ∈ S to η 2 ∈ S.

7. Remarks

Theorem 4. ∀η 1 , η 2 ∈ S, η 1 6= η 2 , then ¬ ∃η ∈ S, s.t.
D(η 1 k η) + D(η k η 2 ) < D(η 1 k η 2 ); ­ ∃η ∈ S, s.t.

We propose a novel approach to learn a stacked mixture
model. We picture this model as a pyramid-shaped network
on a statistical manifold. This network is learned to be tight
in the sense of information divergence. This learning is not
gradual, layer-by-layer, but at once, through minimizing a
global cost function. This fully communicating stack distinguishes from traditional hierarchical learning in letting
adjacent layers to regularize each other.

def

gain(η) = D(η 1 k η 2 ) − D(η 1 k η) − D(η k η 2 )
≥ max{D(η lc k η 1 ) + D(η 1 k η lc ),
D(η rc k η 2 ) + D(η 2 k η rc )},

(14)

where η lc in the θ-coordinates is θ lc = (θ 1 + θ 2 )/2, and
η rc = (η 1 + η 2 )/2.
Example Consider a Bernoulli distribution p(x = 1) =
η = exp θ/(1 + exp θ). Let η1 = 0.1, η2 = 0.5. Then
0.5
ηrc = 0.3. By theorem 4, ∃η, s. t. gain(η) ≥ 0.5 ln 0.3
+
0.3
0.7
0.5
0.5 ln 0.7 + 0.3 ln 0.5 + 0.7 ln 0.5 ≈ 0.17.
The triangle inequality of information divergence was
known to be not satisfied (Amari & Nagaoka, 2000; Nielsen
& Nock, 2009). Theorem 4 is not new in information geometry but presents new meanings in machine learning.
By À, one can always gain a smaller sum of divergence
through intermediate stops, as shown in fig. 2 (right). By
Á, if η 1 and η 2 are distant, this gain can be large. The same
principle holds in an MDL network, or other layered statistical models, where the cost in measured by divergence. Intuitively, a stage-wise incremental description is better than
a one-step description, because it costs less divergence on
S.
With respect to some given data, E(N , A) decreases as the
size of N increases. However, adding a new cell to N
involves a cost given by the second to forth terms on the
right-hand-side of eq. (5). They are regarded as constant
during parameter learning, but has to be considered in postlearning model selection. Within these terms, only the forth
term − dim S ln δ scales with the number of observations
of η, which is explained as follows.
By Rissanen’s proposition (Rissanen, 1989), it is reasonable to discretize
a parameter space S up to a precision
√
δ ∝ 1/ n, where n is the sample size. A justification (Hansen & Yu, 2001) is that, the
√ error magnitude in
parameter estimation scales with 1/ n by Cramér-Rao’s
bound (1946). By regarding Ll , l = 0, . . . , L − 1,
as the samples of p(η | Ll+1 , αl+1 ), we get a criterion
PL
S
MDL = E(N , A) + dim
l=1 nl ln nl−1 , which cor2
rects E(N , A) by considering the constant terms in eq. (5).
This criterion is similar to Bayesian Information Criterion
(BIC, Schwarz, 1978) or the two-stage MDL (Hansen &
Yu, 2001), except that it measures a stacked representation
as a whole. In Figure 6, both MDL and BIC select a 10component mixture model, which achieves a relatively low
testing error with a small model size.

On the intersection of machine learning and information geometry (Banerjee et al., 2005; Garcia et al., 2010;
Schwander & Nielsen, 2012; Nielsen, 2012; Liu et al.,
2012), a novel step is using symmetrized Bregman centroids (Nielsen & Nock, 2009) as basic learning units,
which communicate with both higher-level and lower-level
units. As compared to EM seeking information geometric
compactness in a two-body system (Amari, 1995), MDL
networks seek such compactness in a multi-body system.
On the intersection of information geometry and
MDL (Balasubramanian, 1997; Myung et al., 2000),
a unified view is demonstrated between the concepts of a
small divergence and a short description.
Bayesian mixture learning (Blei & Jordan, 2006; Ghahramani & Beal, 2000) and MDL networks both learn an intermediate representation between the priors and the observations. The former is based on Bayes’ rule. The latter is
based on information geometric quantities. MDL networks
do not need heavy integrations as in Bayesian methods, and
are faster in theory and in our practice. The basic cells are
all same-type distributions in a common space S. This is
simpler to understand and easier to implement.
MDL networks are not neural networks, e.g. (Salakhutdinov & Hinton, 2009), where the non-linearity among a
set of neurons is explicitly formalized. Instead, the nonlinearity among the cells is implicitly introduced by information divergence. This concept could be interesting to
architect learning machines.
The proposed theory is extensible. Different choices of S,
different divergence measures, and different network structures, lead to a pool of learning methods.

Acknowledgments
This work has been partly supported by the Swiss Secretariat for Research and Education (SERI) under grant
C11.0043 for participation in the COST Action MUMIA.

References
Amari, S. Information geometry of the EM and em algorithms for neural networks. Neural Networks, 8(9):

Information Geometry and Minimum Description Length Networks

1379–1408, 1995.
Amari, S. Natural gradient works efficiently in learning.
Neural Comput., 10(2):251–276, 1998.
Amari, S. and Nagaoka, H. Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs. AMS and OUP, 2000. (Published in Japanese in
1993).
Amari, S., Park, H., and Ozeki, T. Singularities affect dynamics of learning in neuromanifolds. Neural Comput.,
18(5):1007–1065, 2006.
Arthur, D. and Vassilvitskii, S. k-means++: The advantages of careful seeding. In 18th Annual ACMSIAM Symposium on Discrete Algorithms, pp. 1027–
1035, 2007.
Balasubramanian, V. Statistical inference, Occam’s razor,
and statistical mechanics on the space of probability distributions. Neural Comput., 9(2):349–368, 1997.
Banerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J.
Clustering with Bregman divergences. JMLR, 6(Oct):
1705–1749, 2005.
Battiti, R. Accelerated backpropagation learning: Two optimization methods. Complex systems, 3(4):331–342,
1989.
Bishop, C. M. Pattern Recognition and Machine Learning.
Information Science and Statistics. Springer-Verlag New
York, Inc., 2006.
Blei, D. M. and Jordan, M. I. Variational inference for
Dirichlet process mixtures. Bayesian Anal., 1(1):121–
143, 2006.
Bregman, L. M. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR Comput. Math. Math. Phys., 7(3):200–217, 1967.
Čencov, N. N. Statistical Decision Rules and Optimal
Inference, volume 53 of Translations of Mathematical
Monographs. AMS, reprint edition, 2000. (Published in
Russian in 1972).

Ghahramani, Z. and Beal, M. J. Variational inference for
Bayesian mixtures of factor analysers. In NIPS 12, pp.
449–455. MIT Press, 2000.
Goldberger, J. and Roweis, S. Hierarchical clustering of a
mixture model. In NIPS 17, pp. 505–512. MIT Press,
2005.
Haines, T. S. F. and Xiang, T. Background subtraction with
Dirichlet processes. IEEE Trans. Pattern Anal. Mach.
Intell., 36(4):670–683, 2014.
Hansen, M. H. and Yu, B. Model selection and the principle
of minimum description length. JASA, 96(454):746–774,
2001.
Heckerman, D. A tutorial on learning with Bayesian networks. Technical Report MSR-TR-95-06, Microsoft Research, 1995.
Heller, K. A. and Ghahramani, Z. Bayesian hierarchical
clustering. In ICML, pp. 297–304, 2005.
Hinton, G. E., Revow, M., and Dayan, P. Recognizing
handwritten digits using mixtures of linear models. In
NIPS 7, pp. 1015–1022. MIT Press, 1995.
Jeffreys, H. An invariant form for the prior probability in
estimation problems. Proc. R. Soc. A, 186(1007):453–
461, 1946.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul,
L. K. An introduction to variational methods for graphical models. Mach. Learn., 37(2):183–233, 1999.
Jost, J. Riemannian Geometry and Geometric Analysis.
Universitext. Springer, 6th edition, 2011.
Krishnamurthy, A., Balakrishnan, S., Xu, M., and Singh,
A. Efficient active algorithms for hierarchical clustering.
In ICML, pp. 887–894, 2012.
Liu, M., Vemuri, B. C., Amari, S., and Nielsen, F. Shape
retrieval using hierarchical total Bregman soft clustering.
IEEE Trans. Pattern Anal. Mach. Intell., 34(12):2407–
2419, 2012.
Myung, J., Balasubramanian, V., and Pitt, M. A. Counting probability distributions: differential geometry and
model selection. PNAS, 97(21):11170–11175, 2000.

Cramér, H. Mathematical Methods of Statistics. Princeton
Univ. Press., 1946.

Nielsen, F. k-MLE: A fast algorithm for learning statistical
mixture models. CoRR, abs/1203.5181, 2012.

Efron, B. Defining the curvature of a statistical problem
(with applications to second order efficiency). Ann. Stat.,
3(6):1189–1242, 1975.

Nielsen, F. Cramer-Rao lower bound and information geometry. CoRR, abs/1301.3578, 2013.

Garcia, V., Nielsen, F., and Nock, R. Hierarchical Gaussian
mixture model. In ICASSP, pp. 4070–4073, 2010.

Nielsen, F. and Nock, R. Sided and symmetrized Bregman
centroids. IEEE Trans. Inf. Theory, 55(6):2882–2904,
2009.

Information Geometry and Minimum Description Length Networks

Nielsen, F., Boissonnat, J. D., and Nock, R. Bregman
Voronoi diagrams: Properties, algorithms and applications. Discrete Comput. Geom., 44(2):281–307, 2010.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. JMLR, 12:
2825–2830, 2011.
Rao, C. R. Information and accuracy attainable in the estimation of statistical parameters. Bull. Cal. Math. Soc.,
37(3):81–91, 1945.
Rasmussen, C. E. The infinite Gaussian mixture model. In
NIPS 12, pp. 554–560. MIT Press, 2000.
Rissanen, J. Modeling by shortest data description. Automatica, 14(5):465–471, 1978.
Rissanen, J. Stochastic Complexity in Statistical Inquiry
Theory. World Scientific Publishing, 1989.
Salakhutdinov, R. and Hinton, G. E. Deep Boltzmann machines. In AISTATS, pp. 448–455, 2009.
Schwander, O. and Nielsen, F. Learning mixtures by simplifying kernel density estimators. In Matrix Information
Geometry, pp. 403–426. Springer, 2012.
Schwarz, G. Estimating the dimension of a model. Ann.
Stat., 6(2):461–464, 1978.
Shannon, C. E. A mathematical theory of communication.
Bell System Technical Journal, 27(3):379–423, 1948.
Vincent, P. and Bengio, Y. Manifold Parzen windows. In
NIPS 15, pp. 825–832. MIT Press, 2003.
Wallace, C. S. and Boulton, D. M. An information measure
for classification. Comput. J., 11(2):185–194, 1968.
Ward, J. H. Hierarchical grouping to optimize an objective function. J. Amer. Statist. Assoc., 58(301):236–244,
1963.

