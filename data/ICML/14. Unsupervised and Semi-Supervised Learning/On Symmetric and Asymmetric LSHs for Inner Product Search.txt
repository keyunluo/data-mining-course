On Symmetric and Asymmetric LSHs for Inner Product Search

Behnam Neyshabur
Nathan Srebro
Toyota Technological Institute at Chicago, Chicago, IL 60637, USA

Abstract

is also widely used in other settings (Gionis et al., 1999;
Datar et al., 2004; Charikar, 2002). An LSH is a random
mapping h(·) from objects to a small, possibly binary, alphabet, where collision probabilities P[h(x) = h(y)] relate
to the desired notion of similarity sim(x, y). An LSH can
in turn be used to generate short hash words such that hamming distances between hash words correspond to similarity between objects. Recent studies have also explored the
power of asymmetry in LSH and binary hashing, where two
different mappings f (·), g(·) are used to approximate similarity, sim(x, y) ≈ P[h(x) = g(y)] (Neyshabur et al., 2013;
2014). Neyshabur et al. showed that even when the similarity sim(x, y) is entirely symmetric, asymmetry in the hash
may enable obtaining an LSH when a symmetric LSH is
not possible, or enable obtaining a much better LSH yielding shorter and more accurate hashes.

We consider the problem of designing locality
sensitive hashes (LSH) for inner product similarity, and of the power of asymmetric hashes in
this context. Shrivastava and Li (2014a) argue
that there is no symmetric LSH for the problem
and propose an asymmetric LSH based on different mappings for query and database points.
However, we show there does exist a simple symmetric LSH that enjoys stronger guarantees and
better empirical performance than the asymmetric LSH they suggest. We also show a variant of
the settings where asymmetry is in-fact needed,
but there a different asymmetric LSH is required.

1. Introduction
Following Shrivastava and Li (2014a), we consider the
problem of Maximum Inner Product Search (MIPS): given
a collection of “database” vectors S ⊂ Rd and a query
q ∈ Rd , find a data vector maximizing the inner product
with the query:
p = arg max q > x
x∈S

BNEYSHABUR @ TTIC . EDU
NATI @ TTIC . EDU

(1)

MIPS problems of the form (1) arise, e.g. when using
matrix-factorization based recommendation systems (Koren et al., 2009; Srebro et al., 2005; Cremonesi et al., 2010),
in multi-class prediction (Dean et al., 2013; Jain et al.,
2009) and structural SVM (Joachims, 2006; Joachims
et al., 2009) problems and in vision problems when scoring filters based on their activations (Dean et al., 2013) (see
Shrivastava and Li, 2014a, for more about MIPS). In order
to efficiently find approximate MIPS solutions, Shrivastava
and Li (2014a) suggest constructing a Locality Sensitive
Hash (LSH) for inner product “similarity”.
Locality Sensitive Hashing (Indyk and Motwani, 1998) is
a popular tool for approximate nearest neighbor search and
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

Several tree-based methods have also been proposed for
inner product search (Ram and Gray, 2012; Koenigstein
et al., 2012; Curtin et al., 2013). Shrivastava and Li (2014a)
argue that tree-based methods, such as cone trees, are
impractical in high dimensions while the performance of
LSH-based methods is in a way independent of dimension
of the data. Although the exact regimes under which LSHbased methods are superior to tree-based methods and vice
versa are not fully established yet, the goal of this paper is
to analyze different LSH methods and compare them with
each other, rather than comparing to tree-based methods,
so as to understand which LSH to use and why, in those
regimes where tree-based methods are not practical.
Considering MIPS, Shrivastava and Li (2014a) argue that
there is no symmetric LSH for inner product similarity, and
propose two distinct mappings, one of database objects and
the other for queries, which yields an asymmetric LSH for
MIPS. But the caveat is that they consider different spaces
in their positive and negative results: they show nonexistence of a symmetric LSH over the entire space Rd , but
their asymmetric LSH is only valid when queries are normalized and data vectors are bounded. Thus, they do not
actually show a situation where an asymmetric hash succeeds where a symmetric hash is not possible. In fact, in
Section 4 we show a simple symmetric LSH that is also

On Symmetric and Asymmetric LSHs for Inner Product Search

valid under the same assumptions, and it even enjoys improved theoretical guarantees and empirical performance!
This suggests that asymmetry might actually not be required nor helpful for MIPS.
Motivated by understanding the power of asymmetry, and
using this understanding to obtain the simplest and best
possible LSH for MIPS, we conduct a more careful study
of LSH for inner product similarity. A crucial issue here is
what is the space of vectors over which we would like our
LSH to be valid. First, we show that over the entire space
Rd , not only is there no symmetric LSH, but there is also no
asymmetric LSH either (Section 3). Second, as mentioned
above, when queries are normalized and data is bounded, a
symmetric LSH is possible and there is no need for asymmetry. But when queries and data vectors are bounded and
queries are not normalized, we do observe the power of
asymmetry: here, a symmetric LSH is not possible, but an
asymmetric LSH exists (Section 5).
As mentioned above, our study also yields an LSH for
MIPS, which we refer to as SIMPLE - LSH, which is not only
symmetric but also parameter-free and enjoys significantly
better theoretical and empirical compared to L 2- ALSH ( SL )
proposed by Shrivastava and Li (2014a). In the supplementary material we show that all of our theoretical observations about L 2- ALSH ( SL ) apply also to the alternative hash
SIGN - LSH ( SL ) put forth by Shrivastava and Li (2014b).
The transformation at the root of SIMPLE - LSH was also recently proposed by Bachrach et al. (2014), who used it in
a PCA-Tree data structure for speeding up the Xbox recommender system. Here, we study the transformation as
part of an LSH scheme, investigate its theoretical properties, and compare it to LS - ALSH ( SL ).

2. Locality Sensitive Hashing
A hash of a set Z of objects is a random mapping from
Z to some alphabet Γ, i.e. a distribution over functions h :
Z → Γ. The hash is sometimes thought of as a “family” of
functions, where the distribution over the family is implicit.
When studying hashes, we usually study the behavior when
comparing any two points x, y ∈ Z. However, for our
study here, it will be important for us to make different
assumptions about x and y—e.g., we will want to assume
w.l.o.g. that queries are normalized but will not be able to
make the same assumptions on database vectors. To this
end, we define what it means for a hash to be an LSH over
a pair of constrained subspaces X , Y ⊆ Z. Given a similarity function sim : Z × Z → R, such as inner product
similarity sim(x, y) = x> y, an LSH is defined as follows1 :
1

This is a formalization of the definition given by Shrivastava
and Li (2014a), which in turn is a modification of the definition
of LSH for distance functions (Indyk and Motwani, 1998), where

Definition 1 (Locality Sensitive Hashing (LSH)). A hash
is said to be a (S, cS, p1 , p2 )-LSH for a similarity function
sim over the pair of spaces X , Y ⊆ Z if for any x ∈ X
and y ∈ Y:
• if sim(x, y) ≥ S then Ph [h(x) = h(y)] ≥ p1 ,
• if sim(x, y) ≤ cS then Ph [h(x) = h(y)] ≤ p2 .
When X = Y, we say simply “over the space X ”.
Here S > 0 is a threshold of interest, and for efficient
approximate nearest neighbor search, we need p1 > p2
and c < 1. In particular, given an (S, cS, p1, p2)-LSH, a
data structure for finding S-similar objects for query points
when cS-similar objects exist in the database can be constructed in time O(nρ log n) and space O(n1+ρ ) where
log p1
ρ = log
p2 . This quantity ρ is therefore of particular interest, as we are interested in an LSH with minimum possible
ρ, and we refer to it as the hashing quality.
In Definition 1, the hash itself is still symmetric, i.e. the
same function h is applied to both x and y. The only asymmetry allowed is in the problem definition, as we allow requiring the property for differently constrained x and y.
This should be contrasted with a truly asymmetric hash,
where two different functions are used, one for each space.
Formally, an asymmetric hash for a pair of spaces X
and Y is a joint distribution over pairs of mappings (f, g),
f : X → Γ, g : Y → Γ. The asymmetric hashes we consider will be specified by a pair of deterministic mappings
P : X → Z and Q : Y → Z and a single random mapping (i.e. distribution over functions) h : Z → Γ, where
f (x) = h(P (x)) and g(y) = h(Q(y)). Given a similarity
function sim : X × Y → R we define:
Definition 2 (Asymmetric Locality Sensitive Hashing (ALSH)). An asymmetric hash is said to be an
(S, cS, p1 , p2 )-ALSH for a similarity function sim over
X , Y if for any x ∈ X and y ∈ Y:
• if sim(x, y) ≥ S then P(f,g) [f (x) = g(y)] ≥ p1 ,
• if sim(x, y) ≤ cS then P(f,g) [f (x) = g(y)] ≤ p2 .
Referring to either of the above definitions, we also say
that a hash is an (S, cS)-LSH (or ALSH) if there exists
p2 > p1 such that it is an (S, cS, p1 , p2 )-LSH (or ALSH).
And we say it is a universal LSH (or ALSH) if for every
S > 0, 0 < c < 1 it is an (S, cS)-LSH (or ALSH).

3. No ALSH over Rd
Considering the problem of finding an LSH for inner product similarity, Shrivastava and Li (2014a) first observe that
for any S > 0, 0 < c < 1, there is no symmetric (S, cS)we also allow different constraints on x and y. Even though inner
product similarity could be negative, this definition is only concerned with the positive values.

On Symmetric and Asymmetric LSHs for Inner Product Search

LSH for sim(x, y) = x> y over the entire space X = Rd ,
which prompted them to consider asymmetric hashes. In
fact, we show that asymmetry doesn’t help here, as there
also isn’t any ALSH over the entire space:

For completeness, we also include in the supplementary
material a full definition of the max-norm and margin complexity, as well as the bounds on the max-norm and margin
complexity used in the proof above.

Theorem 3.1. For any d ≥ 2, S > 0 and 0 < c < 1 there
is no asymmetric hash that is an (S, cS)-ALSH for inner
product similarity over X = Y = Rd .

4. Maximum Inner Product Search

Proof. Assume for contradiction there exists some S >
0, 0 < c < 1 and p1 > p2 for which there exists an
(S, cS, p1 , p2 )-ALSH (f, g) for inner product similarity
over R2 (an ALSH for inner products over Rd , d > 2, is
also an ALSH for inner products over a two-dimensional
subspace, i.e. over R2 , and so it is enough to consider R2 ).
Consider the following two sequences of points:
xi = [−i, 1]
yj = [S(1 − c), S(1 − c)j + S].
For any N (to be set later), define the N × N matrix Z as
follows:

1
x>

i yj ≥ S
(2)
Z(i, j) = −1 x>
i yj ≤ cS


0
otherwise.
Because of the choice of xi and yj , the matrix Z does not
actually contain zeros, and is in-fact triangular with +1 on
and above the diagonal and −1 below it. Consider also
the matrix P ∈ RN ×N of collision probabilities P (i, j) =
P(f,g) [f (xi ) = g(xj )]. Setting θ = (p1 + p2 )/2 < 1 and
 = (p1 − p2 )/2 > 0, the ALSH property implies that for
every i, j:
Z(i, j)(P (i, j) − θ) ≥ 
(3)
or equivalently:
Z

P −θ
≥1


(4)

where  denotes element-wise (Hadamard) product. Now,
for a sign matrix Z, the margin complexity of Z is defined
as mc(Z) = inf ZX≥1 kXkmax (see Srebro and Shraibman, 2005, and also for the definition of the max-norm
kXkmax ), and we know that the margin complexity of an
N ×N triangular matrix is bounded by mc(Z) = Ω(log N )
(Forster et al., 2003), implying
k(P − θ)/kmax = Ω(log N ).

(5)

Furthermore, any collision probability matrix has maxnorm kP kmax ≤ 1 (Neyshabur et al., 2014), and shifting
the matrix by 0 < θ < 1 changes the max-norm by at
most θ, implying kP − θkmax ≤ 2, which combined with
(5) implies  = O(1/ log N ). For any  = p1 − p2 > 0,
selecting a large enough N we get a contradiction.

We saw that no LSH, nor ALSH, is possible for inner product similarity over the entire space Rd . Fortunately, this is
not required for MIPS. As pointed out by Shrivastava and
Li (2014a), we can assume the following without loss of
generality:
• The query q is normalized: Since given a vector q, the
norm kqk does not affect the argmax in (1), we can
assume kqk = 1 always.
• The database vectors are bounded inside the unit
sphere: We assume kxk ≤ 1 for all x ∈ S. Otherwise we can rescale all vectors without changing the
argmax.
We cannot, of course, assume the vectors x are normalized. This means we can limit
to	the behav
 ourd attention
 kxk ≤ 1 and Y◦ =
ior
of
the
hash
over
X
=
x
∈
R
•


	
q ∈ Rd  kqk = 1 . Indeed, Shrivastava and Li (2014a)
establish the existence of an asymmetric LSH, which we
refer to as L 2- ALSH ( SL ), over this pair of database and
query spaces. Our main result in this section is to show
that in fact there does exists a simple, parameter-free, universal, symmetric LSH, which we refer to as SIMPLE - LSH,
over X• , Y◦ . We see then that we do need to consider the
hashing property asymmetrically (with different assumptions for queries and database vectors), but the same hash
function can be used for both the database and the queries
and there is no need for two different hash functions or two
different mappings P (·) and Q(·).
But first, we review L 2- ALSH ( SL ) and note that it is not
universal—it depends on three parameters and no setting
of the parameters works for all thresholds S. We also compare our SIMPLE - LSH to L 2- ALSH ( SL ) (and to the recently
suggested SIGN - ALSH ( SL )) both in terms of the hashing
quality ρ and empirically of movie recommendation data
sets.
4.1. L2-ALSH(SL)
For an integer parameter m, and real valued parameters 0 <
U < 1 and r > 0, consider the following pair of mappings:
2

4

2m

P (x) = [U x; kU xk ; kU xk ; . . . ; kU xk
Q(y) = [y; 1/2; 1/2; . . . ; 1/2],
combined with the standard L2 hash function

 >
a x+b
2
hL
(x)
=
a,b
r

]

(6)

(7)

On Symmetric and Asymmetric LSHs for Inner Product Search

where a ∼ N (0, I) is a spherical multi-Gaussian random
vector, b ∼ U(0, r) is a uniformly distributed random variable on [0, r]. The alphabet Γ used is the integers, the intermediate space is Z = Rd+m and the asymmetric hash
L 2- ALSH ( SL ), parameterized by m, U and r, is then given
by
L2
2
(f (x), g(q)) = (hL
(8)
a,b (P (x)), ha,b (Q(q))).
Shrivastava and Li (2014a) establish2 that for any 0 < c <
1 and 0 < S < 1, there exists 0 < U < 1, r > 0, m ≥ 1,
such that L 2- ALSH ( SL ) is an (S, cS)-ALSH over X• , Y◦ .
They furthermore calculate the hashing quality ρ as a function of m, U and r, and numerically find the optimal ρ over
a grid of possible values for m, U and r, for each choice of
S, c.
Before moving on to presenting a symmetric hash for the
problem, we note that L 2- ALSH ( SL ) is not universal (as defined at the end of Section 2). That is, not only might the
optimal m, U and r depend on S, c, but in fact there is no
choice of the parameters m and U that yields an ALSH for
all S, c, or even for all ratios c for some specific threshold
S or for all thresholds S for some specific ratio c. This is
unfortunate, since in MIPS problems, the relevant threshold S is the maximal inner product maxx∈S q > x (or the
threshold inner product if we are interested in the “top-k”
hits), which typically varies with the query. It is therefore
desirable to have a single hash that works for all thresholds.
Lemma 1. For any m, U, r, and for any 0 < S < 1 and
U2

m+1

−1

m+1

(1 − S 2
)
≤ c < 1,
1−
2S
L 2- ALSH ( SL ) is not an (S, cS)-ALSH for inner product
similarity over X• = {x|kxk ≤ 1} and Y◦ = {q|kqk = 1}.
Proof. Assume for contradiction that it is an (S, cS)ALSH. For any query point q ∈ Y◦ , let x ∈ X• be a vector
s.t. q > x = S and kxk2 = 1 and let y = cSq, so that
q > y = cS. We have that:
 2

L2
p1 ≤ P hL
a,b (P (x)) = ha,b (Q(q)) = Fr (kP (x) − Q(q)k2 )
 2

L2
p2 ≥ P hL
a,b (P (y)) = ha,b (Q(q)) = Fr (kP (y) − Q(q)k2 )
where Fr (δ) is a monotonically decreasing function of δ
(Datar et al., 2004). To get a contradiction it is therefor
2
2
enough to show that kP (y) − Q(q)k ≤ kP (x) − Q(q)k .
We have:
m
2
2m+1
kP (y) − Q(q)k = 1 +
+ kyk
− 2q > y
4
m+1
m
=1+
+ (cSU )2
− 2cSU
4

using 1 −

m+1 −1

U2

m+1

(1−S 2
2S

)

≤ c < 1:

m+1
m
+ (SU )2
− 2cSU
4
m+1
m
+ U2
− 2SU
≤1+
4
2
= kP (x) − Q(q)k

<1+

Corollary 4.1. For any U, m and r, L 2- ALSH ( SL ) is not
a universal ALSH for inner product similarity over X• =
{x|kxk ≤ 1} and Y◦ = {q|kqk = 1}. Furthermore, for
any c < 1, and any choice of U, m, r there exists 0 <
S < 1 for which L 2- ALSH ( SL ) is not an (S, cS)-ALSH
over X• , Y◦ , and for any S < 1 and any choice of U, m, r
there exists 0 < c < 1 for which L 2- ALSH ( SL ) is not an
(S, cS)-ALSH over X• , Y◦ .
In the supplemental material, we show a similar nonuniversality result also for SIGN - ALSH ( SL ).
4.2. SIMPLE-LSH
We propose here a simpler, parameter-free, symmetric
LSH, which we call SIMPLE - LSH.
For x ∈ Rd , kxk ≤ 1, define P (x) ∈ Rd+1 as follows:

 q
(9)
P (x) = x; 1 − kxk22
For any x ∈ X• we have kP (x)k = 1, and for any q ∈ Y◦ ,
since kqk = 1, we have:
 >  q

P (q)> P (x) = q; 0
x; 1 − kxk22 = q > x
(10)
Now, to define the hash SIMPLE - LSH, take a spherical random vector a ∼ N (0, I) and consider the following random mapping into the binary alphabet Γ = {±1}:
ha (x) = sign(a> P (x)).

(11)

Theorem 4.2. SIMPLE - LSH given in (11) is a universal
LSH over X• , Y◦ . That is, for every 0 < S < 1 and
0 < c < 1, it is an (S, cS)-LSH over X• , Y◦ . Furthermore, it has hashing quality:


log

ρ=

1−


log

1−

cos−1 (S)
π

cos−1 (cS)
π

.

Proof. For any x ∈ X• and q ∈ Y◦ we have (Goemans and
Williamson, 1995):

2

Shrivastava and Li (2014a) have the scaling by U as a separate step, and state their hash as an (S0 , cS0 )-ALSH over {kxk ≤
U }, {kqk = 1}, where the threshold S0 = U S is also scaled by
U . This is equivalent to the presentation here which integrates the
pre-scaling step, which also scales the threshold, into the hash.

P[ha (P (q)) = ha (P (x))] = 1 −
Therefore:

cos−1 (q > x)
.
π

(12)

On Symmetric and Asymmetric LSHs for Inner Product Search

S = 0.3

1

S = 0.5

1

S = 0.7

1

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

p

*

0.8

0.2
0
1

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL)
0.8

0.6

0.2

0.4

0.2

0

S = 0.9

1

0.8

0.6

0.2
0.4

0.2

0

S = 0.99

1

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL)

0.8

0
1

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL)

0.8

0.6

0.4

0.6

0.4

0.4

0.4

0.2

0.2

0.2

p*

0.6

0

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL)

0.8

0.6

0.2

S = 0.999

1

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL)

0.8

0
1

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL)

0
1

0.8

0.6

c

0.4

0.2

0

0
1

0.8

0.6

0.4

0.2

0

0
1

0.8

c

0.6

0.4

0.2

0

c

Figure 1. The optimal hashing quality ρ∗ for different hashes (lower is better).

• if q > x ≥ S, then


cos−1 (S)
P ha (P (q)) = ha (P (x)) ≥ 1 −
π
• if q > x ≤ cS, then


cos−1 (cS)
P ha (P (q)) = ha (P (x)) ≤ 1 −
π
−1

Since for any 0 ≤ x ≤ 1, 1 − cos π (x) is a monotonically
increasing function, this gives us an LSH.
4.3. Theoretical Comparison
Earlier we discussed that an LSH with the smallest possible
hashing quality ρ is desirable. In this Section, we compare
the best achievable hashing quality and show that SIMPLE LSH allows for much better hashing quality compared to
L 2- ALSH ( SL ), as well as compared to the improved hash
SIGN - LSH ( SL ).
For L 2- ALSH ( SL ) and SIGN - ALSH ( SL ), for each desired
threshold S and ratio c, one can optimize over the parameters m and U , and for L 2- ALSH ( SL ) also r, to find the hash
with the best ρ. This is a non-convex optimization problem
and Shrivastava and Li (2014a) suggest using grid search to
find a bound on the optimal ρ. We followed the procedure,
and grid, as suggested by Shrivastava and Li (2014a)3 . For
3

We actually used a slightly tighter bound—a careful analy-

SIMPLE - LSH no parameters need to be tuned, and for each
S, c the hashing quality is given by Theorem 5.3. In Figure
1 we compare the optimal hashing quality ρ for the three
methods, for different values of S and c. It is clear that the
SIMPLE - LSH dominates the other methods.

4.4. Empirical Evaluation
We also compared the hash functions empirically, following the exact same protocol as Shrivastava and Li
(2014a), using two collaborative filtering datasets, Netflix
and Movielens 10M.
For a given user-item matrix Z, we followed the pureSVD
procedure suggested by Cremonesi et al. (2010): we first
subtracted the overall average rating from each individual rating and created the matrix Z with these averagesubtracted ratings for observed entries and zeros for unobserved entries. We then take a rank-f approximation (top f
singular components, f = 150 for Movielens and f = 300
for Netflix) Z ≈ W ΣR> = Y and define L = W Σ so that
Y = LR> . We can think of each row of L as the vector
presentation of a user and each row of R as the presentation
for an item.
The database S consists of all rows Rj of R (corresponding
to movies) and we use each row Li of L (corresponding to
sis shows the denominator
p in equation 19 of Shrivastava and Li
(2014a) can be log Fr ( 1 + m/2 − 2cSU + (cSU )2m+1 ))

On Symmetric and Asymmetric LSHs for Inner Product Search
0.4
0.3
Precision

0.5

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.4

0.2

0
0

0.6
0.4

Top 10, K = 64

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.6

0.2

0.4

0.6

0.8

1

0.2

0.2

0.4

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.2

0
0

0.4

0.6

0.8

1

0.3

0
0

0.2

0.4

0.6

0.8

1

0.2

0.8

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.4

0
0

0.4

0.6

0.8

1

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.6

0.3
0.2

0.1

0.4

Top 5, K = 128

Top 5, K = 64

0.2

0.1

0.05
0.2

0.1

0.4

0.6

0.8

1

0.06

0
0

0.2

0.4

0.6

0.8

1

0.15

0.6

0.8

1

1

0
0

0.4
0.6
Recall

0.8

0.2

0.25

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.4

0.6

0.8

1

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.2
0.15

Top 1, K = 256

Top 1, K = 512

0.05

0.2

0
0

0.1

0.05

0.8

0.4

Top 1, K = 128

0.02
0.4
0.6
Recall

0.2

0.1

Top 1, K = 64

0.2

0
0

0.15

0.1

0.04

0.2

0.2

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

Top 5, K = 512

Top 5, K = 256

0.1

0.2

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.08

0
0

Top 10, K = 512

0.2

0.5

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.15

0
0

0.4

Top 10, K = 256

Top 10, K = 128

0.2
0.1

0.25

Precision

0.8

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.3

0.1

Precision

0.8

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

1

0.05

0
0

0.2

0.4
0.6
Recall

0.8

1

0
0

0.2

0.4
0.6
Recall

0.8

1

Figure 2. Netflix: Precision-Recall curves (higher is better) of retrieving top T items by hash code of length K. SIMPLE - LSH is
parameter-free. For L 2- ALSH ( SL ), we fix the parameters m = 3, U = 0.84, r = 2.5 and for SIGN - ALSH ( SL ) we used two different settings of the parameters: m = 2, U = 0.75 and m = 3, U = 0.85.

users) as a query. That is, for each user i we would like
to find the top T movies, i.e. the T movies with highest
hLi , Rj i, for different values of T .
To do so, for each hash family, we generate hash codes of
length K, for varying lengths K, for all movies and a random selection of 60000 users (queries). For each user, we
sort movies in ascending order of hamming distance between the user hash and movie hash, breaking up ties randomly. For each of several values of T and K we calculate
precision-recall curves for recalling the top T movies, averaging the precision-recall values over the 60000 randomly
selected users.
In Figures 2 and 3, we plot precision-recall curves of retrieving top T items by hash code of length K for Netflix and Movielens datasets where T ∈ {1, 5, 10} and
K ∈ {64, 128, 256, 512}. For L 2- ALSH ( SL ) we used
m = 3, U = 0.83, r = 2.5, suggested by the authors
and used in their empirical evaluation. For SIGN - ALSH ( SL )
we used two different settings of the parameters suggested
by Shrivastava and Li (2014b): m = 2, U = 0.75 and
m = 3, U = 0.85. SIMPLE - LSH does not require any parameters.

As can be seen in the Figures, SIMPLE - LSH shows a dramatic empirical improvement over L 2- ALSH ( SL ). Following the presentation of SIMPLE - LSH and the comparison
with L 2- ALSH ( SL ), Shrivastava and Li (2014b) suggested
the modified hash SIGN - ALSH ( SL ), which is based on random projections, as is SIMPLE - LSH, but with an asymmetric transform similar to that in L 2- ALSH ( SL ). Perhaps
not surprising, SIGN - ALSH ( SL ) does indeed perform almost the same as SIMPLE - LSH (SIMPLE - LSH has only a
slight advantage on Movielens), however: (1) SIMPLE - LSH
is simpler, and uses a single symmetric lower-dimensional
transformation P (x); (2) SIMPLER - LSH is universal and
parameter free, while SIGN - ALSH ( SL ) requires tuning two
parameters (its authors suggest two different parameter settings for use). Therefor, we see no reason to prefer SIGN ALSH ( SL ) over the simpler symmetric option.

5. Unnormalized Queries
In the previous Section, we exploited asymmetry in the
MIPS problem formulation, and showed that with such
asymmetry, there is no need for the hash itself to be asymmetric. In this Section, we consider LSH for inner product

On Symmetric and Asymmetric LSHs for Inner Product Search
0.4
0.3
Precision

0.5

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.4

0.2

0
0

0.6
0.4

Top 10, K = 64

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.6

0.2

0.4

0.6

0.8

1

0.2

0.2

0.4

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.2

0
0

0.4

0.6

0.8

1

0.3

0
0

0.2

0.4

0.6

0.8

1

0.2

0.8

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.4

0
0

0.4

0.6

0.8

1

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.6

0.3
0.2

0.1

0.4

Top 5, K = 128

Top 5, K = 64

0.2

0.1

0.05
0.2

0.08

0.4

0.6

0.8

1

0
0

0.2

0.6

0.8

1

0.2

0.08

0
0

0.2

0.2

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.1

0.04

0.4

0.4

0.6

0.8

1

0.1

0.15

1

0
0

0.2

0.4
0.6
Recall

0.8

0.8

1

Top 1, K = 512

0.05

0.8

0.6

0.1
0.05

0.02

0.4
0.6
Recall

0.4

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

Top 1, K = 256

0.04

0.02

0.2

0.2

0.2

Top 1, K = 128

Top 1, K = 64

0
0
0.25

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.15

0.06

Top 5, K = 512

Top 5, K = 256

0.1

0.12

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.06

0
0

Top 10, K = 512

0.2

0.5

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.15

0
0

0.4

Top 10, K = 256

Top 10, K = 128

0.2
0.1

0.25

Precision

0.8

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

0.3

0.1

Precision

0.8

SIMPLE−LSH
L2−ALSH(SL)
SIGN−ALSH(SL),m=2
SIGN−ALSH(SL),m=3

1

0
0

0.2

0.4
0.6
Recall

0.8

1

0
0

0.2

0.4
0.6
Recall

0.8

1

Figure 3. Movielens: Precision-Recall curves (higher is better) of retrieving top T items by hash code of length K. SIMPLE - LSH is
parameter-free. For L 2- ALSH ( SL ), we fix the parameters m = 3, U = 0.84, r = 2.5 and for SIGN - ALSH ( SL ) we used two different
settings of the parameters: m = 2, U = 0.75 and m = 3, U = 0.85.

similarity in a more symmetric setting, where we assume
no normalization and only boundedness. That is, we ask
whether there is an LSH or ALSH for inner product similarity over X• = Y• = {x | kxk ≤ 1}. Beyond a theoretical
interest in the need for asymmetry in this fully symmetric
setting, the setting can also be useful if we are interested in
using sets X and Y interchangeably as query and data sets.
In user-item setting for example, one might be also interested in retrieving the top users interested in a given item
without the need to create a separate hash for this task.
We first observe that there is no symmetric LSH for
this setting. We therefore consider asymmetric hashes.
Unfortunately, we show that neither L 2- ALSH ( SL ) (nor
SIGN - ALSH ( SL )) are ALSH over X• . Instead, we propose a parameter-free asymmetric extension of SIMPLE LSH , which we call SIMPLE - ALSH , and show that it is a
universal ALSH for inner product similarity over X• .
To summarize the situation, if we consider the problem
asymmetrically, as in the previous Section, there is no need
for the hash to be asymmetric, and we can use a single hash
function. But if we insist on considering the problem symmetrically, we do indeed have to use an asymmetric hash.

5.1. No symmetric LSH
We first show we do not have a symmetric LSH:
Theorem 5.1. For any 0 < S ≤ 1 and 0 < c < 1 there is
no (S, cS)-LSH (by Definition 1) for inner product similarity over X• = Y• = {x | kxk ≤ 1}.
Proof. The same argument as in Shrivastava and Li (2014a,
Theorem 1) applies: Assume for contradiction h is an
(S, cS, p1 , p2 )-LSH (with p1 > p2 ). Let x be a vector such
that kxk = cS < 1. Let q = x ∈ X• and y = 1c x ∈ X• .
Therefore, we have q > x = cS and q > y = S. However,
since q = x, Ph (h(q) = h(x)) = 1 ≤ p2 < p1 =
Ph (h(q) = h(y)) ≤ 1 and we get a contradiction.
5.2. L2-ALSH(SL)
We might hope L 2- ALSH ( SL ) is a valid ALSH here. Unfortunately, whenever S < (c + 1)/2, and so in particular
for all S < 1/2, it is not:
Theorem 5.2. For any 0 < c < 1 and any 0 < S <
(c + 1)/2, there are no U, m and r such that L 2- ALSH ( SL )
is an (S, cS)-ALSH for inner product similarity over X• =

On Symmetric and Asymmetric LSHs for Inner Product Search

6. Conclusion

Y• = {x | kxk ≤ 1}.
Proof. Let q1 and x1 be unit vectors such that q1> x1 = S.
Let x2 be a unit vector and define q2 = cSx2 . For any U
and m:
2

m
2m+1
+ kU x2 k
− 2q2> x
4
m+1
m
= c2 S 2 +
+ U2
− 2cSU
4
m+1
m
+ U2
≤1+
− 2SU
4
2
= kP (x2 ) − Q(q2 )k

kP (x2 ) − Q(q2 )k = kq2 k +

where the inequality follows from S < (c + 1)/2. Now,
the same arguments as in Lemma 1 using monotonicity
of collision probabilities in kP (x) − Q(q)k establish LS ALSH ( SL ) is not an (S, cS)-ALSH.
In the supplementary material, we show a stronger negative
result for SIGN - ALSH ( SL ): for any S > 0 and 0 < c < 1,
there are no U, m such that SIGN - ALSH ( SL ) is an (S, cS) −
ALSH.
5.3. SIMPLE-ALSH
Fortunately, we can define a variant of SIMPLE - LSH, which
we refer to as SIMPLE - ALSH, for this more general case
where queries are not normalized. We use the pair of transformations:
 q

P (x) = x; 1 − kxk22 ; 0
(13)
q


Q(x) = x; 0; 1 − kxk22
and the random mappings f (x) = ha (P (x)), g(y) =
ha (Q(x)), where ha (z) is as in (11). It is clear that by
these definitions, we always have that for all x, y ∈ X• ,
P (x)> Q(y) = x> y and kP (x)k = kQ(y)k = 1.
Theorem 5.3. SIMPLE - ALSH is a universal ALSH over
X• = Y• = {x | kxk ≤ 1}. That is, for every 0 < S, c <
1, it is an (S, cS)-ALSH over X• , Y• .
Proof. The choice of mappings ensures that for all x, y ∈
X• we have P (x)> Q(y) = x> y and kP (x)k = kQ(y)k =
−1
>
1, and so P[ha (P (x)) = ha (Q(y))] = 1 − cos π(q x) . As
in the proof of Theorem 4.2, monotonicity of 1 −
establishes the desired ALSH properties.

cos−1 (x)
π

Shrivastava and Li (2015) also showed how a modification
of SIMPLE - ALSH can be used for searching similarity measures such as set containment and weighted Jaccard similarity.

We provide a complete characterization of when symmetric
and asymmetric LSH are possible for inner product similarity:
• Over Rd , no symmetric nor asymmetric LSH is possible.
• For the MIPS setting, with normalized queries kqk =
1 and bounded database vectors kxk ≤ 1, a universal
symmetric LSH is possible.
• When queries and database vectors are bounded but
not normalized, a symmetric LSH is not possible, but
a universal asymmetric LSH is. Here we see the power
of asymmetry.
This corrects the view of Shrivastava and Li (2014a), who
used the nonexistence of a symmetric LSH over Rd to motivate an asymmetric LSH when queries are normalized and
database vectors are bounded, even though we now see that
in these two settings there is actually no advantage to asymmetry. In the third setting, where an asymmetric hash is
indeed needed, the hashes suggested by Shrivastava and Li
(2014a;b) are not ALSH, and a different asymmetric hash
is required (which we provide). Furthermore, even in the
MIPS setting when queries are normalized (the second setting), the asymmetric hashes suggested by Shrivastava and
Li (2014a;b) are not universal and require tuning parameters specific to S, c, in contrast to SIMPLE - LSH which is
symmetric, parameter-free and universal.
It is important to emphasize that even though in the MIPS
setting an asymmetric hash, as we define here, is not
needed, an asymmetric view of the problem is required. In
particular, to use a symmetric hash, one must normalize the
queries but not the database vectors, which can legitimately
be viewed as an asymmetric operation which is part of the
hash (though then the hash would not be, strictly speaking, an ALSH). In this regard Shrivastava and Li (2014a)
do indeed successfully identify the need for an asymmetric
view of MIPS, and provide the first practical ALSH for the
problem.

References
Bachrach, Y., Finkelstein, Y., Gilad-Bachrach, R., Katzir,
L., Koenigstein, N., Nice, N., and Paquet, U. (2014).
Speeding up the Xbox recommender system using a euclidean transformation for inner-product spaces. In Proceedings of the 8th ACM Conference on Recommender
systems, pages 257–264.
Charikar, M. S. (2002). Similarity estimation techniques
from rounding algorithms. STOC.
Cremonesi, P., Koren, Y., and Turrin, R. (2010). Perfor-

On Symmetric and Asymmetric LSHs for Inner Product Search

mance of recommender algorithms on top-n recommendation tasks. In Proceedings of the fourth ACM conference on Recommender systems, ACM, page 3946.

Ram, P. and Gray, A. G. (2012). Maximum inner-product
search using cone trees. SIGKDD.

Curtin, R. R., Ram, P., and Gray, A. G. (2013). Fast exact
max-kernel search. SDM, pages 1–9.

Shrivastava, A. and Li, P. (2014a). Asymmetric LSH
(ALSH) for sublinear time maximum inner product
search (MIPS). NIPS.

Datar, M., Immorlica, N., Indyk, P., and Mirrokni, S. V.
(2004). Locality-sensitive hashing scheme based on pstable distributions. In Proc. 20th SoCG, pages 253–262.

Shrivastava, A. and Li, P. (2014b). Improved asymmetric
locality sensitive hashing (ALSH) for maximum inner
product search (MIPS). arXiv:1410.5410.

Dean, T., Ruzon, M., Segal, M., Shlens, J., Vijayanarasimhan, S., and Yagnik, J. (2013). Fast, accurate
detection of 100,000 object classes on a single machine.
CVPR.

Shrivastava, A. and Li, P. (2015). Asymmetric minwise
hashing for indexing binary inner products and set containment. WWW.

Forster, J., Schmitt, N., Simon, H., and Suttorp, T. (2003).
Estimating the optimal margins of embeddings in euclidean half spaces. Machine Learning, 51:263281.
Gionis, A., Indyk, P., and Motwani, R. (1999). Similarity
search in high dimensions via hashing. VLDB, 99:518–
529.
Goemans, M. X. and Williamson, D. P. (1995). Improved
approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42.6:1115–1145.
Indyk, P. and Motwani, R. (1998). Approximate nearest
neighbors: towards removing the curse of dimensionality. STOC, pages 604–613.
Jain, P., , and Kapoor, A. (2009). Active learning for large
multi-class problems. CVPR.
Joachims, T. (2006). Training linear SVMs in linear time.
SIGKDD.
Joachims, T., Finley, T., and Yu, C.-N. J. (2009). Cuttingplane training of structural SVMs. Machine Learning,
77.1:27–59.
Koenigstein, N., Ram, P., and Shavitt, Y. (2012). Efficient
retrieval of recommendations in a matrix factorization
framework. CIKM, pages 535–544.
Koren, Y., Bell, R., and Volinsky., C. (2009). Matrix factorization techniques for recommender systems. Computer,
42.8:30–37.
Neyshabur, B., Makarychev, Y., and Srebro, N. (2014).
Clustering, hamming embedding, generalized LSH and
the max norm. ALT.
Neyshabur, B., Yadollahpour, P., Makarychev, Y.,
Salakhutdinov, R., and Srebro, N. (2013). The power
of asymmetry in binary hashing. NIPS.

Srebro, N., Rennie, J., and Jaakkola, T. (2005). Maximum
margin matrix factorization. NIPS.
Srebro, N. and Shraibman, A. (2005). Rank, trace-norm
and max-norm. COLT.

