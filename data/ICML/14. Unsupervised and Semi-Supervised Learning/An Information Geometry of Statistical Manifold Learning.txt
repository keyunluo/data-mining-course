An Information Geometry of Statistical Manifold Learning

Ke Sun
K E .S UN @ UNIGE . CH
Stéphane Marchand-Maillet
S TEPHANE .M ARCHAND -M AILLET @ UNIGE . CH
Viper Group, Computer Vision & Multimedia Laboratory, University of Geneva, Switzerland

Abstract
Manifold learning seeks low-dimensional representations of high-dimensional data. The main
tactics have been exploring the geometry in
an input data space and an output embedding
space. We develop a manifold learning theory in a hypothesis space consisting of models.
A model means a specific instance of a collection of points, e.g., the input data collectively
or the output embedding collectively. The semiRiemannian metric of this hypothesis space is
uniquely derived in closed form based on the information geometry of probability distributions.
There, manifold learning is interpreted as a trajectory of intermediate models. The volume of a
continuous region reveals an amount of information. It can be measured to define model complexity and embedding quality. This provides
deep unified perspectives of manifold learning
theory.

Manifold learning (MAL), or non-linear dimensionality reduction, assumes that some given high-dimensional observations y 1 , . . . , y n ∈ <D lie around a low-dimensional
sub-manifold {Γ(z) : z ∈ <d } induced by a smooth mapping Γ : <d → <D (d  D). While it is possible to learn
a parametric form of Γ (Hinton & Salakhutdinov, 2006),
the majority of manifold learners are non-parametric. They
learn directly a set of low-dimensional coordinates {z i }ni=1
to preserve certain information in {y i }ni=1 .
Depending on the choice of information to be preserved,
at least two families of MAL methods thrived in the last
decade. The spectral methods (Tenenbaum et al., 2000;
Roweis & Saul, 2000; Belkin & Niyogi, 2003) and semidefinite embeddings (Weinberger et al., 2004; Sha & Saul,
2005) represent the family with natural convex formulations. They only preserve encodings of local informaProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

NOTATIONS
X n , Y n , Z n — respectively, a generic model in <D ,
an MAL input model in <D , and an MAL output model
in <d . The subscripts denote the sample size and can be
f Mz , On , H2n , H̄2
omitted; M(X), M,
X,k

X,Y ,k

Y ,Z,k,κ

— different model manifolds. The superscripts denote
the dimension. The parameters in parentheses denote
the coordinate system. Both can be omitted; S n−1 —
the (n − 1)-dimensional statistical simplex; g, G —
semi-Riemannian metric of a model family and Fisher
information metric; sX
ij — pairwise dissimilarities of
a model X. The superscript can be omitted; pj|i , pij
— neighbourhood probabilities; θ = (θ1 , . . . , θn ) —
canonical parameters of probability distributions; |M|
— volume or scale of M;

kNNi — indexes of the

k-nearest-neighbours of the i’th sample;

det(·) —

determinant; diag(x1 , . . . , x` ) — a diagonal matrix
with x1 , . . . , x` on its main diagonal.

tion on a weighted k-nearest-neighbour (kNN) graph of
{y i }. Stochastic Neighbour Embedding (SNE) (Hinton &
Roweis, 2003) and its extensions (Cook et al., 2007; Venna
& Kaski, 2007; van der Maaten & Hinton, 2008) represent
the non-convex family. They encode the input and output as
probability distributions and optimize the embedding in a
maximum-likelihood framework. By sacrificing convexity,
non-local information can be preserved as well. The latter
SNE-based family shows robustness to parameter configuration and favorable performance in data visualization. It
is being actively developed (Carreira-Perpiñán, 2010; Vladymyrov & Carreira-Perpiñán, 2012; Sun et al., 2012; Yang
et al., 2013) and stands as state-of-the-art MAL.
Despite such a diversity, several critical problems in the
field of MAL remain unclear. Practically, no standard exists in gauging the data complexity and the embedding

An Information Geometry of Statistical Manifold Learning

quality. The performance is often empirically assessed
via visualization or indirectly evaluated via classification.
Theoretically, an intrinsic MAL theory with deep connections to classical statistical learning theory (Akaike, 1974;
Schwarz, 1978; Amari, 1995; Vapnik, 1998) is not established. MAL emphasizes local information encoded into
sample-wise structures. How to describe and measure such
preservation of local information is unknown.
We attack these problems with a geometry, not in an observation space <D or an embedding space <d , but in a very
high-dimensional hypothesis space made of models.
Definition 1. A model X n is a specific instance of a set of
vectors {x1 , . . . , xn }.
Remark 1.1. By default, a model denoted by X n is a coordinate matrix (x1 , . . . , xn )T . Alternatively, it can be implicitly specified by a n × n matrix of pairwise measurements, e.g., distances or (dis-)similarities.
Definition 2. A model family M is a smooth manifold consisting of continuous models.
For example in MAL, the input Y n = (y 1 , . . . , y n )T or
the output Z n = (z 1 , . . . , z n )T is one single model. The
P
T
model family Mz = {Z n = (z 1 , . . . , z n ) :
i zi =
0; ∀i, z i ∈ <d } includes all possible embeddings centered at 0. Then, MAL can be described as a projection
Y → Z ? (Y ) ∈ Mz through convex optimization, or a
path Z 0 (Y ), Z 1 (Y ), . . . , Z ? (Y ) ∈ Mz along the gradient of some non-convex objective function.

1. Preliminaries
1.1. Manifold Learning
Given a model family M, any model X n ∈ M, representing a collection of coordinates x1 , . . . , xn , can be encoded
into n distributions over {1, 2, . . . , n}:

exp −sX
ij
 (∀j 6= i), pi|i = 0 (∀i)
pj|i (X) = P
X
6=i exp −si
(1)
or one single distribution over {1, 2, . . . , n}2 :

exp −sX
ij
 (∀j 6= i), pii = 0 (∀i).
pij (X) = P
X
ı,:ı6= exp −sı
(2)
In either case, sX
ij is a possibly non-symmetric difference
measure between xi and xj , e.g., square distance. After
normalization, p represents the probability of xi and xj
being similar. The subscript “j|i” of p in eq. (1) signifies a
conditional probability; the subscript “ij” in eq. (2) signifies a joint probability.
It is not arbitrary but natural to employ eqs. (1) and (2) for
statistical MAL, because they encode distributed local in-

formation. The information in p is distributed in a samplewise manner. Each sample xi has limited knowledge encoded into p·|i mostly regarding its neighbours.
Equations (1) and (2) are general enough to cover
SNE (Hinton & Roweis, 2003), symmetric SNE (Cook
et al., 2007), t-SNE (van der Maaten & Hinton, 2008),
and a spectrum of extensions. For example, SNE ap2
plies sX
ij = τi ||xi − xj || to eq. (1) for encoding the input and output, where τi > 0 is a scalar; t-SNE applies
2
sX
ij = log(||xi − xj || + 1) to eq. (2) for encoding the output. From a kernel view (Ham et al., 2004), any MAL technique that encodes into kernels naturally extends to such
probabilities.
Despite a model X can have various forms, after the encoding p(X) is in a unified space. In eq. (1), (pj|i (X)) is
n−1 n
a point on the product manifold
) , where S n−1 =
Pn(S
{(p1 , . . . , pn ) : ∀i, pi > 0;
i=1 pi = 1} is a statistical
simplex consisting of all distributions over {1, 2, . . . , n}. In
2
eq. (2), (pij (X)) is a point on S n −1 . Such a unified representation makes it possible to measure the difference between two models Y and Z with different original forms.
It motivates us to develop an MAL theory on the statistical
simplex regardless of the original representations.
1.2. Information Geometry
We introduce the Riemannian geometry of S n−1 , the (n −
1)-dimensional statistical simplex formed by all distributions in the form (p1 , . . . , pn ). The readers are referred to
(Jost, 2008; Amari & Nagaoka, 2000) for a thorough view.
Any (p1 , . . . , pn ) ∈ S n−1 uniquely corresponds to θ =
(θ1 , . . . , θn ) via the invertible mapping θi = log(pi /pr ),
∀i, where pr (1 ≤ r ≤ n) is a reference probability. These
canonical parameters θ serve as a global coordinate system
of S n−1 . Around ∀θ ∈ S n−1 , the partial derivative operators {∂/∂θ1 , · · · , ∂/∂θr−1 , ∂/∂θr+1 , · · · , ∂/∂θn } represent the velocities passing through θ along the coordinate
curves. An infinitesimal patch of S n−1
P around θ can be
studied as a linear space Tθ S n−1 = { i:i6=r (αi · ∂/∂θi ) :
∀i, αi ∈ <} called the tangent space. A Riemannian metric G defines a local inner product h∂/∂θi , ∂/∂θj iG(θ) on
each tangent space Tθ S n−1 and varies smoothly across
different θ. Locally, it is given by the positive definite
(p.d.) matrix Gij (θ) = h∂/∂θi , ∂/∂θj iG(θ) . Under certain conditions (Čencov, 1982), the Riemannian metric of
statistical manifolds, e.g. S n−1 , is uniquely given by the
Fisher
Pn information metric (FIM) (Rao, 1945) Gij (θ) =
k=1 pk (θ) (∂ log pk (θ)/∂θi ) (∂ log pk (θ)/∂θj ).
Lemma 3. On S n−1 , Gij (θ) = pi (θ)δij − pi (θ)pj (θ).
FIM grants us the power to measure information. With
respect to a coordinate system, e.g. the canonical pa-

An Information Geometry of Statistical Manifold Learning
f
TX M
(tangent space)

Tp(X) S

dp (differential or pushforward)
∂
∂ϕj

f
M

∂
∂ϕi

My

X

S

∂
∂θj

p

probablistic
encoding



p(X)

Y

p(Y )

∂
∂θi

p(My )

manifold learning
g
Z ? (Y )

Mz

pullback

G

p(Z ? (Y ))

statistical projection
p(Mz )

A (product of) statistical simplex S

f
A model family M

Figure 1. A geometry of statistical manifold learning.

rameters θ, the information density of a statistical model
θ ∈ pM is given by the Riemannian volume element det (G(θ)) (Jost, 2008). It means the amount
of information a single
p observation possesses with respect to θ. A small det (G(θ)) means that θ contains much uncertainty and requires many observations
to learn. Then, the information capacity of a statistical
model
family M ⊂ S n−1 is given by its volume |M| =
p
R
det (G(θ))dθ. It means the total amount of inθ∈M
formation, or the “number” of distinct models, contained
in M (Myung et al., 2000). This volume is an intrinsic
measure invariant to the choice of coordinate system.
To extend FIM to a general non-statistical manifold, e.g.,
a model family Mr parametrized by ϕ = (ϕ1 , . . . , ϕr ),
one need to construct a mapping from Mr to S n−1 . Following such a mapping, any tangent vector ∂/∂ϕi of
r
n−1
M
as ∂/∂ϕi =
Pn can be pushed forward to Tθ S
(∂θ
/∂ϕ
·
∂/∂θ
).
In
this
way,
the
inner product
k
i
k
k=1
h∂/∂ϕi , ∂/∂ϕj i can be measured with FIM and used to
define the Riemannian metric of Mr . Such a strategy is
called pullback (Jost, 2008). This is intuitively shown in
fig. 1. Mr (left) as in definition 2 mirrors the information
geometry (right) through a probabilistic encoding p.

2. An Intrinsic Geometry of MAL
The central result of this paper is summarized as follows.
Theorem 4. Consider a model family Mr = {sij (ϕ) :
ϕ = (ϕ1 , . . . , ϕr )T ∈ Φ}, where Φ ⊂ <r . The pullback
information metric with respect to Eq.(1) is given by

g(ϕ) =

" n
n
X
X
k=1

−


pl|k

l=1
n
X
l=1

pl|k

∂skl
∂ϕ
!

∂skl
∂ϕ



∂skl
∂ϕ

n
X
l=1

pl|k

T

∂skl
∂ϕ

!T 
;

the pullback metric with respect to Eq.(2) is
g(ϕ) =

n X
n
X


pkl

k=1 l=1

−

n X
n
X
k=1 l=1

∂skl
∂ϕ



∂skl
pkl
∂ϕ

!

∂skl
∂ϕ

T

n X
n
X
k=1 l=1

∂skl
pkl
∂ϕ

!T
,

where skl , pl|k and pkl vary with ϕ as in eqs. (1) and (2).
Remark 4.1. g(ϕ) is a meta-metric. Its exact form depends on sij (ϕ) and the choice between eqs. (1) and (2).
Different encodings lead to different geometries of information.
Remark 4.2. We leave the reader to verify g(ϕ) 
0. Therefore g(ϕ) is called a semi-Riemannian metric.
“Semi” means g(ϕ) is positive semi-definite (p.s.d.) rather
than p.d 1 . A model X moving rigidly forms a null
space (Jost, 2008), a model family with zero volume, meaning that such movements contribute zero information.
From theorem 4, the pullback metrics induced by eq. (1)
and eq. (2) are in similar forms. Both are some covariance
of ∂skl /∂ϕ. Such similarity agrees with previous studies where SNE and symmetric SNE show similar performance (Cook et al., 2007). Because of space limitation, the
following results are derived based on the metric induced
by eq. (1) only. The subtle difference between these two
kinds of normalizations is left to a longer version.
A natural question arises as to what kind of geometry is
fnD = {X n =
endowed to the ambient model family M
T
D
(x1 , . . . , xn ) : ∀i, xi ∈ < }. This is meaningful because a model family M of interest is often a sub-manifold
f With respect to the Euclidean coordinates, its
of M.
f an
semi-Riemannian metric g̃ defines for any X ∈ M
(nD × nD) p.s.d. matrix g̃(X). We investigate its D × D
sub-block h∂/∂xi , ∂/∂xi ig̃(X) , which can be used to measure the infinitesimal length kdxi k when xi is shifted to
1
In other contexts, a semi-Riemannian metric is also defined
to be non-degenerate with full rank (Jost, 2008)

An Information Geometry of Statistical Manifold Learning

xi + dxi while the other samples stay, or the intrinsic difference caused by such an increment dxi .
Corollary 5.


n
X
∂
∂
,
=4
pi|j (1 − pi|j )(xj − xi )(xj − xi )T
∂xi ∂xi g̃(X)
j=1
!
!T
n
n
n
X
X
X
+4
pj|i xj −
pj|i xj
xj −
pj|i xj
.
j=1

j=1

j=1

Corollary 5 reveals an interesting relationship between information geometry and data geometry. The right-hand
side is like a local covariance matrix around xi with locality defined by p. If dxi is orthogonal to the data manifold, the resulting kdxi k is small. This explains: while
the data manifold is unfolded in the sense that the samples
mainly move along the normal directions, the correspondf with small informaing model goes along a path on M
tion volume. Across different samples, Corollary 5 quantifies the potential information
in each xi with the Riemanp
nian volume element det(h∂/∂xi , ∂/∂xi ig̃(X) ). Such
sample-wise information provides theoretical quantities to
outlier identification or landmark selection in sub-sampling
procedures (van der Maaten & Hinton, 2008). Corollary 5
gives a geometric interpretation of manifold kernel density
estimation (Vincent & Bengio, 2003) as growing density to
f
maximize the information variance on M.
The target of this paper is a learning theory centered on
the above theorem 4 and supported by corollaries 5, 6
and 10 with illustrative simulations. We investigate two
independent-yet-related model families, corresponding to
information geometries of model complexity and model
quality. We do not present a systematical comparison with
other measurements. They are as many as manifold learners. None escapes the fact that it is measured in the observation space or the embedding space (Venna & Kaski,
2007; Zhang et al., 2011). In contrast, we regard all samples collectively as one point and measure information
on a differentiable manifold of such models. In the history of statistical learning, similar information geometric
theories contributed deep insights (Akaike, 1974; Amari
& Nagaoka, 2000; Myung et al., 2000; Lebanon, 2003;
Xu, 2004; Nock et al., 2011). We echo these previous
works and adapt to recent developments of MAL. Given
the uniqueness of FIM, the proposed measurements try to
estimate the true information loss in MAL. This is more
general than and fundamentally different from empirical
measurements.

3. Locally Accumulated Information
We measure the complexity of a fixed model X =
(x1 , . . . , xn )T given by a matrix (sX
ij )n×n of pairwise dif2
ferences. We only study the case that sX
ij = kxi − xj k .

To generalize to other similar cases is trivial. Our strategy
is to vary X in certain ways to form a model family. Such
variation represents different perspectives to measure and
perceive information. The scale of this model family exposes the total amount of information in X with respect to
these variations.
We install on each sample xi an isotropic observer. It
perceives information encoded with eq. (1), where sij is
parametrized as

τi · sX
if j ∈ kNNi ;
ij
(3)
sij (τ ) =
+∞
if otherwise,
or
stij (τ )


=

log(τi · sX
ij + 1)
+∞

if j ∈ kNNi ;
if otherwise.

(4)

∀i, τi > 0 zooms other samples near or far from xi , so
that information at different scales can be incorporated. k
(2 ≤ k ≤ n − 1) specifies the visual range in the maximum number of samples that can be observed by any xi .
The purpose of k is to ignore distant relationships to make
related computation scalable. Datasets of different size
are measured on the same statistical manifold S k−1 . Such
measurements are therefore comparable. As compared to
eq. (3), the distribution defined by eq. (4) has higher entropy. Even if τi → ∞, meaning zero sight, distant pairs
still occupy some probability mass. Hence, eq. (4) puts
more emphasis on non-local information.
Once X and k are fixed, all possible configurations of
τ = (τ1 , . . . , τn ) forms a n-dimensional model manifold
n
denoted as OX,k
(τ ). Its geometry is given as follows.
Corollary 6. If the observers are characterized by
n
eq. (3), the Riemannian metric of OX,k
(τ ) is g(τ ) =
diag (g1 (τ1 ), · · · , gn (τn )), where


∂  X

pj|i (τi ) sX
gi (τi ) = −
ij
∂τi
j∈kNNi

=

X


2
X

2
 ;
pj|i (τi ) sX
−
pj|i (τi )sX
ij
ij

j∈kNNi

j∈kNNi

if the observers in eq. (4) are used instead, the corresponding metric is g t (τ ) = diag (g1t (τ1 ), · · · , gnt (τn )), where
git (τi )

=

X

pj|i (τi )

j∈kNNi

sX
ij
1 + τi sX
ij


−

X

j∈kNNi

!2

pj|i (τi )

sX
ij
1 + τi sX
ij

2
 .

An Information Geometry of Statistical Manifold Learning

DATASET
Spiral
Swiss roll
Faces
Hands
MNIST

n
200
500
698
481
60k

D
3
3
4k
245k
784

TRUE d
1
2
∼3
∼2
unknown

MLE
1.3 ± 0.23
2.1 ± 0.13
3.8 ± 0.42
2.2 ± 0.37
10.1 ± 0.27

LAI
1.2 ± 0.21
2.0 ± 0.08
3.4 ± 0.27
1.7 ± 0.21
9.7 ± 0.55

t-LAI
1.1 ± 0.06
2.0 ± 0.05
3.2 ± 0.15
2.0 ± 0.10
9.8 ± 0.26

Table 1. Estimated intrinsic dimension (avg. ± std.) for each k ∈ {5, 10, . . . , 100}.

We only discuss g(τ ) and leave g t (τ ) for future extentions.
n
The scale of OX,k
(τ ) can be measured as follows.
Definition 7. The locally accumulated information (LAI)
of X given the visual range k is defined as
n

|Ok |(X) =

1X
n i=1

Z

∞


p
gi (t) dt .

(5)

0

n
Remark 7.1. OX,k
(τ ) resembles an orthant (0, ∞)n , and
LAI measures its average side length.

To understand LAI, one need to grasp the concept of information. Shannon’s information, i.e. entropy, measures
the absolute uncertainty of a single distribution. Fisher’s
information (Rao, 1945) measures the relative uncertainty
within a continuous region of distributions (Myung et al.,
2000). In eq. (5), each term in the sum is Fisher’s information integrated along a statistical curve corresponding to
a local observation process. LAI measures how much intrinsic difference, or how “many” distinct distributions, are
observed during zooming the observation radius from 0 to
∞.
Proposition 8. ∀X, ∀k, ∀λ > 0, |Ok |(X) = |Ok |(λX).
Proposition
 √ 9.
 If ∀i, xi ’s 1-NN is unique, then ∀k,
arccos 1/ k ≤ |Ok |(X) < ∞.
LAI is invariant to scalling (proposition 8). LAI is always
finite and has a lower bound (proposition 9). This lower
bound can be approached by approximately placing X as
a regular n-simplex. This is only possible when the dimension of {xi } is large enough. In fact, LAI reflects the intrinsic dimensionality. In high dimension, the pairwise distances present less variance (Bellman, 1961). Correspondingly, the curve of distributions defined by eqs. (3) and (4)
is straighter and shorter. In low dimension, the observerations at different scales are more different. Correspondingly, the curve of distributions is more bended. Consider
a line of cities (London, Paris, Geneva, Rome). As an observer in London expands its sight, it discovers the other
three cities one by one. On S 2 , a curve starts from the Paris
vertex, then bends towards Geneva, then bends towards

Figure 2. Local dimension estimation.

Rome. In this model, LAI ≈ 5.3. A rectangular model
(Berlin, Paris, Vienna, Marseille) has “higher dimensionality” and therefore a smaller value of LAI (≈ 5.1).
LAI can be conveniently calculated with an off-the-shelf
numerical integrator. The computation involves n integrations, which can be reduced by averaging over a random
subset of samples in eq. (5). The cost of each integration
scales with k (k  n). Overall, the computation is scalable.
A Dictionary-based Intrinsic Dimensionality Estimator
We generate random artificial data with different dimension D to build a dictionary of LAI values indexed by k and
D. This dictionary is used to map any input data to an intrinsic dimensionality. Table 1 shows its performance compared to the maximum likelihood estimator (MLE) (Levina & Bickel, 2005) on several benchmark datasets, including a spiral with one intrinsic dimension, a Swiss roll
with two intrinsic dimensions, an artificial face dataset2
rendered with different light directions and different orientations (three degrees of freedom), an image sequence3
recording a hand holding a bowl and rotating (around two
degrees of freedom), and MNIST hand-written digits 4 . In
order to suppress the curse of dimensionality (Bellman,
1961), a dataset is projected to <50 with principal component analysis (PCA) before going to the estimators. Overall, the LAI estimation is closer to the (estimated) ground
truth with less variance. t-LAI is based on eq. (4) with similar definitions. It shows the best robustness to the choice of
k, because stij is able to incorporate more non-local information. MNIST is closer to real-world datasets, where the
ground truth is unknown and the intrinsic dimension varies
from region to region. The large variance of (t-)LAI is because the intrinsic dimension changes with k. At a small
scale (k = 5), the intrinsic dimension tends to be overestimated (∼ 10) because of local high-dimensional noise.
At a larger scale (k = 100), the intrinsic dimension is esti2

http://isomap.stanford.edu/datasets.html
http://vasc.ri.cmu.edu//idb/html/motion/
hand/index.html
4
http://yann.lecun.com/exdb/mnist
3

An Information Geometry of Statistical Manifold Learning

mated as 8 ∼ 9 as the data manifold shows up. This observation agrees with the characteristics of real-world data.
LAI can be used for local dimensionality estimation
R ∞ p(Carter
gi (t)dt
et al., 2010) by computing a local average of 0
around any xi . Figure 2 shows the side view of a candy bar
dataset with a 1D stick, a 2D disk, and a 3D head. Its local
dimension showed by the colors is well-estimated.

where ∀i,
i
gaa
=

pj|i

2

X

2
 ,
pj|i sY
sY
−
ij
ij

j∈kNNi

j∈kNNi

X

pj|i

2
X

2
 ,
−
pj|i sZ
sZ
ij
ij

X



i
gbb
=

j∈kNNi

j∈kNNi
i
i
gab
= gba
=

4. A Gap between Two Models

X

Z
pj|i sY
ij · sij



j∈kNNi

A central problem in MAL is to define the difference between an input Y n and an output Z n . Then, the embedding
quality can be evaluated, and MAL can be implemented
through optimization. According to section 3, Y and Z
individually extend to two families modeling their internal complexity. Our strategy is to continuously deform one
family to the other along a bridging manifold. The volume
of this bridge measures their intrinsic difference.
Fortunately, such a bridge exists for any given Y and Z.
2n
Consider the model family HY
,Z,k defined by


sij (c) =

Z
ai sY
ij + bi sij
+∞

if j ∈ kNNi ;
if otherwise,

(6)






X

−


pj|i sY
ij

X

.
pj|i sZ
ij

j∈kNNi

j∈kNNi

The metric g t (c) with respect to eq. (7) is obtained by reZ
Z
placing sZ
ij with sij / 1 + bi sij in the above equations.
Due to space limitation, the following discussion is only
based on the geometry induced by eq. (6).
2n
HY
,Z,k embeds all information regarding the intra- and
inter-complexity of Y and Z. Across this bridge, we construct a low dimensional film, whose volume can be easily
computed to estimate the closeness between the input and
output boundaries. Consider the 2D sub-manifold
2
H̄Y
,Z,k,κ = {(a1 , b1 , . . . , an , , bn ) : ∀i, ai = aτi (Y , κ),

or

stij (c)

bi = bτi (Z, κ); a ∈ (0, ∞); b ∈ (0, ∞); κ < k}

=

Z
ai sY
ij + log bi sij + 1
+∞



if j ∈ kNNi ;
if otherwise,

(7)

where ∀i, kNNi = kNNi (Y ) ∪ kNNi (Z) are the input or
output neighbours of i, ai > 0, bi > 0, and c =
(a1 , b1 , . . . , an , bn ) serves as a global coordinate system.
The boundary b = 0 deteriorates to a model family inn
duced by Y (similar to OY
,k ). The boundary a = 0 deterin
orates to a family induced by Z (similar to OZ,k
). Among
all possible interpolations, eqs. (6) and (7) are simple and
2n
natural. HY
,Z,k defined in eq. (6) is somehow flat (Amari
& Nagaoka, 2000). Its geometry is given as follows.
Corollary 10. With respect to eq. (6) and the global coordinate system c = (a1 , b1 , . . . , an , bn ), the Riemannian
2n
metric of HY
,Z,k is in the block-diagonal form

 1
gaa
1
 gba


g(c) = 



1
gab
1
gbb

with a global coordinate system (a, b). All observers in
Y (resp. Z) simultaneously zoom according to one single
parameter a (resp. b). A large value of a (resp. b) corresponds to high frequency local information; a small value
of a (resp. b) corresponds to low frequency distant information. For each i, the scalars τi (Y , κ) and τi (Z, κ) are computed by binary search, so that the distribution p·|i defined
by eqs. (1), (6) and (7) has fixed entropy given by log κ
and each sample has effectively the same number of neighbours given by κ (Hinton & Roweis, 2003). Such alignment is to derive two lines a = 0 and b = 0 as close as
possible on each side of the gap. The volume of the film
H̄Y ,Z,k,κ in between these lines approximates the minimal
efforts needed to shift a continuous spectrum of information from one family to the other.
Proposition 11. The volume (area) of some region Ω on
H̄Y ,Z,k,κ is
v
!
u n
ZZ
u X
2
t
i
τi (Y , κ)gaa
|H̄k,κ,Ω |(Y , Z) =
da db
Ω


..

.
n
gaa
n
gba




,

n 
gab
n
gbb

×

n
X
i=1

!
i
τi2 (Z, κ)gbb

−

i=1
n
X

!2
τi (Y

i
, κ)τi (Z, κ)gab

.

i=1

(8)

– 1/a –.

– 1/a –.

– density –.

– 1/b –.
– 1/a –.

(c) 4.6 × 102

– 1/a –.

(d) 9.9 × 102

(e) 8.1 × 102

(f) 3.2 × 10

– 1/a –.
3

(g) 2.8 × 10

(h) 2.6 × 10

– 1/a –.
3

– density –.

– 1/b –.

– 1/b –.

– 1/a –.
3

(i) 2.3 × 10

– 1/a –.
3

0.2

– 1/a –.

– 1/b –.

– 1/b –.

– 1/b –.

MNIST

2

(b) 1.6 × 103

t-SNE

– 1/b –.

– 1/b –.
– 1/a –.

(a) 5.3 × 103

SNE

0.05

Isomap

PCA

– 1/b –.

– 1/b –.

Swiss roll

Random

0.6

An Information Geometry of Statistical Manifold Learning

(j) 1.0 × 103

Figure 3. Performance measurements of different embeddings. In each sub-figure, the color-map shows the local densities
vol(a, b)dσ(a, b) over Ω. From left to right (resp. bottom to up), the input (resp. output) observation radius expands from 5 to 50.
The x-axis (resp. y-axis) is linear in 1/a (resp. 1/b). The number below each square shows the volume, i.e. the integration of local
densities in the corresponding color-map. Note, the colors are different between the two datasets.

Proposition 12. ∀Y , ∀Z, ∀λy > 0, ∀λz > 0,
|H̄k,κ,Ω |(Y , Z) = |H̄k,κ,Ω |(λy Y , λz Z).
The region Ω means interested information in MAL. It can
be chosen empirically as a rectangle to exclude low frequency information above the radius κ and high frequency
information below a minimal radius ks . Given Y and Z,
the volume |H̄k,κ,Ω |(Y , Z), shortly denoted as |H̄Ω |, can
be efficiently computed with a Monte Carlo integrator. It
forms a theoretical objective of MAL that is scale-invariant
(proposition 12).
RR
We re-write eq. (8) as |H̄Ω | = Ω vol(a, b) dσ(a, b). By
i
i
i
i
noting that gaa
, gbb
, gab
and gba
in corollary 10 are all in
the form of (co-)variances, the normalized scalar
vol(a, b) =
s

Pn
i 2
i=1 τi (Y , κ)τi (Z, κ)gab
Pn
1 − Pn
2
2
i
i
i=1 τi (Y , κ)gaa
i=1 τi (Z, κ)gbb

(9)

ranges in [0, 1] and measures the overall linear correlation
Z
between sY
ij and sij for the same i. The more linearly corY
Z
related sij and sij , the smaller the value of vol(a, b).
Proposition 13. |H̄Ω | = 0 iff ∃a, b ∈ Ω, s.t. vol(a, b) = 0.
vol(a, b) is usually positive (proposition 13) and tells the
input-output agreement at a specific frequency. The term
 v

v
u n
u n
uX
uX
i
i da t
τi2 (Y , κ)gaa
τi2 (Z, κ)gbb
db
dσ(a, b) = t
i=1

i=1

measures the information density or interestingness at
(a, b) based on separate observations on Y and Z (see section 3). Therefore, |H̄Ω | can be understood as the linear
agreement at different scales weighted by information density.
Two classical datasets in MAL, Swiss roll (n = 103 ) and
MNIST (n = 5 × 103 ; five classes), are embedded into
<2 by PCA, Isomap, SNE and t-SNE with typical configurations. The parameters k, κ and ks are empirically set to
100, 50 and 5, respectively. The gap on MNIST is computed by randomly sampling 103 input-output pairs to be
comparable with Swiss roll. In fig. 3, the color-maps show
vol(a, b)dσ(a, b) over Ω. Their appearances depend on the
coordinate system of H̄Y ,Z,k,κ . Here, the axises are linear in 1/a and 1/b and represent the observation radii. For
example, the upper-left corner means that the input (resp.
output) information is examined at a radius of 5 (resp. 50)
samples. The gap volume |H̄Ω | below each color-map is
independent of the coordinate system. The best method for
each dataset (Isomap for Swiss roll; t-SNE for MNIST) is
identified by the bluest square with the smallest volume.
The patterns on the color-maps tell more detailed information. The redness on the lower-right corner (e.g. fig. 3(b))
indicates that the original neighbours are heavily invaded
in the embedding. Apparently, t-SNE outperforms SNE in
this region. This is related to an information retrieval perspective (Venna & Kaski, 2007). By comparing fig. 3(f)
with fig. 3(a), MNIST with high dimensional noise is closer
to random data. It is more difficult to improve over random

An Information Geometry of Statistical Manifold Learning

Y

embeddings in this dataset. The spiky pattern in fig. 3(f-j)
shows that some structural information that distinguishes
MNIST from random data is forced to a thin band due to
the high dimensionality (Bellman, 1961).
Most MAL techniques preserve a single frequency or scale
of a specific type of local information. The result strongly
favors this frequency and this type of information. The proposed gap yields a family of criteria that are less-biased towards such choices. By mapping onto the statistical manifold, some redundant information is factored out. By aligning and seeking a minimal gap, an intrinsic difference is exposed. The integration over a spectrum gives accurate estimation of the true information loss. In practice, to compute
the gap always faces the choice of a statistical encoding
and associated parameters, e.g., k, κ, and ks . However, the
relative order of the gap volumes should be robust to such
choices. Despite that the results are developed based on the
SNE encodings, the gap volume is fundamentally different
from SNE’s objective and does not necessarily favor SNE.

5. Related Works and Discussion
Information geometry (Rao, 1945; Čencov, 1982; Nagaoka
& Amari, 1982) plays a vital role in statistical learning
theory. MAL has been developed along a statistical approach. It is a natural and meaningful step to bridge the
profound information geometry. Efforts (Weinberger et al.,
2007; Carreira-Perpiñán, 2010; Vladymyrov & CarreiraPerpiñán, 2012) in seeking efficient MAL implicitly used
such a geometry. There, a common technique is to bend
the gradient 5(Z) of a cost function with M −1 (Z) 5 (Z),
where M (Z)  0. This is equivalent to compute the gradient with respect to a Riemannian metric M (Z) on the
solution space. Such a metric, however, has not been explicitly mentioned or formally studied.
Lebanon (2003; 2005) parametrized the Riemannian metric of a statistical simplex and proposed a metric learning
objective to maximize the inverse volume element. Carter
et al. (2009; 2011) studied MAL on a collections of probability density functions. In these works, the subject is still
a data geometry, where the observed data is assumed to lie
on a statistical manifold. This is different from the picture
shown here, which views all input or output information
jointly as one point and studies its dynamics.
We formally introduce a semi-Riemannian geometry of a
model manifold. It broadens our horizons so that MAL appears as a curve (figs. 1 and 4) and different manifold learners are viewed from a unified perspective. An intriguing
aspect is that any volume corresponds to an amount of information. It can be measured to define intrinsic quantities.
On two specific model manifolds, we demonstrate how to
apply the theoretical results to measure the complexity and

manifold learning
(project Y → Z)

LAI

scaling

Ω
0

H̄(gap)
LAI

scaling

Z
Figure 4. Internal complexity of Y and Z and their gap.

quality of models. These measurements are only briefly
sketched here to testify the learning theory. They can be
further unfold into meaningful theories. This work is summarized in fig. 4. A fundamental trade-off of MAL is to
minimize the volume of the gap (lost information; see section 4) and to maximize the volume of the output (remained
information; see section 3). To unify and combine LAI and
the gap volume into one criterion and to seek parameterfree invariants are worthy of future work.
The gap volume in proposition 11 as a theoretical objective
is hard to optimize directly. This is expected and fits in a
usual two-stage learning scheme (Akaike, 1974; Schwarz,
1978; Xu, 2010). In the parameter learning stage, a simple
objective function is optimized for each candidate model.
In the model selection stage, a sophisticated criterion that
better approximates the generalization error is computed to
select the best model. We seek to derive simple approximations of the gap volume and develop related MAL algorithms.
Several possible extensions are discussed at the end of section 2. A problem that fits in the recent advancements (Vladymyrov & Carreira-Perpiñán, 2012; Yang et al., 2013) of
MAL is to find efficient optimization based on generalizations of Amari’s natural gradient (Amari & Nagaoka, 2000;
Nock et al., 2011). A theoretical problem is to explore the
relationship with graph Laplacian regularization (Belkin &
Niyogi, 2003; Weinberger et al., 2007).

Acknowledgments
This work is jointly supported by the Swiss National Science Foundation (SNSF) and the European COST Action
on Multilingual and Multifaceted Interactive Information
Access (MUMIA) via the Swiss State Secretariat for Education and Research (SER project CC11.0043).

References
Akaike, H. A new look at the statistical model identification.
IEEE Trans. Automat. Contr., 19(6):716–723, 1974.

An Information Geometry of Statistical Manifold Learning
Amari, S. Information geometry of the EM and em algorithms for
neural networks. Neural Networks, 8(9):1379–1408, 1995.
Amari, S. and Nagaoka, H. Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs.
AMS and OUP, 2000. (Published in Japanese in 1993).
Belkin, M. and Niyogi, P. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Comput., 15(6):
1373–1396, 2003.
Bellman, R. Adaptive Control Processes: A Guided Tour. Princeton University Press, 1961.
Carreira-Perpiñán, M. Á. The elastic embedding algorithm for
dimensionality reduction. In ICML, pp. 167–174, 2010.
Carter, K. M., Raich, R., Finn, W. G., and Hero, A. O. FINE:
Fisher Information Nonparametric Embedding. IEEE Trans.
Pattern Anal. Mach. Intell., 31(11):2093–2098, 2009.
Carter, K. M., Raich, R., and Hero, A. O. On local intrinsic dimension estimation and its applications. IEEE Trans. Signal
Processing, 58(2):650–663, 2010.
Carter, K. M., Raich, R., Finn, W. G., and Hero, A. O.
Information-geometric dimensionality reduction. IEEE Signal
Process. Mag., 28(2):89–99, 2011.
Čencov, N. N. Statistical Decision Rules and Optimal Inference, volume 53 of Translations of Mathematical Monographs.
AMS, 1982. (Published in Russian in 1972).
Cook, J., Sutskever, I., Mnih, A., and Hinton, G. E. Visualizing
similarity data with a mixture of maps. In AISTATS, JMLR:
W&CP 2, pp. 67–74, 2007.
Ham, J., Lee, D. D., Mika, S., and Schölkopf, B. A kernel view
of the dimensionality reduction of manifolds. In ICML, pp.
47–54, 2004.
Hinton, G. E. and Roweis, S. T. Stochastic Neighbor Embedding.
In NIPS 15, pp. 833–840. 2003.
Hinton, G. E. and Salakhutdinov, R. R. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–
507, 2006.
Jost, J. Riemannian Geometry and Geometric Analysis. Springer,
5th edition, 2008.
Lebanon, G. Learning Riemannian metrics. In UAI, pp. 362–369,
2003.
Lebanon, G. Riemannian geometry and statistical machine learning. PhD thesis, CMU, 2005.
Levina, E. and Bickel, P. J. Maximum likelihood estimation of
intrinsic dimension. In NIPS 17, pp. 777–784. 2005.
Myung, J., Balasubramanian, V., and Pitt, M. A. Counting probability distributions: differential geometry and model selection.
PNAS, 97(21):11170–11175, 2000.
Nagaoka, H. and Amari, S. Differential geometry of smooth families of probability distributions. Technical Report METR 82-7,
Univ. of Tokyo, 1982.

Nock, R., Magdalou, B., Briys, E., and Nielsen, F. On tracking portfolios with certainty equivalents on a generalization of
markowitz model: the fool, the wise and the adaptive. In ICML,
pp. 73–80, 2011.
Rao, C. R. Information and accuracy attainable in the estimation
of statistical parameters. Bull. Cal. Math. Soc., 37(3):81–91,
1945.
Roweis, Sam T. and Saul, Lawrence K. Nonlinear dimensionality
reduction by locally linear embedding. Science, 290(5500):
2323–2326, 2000.
Schwarz, G. Estimating the dimension of a model. Ann. Stat., 6
(2):461–464, 1978.
Sha, F. and Saul, L.K. Analysis and extension of spectral methods
for nonlinear dimensionality reduction. In ICML, pp. 784–791,
2005.
Sun, K., Bruno, E., and Marchand-Maillet, S. Stochastic unfolding. In MLSP, pp. 1–6, 2012.
Tenenbaum, J. B., de Silva, V., and Langford, J. C. A global
geometric framework for nonlinear dimensionality reduction.
Science, 290(5500):2319–2323, 2000.
van der Maaten, L. J. P. and Hinton, G. E. Visualizing data using
t-SNE. JMLR, 9(Nov):2579–2605, 2008.
Vapnik, V. N. Statistical Learning Theory. Wiley-Interscience,
1998.
Venna, J. and Kaski, S. Nonlinear dimensionality reduction as
information retrieval. In AISTATS, JMLR: W&CP 2, pp. 572–
579, 2007.
Vincent, P. and Bengio, Y. Manifold Parzen windows. In NIPS
15, pp. 825–832. 2003.
Vladymyrov, M. and Carreira-Perpiñán, M. Á. Partial-Hessian
strategies for fast learning of nonlinear embeddings. In ICML,
pp. 345–352, 2012.
Weinberger, K. Q., Sha, F., and Saul, L. K. Learning a kernel
matrix for nonlinear dimensionality reduction. In ICML, pp.
839–846, 2004.
Weinberger, K. Q., Sha, F., Zhu, Q., and Saul, L. K. Graph Laplacian regularization for large-scale semidefinite programming.
In NIPS 19, pp. 1489–1496. 2007.
Xu, L. Advances on BYY harmony learning: Information theoretic perspective, generalized projection geometry, and independent factor auto-determination. IEEE Trans. Neural Networks, 15(4):885–902, 2004.
Xu, L. Machine learning problems from optimization perspective.
J. Global Optim, 47(3):369–401, 2010.
Yang, Z., Peltonen, J., and Kaski, S. Scalable optimization
of neighbor embedding for visualization. In ICML, JMLR:
W&CP 28.2, pp. 127–135, 2013.
Zhang, J., Wang, Q., He, L., and Zhou, Z. H. Quantitative analysis
of nonlinear embedding. IEEE Trans. Neural Networks, 22
(12):1987–1998, 2011.

