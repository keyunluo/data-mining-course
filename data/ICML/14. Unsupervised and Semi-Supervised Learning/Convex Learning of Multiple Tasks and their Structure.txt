Convex Learning of Multiple Tasks and their Structure

Carlo Ciliberto1,2
CCILIBER @ MIT. EDU
Youssef Mroueh1,2
YMROUEH @ MIT. EDU
Tomaso Poggio1,2
TP @ AI . MIT. EDU
1
Laboratory for Computational and Statistical Learning, Istituto Italiano di Tecnologia, Via Morego 30, Genova, Italy
2
Center for Brains Minds and Machines, Massachusetts Institute of Technology, Cambridge, MA 02139 USA
Lorenzo Rosasco1,2,3
3
DIBRIS, Università di Genova, Via Dodecaneso, 35, 16146, Genova, Italy

Abstract
Reducing the amount of human supervision is
a key problem in machine learning and a natural approach is that of exploiting the relations
(structure) among different tasks. This is the idea
at the core of multi-task learning. In this context a fundamental question is how to incorporate
the tasks structure in the learning problem. We
tackle this question by studying a general computational framework that allows to encode a-priori
knowledge of the tasks structure in the form of a
convex penalty; in this setting a variety of previously proposed methods can be recovered as
special cases, including linear and non-linear approaches. Within this framework, we show that
tasks and their structure can be efficiently learned
considering a convex optimization problem that
can be approached by means of block coordinate
methods such as alternating minimization and for
which we prove convergence to the global minimum.

1. Introduction
Current machine learning systems achieve remarkable results in several challenging tasks, but are limited by the
amount of human supervision required. Leveraging similarity among different problems is widely acknowledged
to be a key approach to reduce the need for supervised
data. Indeed, this idea is at the basis of multi-task learning, where the joint solution of different problems (tasks)
has the potential to exploit tasks relatedness (structure) to
improve learning accuracy. This idea has motivated a variProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

LROSASCO @ MIT. EDU

ety of methods, including frequentist (Micchelli & Pontil,
2004; Argyriou et al., 2008a;b) and Bayesian methods (see
e.g. (Álvarez et al., 2012) and references therein), with connections to structured learning (Bakir et al., 2007; Tsochantaridis et al., 2004).
The focus of our study is the development of a general regularization framework to learn multiple tasks as well as their
structure. Following (Micchelli & Pontil, 2004; Evgeniou
et al., 2005) we consider a setting where tasks are modeled
as the components of a vector-valued function and their
structure corresponds to the choice of suitable functional
spaces. Exploiting the theory of reproducing kernel Hilbert
spaces for vector-valued functions (RKHSvv) (Micchelli
& Pontil, 2004), we consider and analyze a flexible regularization framework, within which a variety of previously
proposed approaches can be recovered as special cases, see
e.g. (Jacob et al., 2008; Lozano & Sindhwani, 2011; Minh
& Sindhwani, 2011; Zhang & Yeung, 2010; Dinuzzo et al.,
2011; Sindhwani et al., 2012). Our main technical contribution is a unifying study of the minimization problem
corresponding to such a regularization framework. More
precisely, we devise an optimization approach that can efficiently compute a solution and for which we prove convergence under weak assumptions. Our approach is based on a
barrier method that is combined with block coordinate descent techniques (Tseng, 2001; Razaviyayn et al., 2013). In
this sense our analysis generalizes the results in (Argyriou
et al., 2008a) for which a low-rank assumption was considered; however the extension is not straightforward, since
we consider a much larger class of regularization schemes
(any convex penalty). Up to our knowledge, this is the first
result in multi-task learning proving the convergence of alternating minimization schemes for such a general family
of problems.
The RKHSvv setting allows to naturally deal both with linear and non-linear models and the approach we propose
provides a general computational framework for learning

Convex Learning of Multiple Tasks and their Structure

output kernels as formalized in (Dinuzzo et al., 2011).
The rest of the paper is organized as follows: in Sec 2
we review basic ideas of regularization in RKHSvv. In
Sec. 2.3 we discuss the equivalence of different approaches
to encode known structures among multiple tasks. In
Sec. 3 we discuss a general framework for learning multiple
tasks and their relations where we consider a wide family
of structure-inducing penalties and study an optimization
strategy to solve them. This setting allows us, in Sec. 4, to
recover several previous methods as special cases. Finally
in Sec. 5 we evaluate the performance of the optimization
method proposed.
n
n
Notation. With S++
⊂ S+
⊂ S n ⊂ Rn×n we denote respectively the space of positive definite, positive semidefinite (PSD) and symmetric n × n real-valued matrices. On
denotes the space of orthonormal n × n matrices. For
any square P
matrix M ∈ Rn×n and p ≥ 1, we denote by
n
kM kp = ( i=1 σi (M )p )1/p the p-Schatten norm of M ,
where σi (M ) is the i-th largest singular value of M . For
any M ∈ Rn×m , M > denotes the transpose of M . For any
n
PSD matrix A ∈ S+
, A† denotes the pseudoinverse of A.
n
We denote by In ∈ S++
the n × n identity matrix. The
m
notation Ran(M ) ⊆ R identifies the range of columns of
a matrix M ∈ Rm×n .

2. Background
We study the problem of jointly learning multiple tasks by
modeling individual task-predictors as the components of a
vector-valued function. Let us assume to have T supervised
scalar learning problems (or tasks), each with a “training”
nt
set of input-output observations St = {(xit , yit )}i=1
with
xit ∈ X input space and yit ∈ Y output space1 . Given
a loss function L : R × R → R+ that measures the pertask prediction errors, we want to solve the following joint
regularized learning problem
nt
T
X
1 X
(t)
(t)
L(yi , ft (xi )) + λkf k2H
minimize
f ∈H
n
t
t=1
i=1

(1)

where H is an Hilbert space of vector-valued functions
f : X → Y T with scalar components ft : X → Y. In
order to define a suitable space of hypotheses H, in this
section we briefly recall concepts from the theory of reproducing kernel Hilbert spaces for vector-valued functions
(RKHSvv) and corresponding regularization theory, which
plays a key role in our work. In particular, we focus on a
class of reproducing kernels (known as separable kernels)
that can be designed to encode specific tasks structures (see
(Evgeniou et al., 2005; Argyriou et al., 2013) and Sec. 2.3).
1

To avoid clutter in the notation, we have restricted ourselves
to the typical situation where all tasks share same input and output
spaces, i.e. Xt = X and Yt ⊆ R.

Interestingly, separable kernels are related to ideas such as
defining a metric on the output space or a label encoding in
multi-label problems (see Sec. 2.3)
Remark 2.1 (Multi-task and multi-label learning). Multilabel learning is a class of supervised learning problems in
which the goal is to associate input examples with a label
or a set of labels chosen from a discrete set. In general,
due to discrete nature of the output space, these problems
cannot be solved directly; hence, a so-called surrogate
problem is often introduced, which is computationally
tractable and whose solution allows to recover the solution
of the original problem (Steinwart & Christmann, 2008;
Bartlett et al., 2006; Mroueh et al., 2012).
Multi-label learning and multi-task learning are strongly
related. Indeed, surrogate problems typically consist in
a set of distinct supervised learning problems (or tasks)
that are solved simultaneously and therefore have a natural
formulation in the multi-task setting. For instance, in
multi-class classification problems the “One vs All”
strategy is often adopted, which consists in solving a set of
multiple binary classification problems, one for each class.
2.1. Learning Multiple Tasks with RKHSvv
In the scalar setting, reproducing kernel Hilbert spaces have
already been proved to be a powerful tool for machine
learning applications. Interestingly, the theory of RKHSvv
and corresponding Tikhonov regularization scheme follow
closely the derivation in the scalar case.
Definition 2.2. Let (H, h·, ·iH ) be a Hilbert space of functions from X to RT . A symmetric, positive definite, matrixvalued function Γ : X × X → RT ×T is called a reproducing kernel for H if for all x ∈ X , c ∈ RT and f ∈ H
we have that Γ(x, ·)c ∈ H and the following reproducing
property holds hf (x), ciRT = hf, Γ(x, ·)ciH .
In analogy to the scalar setting, it can be proved (see (Micchelli & Pontil, 2004)) that the Representer Theorem holds
also for regularization in RKHSvv. In particular we have
that any solution of the learning problem introduced in
Eq. (1) can be written in the form
f (x) =

nt
T X
X

(t)

(t)

Γ(x, xi )ci

(2)

t=1 i=1
(t)

with ci ∈ RT coefficient vectors.
The choice of kernel Γ induces a joint representation of
the inputs as well as a structure among the output components (Álvarez et al., 2012); In the rest of the paper we will
focus on so-called separable kernels, where these two aspects are factorized. In Section 3, we will see how separable kernels provide a natural way to learn the tasks structure
as well as the tasks.

Convex Learning of Multiple Tasks and their Structure

2.2. Separable Kernels
Separable (reproducing) kernels are functions of the form
Γ(x, x0 ) = k(x, x0 )A ∀x, x0 ∈ X where k : X × X → R
T
is a scalar reproducing kernel and A ∈ S+
is a positive
semi-definite (PSD) matrix. In this case, the representer
theorem allows to rewrite problem (1) in a more compact
matrix notation as
minimize V (Y, KCA) + λ tr(AC > KC).
C∈Rn×T

(P)

PT
Here Y ∈ Rn×T is a matrix with n = t=1 nt rows conn
taining the output points; K ∈ S+
is the empirical kernel
matrix associated to k and V : Rn×T × Rn×T → R+
generalizes the loss in (1) and consists in a linear combination of the entry-wise application of L. Notice that this
formulation accounts also the situation where not all training outputs y (t) are observed when a given input x ∈ X is
provided: in this case the functional V weights 0 the loss
values of those entries of Y (and the associated entries of
KCA) that are not available in training.
Finally, the second term in (P) follows by observing
Pn that, for all f ∈ H of the form f (·) =
·)Aci , the squared norm can be written as
i=1 k(xi ,P
n
>
>
kf k2H =
i,j k(xi , xj )ci Acj = tr(AC KC) where
n×T
C ∈ R
is the matrix with i-th row corresponding to
the coefficient vector ci ∈ RT of f . Notice that we have
re-ordered the index i to be in {1, . . . , n} to ease the notation.
2.3. Incorporating Known Tasks Structure
Separable kernels provide a natural way to incorporate the
task structure when the latter is known a priori. This strategy is quite general and indeed in the following we comment on how the matrix A can be chosen to recover several
multi-task methods previously proposed in contexts such as
regularization, coding/embeddings or output metric learning, postponing a more detailed discussion in the supplementary material. These observations motivate the extension in Sec. 3 of the learning problem (P) to a setting where
it is possible to infer A from the data.
Regularizers. Tasks relations can be enforced by devising suitable regularizers (Evgeniou et al., 2005). Interestingly, for a large class of such methods it can be shown that
this is equivalent to the choice of the matrix A (or rather its
pseudoinverse) (Micchelli & Pontil,P
2004). If we consider
n
the squared norm of a function f = i=1 k(xi , ·)Aci ∈ H
we have (see (Evgeniou et al., 2005))
kf k2H

=

T
X

A†ts hft , fs iHk

(3)

t,s=1

where At is the t-th column of A, HkPis the RKHS associn
ated to the scalar kernel k and ft = i=1 k(xi , ·)A>
t ci ∈

Hk is the t-th component of f . The above equation suggests to interpret A† as the matrix that models the structural relations between tasks by directly coupling different
predictors. For instance, by setting A† = IT + γ(11> )/T ,
with 1 ∈ RT the vector P
of all 1s, we have that the parameter
T
γ controls the variance t=1 kf¯ − ft k2Hk of the tasks with
PT
respect to their mean f¯ = T1 t=1 ft . If we have access
to some notion of similarity among tasks in the form of a
graph with adjacency
matrix W ∈ S T , we
PT
PTcan consider the
regularizer t,s=1 Wt,s kft −fs k2Hk +γ t kft k2Hk which
corresponds to A† = L + γIT with L the graph Laplacian
induced by W .

Output Metric. A different approach to model tasks relatedness consists in choosing a suitable metric on the output space to reflect the tasks structure (Lozano & Sindhwani, 2011). Clearly a change of metric on the output
space with the standard inner product hy, y0iRT between
two output points y, y0 ∈ Y T corresponds to the choice of
a different inner product hy, y0iΘ = hy, θy0iRT for some
T
positive definite matrix Θ ∈ S++
. Indeed this can be direct related to the choice of a suitable separable kernel. In
particular, for the least squares loss function a direct equivalence holds between choosing a metric deformation assoT
and a separable kernel k(·, ·)IT or
ciated to a Θ ∈ S++
use the canonical metric (i.e. with Θ = IT the identity)
and kernel k(·, ·)Θ. The details of this equivalence can be
found in the supplementary material.

Output Representation. The tasks structure can also be
modeled by designing an ad-hoc embedding for the output
space. This approach is particularly useful for multi-label
scenarios, where output embedding can be designed to encode complex structures such as (e.g. trees, strings, graphs,
etc.) (Fergus et al., 2010; Joachims et al., 2009; Crammer &
Singer, 2000). Interestingly in these cases, or more genere from the
ally whenever the embedding map L : Y T → Y,
original to the new output space, is linear, then it is possible
to show that the learning problem with new code is equivalent to (1) for a suitable choice of separable kernel with
A = L> L. We refer again to the supplementary material
for the details of this equivalence.

3. Learning the Tasks and their Structure
Clearly, an interesting setting occurs when knowledge of
the tasks structure is not available and therefore it is not
possible to design a suitable separable kernel. In this case
a favorable approach is to infer the tasks relations directly
from the data. To this end we propose to consider the fol-

Convex Learning of Multiple Tasks and their Structure

lowing extension of problem (P)
minimize

T
C∈Rn×T ,A∈S+

V (Y, KCA) + λtr(AC > KC) + F (A),

(Q)
T
where the penalty F : S+
→ R+ is designed to learn specific tasks structures encoded in the matrix A. The above
regularization is general enough to encompass a large number of previously proposed approaches by simply specifying a choice of the scalar kernel and the penalty F . A detailed discussion of these connections is postponed to Section 4. In this section, we focus on computational aspects.
Throughout, we restrict ourselves to convex loss functions
V and convex (and coercive) penalties F . In this case, the
objective function in (Q) is separately convex in C and A
but not jointly convex. Hence, block coordinate methods,
which are often used in practice, e.g. alternating minimization over C and A, are not guaranteed to converge to a
global minimum. Our study provides a general framework
to provably compute a solution to problem (Q). First, In
Section 3.1, we prove our main results providing a characterization of the solutions of Problem (Q) and studying a
barrier method to cast their computation as a convex optimization problem. Second, in Section 3.2, we discuss how
block coordinate methods can be naturally used to solve
such a problem, analyze their convergence properties and
discuss some general cases of interest.
3.1. Characterization of Minima and A Barrier
Method
We begin, in Section 3.1.1, providing a characterization
of the solutions to Problem (Q) by showing that it has an
equivalent formulation in terms of the minimization of a
convex objective function, namely Problem (R). Depending on the behavior of the objective function on the boundary of the optimization domain, Problem (R) might not be
solved using standard optimization techniques. This possible issue motivates the introduction, in Section 3.1.2, of
a barrier method; a family of “perturbated” convex programs is introduced whose solutions are shown to converge
to those of Problem (R) (and hence of the original (Q)).
3.1.1. A N E QUIVALENT FORMULATION FOR (Q)
The objective functional in (Q) is not convex, therefore in
principle it is hard to find a global minimizer. As it turns
out however, it is possible to circumvent this issue and efficiently find a global solution to (Q). The following result
represents a first step in this direction.
Theorem 3.1. Let K ∈

n
S+

and consider the convex set


	
T
C = (C, A) ∈ Rn×T × S+
| Ran(C > KC) ⊆ Ran(A) .
T
Then, for any F : S+
→ R+ convex and coercive, problem


minimizeV (Y, KC) + λtr A† C > KC + F (A)
(C,A) ∈ C

(R)

has convex objective function and it is equivalent to (Q).
In particular, the two problems achieve the same minimum
value and, given a solution (CR , AR ) for (R), the couple
(CR A†R , AR ) is a minimizer for (Q). Vice-versa, given a
solution (CQ , AQ ) for (Q), the couple (CQ AQ , AQ ) is a
minimizer for (R).
The above result highlights a remarkable connection between the problems (Q) (non-convex) and (R) (convex). In
particular, we have the following Corollary, which provides
us with a useful characterization of the local minimizers of
problem (Q).
T
Corollary 3.2. Let Q : Rn×T × S+
→ R be the objective
function of problem (Q). Then, every local minimizer for
T
is also a global minimizer.
Q on the open set Rn×T × S++
Corollary 3.2 follows from Theorem 3.1 and the fact that,
T
on the restricted domain Rn×T × S++
, the map Q is the
combination of the objective functional of (R) and the invertible function (C, A) 7−→ (CA, A). Moreover, if Q is
differentiable, i.e. V and the penalty F are differentiable,
this is exactly the definition of a convexifiable function,
which in particular implies invexity (Craven, 1995). The
latter property ensures that, in the differentiable case, all
the stationary points (rather than only local minimizers)
are global minimizers. This result was originally proved
in (Dinuzzo et al., 2011) for the special case of V the leastsquares loss and F (·) = k · k2F the Frobenius norm; Here
we have proved its generalization to all convex losses V
and penalties F .
We end this section adding two comments. First, we note
that, while the objective function in Problem (R) is convex, the corresponding minimization problem might not be
a convex program (in the sense that the feasible set C is
not identified by a set of linear equalities and non-linear
convex inequalities (Boyd & Vandenberghe, 2004)). Second, Corollary (3.2) holds only on the interior of the miniT
mization domain Rn×T × S+
and does not characterize the
behavior of the target functional on its boundary. In fact,
one can see that both issues can be tackled defining a perturbed objective functional having a suitable behavior on
the boundary of the minimization domain. This is the key
motivation for the barrier method we discuss in the next
section.
3.1.2. A BARRIER M ETHOD TO O PTIMIZE (R)
Here we propose a barrier approach inspired by the work
in (Argyriou et al., 2008a) by introducing a perturbation
of problem (R) that enforces the objective functions to be
T
equal to +∞ on the boundary of Rn×T × S+
. As a consequence, each perturbed problem can be solved as a convex

Convex Learning of Multiple Tasks and their Structure

optimization constrained on a closed cone. The latter comment is made more precise in the following result that we
prove in the supplementary material.
Theorem 3.3. Consider the family of optimization problems
minimizeV (Y, KC) + λtr(A−1 (C > KC + δ 2 IT )) + F (A)
C∈Rn×T ,
T
A∈S+

(S δ )
with IT ∈
the identity matrix. Then, for each δ > 0
the problem (S ) admits a minimum. Furthermore, the set
of minimizers for (S δ ) converges to the set of minimizers
for (R) as δ tends to zero. More precisely, given any sequence δm > 0 such that δm → 0 and a sequence of minT
imizers (Cm , Am ) ∈ Rn×T × S+
for (S δ ), there exists a
∗
∗
n×T
T
sequence (Cm , Am ) ∈ R
× S+ of minimizers for (R)
∗
such that kCm −Cm
kF +kAm −A∗m kF → 0 as m → +∞.
T
S++
δ

The barrier δ 2 tr(A−1 ) is fairly natural and can be seen as
preconditioning of the problem leading to favorable computations. The proposed barrier method is similar in spirit
to the approach developed in (Argyriou et al., 2008a) and
indeed Theorem 3.3 and next Corollary 3.4 are a generalization over the two main results in (Argyriou et al.,
2008a) to any convex penalty F on the cone of PSD matrices. However, notice that since we are considering a much
wider family of penalties (than the trace norm as in (Argyriou et al., 2008a)) our results cannot directly derived
from those in (Argyriou et al., 2008a). In the next section
we discuss how to compute the solution of Problem (S δ )
considering a block coordinate approach.
3.2. Block Coordinate Descent Methods
The block variable structure of the objective function in
(S δ ), suggests that it might be beneficial to use block coordinate methods (BCM) (see (Beck & Tetruashvili, 2011))
to solve it. Here with BCM we identify a large class of
methods that, in our setting, iterate steps of an optimization on C, with A fixed, followed by an optimization of A,
for C fixed.
A meta block coordinate algorithm to solve (S δ ) is reported
in in Alg. 1. Here we interpret each optimization step
over C as a supervised step, and each optimization step
over A as a an unsupervised step (in the sense that it involves the inputs but not the outputs). Several optimization
methods can be used as for S UPERVISED S TEP and U N SUPERVISED S TEP in Alg. 1. In particular, the term Block
Coordinate Descent (BCD) identifies a wide class of iterative methods that perform (typically inexact) minimization of the objective function one block of variables at the
time. Different strategies to choose which direction minimize at each step have been proposed: pre-fixed cyclic order, greedy search (Razaviyayn et al., 2013) or randomly,
according to a predetermined distribution (Nesterov, 2012).

Algorithm 1 C ONVEX M ULTI - TASK L EARNING
Input: K, Y,  tolerance, δ perturbation parameter, S objective functional of (S δ ), V loss, F structure penalty.
Initialize: (C, A) = (C0 , A0 ), t = 0
repeat
Ct+1 ← S UPERVISED S TEP (V, K, Y, Ct , At )
At+1 ← U NSUPERVISED S TEP(F, K, δ, Ct+1 , At )
t←t+1
until |S(Ct+1 , At+1 ) − S(Ct , At )| < 

For a review of several BCD algorithms we refer the reader
to (Razaviyayn et al., 2013) and references therein.
A second class of methods is called alternating minimization and corresponds to the situation where at each step in
Alg. 1 and exact minimization is performed. This latter approach is favorable when a closed form solution exists for
at least one block of variables (see Section 3.2.1) and has
been studied extensively in (Tseng, 2001) in the abstract
setting where an oracle provides a block-wise minimizer at
each iteration. The following Corollary describes the convergence properties of BCD and Alternate minimization sequences provided by applying Alg. 1 to (S δ ).
Corollary 3.4. Let the Problem (S δ ) be defined as in Theorem 3.3 then:
(a) Alternating Minimization: Let the two procedures
in Alg. 1 each provide a block-wise minimizer of the
functional with the other block held fixed. Then every
limiting point of a minimization sequence provided by
Alg. 1, is a global minimizer for (S δ ).
(b) Block Coordinate Descent: Let the two procedures in
Alg. 1 each consist in a single step of a first order optimization method (e.g. Projected Gradient Descent,
Proximal methods, etc.). Then every limiting point of
a minimizing sequence provided by Alg. 1 is a global
minimizer for (S δ ).
Corollary (3.4) follows by applying previous results on
BCD and Alternate minimization. In particular, for the
proof of part (a) we refer to Theorem 4.1 in (Tseng, 2001),
while for part (b) we refer to Theorem 2 in (Razaviyayn
et al., 2013).
In the following we discuss the actual implementation of
both S UPERVISED and U NSUPERVISED procedures in the
case where V is chosen to be least-squares loss and the
penalty F to be a spectral p-Schatten norm. This should
provide the reader with a practical example of how the
meta-algorithm introduced in this section can be specialized to a specific multi-task learning setting.
Remark 3.5. (Convergence of Block Coordinate Methods) Several works in multi-task learning have proposed

Convex Learning of Multiple Tasks and their Structure

some form of BCM strategy to solve the learning problem. However, up to our knowledge, so far only the authors
in (Argyriou et al., 2008a) have considered the issue of convergence to a global optimum. Their results where proved
for a specific choice of structure penalty in a framework
similar to that of problem (R) (see Section 4) but do not extend straightforwardly to other settings. Corollary 3.4 aims
to fill this gap, providing convergence guarantees for block
coordinate methods for a large class of multi-task learning
problems.
3.2.1. C LOSED F ORM SOLUTIONS FOR A LTERNATING
M INIMIZATION : E XAMPLES
Here we focus on the alternating minimization case and
discuss some settings in which it is possible to obtain a
closed form solution for the procedures S UPERVISED S TEP
and U NSUPERVISED S TEP.
(S UPERVISED S TEP) Least Squares. Let V be the least
squares loss and let the structure matrix A be fixed. A
closed form solution for the coefficient matrix C returned
by the S UPERVISED S TEP is (see for instance (Álvarez
et al., 2012)):
vec(C) = (IT ⊗ K + λA−1 ⊗ In )−1 vec(Y ),
with ⊗ the Kronecker product, and ∀M ∈ Rn×m ,
vec(M ) ∈ Rnm identifies the concatenation of the
columns of M .
(U NSUPERVISED S TEP) p-Schatten penalties. We consider the case in which F is chosen to be a spectral penalty
of the form F (·) = k·kpp with p ≥ 1. Also in this setting the
optimization problem has a closed form solution, as shown
in the following.
Proposition 3.6. Let the penalty of problem (S δ ) be F =
k · kpp with p ≥ 1. Then, for any C ∈ Rn×T fixed, the
optimization problem (S δ ) in the block variable A has a
minimizer of the form
q
(4)
AδC = p+1 (C > KC + δ 2 IT )/λ.
Proposition 3.6 generalizes a similar result originally
proved in in (Argyriou et al., 2008a) for the special case
p = 1 and provides an explicit formula for the U NSUPER VISED S TEP of Alg. 1. We report the proof in the supplementary material.

4. Previous Work: Comparison and
Discussion
Framework (Q) accounts for several choices of losses and
task-structural priors. While Sec. 3 has been devoted to

deriving optimization procedures to solve such a problem,
here we focus on modeling aspects. In particular, we will
briefly review some multi-task learning method previously
proposed, discussing how they can be formulated as special
cases of (Q) (or, equivalently, (R)).
Spectral Penalties. The penalty F = k · k2F was considered in (Dinuzzo et al., 2011), together with a least squares
loss function and the non convex problem (Q) is solved
directly by alternating minimization. However, as pointed
out in Sec. 3, solving the non convex problem (although
invex, see the discussion on Corollary 3.2) directly could
in principle become problematic when the alternating
minimization sequence gets close to the boundary of
T
Rn×T × S++
. A related idea is that of considering
F (A) = tr(A) (i.e. the 1-Schatten norm). This latter
approach can shown to be equivalent to the Multi-Task
Feature Learning setting of (Argyriou et al., 2008a) (see
supplementary material).
Cluster Tasks Learning. In (Jacob et al., 2008), the authors studied a multi-task setting where tasks are assumed
to be organized in a fixed number r of unknown disjoint
clusters. While the original formulation was conceived for
linear setting, it can be easily extended to non-linear kernels and cast in our framework. Let E ∈ {0, 1}T ×r be the
binary matrix whose entry Est has value 1 or 0 depending
on whether task s is in cluster t or not. Set M = I −E † E > ,
and U = T1 11> . In (Jacob et al., 2008) the authors considered a regularization setting of the form of (R) where the
structure matrix A is parametrized by the matrix M in order
to reflect the cluster structure of the tasks. More precisely:
A−1 (M ) = M U + B (M − U ) + W (I − M )
where the first term characterizes a global penalty on
the average of all tasks predictors, the second term penalizes the between-clusters variance, and the third term
controls the tasks variance within each cluster. Clearly,
it would be ideal to identify an optimal matrix A(M )
minimizing problem (R). However, M belongs to a
discrete non convex set, therefore authors propose a
convex relaxation by constraining M to be in a convex
T
set Sc = {M ∈ S+
, 0  M  I, tr(M ) = r}. In our
notations F (A) is therefore the indicator function over the
set of all matrices A = A(M ) such that M ∈ Sc . The
authors propose a pseudo gradient descent method to solve
the problem jointly.
Convex Multi-task Relation Learning. Starting from
a multi-task Gaussian Process setting, in (Zhang & Yeung, 2010), authors propose a model where the covariance
among the coefficient vectors of the T individual tasks is
T
controlled by a matrix A ∈ S++
in the form of a prior. The

Convex Learning of Multiple Tasks and their Structure
8

2
0

5

34

63

92

121 150

d

4
2
0

6

mean time (s)

4

6

8

MTFL Original
MTFL Alt. Min.

7
mean time (s)

6

8
MTCL Original
MTCL Alt. Min.

MTCL Original
MTCL Alt. Min.
mean time (s)

mean time (s)

8

5
4
3
2
1

5

34

63

92

121 150

T

5

34

63

92
d

121 150

MTFL Original
MTFL Alt. Min.

6
4
2
0

5

34

63

92

121 150

T

Figure 1. Comparison of the computational performance of the alternating minimization strategy studied in this paper with respect to the
optimization methods proposed for MTCL in (Jacob et al., 2008) and MTFL (Argyriou et al., 2008a) in the original papers. Experiments
are repeated for different number of tasks and input-space dimensions as described in Sec. 5.1.

initial maximum likelihood estimation problem is relaxed
to a convex optimization with target functional of the form
kY − KCk2F + λ1 tr(C > KC) + λ2 tr(A−1 C > KC) (5)
T
constrained to the set A = {A | A ∈ S++
, tr(A) = 1).
This setting is equivalent to problem (R) (by choosing F
to be the indicator function of A) with the addition of the
term tr(C > KC).
Non-Convex Penalties. Often times, interesting structural
assumptions require to impose non-convex penalties to recover interpretable relations among tasks. For instance (Argyriou et al., 2013) requires A to be a graph Laplacian,
or (Dinuzzo, 2013) imposes a low-rank factorization of A
in two smaller matrices. In (Mroueh et al., 2011; Kumar &
Daume III, 2012) different sparsity models are proposed.
Most of these methods can be naturally cast in the form (Q)
or (R). Unfortunately our analysis of the barrier method
does not necessarily hold and Alternating Minimization is
not guaranteed to lead to a stationary point.

5. Experiments
We empirically evaluated the efficacy of the block coordinate optimization strategy proposed in this paper on both
artificial and real datasets. Synthetic experiments were performed to assess the computational aspects of the approach,
while we evaluated the quality of solutions found by the
system on realistic settings.
5.1. Computational Times
As discussed in Sec. 4, several methods previously proposed in the literature, such as Multi-task Cluster Learning (MTCL) (Jacob et al., 2008) and Multi-task Feature
Learning (MTFL (Argyriou et al., 2008a)]), can be formulated as special cases of problem (Q) or (R). It is natural
to compare the proposed alternating minimization strategy
with the optimization solution originally proposed for each
method. To assess the system’s performance with respect
to varying dimensions of the feature space and an increas-

ing number of tasks, we chose to perform this comparison
in an artificial setting.
We considered a linear setting where the input data lie in Rd
and are distributed according to a normal distribution with
zero mean and identity covariance matrix. T linear models
wt ∈ Rd for t = 1, . . . , T were then generated according to
a normal distribution in order to sample T distinct training
(t) (t)
sets, each comprising of 30 examples (xi , yi ) such that
(t)
(t)
yi = hwt , xi i +  with  Gaussian noise with zero mean
and 0.1 standard deviation. On these learning problems we
compared the computational performance of our alternating minimization strategy and the original optimization algorithms originally proposed for MTCL and MTFL and for
which the code has been made available by the authors’. In
our algorithm we used A0 = I identity matrix as initialization for the alternating minimization procedure. We used a
least-squares loss for all experiments.
Figure 1 reports the comparison of computational times of
alternating minimization and the original methods to converge to the same minima (of respectively the functional of
MTCL and MTFL). We considered two settings: one where
the number of tasks was fixed to T = 100 and d increased
from 5 to 150 and a second one wher d was fixed to 100 and
T varied bewteen 5 and 150. To account for statistical stability we repeated the experiments for each couple (T, d)
and different choices of hyperparameters while generating
a new random datasets at each time. We can make two observations from these results: 1) in the setting where T is
kept fixed we observe a linear increase in the computational
times for both original MTCL and MTFL methods, while
alternating minimization is almost constant with respect to
the input space dimension. 2) When d is fixed and the number of tasks increases, all optimization strategies require
more time to converge. This shows that in general alternating minimization is a viable option to solve these problems
and in particular, when T << min(d, n) – which is often
the case in non-linear settings –this method is particularly
efficient.

Convex Learning of Multiple Tasks and their Structure
50 tr. samples per class

100 tr. samples per class

150 tr. samples per class

200 tr. samples per class

nMSE (± std)

nI

nMSE (± std)

nI

nMSE (± std)

nI

nMSE (± std)

STL

0.2436 ± 0.0268

0

0.1723 ± 0.0116

0

0.1483 ± 0.0077

0

0.1312 ± 0.0021

nI
0

MTFL

0.2333 ± 0.0213

0.0416

0.1658 ± 0.0107

0.0379

0.1428 ± 0.0083

0.0281

0.1311 ± 0.0055

0.0003

MTRL

0.2314 ± 0.0217

0.0404

0.1653 ± 0.0112

0.0401

0.1421 ± 0.0081

0.0288

0.1303 ± 0.0058

0.0071

OKL

0.2284 ± 0.0232

0.0630

0.1604 ± 0.0123

0.0641

0.1410 ± 0.0087

0.0350

0.1301 ± 0.0073

0.0087

Table 1. Comparison of Multi-task learning methods on the Sarcos dataset. The advantage of learning the tasks jointly decreases as more
training examples became available.

Accuracy (%) per # tr. samples per class

5.2. Real dataset

50

We assessed the benefit of adopting multi-task learning
approaches on two real dataset. In particular we considered the following algorithms: Single Task Learning (STL) as a baseline, Multi-task Feature Learning
(MTFL) (Argyriou et al., 2008a), Multi-task Relation
Learning (MTRL) (Zhang & Yeung, 2010), Output Kernel Learning (OKL) (Dinuzzo et al., 2011). We used least
squares loss for all experiments.
Sarcos. Sarcos2 is a dataset for regression problems (21dimensional inputs and 7 outputs), which report the corresponding torques measured at each joint.
For each task, we randomly sampled 50, 100, 150 and 200
training examples while we kept a test set of 5000 examples
in common for all tasks. We used a linear kernel and performed 5-fold crossvalidation to find the best regularization
parameter according to the normalized mean squared error
(nMSE) of predicted torques. We averaged the results over
10 repetitions of these experiments. The results, reported
in Table 1, show clearly that to adopt a multi-task approach
in this setting is favorable; however, in order to quantify
more clearly such improvement, we report in Table 1 also
the normalized improvement (nI) over single-task learning
(STL). For each multi-task method MTL, the normalized
improvement nI(MTL) is computed as the average
nI(MTL) =

1
nexp

nexp

X nMSEi (STL) − nMSEi (MTL)
p
nMSEi (STL) · nMSEi (MTL)
i=1

over all the nexp = 10 experiments of the normalized differences between the nMSE achieved by respectively the
STL approach and the given multi-task method MTL.
15-Scenes. 15-Scenes3 is a dataset designed for scene
recognition, consisting in a 15-class classification problem.
We represented images using LLC coding (Wang et al.,
2010) and trained the system on a training set comprising 50, 100 and 150 examples per class. The test set consisted in 7500 images evenly divided with respect to the
15 scenes. Table 2 reports the mean classification accuracy on 20 repetitions of the experiments. It can be noticed
that while all multi-task approach seem to achieve approx2
3

urlhttp://www.gaussianprocess.org/gpml/data/
http://www-cvr.ai.uiuc.edu/ponce grp/data/

100

150

STL

72.23

±0.04

76.61

±0.02

79.23

MTFL

73.23

±.08

77.24

±.05

80.11

±0.01
±.03

MTRL

73.13

±0.08

77.53

±0.04

80.21

±0.05

OKL

72.25

±0.03

77.06

±0.01

80.03

±0.01

Table 2. Classification results on the 15-scene dataset.

imately similar performance, these are consistently outperforming the STL baseline.

6. Conclusions
We have studied a general multi-task learning framework
where the tasks structure can be modeled compactly in
a matrix. For a wide family of models, the problem of
jointly learning the tasks and their relations can be cast as
a convex program, generalizing previous results for special
cases (Argyriou et al., 2008a; Dinuzzo et al., 2011). Such
an optimization can be naturally approached by block coordinate minimization, which can be seen as alternating between supervised and unsupervised learning steps optimizing respectively the tasks or their structure. We evaluated
our method real data, confirming the benefit of multi-task
learning when tasks share similar properties.
From an optimization perspective, future work will focus
on studying the theoretical properties of block coordinate
methods, in particular regarding convergence rates. Indeed, the empirical evidence we report suggests that similar strategies can be remarkably efficient in the multi-task
setting. From a modeling perspective, future work will focus on studying wider families of matrix-valued kernels,
overcoming the limitations of separable ones. Indeed, this
would allow to account also for structures in the interaction
space between the input and output domains jointly, which
is not the case for separable models.

7. Acknowledgments
This work was supported by the Center for Brains, Minds
and Machines (CBMM), funded by NSF STC award CCF1231216 and by the Italian Ministry of Education, University and Research FIRB project RBFR12M3AC.

Convex Learning of Multiple Tasks and their Structure

References
Álvarez, M., Lawrence, N., and Rosasco, L.
Kernels for vector-valued functions: a review. Foundations and Trends in Machine Learning, 4(3):195–
266, 2012. URL http://dx.doi.org/10.1561/
2200000036. see also http://arxiv.org/abs/1106.6251.
Argyriou, A., Evgeniou, T., and Pontil, M. Convex multitask feature learning. Machine Learning, 73, 2008a.
Argyriou, Andreas, Maurer, Andreas, and Pontil, Massimiliano. An algorithm for transfer learning in a heterogeneous environment. In ECML/PKDD (1), pp. 71–85,
2008b.

Fergus, Rob, Bernal, Hector, Weiss, Yair, and Torralba, Antonio. Semantic label sharing for learning with many
categories. European Conference on Computer Vision,
2010.
Jacob, Laurent, Bach, Francis, and Vert, Jean-Philippe.
Clustered multi-task learning: a convex formulation. Advances in Neural Information Processing Systems, 2008.
Joachims, Thorsten, Hofmann, Thomas, Yue, Yisong, and
Yu, Chun-Nam. Predicting structured objects with support vector machines. Commun. ACM, 52(11):97–104,
November 2009. ISSN 0001-0782. doi: 10.1145/
1592761.1592783. URL http://doi.acm.org/
10.1145/1592761.1592783.

Argyriou, Andreas, Clémençon, Stéphan, and Zhang, Ruocong. Learning the Graph of Relations Among Multiple
Tasks. Research report, October 2013. URL https:
//hal.inria.fr/hal-00940321.

Kumar, Abhishek and Daume III, Hal. Learning task
grouping and overlap in multi-task learning. arXiv
preprint arXiv:1206.6417, 2012.

Bakir, G. H., Hofmann, T., Scholkopf, B., Smola, A. J.,
Taskar, B., and Vishwanathan, S. V. N. Predicting structured data. MIT Press, 2007.

Lozano, A.C. and Sindhwani, V. Block variable selection
in multivariate regression and high-dimensional causal
inference. Advances in Neural Information Processing
Systems, 2011.

Bartlett, Peter L, Jordan, Michael I, and McAuliffe, Jon D.
Convexity, classification, and risk bounds. Journal
of the American Statistical Association, 101(473):138–
156, 2006.

Micchelli, C. A. and Pontil, M. Kernels for multi-task
learning. Advances in Neural Information Processing
Systems, 2004.

Beck, Amir and Tetruashvili, Luba. On the convergence of
block coordinate descent type methods. Technion, Israel
Institute of Technology, Haifa, Israel, Tech. Rep, 2011.
Boyd, Stephen Poythress and Vandenberghe, Lieven. Convex optimization. Cambridge university press, 2004.
Crammer, Koby and Singer, Yoram. On the learnability
and design of output codes for multiclass problems. In
In Proceedings of the Thirteenth Annual Conference on
Computational Learning Theory, pp. 35–46, 2000.
Craven, BD. Relations between invex properties. WORLD
SCIENTIFIC SERIES IN APPLICABLE ANALYSIS, 5:
25–34, 1995.
Dinuzzo, F., Ong, C. S., Gehler, P., and Pillonetto, G.
Learning output kernels with block coordinate descent.
International Conference on Machine Learning, 2011.

Minh, H. Q. and Sindhwani, V. Vector-valued manifold
regularization. International Conference on Machine
Learning, 2011.
Mroueh, Youssef, Poggio, Tomaso, and Rosasco, Lorenzo.
Multi-category and taxonomy learning: A regularization
approach. In NIPS Workshop on Challenges in Learning
Hierarchical Models: Transfer Learning and Optimization, 2011.
Mroueh, Youssef, Poggio, Tomaso, Rosasco, Lorenzo, and
Slotine, Jean-jeacques. Multiclass learning with simplex
coding. In Advances in Neural Information Processing
Systems, pp. 2789–2797, 2012.
Nesterov, Yu. Efficiency of coordinate descent methods
on huge-scale optimization problems. SIAM Journal on
Optimization, 22(2):341–362, 2012.

Dinuzzo, Francesco. Learning output kernels for multi-task
problems. Neurocomputing, 118:119–126, 2013.

Razaviyayn, Meisam, Hong, Mingyi, and Luo, Zhi-Quan.
A unified convergence analysis of block successive minimization methods for nonsmooth optimization. SIAM
Journal on Optimization, 23(2):1126–1153, 2013.

Evgeniou, Theodoros, Micchelli, Charles A, and Pontil,
Massimiliano. Learning multiple tasks with kernel methods. In Journal of Machine Learning Research, pp. 615–
637, 2005.

Sindhwani, Vikas, Lozano, Aurelie C., and Minh,
Ha Quang. Scalable matrix-valued kernel learning and
high-dimensional nonlinear causal inference. CoRR,
abs/1210.4792, 2012.

Convex Learning of Multiple Tasks and their Structure

Steinwart, Ingo and Christmann, Andreas. Support vector
machines. Springer, 2008.
Tseng, P. Convergence of block coordinate descent method
for nondifferentiable minimization. Journal of Optimization Theory and Applications, 109:475–494, 2001.
Tsochantaridis, Ioannis, Hofmann, Thomas, Joachims,
Thorsten, and Altun, Yasemin. Support vector machine
learning for interdependent and structured output spaces.
International Conference on Machine Learning, 2004.
Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., and Gong, Y.
Locality-constrained linear coding for image classification. In CVPR, 2010.
Zhang, Yu and Yeung, Dit-Yan. A convex formulation for
learning task relationships in multi-task learning. In Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-10),
pp. 733–742, Corvallis, Oregon, 2010. AUAI Press.

