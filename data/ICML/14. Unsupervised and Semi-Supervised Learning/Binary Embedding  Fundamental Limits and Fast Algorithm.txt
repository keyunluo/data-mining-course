Binary Embedding: Fundamental Limits and Fast Algorithm

Xinyang Yi
YIXY @ UTEXAS . EDU
Constantine Caramanis
CONSTANTINE @ UTEXAS . EDU
Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712
Eric Price
Department of Computer Science, The University of Texas at Austin, Austin, TX 78712

Abstract
Binary embedding is a nonlinear dimension reduction methodology where high dimensional
data are embedded into the Hamming cube while
preserving the structure of the original space.
Specifically, for an arbitrary N distinct points
in Sp−1 , our goal is to encode each point using m-dimensional binary strings such that we
can reconstruct their geodesic distance up to
δ uniform distortion. Existing binary embedding algorithms either lack theoretical guaran
tees or suffer from running time O mp . We
make three contributions: (1) we establish a
lower bound that shows any binary embedding
oblivious to the set of points requires m =
Ω( δ12 log N ) bits and a similar lower bound for
non-oblivious embeddings into Hamming distance; (2) we propose a novel fast binary embedding algorithm with provably
optimal bit com
plexity m = O δ12 log N and near linear run√
ning time O(p log p) whenever log N  δ p,
with a slightly worse running time for larger
log N ; (3) we also provide an analytic result
about embedding a general set of points K ⊆
Sp−1 with even infinite size. Our theoretical findings are supported through experiments on both
synthetic and real data sets.

1. Introduction
Low distortion embeddings that transform highdimensional points to low-dimensional space have played
an important role in dealing with storage, information retrieval and machine learning problems for modern datasets.
Perhaps one of the most famous results along these lines
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

ECPRICE @ CS . UTEXAS . EDU

is the Johnson-Lindenstrauss (JL) lemma Johnson &
Lindenstrauss (1984), which shows
that N points can be

embedded into a O δ −2 log N -dimensional space while
preserving pairwise Euclidean distance up to δ-Lipschitz
distortion. This δ −2 dependence has been shown to be
information-theoretically optimal Alon (2003). Significant
work has focused on fast algorithms for computing the
embeddings, e.g., (Ailon & Chazelle, 2006; Krahmer &
Ward, 2011; Ailon & Liberty, 2013; Cheraghchi et al.,
2013; Nelson et al., 2014).
More recently, there has been a growing interest in designing binary codes for high dimensional points with low distortion, i.e., embeddings into the binary cube (Weiss et al.,
2009; Raginsky & Lazebnik, 2009; Salakhutdinov & Hinton, 2009; Gong & Lazebnik, 2011; Yu et al., 2014). Compared to JL embedding, embedding into the binary cube
(also called binary embedding) has two advantages in practice: (i) As each data point is represented by a binary code,
the disk size for storing the entire dataset is reduced considerably. (ii) Distance in binary cube is some function of the
Hamming distance, which can be computed quickly using
computationally efficient bit-wise operators. As a consequence, binary embedding can be applied to a large number
of domains such as biology, finance and computer vision
where the data are usually high dimensional.
While most JL embeddings are linear maps, any binary embedding is fundamentally a nonlinear transformation. As
we detail below, this nonlinearity poses significant new
technical challenges for both upper and lower bounds. In
particular, our understanding of the landscape is significantly less complete. To the best of our knowledge, lower
bounds are not known; embedding algorithms for infinite
sets have distortion-dependence δ significantly exceeding
their finite-set counterparts; and perhaps most significantly,
there are no fast (near linear-time) embedding algorithms
with strong performance guarantees. As we explain below,
this paper contributes to each of these three areas. First, we
detail some recent work and state of the art results.

Binary Embedding: Fundamental Limits and Fast Algorithm

Recent Work. A common approach pursued by several existing works, considers the natural extension of JL embedding techniques via one bit quantization of the projections:
b(x) = sign(Ax),

(1.1)

where x ∈ Rp is input data point, A ∈ Rm×p is a projection matrix and b(x) is the embedded binary code. In particular, Jacques et al. (2011) shows when each entry of A is
generated independently from N (0, 1), with m > δ12 log N
it with high probability achieves at most δ (additive) distortion for N points. Work in Plan & Vershynin (2014) extend these results to arbitrary sets K ⊆ Sp−1 where |K|
can be infinite. They prove that the embedding with δdistortion can be obtained when m & w(K)2 /δ 6 where
w(K) is the Gaussian Mean Width of K. It is unknown
whether the unusual δ −6 dependence is optimal or not.
Despite provable sample complexity guarantees, one bit
quantization
of random projection as in (1.1), suffers from

O mp running time for a single point. This quadratic dependence can result in a prohibitive computational cost for
high-dimensional data. Analogously to the developments
in “fast” JL embeddings, there are several algorithms proposed to overcome this computational issue. Work in Gong
et al. (2013) proposes a bilinear projection method. By setting m = O(p), their method reduces the running time
from O(p2 ) to O(p1.5 ). More recently, work in Yu et al.
(2014) introduces a circulant randomprojection algorithm
that requires running time O p log p . While these algorithms have reduced running time, to the best of our knowledge, the measurement complexities of the two algorithms
are still unknown. Another line of work considers learning binary codes from data by solving certain optimization problems (Weiss et al., 2009; Salakhutdinov & Hinton, 2009; Norouzi et al., 2012; Yu et al., 2014). Unfortunately, there is no known provable bits complexity result
for these algorithms. It is also worth noting that Raginsky
& Lazebnik (2009) provide a binary code design for preserving shift-invariant kernels. Their method suffers from
the same quadratic computational issue compared with the
fully random Gaussian projection method.
Another related dimension reduction technique is locality
sensitive hashing (LSH) where the goal is to compute a discrete data structure such that similar points are mapped into
the same bucket with high probability (see, e.g., Andoni &
Indyk (2006)). The key difference is that LSH preserves
short distances, but binary embedding preserves both short
and far distances. For points that are far apart, LSH only
cares that the hashings are different while binary embedding cares how different they are.
Contributions of this paper. In this paper, we address
several unanswered problems about binary embedding:
1. We provide two lower bounds for binary embeddings.

The first shows that any method for embedding and
for recovering a distance estimate from the embedded points that is independent of the data being embedded must use Ω( δ12 log N ) bits. This is based on
a bound on the communication complexity of Hamming distance used by (Jayram & Woodruff, 2013)
for a lower bound on the “distributional” JL embedding. Separately, we give a lower bound for arbitrarily
data-dependent methods that embed into (any function of) the Hamming distance, showing such algorithms require m = Ω( δ2 log1(1/δ) log N ). This bound
is similar to Alon (2003) which gets the same result
for JL, but the binary embedding requires a different
construction.
2. We provide the first provable fast algorithm with

optimal measurement complexity O δ12 log N .
The proposed algorithm has running
time

O δ12 log 1δ log2 N log p log3 log N +p log p thus has
√
almost linear time complexity when log N . δ p.
Our algorithm is based on two key novel ideas. First,
our similarity is based on the median Hamming
distance of sub-blocks of the binary code; second, our
new embedding takes advantage of a pair-wise independence argument of Gaussian Toeplitz projection
that could be of independent interest.
3. For arbitrary set K ⊆ Sp−1 and the fully random
Gaussian projection algorithm, we prove that m =
O(w(K + )2 /δ 4 ) is sufficient to achieve δ uniform distortion. Here K + is an expanded set of K. Although
in general K ⊆ K + and hence w(K) ≤ w(K + ),
for interesting K such as sparse or low rank sets, one
can show w(K + ) = Θ(w(K))  p. Therefore applying our theory to these sets results in an improved
dependence on δ compared to a recent result in Plan
& Vershynin (2014). See Section 3.3 for a detailed
discussion.
Discussion. For the fast binary embedding, one simple solution, to the best of our knowledge not previously
stated, is to combine a Gaussian projection and the well
known results about fast JL. In detail, consider the strategy
b(x) = sign(AFx), where A is a Gaussian matrix and
F is any fast JL construction such as subsampled WalshHadamard matrix Rudelson & Vershynin (2008) or partial circulant matrix Krahmer et al. (2014) with column
flips. A simple analysis shows that this approach achieves
measurement complexity O( δ12 log N ) and running time
O( δ14 log2 N log p log3 log N + p log p) by following the
best known fast JL results. Our fast binary embedding algorithm builds on this simple but effective thought. Instead
of using a Gaussian matrix after the fast JL transform, we
use a series of Gaussian Toeplitz matrices that have fast
matrix vector multiplication. This novel construction im-

Binary Embedding: Fundamental Limits and Fast Algorithm

proves the running time by δ 2 while keeping measurement
complexity the same. In order for this to work, we need
to change the estimator from straight Hamming distance to
one based on the median of several Hamming distances.
An interesting point of comparison is Ailon & Rauhut
(2014), which considers “RIP-optimal” distributions that
give JL embeddings with optimal measurement complexity
O( δ12 log N ) and running time O(p log p). They show the
existence of such embeddings whenever log N < δ 2 p1/2−γ
for any constant γ > 0, which is essentially no better than
the bound given by the folklore method of composing a
Gaussian projection with a subsampled Fourier matrix. In
our binary setting, we show how to improve the region of
optimality by a factor of δ. It would be interesting to try
and translate this result back to the JL setting.
Notations. We use [n] to denote set {1, 2, . . . , n}. We use
xI to denote the sub-vector of x with index set I ⊆ [n].
We denote entry-wise vector multiplication as x  y =
(x1 y1 , x2 y2 , . . . , xn yn )> . For two random variables X, Y ,
we denote the statement that X and Y are independent as
X⊥Y . For two binary strings a, b ∈ {0, 1}m , we use
dH (a, b) to denote
Pmthe normalized Hamming distance, i.e.,
1
dH (a, b) := m
i=1 1(ai 6= bi ).






bi 6= ci if and only if Ai , x Ai , y < 0. The traditional metric in the embedded space has been the so-called
normalized Hamming distance defined as


m






1 X
dA (x, y) :=
1 sign Ai , x 6= sign Ai , y
.
m i=1
(2.2)
Definition 2.1. (δ-uniform Embedding) Given a set K ⊆
Sp−1 and projection matrix A ∈ Rm×p , we say the embedding b = sign(Ax) provides a δ-uniform embedding
for points in K if


dA (x, y) − d(x, y) ≤ δ, ∀ x, y ∈ K.
(2.3)
Note that unlike for JL, we aim to control additive error instead of relative error. Due to the inherently limited resolution of binary embedding, controlling relative error would
force the embedding dimension m to scale inversely with
the minimum distance of the original points, and in particular would be impossible for any infinite set.
2.2. Uniform Random Projection
Algorithm 1 Uniform Random Projection
|K|

2. Problem Setup and Preliminaries
In this section, we state our problem formally, give some
key definitions and present a simple (known) algorithm that
sets the stage for the main results of this paper.
2.1. Problem Setup
Given a set of p-dimensional points, our goal is to find a
transformation f : Rp 7→ {0, 1}m such that the Hamming
distance (or other related, easily computable metric) between two binary codes is close to their similarity in the
original space. We consider points on the unit sphere Sp−1
and use the normalized geodesic distance (occasionally,
and somewhat misleadingly, called cosine similarity) as the
input space similarity metric. For two points x, y ∈ Rp , we
use d(x, y) to denote the geodesic distance, defined as

input Finite number of points K = {xi }i=1 where K ⊆
Sp−1 , embedding target dimension m.
1: Construct matrix A ∈ Rm×p where each entry Ai,j is
drawn independently from N (0, 1).
2: for i = 1, 2, . . . , |K| do
3:
bi ← sign(Axi ).
4: end for
|K|
output {bi }i=1
Algorithm 1 presents (2.1) formally, when A is an i.i.d.
Gaussian random matrix, i.e., Ai ∼ N (0, Ip ), ∀ i ∈ [m].
It is easy to observe that for two fixed points x, y ∈ Sp−1
we have








	
E 1 sign Ai , x 6= sign Ai , y
= d(x, y).

(2.1)

(2.4)
The above equality has a geometric explanation: each Ai
actually represents a uniformly
distributed 
randomhyper


plane in Rp . Then sign Ai , x 6= sign Ai , y holds
if and only if hyperplane Ai intersects the arc between
x and y. In fact, dA (x, y) is equal to the fraction of
such hyperplanes. Under such uniform tessellation, the
probability with which the aforementioned event occurs is
d(x, y). Applying Hoeffding’s inequality and probabilistic
union bound over N pairs of points, we have the following
straightforward guarantee.

where A is some random projection matrix. Given two
points x, y with embedding vectors b, and c, we have

Proposition 2.2. Given a set K ⊆ Sp−1 with finite size
|K|, consider Algorithm 1 with m ≥ c(1/δ 2 ) log |K| for

d(x, y) :=

∠(x/kxk2 , y/kyk2 )
,
π

where ∠(·, ·) denotes the angle between two vectors. For
x, y ∈ Sp−1 , the metric d(x, y) is proportional to the
length of the shortest path connecting x, y on the sphere.
Given the success of JL embedding, a natural approach is
to consider the one bit quantization of a random projection:
b = sign(Ax),

Binary Embedding: Fundamental Limits and Fast Algorithm

some absolute constant c. Then with probability at least
1 − 2 exp(−δ 2 m), we have


dA (x, y) − d(x, y) ≤ δ, ∀ x, y ∈ K.
Three important questions remain unanswered: (i) Lower
Bounds – is the performance guaranteed by Proposition
2.2 optimal? (ii) Fast Embedding – whereas Algorithm 1
is quadratic (depending on the product mp), fast JL algorithms are nearly linear in p; does something similar exist for binary embedding? (iii) Infinite Sets – proposition
2.2 requires |K| is finite; what guarantee can we get for
|K| = ∞? The rest of the paper focuses on the three problems.

3. Main Results
We now present our main results on lower bounds, on fast
binary embedding, and finally, on a general result for infinite sets.
3.1. Lower Bounds
We offer two different lower bounds. The first shows
that any embedding technique that is oblivious to the input points must use Ω( δ12 log N ) bits, regardless of what
method is used to estimate geodesic distance from the
embeddings. This shows that uniform random projection
and our fast binary embedding achieve optimal bit complexity (up to constants). The bound follows from results
by (Jayram & Woodruff, 2013) on the communication complexity of Hamming distance.
Theorem 3.1. Consider any distribution on embedding
functions f : Sp−1 → {0, 1}m and reconstruction algorithms g : {0, 1}m × {0, 1}m → R such that for any
x1 , . . . , xN ∈ Sp−1 we have


g(f (xi ), f (xj )) − d(xi , xj ) ≤ δ
for all i, j ∈ [N ] with probability 1 − . Then m =
Ω( δ12 log(N/)).
One could imagine, however, that an embedding could use
knowledge of the input point set to embed any specific set
of points into a lower-dimensional space than is possible
with an oblivious algorithm. In the Johnson-Lindenstrauss
setting, Alon (2003) showed that this is not possible beyond (possibly) a log(1/δ) factor. We show the analogous
result for binary embeddings. Relative to Theorem 3.1, our
second lower bound works for data-dependent embedding
functions but loses a log(1/δ) and requires the reconstruction function to depend only on the Hamming distance between the two strings. This restriction is natural because an
unrestricted data-dependent reconstruction function could
simply encode the answers and avoid any dependence on δ.

With the scheme given in (2.1), choosing A as a fully random Gaussian matrix yields dA (x, y) ≈ d(x, y). However, an arbitrary binary embedding algorithm may not
yield a linear functional relationship between Hamming
distance and geodesic distance. Thus for this lower bound,
we allow the design of an algorithm with arbitrary link
function L.
Definition 3.2. (Data-dependent binary embedding problem)
Let L : [0, 1] → [0, 1] be a monotonic and continuous function. Given a set of points x1 , x2 , ..., xN ∈ Sp−1 , we say a
binary embedding mapping f solves the binary embedding
problem in terms of link function L, if for any i, j ∈ [N ],


dH (f (xi ), f (xj )) − L d(xi , xj )  ≤ δ.

(3.1)

Although the choice of L is flexible, note that for the same
point, we always have dH (f (xi ), f (xi )) = d(xi , xi ) = 0,
thus (3.1) implies L(0) < δ. We can just let L(0) = 0.
In particular, we let Lmax = L(1). We have the following
lower bound:
Theorem 3.3. There exist 2N points x1 , x2 , ..., x2N ∈
SN −1 such that for any binary embedding algorithm f on
{xi }2N
i=1 , if it solves the data-dependent binary embedding
problem defined in 3.2 in terms of link function L and any
δ ∈ (0, 161√e Lmax ), it must satisfy
m≥


2
1
Lmax
log N
.
max
128e
δ
log L2δ

(3.2)

Remark 3.4. We make two remarks for the above result.
(1) When Lmax is some constant, our result implies that for
general N points, any binary embedding algorithm (even
1
data-dependent ) must have Ω( δ2 log
1 log N ) number of
δ
measurements. This is analogous to Alon’s lower bound
in the JL setting. It is worth highlighting two differences:
(i) The JL setting considers the same metric (Euclidean
distance) for both the input and the embedded spaces. In
binary embedding, however, we are interested in showing
the relationship between Hamming distance and geodesic
distance. (ii) Our lower bound is applicable to a broader
class of binary embedding algorithms as it involves arbitrary, even data-dependent, link function L. Such an extension is not considered in the lower bound of JL. (2) The
stated lower bound only depends on Lmax and does not depend on any
√ curvature information of L. The constraint
Lmax > 16 eδ is critical for our lower bound to hold, but
some such restriction is necessary because for Lmax < δ,
we are able to embed all points into just one bit. In this
case dH (f (xi ), f (xj )) = 0 for all pairs and condition (3.1)
would hold trivially.

Binary Embedding: Fundamental Limits and Fast Algorithm

3.2. Fast Binary Embedding
In this section, we present a novel fast binary embedding
algorithm. We then establish its theoretical guarantees.
There are two key ideas that we leverage: (i) instead of
normalized Hamming distance, we use a related metric,
the median of the normalized Hamming distance applied
to sub-blocks; and (ii) we show a key pair-wise independence lemma for partial Gaussian Toeplitz projection, that
allows us to use a concentration bound that then implies
nearness in the median-metric we use.
3.2.1. M ETHOD
Our algorithm builds on sub-sampled Walsh-Hadamard
matrix and partial Gaussian Toeplitz matrices with random column flips. In particular, an m-by-p partial WalshHadamard matrix has the form
Φ := P · H · D.

(3.3)

The above construction has three components. We characterize each term as follows: Term D is a p-by-p diagonal matrix with diagonal terms {ζi }pi=1 that are drawn
from i.i.d. Rademacher sequence, i.e, for any i ∈ [p],
Pr(ζi = 1) = Pr(ζi = −1) = 1/2. Term H is a p-byp scaled Walsh-Hadamard matrix such that H> H = Ip .
Term P is an m-by-p sparse matrix where one entry of each
row is set to be 1 while the rest are 0. The nonzero coordinate of each row is drawn independently from uniform
distribution. In fact, the role of P is to randomly select p
rows of H · D.
An m-by-n partial Gaussian Toeplitz matrix has the form
Ψ := P · T · D.

(3.4)

We introduce each term as follows: Term D a is n-by-n diagonal matrix with diagonal terms {ζi }ni=1 that are drawn
from i.i.d. Rademacher sequence. Term T is a n-by-n
Toeplitz matrix constructed from (2n−1)-dimensional vector g such that Ti,j = gi−j+n for any i, j ∈ [n]. In particular, g is drawn from N (0, I2n−1 ). Term P is an m-by-n
sparse matrix where Pi = e>
i for any i ∈ [m]. Equivalently, we use P to select the first m rows of TD. It’s
worth to note we actually only need to select any distinct
m rows.
With the above constructions in hand, we present our fast
algorithm in Algorithm 2. At a high level, Algorithm 2
consists of two parts: First, we apply column flipped partial Hadamard transform to convert p-dimensional point
into n-dimensional intermediate point. Second, we use B
independent (m/B)-by-n partial Gaussian Toeplitz matrices and sign operator to map an intermediate point into
B blocks of binary codes. In terms of similarity computation for the embedded codes, we use the median of

each block’s normalized Hamming distance. In detail, for
b, c ∈ {0, 1}m , B-wise normalized Hamming distance is
defined as



 B−1
dH (b, c; B) := median
dH bTi , cTi
(3.5)
i=0

where Ti = [i + 1, . . . , i + m/B].
It is worth noting that our first step is one construction of
fast JL transform. In fact any fast JL transform would work
for our construction, but we choose a standard one with real
value: based on Rudelson & Vershynin (2008); Cheraghchi
et al. (2013); Krahmer & Ward (2011), it is known that
with m = O −2 log N log p log3 (log N ) measurements,
a subsampled Hadamard matrix with column flips becomes
an -JL matrix for N points.
The second part of our algorithm follows framework (2.1).
By choosing a Gaussian random vector in each row of Ψ,
from our previous discussion in Section 2.2, the probability
that such a hyperplane intersects the arc between two points
is equal to their geodesic distance. Compared to a fully
random Gaussian matrix, as used in Algorithm 1, the key
difference is that the hyperplanes represented by rows of Ψ
are not independent to each other; this imposes the main
analytical challenge.
Algorithm 2 Fast Binary Embedding
input Finite number of points {xi }N
i=1 where each point
xi ∈ Sp−1 , embedded dimension m, intermediate dimension n, number of blocks B.
1: Draw a n-by-p sub-sampled Walsh-Hadamard matrix
Φ according to (3.3). Draw B independent par
	B
tial Gaussian Toeplitz matrices Ψ(j) j=1 with size
(m/B)-by-n according to (3.4).
2: {Part I: Fast JL}
3: for i = 1, 2, . . . , N do
4:
yi ← Φ · xi .
5: end for
6: {Part II: Partial Gaussian Toeplitz Projection}
7: for i = 1, 2, . . . , N do
8:
for j = 1, 2, . . . , B do 
9:
cj ← sign Ψ(j) · yi .
10:
end for
11:
bi ← [c1 ; c2 ; . . . ; cB ]
12: end for
output {bi }N
i=1
3.2.2. A NALYSIS
We give the analysis for Algorithm 2. We first review a
well known result about fast JL transform. It can be proved
by combining Theorem 14 in Cheraghchi et al. (2013) and
Theorem 3.1 in Krahmer & Ward (2011).

Binary Embedding: Fundamental Limits and Fast Algorithm

Lemma 3.5. Consider the column flipped partial
Hadamard matrix defined in (3.3) with size mby-p. pFor N points x1 , x2 , ..., xN ∈ Sp−1 , let
p
yi =
m Φ(ζ) · xi , ∀ i ∈ [N ]. For some absolute
constant c, suppose m ≥ cδ −2 log N log p log3 (log N ),
then with probability at least 0.99, we have that


kyi k2 − 1 ≤ δ, for any i ∈ [N ];
(3.6)


kyi − yj k2 − kxi − xj k2  ≤ δkxi − xj k2

(3.7)

for any i, j ∈ [N ].
The above result suggests that the first part of our algorithm
reduces the dimension while preserving well the Euclidean
distance of each pair. Under this condition, all the pairwise
geodesic distances are also well preserved as confirmed by
the following result.
Lemma 3.6. Consider the set of embedded points {yi }N
i=1
defined in Lemma 3.5. Suppose conditions (3.7)-(3.6) hold
with δ > 0. Then for any i, j ∈ [N ],


d(yi , yj ) − d(xi , xj ) ≤ Cδ
(3.8)
holds with some absolute constant C.
The next result is our independence lemma, and is one of
the key technical ideas that make our result possible. The
result shows that for any fixed x, Gaussian Toeplitz projection (with column flips) plus sign(·) generate pair-wise
independent binary codes.
Lemma 3.7. Let g ∼ N (0, I2n−1 ), ζ = {ζi }ni=1 be an
i.i.d. Rademacher sequence. Let T be a random Toeplitz
matrix constructed from g such that Ti,j = gi−j+n . Consider any two distinct rows of T say ξ, ξ 0 . For any two
fixed vectors x, y ∈ Rn , we define the following random
variables






X = sign ξ  ζ, x , X 0 = sign ξ 0  ζ, x ;






Y = sign ξ  ζ, y , Y 0 = sign ξ 0  ζ, y .
We have that X⊥X 0 , X⊥Y 0 , Y ⊥X 0 , Y ⊥Y 0 .
With these ingredients in hand, we are ready to prove the
following result.
Theorem 3.8. Consider Algorithm 2 with random matrices Φ, Ψ defined in (3.3) and (3.4) respectively. For finite
number of points {xi }N
i=1 , let bi be the binary codes of xi
generated by Algorithm 2. Suppose we set
B ≥ c log N, n ≥ m/B ≥ c00 (1/δ 2 ),
and n ≥ c0 (1/δ 2 ) log N log p log3 (log N )

Algorithm 3 Alternative Fast Binary Embedding
input Finite number of points {xi }N
i=1 where each point
xi ∈ Sp−1 , embedded dimension m, intermediate dimension n.
1: Draw a n-by-p sub-sampled Walsh-Hadamard matrix Φ according to (3.3). Construct m-by-n matrix A where each entry is drawn independently from
N (0, 1).
2: for i = 1, 2, . . . , N do
3:
bi ← sign(AΦxi )
4: end for
output {bi }N
i=1
with some absolute constants c, c0 , c00 , then with probability
at least 0.98, we have that for any i, j ∈ [N ]


dH (bi , bj ; B) − d(xi , xj ) ≤ δ.
Similarity metric dH (·, ·; B) is the median of normalized
Hamming distance defined in (3.5).
The above result suggests that the measurement
complex
ity of our fast algorithm is O δ12 log N which matches the
performance of Algorithm 1 based on fully random matrix. Note that this measurement complexity can not be improved significantly by any data-oblivious binary embedding with any similarity metric, as suggested by Theorem
3.1.
Running time: The first part of our algorithm takes time
O p log p . Generating a single block of binary codes

from partial Toeplitz matrix takes time O n log( 1δ) 1 .
Thus the total running time is O Bn log 1δ + p log p =

O δ12 log 1δ log2 N log p log3 (log N ) + p log p . By ignoring the polynomial log log factor,qthe second term

O p log p dominates when log N . δ p/ log 1δ .
Comparison to an alternative algorithm: Instead of utilizing the partial Gaussian Toeplitz projection, an alternative method, to the best of our knowledge not previously
stated, is to use fully random Gaussian projection in the
second part of our algorithm. We present the details in Algorithm 3. By combining Proposition 2.2 and Lemma 3.5,
it is straightforward to show this algorithm still
 achieves the
same measurement complexity O δ12 log N . The corre2
1
sponding
p log3 (log N ) +
 running time is O δ4 log N 2log
√
p log p , so it is fast when log N . δ p. Therefore our
algorithm has an improved dependence on δ. This improvement comes from fast multiplication of partial Toeplitz
matrix and a pair-wise independence argument shown in
Lemma 3.7.
1
Matrix-vector multiplication for m-by-n partialToeplitz matrix can be implemented in running time O n log m .

Binary Embedding: Fundamental Limits and Fast Algorithm

3.3. δ-uniform Embedding for General K
In this section, we turn back to the fully random projection
binary embedding (Algorithm 1). Recall that in Proposition 2.2, we show for finite size K, m = O( δ12 log |K|)
measurements are sufficient to achieve δ-uniform embedding. For general K, the challenge is that there might be an
infinite number of distinct points in K, so Proposition 2.2
cannot be applied. In proving the JL lemma for an infinite
set K, the standard technique is either constructing an net of K or reducing the distortion to the deviation bound
of a Gaussian process. However, due to the non-linearity
essential for binary embedding, these techniques cannot be
directly extended to our setting. Therefore strengthening
Proposition 2.2 to infinite size K imposes significant technical challenges. Before stating our result, we first give
some definitions.
Definition 3.9. (Gaussian mean width) Let g ∼ N (0, Ip ).
For any set K ⊆ Sp−1 , the Gaussian mean width of K is
defined as



w(K) := Eg sup  g, x .
x∈K

Here, w(K)2 measures the effective dimension of set K.
In the trivial case K = Sp−1 , we have w(K)2 . p.
However, when K has some special structure, we may
have w(K)2  p. For instance, when K = {x ∈
Sp−1
p : | supp(x)| ≤ s}, it has been shown that w(K) =
Θ( s log(p/s)) (see Lemma 2.3 in Plan & Vershynin
(2013)).
For a given δ, we define Kδ+ , the expanded version of K ⊆
Sp−1 as:
[
x−y
Kδ+ := K
z ∈ Sp−1 : z =
,
kx − yk2
	
∀ x, y ∈ K if δ 2 ≤ kx − yk2 ≤ δ .
(3.9)
-0.1in In other words, Kδ+ is constructed from K by adding
the normalized differences between pairs of points in K
that are within δ but not closer than δ 2 . Now we state the
main result as follows.
Theorem 3.10. Consider any K ⊆ Sp−1 . Let A ∈
Rm×p be an i.i.d. Gaussian matrix where each row Ai ∼
N (0, Ip ). For any two points x, y ∈ K, dA (x, y) is defined in (2.2). Expanded set Kδ+ is defined in (3.9). When
m≥c

w(Kδ+ )2
,
δ4

with some absolute constant c, then we have that


sup dA (x, y) − d(x, y) ≤ δ
x,y∈K

holds with probability at least 1 − c1 exp(−c2 δ 2 m) where
c1 , c2 are absolute constants.

Remark 3.11. We compare the above result to Theorem
1.5 from the recent paper (Plan & Vershynin, 2014) where
it is proved that for m & w(K)2 /δ 6 , Algorithm 1 is guaranteed to achieve δ-uniform embedding for general K.
Based on definition (3.9), we have
w(K) ≤ w(Kδ+ ) ≤

1
1
w(K − K) . 2 w(K).
2
δ
δ

Thus in the worst case, Theorem 3.10 recovers the previous
result up to a factor δ12 . More importantly, for many interesting sets one can show w(Kδ+ ) . w(K); in such cases,
our result leads to an improved dependence on δ. We give
several such examples as follows:
• Low rank set. For some U ∈ Rp×r such that
U> U = Ir , let K = {x ∈ Sp−1 : x = Uc, ∀ √
c∈
Sr−1 }. We simply have K = Kδ+ and w(K) . r.
Our result implies m = O r/δ 4 .
• Sparse set. K = {x ∈ Sp−1 : | supp(x)| ≤ s}. In
p−1
this case we have Kδ+ ⊆ {x ∈
: | supp(x)| ≤
pS
+
2s}. Therefore w(Kδ ) = Θ( s log(p/s)). Our re
sult implies m = O s log(p/s)
.
δ4
• p
Set with finite size. |K| < ∞. As w(K) .
log |K| and |Kδ+ | ≤ 2|K|, our result implies m =
O log |K|/δ 4 . We thus recover Proposition 2.2 up
to factor 1/δ 2 .
Applying the result from Plan & Vershynin (2014) to the
above sets implies similar results but the dependence on δ
becomes 1/δ 6 .

4. Numerical Results
In this section, we present the results of experiments we
conduct to validate our theory and compare the performance of the following three algorithms we discussed: uniform random projection (URP) (Algorithm 1), fast binary
embedding (FBE) (Algorithm 2) and the alternative fast binary embedding (FBE-2) (Algorithm 3). We first apply
these algorithms to synthetic datasets. In detail, given parameters (N, p), a synthetic dataset is constructed by sampling N points from Sp−1 uniformly at random. Recall that
δ is the maximum embedding distortion among all pairs of
points. We use m to denote the number of binary measurements. Algorithm FBE needs parameters n, B, which
are intermediate dimension and number of blocks respectively. Based on Theorem 3.8, n is required to be proportional to m (up to some logarithmic factors) and B is required to be proportional to log N . We thus set n ≈ 1.3m,
B ≈ 1.8 log N . We also set n ≈ 1.3m for FBE-2. In addition, we fix p = 512. We report our first result showing
the functional relationship between (m, N, δ) in Figure 1.
In particular, panel 1(a) shows the the change of distortion

Binary Embedding: Fundamental Limits and Fast Algorithm

1

1
0.8

0.4
0.2

20

40

60

80

100

Number of retrieved images

(a) m = 5000

0.6

Recall

FBE
FBE-2
FBE-2(same time)
URP
URP(same time)

Recall

Recall

0.8
0.6

FBE
FBE-2
FBE-2(same time)
URP
URP(same time)

0.4
20

40

60

80

0.8

FBE
FBE-2
FBE-2(same time)
URP
URP(same time)

0.6

0.4

100

20

Number of retrieved images

40

60

80

100

Number of retrieved images

(b) m = 10000

(c) m = 15000

Figure 2. Image retrieval results on Flickr-25600. Each panel presents the recall for specified number of measurements m. Black and
blue dot lines are respectively the recall of FBE-2 and URP with less number of measurements but the same running time as FBE.

δ over the number of measurements m for fixed N . We
observe that, for all the three algorithms, δ decays with m
at the rate predicted by Proposition 2.2 and Theorem 3.8.
Panel 1(b) shows the empirical relationship between m and
log N for fixed δ. As predicted by our theory (lower bound
and upper bound), m has a linear dependence on log N .

0.55
0.5
0.45
0.4

δ

FBE
FBE-2
URP

0.35
0.3
0.25
0.2
50

100

150

200

250

m

(a) N = 300
300
250
200

m

A popular application of binary embedding is image retrieval, as considered in (Gong & Lazebnik, 2011; Gong
et al., 2013; Yu et al., 2014). We thus conduct an experiment on the Flickr-25600 dataset that consists of 10k images from Internet. Each image is represented by a 25600dimensional normalized Fisher vector. We take 500 randomly sampled images as query points and leave the rest
as base for retrieval. The relevant images of each query are
defined as its 10 nearest neighbors based on geodesic distance. Given m, we apply FBE, FBE-2 and URP to convert
all images into m-dimensional binary codes. In particular,
we set B = 10 for FBE and n ≈ 1.3m for FBE and FBE2. Then we leverage the corresponding similarity metrics,
(3.5) for FBE and Hamming distance for FBE-2 and URP,
to retrieve the nearest images for each query. The performance of each algorithm is characterized by recall, i.e., the
number of retrieved relevant images divided by the total
number of relevant images. We report our second result
in Figure 2. Each panel shows the average recall of all
queries for a specified m. We note that FBE-2, as a fast
algorithm, performs as well as URP with the same number of measurements. In order to show the running time
advantage of our fast algorithm FBE, we also present the
performance of FBE-2 and URP with fewer measurements
such that they can be computed with the same time as FBE.
As we observe, with large number of measurements, FBE2 and URP perform marginally better than FBE while FBE
has a significant improvement over the two algorithms under identical time constraint.

150
100

FBE
FBE-2
URP

50
5

6

7

8

log N

(b) δ = 0.3
Figure 1. Results on synthetic datasets. (a) Each point, along with
the standard deviation represented by the error bar, is an average
of 50 trials each of which is based on a fresh synthetic dataset
with size N = 300 and newly constructed embedding mapping.
(b) Each point is computed by slicing at δ = 0.3 in similar plots
like (a) but with the corresponding N .

Binary Embedding: Fundamental Limits and Fast Algorithm

Acknowledgments
C. Caramanis and X. Yi were supported by NSF grants
1056028, 1302435 and 1116955. This research was also
partially supported by the U.S. Department of Transportation through the Data-Supported Transportation Operations
and Planning (D-STOP) Tier 1 University Transportation
Center.

References
Ailon, Nir and Chazelle, Bernard. Approximate nearest
neighbors and the fast johnson-lindenstrauss transform.
In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pp. 557–563. ACM, 2006.
Ailon, Nir and Liberty, Edo. An almost optimal unrestricted fast johnson-lindenstrauss transform. ACM
Transactions on Algorithms (TALG), 9(3):21, 2013.
Ailon, Nir and Rauhut, Holger. Fast and rip-optimal transforms. Discrete & Computational Geometry, 52(4):780–
798, 2014.
Alon, Noga. Problems and results in extremal combinatoricsâĂŤi. Discrete Mathematics, 273(1):31–53, 2003.
Andoni, Alexandr and Indyk, Piotr. Near-optimal hashing
algorithms for approximate nearest neighbor in high dimensions. In Foundations of Computer Science, 2006.
FOCS’06. 47th Annual IEEE Symposium on, pp. 459–
468. IEEE, 2006.
Cheraghchi, Mahdi, Guruswami, Venkatesan, and Velingker, Ameya. Restricted isometry of fourier matrices
and list decodability of random linear codes. SIAM Journal on Computing, 42(5):1888–1914, 2013.
Gong, Yunchao and Lazebnik, Svetlana. Iterative quantization: A procrustean approach to learning binary codes.
In Computer Vision and Pattern Recognition (CVPR),
2011 IEEE Conference on, pp. 817–824, 2011.
Gong, Yunchao, Kumar, Sanjiv, Rowley, Henry A, and
Lazebnik, Svetlana. Learning binary codes for highdimensional data using bilinear projections. In Computer
Vision and Pattern Recognition (CVPR), 2013 IEEE
Conference on, pp. 484–491. IEEE, 2013.
Jacques, Laurent, Laska, Jason N, Boufounos, Petros T,
and Baraniuk, Richard G. Robust 1-bit compressive
sensing via binary stable embeddings of sparse vectors.
arXiv preprint arXiv:1104.3160, 2011.
Jayram, TS and Woodruff, David P. Optimal bounds for
johnson-lindenstrauss transforms and streaming problems with subconstant error. ACM Transactions on Algorithms (TALG), 9(3):26, 2013.

Johnson, William B and Lindenstrauss, Joram. Extensions
of lipschitz mappings into a hilbert space. Contemporary
mathematics, 26(189-206):1, 1984.
Krahmer, Felix and Ward, Rachel. New and improved
johnson-lindenstrauss embeddings via the restricted
isometry property. SIAM Journal on Mathematical Analysis, 43(3):1269–1281, 2011.
Krahmer, Felix, Mendelson, Shahar, and Rauhut, Holger.
Suprema of chaos processes and the restricted isometry
property. Communications on Pure and Applied Mathematics, 67(11):1877–1904, 2014.
Ledoux, Michel and Talagrand, Michel. Probability in Banach Spaces: isoperimetry and processes, volume 23.
Springer, 1991.
Nelson, Jelani, Price, Eric, and Wootters, Mary. New constructions of rip matrices with fast multiplication and
fewer rows. In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, pp.
1515–1528. SIAM, 2014.
Norouzi, Mohammad, Blei, David M, and Salakhutdinov,
Ruslan. Hamming distance metric learning. In Advances
in Neural Information Processing Systems, pp. 1061–
1069, 2012.
Plan, Yaniv and Vershynin, Roman. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. Information Theory, IEEE
Transactions on, 59(1):482–494, 2013.
Plan, Yaniv and Vershynin, Roman. Dimension reduction
by random hyperplane tessellations. Discrete & Computational Geometry, 51(2):438–461, 2014.
Raginsky, Maxim and Lazebnik, Svetlana. Localitysensitive binary codes from shift-invariant kernels. In
Advances in neural information processing systems, pp.
1509–1517, 2009.
Rudelson, Mark and Vershynin, Roman. On sparse reconstruction from fourier and gaussian measurements. Communications on Pure and Applied Mathematics, 61(8):
1025–1045, 2008.
Salakhutdinov, Ruslan and Hinton, Geoffrey. Semantic
hashing. International Journal of Approximate Reasoning, 50(7):969–978, 2009.
Weiss, Yair, Torralba, Antonio, and Fergus, Rob. Spectral
hashing. In Advances in neural information processing
systems, pp. 1753–1760, 2009.
Yu, Felix X, Kumar, Sanjiv, Gong, Yunchao, and Chang,
Shih-Fu. Circulant binary embedding. arXiv preprint
arXiv:1405.3162, 2014.

