Coordinate-descent for learning orthogonal matrices through Givens rotations

Uri Shalit
URI . SHALIT @ MAIL . HUJI . AC . IL
ICNC-ELSC & Computer Science Department, The Hebrew University of Jerusalem, 91904 Jerusalem Israel
The Gonda Brain Research Center, Bar Ilan University, 52900 Ramat-Gan, Israel
Gal Chechik
The Gonda Brain Research Center, Bar Ilan University, 52900 Ramat-Gan, Israel

Abstract
Optimizing over the set of orthogonal matrices
is a central component in problems like sparsePCA or tensor decomposition. Unfortunately,
such optimization is hard since simple operations
on orthogonal matrices easily break orthogonality, and correcting orthogonality usually costs a
large amount of computation.
Here we propose a framework for optimizing orthogonal matrices, that is the parallel of
coordinate-descent in Euclidean spaces. It is
based on Givens-rotations, a fast-to-compute operation that affects a small number of entries in
the learned matrix, and preserves orthogonality.
We show two applications of this approach: an algorithm for tensor decompositions used in learning mixture models, and an algorithm for sparsePCA. We study the parameter regime where a
Givens rotation approach converges faster and
achieves a superior model on a genome-wide
brain-wide mRNA expression dataset.

1. Introduction
Optimization over orthogonal matrices – matrices whose
rows and columns form an orthonormal basis of Rd – is
central to many machine learning optimization problems.
Prominent examples include Principal Component Analysis (PCA), Sparse PCA, and Independent Component Analysis (ICA). In addition, many new applications of tensor orthogonal decompositions were introduced recently, including Gaussian Mixture Models, Multi-view Models and Latent Dirichlet Allocation (e.g., Anandkumar et al. (2012a);
Hsu & Kakade (2013)).
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

GAL . CHECHIK @ BIU . AC . IL

A major challenge when optimizing over the set of orthogonal matrices is that simple updates such as matrix addition usually break orthonormality. Correcting by orthonormalizing a matrix V ∈ Rd×d is typically a costly procedure: even a change to a single element of the matrix,
may require O(d3 ) operations in the general case for reorthogonalization.
In this paper, we present a new approach for optimization
over the manifold of orthogonal matrices, that is based on
a series of sparse and efficient-to-compute updates that operate within the set of orthonormal matrices, thus saving
the need for costly orthonormalization. The approach can
be seen as the equivalent of coordinate descent in the manifold of orthonormal matrices. Coordinate descent methods
are particularly relevant for problems that are too big to fit
in memory, for problems where one might be satisfied with
a partial answer, or in problems where not all the data is
available at one time (Richtárik & Takáč, 2012).
We start by showing that the orthogonal-matrix equivalent
of a single coordinate update is applying a single Givens
rotation to the matrix. In section 3 we prove that for a
differentiable objective the procedure converges to a local
optimum under minimal conditions, and prove an O(1/T )
convergence rate for the norm of the gradient. Sections 4
and 5 describe two applications: (1) sparse PCA, including
a variant for streaming data; (2) a new method for orthogonal tensor decomposition. We study how the performance
of the method depends on the problems hyperparameters
using synthetic data, and demonstrate that it achieves superior accuracy on an application of sparse-PCA for analyzing gene expression data.

2. Coordinate descent on the orthogonal
matrix manifold
Coordinate descent (CD) is an efficient alternative to gradient descent when the cost of computing and applying a
gradient step at a single coordinate is small relative to com-

Coordinate-descent for learning orthogonal matrices through Givens rotations

puting the full gradient. In these cases, convergence can be
achieved with a smaller number of computing operations,
although using a larger number of (faster) steps.
Applying coordinate descent to optimize a function involves choosing a coordinate basis, usually the standard
basis. Then calculating a directional derivative in the direction of one of the coordinates. And finally, updating the
iterate in the direction of the chosen coordinate. To generalize CD to operate over the set of orthogonal matrices, we
need to generalize these ideas of directional derivatives and
updating the orthogonal matrix in a “straight direction”.
In the remaining of this section, we introduce the set of
orthogonal matrices, Od , as a Riemannian manifold. We
then show that applying coordinate descent to the Riemannian gradient amounts to multiplying by Givens rotations.
Throughout this section and the next, the objective function
is assumed to be a differentiable function f : Od → R.
2.1. The orthogonal manifold and Riemannian gradient
The orthogonal matrix manifold Od is the set of d×d matridimences U such that U U T = U T U = Id . It is a d(d+1)
2
sional smooth manifold, and is an embedded submanifold
of the Euclidean space Rd×d (Absil et al., 2009).
Each point U ∈ Od has a tangent space associated with it,
dimensional vector space, that we will use below
a d(d−1)
2
in order to capture the notion of ’direction’ on the manifold. The tangent space is denoted TU Od , and defined
by TU Od = {Z ∈ Rd×d , Z = U Ω : Ω = −ΩT } =
U Skew(d), where Skew(d) is the set of skew-symmetric
d × d matrices.
2.1.1. G EODESIC DIRECTIONS
The natural generalization of straight lines to the manifold
context are geodesic curves. A geodesic curve is locally
the shortest curve between two points on the manifold, or
equivalently, a curve with no acceleration tangent to the
manifold (Absil et al., 2009). For a point U ∈ Od and a
“direction” U Ω ∈ TU Od there exists a single geodesic line
that passes through U in direction Ω. Fortunately, while
computing a geodesic curve in the general case might be
hard, computing it for Od has a closed form expression:
γ : (−1, 1) → Od , γ(θ) = U Expm(θΩ), where γ(θ) with
θ ∈ (−1, 1) is the parameterization of the curve, and Expm
is the matrix exponential function.
In the special case where the operator Expm(Ω) is applied
to a skew-symmetric matrix Ω, it maps Ω into an orthogonal matrix 1 . As a result, γ(θ) = U Expm(θΩ) is also an
orthogonal matrix for all θ.
Because Expm(Ω)Expm(Ω)T = Expm(Ω)Expm(ΩT ) =
Expm(Ω)Expm(−Ω) = I
1

2.1.2. T HE DIRECTIONAL DERIVATIVE
In analogy to the Euclidean case, the Riemannian directional derivative of f in the direction of a vector U Ω ∈
TU Od is defined as the derivative of a single variable
function which involves looking at f along a single curve
(Absil et al., 2009):


d
d


=
f (γ(θ))
f (U Expm(θΩ)) .
dθ
dθ
θ=0
θ=0
(1)
Note that ∇Ω f (U ) is a scalar. The definition means that the
directional derivative is f ′ with f restricted to the geodesic
curve going through U in the direction U Ω.
∇Ω f (U ) ≡

2.1.3. T HE DIRECTIONAL UPDATE
Since the Riemannian equivalent of walking in a straight
line is walking along the geodesic curve, taking a step of
size η > 0 from a point U ∈ Od in direction U Ω ∈ TU Od
amounts to:
Unext = U Expm (ηΩ) ,
(2)
We also have to define the orthogonal basis for Skew(d).
Here we use {ei eTj − ej eTi : 1 ≤ i < j ≤ d}. We denote
each basis vector as Hij = ei eTj − ej eTi , 1 ≤ i < j ≤ d.
2.2. Givens rotations as coordinate descent
Coordinate descent is a popular method of optimization in
Euclidean spaces. It can be more efficient than computing
full gradient steps when it is possible to (1) compute efficiently the coordinate directional derivative, and (2) apply
the update efficiently. We will now show that in the case of
the orthogonal manifold, applying the update (step 2) can
be achieved efficiently. The cost of computing the coordinate derivative (step 1) depends on the specific nature of the
objective function f , and we we show below several cases
where that can be achieved efficiently.
Let Hij be a coordinate direction, let ∇Hij f (U ) be the
corresponding directional derivative, and choose step size
η > 0. A straightforward calculation based on Eq. 2
shows that the update Unext = U Expm(−ηHij ) obeys
Expm(−ηHij ) =
1
 ..
.

0

 ..
.

0

.
 ..


0

···
..
.
···

0
...

···

0
..
.

···

cos(η)
..
.

···
..
.
···

−sin(η)
..
.

···

cos(η)
..
.

···

0

···
..
.
···

···

sin(η)
..
.

···

0


0
.. 
.

0

.. 
.

0


... 
1

Coordinate-descent for learning orthogonal matrices through Givens rotations

This matrix is known as a Givens rotation
(Golub & Van Loan, 2012) and is denoted G(i, j, −η). It
has cos(η) at the (i, i) and (j, j) entries, and ±sin(η) at
the (j, i) and (i, j) entries. It is a simple and sparse orthogonal matrix. For a dense matrix A ∈ Rd×d , the linear
operation A 7→ AG(i, j, η) rotates the ith and j th columns
of A by an angle η in the plane they span. Computing
this operation costs 6d multiplications and additions. As
a result, computing Givens rotations successively for all
d(d−1)
coordinates Hij takes O(d3 ) operations, the same
2
order as ordinary matrix multiplication. Therefore the
relation between the cost of a single Givens relative to a
full gradient update is the same as the relation between
the cost of a single coordinate update and a full update
is in Euclidean space. We note that any determinant-1
orthogonal matrix can be decomposed into at most d(d−1)
2
Givens rotations.
2.3. The Givens rotation coordinate descent algorithm
Based on the definition of Givens rotation, a natural algorithm for optimizing over orthogonal matrices is to perform
a sequence of rotations, where each rotation is equivalent to
a coordinate-step in CD.
To fully specify the algorithm we need two more ingredients: (1) Selecting a schedule for going over the coordinates and (2) Selecting a step size. For scheduling, we
chose here to use a random order of coordinates, following
many recent coordinate descent papers (Richtárik & Takáč,
2012; Nesterov, 2012; Patrascu & Necoara, 2013).
For choosing the step size η we use exact minimization,
since we found that for the problems we aim to solve, using
exact minimization was usually the same order of complexity as performing approximate minimization (like using an
Armijo step rule Bertsekas (1999); Absil et al. (2009)).
Based on these two decisions, Algorithm (1) is a random
coordinate minimization technique.
Algorithm 1 Riemannian coordinate minimization on Od
Input: Differentiable objective function f , initial matrix
U0 ∈ Od
t=0
while not converged do
1. Sample uniformly at random a pair (i(t), j(t)) such
that 1 ≤ i(t) < j(t) ≤ d.
2. θt+1 = argmin f (Ut · G(i, j, θ)).
θ

3. Ut+1 = Ut · G(i, j, θt+1 ).
4. t = t + 1.
end while
Output: Uf inal .

3. Convergence rate for Givens coordinate
minimization
In this section, we show that under the assumption that
the objective function f is differentiable Algorithm 1 converges to critical point of the function f , and the only stable
convergence points are local minima. We further show that
the expectation w.r.t. the random choice of coordinates of
the squared l2 -norm of the Riemannian gradient converges
to 0 with a rate of O( T1 ) where T is the number of iterations. The proofs, including some auxiliary lemmas, are
provided in the supplemental material. Overall we provide the same convergence guarantees as provided in standard non-convex optimization (e.g., Nemirovski (1999);
Bertsekas (1999)).
Definition 1. Riemannian gradient
The Riemannian gradient ∇f (U ) of f at point U ∈
Od is the matrix U Ω, where Ω ∈ Skew(d), Ωji =
−Ωij = ∇ij f (U ), 1 ≤ i < j ≤ d is defined to be
the directional derivative as given in Eq. 1, and Ωii =
0. The norm of the Riemannian gradient ||∇f (U )||2 =
T r(∇f (U )∇f (U )T ) = ||Ω||2f ro .
Definition 2. A point U∗ ∈ Od is asymptotically stable
with respect to Algorithm (1) if it has a neighborhood V of
U∗ such that all sequences generated by Algorithm (1) with
starting point U0 ∈ V converge to U∗ .
Theorem 1. Convergence to local optimum
(a) The sequence of iterates Ut of Algorithm (1) satisfies:
limt→∞ ||∇f (Ut )|| = 0. This means that the accumulation points of the sequence {Ut }∞
t=1 are critical points of
f.
(b) Assume the critical points of f are isolated. Let U∗ be
a critical point of f . Then U∗ is a local minimum of f if
and only if it is asymptotically stable with regard to the sequence generated by Algorithm (1).
Definition 3. For an iteration t of Algorithm (1), and a set
of indices (i(t), j(t)), we define the auxiliary single variable function gtij :
gtij (θ) = f (Ut · G(i, j, θ)) ,

(3)

Note that gtij are differentiable and periodic with a period
of 2π. Since Od is compact and f is differentiable there
exists a single Lipschitz constant L(f ) > 0 for all gtij .
Theorem 2. Rate of convergence
Let f be a continuous function with L-Lipschitz directional
derivatives 2 . Let Ut be the sequence generated by Algorithm 1. For the sequence of Riemannian gradients
∇f (Ut ) ∈ TUt Od we have:
 L · d2 (f (U0 ) − fmin )

max E ||∇f (Ut )||22 ≤
0≤t≤T
T +1

. (4)

2
Because Od is compact, any function f with a continuous
second-derivative will obey this condition.

Coordinate-descent for learning orthogonal matrices through Givens rotations

The proof is a Riemannian version of the proof for the rate
of convergence of Euclidean random coordinate descent for
non-convex functions (Patrascu & Necoara, 2013) and is
provided as supplemental material.

4. Sparse PCA
Principal component analysis (PCA) is a basic dimensionality reducing technique used throughout the sciences.
Given a data set A ∈ Rd×n of n observations in d dimensions, the principal components are a set of orthogonal
z1 , z2 , . . . , zm ∈ Rd , such that the variance
Pmvectors
T
T
AA
zi is maximized. The data is then reprez
i=1 i
sented in a new coordinate system Â = Z T A where
Z = [z1 , z2 , . . . , zm ] ∈ Rd×m .
One drawback of ordinary PCA is lack of interpretability. In the original data A, each dimension usually has
an understandable meaning, such as the level of expression of a certain gene. The dimensions of Â however are
typically linear combinations of all gene expression levels, and as such are much more difficult to interpret. A
common approach to the problem of finding interpretable
principal components is Sparse PCA (Zou et al., 2006;
Journée et al., 2010; d’Aspremont et al., 2007; Zhang et al.,
2012; Zhang & Ghaoui, 2012). SPCA aims to find vectors zi as in PCA, but which are also sparse. In the geneexpression example, the non-zero components of zi might
correspond to a few genes that explain well the structure of
the data A.
One of the most popular approaches for solving the problem of finding sparse principal components is the work
by Journée et al. (2010). In their paper, they formalize
the problem as finding the optimum of the following constrained optimization problem to find the sparse basis vectors Z:
X
argmax
T r(Z T AU ) − γ
|Zij |
(5)
U ∈Rn×m ,Z∈Rd×m

s.t. U T U = Im ,

d
X

ij

2
Zij
= 1 ∀j = 1 . . . m

.

i=1

Journée et al. provide an algorithm to solve Eq. 5 that has
two parts: The first and more time consuming part finds
an optimal U , from which optimal Z is then found. We
focus here on the problem of finding the matrix U . Note
that when m = n, the constraint U T U = Im implies that
U is an orthogonal matrix.
We use a second formulation of the optimization problem,
also given by Journée et al. in section 2.5.1 of their paper:
argmax

d
m X
X

U ∈Rn×m j=1 i=1

T

[|(A · U )ij | − γ]2+

s.t. U U = Im ,

where n is the number of samples, d is the input dimensionality and m is the number of PCA components computed.
This objective is once-differentiable and the objective matrix U grows with the number of samples n.
4.1. Givens rotation algorithm for the full case m = n
If we choose the number of principal components m to be
equal to the number of samples n we can apply Algorithm
((1)) directly to solve the optimization problem of Eq. 6.
Explicitly, at each round t, for choice of coordinates (i, j)
and a matrix Ut ∈ Od , the resulting coordinate minimization problem is:
argmin −
θ

argmin −
θ

d
m X
X

[|(AUt G(i, j, θ))ij | − γ]2+ =

j=1 i=1

d
X

[|cos(θ)(AUt )ki + sin(θ)(AUt )kj | − γ]2+ +

k=1

[| − sin(θ)(AUt )ki + cos(θ)(AUt )kj | − γ]2+
(6)
Algorithm 2 Riemannian coordinate minimization for
sparse PCA
Input: Data matrix A ∈ Rd×n , initial matrix U0 ∈ On ,
sparsity parameter γ ≥ 0
t=0
AU = A · U0 .
while not converged do
1. Sample uniformly at random a pair (i(t), j(t)) such
that 1 ≤ i(t) < j(t) ≤ n.
2. θt+1 = argmax
θ
Pd
2
k=1 ([|cos(θ)(AU )ki(t) + sin(θ)(AU )kj(t) | − γ]+
2
+[| − sin(θ)(AU )ki(t) + cos(θ)(AU )kj(t) | − γ]+ ).
3.AU = AU · G(i(t), j(t)), θt+1 ).
4. t = t + 1.
end while
5. Z = solveF orZ(AU, γ) // Algorithm 6 of
Journée et al. (2010).
Output: Z ∈ Rd×n
See Algorithm (2) for the full procedure. In practice, there
is no need to store the matrices Ut in memory, and one
can work directly with the matrix AUt . Evaluating the expression in Eq. 6 for a given θ requires O(d) operations,
where d is the dimension of the data. We found in practice
that optimizing Eq. 6 required an order of 5-10 evaluations.
Overall each iteration of Algorithm (2) requires O(d) operations.
4.2. Givens rotation algorithm for the case m < n
The major drawback of Algorithm (2) is that it requires the
number of principal components m to be equal to the num-

Coordinate-descent for learning orthogonal matrices through Givens rotations

4.3. Experiments
Sparse PCA attempts to trade-off two variables: the fraction of data variance that is explained by the model’s components, and the level of sparsity of the components. In our
experiment, we monitor a third important parameter, the
number of floating point operations (FLOPS) performed
to achieve a certain solution. To compute the number of
FLOPS we counted the number of additions and multiplications computed on each iteration. This does not include
pointer arithmetic.
We first examined Algorithm 2 for the case where m =
n. We used the prostate cancer gene expression data by
Singh et al. (2002). This dataset consists of the gene expression levels for 52 tumor and 50 normal samples over
12,600 genes, resulting in a 12, 600 × 102 data matrix.
We compared the performance of our approach with that of
the Generalized Power Method of Journée et al. (2010). We
focus on this method for comparisons because both methods optimize the same objective function, which allows to
characterize the relative strengths and weaknesses of the
two approaches.
As can be seen in Figure 1, the Givens coordinate minimization method finds a sparser solution with better explained variance, and does so faster than the generalized
power method.
We tested the streaming version of the coordinate descent
algorithm for sparse PCA (Algorithm 5, supp. material)
on a recent large gene expression data set collected from of
six human brains (Hawrylycz et al., 2012). Overall, each of
the 20K human genes was measured at 3702 different brain
locations, and this data can be used to study the spatial patterns of mRNA expression across the human brain. We
again compared the performance of our approach with that
of the Generalized Power Method of Journée et al. (2010).
We split the data into 5 train/test partitions, with each train
set including 2962 examples and each test set including 740
examples. We evaluated the amount of variance explained
by the model on the test set. We use the adjusted vari-

0.5

Givens coordinate minimization
Generalized Power method
mean # non−zeros

mean explained variance

ber of samples n. This kind of “full dimensional sparse
PCA” may not be necessary when researchers are interested
to obtain a small number of components. We therefore develop a streaming version of Algorithm (2). For a small
given m, we treat the data as if only m samples exist at
any time, giving an intermediate model AU ∈ Rd×m . After a few rounds of optimizing over this subset of samples,
we use a heuristic to drop one of the previous samples and
incorporate a new sample. This gives us a streaming version of the algorithm because in every phase we need only
m samples of the data in memory. The full details of the
algorithm are given in the supplemental material.

0.4

0.3

0.2
0

Givens coordinate minimization
Generalized Power method

1

10

10

2
3
# FLOPS

(a) explained variance

4

360

330

300
0

1

2
3
1010 # FLOPS

4

(b) number of non-zeros

Figure 1. (a) The explained variance as function of FLOPS of the
coordinate minimization method from Algorithm 2 and of the generalized power method by Journée et al. (2010), on a prostate cancer gene expression dataset. (b) The number of non-zeros in the
sparse PCA matrix as function of FLOPS of the coordinate minimization method from Algorithm 2 and of the generalized power
method by Journée et al. (2010), on a prostate cancer gene expression dataset. The size of the sparse PCA matrix is 12, 600 × 102.

ance procedure suggested in this case by Zou et al. (2006),
which takes into account the fact that the sparse principal
components are not orthogonal.
For the Generalized Power Method we use the greedy l1
version of Journée et al. (2010), with the parameter µ set
to 1. We found the greedy version to be more stable and
to be able to produce sparse solutions when the number of
components was m > 1. We used values of γ ranging from
0.01 to 0.2, and two stopping conditions: “convergence”,
where the algorithm was run until its objective converged
within a relative tolerance level of 10−4 , and “early stop”
where we stopped the algorithm after 14% of the iterations
required for convergence. For our algorithm we used the
same range of γ values, and an early-stop condition where
the algorithm was stopped after using 14% of the samples.
Figure 2 demonstrates the tradeoff between floating point
operations and explained variance for SPCA with 3, 5 and
10 components and with 3 sparsity levels: 5%, 10% and
20%. Using low dimensions is often useful for visual exploration of the data. Each dot represents one instance of
the algorithm, run with a certain value of γ and stopping criterion. To avoid clutter we only show instances which performed best in terms of explained variance or few FLOPS.
When strong sparsity is required (5% or 10% sparsity),
the Givens-rotation coordinate descent algorithm finds solutions faster (blue rectangles are more to the left in Figure
2), and these solutions are similar or better in terms of explained variance. For low-dimensional less sparse solutions
(20% sparsity) we find that the generalized power method
finds comparable or better solutions using the same computational cost, but only when the number of components is
small, as seen in Figure 2.c,f,i.

Coordinate-descent for learning orthogonal matrices through Givens rotations

(a) max. sparsity 5%

(d) max. sparsity 5%

0.1

2

4

9

6

0.3

explained variance

Generalized Power method
Givens coordinate minimization

0.2

0
0

(g) max. sparsity 5%
0.5

explained variance

explained variance

0.3

0.2

0.1

0
0

8

4

10 # FLOPS

(b) max. sparsity 10%

8

12

109 # FLOPS

1

10

2

3

4

# FLOPS

(h) max. sparsity 10%

0.1

2

9

4

6

0.3

0.2

0.1

0
0

8

explained variance

explained variance

explained variance

0.1

0.5

4

10 # FLOPS

(c) max. sparsity 20%

8

12

109 # FLOPS

0.4
0.3
0.2
0.1
0
0

16

1

10

10

(f) max. sparsity 20%

2

3

4

# FLOPS

(i) max. sparsity 20%

0.3

0.5

0.2

0.1

2

9

4

6

8

10 # FLOPS

3 components

0.3

explained variance

explained variance

explained variance

0.2

10

(e) max. sparsity 10%

0.2

0
0

0.3

0
0

16

0.3

0
0

0.4

0.2

0.1

0
0

4

8

12

109 # FLOPS

5 components

16

0.4
0.3
0.2
0.1
0
0

1

10

10

2

3

4

# FLOPS

10 components

Figure 2. The tradeoff between explained variance and computational cost for 3, 5 and 10-component sparse PCA models applied to
human gene expression data. The models are constrained for maximum sparsity of 5% (a), (d) & (g), 10% (b), (e) & (h) and 20% (c), (f)
& (i). Red pluses indicate the Generalized Power method (Journée et al., 2010); blue squares represent the Givens coordinate procedure.
See Subsection 4.3 for experimental conditions. Explained variance was adjusted following Zou et al. (2006).

5. Orthogonal tensor decomposition
Recently it has been shown that many classic machine learning problem such as Gaussian Mixture Models and Latent Dirichlet Allocation can be solved efficiently by using 3rd order moments (Anandkumar et al.,
2012a; Hsu & Kakade, 2013; Anandkumar et al., 2012b;c;
Chaganty & Liang, 2013). These methods ultimately rely
on finding an orthogonal decomposition of 3-way tensors
T ∈ Rd×d×d , and reconstructing the solution from this decomposition. Here we show that the problem of finding an
orthogonal decomposition for a tensor T ∈ Rd×d×d can
be naturally cast as an optimization problem over the orthogonal matrix manifold. We apply Algorithm 1 to this
problem, and compare its performance on a task of finding a Gaussian Mixture Model with a state-of-the-art tensor
decomposition method, the robust Tensor Power Method
(Anandkumar et al., 2012a). We find that the Givens coordinate method consistently finds better solutions when the
number of mixture components is large.

5.1. Orthogonal tensor decomposition
The problem of tensor decomposition is very hard in general (Kolda & Bader, 2009). However, a certain class of
tensors known as orthogonally decomposable tensors are
easier to decompose, as has been discussed recently by
Anandkumar et al. (2012a); Hsu & Kakade (2013) and others. Here we introduce the problem of orthogonal tensor
decomposition, and provide a new characterization of the
solutions to the decomposition problem as extrema of an
optimization problem on the orthogonal matrix manifold.
The resulting algorithm is similar to one recently proposed
by Ishteva et al. (2013). However, we aim for full diagonalization, while they focus on finding a good low-rank approximation. This results in different objective functions:
ours involves third-order polynomials on Od , while Ishteva
et al.’s results in sixth-order polynomials on the low-rank
compact Stiefel manifold. Diagonalizing the tensor T is
attainable in our case thanks to the strong assumption that

Coordinate-descent for learning orthogonal matrices through Givens rotations

it is orthogonally decomposable. Nonetheless, both methods are extensions of Jacobi’s eigenvalue algorithm to the
tensor case, in different setups.
We start with preliminary notations and definitions. We focus here on symmetric tensors T ∈ Rd×d×d . A third-order
tensor is symmetric if its values are identical for any permutation σ of the indices: with Ti1 i2 i3 = Tiσ(1) iσ(2) iσ(3) .
We also view a tensor T as a trilinear map.
T : Rd × Rd × Rd → R: T (v1 , v2 , v3 )
Pd
a,b,c=1 Tabc v1a v2b v3c .

=

Finally, we also use the three-form tensor product of a
vector u ∈ Rd with itself: u ⊗ u ⊗ u ∈ Rd×d×d ,
(u ⊗ u ⊗ u)abc = ua · ub · uc . Such a tensor is called a
rank-one tensor.
Definition 4. A symmetric tensor T is orthogonally decomposable if there exists an orthonormal set v1 , . . . vd ∈ Rd ,
and positive scalars λ1 , . . . λd > 0 such that:
T =

d
X

λi (vi ⊗ vi ⊗ vi ).

(7)

i=1

Unlike matrices, most symmetric tensors are not orthogonally decomposable.
However, as shown by
Anandkumar et al. (2012a); Hsu & Kakade (2013);
Anandkumar et al. (2013), several problems of interest,
notably Gaussian Mixture Models and Latent Dirichlet
Allocation do give rise to third-order moments which are
orthogonally decomposable in the limit of infinite data.

For this we need to calculate the form of the function
gtij (θ) = f (U · G(i, j, θ)). We have:
gtij (θ) = f (U · G(i, j, θ)) =
d
X

T (uk , uk , uk ) + T (ũi , ũi , ũi ) + T (ũj , ũj , ũj ) .

k6=i,j

where we used ũi = cos(θ)ui + sin(θ)uj and ũj =
cos(θ)uj − sin(θ)ui .
Denote by T̃ the tensor such that: T̃ijk = T (ui , uj , uk ).
We will abuse notation and denote T̃ = T (U, U, U ). The
tensor T̃ is the three-way multiplication of T by the matrix U . This is the lifting of the matrix operation M̃ =
M (U, U ) = U M U T to the tensor domain.
Collecting terms, using the symmetry of T and some basic
trigonometric identities, we then have:


(9)
gtij (θ) =cos3 (θ) T̃iii + T̃jjj − 3T̃ijj − 3T̃jii


+sin3 (θ) T̃iii − T̃jjj − 3T̃ijj + 3T̃jii


+cos(θ) 3T̃ijj + 3T̃jii


+sin(θ) 3T̃ijj − 3T̃jii .
In each step of the algorithm, we maximize gtij (θ) over
−π ≤ θ < π. The scalar function gtij has at most 3 maxima that can be obtained in closed form solution, and thus
can be maximized in constant time.

The goal of orthogonal tensor decomposition is, given an
orthogonally decomposable tensor T , to find the orthogonal
vector set v1 , . . . vd ∈ Rd and the scalars λ1 , . . . λd > 0.

Algorithm 3 Riemannian coordinate maximization for orthogonal tensor decomposition

We now show that finding an orthogonal decomposition can
be stated as an optimization problem over Od :
Theorem 3. Let T ∈ Rd×d×d have an orthogonal decomposition as in Definition 4, and consider the optimization
problem
d
X
T (ui , ui , ui ),
(8)
max f (U ) =

Input: Symmetric tensor T ∈ Rd×d×d .
Initialize t = 0, T̃ 0 = T , U0 = Id .
while not converged do
1. Sample uniformly at random a pair (i(t), j(t)) such
that 1 ≤ i(t) < j(t) ≤ d.
t
t
t
t
2. Obtain T̃iii
, T̃jjj
, T̃ijj
, T̃jii
.
ij
3. θt = argmax gt (θ), where gtij is defined as in 9.

where U = [u1 u2 . . . ud ]. The stable stationary points of
the problem are exactly orthogonal matrices U such that
ui = vπ(i) for a permutation π on [d]. The maximum value
Pd
they attain is i=1 λi .

4. T̃ t+1 = T̃ t (G(i, j, θt ), G(i, j, θt ), G(i, j, θt )).
// Three way multiplication of T̃ t by G(i, j, θt ).
5. Ut+1 = Ut G(i, j, θt ).
6. t = t + 1.
end while
Output: Uf inal .

U ∈Od

i=1

The proof is given in the supplemental material.
5.2. Coordinate minimization algorithm for orthogonal
tensor decomposition
We now adapt Algorithm 1 for solving the problem of orthogonal tensor decomposition of a tensor T , by maximizPd
ing the objective function 8, f (U ) = i=1 T (ui , ui , ui ).

θ

The most computationally intensive part of Algorithm 3 is
line 4. Multiplying a tensor by the Givens rotation G(i, j, θ)
only affects tensor entries on the i-th and j-th slice. This
requires O(d2 ) operations per iteration. In Section D of the
supplemental material we provide a different version of this

Coordinate-descent for learning orthogonal matrices through Givens rotations
1

0.75

0.75

0.5

0.5

Tensor power method
Givens coordinate descent
Optimal

Tensor power method
0.25
Givens coordinate descent
Optimal

0.25
0
20

0.98

50

100

Dimension

(a) 10,000 samples

200

0

20

50

100

Dimension

200

(b) 200,000 samples

Figure 3. Clustering performance in terms of normalized MI
of the Givens algorithm vs. the tensor power method of
Anandkumar et al. (2012a). Clustering by fitting a GMM from
samples drawn from a 20-component GMM with varying dimension, using 3rd order moments. Reconstruction is performed from
(a) 10K and (b) 200K samples. Blue line with triangles marks the
Givens coordinate method. Red line with circles marks the tensor
power method, and the black line is the optimal performance if all
GMM parameters are known.

algorithm which does not require calculating the tensor T .
Instead, it operates directly on the data points, calculating
cross products on demand. This version of the algorithm
has complexity per step of O(#samples) instead.
5.3. Experiments
Hsu & Kakade (2013) and Anandkumar et al. (2012a) have
recently shown how fitting a Gaussian Mixture Model
(GMM) with common spherical covariance can be reduced
to orthogonally decomposing a third moment tensor. We
evaluate the Givens coordinate minimization algorithm using this problem. We compare with a state of the art tensor
decomposition method, the robust tensor power method, as
given in Anandkumar et al. (2012a).
We generated GMMs with the following parameters: number of dimensions in {10, 20, 50, 100, 200}, number of
samples in {10K, 30K, 50K, 100K, 200K}. We used 20
components, each with a spherical variance of 2. The
centers were sampled from a Gaussian distribution with
an inverse-Wishart distributed covariance matrix. Given
the samples, we constructed the 3rd order moment, decomposed it, and reconstructed the model following the
procedure in Anandkumar et al. (2012a). We then clustered the samples according to the reconstructed model,
and measured the normalized mutual information (NMI)
(Manning et al., 2008) between the learned clustering and
the true clusters.
Figure 3 compares the performance of the two methods
with the optimal NMI across dimensions. The coordinate minimization method outperforms the tensor power
method for the large sample size (200K), whereas for small
sample size (10K) the tensor power method performs better for the intermediate dimensions. In Figure 4 we see the

Normalized MI

Normalized MI

1

0.94
0.9
Tensor power method
Givens coordinate descent
Optimal

0.86
0.82

10K

30K 50K

100K 200K

Number of samples

Figure 4. Same task as Figure 3, but for fixed dimension d = 100
and varying number of samples.

performance of both algorithms across all sample sizes for
dimension = 100. We see that the coordinate minimization
method again performs better for larger sample sizes. We
observed this phenomenon for 50 components as well, and
for mixture models with larger variance.

6. Conclusion
We described a framework to efficiently optimize differentiable functions over the manifold of orthogonal matrices.
The approach is based on Givens rotations, which we show
can be viewed as the parallel of coordinate updates in Euclidean spaces. We prove the procedure’s convergence to
a local optimum. Using this framework, we developed algorithms for two unsupervised learning problems: Finding
sparse principal components; and learning a Gaussian mixture model through orthogonal tensor decomposition. Our
method poses an alternative to the tensor power method for
orthogonal tensor decompositions. Our alternative extends
the way the Jacobi eigenvalue algorithm is an alternative to
the matrix power method for matrix decompositions.
We expect that the proposed framework can be further extended to other problems requiring learning over orthogonal matrices including ICA. Moreover, coordinate descent
approaches have some inherent advantages and are sometimes better amenable to parallelization. Developing distributed Givens-rotation algorithms would be an interesting
future research direction.

Acknowledgments
We wish to thank the anonymous reviewers for numerous
improvements to the paper, and Haim Avron for fruitful discussions. U.S. is a recipient of the Google Europe Fellowship in Machine Learning, and this research is supported in
part by this Google Fellowship. G.C. was supported by the
Israeli science foundation grant 1090/12, and by a Marie
Curie reintegration grant PIRG06-GA-2009-256566.

Coordinate-descent for learning orthogonal matrices through Givens rotations

References
Absil, P-A, Mahony, Robert, and Sepulchre, Rodolphe. Optimization algorithms on matrix manifolds. Princeton
University Press, 2009.
Anandkumar, Anima, Ge, Rong, Hsu, Daniel, Kakade,
Sham M, and Telgarsky, Matus. Tensor decompositions for learning latent variable models. arXiv preprint
arXiv:1210.7559, 2012a.
Anandkumar, Anima, Ge, Rong, Hsu, Daniel, and Kakade,
Sham M. A tensor spectral approach to learning
mixed membership community models. arXiv preprint
arXiv:1302.2684, 2013.
Anandkumar, Animashree, Foster, Dean P, Hsu, Daniel,
Kakade, Sham M, and Liu, Yi-Kai. A spectral algorithm for latent dirichlet allocation. arXiv preprint
arXiv:1204.6703, 2012b.

Journée, Michel, Nesterov, Yurii, Richtárik, Peter, and
Sepulchre, Rodolphe. Generalized power method for
sparse principal component analysis. The Journal of Machine Learning Research, 11:517–553, 2010.
Kolda, Tamara G and Bader, Brett W. Tensor decompositions and applications. SIAM review, 51(3):455–500,
2009.
Manning, Christopher D, Raghavan, Prabhakar, and
Schütze, Hinrich. Introduction to information retrieval,
volume 1. Cambridge University Press Cambridge,
2008.
Nemirovski, A. Optmization II Numerical Methods for
Nonlinear Continuous Optimization. 1999.
Nesterov, Yu. Efficiency of coordinate descent methods
on huge-scale optimization problems. SIAM Journal on
Optimization, 22(2):341–362, 2012.

Anandkumar, Animashree, Hsu, Daniel, and Kakade,
Sham M. A method of moments for mixture models and
hidden markov models. arXiv preprint arXiv:1203.0683,
2012c.

Patrascu, Andrei and Necoara, Ion. Efficient random coordinate descent algorithms for large-scale structured nonconvex optimization. arXiv preprint arXiv:1305.4027,
2013.

Bertsekas, Dimitri P. Nonlinear programming. Athena Scientific, 1999.

Richtárik, Peter and Takáč, Martin. Iteration complexity of
randomized block-coordinate descent methods for minimizing a composite function. Mathematical Programming, pp. 1–38, 2012.

Chaganty, Arun Tejasvi and Liang, Percy. Spectral experts for estimating mixtures of linear regressions. arXiv
preprint arXiv:1306.3729, 2013.
d’Aspremont, Alexandre, El Ghaoui, Laurent, Jordan,
Michael I, and Lanckriet, Gert RG. A direct formulation
for sparse pca using semidefinite programming. SIAM
review, 49(3):434–448, 2007.
Golub, Gene H and Van Loan, Charles F. Matrix computations, volume 3. JHUP, 2012.
Hawrylycz, Michael J, Lein, S, Guillozet-Bongaarts, Angela L, Shen, Elaine H, Ng, Lydia, Miller, Jeremy A,
van de Lagemaat, Louie N, Smith, Kimberly A, Ebbert,
Amanda, Riley, Zackery L, et al. An anatomically comprehensive atlas of the adult human brain transcriptome.
Nature, 489(7416):391–399, 2012.
Hsu, Daniel and Kakade, Sham M. Learning mixtures of
spherical gaussians: moment methods and spectral decompositions. In Proceedings of the 4th conference on
Innovations in Theoretical Computer Science, pp. 11–20.
ACM, 2013.
Ishteva, Mariya, Absil, P-A, and Van Dooren, Paul. Jacobi algorithm for the best low multilinear rank approximation of symmetric tensors. SIAM Journal on Matrix
Analysis and Applications, 34(2):651–672, 2013.

Singh, Dinesh, Febbo, Phillip G, Ross, Kenneth, Jackson,
Donald G, Manola, Judith, Ladd, Christine, Tamayo,
Pablo, Renshaw, Andrew A, D’Amico, Anthony V,
Richie, Jerome P, et al. Gene expression correlates of
clinical prostate cancer behavior. Cancer cell, 1(2):203–
209, 2002.
Zhang, Youwei and Ghaoui, Laurent El. Large-scale sparse
principal component analysis with application to text
data. arXiv preprint arXiv:1210.7054, 2012.
Zhang, Youwei, dAspremont, Alexandre, and El Ghaoui,
Laurent. Sparse pca: Convex relaxations, algorithms and
applications. In Handbook on Semidefinite, Conic and
Polynomial Optimization, pp. 915–940. Springer, 2012.
Zou, Hui, Hastie, Trevor, and Tibshirani, Robert. Sparse
principal component analysis. Journal of computational
and graphical statistics, 15(2):265–286, 2006.

