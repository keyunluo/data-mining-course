The f -Adjusted Graph Laplacian:
a Diagonal Modification with a Geometric Interpretation

Sven Kurras
Ulrike von Luxburg
Department of Computer Science, University of Hamburg, Germany
Gilles Blanchard
Department of Mathematics, University of Potsdam, Germany

Abstract
Consider a neighborhood graph, for example a
k-nearest neighbor graph, that is constructed on
sample points drawn according to some density p. Our goal is to re-weight the graph’s
edges such that all cuts and volumes behave as if
the graph was built on a different sample drawn
from an alternative density p. We introduce
the f -adjusted graph and prove that it provides
the correct cuts and volumes as the sample size
tends to infinity. From an algebraic perspective,
we show that its normalized Laplacian, denoted
as the f -adjusted Laplacian, represents a natural
family of diagonal perturbations of the original
normalized Laplacian. Our technique allows to
apply any cut and volume based algorithm to the
f -adjusted graph, for example spectral clustering,
in order to study the given graph as if it were built
on an unaccessible sample from a different density. We point out applications in sample bias
correction, data uniformization, and multi-scale
analysis of graphs.

1. Introduction
Assume that we are given a neighborhood graph G, say
a k-nearest neighbor graph, based on a sample x1 , ..., xn
of points drawn according to some probability density p.
Many properties of p reflect in properties of G. However,
we are actually interested in another density p that differs
from p in some known aspects, but we cannot access a sample from p. For example, p may be biased in some way
known to us. How could we correct for such a bias if we
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

KURRAS @ INFORMATIK . UNI - HAMBURG . DE
LUXBURG @ INFORMATIK . UNI - HAMBURG . DE

GILLES . BLANCHARD @ MATH . UNI - POTSDAM . DE

just have access to the given graph G, but not to the underlying sampling mechanism? In order to tackle this problem,
our general strategy is to transform G into another graph G
that “reflects properties of p ”, although the vertices still refer to the original sample points distributed according to p.
We focus on volume and cut properties, as these are used in
a large variety of machine learning algorithms. Our goal is
that volumes and cuts of G reveal the same information on
p as another graph would do that is directly built on a sample from p. We provide a solution that provably achieves
this goal in the limit of increasing sample size. We first
define a vector f of vertex weights that reflects the continuous volume properties of p. Then we “merge” these
vertex weights into G by re-weighting its edges in such a
way that the modified graph G attains f as its degree vector. Any such “merging operation” implicitly sets all volumes in G corresponding to p. The crux is to construct a
merging operation that simultaneously ensures that the cut
weights in G correspond to the continuous cut properties
of p. We provide such an operation by defining G as the
“f -adjusted graph of G”.
The above delineates the geometric interpretation of
f -adjusting. It allows to think of transforming G into G
as of modifying the underlying density from p to p. This
provides all the intuition that we need in order to choose
f in a meaningful way and to interpret volumes and cut
weights in the f -adjusted graph. Although this motivation is purely geometric, it turns out that f -adjusting also
has appealing algebraic properties. We denote by the term
“f -adjusted Laplacian” the normalized Laplacian matrix of
the f -adjusted graph. The algebraic interpretation shows
that all f -adjusted Laplacians that can be obtained from G
represent a natural family of diagonal perturbations of the
normalized Laplacian of G. Thus we can think of transforming G into G as of a meaningful modification along the
main diagonal of the original normalized Laplacian. The
algebraic results do not require that G is a geometric graph.

The f -Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation

These interpretations allow us to apply f -adjusting to a variety of problems from machine learning:
(a) Correcting for sampling bias: we obtain a method to
correct for a sampling bias in graph-based algorithms.
(b) Spectral clustering: although G is built on a fixed sample drawn from p, we can apply spectral clustering as if the
graph were built on another sample drawn from density p.
(c) Multi-scale analysis: the original density p can be modified in various ways, for example by a convolution. Let kh
denote some convolution kernel of bandwidth h. Estimates
on p∗kh can be extracted from the graph by averaging over
neighborhoods. f -adjusting can then be applied in order to
merge these estimates into edge weights appropriately, so
that the new volumes and cut weights represent p ∗ kh .
(d) Manifold learning and clustering: there is a complicated
interplay between the geometric structure of the data and
the sampling density. It can be helpful to separate these two
aspects, for example if we are interested in a function f of
which we only know f (xi ) with xi sampled according to
some unknown sampling density on a manifold. We would
like to remove the artifacts induced by the sampling density
and construct a situation that looks like as if the sample
points xi were distributed uniformly.

2. Applications
Before we dive into the theoretical analysis, we motivate
the reader by two example applications.
2.1. Removing density information
In some scenarios the sample distribution does not provide the structural information that we are interested in.
For example in sea temperature recording or general wireless sensor networks one has to deal with measurement
values ti at random spatial positions xi . The goal is
to study the measurement values without any bias introduced by their random positions. If the measurements were
taken at uniform positions, then one could apply spectral
clustering. But any non-uniform spatial distribution distorts all cut weights. There is no obvious mechanism to
compensate for this influence. We demonstrate this effect in an image segmentation task. The classical spectral clustering for image data on a d-dimensional grid (Shi
& Malik, 2000) constructs an r-graph of radius r on the
pixel positions. Edge weights are defined by the products wij := sims (xi , xj ) · simv (ti , tj ) of spatial similarities sims (xi , xj ) := exp(−0.5kxi − xj k2 /σs2 ) and value
similarities simv (ti , tj ) := exp(−0.5kti − tj k2 /σv2 ). The
normalized cuts in the resulting graph provide useful clustering information on the intensity values. But this only
holds because the spatial similarities of a grid have a uni-

Figure 1. Top left: original image of a cloud, unknown to us.
Top right: 5000 sample points drawn from two Gaussians, colors indicating intensity values. We can access data only at these
positions. Bottom left: spectral image clustering applied to the
non-uniform sample positions. We show the heat plot of the NCut
score vector (eigenvector that solves the relaxed minimum normalized cut problem). The scores are biased to a wrong segmentation. Bottom right: heat plot of the NCut score vector of
our density-corrected graph. It separates the cloud from its background because the spatial bias is removed.

form impact on the product in wij . It does not generalize
to the case where the sample positions are distributed nonuniformly. In this case the spatial similarities distort the
cut weights and hence the clustering result in an undesired
way. Consider the image in Figure 1 (top left), and assume
that its intensity values are not given at the positions of a
grid, but only at sample positions drawn from an aggregation of two Gaussians (top right). In order to remove the
spatial unbalancedness, we suggest to proceed as follows:
(1) Build the neighborhood graph G on the sample points
from only their spatial similarities wij := sims (xi , xj ).
(2) With d the vector of vertex degrees, create the
d−1 -adjusted graph G. It provides the edge weights wij .
We show below that these weights remove the density information from the original volumes and cut weights of G.
(3) Define the final edge weights by wij · simv (ti , tj ).
(4) Apply spectral clustering to the resulting graph.
This approach removes the influence of the spatial distribution from the final edge weights. It solely considers intensity values. Figure 1 (bottom right) shows that this leads to
the correct segmentation, whereas the biased segmentation
provided by the original graph is not correct (bottom left).
2.2. Correcting for a sampling bias
Consider a data set in which some areas are known to be
over- or underrepresented. For example, a poll among shop

3

3

2

2

1

1

0

0

y

y

The f -Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation

−1

−1

−2

−2

−3
−4

−2

0
x

2

4

−3
−4

3. The f -Adjusted Graph Laplacian

−2

0
x

2

4

max

3
2

y

1
0
−1

true
biased
d⋅b−2−adjusted

−2
−3
−4

−2

0
x

2

4

min
−4

−2

0
x

2

4

3.1. Graph notation

max

3
2

y

1
0
−1

true
biased
d⋅b−2−adjusted

−2
−3
−4

−2

0
x

2

4

min
−4

−2

0
x

In this section we formally define the f -adjusted graph and
its Laplacian. f -adjusting addresses two goals. The first
goal is to re-weight the edges of a given graph in such a
way that the new degree vector equals the vector f . This
implicitly sets the volumes of all vertex subsets according
to f . The second goal is to provide a geometric interpretation that simultaneously relates the new cut weights to f . As
shown in Section 5, f -adjusting indeed fits both volumes
and cut weights to f in a way that allows for a geometric
interpretation: both quantities represent one and the same
modified underlying density.

2

4

Figure 2. Top left: exemplary sample from true density p that we
cannot access. Top right: 3000 sample points from biased density
p = b · p that underrepresents around x ≈ 0. This is the sample
we can access. Middle right: cut weights of vertical cuts at x ∈
[−4, 4], for a single graph drawn from each the true and the biased
density, and for the db−2 -adjusted graph. Bottom right: same,
averaged over 20 runs. Middle left: heat plot of the NCut score
vector of the biased graph, giving the wrong vertical cut. Bottom left: heat plot of the NCut score vector of the db−2 -adjusted
graph. It identifies the correct clusters, since the bias is removed.

customers in the morning will particularly have the ages 20
to 30 being underrepresented. How can we correct for this
in a graph-based learning scenario? Let p = b · p denote
the erroneous density that we can access under some bias
b : Rd → R>0 and p the true density that we cannot access.
Using our framework, one can use any estimate of b in order to compensate for the bias. This can be achieved by the
f -adjusted graph for f = db−2 := (di /b(xi )2 )i . It provides volumes and cut weights just as if the sample points
were drawn from p instead of p. Since f -adjusting does not
require any coordinates of the sample points, the bias b can
also be given as external knowledge on the vertices.
Consider the example in Figure 2. It shows two Gaussians
(top left) that are underrepresented around x ≈ 0 due to
bias b(x, y) = min{1, (2 + 3|x|)/8} (top right). The minimum normalized cut of a graph built on the biased sample is misdirected by this bias to the wrong vertical clusters
(middle left). However, the f -adjustment appropriately “repairs” the volumes and cut weights in the graph. Now the
correct horizontal cut is revealed (bottom left).

For any matrix A = [aij ]ij ∈ Rn×n let A ≥ 0 denote
that all entries are non-negative. For vector b ∈ Rn we
refer by diag(b) ∈ diag(Rn ) to the corresponding diagonal matrix. For any q ∈ R we define bq := (bqi )i
element-wise, as well as the product bc := (bi ci )i of vectors b, c ∈ Rn . We define the set of graph matrices as
T
W := {X ∈ Rn×n
≥0 | X = X , X1 > 0}. We also study
its generalization to the set of weak graph matrices
W := {X ∈ W + diag(Rn ) | X1 > 0}, whose elements
further allow for negative diagonal entries as long as all
row sums remain positive. Obviously W ⊂ W . To each
W ∈ W there corresponds an undirected weighted graph
G(W ) := (V, E, W ) that has W as its weighted adjacency
matrix, and edge set E P
:= {ij | wij 6= 0}. The degree of
vertex i ∈ V is di := j∈V wij > 0, leading to the allpositive degree vector dP= W 1. The volume of S ⊆ V is
defined as volG (S) := i∈S di . Any partition V = S ∪· S
P
is a cut of weight cutG (S, S) := i∈S,j∈S wij . For the
two frequently used vectors f = (fi )i and d = (di )i we set
F := diag(f ) and D := diag(d).
3.2. Definition of the f -adjusted graph
From now on fix some W ∈ W and set G := G(W ). There
are two straightforward methods for modifying G’s edge
weights (= entries in W ) in order to fit all vertex degrees
(= row/column sums of W ) to some prescribed vector f .
f -adjusting combines these two methods because they mutually compensate their respective drawbacks.
f -selflooping. The naive strategy to adjust the vertex degrees of the graph G(W ) to any prescribed vector f ∈ Rn>0
is by modifying selfloops: fix all off-diagonal entries in W ,
and set its main diagonal such that the row sums equal f :
Wf◦ := W − D + F

∈ W .

This yields the vertex degrees Wf◦ 1 = f exactly. We refer
to G(Wf◦ ) as the f -selflooped graph. Despite of its positive row sums, this approach can force some entries on the

The f -Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation

main diagonal of Wf◦ to take negative values. We can fix
this by considering cf -selflooping for c > 0. This yields
vertex degrees Wc◦f 1 = cf , and it is straightforward to see
that the main diagonal of Wc◦f is non-negative if and only if
c ≥ maxi∈V {(di −wii )/fi }. The drawback is that selfloop
weights only affect volumes but not the cut weights. In general there is no relation between volumes and cut weights
in the f -selflooped graph.
f -scaling. Another natural strategy to adapt the row sums
of W to any prescribed vector f ∈ Rn>0 is by proportional
scaling, that is by considering F D−1 W . This matrix yields
the row sums f exactly, but is not symmetric in general. For
that reason, we use a (multiplicative) symmetrization:
√
√
∈ W.
W̃f := F D−1 W F D−1
We refer to G(W̃f ) as the f -scaled graph. The drawback of
this approach is that its degree
d̃ = W̃f 1 does not
P vectorp
equal f exactly, since d˜i = j∈V wij (fi fj )/(di dj ) differs from fi in general. In the following let D̃f := diag(d̃).
f -adjusting. For any f ∈ Rn>0 and c > 0, we combine
both approaches by first applying f -scaling, followed by
cf -selflooping. This defines the weak graph matrix
W f ,c := W̃f − D̃f + cF

∈ W .

We refer to G(W f ,c ) as the (f , c)-adjusted graph of G,
and for the case c = 1 simply as the f -adjusted graph. All
vertex degrees are positive because W f ,c 1 = cf > 0. The
(f , c)-adjusted graph has no negative selfloop if and only if
c ≥ maxi∈V {(d˜i −w̃ii )/fi } =: c∗ . In this case it holds that
W f ,c ∈ W. Although this definition of f -adjusting appears
somewhat artificial at the first glance, it reveals appealing
properties in its algebraic and geometric interpretation.
f -adjusted Laplacian. For W ∈ W, the unnormalized
Laplacian matrix L(W ) := D − W and the normalized
Laplacian matrix L(W ) := I − W̃1 are powerful and wellstudied objects (see Chung, 1997). We define
Lf (W ) := L(W f ,1 )
for any f ∈ Rn>0 as the f -adjusted Laplacian of G(W ).

4. Algebraic Interpretation
All diagonal modifications of the form W + X ∈ W
for X ∈ diag(Rn ) obviously represent all possible selfloop modifications of W , that is f -selflooping for every f ∈ Rn>0 . In this section, we derive a similar result
for the normalized Laplacian: all diagonal modifications
of the form L(W ) + X ∈ L(W ) characterize all possible f -adjustments of W . In this sense, f -adjusting deals
with diagonal modifications of the normalized Laplacian
just in the same way as f -selflooping deals with diagonal

modifications of the adjacency matrix. This shows that
f -adjusting is a natural graph modification. All proofs and
further details are provided in the supplement.
4.1. (f , c)-adjusting is as powerful as f -adjusting
The following lemma shows that it is sufficient to focus on
f -adjusting in order to study every (f , c)-adjusting.
Lemma 4.1 (Scaling Relation). For all W ∈ W, f ∈ Rn>0
and c > 0 it holds that
L(W f ,1 ) = c · L(W f ,c ).
Consequently, every L(W f ,c ) has the same eigenvectors as
L(W f ,1 ), with all eigenvalues scaled by c−1 . In particular
the order of the eigenvalues is preserved. This implies further that L(W f ,c ) inherits many spectral properties from
∗
L(W
√ f ,c ) such as its positive semi-definiteness and that
f is an eigenvector to eigenvalue 0 whose multiplicity
matches the number of connected components in G(W ).
For that reason, the spectral analysis of any (f , c)-adjusted
graph can be reduced to the f -adjusted graph. In particular
it does not matter whether or not it has negative selfloops.
4.2. f -adjustments are diagonally modified Laplacians
The next lemma shows that every f -adjustment represents a
meaningful modification along the main diagonal of L(W ).
Lemma 4.2 (Diagonally Modified Laplacian). For all
W ∈ W and f ∈ Rn>0 it holds that
Lf (W ) = D̃f F −1 − W̃1 .
The main insight provided by Lemma 4.2 is that we can
think of f -adjusting as of replacing the identity matrix in
L(W ) = I − W̃1 with the new matrix D̃f F −1 . Its diagonal entries reflect the relative deviation d̃/f between the
intended new degrees f and the degrees d̃ that are obtained
by f -scaling without subsequent f -selflooping. Note that
f -scaling alone does not reveal any clear relation to L(W ),
only its combination with the subsequent f -selflooping collapses down to this “simple” algebraic form. We denote
any modification along the main diagonal of a matrix as
a diagonal modification. Whenever it exceeds just a tiny
perturbation, it has a strong non-linear impact on the spectrum: the eigenvalues can only loosely and abstractly be
bounded by Horn’s inequalities (Bhatia, 2001), and nothing is known on the impact on the eigenvectors.
4.3. Diagonally modified Laplacians are f -adjustments
Lemma 4.2 shows that every f -adjusting can be understood
as a diagonal modification of the form L(W ) + X = L(A)
for some X ∈ diag(Rn ) and A ∈ W . This rises the question whether also the converse is true: does every such diagonal modification imply that A is an f -adjustment of W ?
The following theorem gives a positive answer.

The f -Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation

Theorem 4.3 (Complete Characterization). For any
W ∈ W with L(W ) 6= 0 consider all solutions (X, A, c) ∈
diag(Rn ) × W × R of the equation
L(W ) + X = c · L(A).
For c ≤ 0 no solution exists. For c > 0, all solutions are
given by A = W f ,c and X + I = D̃f F −1 = Z for any
choice of f ∈ Rn>0 . For connected G(W ), choosing f is
equivalent to choosing any Z ∈ diag(Rn>0
√) with spectral
radius ρ(Z −1 W̃1 ) = 1. This determines f uniquely (up
to scaling) as the eigenvector corresponding to the simple
eigenvalue 1 of the matrix Z −1 W̃1 .
Theorem 4.3 gives two further results for connected graphs:
(i) In order to transform L(W ) by a diagonal modification
into another normalized Laplacian matrix, we are free to
orient the identity matrix in L(W ) = I − W̃1 toward any
“direction” Y ∈ diag(Rn>0 ). But at the same time this determines only a single possible “distance” µ := ρ(Y −1 W̃1 )
for which indeed it holds that µY − W̃1 ∈ L(W ), be1. Moreover, it follows
cause only then ρ((µY )−1 W̃1 ) = √
that µY − W̃1 = L(W f ,1 ), where f is the eigenvector of
(µY )−1 W̃1 corresponding to the simple eigenvalue 1.
(ii) We get deeper insights into the structure of the deviation error d̃/f under f -scaling: given any vector of relative
deviations ξ ∈ Rn>0 , there exists a unique (up to scaling)
vector f ∈ Rn>0 that satisfies d̃/f = αξ for a unique α > 0.
In particular this implies that no vector f 6= d can be obtained by f -scaling exactly as the new degree vector d̃, not
even up to scaling, because the zero-deviation case ξ = 1
is already reached for f = d.
These two results generalize to unconnected graphs by applying them individually to each connected component. In
particular the uniqueness is affected in the way that f may
be scaled individually within each connected component.
4.4. Application: f -adjusted spectral clustering
We are going to show that the spectral clustering technique
generalizes to weak graph matrices. This allows to apply it
to any f -adjusted graph. Spectral clustering of a connected
graph G = (V, E, W ) relies on a relaxation for solving the
NP-hard problem of minimizing the normalized cut value

N Cut(S, S) = cutG (S, S) volG (S)−1 + volG (S)−1
over all S ⊆ V and S := V \S. Instead of the optimal characteristic vector, the√ relaxation determines a real-valued
score vector s := D−1 v2 , where v2 denotes the second smallest eigenvector of L(W ). Thresholding is used to
define S from s. We refer to s as the NCut score vector.
Variants use multiple smallest eigenvectors v2 , v3 , . . . , v`
for embedding vertex i ∈ V in R`−1 at the i’th coordinate
entries of v2 , . . . , v` , and then apply for example k-means
on the embedded points in order to identify k clusters.

Since the (f , c∗ )-adjusted graph has no negative selfloops,
we can apply any spectral clustering technique to G(W f ,c∗ )
as usual. Lemma 4.1 shows that L(W f ,c∗ ) and Lf (W )
share the same eigenvectors in the same order. Thus, the
NCut score vector does not change by considering Lf (W )
instead of L(W f ,c∗ ). Consequently, in all variants of spectral clustering we can simply replace L(W ) by Lf (W ) for
any f ∈ Rn>0 in order to study the normalized cuts of the
graphs G(W f ,c ) for all c > 0 simultaneously, even though
they contain negative selfloops for c < c∗ .

5. Geometric Interpretation
Assume that we are given a graph G plus the metainformation that it is some neighborhood graph built on
an unknown sample from an unknown density. Our goal
is to infer from G about structural properties of the underlying density. The additional meta-information allows
to interpret certain quantities in G, such as volumes and
cut weights, in a meaningful way.
In the following we show that our modification of G still
keeps the geometric interpretation of volumes and cut
weights in the modified graph G. Thus we can think of
modifying edge weights as of modifying the underlying
density, although the unknown sample points remain fixed
at their original positions. These insights provide answers
to the following questions: how to interpret clustering results for G? How to choose f in a meaningful way? Can
we use f to “merge” external information into the graph in
order to extract hidden information from G?
5.1. Geometric graphs
In a random geometric graph, each vertex i ∈ V is identified with a sample point xi ∈ Rd drawn i.i.d. according
to some continuous probability density p : X → R on a
compact domain X ⊆ Rd . The edge set and weights are
given from the construction of a neighborhood graph on
these sample points. Prominent choices are:
(i) the Gaussian graph: the complete graph with Gaussian
weights wij = exp(− 12 kxi − xj k2 /σ 2 ))
(ii) the unweighted r-graph, which has an edge ij of unit
weight if and only if kxi − xj k ≤ r
(iii) the Gaussian weighted kNN graph, which has an edge
ij of Gaussian weight if and only if xj is among the nearest
k neighbors of xi or vice versa.
The results in this section refer to the limit case n → ∞.
We consider the following convergence conditions: for
the Gaussian graph we require that n → ∞, σ → 0,
nσ d+1 / log n → ∞. For the r-graph that n → ∞,
r → 0, nrd+1 / log n → ∞. For the Gaussian weighted
kNN graph that n → ∞, k → ∞, k/ log n → ∞,
nσ d+1 → ∞, (k/n)1/d ≥ σ α for some α ∈]0, 1[.

The f -Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation

We require that the following regularity conditions are
satisfied: X has a smooth boundary with bounded curvature. p is twice differentiable with bounded gradient,
and bounded away from zero, that is p(x) ≥ pmin > 0
for x ∈ X .
5.2. Different views on graphs
We introduce a distinction between three different “views”
on the graph. These views help us to interpret graph modifications in terms of modifications applied to the underlying
probability distribution.
Discrete graph view. This is the well-known setting of
the discrete graph with cutG and volG as introduced in
Section 3.1. Specifically, we do not have access to the information about the underlying space. This is the standard
situation for algorithms like spectral clustering or Isomap.
In particular, in the graph view we cannot evaluate index
sets of the form {i | xi ∈ A} where A is a subset of Rd .
Continuous space view. Here we define volumes and
cuts with respect to the underlying density p. Specifically, we define the p-volume
of a measurable A ⊆ X
R
as volp (A) := p(A) = A p(x) dx. Given a hyperplane H,
we interpret it as a cut of the underlying space in the two
half-spaces denoted by H + and H − . RWe denote its corresponding p-weight as cutp (H) := H p(x) dx. In this
work we focus on cuts induced by hyperplanes. We believe that all stated results can be generalized to any other
cut surfaces that are sufficiently regular.
Interspace view.
This view “mediates” between the
discrete and continuous world. Its objects are not accessible to algorithms on graphs, but they serve as a
theoretical construction to express the relationships between discrete and continuous cuts and volumes. In
this view, we assume that we can evaluate index sets of
the form V (A) := {i | xi ∈ A} ⊆ V where A is a subset
of Rd . For any vector d and any sample
Pdrawn from p,
we refer to vold (A) := volG (V (A)) = xi ∈A di as the
interspace volume of A with respect to vector d. Intuitively, it sums over the degrees as provided in the discrete graph view, while the sum is indexed by the sample points as provided by the continuous space view.
Moreover, let cutW (H) := cutG (V (H − ), V (H + )) =
P
xi ∈H − ,xj ∈H + wij denote the interspace cut weight of
the cut H with respect to weight matrix W . Again, it sums
over the edge weights as in the graph view, while the summands are indicated by the sample points in the space view.
5.3. Geometric problem statement
We already stated the problem informally as modifying a
given neighborhood graph from density p such that its volumes and cuts “looks like” those of a neighborhood graph

from an alternative density p. We are now ready to state this
requirement formally by using the interspace view concept.
Geometric Graph Adjustment Problem Given a neighborhood graph G = (V, E, W ) built on a sample from density p : X → R, and a second density p : X → R. How
can we define a modified graph G = (V, E, W ) such that,
in the limit case, vold (A) ≈ volp (A) for all measurable
sets A ⊆ X and cutW (H) ≈ cutp (H) for all cuts H?
The vertex set and the corresponding sample points remain
fixed throughout the modification. All that is allowed to
change are edges and edge weights. As the term modification indicates, G should stay “similar” to G. This can be
quantified in different ways. Here, we focus on the constraint that the edge sets E and E coincide up to selfloops.
It is well known that |V (A)| is proportional to p(A) in expectation and that the degree di of vertex i can serve, up
to global scaling, as a consistent estimate of the underlying density p(xi ) at sample point xi . This motivates the
following obvious approach to fit G’s interspace volumes
to p-volumes: determine fi proportional to p(xi ), and use
f -selflooping to let the modified graph G attain f as its degree vector. However it is not obvious how to define G such
that further the interspace cut weights in G correspond to
p-weights. We will see in the following that this is provided by f -scaling. Since f -adjusting combines both approaches, it provides both goals simultaneously. Moreover,
if we want to avoid negative selfloops in G, then we can apply (f , c∗ )-adjusting, which keeps all cut weights and volume proportions intact; it just scales all volumes by c∗ .
5.4. Convergence of f -adjusted volumes
Recall that the interspace
volume of any A ⊆ Rd is defined
P
as vold (A) = xi ∈A di . This expression already shows
how we can intuitively think of discrete vertex degrees as a
weight distribution on the underlying space: the positions
of the sample points are distributed according to density p,
and each sample point is additionally weighted by di . Since
the degree di can serve as a density estimate for p(xi ), this
weighting brings in an additional factor p. Therefore we
expect that the
R interspace volume vold (A) behaves like the
p2 -volume A p2 (x)dx in the limit case. Moreover, if we
consider any graph modification that changes the original
degree vector d into f , then this affects the above expression by replacing each individual di by fi , while the index set {i | xi ∈ A} remains the same. Thus, if the
new vertex degrees fi are given as fi := f (xi ) for some
continuous function f : X → R>0 then we expect that
the
R interspace volume volf (A) behaves like the f p-volume
f (x)p(x)dx in the limit case. Indeed this intuition is
A
correct. The following proposition shows that the interspace volume and the continuous f p-volume are proportionally related as n → ∞.

The f -Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation

Proposition 5.1. (Interspace Volumes) Let G be a geometric graph based on n vertices drawn according to p.
Denote its degree vector by d and let f : X → R>0 be
a continuous function. Define the vector f := (f (xi ))i ,
and let G be any graph modification of G that attains
the degrees d = f . Then, under the convergence conditions mentioned above, for any measurable A ⊂ Rd ,
C · vold (A) → volf ·p (A) almost surely as n → ∞, where
C is a scaling constant that depends on n, d.

ther, (f , c)-adjusting simply puts another global scaling
factor c on all volumes.

This result shows that we can modify the graph such that
the interspace volume of the new graph corresponds to any
density p we would like, as long as p is absolutely continuous with respect to p (that is, sets A with p(A) = 0 also
have p(A) = 0). In Section 5.6 and Section 2 we outline a
number of important consequences and applications.

(1) In the original graph (case f = d), both interspace
volumes and interspace cuts correspond to the continuous
quantities of the density p2 . This shows that the original
notion of volume and cut in the graph has an artifact in
the geometric setting in the sense that it does not correspond to volumes according to the original density, but to
the squared density. See below for an illustration.

5.5. Convergence of f -adjusted cut weights

(2) f -adjusting to uniform vertex degrees f = 1 corresponds to the continuous quantities of the original density p. This choice removes the artifact of (1).

In order to study the cut weights after (f , c)-adjusting, we
can solely focus on f -scaling, since it provides exactly the
same cut weights. Again we can derive
Pthe intuition from
the interspace view: let cutW (H) = xi ∈H − ,xj ∈H + wij
denote the interspace cut weight according to any hyperplane H. f -scaling replaces each edge
p weight wij in this
sum by the new weight w̃ij := wij · (fi fj )/(di dj ). Assume that the original interspace
cut weight cutW (H) beR
haves like the p2 -weight H p2 (x)dx in the limit. Then we
p
expect due to (fi fj )/(di dj ) ∼ f p−1 that the modified
interspace
cut weight cutW̃ (H) behaves like the f p-weight
R
f
(x)p(x)dx.
The following proposition makes this inH
tuition explicit, by relating the interspace cut weights after
f -scaling to the continuous f p-weights.
Proposition 5.2. (Interspace Cuts) Let G be a geometric
graph based on n vertices drawn according to p. Denote its
degree vector by d and let f : X → R>0 be a continuous
function that is twice differentiable and has bounded gradient. Define the vector f := (f (xi ))i , and let G̃ be the corresponding f -scaled graph with weight matrix W̃f . Consider a hyperplane H in Rd . Then, under the convergence
conditions mentioned above, C · cutW̃f (H) → cutf ·p (H)
almost surely as n → ∞, where C is a scaling constant
that depends on n, d.
The proofs of both propositions in this section are based on
the arguments in Maier et al. (2009), who study volumes
and cut weights in neighborhood graphs.
Note that the results on the interspace cut weights are
perfectly aligned with the results on the interspace volumes
in the sense that both share the same integrand
R
f (x)p(x)dx. Since f -adjusting provides the same cut
weights as f -scaling, and further degree vector f , we
get that the interspace volumes and interspace cuts of
the f -adjusted graph both converge to the continuous
f p-volumes and f p-cuts, respectively (up to scaling). Fur-

5.6. Interesting consequences
The intuitive interpretation of the geometric results is as
follows: the continuous
volumes
and cuts change under
R
R
f -adjusting from p2 to f p. This implies a number of
interesting special cases:

(3) f -adjusting to inverse degrees f = d−1 corresponds to
a continuous density that is uniform. This can be used to
remove density information altogether and make the underlying volumes behave “uniformly”. The density removal
application in Section 2.1 relies on this strategy.
(4) More generally, all three above cases are modifications
of the form f (x) = g(p(x)) for some g : R>0 → R>0 . In
practice, we can replace the p(x) inside g by any density
estimate (such as the degrees in the original graph). If, additionally, g(αx) = const(α)g(x) for all α > 0, then any
global scaling factor of the density estimate simply translates in a global scaling factor of the continuous quantities.
This approach describes the target density p implicitly, relative to p. See the supplement for an application to the
biased random walks studied by Zlatić et al. (2010), and
for further details on the implicit definition of p from p.
(5) For any twice differentiable function f : X → R>0 ,
define the new degree vector f = (f (xi ))i . In this setting,
interspace cuts and volumes correspond to the quantities as
provided by the density f · p. This case is particularly interesting for spatial corrections of biased sample data, as in
Section 2.2, or if sample coordinates are known.
Anomaly in Volumes and Cuts As mentioned above
in (1), cuts and volumes in the original graph represent integrals over the squared density p2 . To illustrate this anomaly, we consider in Figure 3 the density
p : [0, 1]2 → R, p(x, y) = 2x. Let G = (V, E, W ) denote the 200-NN graph built on 5000 sample points drawn
from p with Gaussian weighted edges (σ = 0.03).
Volume anomaly: assume that we want to partition V into
L ∪· R by splitting the sample points at some x ∈ [0, 1]
into vertices to its left L and to its right R. The split point
should be such that L and R cover the same probability

The f -Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation

Cut weight anomaly: consider vertical cuts at all positions
x ∈ [0.1, 0.9]. Fitting a cubic polynomial ax3 +bx2 +cx+d
to the cut weights of the original graph gives, averaged over
10 runs, the coefficients (−0.06, 1.30, −0.04, 0.01), which
clearly shows the quadratic behavior. For the 1-adjusted
graph we get (−0.08, −0.08, 1.13, −0.11), which is basically linear as expected, since it corresponds to p without squaring. Finally, the d2 -adjusted graph provides
(1.21, 0.16, −0.06, 0.01) which has a significant cubic
term as expected, since its cuts integrate over p3 .
Whenever we are interested in p (which is usually the case),
we must not consider the original graph, but the 1-adjusted
graph, as it provides the desired volumes and cuts. In particular, spectral clustering of a sample drawn from p approximates the normalized cut of p2 , not of p. We can
remove this artifact by replacing the typically used normalized Laplacian L(W ) with the 1-adjusted Laplacian
L1 (W ). But note that squaring p can be beneficial for the
clustering result, since it emphasizes high density clusters.
However, we are no longer restricted to squaring: we can
approximate the normalized cut of the modified density pr
for any r ∈ R simply by taking the dr−1 -adjusted Laplacian for spectral clustering.

6. Further Related Work
Another approximative strategy to change the degree vector of a graph to some vector f is iterative matrix scaling. Pukelsheim (2014) presents recent results on Iterative
Proportional Fitting, which scales row and column sums
alternately to f until convergence. Knight et al. (2014)
study a multiplicative symmetrization for f = 1. Iterative matrix scaling allows for a statistical interpretation, as
it converges to the relative entropy closest solution among
all non-negative matrices that provide the degree vector f
(Csiszar, 1975). However, in contrast to f -adjusting, no geometric interpretation of the resulting cut weights is known.
Bapat et al. (2001) study diagonal modifications of the unnormalized Laplacian. Note that L(W ) + X ∈
/ L(W )
for all X 6= 0, hence no diagonal modification of the unnormalized Laplacian represents the unnormalized Laplacian of any other graph. Nevertheless, diagonal modifications of L(W ) can provide useful meta-information

x1

1

x2 x3

max
original
1−adjusted
d2−adjusted

0.8
0.6
y

mass of the underlying density p. The correct approach is
then to choose L and R such that |L| ≈ |R|, which defines x2 in Figure 3. In contrast to that, constraining to
volG (L) ≈ volG (R) expands low density regions, since the
split point x3 is now implicitly
squared
P set according
P to the−1
density. Constraining to i∈L d−1
≈
gives
i
j∈R dj
x1 . This lets L and R cover the same uniform amount of
p’s support {x ∈ Rd | p(x) > 0}, hiding any other density
information. Note that the analytically expected value of
xi is 0.51/i , which gives roughly (0.5, 0.71, 0.79).

0.4
0.2
0
0

0.2

0.4

0.6
x

0.8

1

min
0

0.2

0.4

0.6

0.8

1

x

Figure 3. Left: 5000 sample points with heat colors indicating the
degrees in the weighted kNN-graph. Dotted lines mark from left
to right the estimated split points (x1 , x2 , x3 ) ≈ (0.51, 0.7, 0.77)
as described in text. Right: polynomials fitted to the cut weights
of a single instance of the original graph G, its 1-adjustment and
its d2 -adjustment, for vertical cuts at x ∈ [0.1, 0.9].

on G. Wu et al. (2012) consider this diagonal modification
for X ≥ 0, and show how to interpret (L(W ) + X)−1 X as
meaningful random walk absorption probabilities on G.

7. Discussion
We introduce f -adjusting as a transformation of a given
graph G into another graph G. We show that f -adjusting
corresponds to a natural diagonal modification of the normalized Laplacian that further allows for a clear geometric
interpretation in terms of a density shift.
The algebraic interpretation shows that f -adjusting represents all diagonal modifications of L(W ) of the form
L(W ) + X ∈ L(W ). This is the normalized Laplacian’s
pendant to the fact that f -selflooping represents all diagonal modifications of W of the form W + X ∈ W . Thus,
f -adjusting acts on the normalized Laplacian in the same
way as f -selflooping acts on the adjacency matrix.
For the geometric interpretation we introduce an explicit
distinction between three different views on a graph. In particular the interspace view is a helpful tool, since it serves
as a bridge between the discrete graph world and the continuous world of density functions beneath. In terms of
these views we express how f -adjusting represents a modification applied to the underlying density p. As a result,
volumes and cuts in G refer to well-specified continuous
quantities. This allows to apply any volume and cut based
algorithm to the graph as if it were drawn according to another distribution.
More experiments and details can be found in the supplementary material.
Acknowledgements This research is supported by the
German Research Foundation via the Research Unit 1735
“Structural Inference in Statistics: Adaptation and Efficiency” and grant LU1718/1-1.

The f -Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation

References
Bapat, R. B., Kirkland, S. J., and Pati, S. The perturbed
Laplacian matrix of a graph. Linear and Multilinear Algebra, 49(3):219–242, 2001.
Bhatia, R. Linear Algebra to Quantum Cohomology: The
Story of Alfred Horn’s Inequalities. The American Mathematical Monthly, 108(4):289–318, 2001.
Chung, F. R. K. Spectral Graph Theory. American Mathematical Society, 1997.
Csiszar, I. I-Divergence Geometry of Probability Distributions and Minimization Problems. The Annals of Probability, 3(1):146–158, 1975.
Knight, P., Ruiz, D., and Uçar, B. A Symmetry Preserving
Algorithm for Matrix Scaling (to appear). SIAM Journal
on Matrix Analysis and Applications, 2014.
Maier, M., von Luxburg, U., and Hein, M. Influence of
graph construction on graph-based clustering measures.
In Neural Information Processing Systems (NIPS), 2009.
Pukelsheim, F. Biproportional scaling of matrices and the
iterative proportional fitting procedure. Annals of Operations Research, 215(1):269–283, 2014.
Shi, J. and Malik, J. Normalized Cuts and Image Segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 22(8):
888–905, 2000.
Wu, X., Li, Z., So, A., Wright, J., and Chang, S. Learning with Partially Absorbing Random Walks. In Neural
Information Processing Systems (NIPS), 2012.
Zlatić, V., Gabrielli, A., and Caldarelli, G. Topologically
biased random walk and community finding in networks.
Physical Review E, 82(6):066109, 2010.

