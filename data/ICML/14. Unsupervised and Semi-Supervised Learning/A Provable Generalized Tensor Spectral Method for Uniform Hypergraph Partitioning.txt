A Provable Generalized Tensor Spectral Method
for Uniform Hypergraph Partitioning

Debarghya Ghoshdastidar
Ambedkar Dukkipati
Departiment of Computer Science & Automation,
Indian Institute of Science, Bangalore â€“ 560012, India

Abstract
Matrix spectral methods play an important role
in statistics and machine learning, and most often
the word â€˜matrixâ€™ is dropped as, by default, one
assumes that similarities or affinities are measured between two points, thereby resulting in
similarity matrices. However, recent challenges
in computer vision and text mining have necessitated the use of multi-way affinities in the
learning methods, and this has led to a considerable interest in hypergraph partitioning methods
in machine learning community. A plethora of
â€œhigher-orderâ€ algorithms have been proposed in
the past decade, but their theoretical guarantees
are not well-studied. In this paper, we develop
a unified approach for partitioning uniform hypergraphs by means of a tensor trace optimization problem involving the affinity tensor, and a
number of existing higher-order methods turn out
to be special cases of the proposed formulation.
We further propose an algorithm to solve the
proposed trace optimization problem, and prove
that it is consistent under a planted hypergraph
model. We also provide experimental results to
validate our theoretical findings.

1. Introduction
The underlying problem in most clustering approaches is
to optimize a certain objective function involving pairwise
relations among all data instances, where the optimization is performed over all possible cluster assignment matrices. Various relaxations of this NP-hard problem are
common in practice. For instance, k-means (Lloyd, 1982)
uses a greedy approach for achieving a local optimum,
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

DEBARGHYA . G @ CSA . IISC . ERNET. IN
AD @ CSA . IISC . ERNET. IN

while spectral clustering does not restrict the optimization
to binary-valued assignment matrices (Donath & Hoffman,
1973). On the other hand, non-negative matrix factorization (Lee & Seung, 2001) derives an approximation of the
assignment matrix by decomposing the similarity matrix.
Still, there are numerous applications, where pairwise similarities are either inappropriate for the purpose or provide
poor performance, for example, subspace clustering (Agarwal et al., 2005), graph matching (Duchenne et al., 2011)
etc. To tackle such problems, a wide class of algorithms,
often coined as â€œhigher-orderâ€ methods, have been proposed in the last decade. The basic idea in these approaches
is to use a m-way similarity measure with m > 2. While
it is common to simply pose this as an optimization problem, where the objective function is justified from various
perspectives (Kim et al., 2011; Rota Bulo & Pelillo, 2013),
there are works that formulate above problem as a hypergraph partitioning problem and use hypergraph reduction
techniques (Agarwal et al., 2005; 2006; Arias-Castro et al.,
2011) or alternative solution strategies (Hein et al., 2013;
Karypis & Kumar, 2000). Since, the constructed hypergraph is m-uniform, i.e., every edge spans m vertices, it is
more natural to exploit the structure of the mth -order affinity tensor of the hypergraph. Hence, one can partition the
hypergraph using higher-order SVD of tensors (Govindu,
2005), non-negative tensor factorization (Shashua et al.,
2006) or tensor power iterations (Duchenne et al., 2011).
Despite the wide variety of solution strategies, one finds
that most of the above algorithms attempt to solve a similar
optimization problem. This is not surprising because, as in
the case of graph based clustering (Shi & Malik, 2000), the
primary objective in above methods is to obtain a grouping
with high intra-cluster similarity. Unfortunately, unlike the
case of graphs, there still remains a lack of unified treatment of above methods from a common perspective. This
paper fills the gap between graph and uniform hypergraph
partitioning by providing a general notion of associativity
maximization and its formulation as a tensor trace maximization problem involving the affinity tensor of the hyper-

A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning

graph. Our formulation encompasses normalized spectral
clustering as well as several existing higher-order methods.
Another concern with the related literature is the absence of
provable higher-order clustering algorithms. Most standard
clustering algorithms with pairwise information have been
well studied using tools from spectral graph theory (Chung,
1997) or matrix theory (Stewart & Sun, 1990). In fact,
perturbation bounds (Ng et al., 2002) and consistency results (von Luxburg et al., 2008; Rohe et al., 2011) have become standard techniques for proving correctness of spectral clustering and similar algorithms. On the other hand,
spectral theory of uniform hypergraphs (Cooper & Dutle,
2012; Hu & Qi, 2012) is still in its early stage, and is yet to
provide guarantees for partitioning algorithms. Moreover,
the role of tensor decompositions (De Lathauwer et al.,
2000; Lim, 2005) and tensor perturbation analysis (Anandkumar et al., 2014) in higher-order clustering is rarely studied in the literature. Till date, error bounds for clustering
have only been studied in the context of hybrid linear modeling (Arias-Castro et al., 2011), and recently, in a planted
partition setting (Ghoshdastidar & Dukkipati, 2014). Our
second contribution is an algorithm for solving the proposed trace maximization problem. We prove that, under a
planted hypergraph model, the proposed algorithm is consistent. The techniques used in our results are significantly
different from existing analysis (Arias-Castro et al., 2011;
Ghoshdastidar & Dukkipati, 2014). We also supplement
our studies with numerical results on benchmark datasets.

is the normalized affinity matrix, and D is a diagonal matrix containing degrees of every vertex. One can see that H
contains orthonormal columns, and hence, a spectral relaxation of the NP-hard problem in (1) is usually solved as
maximize Trace Z T W Z



ZâˆˆRnÃ—k

2.1. Maximizing Normalized Associativity
We formulate an objective for partitioning hypergraphs that
is similar in essence to the objective of spectral clustering
given in (1)-(2). For this, we introduce the following notion
of associativity in a uniform hypergraph.
Definition 1 (Normalized Associativity). Let V1 âŠ† V represent a set of vertices in a m-uniform hypergraph with
mth -order affinity tensor W âˆˆ RnÃ—nÃ—...Ã—n . The associativity of V1 is defined as
X

Assoc(V1 ) =

Wi1 i2 ...im .

vi1 ,vi2 ,...,vim âˆˆV1

Further, if V1 , . . . , Vk is such that âˆªki=1 Vi âŠ† V and Vi âˆ©
Vj = Ï† for all i 6= j, then the normalized associativity of
the partition is defined as

2. Partitioning Uniform Hypergraphs

H

(1)

where the maximum is taken over all matrices H âˆˆ RnÃ—k
with Hij = âˆš 1 1{vi âˆˆVj } . Here, W = Dâˆ’1/2 W Dâˆ’1/2
|Vj |

k
X
Assoc(Vi )
i=1

An undirected hypergraph is a structure on n vertices, V =
{v1 , v2 , . . . , vn }, where each edge connects a subset of vertices and may have a non-negative weight associated with
it. The aim is to partition V into k disjoint sets, V1 , ..., Vk ,
based on the presence or weight of edges. For m-uniform
hypergraphs, every edge is a collection of exactly m vertices and the associated weights are often termed as m-way
affinities. One often represents such affinities in a mth order n-dimensional symmetric tensor, W, usually called
the affinity tensor of the hypergraph. A special case is that
of graphs (2-uniform hypergraphs), where the affinities are
given by a n-dimensional symmetric matrix W . In normalized spectral clustering, one partitions the graphs such
that the normalized cut of the partition is minimized. The
problem can be alternatively formulated in terms of maximization of normalized associativity (Shi & Malik, 2000)
that can be expressed as a trace maximization problem (von
Luxburg, 2007)

(2)

which is maximized by the k leading eigenvectors of W .

N-associativity(V1 , . . . , Vk ) =

maximize Trace(H T W H),

s.t. Z T Z = I ,

|Vi |m/2

,

(3)

where |Vi | denotes the size of ith partition.
For m = 2, the above normalization is similar to that used
in ratio cut (von Luxburg, 2007). For higher values of m,
the normalizing factor increases to counter the increase in
the number of edges. Our objective for hypergraph partitioning is based on maximization of above notion of associativity. We later show that this leads to standard tensor
problems more naturally as compared to a cut formulation.
One can find definitions of hypergraph cuts and Laplacians
and related partitioning approaches in (Hein et al., 2013;
Hu & Qi, 2012). We now provide an equivalent problem
based on tensor trace maximization similar to (2). To make
the notation simple, we use following definition from (De
Lathauwer et al., 2000).
Definition 2 (mode-k product). Let A âˆˆ RpÃ—nk and mth order tensor W âˆˆ Rn1 Ã—n2 Ã—...Ã—nm . The mode-k product of
W and A is a mth -order tensor, represented as W Ã—k A âˆˆ
Rn1 Ã—...nkâˆ’1 Ã—pÃ—nk+1 Ã—...nm , whose elements are
(W Ã—k A)i1 ..ikâˆ’1 jik+1 ..im =

nk
X
ik =1

Wi1 ..ikâˆ’1 ik ik+1 ..im Ajik .

A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning

Theorem 3. Let W be the affinity tensor of a m-uniform
hypergraph, and 1{Â·} denote indicator function. The problem of partitioning the vertices into k disjoint clusters while
maximizing normalized associativity (3) is equivalent to
maximize Trace(W Ã—1 H T Ã—2 . . . Ã—m H T ),
H

(4)

where the maximum is taken over all matrices H âˆˆ RnÃ—k
with Hij = âˆš 1 1{vi âˆˆVj } .
|Vj |

Though the above problem is NP-hard, one can observe that
the columns of H are orthonormal. So, we may relax (4) as
maximize

ZâˆˆRnÃ—k :Z T Z=I

Trace(W Ã—1 Z T . . . Ã—m Z T ),

(5)

which is similar to the spectral relaxation in (2).
2.2. Related Problems in Tensor Algebra
In this section, we discuss the connections of the problem
in (5) to general problems studied in the context of tensors. A dedicated discussion on the literature of higherorder clustering is postponed till the next section.
In (5), we essentially maximize the trace of a tensor via orthogonal transformations. This has been previously studied
in the signal processing community for blind source separation problems (Comon, 2014). One also solves a variant of
this where the sum of squares of diagonal elements is maximized. Such an objective leads to the approach studied
in (Ghoshdastidar & Dukkipati, 2014). Thus, our formulation complements the above work in the sense that both
methods try to perform higher-order clustering by formulating the problem as two well-known tensor problems related to maximization of diagonal terms.
Our proposed partitioning objective (5) is also related to
the tensor eigenvalue problem (Lim, 2005), where the
mth -order tensor W is viewed as a m-linear functional.
This is easy to observe since for any x1 , . . . , xm âˆˆ Rn ,
W(x1 , . . . , xm ) := W Ã—1 xT1 Ã—2 . . . Ã—m xTm is a scalar
function, linear in each argument x1 , . . . , xm . The maximizers of this function under unit `2 -norm constraints are
known as the `2 -eigenvectors of W (Lim, 2005). Based on
this definition, we observe the following regarding (5).
Corollary 4. Let z1 , . . . , zk be the columns of Z, then
k
 X
Trace W Ã—1 Z T . . . Ã—m Z T =
W(zj , ..., zj ), (6)
j=1

where each term in the sum is the normalized associativity
of individual clusters. So, if one relaxes the orthogonality
constraint in (5) but retains the constraint kzj k2 = 1 for
all j, then the stationary points of the trace maximization
are matrices whose columns are `2 -eigenvectors of W.

3. Relation with Existing Clustering Methods
The purpose of this section is to show that a wide variety of
pairwise and higher-order clustering algorithms solve some
variant of the optimization problems in (4)-(5). This establishes the generality of the associativity based objective
presented in Section 2.1.
Spectral Methods. The similarity of the formulation in (5)
to standard spectral clustering (2) is quite evident, though
one may note that, in our framework, one considers a graph
with affinities given by W instead of W . In this respect,
our framework is more similar to a spectral relaxation of
the k-means algorithm (Zha et al., 2001). However, the decoupling of the normalization from the eigenvalue problem
gives the freedom to use alternative normalizations. For instance, one can use a doubly stochastic normalization (Zass
& Shashua, 2006), which gives superior performance.
Non-negative tensor factorization. Shashua et al. (2006)
generalized the use of non-negative matrix factorization for
clustering to the case of tensors. The objective is to approximate a hyperstochastic affinity tensor W by a sum
of k non-negative rank-one tensors. Note that for a vector h âˆˆ Rn , the mth -order rank-one tensor, hâŠ—m , has the
entries (hâŠ—m )i1 i2 ...im = hi1 hi2 . . . him . The reason for
such an approximation is that W contains the total probability that m points lie in same cluster, whereas, under
conditional independence, each rank-one tensor represent
the joint probability that m vertices lie in each cluster. So
the optimization problem in (Shashua et al., 2006) is

2
Pk


minimizen W âˆ’ j=1 zjâŠ—m  s.t. ziT zj = 1{i=j} , (7)
z1 ,..,zk âˆˆR+

o

where k Â· k2o is the sum of squares of entries in the tensor.
Non-negativity of zi â€™s is due to probabilistic reason, and
orthogonality ensures hard clustering. The objective functions of (4) and (7) are related by following relation.

2
Pk


2
W âˆ’ j=1 zjâŠ—m  = kWk + k
o

âˆ’2 Trace W Ã—1 Z T Ã—2 . . . Ã—m Z T , (8)
where Z = [z1 . . . zk ]. It immediately follows that (7) is
a relaxation of (4), which is tighter than (5) because of the
non-negativity constraint.
Hypergraph reduction by clique averaging. A common
approach to partitioning general hypergraphs is by reducing it to a graph, and subsequently partitioning the equivalent graph. Two standard reductions include the clique and
star expansions (Agarwal et al., 2005). In fact, Agarwal
et al. (2006) showed that a variety of popular hypergraph
Laplacian formulations are equivalent one of these expansions. Moreover, it is known that for uniform hypergraphs,
the eigenvectors for both expansions are similar (Agarwal
et al., 2006), proving generality of clique expansion.

A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning

We show that the use of clique expansion for hypergraph
partitioning is a relaxation of (5). This is true as the affinity
matrix obtained after clique averaging is
Wc =

X WÂ·Â·i ...i
W(Â·, Â·, 1n , . . . , 1n )
3
m
=
,
(m
âˆ’
2)!
(m âˆ’ 2)!
i ,..,i
3

(9)

m

where we view W c as a bilinear functional, and 1n is a vector of all ones. A similar reduction of W was used in the
case of local linear approximation based grouping (AriasCastro et al., 2011). The final form in (9), ignoring constant scaling, clearly indicates that a spectral clustering of
the reduced graph is same as finding orthonormal vectors
z1 , . . . , zk that maximizes
k
X

W c (zj , zj ) =

j=1

k
X

W(zj , zj , 1n . . . . , 1n ).

(10)

j=1

From the representation in (6), we clearly see that the above
problem is a relaxation of (5), where we fix the last m âˆ’ 2
arguments to reduce (5) to a spectral problem.
Matching via tensor power iterations. In the matching
problem, given two sets of points (for instance, sets of features in two images), one needs to extract the points of correspondence. Let s be the size of each set, then one can find
s2 candidate matches. If X âˆˆ {0, 1}sÃ—s denotes the correspondence matrix, then kXk2F = s. Duchenne et al. (2011)
optimizes a score function over a vectorized form of X as
X
maximize
Wi1 ...im xi1 ...xim s.t. kxk22 = s, (11)
xâˆˆ{0,1}s2 i ,..,i
1
m

On the other hand, (12) corresponds to maximizing the ensemble m-way affinity of a cluster in (Liu et al., 2010;
Leordeanu & Sminchisescu, 2012). In practice, the use of
`1 -norm makes the solution sparse, and hard clustering is
achieved by sequentially extracting clusters and removing
them from the problem. The latter works further restrict the
search space to [0, ]n to extract larger clusters.
Though (12) involves a `1 -norm constraint, we claim that
the problem is a special case of (5). This can be seen by
constructing a (2m)-uniform hypergraph on the given n
vertices, whose affinities are given by the tensor W with
Wi1 i2 ...i2m = Wi1 i3 ...i2mâˆ’1 1{il =il+1 âˆ€l=1,3,..,mâˆ’1} . (13)
âˆš
Now, for x âˆˆ [0, 1]n , kxk1 = 1 and y = x (element-wise
root), one can verify that W(y, . . . , y) = W(x, . . . , x) and
kyk2 = 1. This immediately implies that the equivalence
of (12) with the single cluster case of (5).

4. Higher-order Clustering Algorithm
Here, we propose a spectral method that solves a relaxed
form of (5). We also provide a theoretical analysis of this
algorithm under a planted partition model. Note that, in
the literature, theoretical guarantees for higher-order clustering algorithms have been mostly overlooked. We note
that some of the tools used in our analysis are quite different from existing analysis for clustering (Arias-Castro
et al., 2011; Ghoshdastidar & Dukkipati, 2014) and in the
case of tensors (Anandkumar et al., 2014).
4.1. The Proposed Method

where W is constructed from m-way similarities among
the candidate matches. A similar optimization has been
considered in (Lee et al., 2011). Duchenne et al. (2011)
2
relaxed the search space to Rs , and solved (11) using tensor power iterations. Since, the objective in (11) is simply
W(x, . . . , x), above problem and its relaxation are identical to (4)-(5), where one finds a single cluster that maximizes normalized associativity, i.e., k = 1.
Methods with `1 -norm constraint. Given a mth -order
similarity tensor W among n data instances, this class of
algorithms extract a cluster by solving a generic problem
X
maximize
Wi1 ..im xi1 ...xim s.t. kxk1 = 1. (12)
xâˆˆ[0,1]n

i1 ,..,im

The justification for such an optimization varies in different
approaches. For instance, Rota Bulo & Pelillo (2013) originally viewed the above objective as the expected payoff
when each of m players in an evolutionary game chooses
one of n vertices of the hypergraph. The solution of (12),
x, is the probability distribution corresponding to the equilibrium strategy of the game (Rota Bulo & Pelillo, 2013).

The proposed method, listed in Algorithm 1, relaxes the
problem in (5), following the lines of the clique averaging
technique. To be precise, in (5), we replace Z in mode-3
to mode-m multiplication by a n Ã— k matrix with all entries as âˆš1n . The substituted matrix is not orthonormal, but
has columns of unit norm. Retaining Z in modes-1 and 2
relaxes (5) to a matrix eigendecomposition problem.
Algorithm 1 Tensor Spectral Hypergraph Partitioning
input m-way affinity tensor W; number of partitions k.
 T
 T
1n
1n
1: Compute matrix A = W Ã—3 âˆš
. . . Ã—m âˆš
,
n
n
where 1n is a n-dimensional vector of ones.
2: Compute matrix Z âˆˆ RnÃ—k of k leading orthonormal
eigenvectors of A.
3: Cluster rows of Z into k clusters by k-means.
output Assign node-i to partition-j if row-i of Z lies in
cluster-j.
Before we proceed to the analysis of the algorithm few
comments are in place. Note that the Algorithm 1 is listed

A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning

in such a way that it is suitable for a theoretical analysis.
In practice, one can incorporate few modifications, for instance, one can normalizes the rows of Z before performing k-means (Ng et al., 2002). Moreover, the complexity
of Algorithm 1 is O (knm ). Such complexity is due to the
computation of the tensor, and is common in tensor based
approaches. In practice, one uses sampling techniques to
reduce the complexity (Ghoshdastidar & Dukkipati, 2015).
4.2. Consistency of Algorithm 1
We analyze Algorithm 1 under a planted partition model
defined as follows. Consider a m-uniform random hypergraph on n vertices, where every m-edge occurs with probability q âˆˆ [0, 1). Divide the vertices into k disjoint classes
of sizes n1 , . . . , nk . Let c1 , . . . , ck âˆˆ {0, 1}n be the assignment vector for each class, and for j = 1, . . . , k, let
pj âˆˆ [0, 1 âˆ’ q]. Generate additional m-edges within each
class such that for vertices in class-j, m-edges occur with
probability (pj + q). The goal of Algorithm 1 is to determine the true assignments c1 , . . . , ck from the affinity
tensor W of a random realization of the hypergraph.
We note that the above model includes a number of learning problems, where higher-order methods are used. For
instance, the models for subspace clustering and feature
matching presented in (Ghoshdastidar & Dukkipati, 2014)
are both special cases of the above model. We also mention here that the model in (Ghoshdastidar & Dukkipati,
2014) in more general, and allows complicated random hypergraphs. However, our model suffices for standard learning problems. Few settings are mentioned below.
Example 1. When all classes are of equal size, and p1 =
. . . = pk are strictly positive, we obtain the model for subspace clustering (Ghoshdastidar & Dukkipati, 2014).
Example 2. One may extend this model to incorporate the
presence of outliers. The outliers are modelled as an additional class that is not strongly connected, i.e., pk+1 = 0.
Example 3. For the matching problem, âˆš
there are k = 2
n, while the reclasses â€“ the set
of
correct
matches
of
size
âˆš
maining n âˆ’ n matches are incorrect, and hence, p2 = 0.
The following result bounds the error incurred by Algorithm 1 under the above random model.
âˆš
Theorem 5. If there exists n0 such that Î´n > n log n
for all
 n â‰¥ n0 , then
 the number of misclustered vertices
knmax n log n
is O
almost surely as n â†’ âˆž. Thus, Algo2
Î´n

âˆš
rithm 1 is consistent whenever Î´n = Ï‰ knmax n log n .
We validate the significance of Theorem 5 in the case of
subspace clustering model (Example 1). Similar results can
be studied for other models. We let the number of partitions
to grow with the size of the hypergraph as k = O(n1/2m ).
This has been considered in (Ghoshdastidar & Dukkipati,
2014).

Corollary 6. Let k = O(n1/2m ) in the subspace clustering
problem where nj = nk and pj = p for all j = 1, . . . , k.
Then almost surely as n â†’ âˆž, the misclustering error of
Algorithm 1 is at most

|Mn | = O



log n

for all m â‰¥ 2, and p > 0.

1

p2 nmâˆ’3+ m

Thus, Algorithm 1 is almost surely consistent for all m â‰¥ 3,
|Mn |
while
eventually vanishes for m = 2.
n
We observe that the above bounds are quite similar to
the bounds obtained for the HOSVD based algorithm in
(Govindu, 2005; Ghoshdastidar & Dukkipati, 2014), which
is subsequently referred to as HOSVD. To elaborate on
their differences, we note that under the setting of Corollary 6, the misclustering error for HOSVD is
|MnHOSV D |


=O

(log n)2


1

p4 nmâˆ’3+ 2m

for m â‰¥ 2, p > 0.

Comparison of above two results clearly shows that the error bounds for Algorithm 1 are clearly better than that of
HOSVD, particularly in terms of the density gap p. More
precisely, if the gap between the intra-cluster and intercluster edge probabilities are small or decrease with n, then
Algorithm 1 is less affected compared to the HOSVD algorithm in (Ghoshdastidar & Dukkipati, 2014).
4.3. Proof of Theorem 5
Here, we present an outline of the proof of Theorem 5. The
proofs of the technical lemmas are given in the supplementary material. The key to our analysis is the correctness of
Algorithm 1 in the expected case. One can verify that in
presence of the partitions, the expected affinity tensor can
be expressed as

W := E[W|c1 , . . . , ck ] =

k
X

pj câŠ—m
+ q1âŠ—m
n ,
j

(14)

j=1

where 1âŠ—m
is a rank-one tensor formed from vector 1n . We
n
note that W has a CP-decomposition of rank (k +1), which
means that W can be expressed as a sum of (k + 1) rank-1
tensors, i.e., each of the (k+1) terms can be written as a mway outer product of a vector. But, the vectors forming the
rank-1 terms are not orthogonal or incoherent. Hence, one
cannot use standard tensor perturbation bounds (Anandkumar et al., 2014) to comment on the error of determining
the rank-one terms. The next result shows that Algorithm 1
is accurate in the expected case. We require the following.

A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning


Let j âˆ— = arg minj pj nmâˆ’1
,
j




1
mâˆ’1
mâˆ’1
âˆ’ pj âˆ— nj âˆ—
g = (0.5mâˆ’1) minâˆ— pj nj
,
j6=j
n
!
pj âˆ— nmâˆ’1
gqnj âˆ— n(0.5mâˆ’1)
jâˆ—
+
and Î´n =
.
(15)
2(g + qn0.5m )
2n(0.5mâˆ’1)
The quantity Î´n is a lower bound on the eigen-gap that separates the largest k eigenvalues from other eigenvalues.
Lemma 7. Let W be the input for Algorithm 1, and let Z âˆˆ
RnÃ—k be the corresponding eigenvector matrix. If Î´n > 0
for the model, then the rows i and j of Z are identical if
and only if vertices i and j belong to the same class.
Above result implies that the k-means algorithm clusters all
rows accurately in this case. However, in practice, we work
with a random realization W instead of W, which can be
viewed as a perturbation of W, ie W = W + E. We quantify the perturbation in terms of the following definition of
norm of a tensor (Anandkumar et al., 2014).
Definition 8 (Operator norm). The operator norm of a
mth -order n-dimensional tensor E is defined as
kEkop =

max

kx1 k2 =...=kxm k2 =1

|E(x1 , . . . , xm )| ,

where the maximum of the m-linear functional is taken over
all vectors x1 , . . . , xm âˆˆ Rn with kxi k2 = 1 for all i.
In addition, if E is symmetric, it is known that (Chen et al.,
2012)
kEkop = max |E(x, . . . , x)| .
kxk2 =1

Let Z be the eigenvector matrix associated with W. The
following result provides an upper bound on the perturbation of Z from Z in terms of the perturbation E.
Lemma 9. If kEkop < Î´n , then there exists an orthonormal
(rotation) matrix Q âˆˆ RkÃ—k such that
âˆš
2kkEkop
,
kZ âˆ’ ZQkF â‰¤
Î´n
where k Â· kF is the Frobenius norm.
The above result is useful only if kEkop is reasonably small.
This is ensured in the following lemma, where we use an
-net argument to bound kEkop with high probability.
Lemma 10. For any Î» > 0,


2Î»2
.
P(kEkop > Î») â‰¤ 2(1 + 2m)n exp âˆ’
m!m2
One needs
âˆš to choose Î» appropriately. In our case, choosing
Î» = n log n ensures that, for any
âˆš m, the above bound
vanishes as n â†’ âˆž. So, kEkop â‰¤ n log n almost surely

as n â†’ âˆž. This bound is appears to be significant even
in the context of existing works on random tensors. For
instance, though bounds on the expected operator norm are
known (Nguyen et al., 2010), Lemma 10 provides a similar,
yet simpler, bound that holds with high probability.
Combining the bound on kEkop with Lemma 9, we obtain
a bound on the difference in eigenspaces for the random
and expected cases. This result becomes interesting due to
the some keys observations (Rohe et al., 2011). These are
summarized in the next lemma, which uses the following
notations. Let nmax = maxj nj . Let Î³i be the ith row of
ZQ, and Î±i be the center of the cluster in which ith row of
Z is grouped. The following result characterizes the set


1
âˆš
.
Mn = i âˆˆ {1, . . . , n} : kÎ±i âˆ’ Î³i k2 â‰¥
2nmax
Lemma 11. Whenever i âˆˆ
/ Mn and vertices i,j are in different classes, kÎ±i âˆ’ Î³i k2 < kÎ±i âˆ’ Î³j k2 . Also, if global
optimum is achieved in k-means, then
2

|Mn | â‰¤ 8nmax kZ âˆ’ ZQkF .

(16)

The above lemma claims that for any vertex not in Mn ,
the k-means objective is reduced if the vertex is correctly
clustered. Hence, all misclustered vertices must belong to
Mn , and a bound on the number of such vertices is given
by (16). We note that, in practice, standard k-means algorithm finds a local minimum. However, there are variants of the k-means algorithm, for instance (Kumar et al.,
2004), which provide a near-optimal solution with error at
most (1 + ) times the optimal error for some  > 0. Using
such a method allows one to relax the condition of global
optimality, while the bound in (16) increases only by a constant factor of (1 + )2 . At this stage, one can combine the
above results to arrive at the claim of Theorem 5.

5. Experimental Results
We now numerically demonstrate the performance of Algorithm 1 in a number of problems.
5.1. Partitioning Uniform Random Hypergraphs
We first compare the performance of Algorithm 1 with
the HOSVD based algorithm in an artificial setting based
on the planted partition model. Here, we randomly generate uniform hypergraphs from the model described in
Section 4.2. In Figure 1, we consider bi-partitioning of
a m-uniform hypergraph, with inter-class edge probability
q = 0.2, and density gap p1 = p2 = 0.1. So, within class
edges occur with probability (p1 + q) = (p2 + q) = 0.3.
We consider 2, 3 and 4-uniform hypergraphs with varying
number of nodes. The results are averaged over 50 independent runs. Figure 1 shows that error incurred by Algo-

A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning

rithm 1 is less than HOSVD algorithm. It also shows that
the error reduces for tensors of higher order.

Dataset
Iris
Vertebral Column
Wine
Ionosphere
Habermanâ€™s Survival
Blood Transfusion

Spectral
0.117Â±0.138
0.345Â±0.017
0.342Â±0.084
0.325Â±0.000
0.423Â±0.082
0.325Â±0.000

Algorithm 1
0.094Â±0.114
0.333Â±0.002
0.331Â±0.040
0.316Â±0.000
0.392Â±0.000
0.319Â±0.000

Table 1. Performance of spectral clustering and Algorithm 1.

Figure 1. Number of nodes misclustered by HOSVD based approach (dashed lines) and Algorithm 1 (solid lines) as the total
number of nodes increases. The black, red and blue lines correspond to cases with m = 2, 3 and 4, respectively.

We conduct another study on bi-partitioning 3-uniform hypergraphs, where we fix q = 0.2 but the density gaps, p1 ,
p2 , are varied. We let p1 = p2 = p, which varies over
{0.025, 0.05, 0.075, 0.1}. Figure 2 shows the number of
misclustered nodes, averaged over 50 runs, as the hypergraph grows. Note that the problem becomes harder as
p reduces, and the performance of HOSVD is highly affected. But, the effect is much less in case of Algorithm 1.

tween two data instances x, y as exp(âˆ’Î²kx âˆ’ yk22 ) with Î²
being a tuning parameter. We show that simple 3-way extension of this gives more robust performance. For any 3
points, x, y, z, we compute the similarity among them as

	
exp âˆ’Î² max kx âˆ’ yk22 , ky âˆ’ zk22 , kz âˆ’ xk22 . (17)
We run spectral clustering and Algorithm 1, with above 3way similarity, on some 2 and 3 class datasets from UCI
repository (Frank & Asuncion, 2010), where each dataset
is normalized. Table 1 reports the mean and standard deviation of the fractional error incurred by both algorithms over
100 runs of k-means. The results show that slight reduction
in error is obtained in case of Algorithm 1. This is expected
since the 3-way similarity in (17) is essentially constructed
from pairwise relations. Moreover, in most cases, there is a
significant reduction in the standard deviation of the errors.
Since randomness is only at the k-means steps, this implies
that 3-way relations provide better embedding of the data
points, making the k-means step more consistent.
The above results indicate that even simple higher-order
extensions can improve the performance of the algorithm.
We show a sample result of how this approach can be extended to image segmentation. Figure 3 shows an image of
a maze, which is divided into two segments based on only
color and distance information. We can see that better segments are obtained when Algorithm 1 is run with the 3-way
similarity of (17). We note that above approaches are quite
simple, and do not use any filters or post-processing of the
segments. Better segments can be obtained using sophisticated hypergraph based methods (Kim et al., 2011).

Figure 2. Number of nodes misclustered by HOSVD based approach (dashed lines) and Algorithm 1 (solid lines) as total number of nodes increases. The black, red, blue and green lines correspond to density gap p = 0.025, 0.05, 0.075 and 0.1, respectively.

5.2. Comparison with Pairwise Similarity
Before discussing about problems, where higher-order relations are essential, we present a comparative study with
normalized spectral clustering (Ng et al., 2002). In spectral clustering, one often defines the pairwise similarity be-

In large datasets, for instance, in image segmentation, it
is expensive to compute the 3-way tensor. So, we use the
following approximation. We observe that in Algorithm 1,
the matrix A can be approximated (upto a scaling factor) as
n
s
1 X
1 X
Aij = âˆš
Wijl â‰ˆ âˆš
Wijlp ,
n
n p=1

(18)

l=1

where we randomly select s out of n samples. Alternatively, this implies that one select s data points, and constructs s similarity matrices, each computed by fixing one
data. A is approximated as a sum of these matrices.

A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning

are comparable. However, SNTF does not relax the problem (5), and hence, achieves minimum error. hMETIS is
poor than CA and Algorithm 1. HGT is known to be good
in removing outliers, but also labels true data as outliers.
Figure 3. (left) Original image; (middle) Segments obtained from
spectral clustering; (right) Segments obtained from Algorithm 1.

5.3. Subspace (Line) Clustering
We now focus on problems, where higher-order clustering
is required as pairwise relations are inappropriate in these
cases. For instance, in subspace clustering, each cluster
is formed by points that closely represent a subspace of
dimension less than the data dimension. In this section, we
conduct experiments on the line clustering problem, where
each cluster is a one-dimensional subspace (line).
We generate three random lines in [âˆ’1, 1]5 , and sample 20
points from each line. The points are perturbed by Gaussian noise of standard deviation Ïƒ = 0.02 and 0.05, respectively. For each value of Ïƒ, 20 random examples are generated, and clustered using the higher-order approaches discussed in this paper. Two such instances are shown in Figure 4. A 3-uniform hypergraph is constructed
 with affinities
among three points as exp âˆ’ Î²(Ïƒ22 + Ïƒ32 ) , where Ïƒi is ith
singular value of the 5 Ã— 3 matrix containing data vector in
each column. Note that (Ïƒ22 + Ïƒ32 ) is the least squared error
of fitting a line through these three points.
For clustering, we use non-negative tensor factorization
(SNTF) (Shashua et al., 2006), game theoretic method with
`1 -norm constraint (HGT) (Rota Bulo & Pelillo, 2013),
clique averaging (CA) (Agarwal et al., 2005) and Algorithm 1. All these algorithms solve the trace maximization problem (5). In addition, we also run HOSVD based
algorithm (Ghoshdastidar & Dukkipati, 2014) and a hypergraph partitioning algorithm popular in VLSI community
(hMETIS) (Karypis & Kumar, 2000). Table 2 shows the
percentage errors incurred by each method. As noted before, Algorithm 1 is better than HOSVD. The relaxation in
CA is similar to Algorithm 1, and so their performances

Figure 4. (left) 2-dimensional projection of three random lines in
[âˆ’1, 1]5 perturbed by Gaussian noise with Ïƒ = 0.02; (right)
three lines perturbed by Gaussian noise with Ïƒ = 0.05.

Algorithm
SNTF
hMETIS
HGT
HOSVD
CA
Algorithm 1

Error (Ïƒ = 0.02)
2.50
4.50
8.33
5.17
3.33
3.25

Error (Ïƒ = 0.05)
8.58
11.75
22.17
12.58
10.92
10.33

Table 2. Mean percentage error for different algorithms.

A real-world application of subspace clustering is encountered in motion segmentation, where one needs to group
different moving objects in a video. We conducted experiments on the Hopkins 155 database (Tron & Vidal, 2007),
and the results are given in the supplementary material. The
results show that Algorithm 1 performs better than most
approaches, but best results are obtained by sampled variants of HOSVD (Jain & Govindu, 2013; Ghoshdastidar &
Dukkipati, 2015). Thus, the accuracy of Algorithm 1 may
be improved by using better sampling techniques.

6. Concluding Remarks
This paper provides a unified objective for partitioning uniform hypergraphs by maximizing the normalized associativity of the partition. This general idea appears to be at
the heart of various higher-order clustering algorithms, but
was not previously formalized in the literature. The above
objective can be posed as a tensor trace maximization problem (Theorem 3) that is quite similar in spirit to the underlying problem of spectral clustering. Theorem 5 provides
an almost sure error bound for the proposed Algorithm 1
in a random setting. The result shows higher-order clustering is consistent, and Algorithm 1 is provably better than
the approach in (Ghoshdastidar & Dukkipati, 2014). Experiments validate this fact, and also provide insights into
different formulations of the trace maximization problem.
In summary, this paper addresses two important aspects
that has been crucial for the popularity of matrix spectral
methods, but has been overlooked in the case of higherorder methods: (1) a well-defined objective for hypergraph
partitioning, and (2) guarantees on the error incurred by
tensor-based clustering. In the process of addressing above
aspects, we also provide more general results. For instance,
the operator norm of tensors are often used to quantify perturbations in recent literature (Anandkumar et al., 2014).
Lemma 10 provides a bound on the tail probability of the
operator norm that is applicable in other problems as well.

A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning

Acknowledgments
D. Ghoshdastidar is supported by Google Ph.D. Fellowship
in Statistical Learning Theory.

References
Agarwal, S., Lim, J., Zelnik-Manor, L., Perona, P., Kriegman, D., and Belongie, S. Beyond pairwise clustering.
In IEEE Conference on Computer Vision and Pattern
Recognition, pp. 838â€“845, 2005.
Agarwal, S., Branson, K., and Belongie, S. Higher order
learning with graphs. In Proceedings of the International
Conference on Machine Learning, pp. 17â€“24, 2006.
Anandkumar, A., Ge, R., Hsu, D., and Kakade, S. M. A
tensor approach to learning mixed membership community models. Journal of Machine Learning Research, 15:
2239â€“2312, 2014.
Arias-Castro, E., Chen, G., and Lerman, G. Spectral clustering based on local linear approximations. Electronic
Journal of Statistics, 5:1537â€“1587, 2011.
Chen, B., He, S., Li, Z., and Zhang, S. Maximum block improvement and polynomial optimization. SIAM Journal
on Optimization, 22(1):87â€“107, 2012.
Chung, F. R. K. Spectral graph theory, volume 92. American Mathematical Soc., 1997.
Comon, P. From source separation to blind equalization:
Contrast based approaches. In International Conference
on Image and Signal Processing, 2014.
Cooper, J. and Dutle, A. Spectra of uniform hypergraphs.
Linear Algebra and its Applications, 436(9):3268â€“3292,
2012.
De Lathauwer, L., De Moor, B., and Vandewalle, J. A
multilinear singular value decomposition. SIAM Journal
on Matrix Analysis and Applications, 21(4):1253â€“1278,
2000.
Donath, W. E. and Hoffman, A. J. Lower bounds for the
partitioning of graphs. IBM Journal of Research and Development, 17(5):420â€“425, 1973.
Duchenne, O., Bach, F., Kweon, I.-S., and Ponce, J. A
tensor-based algorithm for high-order graph matching.
IEEE Trans. on Pattern Analysis and Machine Intelligence, 33(12):2383â€“2395, 2011.
Frank, A. and Asuncion, A. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml, University of California, Irvine, School of Information and Computer Sciences, 2010.

Ghoshdastidar, D. and Dukkipati, A. Consistency of spectral partitioning of uniform hypergraphs under planted
partition model. In Advances in Neural Information Processing Systems, 2014.
Ghoshdastidar, D. and Dukkipati, A. Spectral clustering using multilinear SVD: Analysis, approximations and applications. In Proceedings of the 29th AAAI Conference
on Artificial Intelligence, 2015.
Govindu, V. M. A tensor decomposition for geometric
grouping and segmentation. In IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1150â€“
1157, 2005.
Hein, M., Setzer, S., Jost, L., and Rangapuram, S. The
total variation on hypergraphs-learning on hypergraphs
revisited. In Advances in Neural Information Processing
Systems, pp. 2427â€“2435, 2013.
Hu, S. and Qi, L. Algebraic connectivity of even uniform
hypergraph. Journal of Combinatorial Optimization, 24:
564â€“579, 2012.
Ipsen, I. C. F. and Nadler, B. Refined perturbation bounds
for eigenvalues of hermitian and non-hermitian matrices.
SIAM Journal on Matrix Analysis and Applications, 31
(1):40â€“53, 2009.
Jain, S. and Govindu, V. M. Efficient higher-order clustering on the grassmann manifold. In IEEE International
Conference on Computer Vision, 2013.
Karypis, G. and Kumar, V. Multilevel k-way hypergraph
partitioning. VLSI Design, 11(3):285â€“300, 2000.
Kim, S., Nowozin, S., Kohli, P., and Yoo, C. D. Higherorder correlation clustering for image segmentation. In
Advances in Neural Information Processing Systems,
2011.
Kumar, A., Sabharwal, Y., and Sen, S. A simple linear time
(1 + )-approximation algorithm for geometric k-means
clustering in any dimensions. In Proceedings-Annual
Symposium on Foundations of Computer Science, 2004.
Lee, D. D. and Seung, H. S. Algorithms for non-negative
matrix factorization. In Advances in Neural Information
Processing Systems, pp. 556â€“562, 2001.
Lee, J., Cho, M., and Lee, K. M. Hyper-graph matching
via reweighted random walks. In IEEE Conference on
Computer Vision and Pattern Recognition, 2011.
Leordeanu, M. and Sminchisescu, C. Efficient hypergraph
clustering. In International Conference on Artificial Intelligence and Statistics, 2012.

A Provable Generalized Tensor Spectral Method for Uniform Hypergraph Partitioning

Lim, L.-H. Singular values and eigenvalues of tensors:
a variational approach. In Proceedings of the IEEE
International Workshop on Computational Advances in
Multi-Sensor Adaptive Processing, pp. 129â€“132, 2005.
Liu, H., Latecki, L. J., and Yan, S. Robust clustering as
ensembles of affinity relations. In Advances in Neural
Information Processing Systems, pp. 1414â€“1422, 2010.
Lloyd, S. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129â€“137, 1982.
Ng, A., Jordan, M., and Weiss, Y. On spectral clustering:
analysis and an algorithm. In Advances in Neural Information Processing Systems, pp. 849â€“856, 2002.
Nguyen, N. H., Drineas, P., and Tran, T. D. Tensor sparsification via a bound on the spectral norm of random
tensors. arXiv preprint, (arXiv:1005.4732), 2010.
Rohe, K., Chatterjee, S., and Yu, B. Spectral clustering and
the high-dimensional stochastic blockmodel. Annals of
Statistics, 39(4):1878â€“1915, 2011.
Rota Bulo, S. and Pelillo, M. A game-theoretic approach to
hypergraph clustering. IEEE Trans. on Pattern Analysis
and Machine Intelligence, 35(6):1312â€“1327, 2013.
Shashua, A., Zass, R., and Hazan, T. Multi-way clustering using super-symmetric non-negative tensor factorization. In European Conference on Computer Vision,
pp. 595â€“608, 2006.
Shi, J. and Malik, J. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888â€“905, 2000.
Stewart, G. W. and Sun, J. Matrix Perturbation Theory.
Academic Press, 1990.
Tron, R. and Vidal, R. A benchmark for the comparison of
3-D motion segmentation algorithms. In IEEE Conference on Computer Vision and Pattern Recognition, 2007.
von Luxburg, U. A tutorial on spectral clustering. Statistics
and computing, 17(4):395â€“416, 2007.
von Luxburg, U., Belkin, M., and Bousquet, O. Consistency of spectral clustering. Annals of Statistics, 36(2):
555â€“586, 2008.
Zass, R. and Shashua, A. Doubly stochastic normalization
for spectral clustering. In Advances in Neural Information Processing Systems, 2006.
Zha, H., He, X., Ding, C., Simon, H., and Gu, M. Spectral
relaxation for k-means clustering. In Advances in Neural
Information Processing Systems, 2001.

