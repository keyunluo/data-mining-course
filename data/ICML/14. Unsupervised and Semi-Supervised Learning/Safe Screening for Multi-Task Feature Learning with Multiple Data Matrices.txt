Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices

Jie Wang1
JWANGUMI @ UMICH . EDU
Jieping Ye1,2
JPYE @ UMICH . EDU
1
Department of Computational Medicine and Bioinformatics, University of Michigan, MI 48109 USA
2
Department of Electrical Engineering and Computer Science, University of Michigan, MI 48109 USA

Abstract
Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously.
However, solving the MTFL problem remains
challenging when the feature dimension is
extremely large. In this paper, we propose a
novel screening rule—that is based on the dual
projection onto convex sets (DPC)—to quickly
identify the inactive features—that have zero
coefficients in the solution vectors across all
tasks. One of the appealing features of DPC is
that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks.
Thus, by removing the inactive features from
the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of
our knowledge, it is the first screening rule that
is applicable to sparse models with multiple data
matrices. A key challenge in deriving DPC is to
solve a nonconvex problem. We show that we
can solve for the global optimum efficiently via a
properly chosen parametrization of the constraint
set. Moreover, DPC has very low computational
cost and can be integrated with any existing
solvers. We have evaluated the proposed DPC
rule on both synthetic and real data sets. The
experiments indicate that DPC is very effective in
identifying the inactive features—especially for
high dimensional data—which leads to a speedup
up to several orders of magnitude.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

1. Introduction
Empirical studies have shown that learning multiple
related tasks (MTL) simultaneously often provides superior predictive performance relative to learning each tasks
independently (Ando & Zhang, 2005; Argyriou et al., 2008;
Bakker & Heskes, 2003; Evgeniou et al., 2005; Zhang
et al., 2006; Chen et al., 2013). This observation also has
solid theoretical foundations (Ando & Zhang, 2005; Baxter, 2000; Ben-David & Schuller, 2003; Caruana, 1997), especially when the training sample size is small for each
task. One popular MTL method especially for highdimensional data is multi-task feature learning (MTFL),
which uses the group Lasso penalty to ensure that all tasks
select a common set of features (Argyriou et al., 2007).
MTFL has found great success in many real-world applications including but not limited to: breast cancer classification (Zhang et al., 2010), disease progression prediction (Zhou et al., 2012), gene data analysis (Kim & Xing, 2009), and neural semantic basis discovery (Liu et al.,
2009a). A major issue in MTFL—that is of great practical importance—is to develop efficient solvers (Liu et al.,
2009b; Sra, 2012; Wang et al., 2013a; Gong et al., 2014).
However, it remains challenging to apply the MTFL models to large-scale problems.
The idea of screening has been shown to be very effective
in scaling the data and improving the efficiency of many
popular sparse models, e.g., Lasso (El Ghaoui et al., 2012;
Wang et al., 2013b; Wang et al.; Xiang et al., 2011; Tibshirani et al., 2012), nonnegative Lasso (Wang & Ye, 2014),
group Lasso (Wang et al., 2013b; Wang et al.; Tibshirani
et al., 2012), mixed-norm regression (Wang et al., 2013a),
`1 -regularized logistic regression (Wang et al., 2014b),
sparse-group Lasso (Wang & Ye, 2014), support vector machine (SVM) (Ogawa et al., 2013; Wang et al., 2014a), and
least absolute deviations (LAD) (Wang et al., 2014a). Essentially, screening aims to quickly identify the zero components in the solution vectors such that the corresponding features—called inactive features (e.g., Lasso)—or data samples—called non-support vectors (e.g., SVM)—can
be removed from the optimization. Therefore, the size of

Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices

the data matrix and the number of variables to be computed
can be significantly reduced, which may lead to substantial
savings in the computational cost and memory usage without sacrificing accuracy. Compared to the solvers without
screening, the speedup gained by the screening methods
can be several orders of magnitude.
However, we note that all the existing screening methods
are only applicable to sparse models with a single data matrix. Therefore, motivated by the challenges posed
by large-scale data and the promising performance of existing screening methods, we propose a novel framework
for developing effective and efficient screening rules for a
popular MTFL model via the dual projection onto convex
sets (DPC). The framework of DPC extends the state-ofthe-art screening rule, called EDPP (Wang et al.), for the
standard Lasso problem (Tibshirani, 1996)—that assumes
a single data matrix—to a popular MTFL model—that involves multiple data matrices across different tasks. To the
best of our knowledge, DPC is the first screening rule that
is applicable to sparse models with multiple data matrices.
The DPC screening rule detects the inactive features by
maximizing a convex function over a convex set containing the dual optimal solution, which is a nonconvex problem. To find the region containing the dual optimal solution, we show that the corresponding dual problem can be
formulated as a projection problem—which admits many
desirable geometric properties—by utilizing the bilinearity
of the inner product. Then, by a carefully chosen parameterization of the constraint set, we transform the nonconvex problem to a quadratic programming problem over one
quadratic constraint (QP1QC) (Gay, 1981), which can be
solved for the global optimum efficiently. Experiments on
both synthetic and real data sets indicate that the speedup
gained by DPC can be orders of magnitude. Moreover,
DPC shows better performance as the feature dimension
increases, which makes it a very competitive candidate for
the applications of very high-dimensional data.
We organize this paper as follows. In Section 2, we briefly
review some basics of a popular MTFL model. Then, we
derive the dual problem in Section 3. Based on an indepth
analysis of the geometric properties of the dual problem
and the dual feasible set, we present the proposed DPC
screening rule in Section 4. In Section 5, we evaluate the
DPC rule on both synthetic and real data sets. We conclude
this paper in Section 6. Please refer to the journal version
(Wang & Ye, 2015) for proofs not included in the main text.
Notation: Denote the `2 norm by k · k. For x ∈ Rn , let
its ith component be xi , and the diagonal matrix with the
entries of x on the main diagonal be diag(x). For a set of
PT
positive integers {Nt : t = 1, . . . , T, t=1 Nt = N }, we
denote the tth subvector of x ∈ RN by xt such that x =
(xT1 , . . . , xTT )T , where xt ∈ RNt for t = 1, . . . , T . For

vectors x, y ∈ Rn , we use hx, yi and xT y interchangeably to denote the inner product. For a matrix M ∈ Rm×n ,
let mi , mj , and mij be its ith row, j th column and (i, j)th
entry, respectively.
We define the (2, 1)-norm of M by
Pm
kM k2,1 = i=1 kmi k. For two matrices A, B ∈ Rm×n ,
we define their inner product by hA, Bi = tr(AT B). Let I
be the identity matrix. For a convex function f (·), let ∂f (·)
be its subdifferential. For a vector x and a convex set C, the
projection operator is:
PC (x) := argminy∈C

1
2 ky

− xk.

2. Basics
In this section, we briefly review some basics of a popular
MTFL model and mention several equivalent formulations.
Suppose that we have T learning tasks {(Xt , yt ) : t =
1, . . . , T }, where Xt ∈ RNt ×d is the data matrix of the
tth task with Nt samples and d features, and yt ∈ RNt is
the corresponding response vector. A widely used MTFL
model (Argyriou et al., 2007) takes the form of
XT
2
1
min
(1)
2 kyt − Xt wt k + λkW k2,1 ,
W ∈Rd×T

t=1

where wt ∈ Rd is the weight vector of the tth task and
W = (w1 , . . . , wT ). Because the k · k2,1 -norm induces
sparsity on the rows of W , the weight vectors across all
tasks share the same sparse pattern. We note that the model
in (1) is equivalent to several other popular MTFL models.
The first example introduces a positive weight parameter ρt
for t = 1, . . . , T to each term in the loss function:
XT
2
1
min
2ρt kyt − Xt wt k + λkW k2,1 ,
W ∈Rd×T

t=1

et =
which reduces to (1) by setting y

y
√t
ρt

et =
and X

Xt
√
ρt .

The second example introduces another regularizer to (1):
XT
2
2
1
min
2 kyt − Xt wt k + λkW k2,1 + ρkW kF ,

W ∈Rd×T

t=1

where ρ is a positive parameter and k · kF is the Frobenius
norm. Let I ∈ Rd×d be the identity matrix and 0 be the
d-dimensional vector with all zero entries. By letting
p
X̄t = (XtT , 2ρt I)T , ȳt = (ytT , 0T )T , t = 1, . . . , T,
we can also simplify the above MTFL model to (1).
In this paper, we focus on developing the DPC screening
rule for the MTFL model in (1).

3. The Dual Problem
In this section, we show that we can formulate the dual
problem of the MTFL model in (1) as a projection problem
by utilizing the bilinearity of the inner product.

Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices

We first introduce a new set of variables:
zt = yt − Xt wt , t = 1, . . . , T.

(2)

Then, the MTFL model in (1) can be written as
min

XT

1
kzt k2
t=1 2

W,z

+ λkW k2,1 ,

(3)

zt = yt − Xt wt , t = 1, . . . , T.

s.t.

Let λθ ∈ RN be the vector of Lagrangian multipliers.
Then, the Lagrangian of (1) is
L(W, z; θ) =

XT
+

1
kzt k2 + λkW k2,1
t=1 2
XT
λ
hθt , yt − Xt wt −
t=1

(4)
zt i.

To get the dual problem, we need to minimize L(W, z; θ)
over W and z. We can see that
0 = ∇z L(W, z; θ) ⇒ argminz L(W, z; θ) = λθ.

(5)

subdifferential counterpart of the Fermat’s rule (Bauschke
& Combettes, 2011), i.e., 0 ∈ ∂f (`) (ŵ` ), yields:
( `
ŵ /kŵ` k,
if ŵ` 6= 0,
m` ∈
(9)
{u ∈ Rd : kuk ≤ 1}, if ŵ` = 0,
where ŵ` is the minimizer of f (`) (·).
We note that Eq. (9) implies km` k ≤ 1. If this is not the
case, then f ` (·) is not lower bounded (see the supplements
for discussions), i.e., minw` f ` (w` ) = −∞. Thus, by Eqs. (5) and (9), the dual function is
q(θ) = minW,z L(W, z; θ)
(10)
( 2
2
`
λ
− 2 kθk + λhθ, yi, km k ≤ 1, ∀ ` ∈ {1, . . . , d},
=
−∞,
otherwise.
Maximizing q(θ) yields the dual problem of (1) as follows:
max
θ

For notational convenience, let

s.t.

f (W ) = λkW k2,1 − λ

XT
t=1

hθt , Xt wt i.

{Ŵ : 0 ∈ ∂W L(Ŵ , z; θ)} = {Ŵ : 0 ∈ ∂ f (Ŵ )}.

t=1

XT
t=1

hXtT θt , wt i = hM, W i, (6)

where M = (X1T θ1 , . . . , XTT θT ). Eq. (6) expresses
hM, W i by the sum of the inner products of the corresponding columns. By the bilinearity of the inner product,
we can also express hM, W i by the sum of the inner products of the corresponding rows:
XT
t=1

hθt , Xt wt i = hM, W i =

Xd
`=1

hm` , w` i.

(7)

(t)

Denote the j th column of Xt by xj . We can see that
(1)

(2)

(T )

m` = (hx` , θ1 i, hx` , θ2 i, . . . , hx` , θT i).
Moreover, as kW k2,1 =

Pd

f (W ) = λ
(`)

`

`

`=1

(8)

kw` k, Eqs. (7) implies that:

Xd
`=1
`

f (`) (w` ),
`

t=1

λ2
2

y

 − θ2 ,
λ

(11)

(t)

hx` , θt i2 ≤ 1, ` = 1, . . . , d.

y

 − θ2 ,
λ
θ
XT
(t)
s.t.
hx` , θt i2 ≤ 1, ` = 1, . . . , d.

min

1
2

(12)

t=1

By the bilinearity of the inner product, we can decouple
f (W ) into a set of independent subproblems. Indeed, we
can rewrite the second term of f (W ) as
hθt , Xt wt i =

XT

−

It is evident that the problem in (11) is equivalent to

Thus, to minimize L(W, z; θ) with respect to W , it is
equivalent to minimize f (W ), i.e.,

XT

2
1
2 kyk

where f (w ) = kw k − hm , w i. Thus, to minimize
f (W ), we can minimize each f (`) (w` ) separately. The

In view of (12), it is indeed a projection problem. Let F be
the feasible set of (12). Then, the optimal solution of (12),
denoted by θ∗ (λ), is the projection of y/λ onto F, namely,

(13)
θ∗ (λ) = PF yλ .

4. The DPC Rule
In this section, we present the proposed DPC screening rule
for the MTFL model in (1). Inspired by the Karush-KuhnTucker (KKT) conditions (Güler, 2010), in Section 4.1, we
first present the general guidelines. The most challenging
part lies in two folds: 1) we need to estimate the dual optimal solution as accurately as possible; 2) we need to solve
a nonconvex optimization problem. In Section 4.2, we give
an accurate estimation of the dual optimal solution based on
the geometric properties of the projection operators. Then,
in Section 4.3, we show that we can efficiently solve for the
global optimum to the nonconvex problem. We present the
DPC rule for the MTFL model (1) in Section 4.4.
4.1. Guidelines for Developing DPC
We present the general guidelines to develop screening
rules for the MTFL model (1) via the KKT conditions.

Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices

Let W ∗ (λ) = (w1∗ (λ), . . . , wT∗ (λ)) be the optimal solution
(1). By Eqs. (2), (5) and (9), the KKT conditions are:
yt = Xt wt∗ (λ) + λθt∗ (λ), t = 1, . . . , T,
(14)
(
1,
if (w` )∗ (λ) 6= 0,
g` (θ∗ (λ)) ∈
` = 1, . . . , d. (15)
[−1, 1], if (w` )∗ (λ) = 0,
where (w` )∗ (λ) is the `th row of W ∗ (λ), and
XT
(t)
g` (θ) =
hx` , θt i2 , ` = 1, . . . , d.
t=1

(16)

4.2.2. T HE GENERAL CASES
Theorem 1 gives a closed form solution of θ∗ (λ) for λ ≥
λmax . Therefore, we can estimate θ∗ (λ) with λ < λmax
in terms of a known θ∗ (λ0 ). Specifically, we can simply
set λ0 = λmax and utilize the result θ∗ (λmax ) = y/λmax .
To make this paper self-contained, we first review some
geometric properties of projection operators.
Theorem 2. (Ruszczyński, 2006) Let C be a nonempty
closed convex set. Then, for any point ū, we have
u = PC (u) ⇔ u − u ∈ NC (u),

For ` = 1, . . . , d, Eq. (15) yields
(R)

where NC (u) = {v : hv, u0 − ui ≤ 0, ∀u0 ∈ C} is called
the normal cone to C at u ∈ C.

The rule in (R) provides a method to identify the rows in
W ∗ (λ) that have only zero entries. However, (R) is not
applicable to real applications, as it assumes knowledge
of θ∗ (λ), and solving the dual problem (12) could be as
expensive as solving the primal problem (1). Inspired by
SAFE (El Ghaoui et al., 2012), we can first estimate a set
Θ that contains θ∗ (λ), and then relax (R) as follows:

Another useful property of the projection operator in estimating θ∗ (λ) is the so-called firmly nonexpansiveness.
Theorem 3. (Bauschke & Combettes, 2011) Let C be a
nonempty closed convex subset of a Hilbert space H. The
projection operator with respect to C is firmly nonexpansive, namely, for any u1 , u2 ∈ H,

g` (θ∗ (λ)) < 1 ⇒ (w` )∗ (λ) = 0.

maxθ∈Θ g` (θ) < 1 ⇒ (w` )∗ (λ) = 0, ` = 1, . . . , d. (R∗ )
Therefore, to develop a screening rule for the MTFL model in (1), (R∗ ) implies that: 1) we need to estimate a region Θ—that turns out to be a ball (please refer to Section
4.2)—containing θ∗ (λ); 2) we need to solve the maximization problem—that turns out to be nonconvex (please refer
to Section 4.3)—on the left hand side of (R∗ ).
4.2. Estimation of the Dual Optimal Solution
Based on the geometric properties of the dual problem (12)
that is a projection problem, we first derive the closed form solutions of the primal and dual problems for specific
values of λ in Section 4.2.1, and then give an accurate estimation of θ∗ (λ) for the general cases in Section 4.2.2.
4.2.1. C LOSED FORM SOLUTIONS
The primal and dual optimal solutions W ∗ (λ) and θ∗ (λ)
are generally unknown. However, when the value of λ is
sufficiently large, we expect that W ∗ (λ) = 0, and θ∗ (λ) =
y
λ by Eq. (14). The following theorem confirms this.
Theorem 1. For the MTFL model in (1), let
r
XT
(t)
λmax = max
hx` , yi2 .
(17)
`=1,...,d

t=1

Then, the following statements are equivalent:
y
λ

∈ F ⇔ θ∗ (λ) = yλ ⇔ W ∗ (λ) = 0 ⇔ λ ≥ λmax .
Remark 1. Theorem 1 indicates that: both the primal and
dual optimal solutions of the MTFL model (1) admit closed
form solutions for λ ≥ λmax . Thus, we will focus on the
cases with λ ∈ (0, λmax ) in the rest of this paper.

kPC (u1 ) − PC (u2 )k2 + k(I − PC )(u1 ) − (I − PC )(u2 )k2
≤ ku1 − u2 k2 . (18)
The firmly nonexpansiveness of projection operators leads
to the following useful result.
Corollary 4. Let C be a nonempty closed convex subset of
a Hilbert space H and 0 ∈ C. For any u ∈ H, we have:
1. kPC (u)k2 + ku − PC (u)k2 ≤ kuk2 .
2. hu, u − PC (u)i ≥ 0.
Remark 2. Part 1 of Corollary 4 indicates that: if a closed
convex set C contains the origin, then, for any point u, the
norm of its projection with respect to C is upper bounded by
the norm of kuk. The second part is a useful consequence
of the first part and plays a crucial role in the estimation of
the dual optimal solution (see Theorem 5).
We are now ready to present an accurate estimation of the
dual optimal solution θ∗ (λ).
Theorem 5. For the MTFL model in (1), suppose that
θ∗ (λ0 ) is known with λ0 ∈ (0, λmax ]. Let g` be given by
Eq. (16) for ` = 1, . . . , d, and

	
`∗ ∈ argmax`=1,...,d g` (y) .
(19)
For any λ ∈ (0, λ0 ), we define
y
 λ0 − θ∗ (λ0 ), if λ0 ∈ (0, λmax ),


n(λ0 ) =
(20)
y
∇g`∗
λmax , if λ0 = λmax .
r(λ, λ0 ) =

y
λ

− θ∗ (λ0 ),

r⊥ (λ, λ0 ) = r(λ, λ0 ) −

hn(λ0 ), r(λ, λ0 )i
n(λ0 ).
kn(λ0 )k2

(21)
(22)

Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices

where ∆ > 0. Then, u∗ minimizes ψ(u) over the constraint
set if and only if there exists α∗ ≥ 0—that is unique—such
that (H + α∗ DT D)u∗ is positive semidefinite,

Then, the following holds:
1. n(λ) ∈ NF (θ∗ (λ)),
2. hy, n(λ0 )i ≥ 0,

(H + α∗ DT D)u∗ = −q,

3. hr(λ, λ0 ), n(λ0 )i ≥ 0,


4. θ∗ (λ) − θ∗ (λ0 ) + 12 r⊥ (λ, λ0 )  ≤ 12 kr⊥ (λ, λ0 )k.
Consider Theorem 5. Part 1 characterizes θ∗ (λ) via the
normal cone. Parts 2 and 3 illustrate key geometric identities that lead to the accurate estimation of θ∗ (λ) in part 4
(see supplement for details).

(27)

kDu∗ k = ∆, if α∗ > 0.
We are now ready to solve for s` (λ, λ0 ).

(28)

Theorem 7. Let o = o(λ, λ0 ) and u∗ be the optimal solution of problem (26) with ∆ = 21 kr⊥ (λ, λ0 )k, D = I,
(1)
(T )
H = − diag(2kxk` , . . . , 2kxk` ),

T
(1)
(1)
(T )
(T )
q = − 2kx` k|hx` , o1 i|, . . . , 2kx` k|hx` , oT i| ,
namely, there exists a α∗ ≥ 0 such that α∗ and u∗ solve
Eqs. (27) and (28). Let
n
o
(t)
(t )
ρ` = maxt=1,...,T kx` k, I` = t∗ : kx` ∗ k = ρ` .
Then, the following hold:
1. α∗ is unique, and α∗ ≥ 2ρ` .
2. We define ū ∈ RT by
(
−qt /(htt + 2ρ` ), if t ∈
/ I` ,
ūt =
0,
otherwise.

Remark 3. The estimation of the dual optimal solution in
DPC and EDPP (Wang et al.)—that is for Lasso—are both
based on the geometric properties of the projection operators. Thus, the formulas of the estimation in Theorem 5 are
similar to that of EDPP. However, we note that the estimations in DPC and EDPP are determined by the completely different geometric structures of the corresponding dual
feasible sets. Problem (12) implies that the dual feasible
set of the MTFL model (1) is much more complicated than
that of Lasso—which is a polytope (the intersection of a set
of closed half spaces). Therefore, the estimation of the duThen, we have
al optimal solution in DPC is much more challenging than
(
(t )
2ρ` , if kūk ≤ ∆, and hx` ∗ , ot∗ i = 0, for t∗ ∈ I` ,
that of EDPP, e.g., we need to find a vector in the normal
∗
α ∈
cone to the dual feasible set at y/λmax [see n(λmax )].
(2ρ` , ∞), otherwise.
For notational convenience, let
1
o(λ, λ0 ) = θ∗ (λ0 ) + r⊥ (λ, λ0 ).
2

(23)

Theorem 5 implies that θ∗ (λ) lies in the ball:


1 ⊥
Θ(λ, λ0 ) = θ : kθ − o(λ, λ0 )k ≤ kr (λ, λ0 )k . (24)
2
4.3. Solving the Nonconvex Problem
In this section, we solve the optimization problem in (R∗ )
with Θ given by Θ(λ, λ0 ) [see Eq. (24)], namely,


XT
(t)
2
s` (λ, λ0 ) = max
g` (θ) =
hx` , θt i . (25)
θ∈Θ(λ,λ0 )

t=1

Although g` (·) and Θ(λ, λ0 ) are convex, problem (25) is
nonconvex, as it is a maximization problem. However, we
can efficiently solve for the global optimal solutions to (25)
by transforming it to a QP1PC via a parametrization of the
constraint set. We first cite the following result.
Theorem 6. (Gay, 1981) Let H be a symmetric matrix and
D be a positive definite matrix. Consider
min

kDuk≤∆

1
ψ(u) = uT Hu + qT u,
2

(26)

3. Let V = {v ∈ RT : vt = 0 for t ∈
/ I` , kū + vk = ∆}.
Then, we have
(
ū + v, v ∈ V,
if α∗ = 2ρ` ,
u∗ ∈
−(H + α∗ I)−1 q, otherwise.
4. The maximum value of problem (25) is given by
PT
∗
(t)
s` (λ, λ0 ) = t=1 hx` , ot i2 + α2 ∆2 − 21 qT u∗ .
Proof. We first transform problem (25) to a QP1PC by a
parameterization of Θ(λ, λ0 ):
Θ(λ, λ0 )





 o1 + u1 θ1



..
= 
:
kuk
≤
r,
kθ
k
≤
1,
,
t
=
1,
.
.
.
,
T
,

t
.




oT + uT θ T
where u = (u1 , . . . , uT )T . We define


o1 + u1 θ 1


..
h` (u, θ) = g` 
 .
.
oT + uT θT
Thus, problem (25) becomes

s` (λ, λ0 ) = max
kuk≤∆

max

{θ:kθt k≤1,t=1,...,T }


h` (u, θ) .

Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices

By the Cauchy-Schwartz inequality, for a fixed u, we have
φ(u) =
=

max

{θ:kθt k≤1,t=1,...,T }

XT
t=1

h` (u, θ)

(t)

(t)

(t)

(t)

u2t kx` k2 + 2|ut |kx` k|hx` , ot i| + hx` , ot i2 .

Let −ψ(u) =
can see that

PT

t=1

(t)

(t)

(t)

u2t kx` k2 +2ut kx` k|hx` , ot i|. We

maxkuk≤r φ(u) = maxkuk≤r −ψ(u) +

XT
t=1

(t)

hx` , ot i2 .

Thus, problem (25) becomes
PT
(t)
s` (λ, λ0 ) = − minkuk≤r ψ(u) + t=1 hx` , ot i2 .
Therefore, to solve (25), it suffices to solve problem (26)
with ∆, D, H, and q as in the theorem.
The statement follows immediately from Theorem 6.
Remark 4. To develop the DPC rule, (R∗ ) implies that we
only need the maximum value of problem (25). Thus, Theorem 6 does not show the global optimal solutions. However, in view of the proof, we can easily compute the global
optimal solutions in terms of α∗ and u∗ .
Computing α∗ and u∗ Consider Theorem 7. If kūk ≤ ∆
(t )
and hx` ∗ , ot∗ i = 0 for t∗ ∈ I` , then α∗ and u∗ admit
closed form solutions. Otherwise, α∗ is strictly larger than
2ρ` , which implies that H + α∗ I is positive definite and
invertible. If this is the case, we apply Newton’s method
(Gay, 1981) to find α∗ as follows. Let
ϕ(α) = k(H + αI)−1 qk−1 − ∆−1 .
Because ϕ(·) is strictly increasing on (2ρ` , ∞), α∗ is the
unique root of ϕ(·) on (2ρ` , ∞). Let α0 = 2ρ` . Then, the
k th iteration of Newton’s method to solve ϕ(α∗ ) = 0 is:
uk = − (H + αk−1 I)−1 q,
αk =αk−1 + kuk k2

(29)
kuk k − ∆
.
+ αk−1 I)−1 uk

∆uTk (H

4.4. The Proposed DPC Rule
As implied by R∗ , we present the proposed screening rule,
DPC, for the MTFL model (1) in the following theorem.
Theorem 8. For the MTFL model (1), suppose that θ∗ (λ0 )
is known with λ0 ∈ (0, λmax ]. Then, we have

where s` (λ, λ0 ) is given by Theorem 7.

Corollary 9. DPC For the MTFL model (1), suppose that
we are given a sequence of parameter values λmax = λ0 >
λ1 > . . . > λK . Then, for any k = 1, 2, . . . , K − 1, if
W ∗ (λk ) is known, we have
s` (λk+1 , λk ) < 1 ⇒ (w` )∗ (λk+1 ) = 0,
where s` (λ, λ0 ) is given by Theorem 7.
We omit the proof of Corollary 9 as it is a direct application
of Theorem 8.

5. Experiments
We evaluate DPC on both synthetic and real data sets. To
measure the performance of DPC, we report the rejection
ratio, namely, the ratio of the number of inactive features identified by DPC to the actual number of inactive features.
We also report the speedup, i.e., the ratio of the running
time of solver without screening to the running time of
solver with DPC. The solver is from the SLEP package (Liu
et al., 2009c). For each data set, we solve the MTFL model
in (1) along a sequence of 100 tuning parameter values of λ
equally spaced on the logarithmic scale of λ/λmax from 1.0
to 0.01. We only evaluate DPC since no existing screening
rule is applicable for the MTFL model in (1).

(30)

As pointed out by Moré & Sorensen (1983), Newton’s
method is very efficient to find α∗ as ϕ(α) is almost linear
on (2ρ` , ∞). Our experiments indicates that five iterations
usually leads to an accuracy higher than 10−15 .

s` (λ, λ0 ) < 1 ⇒ (w` )∗ (λ) = 0, λ ∈ (0, λ0 ),

In real applications, the optimal parameter value of λ is
generally unknown. Commonly used approaches to determine an appropriate value of λ, such as cross validation
and stability selection, need to solve the MTFL model over
a grid of tuning parameter values λ1 > λ2 > . . . > λK ,
which is very time consuming. Inspired by the ideas of
Strong Rule (Tibshirani et al., 2012) and SAFE (El Ghaoui
et al., 2012), we develop the sequential version of DPC.
Specifically, suppose that the optimal solution W ∗ (λk ) is
known. Then, we apply DPC to identify the inactive features of MTFL model (1) at λk+1 via W ∗ (λk ). We repeat
this process until all W ∗ (λk ), k = 1, . . . , K are computed.

5.1. Synthetic Studies
We perform experiments on two synthetic data sets, called
Synthetic 1 and Synthetic 2, that are commonly used in
the literature (Tibshirani et al., 2012; Zou & Hastie, 2005).
Both synthetic 1 and Synthetic 2 have 50 tasks. Each task
contains 50 samples. For t = 1, . . . , 50, the true model is
yt = Xt wt∗ + 0.01,  ∼ N (0, 1).
For Synthetic 1, the entries of each data matrix Xt are
i.i.d. standard Gaussian
with pairwise correlation zero, i.e.,

(t)
(t)
= 0. For Synthetic 2, the entries of each
corr xi , xj
data matrix Xt are drawn from i.i.d. standard
Gaussian


(t)
(t)
with pairwise correlation 0.5|i−j| , i.e., corr xi , xj
=
0.5|i−j| . To construct wt∗ , we first randomly select 10% of
the features. Then, the corresponding components of wt∗

1

1

0.8

0.8

0.8

0.6

0.4

0.2
0.05

0.1

0.2

0.4

0.6

0.4

0.2

DPC

0.01 0.02

Rejection Ratio

1

Rejection Ratio

Rejection Ratio

Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices

1

0.05

6=6max

0.2

0.4

1

(b) Synthetic 1, d = 20000

0.8

0.8

0.4

0.2
0.2

0.4

0.6

0.4

0.2

DPC

0.1

Rejection Ratio

0.8

Rejection Ratio

1

0.6

1

0.05

6=6max

(d) Synthetic 2, d = 10000

0.1

0.2

0.4

1

0.2

0.4

6=6max

(e) Synthetic 2, d = 20000

0.6

0.4

0.2

DPC

0.01 0.02

0.1

(c) Synthetic 1, d = 50000

1

0.05

0.05

6=6max

1

0.01 0.02

DPC

0.01 0.02

6=6max

(a) Synthetic 1, d = 10000

Rejection Ratio

0.1

0.4

0.2

DPC

0.01 0.02

0.6

1

DPC

0.01 0.02

0.05

0.1

0.2

0.4

1

6=6max

(f) Synthetic 2, d = 50000

Figure 1. Rejection ratios of DPC on two synthetic data sets with different feature dimensions.

are populated from a standard Gaussian, and the remaining
ones are set to 0. For both Synthetic 1 and Synthetic 2,
we set the feature dimension to 10000, 20000, and 50000,
respectively. For each setting, we run 20 trials and report
the average performance in Fig. 1 and Table 1. Fig. 1 shows
the rejection ratios of DPC on Synthetic 1 and Synthetic
2. For all the six settings, the rejection ratios of DPC are
higher than 90%, even for small parameter values. This
demonstrates one of the advantages of DPC, as previous
empirical studies (El Ghaoui et al., 2012; Tibshirani et al.,
2012; Wang et al.) indicate that the capability of screening rules in identifying inactive features usually decreases
as the parameter value decreases. Moreover, Fig. 1 also
shows that as the feature dimension increases, the rejection
ratios of DPC become higher—that is very close to 1. This
implies that the potential capability of DPC in identifying
the inactive features on high-dimensional data sets would
be even more significant.
Table 1 presents the running time of the solver with and
without DPC. The speedup is very significant, which is
up to 60 times. Take Synthetic 1 for example. When the
feature dimension is 50000, the solver without DPC takes
about 40.68 hours to solve problem (1) at 100 paramater
values. In contrast, combined with DPC, the solver only
takes less than one hour to solve the same 100 problems—
which leads to a speedup about 60 times. Table 1 also
shows that the computational cost of DPC is very low—
which is negligible compared to that of the solver without screening. Moreover, as the rejection ratios of DPC

increases with feature dimension growth (see Fig. 1), Table
1 shows that the speedup by DPC increases as well.
5.2. Experiments on Real Data Sets
We perform experiments on three real data sets: 1) the
TDT2 text data set (Cai et al., 2009); 2) the animal data
set (Lampert et al., 2009); 3) the Alzheimers Disease Neuroimaging Initiative (ADNI) data set (http://adni.
loni.usc.edu/).
The Animal Data Set The data set consists of 30475 images of 50 animals classes. By following the experiment
settings in (Kang et al., 2011), we choose 20 animal classes
in the data set: antelope, grizzly-bear, killer-whale, beaver,
Dalmatian, Persiancat, horse, german- shepherd, bluewhale, Siamese-cat, skunk, ox, tiger, hippopotamus, leopard, moose, spidermonkey, humpback-whale, elephant, and
gorilla. We construct 20 tasks, each of which is a classification task of one type of animal against all the others. For
the tth task, we first randomly select 30 samples from the
tth class as the positive samples; and then we randomly select 30 samples from all the other classes as the negative
samples. We utilize all the seven sets of features kindly provided by Lampert et al. (2009): color histogram features,
local self-similarity features, PyramidHOG (PHOG) features, SIFT features, colorSIFT features, SURF features,
and DECAF features. Thus, each image is represented by
a 15036-dimensional vectors. Hence, the data matrix Xt of
the tt h task is of 60 × 15036, where t = 1, . . . , 20.

1

1

0.8

0.8

0.8

0.6

0.4

0.2
0.05

0.1

0.2

0.4

0.6

0.4

0.2

DPC

0.01 0.02

Rejection Ratio

1

Rejection Ratio

Rejection Ratio

Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices

1

0.05

6=6max

0.1

0.2

0.4

0.4

0.2

DPC

0.01 0.02

0.6

1

6=6max

(a) Animal, d = 15036

DPC

0.01 0.02

(b) TDT2, d = 24262

0.05

0.1

0.2

0.4

1

6=6max

(c) ADNI, d = 504095

Figure 2. Rejection ratios of DPC on three real data sets.

10000 405.75 0.7
Synthetic 1 20000 913.70 1.36
50000 2441.57 3.50

28.12
37.02
42.08

14.43
24.68
58.03

10000 406.85 0.70
Synthetic 2 20000 906.09 1.37
50000 2435.38 3.46

29.28
36.66
44.78

13.89
24.72
54.39

90%—on the aforementioned three real data sets. In particular, the rejection ratios of DPC on the ADNI data set
are higher than 99% at the 100 parameter values. Table 1
shows that the resulting speedup is very significant—that is
up to 270 times. We note that the feature dimension of the
ADNI data set is more than half million. Without screening, Table 1 shows that the solver takes about seven days
(approximately one week) to compute the MTFL model (1)
at 100 parameter values. However, integrated with the DPC
screening rule, the solver computes the 100 solutions in
about half an hour. The experiments again indicate that
DPC provides better performance (in terms of rejection ratios and speedup) for higher dimensional data sets.

16.36
44.11
35.34

19.05
21.74
272.37

6. Conclusion

Table 1. Running time (in minutes) for solving the MTFL model
(1) along a sequence of 100 tuning parameter values of λ equally
spaced on the logarithmic scale of λ/λmax from 1.0 to 0.01 by
(a): the solver (Liu et al., 2009c) without screening (see the third
column); (b): the solver with DPC (see the fifth column).
d

Animal
TDT2
ADNI

solver DPC DPC+solver speedup

15036 311.71 0.47
24262 958.66 1.87
504095 9625.58 21.13

The TDT2 Data Set The original data set contains 9394
documents of 30 categories. Each document is represented
by a 36771-dimensional vector. Similar to the Animal data
set, we construct 30 tasks, each of which is a classification task of one category against all the others (Amit et al.,
2007). Also, for the tth task, we first randomly select 50
samples from the tth category as the positive samples, and
then we randomly select 50 samples from all the other categories as the negative samples. Moreover, we remove the
features that have only zero entries, thus leaving us 24262
features. Hence, the data matrix Xt of the tt h task is of
100 × 24262, where t = 1, . . . , 30.
The ADNI Data Set The data set consists of 747 patients
with 504095 single nucleotide polymorphisms (SNPs), and
the volume of 93 brain regions for each patient. We first
randomly select 20 brain regions. Then, for each region, we
randomly select 50 patients, and utilize the corresponding
SNPs data as the data matrix and the volumes of that brain
region as the response. Thus, we have 20 tasks, each of
which is a regression task. The data matrix Xt of the tth
task is of 50 × 504095, where t = 1, . . . , 20.
Fig. 2 shows the rejection ratios of DPC—that are above

In this paper, we propose a novel screening method for the
MTFL model in (1), called DPC. The DPC screening rule
is based on an indepth analysis of the geometric properties
of the dual problem and the dual feasible set. To the best of
our knowledge, DPC is the first screening rule that is applicable to sparse models with multiple data matrices. DPC
is safe in the sense that the identified features by DPC are
guaranteed to have zero coefficients in the solution vectors
across all tasks. Experiments on synthetic and real data sets demonstrate that DPC is very effective in identifying the
inactive features, which leads to a substantial savings in
computational cost and memory usage without sacrificing
accuracy. Moreover, DPC is more effective as the feature
dimension increases, which makes DPC a very competitive
candidate for the applications of very high-dimensional data. We plan to extend DPC to more general MTFL models,
e.g., the MTFL models with multiple regularizers.

Acknowledgments
We would like to acknowledge support for this project
from the National Science Foundation (IIS-0953662, IIS1421057, and IIS-1421100) and the National Institutes of
Health (R01 LM010730 and U54 EB020403).

Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices

References

Gong, P., Zhou, J., Fan, W., and Ye, J. Efficient multitask feature learning with calibration. In International
Conference On Knowledge Discovery and Data Mining,
2014.

Amit, Y., Fink, M., Srebro, N., and Ullman, S. Uncovering shared structures in multiclass classification. In Proceedings of the 24th Annual International Conference on
Machine Learning, 2007.

Güler, O. Foundations of Optimization. Springer, 2010.

Ando, R. and Zhang, T. A framework for learning predictive structures from multiple tasks and unlabeled data.
Journal of Machine Learning Research, 6:1817–1853,
2005.

Kang, Z., Grauman, K., and Sha, F. Learning with whom
to share in multi-task feature learning. In Proceedings of
the 28th Annual International Conference on Machine
Learning, 2011.

Argyriou, A., Evgeniou., T., and Pontil, M. Multi-task feature learning. In Advances in neural information processing systems, 2007.
Argyriou, A., Evgeniou, T., and Pontil, M. Convex multitask feature learning. Machine Learning, 73:243–272,
2008.
Bakker, B. and Heskes, T. Task clustering and gating for
bayesian multictask learning. Journal of Machine Learning Research, 4:83–99, 2003.
Bauschke, H. H. and Combettes, P. L. Convex Analysis and
Monotone Operator Theory in Hilbert Spaces. Springer,
2011.
Baxter, J. A model for inductive bias learning. Journal of
Artificial Intelligence Research, 12:149–198, 2000.
Ben-David, S. and Schuller, R. Exploiting task relatedness
for multiple task learning. In Proceedings of Computational Learning Theory, 2003.
Cai, Deng, Wang, Xuanhui, and He, Xiaofei. Probabilistic
dyadic data analysis with local and global consistency.
In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 105–112, 2009.
Caruana, R. Multitask learning. Machine Learning, 28:
41–75, 1997.
Chen, J., Tang, L., Liu, J., and Ye, J. A convex formulation for learning shared structures from multiple tasks.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 35:1025–1038, 2013.
El Ghaoui, L., Viallon, V., and Rabbani, T. Safe feature elimination in sparse supervised learning. Pacific Journal
of Optimization, 8:667–698, 2012.
Evgeniou, T., Micchelli, C., and Pontil, M. Learning multiple tasks with kernel methods. Journal of Machine
Learning Research, 6:615–637, 2005.
Gay, D. Computing optimal locally constrained steps.
SIAM Journal on Scientific and Statistical Computing,
1981.

Kim, S. and Xing, E. Tree-guided group lasso for multitask regression with structured sparsity. In Proceedings
of the 26th Annual International Conference on Machine
Learning, 2009.
Lampert, C., Nickisch, H., and Harmeling, S. Learning to
detect unseen object classes by between-class atttribute
transfer. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2009.
Liu, H., Palatucci, M., and Zhang, J. Bsparsity coordinate
descent procedures for the multi-task with applications to neural semantic basis discovery. In International
Conference on Machine Learning, 2009a.
Liu, J., Ji, S., and Ye, J. Multi-task feature learning with efficient `2,1 -norm minimization. In The 25th Conference
on Uncertainty in Artificial Intelligence, 2009b.
Liu, J., Ji, S., and Ye, J. SLEP: Sparse Learning with Efficient Projections. Arizona State University, 2009c.
Moré, J. and Sorensen, D. Computing a trust region step.
SIAM Journal on Scientific and Statistical Computing,
1983.
Ogawa, K., Suzuki, Y., and Takeuchi, I. Safe screening
of non-support vectors in pathwise SVM computation.
In Proceedings of the 30th Annual International Conference on Machine Learning, 2013.
Ruszczyński, A. Nonlinear Optimization. Princeton University Press, 2006.
Sra, S. Fast projections onto mixed-norm balls with applications. Data Mining and Knowledge Discovery, 2012.
Tibshirani, R. Regression shringkage and selection via the
lasso. Journal of the Royal Statistical Society Series B,
58:267–288, 1996.
Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N.,
Taylor, J., and Tibshirani, R. Strong rules for discarding
predictors in lasso-type problems. Journal of the Royal
Statistical Society Series B, 74:245–266, 2012.

Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices

Wang, J. and Ye, J. Two-layer feature reduction for sparsegroup lasso via decomposition of convex sets. In Advances in Neural Information Processing Systems, 2014.
Wang, J., Wonka, P., and Ye, J. Lasso screening rules via
dual polytope projection. Journal of Machine Learning
Research, to appear.
Wang, J., Liu, J., and Ye, J. Efficient mixed-norm regularization: Algorithms and safe screening methods. arXiv:1307.4156, 2013a.
Wang, J., Zhou, J., Wonka, P., and Ye, J. Lasso screening
rules via dual polytope projection. In Advances in Neural
Information Processing Systems, 2013b.
Wang, J., Wonka, P., and Ye, J. Scaling SVM and least absolute deviations via exact data reduction. In Proceedings of the 31th Annual International Conference on Machine Learning, 2014a.
Wang, J., Zhou, J., Liu, J., Wonka, P., and Ye, J. A safe
screening rule for sparse logistic regression. In Advances
in Neural Information Processing Systems, 2014b.
Wang, Jie and Ye, Jieping. Safe screening for multitask feature learning with multiple data matrices. arXiv:1505.04073, 2015.
Xiang, Z. J., Xu, H., and Ramadge, P. J. Learning sparse
representation of high dimensional data on large scale
dictionaries. In Advances in Neural Information Processing Systems, 2011.
Zhang, J., Ghahramani, Z., and Yang, Y. Learning multiple related tasks using latent independent component
analysis. In Advances in Neural Information Processing
Systems, 2006.
Zhang, Y., Yeung, D., and Xu, Q. Probabilistic multi-task
feature selection. In Advances in Neural Information
Processing Systems, 2010.
Zhou, J., Liu, J., Narayan, V., and Ye, J. Modeling disease progression via fused sparse group lasso. In International Conference On Knowledge Discovery and Data
Mining, 2012.
Zou, H. and Hastie, T. Regularization and variable selection via the elastic net. Journal of the Royal Statistical
Society Series B, 67:301–320, 2005.

