A Convex Optimization Framework for Bi-Clustering

Shiau Hong Lim
National University of Singapore, 9 Engineering Drive 1, Singapore 117575
Yudong Chen
University of California, Berkeley, CA 94720, USA

YUDONG . CHEN @ EECS . BERKELEY. EDU

Huan Xu
National University of Singapore, 9 Engineering Drive 1, Singapore 117575

Abstract
We present a framework for biclustering and
clustering where the observations are general labels. Our approach is based on the maximum
likelihood estimator and its convex relaxation,
and generalizes recent works in graph clustering
to the biclustering setting. In addition to standard biclustering setting where one seeks to discover clustering structure simultaneously in two
domain sets, we show that the same algorithm
can be as effective when clustering structure only
occurs in one domain. This allows for an alternative approach to clustering that is more natural
in some scenarios. We present theoretical results
that provide sufficient conditions for the recovery of the true underlying clusters under a generalized stochastic block model. These are further
validated by our empirical results on both synthetic and real data.

1. Introduction
In a regular clustering task, we look for clustering structure
within a set F through observing the pairwise interactions
between elements in this set. In biclustering, we instead
have two sets F and G and the observed pairwise interactions are between object pairs (i, j) with i ∈ F and j ∈ G.
The aim is to discover clustering structure within F, G or
both through these observations. For example, in a recommender system, F consists of customers and G a set of
products. In DNA microarray analysis, F could be biological samples and G a set of genes. The standard clustering
task can be viewed as a special case of biclustering with
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

SHONGLIM @ GMAIL . COM

MPEXUH @ NUS . EDU . SG

F = G.
While many biclustering algorithms have been proposed
and applied to a variety of problems, most are without
formal guarantees in terms of the actual clustering performance. A typical approach begins with an objective
function that measures the quality or cost of a candidate
clustering, and then searches for one that optimizes the
objective. Many interesting objective functions have intractable computational complexity and the focus of many
past works have been on finding efficient approximate solutions to these problems.
We propose a tractable biclustering algorithm based on
convex optimization. The algorithm can be viewed as a
convex relaxation to the computationally intensive problem of finding a maximum likelihood solution under a
generalized stochastic block model. The stochastic block
model (Holland et al., 1983; Rohe et al., 2011) has been
widely used in graph clustering. In this model, it is assumed that a true but unknown underlying clustering exists
in both F and G. A probabilistic generative model is defined for the observations and the performance of the clustering algorithm is evaluated in terms of the ability to recover the true underlying clusters.
Our main contribution is in extending the current advances
in standard graph clustering to the biclustering setting. We
provide the conditions under which our biclustering algorithm can recover the true clusters with high probability.
Our theoretical results are consistent with existing results
in graph clustering, which have been shown to be optimal
in some cases.
One novel aspect of our result is in providing new insight
on the case where clustering structure only occurs in one
domain set, say F but not necessarily in G. We show that
under reasonable assumptions, the same algorithm can be
used to recover the clusters in F regardless of the clustering structure in G. This provides an alternative approach to

A Convex Optimization Framework for Bi-Clustering

standard graph clustering, where instead of relying on pairwise interactions within F, we cluster objects in F through
their interactions with elements of G.
We employ the observation model as proposed in (Lim
et al., 2014) where each observation is a label Λij ∈ L.
The label set L can be very general. In standard graph
clustering, L would consist of two labels “edge” and “noedge”. An additional “unknown” label may be included
for partially observed graphs. We refer the reader to (Lim
et al., 2014) for examples of other, more complex label sets,
which include observations from time-varying graphs.
The paper is organized as follows. After discussing related
works in the next section, we present the formal setup of
our approach in Section 3. The main algorithm and its
theoretical results are presented in Section 4. An implementation of the main algorithm is provided in Section 5.
Empirical results on both synthetic and real-world data are
presented in Section 6. The proofs of the theoretical results
are given in the supplementary materials.

2. Related Work
A comparative study of many biclustering algorithms in
the domain of analyzing gene expression data is provided
in (Eren et al., 2012), and a comprehensive survey can be
found in (Tanay et al., 2005). Beginning with the work of
Hartigan (1972) and Cheng & Church (2000), many approaches to biclustering are based on optimizing certain
combinatorial objective functions, which are typically NPHard, and heuristic and approximate algorithms have been
developed but with no formal performance guarantees. One
notable exception that is related to our settings with labels, is the work of Wulff et al. (2013). They proposed a
very intuitive monochromatic cost function, proved its NPhardness and developed a polynomial-time approximate algorithm. Another related approach is correlation clustering (Bansal et al., 2004), which was originally developed
for clustering but can be extended to biclustering; results
on computational complexity and approximate algorithms
can be found in, e.g., (Demaine et al., 2005; Swamy, 2004;
Puleo & Milenkovic, 2014). A popular approach to clustering and biclustering is spectral clustering and its variants (Chaudhuri et al., 2012; Rohe et al., 2011; Anandkumar et al., 2013; Kannan et al., 2000; Shamir & Tishby,
2011; Lelarge et al., 2013; McSherry, 2001).
Here we focus on average-case performance under a probabilistic generative model for generalized graphs with labels,
and our algorithms are inspired by recent convex optimization approaches to graph clustering (Mathieu & Schudy,
2010; Ames & Vavasis, 2011; Lim et al., 2014; Chen et al.,
2012; 2014; Cai & Li, 2014; Vinayak et al., 2014). For biclustering, the work by Ames (2013); Kolar et al. (2011)

considers the setting with a “block-diagonal” structure (in
the matrix B to be defined below). The recent work by
Xu et al. (2014) studies a more general setting with “blockconstant” structure. Both these settings are special cases
of ours with 3 labels (1, −1 and ”unobserved”) and with
clusters in both the rows and columns.

3. Problem Setup
A bicluster is defined as a cluster-pair (C, D) with C ⊆ F
and D ⊆ G. We say that (i, j) is a member-pair of (C, D)
if i ∈ C and j ∈ D. The key property that is shared by
member-pairs of the same bicluster is the label distribution.
Ideally, if two pairs (i, j) and (i0 , j 0 ) belong to the same
bicluster, then their respective labels Λij and Λi0 j 0 should
have similar distributions.
Let n1 = |F|, n2 = |G| and n = max{n1 , n2 }. Without
loss of generality, we use i = 1 . . . n1 to denote members
of F and j = 1 . . . n2 to denote members of G.
We assume an underlying clustering in F such that there
exists a partition of F into r1 disjoint subsets {Cp : p =
1 . . . r1 }. Similarly, G is partitioned into {Dq : q =
1 . . . r2 }. These partitionings result in a total of r1 × r2
biclusters (Cp , Dq ). Let Kp = |Cp | and Lq = |Dq | be the
respective cluster sizes of Cp and Dq , with K = minp Kp
and L = minq Lq .
We group all the biclusters into two classes. This is specified using an r1 ×r2 matrix B whose entries Bpq ∈ {b0 , b1 }
where b0 and b1 (b0 < b1 ) are two arbitrarily defined real
numbers, identifying the class of each bicluster. Memberpairs of each bicluster share the same class. This is specified using an n1 ×n2 matrix Y ∗ , where Yij∗ = Bpq if i ∈ Cp
and j ∈ Dq .
Associated with each class is a set of label-generating distributions. We assume that each observed label Λij is
generated independently from some distribution µij . If
Yij∗ = b1 then µij ∈ M, otherwise if Yij∗ = b0 then
µij ∈ N . In the simplest case, there are only two label
distributions µ and ν such that M = {µ} and N = {ν}.
In this case Λij ∼ µ if Y ∗ = b1 and Λij ∼ ν if Y ∗ = b0 .
Let UC be an n1 × r1 matrix denoting thepmembership
of each cluster Cp , where (UC )ip = 1/ Kp if i ∈
Cp , otherwise (UC )ip = 0. Note that each row of UC
contains only one non-zero entry. Similarly, VD is an
n2 ×√r2 membership
matrix for clusters Dq . Let K =
p
diag( K1 . . . Kr1 ) be a r1 × r1 diagonal matrix. Simip
√
larly let L = diag( L1 . . . Lr2 ).
Y ∗ can therefore be related to B by:

Y ∗ = UC KBLVD> .

A Convex Optimization Framework for Bi-Clustering

Let the reduced SVD of KBL be UB SB VB> . It is easy
to see that a valid SVD of Y ∗ is given by U = UC UB ,
V = VD VB and S = SB such that Y ∗ = U SV > .

Note that the probability in Theorem 1 is with respect to
the randomness of the observed labels, given a fixed B and
Y ∗.

The difficulty of the biclustering task, especially when B
is non-square, depends on the r1 × r2 matrix KBL =
UB SB VB> . To capture this dependence, we introduce the
notion of coherence, defined as

Theorem 1 is general in the sense that it applies to any
bounded weight function w and any choice of b0 < b1 1 .
The clustering structure in F and G is reflected by the dependence on u1 , u2 , K and L. The proof for Theorem 1
(given in the supplementary material) follows the idea as in
the proof of Theorem 1 in (Lim et al., 2014), but with extra
consideration for the above structural properties. For the
case of standard clustering, where F = G, both B and Y ∗
are symmetric and we have u1 = u2 and K = L. In this
case we recover almost all the results in (Lim et al., 2014).

u1 =
u2 =

max

kUB UB> ep k2 =

max

kVB VB> eq k2 =

p∈{1...r1 }

q∈{1...r2 }

max

kUBp k2 ,

max

kVBq k2

p∈{1...r1 }

q∈{1...r2 }

where ei is the i-th standard basis vector and we define
UBp = UB> ep and similarly VBq = VB> eq . Since both UB
and VB have orthonormal columns, it is straightforward to
see that r11 ≤ u1 ≤ 1 and r12 ≤ u2 ≤ 1. Our definition of
coherence is based on similar notion in the literature of lowrank matrix estimation/approximation. In our biclustering
setting, it characterizes how easy it is to infer the structure
in the columns from that in the rows, and vice versa.

4. Algorithm and Main Results
The biclustering task is to find Y ∗ given the observations Λ.
The algorithm consists of two steps. First, a weight function w : L → R is chosen to construct an n1 × n2 weight
matrix W , where Wij = w(Λij ). The second step involves
solving the following convex optimization problem for the
n1 × n2 matrix Y :
max
Y

hW, Y i − λkY k∗

(1)

s.t. b0 ≤ Yij ≤ b1 , ∀i, j
√
where λ = c n log n for some sufficiently large constant
c and kY k∗ denotes the nuclear norm of Y .
Our main theorem provides the sufficient condition for the
exact recovery of the true clustering matrix Y ∗ using program (1) with high probability. The condition depends on
the following population quantities:
E1 := min Eµ w,

E0 := min Eµ [−w],

It is important to note that any two clusters in F or G can
only be distinguished if the corresponding rows or columns
in B are unique. It is, however, unclear whether the fullrank requirement for B is strictly necessary for the purpose
of recovering Y ∗ .
One sided clusters: For the case of biclustering, both K
and L play a similar role in Theorem 1. If one uses the
naive bound u1 ≤ 1 and u2 ≤ 1 then it suggests that sufficiently large clusters in both F and G are necessary for (1)
to be successful. This renders the result particularly weak
when, for example, L is small relative to K. Yet, the matrix
B has the same low rank as long as K is large, regardless
of L. In this case, one should still be able to recover the
clusters in F.
The following result shows that this is indeed the case, assuming that B has rather uniformly spread out ±1 entries:
r1
for
Theorem 2. Suppose that r1 ≤ r2 and r1 ≥ β log
c
K
L
some universal constant c. Let φ = maxp Kp , ψ = maxq Lq
and b such that |w(l)| ≤ b, ∀l ∈ L. Suppose that Bpq
for each (p, q) is independent ±1 random variable with
Pr(Bpq = +1) = Pr(Bpq = −1) = 21 . There exists a
universal constant c0 such that if
√
 0

c n1
bβ log n + βV n log n
min{E1 , E0 } >
φψ 2 n2
K

V := max Varµ w,

then the solution of (1) is unique and equals Y ∗ with probability at least 1 − n−β .

where Eµ and Varµ denote the expectation and variance
under the distribution µ. With this notation, we have the
following:
Theorem 1. Suppose that B is full rank and |w(l)| ≤
b, ∀l ∈ L. There exists a universal constant c such that
if

Note that the probability in Theorem 2 is with respect
to both the randomness in B as well as the observed labels, holding the clusterings in F and G fixed. The proof
(again, given in the supplementary material) is via bounding max{u1 /K, u2 /L} by applying a bound on the singular values of B by Rudelson & Vershynin (2009).

µ∈M

µ∈N

µ∈M∪N



nu u o
p
1
2
min{E1 , E0 } > c bβ log n + βV n log n max
,
K L

then with probability at least 1 − n−β the solution of (1) is
unique and equals Y ∗ .

Theorem 2 implies that the success of (1) is essentially independent of L when we are only interested in clustering
1
For all practical purposes, the choice of either (b0 , b1 ) =
(0, 1) or (b0 , b1 ) = (−1, 1) should suffice.

A Convex Optimization Framework for Bi-Clustering

F. In particular, we can think of each row in the observed
matrix as the feature representation (with G the feature set)
of the corresponding element in F. As long as elements
that belong to the same cluster in F have the same feature
relationships, we can perform clustering through these features regardless of whether the features themselves show
a clustering structure. This is illustrated in Figure 1. The
example on the left shows clustering structure only in the
rows (each column is unique) while the example on the
right shows clustering structure in both the rows and the
columns. According to Theorem 2 program (1) should be
equally effective in both cases, with the same order-wise
dependency on K.
It is interesting to note that in the unbalanced case where
n1 is fixed, the condition of success improves as n2 grows,
even when the cluster sizes K and L remain the same.
This situation cannot occur in standard clustering, where
the problem always gets harder as n grows if K stays the
same. This phenomenon is not discussed in the previous
work (Kolar et al., 2011; Ames, 2013) on biclustering.

We have the following guarantee for the performance using
this weight function.
Theorem 3. Suppose M = {µ} and N = {ν}. Suppose
that B is full rank and | log µ(l)
ν(l) | ≤ b, ∀l ∈ L. Let ζ =
D(ν||µ)
max{ D(µ||ν)
D(ν||µ) , D(µ||ν) }. There exists a universal constant c
such that if

min{D(µ||ν), D(ν||µ)}


u21 u22
,
K 2 L2





u21 u22
,
K 2 L2



> cβ(ζ + 1)(b + 2)(n log n) max
or
X (µ(l) − ν(l))2
l∈L

µ(l) + ν(l)

> cβ(ζ + 1)(b + 2)(n log n) max

then with probability at least 1 − n−β the solution of (1)
with w = W MLE is unique and equals Y ∗ .
D(·k·) in the theorem denotes the KL-divergence.
The weight function wMLE has been shown to be optimal up
to a constant factor, at least for the case with two equal-size
clusters (Lim et al., 2014).
4.2. Monotonicity

Figure 1. Two clustering/biclustering structures

4.1. Optimal Weights
We now consider the case where there are only two label
distributions, i.e. M = {µ} and N = {ν}.
Following previous works such as (Lim et al., 2014) and
(Chen et al., 2012) we can view our algorithm as a convex
relaxation of the maximum likelihood estimator, which is
given by
arg max log Pr(Λ|Y ∗ = Y )
Y a cluster matrix

= arg max

X

Yij −b0

Y a cluster matrix i,j

= arg max

X

Y a cluster matrix i,j

b1 −Yij

log µ(Λij ) b1 −b0 ν(Λij ) b1 −b0
Yij log

µ(Λij )
.
ν(Λij )

This therefore suggests the weight function
wMLE (l) = log

µ(l)
.
ν(l)

In the general case where we allow M and N to contain
multiple distributions, it is unclear what weight function
to use. The following property suggests a “conservative”
weight function:
Theorem 4. Suppose the weight function w(l) =
log µ̄(l)
ν̄(l) , ∀l ∈ L is used in program (1) where µ̄ and ν̄
are two distributions on L such that for any distributions
µ ∈ M and ν ∈ N , we have
µ(l)
µ(l)
µ̄(l)
ν(l)
ν(l)
ν̄(l)
≥
≥
≥ 1 or
≥
≥
≥1
ν(l)
ν̄(l)
ν̄(l)
µ(l)
µ̄(l)
µ̄(l)
∀l ∈ L. If µ̄ and ν̄ satisfy the conditions of Theorem 3
then with probability at least 1 − n−β the solution of (1) is
unique and equals Y ∗ .
Theorem 4 says that if the distributions in M are labelwised well separated from the distributions in N , at least
as much as µ̄ from ν̄, then program (1) will perform no
worse than if M = {µ̄} and N = {ν̄} with the associated
wMLE based on µ̄ and ν̄.
4.3. General Stochastic Block Model for Two Labels
We now discuss the results above by considering a special
case of the general stochastic block model where there are

A Convex Optimization Framework for Bi-Clustering

only two labels, L = {+, −}. Suppose that all memberpairs of the same bicluster (Cp , Dq ) have the same labeldistribution on L. Let
µpq = Pr(Λij = +|i ∈ Cp , j ∈ Dq ), ∀p, q, i, j.
Suppose it is known (or estimated) that all µpq are in a
set {µ1 , µ2 , . . . , µm }. Theorem 3 and 4 suggests a simple
strategy to discover the clustering in F and G:
1. Sort µi for i = 1 . . . m in ascending order and find the
largest gap between any two consecutive µi .
2. Suppose the largest gap is between ν = µi0 and µ =
µi1 where ν < µ, then set N = {µi : µi ≤ ν} and
M = {µi : µi ≥ µ}.
3. Solve program (1) with the weight function
w(+) = log

µ
ν

and

w(−) = log

1−µ
.
1−ν

To illustrate, we use the example problem posed by Cai &
Li (2014). This is a clustering problem (F = G) with 3
clusters, where the µpq for each block is given as follows:


0.4
0.2 0.05
 0.2
0.3 0.05 
0.05 0.05 0.1
The traditional graph clustering approach where B is a diagonal matrix


b1 b0 b0
 b0 b1 b0 
b0 b0 b1
could not solve this problem since the smallest diagonal
µpq (0.1) is smaller than the largest off-diagonal µpq (0.2).
Using our strategy one could come up with the following
assignment where ν = 0.2 and µ = 0.3:


b1 b0 b0
 b0 b1 b0 
b0 b0 b0
Suppose µ and ν satisfy the conditions in Theorem 3, then
by Theorem 4 the 3 clusters can be identified with high
probability since each row of B is unique.
4.4. General Multi-label Multi-distribution Case
In the general case where there are more than two labels,
the strategy is to separate all possible distributions into two
sets M and N such that they are as “far apart” as possible.
The weight function can then be set with respect to a pair
of distributions µ ∈ M and ν ∈ N .

Sometimes it may be preferable to merge two or more labels into one and use their marginal distributions instead.
Ultimately, the performance of program (1) depends on E0 ,
E1 and V in Theorem 1.
Program (1) may be run multiple times, each with a different weight function, to discover finer clustering or biclustering structure within the previous output. Full exploration
of these possibilities is left to future work.

5. Implementation
Program (1) can be solved efficiently using the Alternating Direction Method of Multipliers (ADMM) (Boyd et al.,
2011). We provide the pseudocode for a complete implementation in Algorithm 1, with explicit stopping condition.
The inputs and output of Algorithm 1 are the same terms
used √
in program (1). We find that in practice, setting
λ = 2n works well. The threshold for convergence is
specified by . We find that  = 10−4 is a good tradeoff
between the speed of convergence and the quality of the solution. All our experiment
results, unless otherwise stated,
√
were based on λ = 2n and  = 10−4 .
An optional Step 7 for updating ρ may potentially improve
the speed of convergence. This takes an additional parameter τ . If kX k+1 − Y k+1 kF > τ ρkY k+1 − Y k kF then
set ρ := 2ρ and Qk+1 := Qk+1 /2. On the other hand, if
τ kX k+1 − Y k+1 kF < ρkY k+1 − Y k kF then set ρ := ρ/2
and Qk+1 := 2Qk+1 . Typically τ = 10 is a stable choice.
We refer the reader to Boyd et al. (2011) for further details.
Algorithm 1 ADMM solver for Program (1)
Input: W ∈ Rn1 ×n2 , λ, b0 , b1 , 
Output: Y
1. ρ := 1, k := 0
2. Y k := 0, Qk := 0, (Y k , Qk ∈ Rn1 ×n2 )
3. X k+1 := U max{Σ − λρ , 0}V > where U ΣV > is an
SVD of (Y k − Qk ).
n
n
o o
4. Y k+1 := min max X k+1 + Qk + ρ1 W, b0 , b1
5. Qk+1 := Qk + X k+1 − Y k+1
6. If kX k+1 −Y k+1 kF ≤  max{kX k+1 kF , kY k+1 kF }
and kY k+1 − Y k kF ≤ kQk+1 kF then stop and output Y := Y k+1 .
7. (Optional) Update ρ and Qk+1
8. k := k + 1, go to step 3.
In practice, due to finite precision and numerical errors, the

A Convex Optimization Framework for Bi-Clustering

becomes “easier” as the ratio gets smaller.
Size Y: 100 x 100, B: 5 x 5

0.6
0.4

0
0

6. Experiments

Running time (seconds)

0.8

0.2

6.1. Synthetic data

k−means
conv
conv + k−means

0.1

0.2
Noise level

Figure 2 shows the results. The plot “conv” indicates results based on the output of Program (1) with each entry
of Y rounded to the closest ±1. Since the noise is symmetric (for +1 and −1 observations), the weight for Program (1) can simply be set to Wij = Λij . Included in the
figure are two additional plots “k-means” and “conv + kmeans”. The “k-means” results are based on running the
k-means algorithm directly on the observed labels (since
they are numeric), once on the rows and a separate run on
the columns. After running k-means, all entries in the same
bicluster (“block”) are set to take the majority label in the
block. The “conv + k-means” results are based on applying
the same k-means process to the output of “conv”. Note
that Program (1) itself does not need to know the number
of clusters while k-means requires the number of clusters
as input. We choose k-means as reference since, despite its
simplicity, it is still one of the top performing biclustering
methods according to the report by Oghabian et al. (2014).
We can observe, from Fig. 2 that the simple k-means algorithm performs rather well but it is significantly outperformed by “conv” especially for larger n. We recommend
running k-means on top of the output of program (1) especially when the number of clusters is known, since this
produces the best recovery result.√ We can also validate both
Theorem 1 and 3. The ratios Kn for the three sizes are
0.50, 0.45 and 0.32 respectively, and indeed the problem

0.6
0.4
0.2
0
0

0.3

Running time (seconds)

Full recovery rate

0.6
0.4

0
0

k−means
conv
conv + k−means
0.1

0.2
Noise level

20
10

0.2
0.3
Noise level

0.4

200
Running time (seconds)

Full recovery rate

0.1

Size Y: 1000 x 1000, B: 10 x 10

Size Y: 1000 x 1000, B: 10 x 10

0.8
0.6
0.4

0
0

0.4

30

0
0

0.3

1

0.2

0.2
0.3
Noise level

40

0.8

0.2

0.1

Size Y: 500 x 500, B: 10 x 10

Size Y: 500 x 500, B: 10 x 10
1

We first evaluate the performance of our algorithm on synthetic data, where the complete ground truth is available.
We test a simple setup where there are only two labels and
two label distributions. Three different sizes for n1 (with
n2 = n1 ) are used: 100, 500, and 1000. The number of
clusters is 5 for n1 = 100 and 10 for the others, all with
equal sizes. The matrix B has independently generated ±1
entries with uniform probability. A noise level σ is defined
such that each observed label Λij equals Yij∗ with probability 1 − σ and equals the opposite value with probability
σ. This setup is very similar to that used in (Wulff et al.,
2013), except that in our case, we report the actual “full recovery rate”, i.e. the fraction of trials where the final output
Y exactly equals Y ∗ , out of 100 independent trials. Error
bars in all figures show 95% confidence intervals.

Size Y: 100 x 100, B: 5 x 5
0.8

1
Full recovery rate

output Y of Algorithm 1 will not have entries that are exactly b0 or b1 , even if it is a successful recovery. A simple
1
rounding, say, around b0 +b
would give the correct solu2
tion. The clusters in F (resp. G) can then be obtained by
sorting the rows (resp. columns) of Y . Even if the recovery
is not 100%, a simple k-means algorithm can be applied to
the rows/columns to obtain a desired number of clusters.

k−means
conv
conv + k−means
0.1

0.2
Noise level

0.3

150
100
50
0
0

0.1

0.2
0.3
Noise level

0.4

Figure 2. Full recovery rate and running time of program (1)

The average computational time needed to finish one instance of program (1) in Matlab on a Core i5 desktop machine is also reported in Fig. 2. It is interesting to note
that convergence is typically very fast when the problem
instance is either very “easy” (e.g. low noise) or too “hard”,
while the “borderline” instances usually require a much
larger number of iterations to converge.
6.1.1. O NE - SIDED B ICLUSTERING
Our second set of experiments evaluates the prediction of
Theorem 2. In particular, when large clusters exist in F (i.e.
the rows), and B is more or less uniformly distributed, the
recovery of Y ∗ depends mainly on K and not L. Figure 3
shows two sets of plots. Both experiments use n1 = 400
and n2 = 800. B has ±1 entries and the observations consist of two labels, 0 or 1. Entries with Y ∗ = +1 produce
the label 1 with probability 0.7 while the rest produce the
label 1 with probability 0.05. This is the case where the
observation noise is non-symmetric.
In the left figure, B in each trial is generated with uniformly
random ±1 entries. Equal-size clusters are generated in
both the rows and the columns, where each row (resp. column) cluster has size K (resp. L). We test the biclustering
performance in two cases: (1) fixed K and varying L, (2)
varying K and L. Case 1 is shown in the blue plot while

A Convex Optimization Framework for Bi-Clustering

case 2 is in red. As predicted by Theorem 2, the full recovery rate remains high in case 1 where K remains fixed
and large regardless of L. In case 2, the performance drops
when both K and L become small, as expected.
In the right figure, we evaluate the biclustering performance in a scenario where the assumption of Theorem 2
is violated. In particular, we force B to be a 10 × 10 diagonal matrix – the number of clusters is fixed at 10 in both
the rows and the columns. Note that in this case, u1 and u2
both equal 1. The first cluster in the rows (resp. columns)
is chosen to have size equal to K (resp. L). The remaining
clusters all have equal, but larger sizes. Here, we observe
that the performance drops significantly in both cases, even
though K is held large in the first case.
1

0.6
0.4
0.2
0
0

K = 40
K = L/2
20

40
L

60

80

Full recovery rate

Full recovery rate

1
0.8

0.8
0.6
0.4
0.2
0
0

K = 40
K = L/2
20

40
L

60

80

Figure 3. Full recovery rate for two different setups with varying
cluster sizes. Left: B has uniform ±1 entries. Right: B diagonal.

6.2. Real World Data
We further evaluate our algorithm on real world data, where
the true data generating model is unknown.
6.2.1. A NIMAL -F EATURE DATASET
Our first dataset is the animal-feature data originally published by Osherson et al. (1991) and has been used in
(Wulff et al., 2013) and (Kemp et al., 2006) for biclustering. The dataset contains human-produced association between a set of 50 animals and 85 features. For each animalfeature pair, the degree of association–a real value between
0 and 100, is provided. The biclustering task is to simultaneously cluster the animals and features. Unfortunately no
ground truth is available so evaluation is subjective. Nevertheless this allows a qualitative comparison with the results reported in, e.g. (Wulff et al., 2013) and (Kemp et al.,
2006).
We use the raw data in two ways. First, we follow Kemp
et al. (2006) and use the global mean value t ≈ 20.8 as
the cutoff point between a “yes” and “no”. For the “yes”
entries, we assume that the probability of error decreases
linearly with the recorded degree of association, where we
set 101 to be the point of 0 error probability while the probability of error at t is 0.5. Similarly for the “no” entries the
point of 0-error is set at −1. We then employ the MLE
weight and run program (1). To extract clusters (both the

animals and the features) we run k-means on the rows and
columns of the program output Y . Given that this is unlikely to be the true model, we expect a need to adjust the
regularization parameter λ as well as the number of clusters. We tested a range of λ and cluster sizes, and choose a
combination that produces clusters with the√most uniform
sizes. Figure 4 shows the result with λ = n, for 9 animal clusters and 20 feature clusters. There is randomness
involved in using k-means that depends on the initialization but for the chosen parameters the resulting clusters are
more-or-less stable. We find the results reasonable and not
unlike those obtained by Wulff et al. (2013) and Kemp et al.
(2006).
6.2.2. G ENE E XPRESSION DATA
For our second real-world example, we use the gene expression data for leukemia, in the form provided by Monti
et al. (2003). This is a filtered version of the original raw
data published in (Golub et al., 1999), with mostly uninformative genes removed. These are DNA microarray data
collected from bone marrow samples of acute leukemia patients, with 11 acute myeloid leukemia (AML) samples,
8 T-lineage acute lymphoblastic leukemia (ALL) samples,
and 19 B-lineage ALL samples. The “features” are the
recorded expression levels of 999 genes. The expression
levels are real values with widely different range for each
gene. We scale and shift the expression levels for each gene
such that the mean is 0 and the standard deviation is 1.
Since the actual model is unknown, we assume a Gaussian
distribution where we use N (1, 1) for µ and N (−1, 1) for
ν. The MLE weight is therefore log µ(l)
ν(l) , where l is the ob2
served expression level . Note that the resulting weight is
simply a linear function of l.
Since the type of each sample is known, we can evaluate
the clustering performance for the 38 samples. Again, we
ran k-means on the program output and tested a range of λ.
Figure 5 shows the clustering performance with respect to
λ, against the ground-truth clustering, in terms of the adjusted mutual information and the percent of sample-pairs
that are correctly clustered together. The two performance
measures give qualitatively similar√results and the best performance is achieved around λ = 5n. In general, a small
λ would produce a finer clustering structure while a larger
λ produce a coarser structure. Note that using λ = 0 is
equivalent to simply thresholding the raw value to the nearest ±1 (right panel of Fig. 6). Running k-means directly
on the raw data (middle panel of Fig. 6) results in the worst
performance, with an adjusted mutual information of 0.4.
2

Although in general the likelihood ratio is unbounded for
Gaussian distributions, a standard truncation trick can be used to
bound the weights such that our theoretical results still hold (see
Lim et al., 2014).

A Convex Optimization Framework for Bi-Clustering
F1

F2

F3 F4 F5 F6 F7

F1

A1

A1

A2

A2

A3

A3

A4

A4

A5

A5

A6

A6

A7
A8
A9

A7
A8
A9

F2

F3 F4 F5 F6 F7

A1: leopard, lion, wolf, bobcat, fox, german shepherd, tiger, grizzly bear, polar bear

F1: flippers, ocean, water, swims, fish, arctic

A2: blue whale, humpback whale, walrus, seal, dolphin, killer whale, otter

F2: fierce, hunter, stalker, meat, meatteeth, muscle

A3: rabbit, mouse, hamster, squirrel, mole, beaver, skunk

F3: walks, quadrapedal, ground, furry, chewteeth, brown

A4: giant panda, sheep, cow, ox, pig, buffalo

F4: smart, fast, active, agility

A5: zebra, horse, antelope, deer, giraffe, moose

F5: claws, solitary, paws, small

A6: chihuahua, persian cat, collie, siamese cat, dalmatian

F6: group, big, strong

A7: weasel, rat, raccoon, bat

F7: newworld, tail, oldworld

A8: spider monkey, chimpanzee, gorilla

F8: white, domestic

A9: elephant, hippopotamus, rhinoceros

F9: hooves, grazer, plains, fields, vegetation

Figure 4. Left: Program (1) output Y , Right: Raw data with the same rows/columns order

√
Figure 6 shows the program output Y (for λ = 5n), along
with the raw data and its thresholded version. Each column
corresponds to one sample and they have been arranged according to the ground truth clustering where columns 119 corresponds to B-ALL, 20-27 T-ALL and 28-38 AML.
We observe that finer clustering seems to exist especially
within the B-ALL group, and according to Monti et al.
(2003) it is widely accepted that meaningful sub-classes
of ALL do exist but the composition and nature of these
sub-classes is not as well accepted.

100

200

300

400

adjusted mutual information

1
0.9
0.8
0.7

conv + k−means
k−means (raw)

0.6
0.5
0.4
0.3
0

5

10
λ2/n

fraction of correctly classified pairs

500
1

600
0.9
conv + k−means
k−means (raw)

0.8

700

0.7

800
0.6
0

5

10
λ2/n

Figure 5. Clustering performance. Left: Adjusted mutual information, Right: Fraction of pairs correctly clustered together

Acknowledgments
S.H. Lim and H. Xu were supported by the Ministry of
Education of Singapore through AcRF Tier Two grants
R265000443112 and R265000519112, and A*STAR Public Sector Funding R265000540305. Y. Chen was supported by NSF grant CIF-31712-23800 and ONR MURI
grant N00014-11-1-0688.

900

10 20 30

Figure 6. Left: Output Y from program (1), Middle: Raw data,
Right: Raw data (binarized)

References
Ames, B. and Vavasis, S. Nuclear norm minimization for
the planted clique and biclique problems. Mathematical

A Convex Optimization Framework for Bi-Clustering

Programming, 129(1):69–89, 2011.
Ames, Brendan P.W. Guaranteed clustering and biclustering via semidefinite programming. Mathematical Programming, pp. 1–37, 2013. ISSN 0025-5610. doi:
10.1007/s10107-013-0729-x.
Anandkumar, Anima, Ge, Rong, Hsu, Daniel, and Kakade,
Sham M. A tensor spectral approach to learning
mixed membership community models. arXiv preprint
arXiv:1302.2684, 2013.
Bansal, N., Blum, A., and Chawla, S. Correlation clustering. Machine Learning, 56(1), 2004.
Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and
Eckstein, Jonathan. Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. Trends Mach. Learn., 3(1):1–122, January 2011.
Cai, T. Tony and Li, Xiaodong. Robust and computationally feasible community detection in the presence of arbitrary outlier nodes. arXiv preprint arXiv:1404.6000,
2014.
Chaudhuri, K., Chung, F., and Tsiatas, A. Spectral clustering of graphs with general degrees in the extended
planted partition model. COLT, 2012.
Chen, Y., Sanghavi, S., and Xu, H.
graphs. In NIPS 2012., 2012.

Hartigan, J. A. Direct Clustering of a Data Matrix. Journal
of the American Statistical Association, 67(337):123–
129, 1972. ISSN 01621459. doi: 10.2307/2284710.
Holland, Paul W., Laskey, Kathryn B., and Leinhardt,
Samuel. Stochastic blockmodels: Some first steps. Social Networks, 5:109–137, 1983.
Kannan, R., Vempala, S., and Vetta, A. On clusterings good, bad and spectral. In IEEE Symposium on Foundations of Computer Science, 2000.
Kemp, Charles, Tenenbaum, Joshua B., Griffiths,
Thomas L., Yamada, Takeshi, and Ueda, Naonori.
Learning systems of concepts with an infinite relational
model. In Proceedings of the 21st National Conference
on Artificial Intelligence - Volume 1, AAAI’06, pp. 381–
388. AAAI Press, 2006.
Kolar, Mladen, Balakrishnan, Sivaraman, Rinaldo,
Alessandro, and Singh, Aarti. Minimax localization of
structural information in large noisy matrices. In NIPS,
pp. 909–917, 2011.
Lelarge, Marc, Massoulié, Laurent, and Xu, Jiaming. Reconstruction in the Labeled Stochastic Block Model.
In IEEE Information Theory Workshop, Seville, Spain,
September 2013. URL http://hal.inria.fr/
hal-00917425.

Clustering sparse

Chen, Y., Jalali, A., Sanghavi, S., and Xu, H. Clustering
partially observed graphs via convex optimization. Journal of Machine Learning Research, 15:2213–2238, June
2014.
Cheng, Yizong and Church, George M. Biclustering of
expression data. In Proc. of the 8th ISMB, pp. 93–103.
AAAI Press, 2000.
Demaine, E. D., Immorlica, N., Emmanuel, D., and Fiat, A.
Correlation clustering in general weighted graphs. SIAM
special issue on approximation and online algorithms,
2005.
Eren, Kemal, Deveci, Mehmet, Küçüktunç, Onur, and
Çatalyürek, Ümit V. A comparative analysis of biclustering algorithms for gene expression data. Briefings in
Bioinformatics, 2012.
Golub, T. R., Slonim, D. K., Tamayo, P., Huard, C.,
Gaasenbeek, M., Mesirov, J. P., Coller, H., Loh, M. L.,
Downing, J. R., Caligiuri, M. A., and Bloomfield, C. D.
Molecular classification of cancer: class discovery and
class prediction by gene expression monitoring. Science,
286:531–537, 1999.

Lim, S.H., Chen, Y., and Xu, H. Clustering from labels and
time-varying graphs. In NIPS 2014., 2014.
Mathieu, C. and Schudy, W. Correlation clustering with
noisy input. In SODA, pp. 712, 2010.
McSherry, F. Spectral partitioning of random graphs. In
FOCS, pp. 529–537, 2001.
Monti, Stefano, Tamayo, Pablo, Mesirov, Jill, and Golub,
Todd.
Consensus clustering: A resampling-based
method for class discovery and visualization of gene expression microarray data. Mach. Learn., 52(1-2):91–
118, July 2003.
Oghabian, Ali, Kilpinen, Sami, Hautaniemi, Sampsa, and
Czeizler, Elena. Biclustering methods: Biological relevance and application in gene expression analysis. PLoS
ONE, 9(3), 2014.
Osherson, D.N., Stern, J., Wilkie, O., Stob, M., and Smith,
E.E. Default probability. Cognitive Science, 15(2):251–
269, 1991.
Puleo, G. J. and Milenkovic, O. Correlation Clustering
with Constrained Cluster Sizes and Extended Weights
Bounds. ArXiv e-print 1411.0547, 2014.

A Convex Optimization Framework for Bi-Clustering

Rohe, K., Chatterjee, S., and Yu, B. Spectral clustering and
the high-dimensional stochastic block model. Annals of
Statistics, 39:1878–1915, 2011.
Rudelson, Mark and Vershynin, Roman. Smallest singular
value of a random rectangular matrix. Communications
on Pure and Applied Mathematics, 62(12):1707–1739,
2009.
Shamir, O. and Tishby, N. Spectral Clustering on a Budget.
In AISTATS, 2011.
Swamy, C. Correlation clustering: maximizing agreements
via semidefinite programming. In Proceedings of the
15th Annual ACM-SIAM Symposium on Discrete Algorithms, 2004.
Tanay, Amos, Sharan, Roded, and Shamir, Ron. Biclustering algorithms: A survey. In In Handbook of Computational Molecular Biology, 2005.
Vinayak, Ramya Korlakai, Oymak, Samet, and Hassibi,
Babak. Graph clustering with missing data: Convex algorithms and analysis. In Advances in Neural Information Processing Systems, pp. 2996–3004, 2014.
Wulff, S., Urner, R., and Ben-David, S. Monochromatic
Bi-Clustering. ICML 2013, 2013.
Xu, Jiaming, Wu, Rui, Zhu, Kai, Hajek, Bruce, Srikant, R.,
and Ying, Lei. Jointly clustering rows and columns of binary matrices: Algorithms and trade-offs. SIGMETRICS
Perform. Eval. Rev., 42(1):29–41, June 2014.

