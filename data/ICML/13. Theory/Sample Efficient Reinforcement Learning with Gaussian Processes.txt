Sample Efficient Reinforcement Learning with Gaussian Processes

Robert C. Grande
RGRANDE @ MIT. EDU
Thomas J. Walsh
THOMASJWALSH @ GMAIL . COM
Jonathan P. How
JHOW @ MIT. EDU
Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139 USA

Abstract
This paper derives sample complexity results for
using Gaussian Processes (GPs) in both modelbased and model-free reinforcement learning
(RL). We show that GPs are KWIK learnable,
proving for the first time that a model-based RL
approach using GPs, GP-Rmax, is sample efficient (PAC-MDP). However, we then show that
previous approaches to model-free RL using GPs
take an exponential number of steps to find an
optimal policy, and are therefore not sample efficient. The third and main contribution is the
introduction of a model-free RL algorithm using GPs, DGPQ, which is sample efficient and,
in contrast to model-based algorithms, capable
of acting in real time, as demonstrated on a fivedimensional aircraft simulator.

1. Introduction
In Reinforcement Learning (RL) (Sutton & Barto, 1998),
several new algorithms for efficient exploration in continuous state spaces have been proposed, including GP-Rmax
(Jung & Stone, 2010) and C-PACE (Pazis & Parr, 2013).
In particular, C-PACE was shown to be PAC-MDP, an important class of RL algorithms that obtain an optimal policy in a polynomial number of exploration steps. However,
these approaches require a costly fixed-point computation
on each experience, making them ill-suited for real-time
control of physical systems, such as aircraft. This paper
presents a series of sample complexity (PAC-MDP) results
for algorithms that use Gaussian Processes (GPs) (Rasmussen & Williams, 2006) in RL, culminating with the
introduction of a PAC-MDP model-free algorithm which
does not require this fixed-point computation and is better
suited for real-time learning and control.
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

First, using the KWIK learning framework (Li et al., 2011),
we provide the first-ever sample complexity analysis of GP
learning under the conditions necessary for RL. The result of this analysis actually proves that the previously described model-based GP-Rmax is indeed PAC-MDP. However, it still cannot be used in real time, as mentioned above.
Our second contribution is in model-free RL, in which a
GP is used to directly model the Q-function. We show that
existing model-free algorithms (Engel et al., 2005; Chung
et al., 2013) that use a single GP may require an exponential (in the discount factor) number of samples to reach a
near-optimal policy.
The third, and primary, contribution of this work is the introduction of the first model-free continuous state space
PAC-MDP algorithm using GPs: Delayed-GPQ (DGPQ).
DGPQ represents the current value function as a GP, and
updates a separately stored value function only when sufficient outlier data has been detected. This operation ‚Äúoverwrites‚Äù a portion of the stored value function and resets the
GP confidence bounds, avoiding the slowed convergence
rate of the naƒ±Ãàve model-free approach.
The underlying analogy is that while GP-Rmax and CPACE are generalizations of model-based Rmax (Brafman
& Tennenholtz, 2002), DGPQ is a generalization of modelfree Delayed Q-learning (DQL) (Strehl et al., 2006; 2009)
to general continuous spaces. That is, while early PACMDP RL algorithms were model-based and ran a planner
after each experience (Li et al., 2011), DQL is a PAC-MDP
algorithm that performs at most a single Bellman backup
on each step. In DGPQ, the delayed updates of DQL are
adapted to continuous state spaces using a GP.
We prove that DGPQ is PAC-MDP, and show that using a
sparse online GP implementation (CsatoÃÅ & Opper, 2002)
DGPQ can perform in real-time. Our empirical results, including those on an F-16 simulator, show DGPQ is both
sample efficient and orders of magnitude faster in persample computation than other PAC-MDP continuous-state
learners.

Sample Efficient Reinforcement Learning with Gaussian Processes

2. Background

2.2. Gaussian Processes

We now describe background material on Reinforcement
Learning (RL) and Gaussian Processes (GPs).

This paper uses GPs as function approximators both in
model-based RL (where GPs represent T and R) and
model-free RL (where GPs represent Q‚àó ), while showing
how to maintain PAC-MDP sample efficiency in both cases.
A GP is defined as a collection of random variables, any finite subset of which has a joint Gaussian distribution (Rasmussen & Williams, 2006) with prior mean ¬µ(x) and covariance kernel k(x, x0 ). In this paper, we use the radial
0 2
k
).
basis function (RBF), k(x, x0 ) = exp(‚àí kx‚àíx
2Œ∏ 2

2.1. Reinforcement Learning
We model the RL environment as a Markov Decision Process (Puterman, 1994) M = hS, A, R, T, Œ≥i with a potentially infinite set of states S, finite actions A, and 0 ‚â§
Œ≥ < 1. Each step elicits a reward R(s, a) 7‚Üí [0, Rmax ]
and a stochastic transition to state s0 ‚àº T (s, a). Every
‚àó
MDP
R has an optimal value function Q (s, a) = R(s, a) +
Œ≥ s0 T (s, a, s0 )V ‚àó (s0 ) where V ‚àó (s) = maxa Q‚àó (s, a) and
corresponding optimal policy œÄ ‚àó : S 7‚Üí A. Note the
bounded reward function means V ‚àó ‚àà [0, Vmax ].
In RL, an agent is given S,A, and Œ≥ and then acts in M with
the goal of enacting œÄ ‚àó . For value-based methods (as opposed to policy search (Kalyanakrishnan & Stone, 2009)),
there are roughly two classes of RL algorithms: modelbased and model-free. Model-based algorithms, such as
KWIK-Rmax (Li et al., 2011), build models of T and R
and then use a planner to find Q‚àó . Many model-based approaches have sample efficiency guarantees, that is bounds
on the amount of exploration they perform. We use the
PAC-MDP definition of sample complexity (Strehl et al.,
2009). The definition uses definitions of the covering number of a set (adapted from (Pazis & Parr, 2013)).
Definition 1 The Covering Number NU (r) of a compact
domain U ‚äÇ Rn is the cardinality of the minimal set C =
{ci , . . . , cNc } s.t. ‚àÄx ‚àà U , ‚àÉcj ‚àà C s.t. d(x, cj ) ‚â§ r,
where d(¬∑, ¬∑) is some distance metric.
Definition 2 Algorithm A (non-stationary policy At ) with
accuracy parameters  and Œ¥ in an MDP of size NSA (r)
is said to be Probably Approximately Correct for MDPs
(PAC-MDP) if, with probability 1‚àíŒ¥, it takes no more than
1
, 1 , 1Œ¥ i) number of steps
a polynomial (in hNSA (r), 1‚àíŒ≥
At
‚àó
where V (st ) < V (st ) ‚àí .
By contrast, model-free methods such as Q-Learning
(Watkins, 1992) build Q‚àó directly from experience without
explicitly representing T and R. Generally model-based
methods are more sample efficient but require more computation time for the planner. Model-free methods are generally computationally light and can be applied without a
planner, but need (sometimes exponentially) more samples,
and are usually not PAC-MDP. There are also methods that
are not easily classified in these categories, such as C-PACE
(Pazis & Parr, 2013), which does not explicitly model T
and R but performs a fixed-point operation for planning.

The elements of the GP kernel matrix K(X, X) are defined
as Ki,j = k(xi , xj ), and k(X, x0 ) denotes the kernel vector. The conditional probability distribution of the GP at
new data point, x0 , can then be calculated as a normal variable (Rasmussen & Williams, 2006) with mean ¬µÃÇ(x0 ) =
k(X, x0 )T [K(X, X) + œân2 I]‚àí1 y, and covariance Œ£(x0 ) =
k(x0 , x0 ) ‚àí k(X, x0 )T [K(X, X) + œân2 I]‚àí1 k(X, x0 ), where
y is the set of observations, X is the set of observation input locations, and œân2 is the measurement noise variance.
For computational and memory efficiency, we employ an
online sparse GP approximation (CsatoÃÅ & Opper, 2002) in
the experiments.
2.3. Related Work
GPs have been used for both model-free and model-based
RL. In model-free RL, GP-Sarsa (Engel et al., 2005) has
been used to model a value function and extended to offpolicy learning (Chowdhary et al., 2014), and for (heuristically) better exploration in iGP-Sarsa (Chung et al., 2013).
However, we prove in Section 4.1 that these algorithms
1
) number of samples
may require an exponential (in 1‚àíŒ≥
to reach optimal behavior since they only use a single GP
to model the value function.
In model-based RL, the PILCO algorithm trained GPs to
represent T , and then derived policies using policy search
(Deisenroth & Rasmussen, 2011). However, PILCO does
not include a provably efficient (PAC-MDP) exploration
strategy. GP-Rmax (Jung & Stone, 2010) does include
an exploration strategy, specifically replacing areas of low
confidence in the (T and R) GPs with high valued states,
but no theoretical results are given in that paper. In Section
3, we show that GP-Rmax actually is PAC-MDP, but the algorithm‚Äôs planning phase (comparable to the C-PACE planning in our experiments) after each update makes it computationally infeasible for real-time control.
REKWIRE (Li & Littman, 2010) uses KWIK linear regression for a model-free PAC-MDP algorithm. However,
REKWIRE needs H separate approximators for finite horizon H, and assumes Q can be modeled as a linear function,
in contrast to the general continuous functions considered
in our work. The C-PACE algorithm (Pazis & Parr, 2013)

Sample Efficient Reinforcement Learning with Gaussian Processes

has already been shown to be PAC-MDP in the continuous setting, though it does not use a GP representation.
C-PACE stores data points that do not have close-enough
neighbors to be considered ‚Äúknown‚Äù. When it adds a new
data point, the Q-values of each point are calculated by
a value-iteration like operation. The computation for this
operation grows with the number of datapoints and so (as
shown in our experiments) the algorithm may not be able
to act in real time.

Proof sketch Here we ignore the effect of the GP-prior,
which is considered in depth in the Supplementary material.
Using McDiarmid‚Äôs inequality
we
 have that

22
P r{|¬µ(x) ‚àí f1 (x)| ‚â• 1 } ‚â§ 2exp ‚àí P c12 , where ci is
i
the maximum amount that yi can alter the prediction value
¬µ(x). Using algebraic manipulation, Bayes law, and noting that the max singular value œÉÃÑ(K(X,
X)(K(X, X) +
P
œâ 2 I)‚àí1 ) < 1, it can be shown that c2i ‚â§ œÉ 2 (xi )Vm2 /œâ 2 .
Substituting œÉ 2 (xi ) to McDiarmid‚Äôs inequality and rearranging yields (1). See supplemental material.

3. GPs for Model-Based RL
In model-based RL (MBRL), models of T and R are created from data and a planner derives œÄ ‚àó . Here we review
the KWIK-Rmax MBRL architecture, which is PAC-MDP.
We then show that GPs are a KWIK-learnable representation of T and R, thereby proving that GP-Rmax (Jung &
Stone, 2010) is PAC-MDP given an exact planner.
3.1. KWIK Learning and Exploration
The ‚ÄúKnows What it Knows‚Äù (KWIK) framework (Li et al.,
2011) is a supervised learning framework for measuring the
number of times a learner will admit uncertainty. A KWIK
learning agent is given a hypothesis class H : X 7‚Üí Y
for inputs X and outputs Y and parameters  and Œ¥. With
probability 1 ‚àí Œ¥ over a run, when given an adversarially chosen input xt , the agent must, either (1) predict yÃÇt
if ||yt ‚àí yÃÇt || ‚â§  where yt is the true expected output
(E[h‚àó (x)] = yt ) or (2) admit ‚ÄúI don‚Äôt know‚Äù (denoted ‚ä•).
A representation is KWIK learnable if an agent can KWIK
learn any h‚àó ‚àà H with only a polynomial (in h|H|, 1 , 1Œ¥ i)
number of ‚ä• predictions. In RL, the KWIK-Rmax algorithm (Li et al., 2011), uses KWIK learners to model T and
R and replaces the value of any Q‚àó (s, a) where T (s, a) or
max
R(s, a) is ‚ä• with a value of Vmax = R1‚àíŒ≥
. If T and R
are KWIK-learnable, then the resulting KWIK-Rmax RL
algorithm will be PAC-MDP under Definition 2. We now
show that a GP is KWIK learnable.
3.2. KWIK Learning a GP
First, we derive a metric for determining if a GP‚Äôs mean
prediction at input x is -accurate with high probability,
based on the variance estimate at x.
Lemma 1 Consider a GP trained on samples ~y =
[y1 , . . . , yt ] which are drawn from p(y | x) at input locations X = [x1 , . . . , xt ], with E[y | x] = f (x) and
yi ‚àà [0, Vm ]. If the predictive variance of the GP at xi ‚àà X
is
2œâ 2 2
2
(1)
œÉ 2 (xi ) ‚â§ œÉtol
= 2 n 12
Vm log( Œ¥1 )
then the prediction error at xi is bounded in probability:
P r {|¬µÃÇ(xi ) ‚àí f (xi )| ‚â• 1 } ‚â§ Œ¥1 .

Lemma 1 gives a sufficient condition to ensure that, with
high probability, the mean of the GP is within 1 of
E[h‚àó (x)]. A KWIK agent can now be constructed that pre2
dicts ‚ä• if the variance is greater than œÉtol
and otherwise
predicts yÃÇt = ¬µ(x), the mean prediction of the GP. Theorem 1 bounds the number of ‚ä• predictions by such an
agent, and when Œ¥1 = N (r(Œ¥1 œÉ2 )) , establishes the KWIK
U
2 tol
learnability of GPs. For ease of exposition, we define the
equivalent distance as follows:
Definition 3 The equivalent distance r(tol ) is the maximal distance s.t. ‚àÄx, c ‚àà U , if d(x, c) ‚â§ r(tol ),
then the linear independence test Œ≥(x, c) = k(x, x) ‚àí
k(x, c)2 /k(c, c) ‚â§ tol .
Theorem 1 Consider a GP model trained over a compact
2
domain U ‚äÇ Rn with covering number NU (r( 21 œÉtol
)). Let
the observations ~y = [y1 , . . . , yn ] be drawn from p(y | x),
with E[y | x] = f (x), and x drawn adversarially. Then, the
worst case bound on the number of samples for which
P r {|¬µÃÇ(x) ‚àí f (x)| ‚â• 1 } ‚â§ Œ¥1 ,‚àÄx ‚àà U

(2)

and the GP is forced to return ‚ä• is at most
 

 2
 
2
4Vm
1 2
m=
log
N
œÉ
. (3)
r
U
21
Œ¥1
2 tol

2
Furthermore, NU r 12 œÉtol
grows polynomially with 11
and Vm for the RBF kernel.
2
Proof sketch If œÉ 2 (x) ‚â§ œÉtol
, ‚àÄx ‚àà U , then (2) follows from Lemma 1. By partitioning the domain into
the Voronoi regions around the covering set C, it can
be shown using the covariance equation (see supplemental material), that given nV samples in a Voronoi region
2
n 1 œÉtol
+œâ 2
everywhere in the region.
ci ‚àà C, œÉ 2 (x) ‚â§ V n2V +œâ
2

Solving for nV , we find that nV ‚â•
2

2
œÉtol
.

drive œÉ (x) ‚â§
V2
that nV = m
2 log
1

2œâ 2
2
œÉtol

is sufficient to

Plugging
in nV into (1), we have

2
points
reduce the variance at
Œ¥1

2
ci below œÉtol
. Therefore, the total number of points we can
sample anywhere in U before reducing the variance below

Sample Efficient Reinforcement Learning with Gaussian Processes
2
œÉtol
everywhere is equal to the sum of points nV over all
regions, NU . The total probability of an incorrect prediction is given byPthe union bound of an incorrect prediction
in any region, ci Œ¥1 = Œ¥1 NU = Œ¥. See supplemental material for proof that NU grows polynomially with 11 and
Vm .

Intuitively, the KWIK learnability of GPs can be explained
as follows. By knowing the value at a certain point within
some tolerance 2 , the Lipschitz smoothness assumption
means there is a nonempty region around this point where
values are known within a larger tolerance . Therefore,
given sufficient observations in a neighborhood, a GP is
able to generalize its learned values to other nearby points.
Lemma 1 relates the error at any of these points to the predictive variance of the GP, so a KWIK agent using a GP
can use the variance prediction to choose whether to predict
‚ä• or not. As a function becomes less smooth, the size of
these neighborhoods shrinks, increasing the covering number, and the number of points required to learn over the
entire input space increases.
Theorem 1, combined with the KWIK-Rmax Theorem
(Theorem 3 of (Li et al., 2011)) establishes that the previously proposed GP-Rmax algorithm (Jung & Stone, 2010),
which learns GP representations of T and R and replaces
uncertainty with Vmax values, is indeed PAC-MDP. GPRmax was empirically validated in many domains in this
previous work but the PAC-MDP property was not formally
proven. However, GP-Rmax relies on a planner that can derive Q‚àó from the learned T and R, which may be infeasible
in continuous domains, especially for real-time control.

4.1. Naƒ±Ãàve Model-free Learning using GPs
We now show that a naƒ±Ãàve use of a single GP to model
Q‚àó will result in exponentially slow convergence similar to
greedy Q-learning with a linearly decaying Œ±. Consider the
following model-free algorithm using a GP to store values
of QÃÇ = Qt . A GP for each action (GPa ) is initialized optimistically using the prior. At each step, an action is chosen
greedily and GPa is updated with an input/output sample:
hxt , rt + Œ≥ maxb GPb (st+1 )i. We analyze the worst-case
performance through a toy example and show that this approach requires an exponential number of samples to learn
Q‚àó . This slow convergence is intrinsic to GPs due to the
variance reduction rate and the non-stationarity of the QÃÇ
estimate (as opposed to T and R, whose sampled values
do not depend on the learner‚Äôs current estimates). So, any
model-free algorithm using a GP that does not reinitialize
the variance will have the same worst-case complexity as
in this toy example.
Consider an MDP with one state s and one action a that
transitions deterministically back to s with reward r = 0,
and discount factor Œ≥. The Q function is initialized optimistically to QÃÇ0 (s) = 1 using the GP‚Äôs prior mean. Consider the naƒ±Ãàve GP learning algorithm described above, kernel k(s, s) = 1, and measurement noise œâ 2 to predict the
Q-function using the GP regression equations. We can analyze the behavior of the algorithm using induction.
Consider the first iteration: QÃÇ0 = 1, and the first measurement is Œ≥. In this case, the GP prediction is equivalent to
the MAP estimate of a random variable with Gaussian prior
and linear measurements subject to Gaussian noise.
QÃÇ1 =

4. GPs for Model-Free RL
Model-free RL algorithms, such as Q-learning (Watkins,
1992), are often used when planning is infeasible due to
time constraints or the size of the state space. These algorithms do not store T and R but instead try to model
Q‚àó directly through incremental updates of the form:
Qt+1 (s, a) = (1 ‚àí Œ±)Qt (s, a) + Œ± (rt + Œ≥Vt‚àó (st+1 )). Unfortunately, most Q-learning variants have provably exponential sample complexity, including optimistic/greedy Qlearning with a linearly decaying learning rate (Even-Dar
& Mansour, 2004), due to the incremental updates to Q
combined with the decaying Œ± term.
However, the more recent Delayed Q-learning (DQL)
(Strehl et al., 2006) is PAC-MDP. This algorithm works by
initializing
Q(s, a) to Vmax and then overwriting Q(s, a) =
Pm
0
i=1 ri +Œ≥V (si )
after m samples of that hs, ai pair have
m
been seen. In section 4.1, we show that using a single GP
results in exponential sample complexity, like Q-learning;
in section 4.2, we show that using a similar overwriting approach to DQL achieves sample efficiency.

œÉ02
œâ2
1
+
Œ≥
œÉ02 + œâ 2
œÉ02 + œâ 2
2

Recursively, QÃÇi+1 = ( œÉ2œâ+œâ2 +
2

œÉ02

œâ
œâ 2 +iœÉ02

i

œÉi2
Œ≥)QÃÇi
œÉi2 +œâ 2

(4)
where œÉi2 =

is the GP variance at iteration i. Substituting for œÉi2

and rearranging yields a recursion for the prediction of QÃÇi
at each iteration,
QÃÇi+1 =

œâ 2 + œÉ02 (i + Œ≥)
QÃÇi
œâ 2 + œÉ02 (i + 1)

(5)

From (Even-Dar & Mansour, 2004), we have that a series
of the form QÃÇi+1 = i+Œ≥
i+1 QÃÇi will converge to  exponentially
1
slowly in terms of 1 and 1‚àíŒ≥
. However, for each term in
our series, we have
œâ2
œÉ02
œâ2
œÉ02

+i+Œ≥
+i+1

‚â•

i+Œ≥
i+1

(6)

The modulus of contraction is always at least as large as
the RHS, so the series convergence to  (since the true

Sample Efficient Reinforcement Learning with Gaussian Processes
Action taken for 2 Action MDP
2.5
2
1.5
1

Action Chosen

0.5

Greedy GP
0

50

100

150

200

250

0

50

100

150

200

250

0

50

100

150

200

250

300

2.5
2
1.5
1
0.5

Œµ‚àíGreedy GP
300

2.5
2
1.5
1
0.5

DGPQ
300

Step

Figure 1. Single-state MDP results: DGPQ converges quickly to
the optimal policy while the naƒ±Ãàve GP implementations oscillate.

value is 0) is at least as slow as the example in (Even-Dar
& Mansour, 2004). Therefore, the naƒ±Ãàve GP implementation‚Äôs convergence to  is also exponentially slow, and, in
fact, has the same learning speed as Q-learning with a linear learning rate. This is because the variance of a GP decays linearly with number of observed data points, and the
magnitude of GP updates is proportional to this variance.
Additionally, by adding a second action with reward r =
1 ‚àí Œ≥, a greedy naƒ±Ãàve agent would oscillate between the
1
two actions for an exponential (in 1‚àíŒ≥
) number of steps,
as shown in Figure 1 (blue line), meaning the algorithm is
not PAC-MDP, since the other action is not near optimal.
Randomness in the policy with -greedy exploration (green
line) will not improve the slowness, which is due to the
non-stationarity of the Q-function and the decaying update
magnitudes of the GP. Many existing model-free GP algorithms, including GP-Sarsa (Engel et al., 2005), iGP-Sarsa
(Chung et al., 2013), and (non-delayed) GPQ (Chowdhary
et al., 2014) perform exactly such GP updates without variance reinitialization, and therefore have the same worstcase exponential sample complexity.
4.2. Delayed GPQ for model-free RL
We now propose a new algorithm, inspired by Delayed
Q-learning, that guarantees polynomial sample efficiency
by consistently reinitializing the variance of the GP when
many observations differ significantly from the current QÃÇ
estimate.
Delayed GPQ-Learning (DGPQ: Algorithm 1) maintains
two representations of the value function. The first is a set
of GPs, GPa for each action, that is updated after each step
but not used for action selection. The second representation
of the value function, which stores values from previously
converged GPs is denoted QÃÇ(s, a). Intuitively, the algorithm uses QÃÇ as a ‚Äúsafe‚Äù version of the value function for
choosing actions and retrieving backup values for the GP

Algorithm 1 Delayed GPQ (DGPQ)
1: Input: GP kernel k(¬∑, ¬∑), Lipschitz Constant LQ , Environment Env, Actions A, initial state s0 , discount Œ≥,
2
threshold œÉtol
, 1
2: for a ‚àà A do
3:
QÃÇa = ‚àÖ
max
4:
GPa = GP.init(¬µ = R1‚àíŒ≥
, k(¬∑, ¬∑))
5: for each timestep t do
6:
at = arg maxa QÃÇa (st ) by Eq 7
7:
hrt , st+1 i = Env.takeAct(at )
8:
qt = rt + Œ≥ maxa QÃÇa (st+1 )
9:
œÉ12 = GPat .variance(st )
2
10:
if œÉ12 > œÉtol
then
11:
GPat .update(st , qt )
12:
œÉ22 = GPat .variance(st )
2
13:
if œÉ12 > œÉtol
‚â• œÉ22 and
QÃÇat (st ) ‚àí GPat .mean(st ) > 21 then
14:
QÃÇa .update(st , GPat .mean(st ) + 1 )
15:
‚àÄa ‚àà A, GPa = GP.init(¬µ = QÃÇa , k(¬∑, ¬∑))

updates, and only updates QÃÇ(s, a) when GPa converges to
a significantly different value at s.
The algorithm chooses actions greedily based on QÃÇ (line
6) and updates the corresponding GPa based on the observed rewards and QÃÇ at next the state (line 8). Note, in
practice one creates a prior of QÃÇa (st ) (line 15) by updating
the GP with points zt = qt ‚àí QÃÇa (st ) (line 11), and adding
back QÃÇa (st ) in the update on line 14. If the GP has just
crossed the convergence threshold at point s and learned
a value significantly lower (21 ) than the current value of
QÃÇ (line 13), the representation of QÃÇ is partially overwritten
with this new value plus a bonus term using an operation
described below. Crucially, the GPs are then reset, with
all data erased. This reinitializes the variance of the GPs
so that updates will have a large magnitude, avoiding the
slowed convergence seen in the previous section. Guide2
lines for setting the sensitivity of the two tests (œÉtol
and 1 )
are given in Section 5.
There are significant parallels between DGPQ and the
discrete-state DQL algorithm (Strehl et al., 2009), but the
advancements are non-trivial. First, both algorithms maintain two Q functions, one for choosing actions (QÃÇ), and
a temporary function that is updated on each step (GPa ),
but in DGPQ we have chosen specific representations to
handle continuous states and still maintain optimism and
sample complexity guarantees. DQL‚Äôs counting of (up to
m) discrete state visits for each state cannot be used in
continuous spaces, so DGPQ instead checks the immediate
convergence of GPa at st to determine if it should compare QÃÇ(st , a) and GPa (st ). Lastly, DQL‚Äôs discrete learning flags are not applicable in continuous spaces, so DGPQ
only compares the two functions as the GP variance crosses

Sample Efficient Reinforcement Learning with Gaussian Processes

a threshold, thereby partitioning the continuous MDP into
areas that are known (w.r.t. QÃÇ) and unknown, a property
that will be vital for proving the algorithm‚Äôs convergence.
One might be tempted to use yet another GP to model
QÃÇ(s, a). However, it is difficult to guarantee the optimism
of QÃÇ (QÃÇ ‚â• Q‚àó ‚àí ) using a GP, which is a crucial property
of most sample-efficient algorithms. In particular, if one attempts to update/overwrite the local prediction values of a
GP, unless the kernel has finite support (a point‚Äôs value only
influences a finite region), the overwrite will affect the values of all points in the GP and possibly cause a point that
was previously correct to fall below Q‚àó (s, a) ‚àí .
In order to address this issue, we use an alternative function
approximator for QÃÇ which includes an optimism bonus as
used in C-PACE (Pazis & Parr, 2013). Specifically, QÃÇ(s, a)
is stored using a set of values that have been updated from
the set of GP s, (hsi , ai i, ¬µÃÇi ) and a Lipschitz constant LQ
that is used to find an optimistic upper bound for the Q
function for hs, ai:

QÃÇ(s, a) = min

min

hsi ,ai‚ààBV


¬µÃÇi +LQ d((s, a), (si , a)), Vmax (7)

We refer to the set (si , a) as the set of basis vectors (BV ).
Intuitively, the basis vectors store values from the previously learned GPs. Around these points, Q‚àó cannot be
greater than ¬µÃÇi + LQ d((s, a), (si , a)) by continuity. To
predict optimistically at points not in BV , we search over
BV for the point with the lowest prediction including the
weighted distance bonus. If no point in BV is sufficiently
close, then Vmax is used instead. This representation is
also used in C-PACE (Pazis & Parr, 2013) but here it simply stores the optimistic Q-function; we do not perform a
fixed point operation. Note the GP still plays a crucial role
in Algorithm 1 because its confidence bounds, which are
not captured by QÃÇ, partition the space into known/unknown
areas that are critical for controlling updates to QÃÇ.
In order to perform an update (partial overwrite) of QÃÇ, we
add an element h(si , ai ), ¬µÃÇi i to the basis vector set. Redundant constraints are eliminated by checking if the new
constraint results in a lower prediction value at other basis
vector locations. Thus, the pseudocode for updating QÃÇ is as
follows: add point h(si , ai ), ¬µÃÇi i to the basis vector set; if for
any j, ¬µi +LQ d((si , a), (sj , a)) ‚â§ ¬µj , delete h(sj , aj ), ¬µÃÇj i
from the set.
Figure 1 shows the advantage of this technique over the
naƒ±Ãàve GP training discussed earlier. DGPQ learns the optimal action within 50 steps, while the naƒ±Ãàve implementation as well as an -greedy variant both oscillate between
the two actions. This example illustrates that the targeted
exploration of DGPQ is of significant benefit compared to
untargeted approaches, including the -greedy approach of
GP-SARSA.

5. The Sample Complexity of DGPQ
In order to prove that DGPQ is PAC-MDP we adopt a proof
structure similar to that of DQL (Strehl et al., 2009) and
refer throughout to corresponding theorems and lemmas.
First we extend the DQL definition of a ‚Äúknown state‚Äù
MDP MKt , containing state/actions from QÃÇ that have low
Bellman residuals.
Definition 4 During timestep t of DGPQ‚Äôs execution with
QÃÇ as specified and VÃÇ (s) = maxa QÃÇ(s, a), the set of known
states
is given by Kt = {hs, ai|QÃÇ(s, a) ‚àí (R(s, a) +
R
Œ≥ s0 T (s0 |s, a)VÃÇ (s0 )ds0 ) ‚â§ 31 }. MK contains the same
MDP parameters as M for hs, ai ‚àà K and values of Vmax
for hs, ai ‚àà
/ K.
We now recap (from Theorem 10 of (Strehl et al., 2009))
three sufficient conditions for proving an algorithm with
greedy policy values Vt is PAC-MDP. (1) Optimism:
VÃÇt (s) ‚â• V ‚àó (s) ‚àí  for all timesteps t. (2) Accuracy with
œÄt
respect to MKt ‚Äôs values: VÃÇt (s) ‚àí VM
(s) ‚â§  for all t.
K
t

(3) The number of updates to QÃÇ and the number of times
a state outside MK is reached is bounded by a polynomial
1
i.
function of hNS (r), 1 , 1Œ¥ , 1‚àíŒ≥
The proof structure is to develop lemmas that prove these
three properties. After defining relevant structures and
concepts, Lemmas 2 and 3 bound the number of possible
changes and possible attempts to change QÃÇ. We then define the ‚Äútypical‚Äù behavior of the algorithm in Definition
6 and show this behavior occurs with high probability in
Lemma 4. These conditions help ensure property 2 above.
After that, Lemma 5 shows that the function stored in QÃÇ is
always optimistic, fulfilling property 1. Finally, property 3
is shown by combining Theorem 1 (number of steps before
the GP converges) with the number of updates to QÃÇ from
Lemma 2, as formalized in Lemma 7.
We now give the definition of an ‚Äúupdate‚Äù (as well as ‚Äúsuccessful update‚Äù) to QÃÇ, which extends definitions from the
original DQL analysis.
Definition 5 An update (or successful update) of stateaction pair (s, a) is a timestep t for which a change to QÃÇ (an
overwrite) occurs such that QÃÇt (s, a)‚àí QÃÇt+1 (s, a) > 1 . An
attempted update of state-action pair (s,a) is a timestep t
2
for which hs, ai is experienced and GPa,t .Var(s) > œÉtol
‚â•
GPa,t+1 .Var(s). An attempted update that is not successful
(does not change QÃÇ) is an unsuccessful update.
We bound the total number of updates to QÃÇ by a polynomial
term Œ∫ based on the concepts defined above.
Lemma 2 The total number of successful updates (overwrites) during any execution of DGPQ with 1 = 13 (1‚àíŒ≥)

Sample Efficient Reinforcement Learning with Gaussian Processes

is


Œ∫ = |A|NS

(1 ‚àí Œ≥)
3LQ



3Rmax
+1
(1 ‚àí Œ≥)2 


(8)

Proof The proof proceeds by showing that the bound on
the number of updates in the single-state case from Sec3Rmax
based on driving the full function 4.1 is, (1‚àíŒ≥)
2 + 1
tion from Vmax to Vmin . This quantity is then multiplied
by the covering number and the number of actions. See the
Supplementary Material.
The next lemma bounds the number of attempted updates.
The proof is similar to the DQL analysis and shown in the
Supplementary Material.
Lemma 3 The total number of attempted updates (overwrites)
during any execution of GPQ is


(1‚àíŒ≥)
|A|NS 3LQ (1 + Œ∫).
We now define the following event, called A2 to link back
to the DQL proof. A2 describes the situation where a
state/action that currently has an inaccurate value (high
Bellman residual) with respect to QÃÇ is observed m times
and this causes a successful update.
Definition 6 Define Event A2 to be the event that for all
timesteps t, if hs, ai ‚àà
/ Kk1 and an attempted update of
hs, ai occurs during timestep t, the update will be successful, where k1 < k2 < ... < km = t are the timesteps
2
where GPak .Var(sk ) > œÉtol
since the last update to QÃÇ that
affected hs, ai.
We can set an upper bound on m, the number of experi2
ences required to make GPat .Var(st ) < œÉtol
, based on m
in Theorem 1 from earlier. In practice m will be much
smaller but unlike discrete DQL, DGPQ does not need to
be given m, since it uses the GP variance estimate to decide
if sufficient data has been collected. The following lemma
shows that with high probability, a failed update will not
occur under the conditions of event A2.

max
, we have from Lemma 1 that during each overwrite,
Vm = R1‚àíŒ≥

 Œ¥
there is no greater than probability Œ¥1 =
(1‚àíŒ≥)

3|A|NS

2
œÉtol
=

2œân2 2 (1 ‚àí Œ≥)4


2
9Rmax
(1 + Œ∫))
log( 6Œ¥ |A|NS (1‚àíŒ≥)
3LQ

(9)

we ensure that the probability of event A2 occurring is ‚â•
1 ‚àí Œ¥/3.
Proof The
maximum
number of attempted updates is


(1‚àíŒ≥)
|A|NS 3LQ (1 + Œ∫) from Lemma 3. Therefore, by
Œ¥

setting Œ¥1 =


3|A|NS

(1‚àíŒ≥)
3LQ


,
(1+Œ∫)

1 =

1
(1
3

‚àí Œ≥), and

(1+Œ∫)

The next lemma shows the optimism of QÃÇ for all timesteps
with high probability 3Œ¥ . The proof structure is similar to
Lemma 3.10 of (Pazis & Parr, 2013). See supplemental
material.
Lemma 5 During execution of DGPQ, Q‚àó (s, a)
21
holds for all hs, ai with probability 3Œ¥ .
Qt (s, a) + 1‚àíŒ≥

‚â§

The next Lemma connects an unsuccessful update to a
state/action‚Äôs presence in the known MDP MK .
Lemma 6 If event A2 occurs, then if an unsuccessful up2
date occurs at time t and GPa .Var(s) < œÉtol
at time t + 1
then hs, ai ‚àà Kt+1 .
Proof The proof is by contradiction and shows that an unsuccessful update in this case implies a previously success2
ful update that would have left GPa .Var(s) ‚â• œÉtol
. See the
Supplemental material.
The final lemma bounds the number of encounters with
state/actions not in Kt , which is intuitively the number of
points that make updates to the GP from Theorem 1 times
the number of changes to QÃÇ from Lemma 2. The proof is
in the Supplemental material.


Lemma 7 Let Œ∑ = NS (1‚àíŒ≥)
. If event A2 occurs and
3LQ
21
holds for all t and hs, ai then
QÃÇt (s, a) ‚â• Q‚àó (s, a) ‚àí 1‚àíŒ≥
the number of timesteps Œ∂ where hst , at i ‚àà
/ Kt is at most


3Rmax
Œ∂ = m|A|Œ∑
+1
(10)
(1 ‚àí Œ≥)2 

where

m=

Lemma 4 By setting

3LQ

of an incorrect update. Applying the union bound over all of the
possible attempted updates, we have the total probability of A2
not occuring is 3Œ¥ .

2
36Rmax
log
(1 ‚àí Œ≥)4 2



6
|A|Œ∑(1 + Œ∫)
Œ¥


|A|Œ∑

(11)

Finally, we state the PAC-MDP result for DGPQ, which is
an instantiation of the General PAC-MDP Theorem (Theorem 10) from (Strehl et al., 2009).
1
and 0 <
Theorem 2 Given real numbers 0 <  < 1‚àíŒ≥
2
Œ¥ < 1 and a continuous MDP M there exist inputs œÉtol
1
(see (9)) and 1 = 3 (1 ‚àí Œ≥) such that DGPQ executed on
M will be PAC-MDP by Definition 2 with only

Rmax Œ∂
log
(1 ‚àí Œ≥)2

 


1
1
log
Œ¥
(1 ‚àí Œ≥)

(12)

Sample Efficient Reinforcement Learning with Gaussian Processes

150

0

10

100

‚àí2

10

50
0
0

CPACE
DGPQ

‚àí4

100

200

10

300

Episode

0

1000

2000
Step

3000

4000

Figure 2. Average (10 runs) steps to the goal and computation
time for C-PACE and DGPQ on the square domain.

Reward Averaged over 10 Trials

Reward per Episode

CPACE
DGPQ

CPU Time

Steps to goal

200

0
‚àí100
‚àí200
‚àí300
‚àí400
‚àí500
LQ = 5

‚àí600
‚àí700
0

LQ = 10
200

400
600
Episode

800

1000

Figure 3. Average (10 runs) reward on the F16 domain.
‚àó

timesteps where Qt (st , at ) < V (st ) ‚àí , where Œ∂ is defined in
Equation (10).

The proof of the theorem is the same as Theorem 16 by
(Strehl et al., 2009), but with the updated lemmas from
above. The three crucial properties hold when A2 occurs,
which Lemma 4 guarantees with probability 3Œ¥ . Property
1 (optimism) holds from Lemma 5. Property 2 (accuracy)
holds from the Definition 4 and analysis of the Bellman
Equation as in Theorem 16 by (Strehl et al., 2009). Finally, property 3, the bounded number of updates and escape events, is proven by Lemmas 2 and 7.

6. Empirical Results
Our first experiment is in a 2-dimensional square over
[0, 1]2 designed to show the computational disparity between C-PACE and DGPQ. The agent starts at [0, 0] with a
goal of reaching within a distance of 0.15 of [1, 1]. Movements are 0.1 in the four compass directions with additive
uniform noise of ¬±0.01. We used an L1 distance metric,
LQ = 9, and an RBF kernel with Œ∏ = 0.05, œân2 = 0.1
for the GP. Figure 2 shows the number of steps needed per
episode (capped at 200) and computational time per step
used by C-PACE and DGPQ. C-PACE reaches the optimal
policy in fewer episodes but requires orders of magnitude
more computation during steps with fixed point computations. Such planning times of over 10s are unacceptable in
time sensitive domains, while DGPQ only takes 0.003s per
step.
In the second domain, we use DGPQ to stabilize a simulator of the longitudinal dynamics of an unstable F16
with linearized dynamics, which motivates the need for a
real-time computable policy. The five dimensional state
space contains height, angle of attack, pitch angle, pitch
rate, and airspeed. Details of the simulator can be found
in (Stevens & Lewis, 2003). We used the reward r =
‚àí|h ‚àí hd |/100ft ‚àí |hÃá|/100ft/s ‚àí |Œ¥e |, with aircraft height
h, desired height hd and elevator angle (degrees) Œ¥e . The
control input was discretized as Œ¥e ‚àà {‚àí1, 0, 1} and the
elevator was used to control the aircraft. The thrust input
to the engine was fixed as the reference thrust command at

the (unstable) equilibrium. The simulation time step size
was 0.05s and at each step, the air speed was perturbed
with Gaussian noise N (0, 1) and the angle of attack was
perturbed with Gaussian noise N (0, 0.012 ). A RBF kernel
with Œ∏ = 0.05, œân2 = 0.1 was used. The initial height was
hd , and if |h ‚àí hd | ‚â• 200, then the vertical velocity and
angle of attack were set to zero to act as boundaries.
DGPQ learns to stabilize the aircraft using less than 100
episodes for LQ = 5 and about 1000 episodes for LQ =
10. The disparity in learning speed is due to the curse of
dimensionality. As the Lipschitz constant doubles, Nc increases in each dimension by 2, resulting in a 25 increase in
Nc . DGPQ requires on average 0.04s to compute its policy
at each step, which is within the 20Hz command frequency
required by the simulator. While the maximum computation time for DGPQ was 0.11s, the simulations were run in
MATLAB so further optimization should be possible. Figure 3 shows the average reward of DGPQ in this domain
using LQ = {5, 10}. We also ran C-PACE in this domain
but its computation time reached over 60s per step in the
first episode, well beyond the desired 20 Hz command rate.

7. Conclusions
This paper provides sample efficiency results for using GPs
in RL. In section 3, GPs are proven to be usable in the
KWIK-Rmax MBRL architecture, establishing the previously proposed algorithm GP-Rmax as PAC-MDP. In section 4.1, we prove that existing model-free algorithms using
a single GP have exponential sample complexity, connecting to seemingly unrelated negative results on Q-learning
learning speeds. Finally, the development of DGPQ provides the first provably sample efficient model-free (without a planner or fixed-point computation) RL algorithm for
general continuous spaces.

Acknowledgments
We thank Girish Chowdhary for helpful discussions, Hassan Kingravi for his GP implementation, and AerojetRocketdyne and ONR #N000141110688 for funding.

Sample Efficient Reinforcement Learning with Gaussian Processes

References
Brafman, Ronen I. and Tennenholtz, Moshe. R-MAX - a
general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 3:213‚Äì231, 2002.
Chowdhary, Girish, Liu, Miao, Grande, Robert C., Walsh,
Thomas J., How, Jonathan P., and Carin, Lawrence. Offpolicy reinforcement learning with gaussian processes.
Acta Automatica Sinica, To appear, 2014.
Chung, Jen Jen, Lawrance, Nicholas R. J., and Sukkarieh,
Salah. Gaussian processes for informative exploration
in reinforcement learning. In Proceedings of the IEEE
International Conference on Robotics and Automation,
pp. 2633‚Äì2639, 2013.
CsatoÃÅ, L. and Opper, M. Sparse on-line gaussian processes.
Neural Computation, 14(3):641‚Äì668, 2002.
Deisenroth, Marc Peter and Rasmussen, Carl Edward.
Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the International Conference on Machine Learning (ICML), pp. 465‚Äì472, 2011.
Engel, Y., Mannor, S., and Meir, R. Reinforcement learning
with Gaussian processes. In Proceedings of the International Conference on Machine Learning (ICML), 2005.
Even-Dar, Eyal and Mansour, Yishay. Learning rates for
Q-learning. Journal of Machine Learning Research, 5:
1‚Äì25, 2004.
Jung, Tobias and Stone, Peter. Gaussian processes for sample efficient reinforcement learning with RMAX-like exploration. In Proceedings of the European Conference
on Machine Learning (ECML), 2010.
Kalyanakrishnan, Shivaram and Stone, Peter. An empirical
analysis of value function-based and policy search reinforcement learning. In Proceedings of the International
Conference on Autonomous Agents and Multiagent Systems, pp. 749‚Äì756, 2009.
Li, Lihong and Littman, Miachael L. Reducing reinforcement learning to KWIK online regression. Annals of
Mathematics and Artificial Intelligence, 58(3-4):217‚Äì
237, 2010.
Li, Lihong, Littman, Michael L., Walsh, Thomas J., and
Strehl, Alexander L. Knows what it knows: a framework
for self-aware learning. Machine Learning, 82(3):399‚Äì
443, 2011.
Pazis, Jason and Parr, Ronald. PAC optimal exploration in
continuous space markov decision processes. In Proceedings of the AAAI Conference on Artificial Intelligence, 2013.

Puterman, Martin L. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. Wiley, New York,
1994.
Rasmussen, C. and Williams, C. Gaussian Processes for
Machine Learning. MIT Press, Cambridge, MA, 2006.
Stevens, Brian L. and Lewis, Frank L. Aircraft control and
simulation. Wiley-Interscience, 2003.
Strehl, Alexander L., Li, Lihong, Wiewiora, Eric, Langford, John, and Littman, Michael L. PAC model-free
reinforcement learning. In Proceedings of the International Conference on Machine Learning (ICML), pp.
881‚Äì888, 2006.
Strehl, Alexander L., Li, Lihong, and Littman, Michael L.
Reinforcement learning in finite MDPs: PAC analysis.
Journal of Machine Learning Research, 10:2413‚Äì2444,
2009.
Sutton, R. and Barto, A. Reinforcement Learning, an Introduction. MIT Press, Cambridge, MA, 1998.
Watkins, C. J. Q-learning. Machine Learning, 8(3):279‚Äì
292, 1992.

