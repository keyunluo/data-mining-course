Deterministic Independent Component Analysis

Ruitong Huang
RUITONG @ UALBERTA . CA
AndraÌs GyoÌˆrgy
GYORGY @ UALBERTA . CA
Csaba SzepesvaÌri
SZEPESVA @ UALBERTA . CA
Department of Computing Science, University of Alberta, Edmonton, AB T6G2E8 Canada

Abstract
We study independent component analysis with
noisy observations. We present, for the first time
in the literature, consistent, polynomial-time algorithms to recover non-Gaussian source signals
and the mixing matrixâˆšwith a reconstruction error
that vanishes at a 1/ T rate using T observations and scales only polynomially with the natural parameters of the problem. Our algorithms
and analysis also extend to deterministic source
signals whose empirical distributions are approximately independent.

1. Introduction
Independent Component Analysis (ICA) has received
much attention in the past decades. In the standard ICA
model one can observe a d-dimensional vector X that is a
linear mixture of d independent variables (S1 , . . . , Sd ) with
Gaussian noise:
X = AS + ,
(1)
where  âˆ¼ N (0, Î£) is a d-dimensional Gaussian noise with
zero mean and covariance matrix Î£, and A is a nonsingular
d Ã— d mixing matrix. The goal of the observer is to recover
(separate) the source signals and the mixing matrix given
several independent and identically distributed (i.i.d.) observations from the above model. The ICA literature is vast
in both practical algorithms and theoretical analyses; we
refer to the book of Comon and Jutten (2010) for a comprehensive survey. In this paper we investigate one of he
most important problems in ICA: finding consistent, computationally efficient algorithms with finite-sample performance guarantees. In particular, we aim to develop algorithms whose computational and sample complexity are
polynomial in the natural parameters of the problem.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

A popular approach to the ICA problem is to find a linear
transformation W for X by optimizing a, so-called, contrast function that measures dependence or non-gaussianity
of the resulting coordinates of W X. The optimal W then
can serve as an estimate of Aâˆ’1 , thereby recovering the
mixing matrix A. One of the most popular ICA algorithms,
FastICA (Hyvarinen, 1999), follows this approach for a
specific contrast function. FastICA has been analyzed theoretically from many aspects (Tichavsky et al., 2006; Oja
and Yuan, 2006; Ollila, 2010; Dermoune and Wei, 2013;
Wei, 2014). In particular, recently Miettinen et al. (2014)
showed that in the noise-free case (i.e., when X = AS), the
error of FastICA (when using a particular forth-momentsâˆš
based contrast function) vanishes at a rate of 1/ T where
T is the sample size. In addition, several other methods
have been shown to achieve similar error rates in the noisefree setting (e.g., Eriksson and Koivunen, 2003; Samarov
et al., 2004; Chen and Bickel, 2005; Chen et al., 2006).
However, to our knowledge, no similar finite sample results
are available in the noisy case.
On the other hand, several promising algorithms are available in the noisy case that make significant advances towards provably efficient and effective ICA algorithms, albeit fall short of providing a complete solution. Using a
quasi-whitening procedure, Arora et al. (2012) reduces the
problem to finding all the local optima of a specific function defined using the forth order cumulant, and propose a
polynomial-time algorithm to find them with appealing theoretical guarantees. However, the results depend on an unspecified parameter (Î² in the original paper) whose proper
tuning is essential; note that even an exhaustive search over
Î² is problematic, since its valid range is not well understood.
The exploitation of the special algebraic structure of the
forth moments induced by the independence leads to several other works related to ICA (Hsu and Kakade, 2013;
Anandkumar et al., 2012a;b). A similar idea is also discussed earlier as a intuitive argument to construct a contrast
function (Cardoso, 1999). The first rigorous proofs for this
idea are developed using matrix perturbation tools in a gen-

Deterministic Independent Component Analysis

eral tensor perspective (Anandkumar et al., 2012a;b; Goyal
et al., 2014). A common problem faced by these methods
is a minimal gap of the eigenvalues, which may result in
an exponential dependence on the number of source signals d. More precisely, these methods all require an eigendecomposition of some flattened tensor where the minimal
gap between the eigenvalues plays an essential role. Although the exact size of this gap is not yet understood,
a naive analysis introduces an exponential dependence on
the dimension d. Such dependence is also observed in the
literature (Cardoso, 1999; Goyal et al., 2014). One way
to circumvent such dependence is to directly decompose a
high-order tensor using the power method, which requires
no flattening procedure (Anandkumar et al., 2014). However, when applied to the ICA problem, this introduces
a bias term and so the error does not approach 0 as the
sample size approaches infinity. Another issue is the wellknown fact that the power method is unstable in practice for
high-order tensors. Goyal et al. (2014) proposed another
method by exploring the characteristic function rather than
the forth moments. However, their algorithm requires picking a parameter (Ïƒ in the original paper) that is smaller than
some unknown quantity, making their algorithm impossible to tune. Recently, Vempala and Xiao (2014) proposed
an ICA algorithm based on an elegant, recursive version of
the method of Goyal et al. (2014) that avoids dealing with
the aforementioned minimal gap; however, they still need
an oracle to set the unspecified parameter of Goyal et al.
(2014).
In this paper we propose a provably polynomial-time algorithm for the noisy ICA model. Our algorithm is a refined
version of the ICA method proposed by (Hsu and Kakade,
2013) (HKICA). However, we propose two simpler ways,
one inspired by Frieze et al. (1996), Arora et al. (2012),
and another based on Vempala and Xiao (2014), to deal
with the spacing problem of the eigenvalues under similar conditions to those of Goyal et al. (2014). Unlike the
method proposed by Goyal et al. (2014), our first method
can force the eigenvalues to be well-separated with a gap
that is independent of the mixing matrix A, while our second method, based on the recursive decomposition idea of
Vempala and Xiao (2014), avoids dealing with the minimum gap (on the price of introducing other complications).
âˆš
We prove that our methods achieve an O(1/ T ) error in
estimating A and the source signals, with high probability,
such that both the convergence rate and the computational
complexity scale polynomially with the natural parameters
of the problem. Our method needs no parameter tuning,
which makes it even more appealing.
Another contribution of the present paper is that our analysis is conducted in a deterministic manner. In practice,
ICA is also known to work well for unmixing the mixture of various deterministic signals. One of the classical

Source 1

1
0
âˆ’1

Source 2

0

5

10

15

0

5

10

15

0

5

10

15

0

5

10

15

0

5

10

15

0

5

10

15

0

5

10

15

0

5

10

15

0

5

10

15

0

5

10

15

1
0
âˆ’1
5

Mixture 1

0
âˆ’5
10

Mixture 2

0
âˆ’10

FastICA 1

1
0
âˆ’1

FastICA 2

1
0
âˆ’1

DICA 1

1
0
âˆ’1

DICA 2

1
0
âˆ’1

HKICA 1

1
0
âˆ’1

HKICA 2

1
0
âˆ’1

Figure 1. Example of ICA for deterministic sources: The first two
rows show the source signals s1 (t) = 0.5 âˆ’ bt âˆ’ 2bt/2cc, s2 =
cos(t), the next
the observations with mixing
 two rows present

1
âˆ’2
matrix A =
. The reconstructed (and rescaled)
2.6 âˆ’5.1
signals are shown for FastICA, HKICA, and DICA after sampling
As(t) at 10000 uniformly spaced points in the interval [0, 15].

demonstrations of ICA is showing that two periodic signals can be well recovered from their mixtures (HyvaÌˆrinen
and Oja, 2000). Such an example is shown in Figure 1.
It can be seen that our algorithm, DICA, in this particular example, can solve the problem better than other algorithms, FastICA (Hyvarinen, 1999) and HKICA (Hsu and
Kakade, 2013). Such phenomenon suggests that the usual
probabilistic notion is unsatisfactory if one wishes to have
deeper understanding of ICA. Our deterministic analysis
helps investigate this curious phenomenon without losing
any generality to the traditional stochastic setting. Formally, instead of observing T i.i.d. samples from (1), the
source signals are defined by the function s : N â†’ Rd be a
d-dimensional deterministic â€œsignalâ€, and the observations
are x(t) = As(t) + t , where (t )âˆ
t=1 is an i.i.d. sequence
of d-dimensional N (0, Î£) random variables.
The rest of this paper is organized as follows: The ICA
problem is introduced in detail in Section 2 and our main
results are highlighted in Section 3. The polynomial-time
algorithms underlying these results are developed through
the next two sections: Section 4.1 is devoted to the analysis
of the HKICA algorithm, also showing its disadvantages,
while our new algorithms are presented in Section 5. Experimental results are reported in Section 6. Proofs are presented in the full version of the paper (Huang et al., 2015).
1.1. Notation
We denote the set of real and natural numbers by R and N,
respectively. A vector v âˆˆ K d for a field K is assumed
to be a column vector. Let kvk2 denote its L2 -norm, and
for any matrix Z let kZk2 = maxv:kvk2 =1 kZvk2 denote
the corresponding induced norm. Denote the maximal and
minimal singular value of Z by Ïƒmax (Z) and Ïƒmin (Z),
respectively. Also, let Zi and Zi: denote the ith column
and, resp., row of Z, and let Z(2,min) = mini kZi k2 ,
Z(2,max) = maxi kZi k2 and Zmax = maxi,j |Zi,j |.

Deterministic Independent Component Analysis

Clearly, Ïƒmax (Z) = kZk2 â‰¥ Z(2,max) â‰¥ Zmax , and
Ïƒmin (Z) â‰¤ Z(2,min) . For a tensor (including vectors and
matrices) T , its Frobenious norm (or L2 norm) kT kF is
defined as the square root of the sum of the square of all
the entries. For a vector v = (v1 , . . . , vd ) âˆˆ K d , |v| is
defined coordinatewise, that is |v| = (|v1 |, . . . , |vd |). The
transpose of a vector/matrix Z is denoted by Z > , while
the inverse of the transpose is denoted by Z âˆ’> . The
outer product of two vectors v, u âˆˆ K d is denoted by
u âŠ— v = uv > . v âŠ—k denotes the k-fold outer product of v
with itself, that is, vâŠ—vâŠ—v . . .âŠ—v, which is a k-dimensional
tensor. Given a 4-dimensional tensor T , we denote the
matrix Z by T (Î·, Î·, Â·, Â·) that is generated by marginalizing the first two coordinates of T on the direction Î·, that
Pd
is, Zi,j = k1 ,k2 =1 Î·k1 Î·k2 Tk1 ,k2 ,i,j . (Similar definitions
for marginalizing different coordinates of the tensor.) For
a real vector v and some real number C, v â‰¤ C means
that all the entries of v are at most C. The bold symbol
1 denotes a vector with all entries being 1 (the dimension
of this vector will always be clear from the context). Finally, Poly (Â·, Â· Â· Â· , Â·) denotes a polynomial function of its
argument.



âˆ’ 2(EY âˆ¼Î½ () [Y âŠ—2 ])âŠ—2 (Î·, Â·, Î·, Â·) â‰¤ g(t)kÎ·k22 .
Here L and the function g may depend on {A, Î£, C, d}.
Remark 2.2. The first assumption forces the average of s
and  decay to 0 at a rate of g(t). The next one requires
that both the second and third moments of the noise be
bounded. The last assumption basically says that the induced measure of the noise function  has 0 kurtosis in the
limit.
We will also need to guarantee that the source signals and
the noise be approximately independent:
Assumption 2.3. Assume the source signal function and
the noise function are â€˜independentâ€™ up to the 4th moment
in the sense that for any i1 , i2 , j1 , j2 â‰¥ 0 such that i1 + i2 +
j1 + j2 â‰¤ 4,
kESâˆ¼Î½ (s) [(AS)âŠ—i1 âŠ— EY âˆ¼Î½ () [Y âŠ—j1 ]âŠ— (AS)âŠ—i2 ]
t

t

âˆ’ E(S,Y )âˆ¼Î½ (s,) [(AS)

kEY âˆ¼Î½ () [Y âŠ—j1 âŠ— ESâˆ¼Î½ (s) [(AS)âŠ—i1 ] âŠ— Y âŠ—j2 ]
t

t

In this paper we consider the following non-stochastic version of the ICA problem. Assume that we can observe
the d dimensional mixed signal x(t) âˆˆ Rd , t âˆˆ [T ] :=
{1, 2, . . . , T } generated by
x(t) = As(t) + (t),

(2)

where A is a d Ã— d nonsingular mixing matrix, s : [T ] â†’
[âˆ’C, C]d is a bounded, d-dimensional source function for
some constant C â‰¥ 1. , and  : [T ] â†’ Rd is the noise
function. We will denote the ith component of s by si .
Furthermore, we will use the notation Ïƒmin = Ïƒmin (A)
and Ïƒmax = Ïƒmax (A)
For any t, k â‰¥ 1 and signal u : [t] â†’ Rk , we introduce the
(u)
(u)
empirical distribution Î½t defined by Î½t (B) = 1t |{Ï„ âˆˆ
[t] : u(t) âˆˆ B}| for all Borel sets B âŠ‚ Rk . Next we will
impose assumptions on the empirical measure that guarantee that on the average we do not deviate too much from the
stochastic model. The next assumption implies that the empirical distributions of the source signals are approximately
zero mean, and that the noise is approximately zero-mean
Gaussian.
Assumption 2.1. Assume there exists a constant L and a
function g : N â†’ R such that g(t) â†’ 0 as t â†’ âˆ and
[Si ]kF , kEY âˆ¼Î½ () [Y ]kF â‰¤ g(t);
t

(ii) kEY âˆ¼Î½ () [Y âŠ—2 ]kF , kEY âˆ¼Î½ () [Y âŠ—3 ]kF â‰¤ L;
t
t



(iii)  EY âˆ¼Î½ () [Y âŠ—4 ] âˆ’ (EY âˆ¼Î½ () [Y âŠ—2 ])âŠ—2 (Î·, Î·, Â·, Â·)
t

âŠ— Y âŠ—j1 âŠ— (AS)âŠ—i2 ]kF â‰¤ g(t),

âˆ’ E(S,Y )âˆ¼Î½ (s,) [Y âŠ—j1 âŠ— (AS)âŠ—i1 âŠ— Y âŠ—j2 ]kF â‰¤ g(t),

2. The ICA Problem

(si )
i âˆ¼Î½t

âŠ—i1

t

t

(i) kES

F

t

t

for the same function g in Assumption 2.1, where (s, ) is
the function obtained by concatenating s and  together.
The sufficiency of such weaker assumptions is also discussed in the paper of Frieze et al. (1996). The next proposition shows that these assumptions are all satisfied, with
high probability, for the traditional stochastic setting of the
ICA model with Gaussian noise independent to the source
signals.
Proposition 2.4. In the traditional stochastic setting of
ICA, that is, when (s(t))tâˆˆ[T is an i.i.d. sequence, independent of the i.i.d. Gaussian noise sequence ((t))tâˆˆ[T ] ,there
âˆš

exists L = Poly Amax , kÎ£k2 , C, d, 1Î´ and g(t) = L/ t,
such that Assumptions 2.1 and 2.3 hold with probability at
least 1 âˆ’ Î´.
On the other hand, our setting can also cover some other
examples excluded by the traditional setting, such as the
example of Figure 1 in Section 1.
Example 2.5. Assume that the unknown sources si (1 â‰¤
i â‰¤ d) are deterministic and periodic. Our observation
x = As +  is a linear mixture of s contaminated by
i.i.d. Gaussian noise for each time step, where A is a nonsingular matrix and  âˆ¼ N (0, Î£) is Gaussian. Even though
 is i.i.d. for every time step, the observations cannot satisfy
the traditional i.i.d. assumption, since the source s is deterministic. However, it can be proved that if the ratio of the
periods of each pair of (si , sj ) is irrational, this example
satisfies all the assumptions above for T large enough.

Deterministic Independent Component Analysis

Our setting also extends the traditional one to a practically
important case, Markov sources.
Example 2.6. Assume that si is a stationary and ergodic
Markov source, and the sources are independent of each
other for 1 â‰¤ i â‰¤ d. Our observations are similar to the
setting in Example 2.5. Because of the Markov property,
the observations do not satisfy the i.i.d. assumptions. On
the other hand, it can be verified that this example satisfies
the above assumptions.

3. Main Results
The ICA approach requires that the components si of the
source signal s be statistically independent. In our setup,
(s)
we require that the empirical distribution Î½T be close to a
product distribution.
Fix some product distribution Âµ = Âµ1 âŠ— . . . âŠ— Âµd over
Rd such that ESi âˆ¼Âµi [Si ] = 0 and Îºi := |ESi âˆ¼Âµi [Si4 ] âˆ’
2
3 ESi âˆ¼Âµi [Si2 ] | =
6 0. Let K denote the diagonal matrix
diag(Îº1 , Â· Â· Â· , Îºd ), and define Îºmax = maxi Îºi and Îºmin =
mini Îºi .
(s)

To measure the distance of Î½T from Âµ, define the following family of â€œdistancesâ€ to measure the closeness of two
d
distributions: Given two distributions
Î½1 and
R
R Î½2 over R ,
let Dk (Î½1 , Î½2 ) = supf âˆˆF | f (s)dÎ½1 (s) âˆ’ f (s)dÎ½2 (s)|,
Qk
where F = {f : Rd â†’ R : f (s) =
j=1 sij , 1 â‰¤
i1 , . . . , ik â‰¤ d} is the set of all monomials up to degree
k. Finally let


(s)
(s)
Î¾ = 6C 2 D2 (Âµ, Î½T ) + D4 (Âµ, Î½T ) .
(3)
In general, we will need a condition that Î¾ is small enough,
so that the components of s are â€œindependentâ€ enough. To
this end, one should choose Âµ to minimize Î¾; however, such
a minimizer does not always exists. Generally, Âµ could
be selected as the product of the limit distributions, if applicable, of the individual sources. On the other hand, in
the traditional stochastic setting where the observations are
i.i.d. samples, the empirical distribution will converge to
the population distribution, which, based on the independence assumption, is a product probability measure. Therefore, in this case, Î¾ will be small for large enough sample
sizes.
Example 3.1. Let Âµ1 be a Bernoulli distribution
Âµ1 ({0.5}) = 1/2 and Âµ1 ({âˆ’0.5}) = 1/2, and Âµ2 to be
1
for
a distribution with density function p(x) = Ï€âˆš1âˆ’x
2
âˆ’1 â‰¤ x â‰¤ 1. For the demonstration example in Figure
1, pick Âµ = Âµ1 âŠ— Âµ2 . It is easy to see that Âµ1 (Âµ2 ) is the
limit distribution of source 1 (respectively, source 2). Let
T = 2 âˆ— u + b as the division with remainder, where u is
integer and 0 â‰¤ b < 2. Moreover, assume b â‰¤ 1 (similar analysis will go through for the case of b > 1). The
induced distribution Î½Ts1 of source 1 is Î½Ts1 ({0.5}) = u+b
T

and Î½Ts1 ({âˆ’0.5}) = Tu . Thus the total variation distance of
Âµ1 and Î½Ts1 is at most 1/(2T ). Similarly, it can be verified
that the total variation distance of Î½T and Âµ also decays as
1/T . Thus, D4 is O(1/T ), since the monomials f (s) in the
definition of D4 are bounded from above by 1. Lastly, note
that D2 is upper bounded by D4 by definition, so Î¾ decays
at a 1/T rate.
Now we are ready to state our main result, which shows
the existence of polynomial-time algorithms for ICA that
reconstructsâˆšthe mixing matrix A with error that vanishes
at an O(1/ T ) rate for T samples and is also polynomial
in the natural parameters of the problem:
Theorem 3.2. Consider the ICA problem (2). There exists an algorithm that estimates the mixing matrix A from
T samples of x such that (i) the computational complexity of the algorithm is O(d3 T ); and (ii) if Assumptions 2.1
and 2.3 are satisfied,


1
1 1
, , L, C, Ïƒmax ,
,
T â‰¥ Poly d,
Îºmin Î´
Ïƒmin
and there exists a product
 distribution Âµ such that 
1 1
1
, Ïƒmin ,
, , Î´, Îºmin ,
D4 (Âµ, Î½T ) â‰¤ Poly
C
Ïƒmax d
then, with probability at least 1 âˆ’ Î´, there exists a permutation Ï€ and constants {c1 , . . . , cd }, such that for all
1 â‰¤ k â‰¤ d,

kck AÌ‚Ï€(k) âˆ’ Ak k2 â‰¤ C D4 (Âµ, Î½T ) + g 2 (T ) + g(T ) ,
where C = Poly (Ïƒmax , 1/Ïƒmin , 1/Îºmin , 1/Î´, d, C, L), and
AÌ‚ is the output of the algorithm.
In particular, in the traditional stochastic setting, if S has
distribution Âµ and


1
1 1
T â‰¥ Poly d,
, , C, Ïƒmax ,
, kÎ£k2 ,
Îºmin Î´
Ïƒmin
then, with probability at least 1 âˆ’ Î´, there exists a permutation Ï€ and constants {c1 , . . . , cd }, such that for all
1 â‰¤ k â‰¤ d,


1
1
Poly C, Ïƒmax , Ïƒmin
, Îºmin
, 1Î´ , d
âˆš
.
kck AÌ‚Ï€(k) âˆ’ Ak k2 â‰¤
2T
Remark 3.3. Note that the result is polynomial in 1/Î´
which is weaker than being polynomial in log(1/Î´).
In the next sections, we will present two algorithms, DICA
(Algorithm 2 and HKICA.R (Algorithm 3) in Section 5 that
satisfy the theorem.

4. Estimating Moments: the HKICA
Algorithm
In this section we introduce the ICA method of Hsu and
Kakade (2013) which is based on the well-known excesskurtosis-like quantity defined as follows:

Deterministic Independent Component Analysis

For any p â‰¥ 1, Î· âˆˆ Rd , and distribution Î½ over Rd , let

4.1. Analysis of HKICA

>
p
mp(Î½) (Î·)
(4)
 = EXâˆ¼Î½ [(Î· X) ], 
(Î½)
(Î½)
2
1
fÎ½ (Î·) = 12 m4 (Î·) âˆ’ 3m2 (Î·) .
(5)
Hsu and Kakade (2013) showed that âˆ‡2 fÎ½ (x) (Î·), the secT
ond derivative of the function fÎ½ (x) , is extremely useful

Hsu and Kakade (2013) claimed that HKICA is easy to analyze using matrix perturbation techniques. In this section
we provide a rigorous analysis of the algorithm, which reveals some unexpected complications.
Definition 4.1. Let EâˆšÏˆ denote the following event: For

T

for the ICA problem: They showed that if Âµ(X) is the
distribution of the observations X in the stochastic setting where S comes from the product distribution Âµ, then
fÂµ(X) (Î·) = fAÂµ (Î·) for all Î· (where AÂµ denotes the distribution of AS) and, consequently, the eigenvectors1 of the
matrix M = âˆ‡2 fÂµ(X) (Ï†)(âˆ‡2 fÂµ(X) (Ïˆ))âˆ’1 are the rescaled
>

i
are distinct for all i. Thus, to obtain
columns of A if ÏˆÏ† >A
Ai
an algorithm, one needs to estimate âˆ‡2 fÂµ(X) in such a way
that the noise  could still be neglected.

An estimate âˆ‡2 fË† of âˆ‡2 fÂµ(X) is not hard, since for any Î½,
âˆ‡2 fÎ½ (Î·) can be computed as
(Î½)

(Î½)

(Î½)

âˆ‡2 fÎ½ (Î·) = GÎ½ (Î·) := G1 (Î·)âˆ’G2 (Î·)âˆ’2G3 (Î·), (6)
where
2
(Î½)
G1 (Î·) = EXâˆ¼Î½ [ Î· > X XX > ];
2
(Î½)
G2 (Î·) = EXâˆ¼Î½ [ Î· > X ]EXâˆ¼Î½ [XX > ];


(Î½)
G3 (Î·) = EXâˆ¼Î½ [ Î· > X X]EXâˆ¼Î½ [ Î· > X X > ],
and these quantities can be estimated using the observed
samples. In what follows, we will use the estimate
âˆ‡2 fË† := âˆ‡2 fÎ½ (x) and, in general, we will add a â€œhatâ€
T
to quantities which are derived from the empirical distri(x)
bution Î½T . It is important to note that, under our assumptions, the noise  has limited effect in the estimation procedure, as shown in the full version of the paper (Huang et al., 2015). In particular, the difference in
the estimation of the Hessian matrix caused by the noise
is Poly (LÎ· , L, d, Ïƒmax , C) (g(T ) + 1) g(T ). Denote this
quantity by P (LÎ· ). Note
âˆš that this error caused by the noise
decays at a rate of T . Putting everything together, we
obtain the algorithm HKICA, named after Hsu and Kakade
(2013), which is shown in Algorithm 1,
Algorithm 1 The HKICA algorithm.
input x(t) for 1 â‰¤ t â‰¤ T .
output An estimation of the mixing matrix A.
1: Sample Ï† and Ïˆ independently from a standard Gaussian distribution of dimension d;
2: Evaluate âˆ‡2 fË†(Ï†) and âˆ‡2 fË†(Ïˆ),
3: Compute MÌ‚ = (âˆ‡2 fË†(Ï†))(âˆ‡2 fË†(Ïˆ))âˆ’1 ;
4: Compute all the eigenvectors of MÌ‚ , {Âµ1 , . . . , Âµd };
5: Return AÌ‚ = (Âµ1 , . . . , Âµd ).
1
Throughout the paper eigenvectors always mean right eigenvectors, unless specified otherwise.

Ï€A

âˆš(2,min) ` for 0 â‰¤ ` â‰¤ 1, and
some fixed C1 =
2d
âˆš
Lu â‰¥ 2d, mini |Ïˆ > Ai | â‰¥ C1 and kÏˆk2 â‰¤ Lu hold
simultaneously.

The performance of the HKICA algorithm will essentially
depend on the parameter , as shown in the following theo

rem, where
 Ï†> A  2  Ï†> A  2 


j
i
(7)
âˆ’
Î³A = min 
.
i,j:i6=j  Ïˆ > Ai
Ïˆ > Aj 
Theorem 4.2. Suppose Assumptions 2.1 and 2.3 hold. Furthermore, assume that


1 1
1
1
T â‰¥ Poly d, Lu , C, Ïƒmax , Îºmax , L, ,
,
,
,
` Îºmin Ïƒmin Î³A
and that there exist a product measure Âµ such that


1
1
1 1
,
,
, Îºmin , Ïƒmin , ` .
Î¾ â‰¤ Poly Î³A , ,
d Lu Ïƒmax Îºmax
Then, on the event EÏˆ , there exists a permutation Ï€ and
constants {c1 , . . . , cd }, such that for any k,
max kc1 AÌ‚Ï€(k) âˆ’ Ak k2 â‰¤

1â‰¤kâ‰¤d

1
(Î¾ + P (Lu ))Q
Î³A

(8)

where AÌ‚ is the output of the HKICA algorithm, and


1
1 1
.
Q = Poly d, Lu , Ïƒmax , Îºmax ,
,
,
Îºmin Ïƒmin `
Remark 4.3. (i) Note that the bound in (8) goes to zero
âˆš
âˆš
(s)
at an O(1/ T ) rate
âˆš whenever D4 (Âµ, Î½T ) = O(1/ T )
and g(T ) = O(1/ T ), as, e.g., in the stochastic setting.
(ii) The parameter 1/Î³A is essential in the above theorem, in the sense that not only the reconstruction error
bound is linear in 1/Î³A , but the condition also requires a
small 1/Î³A so that the above error bound is valid. Also,
since Î³A is the minimum spacing of the eigenvalues of
M = âˆ‡2 fAÂµ (Ï†)(âˆ‡2 fAÂµ (Ïˆ))âˆ’1 , the eigenvalue perturbations imposed by the noise cannot be too large compared to
Î³A without potentially ruining the eigenvectors of M ; thus,
the dependence on Î³A seems to necessary.
Despite the important role that Î³A plays in the efficiency
of the HKICA algorithm, it is not clear how it depends on
different properties of A. To the best of our knowledge,
even a polynomial (in the dimension d) lower bound of Î³A
is not yet available in the literature. Similar problems have
been discussed by HuÌˆsler (1987) and Goyal et al. (2014),
but there solutions are not applicable to our case.

Deterministic Independent Component Analysis

5. A Refined HKICA Algorithm
The problems with Î³A motivate us to refine the HKICA
algorithm. The idea is inspired by Arora et al. (2012) and
Frieze et al. (1996) using a quasi-whitening procedure:
One can show that âˆ‡2 fÂµ (Ïˆ) = AKDÏˆ A> where
DÏˆ
=
diag (Ïˆ > A1 )2 , Â· Â· Â· , (Ïˆ > Ad )2 , and so
1/2
B = AK 1/2 DÏˆ R> for some orthonormal matrix R.
Defining Ti = âˆ‡2 fÂµ (B âˆ’> Ï†i ), one can
âˆ’1/2
calculate that Ti
=
AK 1/2 DÏˆ Î›i A> where

2
>
2
Î›i = diag (Ï†>
and Ri denote
i R1 ) , . . . , (Ï†i Rd )
the ith column of R. Then M = T1 T2âˆ’1 = AÎ›Aâˆ’1
 > 2 
 > 2
Ï†1 Rd
Ï†1 R1
,
.
.
.
,
.
with Î› = Î›1 Î›âˆ’1
=
diag
>
2
Ï† R1
Ï†> R
2

2

d

Thus, Ai are again the eigenvectors of M , but now the
eigenvalues of M are defined in terms of the orthogonal
matrix R instead of A, and so the resulting minimum


spacing
 Ï†> R 2  Ï†> R 2 

 1 i
1 j
(9)
âˆ’
Î³R = min  >


i,j:i6=j  Ï†2 Ri
Ï†>
R
2 j
is much easier to handle.
The resulting algorithm, called Deterministic ICA (DICA),
is shown in Algorithm 2. Note that on the event EÏ† ,
Algorithm 2 Deterministic ICA (DICA)
input x(t) for 1 â‰¤ t â‰¤ T .
output An estimation of the mixing matrix A.
1: Sample Ïˆ from a d-dimensional standard Gaussian distribution;
2: Evaluate âˆ‡2 fË†(Ïˆ),
3: Compute BÌ‚ such that âˆ‡2 fË†(Ïˆ) = BÌ‚ BÌ‚ > ;
4: Sample Ï†1 and Ï†2 independently from the standard
Gaussian distribution;
5: Compute TÌ‚1
=
âˆ‡2 fË†(BÌ‚ âˆ’> Ï†1 ) and TÌ‚2
=
2Ë†
âˆ’>
âˆ‡ f (BÌ‚ Ï†2 );
 âˆ’1
6: Compute all the eigenvectors of MÌ‚ = TÌ‚1 TÌ‚2
,
{Âµ1 , . . . , Âµd };
7: Return AÌ‚ = {Âµ1 , . . . , Âµd }.

kÏ†>
j Rk2 â‰¤ Lu , j âˆˆ {1, 2}. We will show later that this
event EÏ† , as well as other events defined later, will hold
simultaneously with high probability.
Definition 5.1. Let EÏ† denote the following event: âˆšFor
âˆš
Ï€
some fixed constant Lu â‰¥ 2d and `l such that `l = âˆš2d
`
for 0 â‰¤ ` â‰¤ 1, kÏ†1 k2 â‰¤ Lu , kÏ†2 k2 â‰¤ Lu , and
mini {|Ï†>
2 Ri |} â‰¥ `l hold simultaneously.
Similarly to Theorem 4.2, one can show that under some
technical assumptions,
which hold

 with probability 1 if Î¾,
âˆš

P (Lu ), and P

âˆš

3Lu
1/2
2Ïƒmin Îºmin C1

are small enough, on the

event EÏˆ âˆ© EÏ† , there exists a permutation Ï€ and constants
{c1 , . . . , cd }, such that for 1 â‰¤ k â‰¤ d,
2
4Ïƒmax
kck AÌ‚Ï€(k) âˆ’ Ak k2 â‰¤
QÌƒ,
Î³R Ïƒmin
where AÌ‚ is the output of the DICA algorithm and QÌƒ is polinomial in the usual problem parameters and decays roughly
as (Î¾ + P (Lu )). Details are given in the full version of the
paper (Huang et al., 2015). It is very similar to the result of
Theorem 4.2, with Î³R in place of Î³A , as required.
To analyze Î³R analytically, note that Ï†1 and Ï†2 are independently sampled from standard Gaussian distribution.
>
>
>
Thus, {Ï†>
1 R1 , Â· Â· Â· , Ï†1 Rd , Ï†2 R1 , Â· Â· Â· , Ï†2 Rd } are 2d independent standard Gaussian random variables. Let Zi =
Ï†>
1 Ri
. Therefore, Zi , 1 â‰¤ i â‰¤ d are d independent
Ï†>
2 Ri
Cauchy(0, 1) random variables. Using this observation, we
show in the full version (Huang et al., 2015) that, among
others, Î³R â‰¥ 2dÎ´ 2 with probability at least 1 âˆ’ Î´.
Based on the above, one can show that Theorem 3.2 holds
for DICA (Huang et al., 2015). Furthermore, a heuristic
modification of DICA can also be derived that performs
better in the experiments, but proving performance guarantees for that algorithm has defied our efforts so far (details
are given in the full version of the paper, Huang et al. 2015).
5.1. Recursive Versions
Recently, Vempala and Xiao (2014) proposed a recursion
idea to improve the sample complexity of the Fourier PCA
algorithm of Goyal et al. (2014). Instead of recovering all
the columns of A in a single eigen-decomposition, the recursive algorithm only decomposes the whole space into
two subspaces according to the maximal spacing of the
eigenvalues, then recursively decomposes each subspaces
until they are all 1-dimensional. The insight of this recursive procedure is the following: when the maximal spacing of the eigenvalues are much larger than the minimal
one, the algorithm may win over a single decomposition
even with the accumulating errors through the recursion.
However, this algorithm is based on the assumption that
the mixing matrix is orthonormal, so that the projection to
its subspaces can always eliminate some component of the
source signal.
We adapt the above idea to our algorithms. Due to space
limitations, we will only consider the simplest recursive algorithm, the recursive version of HKICA, as an example.
To force an orthonormal mixing matrix, we will first compute the square root matrix B of âˆ‡2 f (Ïˆ) = ADÏˆ KA> .
1/2
Thus B = ADÏˆ K 1/2 R> for some orthonormal matrix R. Transforming our observations by B âˆ’1 , we have
the new observations y(t) = B âˆ’1 x(t) + B âˆ’1 (t) =
1/2
RDÏˆ K 1/2 s(t) + B âˆ’1 (t). Note that transformed noise

Deterministic Independent Component Analysis
1/2

vector B âˆ’1 (t) is still Gaussian. Also, DÏˆ K 1/2 is diago1/2
RDÏˆ K 1/2 s(t)

nal, thus
is an orthonormal mixture of independent sources. We then apply the recursive algorithm
to recover the mixing matrix R. Finally, BR gives an estimate of A up to scaling.
To recover R using a recursive algorithm, we follow the
idea of HKICA (and DICA) to compute two Hessian maâˆ’1
âˆ’1
trices T1 = RDÏˆ
Î›1 R> and T2 = RDÏˆ
Î›2 R> . Then,
instead of computing the eigen-decomposition of T0 =
T1 T2âˆ’1 (as in HKICA), we only decompose its eigenspace
into two subspaces, according to the maximal spacing of
the eigenvalues of T0 . The Decompose helper function
takes a projection matrix P of a subspace spanned by
some columns of R (WLOG we assume it is the first k
columns of R). Then we compute the projection of T0 as
M = P > T0 P . Thus the eigenspace of P M P > is in the
span of P . Lastly, by separating the eigenvectors of M according to its eigenvalues into P P1 and P P2 , the Decompose function repeatedly decomposes the subspaces into
two smaller subspaces.
Algorithm 3 Recursive version of HKICA (HKICA.R)
input x(t) for 1 â‰¤ t â‰¤ T .
output An estimation of the mixing matrix A.
1: Sample Ïˆ from a d-dimensional standard Gaussian distribution;
2: Evaluate âˆ‡2 fË†(Ïˆ) = GÌ‚(Ïˆ);
3: Compute BÌ‚ such that âˆ‡2 fË†(Ïˆ) = BÌ‚ BÌ‚ > ;
4: Compute yÌ‚(t) = BÌ‚ âˆ’1 x(t) for 1 â‰¤ t â‰¤ T ;
5: Let P = Id ;
6: Compute RÌ‚ = Decompose(yÌ‚, P );
7: Return BÌ‚ RÌ‚;
Algorithm 4 The Decompose helper function
input x(t) for 1 â‰¤ t â‰¤ T , a projection matrix P âˆˆ RdÃ—k
(d â‰¥ k).
output An estimation of the mixing matrix A âˆˆ RdÃ—k .
1: if k == 1, Return P ;
2: Sample Ï†1 and Ï†2 independently from a standard
Gaussian distribution of dimension d;
3: Evaluate âˆ‡2 fË†(Ï†1 ) and âˆ‡2 fË†(Ï†2 ),
4: Compute TÌ‚ = (âˆ‡2 fË†(Ï†1 ))(âˆ‡2 fË†(Ï†2 ))âˆ’1 ;
5: Compute MÌ‚ = P > TÌ‚ P ;
6: Compute all the eigen-decomposition of MÌ‚ , its
eigenvalues{Ïƒ1 , . . . , Ïƒd } where Ïƒ1 â‰¥ . . . â‰¥ Ïƒk and
their corresponding eigenvectors {Âµ1 , . . . , Âµk };
7: Find the index m = arg max Ïƒm âˆ’ Ïƒm+1 ;
8: Let P1 = (Âµ1 , . . . , Âµm ), and P2 = (Âµm+1 , . . . , Âµk );
9: Compute W1 = Decompose(x, P P1 ), and W2 =
Decompose(x, P P2 );
10: Return [W1 , W2 ];

Remark 5.2. Other algorithms can be modified into a recursive version in a similar way.
Theorem 5.3. Under the conditions of Theorem 3.2, with
probability at least 1 âˆ’ Î´, the recursive version of HKICA
returns a mixing matrix AÌ‚ with an error kAÌ‚ âˆ’ ADP k2
bounded by


1 1
1
Â¯
,
, , Lu , L, C, Ïƒmax (QÌƒ2 + Î¾)
Poly d,
Îºmin Ïƒmin `
for some diagonal matrix D and permutation matrix P .
Remark 5.4. Note that when T is large enough, the term
Â¯ which is the error carried over
QÌƒ2 will be dominated by Î¾,
from quasi-whitening. The recursion idea improves the
sample complexity of the eigen-decomposition (to recover
the orthonormal mixing matrix R).

6. Experimental Results
In this section we compare the performance of different
ICA algorithms in some synthetic examples, with mixing
matrices of different coherences.
We test 9 algorithms: HKICA (HKICA), and its recursive version (HKICA.R); DICA (DICA), and its recursive version (DICA.R); the modified version of DICA
(MDICA), and its recursive version (MDICA.R); the default FastICA algorithm from the â€™ITEâ€™ toolbox (SzaboÌ
et al., 2012) (FICA); the recursive Fourier PCA algorithm
of Xiao (2014) (FPCA); and random guessing (Random).
FPCA is modified so that it can be applied to the case of
non-orthogonal mixing matrix.
In the simulation, a common mixing matrix A of dimension 6 is generated in the following ways: We construct
four kinds of matrices: A1 = P ; A2 = vb Ã— 10 + 0.3 Ã— P ;
A3 = vb Ã— 10 + 0.05 Ã— P ; and A4 = vb Ã— 10 + 0.005 Ã— P .
Here the vector vb and the matrix P are both generated
from standard normal distribution (with different dimensions). Then all the mixing matrices are rescaled to a same
magnitude. We also generate an orthonormal mixing matrix R, obtained by computing the left column space of a
non-singular random matrix (from standard normal distribution). Then we generate
âˆš âˆš âˆšBPSK
âˆš signal s
âˆš aâˆš6-dimensional
as follows. Let p = ( 2, 5, 7, 11, 13, 19). We
generate a {+1, âˆ’1} valued sequence q(t) uniformly at
random for 1 â‰¤ t â‰¤ T , and set si (t) = q(t)i Ã— sin(pi t).
Note that in order to have the components of s close to
independent, we need the ratio of their frequencies are irrational.
Lastly, the observed signal is generated as x = As + c
where  is the noise generated from a d-dimensional normal
distribution with randomly generated covariance. We take
T = 20000 instances of the observed signal on time steps
t = 1, . . . , 20000. We test the noise ratio c from 0 (noise-

Deterministic Independent Component Analysis

2

2

Reconstruction Error

Reconstruction Error

2.5

1.5

1

FICA
HKICA
MDICA
DICA
FPCA
HKICA.R
DICA.R
MDICA.R

0.5

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.5

1

FICA
HKICA
MDICA
DICA
FPCA
HKICA.R
DICA.R
MDICA.R

0.5

0

1

0

0.1

0.2

0.3

Noise_ratio c

2

2

1.5

1

FICA
HKICA
MDICA
DICA
FPCA
HKICA.R
DICA.R
MDICA.R

0.5

0.1

0.2

0.3

0.4

0.5

0.6

0.5

0.6

0.7

0.8

0.9

1

The Reconstruction Error of A3
2.5

Reconstruction Error

Reconstruction Error

The Reconstruction Error of A2

0

0.4

Noise_ratio c

2.5

0

formance in case of low coherence. As the coherence of
the mixing matrix A increases, its performance decreases
quickly and becomes sensitive to noise.

The Reconstruction Error of A1

The Reconstruction Error of R
2.5

0.7

0.8

0.9

1

Noise_ratio c

On the other hand, MDICA tries to achieve a small estimation error, meanwhile we expect it to keep the eigenvalue
spacing large (intuitively, it is approximately the spacing
of the square of d Gaussian random variables), leading to
good performance. This is confirmed by the experimental
results, in both the non-recursive and recursive versions.

1.5

1

FICA
HKICA
MDICA
DICA
FPCA
HKICA.R
DICA.R
MDICA.R

0.5

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Noise_ratio c

The Reconstruction Error of A4
2.5

Reconstruction Error

2

1.5

1

FICA
HKICA
MDICA
DICA
FPCA
HKICA.R
DICA.R
MDICA.R

0.5

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

We expected that DICA will achieve smaller error for an
extremely coherent A, since 1/Î³A will be much larger than
1/Î³R . However, the experimental results indicate the opposite. Note that high coherence implies small minimal
singular value. In this case, the estimation error of M in
DICA could be much larger than that in HKICA, because
of the fourth degree of Aâˆ’1 . This error overwhelms the
improvement brought by larger eigenvalue spacings, if the
sample size is not large enough. The investigation of this
phenomenon is left for future work.

1

Noise_ratio c

Figure 2. Reconstruction Error

free) to 1 (very noisy). All the algorithms are evaluated on
a 150 repetitions. For each repetition, we try 3 times and
report the best.
We measure the performances of the algorithms by its actual reconstruction error. In particular, we evaluate the following quantity between the true mixing matrix A and the
estimate AÌ‚ returned by the algorithms: minÎ ,S kAÌ‚Î S âˆ’
AkFrob , where Î  is a permutation matrix, and S is a column
scaling matrix (diagonal). The calculation of this measure
would require a exhaust search for the optimal permutation.
6.1. Results
We report the reconstruction errors for different kinds of
mixing matrices and noise ratios.
The experimental results suggest that moment methods are
more robust to high-coherence mixing matrices and Gaussian noise than FastICA. FastICA achieves the best per-

The recursive idea is not always helpful for the moment
methods. For a highly coherent A, the recursive versions
outperform their non-recursive counterparts. Note that in
this case, A is close to singular (small minimal singular
value), and thus it requires more samples. On the other
hand, when A has relatively low coherence, the estimation
error of the fourth moments contributes more to the reconstruction error. Recursive algorithms suffers from making
several such estimations.
In summary, the results suggest that these moment methods
are comparable to each other in practice, while FastICA is
better for mixing matrices with low coherence or mild coherence with low noise. If the mixing matrix is orthonormal, then FPCA performs better than the other algorithms.
If the observations have large noise and the mixing matrix
is not extremely coherent, then HKICA may be the best
choice. In the case of an extremely coherent mixing matrix, MDICA performs the best. Also, the recursive idea is
very helpful for small sample sizes.

7. Conclusions
We considered the problem of independent component
analysis with noisy observation. For the first time in the literature, we presented ICA algorithms that can recover nonGaussian source signals with polynomial computational
complexity and provable performance guarantees on the
reconstruction error that guarantee that
âˆš for T samples the
reconstruction error vanishes at a 1/ T rate and depends
only polynomially on the natural parameters of the problem. The algorithms do not depend on unknown problem
parameters, and also extend to deterministic sources with
approximately independent empirical distributions.

Deterministic Independent Component Analysis

Acknowledgements
This work was supported by the Alberta Innovates Technology Futures and NSERC.

References
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning latent variable models. CoRR, abs/1210.7559, 2012a.
A. Anandkumar, D. Hsu, and S. M. Kakade. A method of
moments for mixture models and hidden markov models. arXiv preprint arXiv:1203.0683, 2012b.
A. Anandkumar, R. Ge, and M. Janzamin. Guaranteed nonorthogonal tensor decomposition via alternating rank-1
updates. arXiv preprint arXiv:1402.5180, 2014.
S. Arora, R. Ge, A. Moitra, and S. Sachdeva. Provable
ica with unknown gaussian noise, with implications for
gaussian mixtures and autoencoders. In Advances in
Neural Information Processing Systems, pages 2375â€“
2383, 2012.
J. Cardoso. High-order contrasts for independent component analysis. Neural computation, 11(1):157â€“192,
1999.
A. Chen and P. J Bickel. Consistent independent component analysis and prewhitening. Signal Processing, IEEE
Transactions on, 53(10):3625â€“3632, 2005.
A. Chen, P. J Bickel, et al. Efficient independent component analysis. The Annals of Statistics, 34(6):2825â€“
2855, 2006.
P. Comon and C. Jutten. Handbook of Blind Source Separation: Independent component analysis and applications.
Academic press, 2010.
A. DasGupta. Finite sample theory of order statistics and
extremes. In Probability for Statistics and Machine
Learning, pages 221â€“248. Springer, 2011.
A. Dermoune and T. Wei. FastICA algorithm: Five criteria
for the optimal choice of the nonlinearity function. IEEE
transactions on signal processing, 61(5-8):2078â€“2087,
2013.
J. Eriksson and V. Koivunen. Characteristic-function-based
independent component analysis. Signal Processing, 83
(10):2195â€“2208, 2003.
A. Frieze, M. Jerrum, and R. Kannan. Learning linear
transformations. In 37th IEEE Annual Symposium on
Foundations of Computer Science, pages 359â€“359. IEEE
Computer Society, 1996.
N. Goyal, S. Vempala, and Y. Xiao. Fourier PCA and robust

tensor decomposition. In Proceedings of the 46th Annual
ACM Symposium on Theory of Computing, pages 584â€“
593. ACM, 2014.
D. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer Science, pages 11â€“20.
ACM, 2013.
R. Huang, A. GyoÌˆrgy, and Cs. SzepesvaÌri. Deterministic
independent component analysis. in preparation, 2015.
J. HuÌˆsler. Minimal spacings of non-uniform densities.
Stochastic processes and their applications, 25:73â€“81,
1987.
A. Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis. Neural Networks, IEEE
Transactions on, 10(3):626â€“634, 1999.
A. HyvaÌˆrinen and E. Oja. Independent component analysis:
algorithms and applications. Neural Networks, 13(4â€“5):
411â€“430, 2000.
B. Laurent and P. Massart. Adaptive estimation of a
quadratic functional by model selection. Annals of
Statistics, pages 1302â€“1338, 2000.
S. Miettinen, J.and Taskinen, K. Nordhausen, and H. Oja.
Fourth moments and independent component analysis.
arXiv preprint arXiv:1406.4765, 2014.
E. Oja and Z. Yuan. The FastICA algorithm revisited: Convergence analysis. Neural Networks, IEEE Transactions
on, 17(6):1370â€“1381, 2006.
E. Ollila. The deflation-based FastICA estimator: statistical
analysis revisited. Signal Processing, IEEE Transactions
on, 58(3):1527â€“1541, 2010.
A. Samarov, A. Tsybakov, et al. Nonparametric independent component analysis. Bernoulli, 10(4):565â€“582,
2004.
G.W. Stewart and J.-g. Sun. Matrix perturbation theory.
Computer science and scientific computing. Academic
Press, 1990. ISBN 9780126702309.
Z. SzaboÌ, B. PoÌczos, and A. LoÌ‹rincz. Separation theorem
for independent subspace analysis and its consequences.
Pattern Recognition, 45:1782â€“1791, 2012.
P. Tichavsky, Z. Koldovsky, and E. Oja. Performance analysis of the FastICA algorithm and CrameÌr-Rao bounds
for linear independent component analysis. Signal Processing, IEEE Transactions on, 54(4):1189â€“1203, 2006.
S. Vempala and Y. Xiao. Max vs min: Independent component analysis with nearly linear sample complexity.
CoRR, abs/1412.2954, 2014. URL http://arxiv.

Deterministic Independent Component Analysis

org/abs/1412.2954.
T. Wei. The convergence and asymptotic analysis of
the generalized symmetric FastICA algorithm. arXiv
preprint arXiv:1408.0145, 2014.
Y. Xiao. Fourier pca package. GitHub, 2014.
https://github.com/yingusxiaous/
libFPCA.

URL

