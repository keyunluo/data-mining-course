Finding Dense Subgraphs via Low-Rank Bilinear Optimization

Dimitris S. Papailiopoulos
Ioannis Mitliagkas
Alexandros G. Dimakis
Constantine Caramanis
The University of Texas at Austin

Abstract
Given a graph, the Densest k-Subgraph (DkS)
problem asks for the subgraph on k vertices that
contains the largest number of edges. In this
work, we develop a new algorithm for DkS that
searches a low-dimensional space for provably
dense subgraphs. Our algorithm comes with
novel performance bounds that depend on the
graph spectrum. Our graph-dependent bounds
are surprisingly tight for real-world graphs where
we find subgraphs with density provably within
70% of the optimum. These guarantees are significantly tighter than the best available worst
case a priori bounds.
Our algorithm runs in nearly linear time, under spectral assumptions satisfied by most graphs
found in applications. Moreover, it is highly
scalable and parallelizable. We demonstrate this
by implementing it in MapReduce and executing numerous experiments on massive real-world
graphs that have up to billions of edges. We empirically show that our algorithm can find subgraphs of significantly higher density compared
to the previous state of the art.

1. Introduction
Given a graph G on n vertices with m edges and a parameter k, we are interested in finding an induced subgraph on k
vertices with the largest average degree, also known as the
maximum density. This is the Densest k-Subgraph (DkS) ‚Äì
a fundamental problem in combinatorial optimization with
applications in numerous fields including social sciences,
communication networks, and biology (see e.g. (Hu et al.,
2005; Gibson et al., 2005; Dourisboure et al., 2007; Saha
et al., 2010; Miller et al., 2010; Bahmani et al., 2012)).
DkS is a notoriously hard problem. It is NP-hard by reducProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

DIMITRIS @ UTEXAS . EDU
IOANNIS @ UTEXAS . EDU
DIMAKIS @ AUSTIN . UTEXAS . EDU
CONSTANTINE @ UTEXAS . EDU

tion to M AX C LIQUE. Moreover, Khot showed in (Khot,
2004) that, under widely believed complexity-theoretic assumptions, DkS cannot be approximated within an arbitrary constant factor.1 The best known approximation ratio
was n1/3+ (for some small ) due to (Feige et al., 2001).
Recently, (Bhaskara et al., 2010) introduced an algorithm
with approximation ratio n1/4+ , that runs in time nO(1/) .
Such results, where the approximation factor scales as a
polynomial in the number of vertices, are too pessimistic
for real-world applications. This resistance to better approximations, despite the long history of the problem, suggests that DkS is probably very hard in the worst case.
Our Contributions. In this work we move beyond the
worst case framework. We present a novel DkS algorithm
that has two key features: i) it comes with approximation
guarantees that are surprisingly tight on real-world graphs
and ii) it is fully parallelizable and can scale up to graphs
with billions of edges.
Our algorithm combines spectral and combinatorial techniques; it relies on examining candidate subgraphs obtained from vectors lying in a low-dimensional subspace
of the adjacency matrix of the graph. This is accomplished
through a framework called the Spannogram, which we define below.
Our approximation guarantees are graph-dependent: they
are related to the spectrum of the adjacency matrix of the
graph. Let opt denote the average degree (i.e., the density)
of the densest k-subgraph, where 0 ‚â§ opt ‚â§ k ‚àí 1. Our
algorithm takes as input the graph, the subgraph size k, and
an accuracy parameter d ‚àà {1, . . . , n}. The output is a
subgraph on k vertices with density optd , for which we
obtain the following approximation result:
Theorem 1. For
unweighted
graph, our algorithm out any

d+2
puts in time O n Œ¥¬∑log n a k-subgraph that has density
optd ‚â• 0.5 ¬∑ (1 ‚àí Œ¥) ¬∑ opt ‚àí 2 ¬∑ |Œªd+1 |,
with probability 1 ‚àí n1 , where Œªi is the ith largest, in mag1

approximation ratio œÅ means that there exists an algorithm
that produces in polynomial time a number A, such that 1 ‚â§ opt
‚â§
A
œÅ, where opt is the optimal density.

Finding Dense Subgraphs via Low-Rank Bilinear Optimization

nitude, eigenvalue of the adjacency matrix of the graph.
If the graph is bipartite, or if the largest d eigenvalues
of the graph are
 positive, then our algorithm runs in time
O nd+1 + Td , and outputs a k-subgraph with density
optd ‚â• opt ‚àí 2 ¬∑ |Œªd+1 |,
where Td is the time to compute the d leading eigenvectors
of the adjacency matrix of the graph.
Our bounds come close to 2+ and 1+ factor approximations, when Œªd+1 is significantly smaller than the density of
the densest k-subgraph. In the following theorem, we give
such an example. However, we would like to note that in
the worst case our bounds might not yield something meaningful.
Theorem 2. If the densest-k-subgraph contains
a constant
‚àö
fraction of all the edges, and k = Œò( E), then we can
2
approximate DkS within a factor of 2 + , in time nO(1/ ) .
If additionally the graph is bipartite, we can approximate
DkS within a factor of 1 + .
The above result is similar to the 1 +  approximation ratio
of (Arora et al., 1995) for dense graphs, where the densestk-subgraph contains a constant fraction of the ‚Ñ¶(n2 ) edges,
where k = ‚Ñ¶(n). The innovation here is that our ratio also
applies to sparse graphs with sublinear number of edges.
Computable upper bounds. In addition to these theoretical guarantees, our analysis allows us to obtain a graphdependent upper bound for the optimal subgraph density.
This is shown in Fig. 3 in our experimental section, where
for many graphs our algorithm is provably within 70% from
the upper bound of opt. These are far stronger guarantees
than the best available a priori bounds. This illustrates the
potential power of graph-dependent guarantees that, however, require the execution of an algorithm.
Nearly-linear time approximation.
 d+2 Our algorithm has a
worst-case running time of O n Œ¥¬∑log n . Under some
mild spectral assumptions, a randomized version of our algorithm runs in nearly-linear time.
Theorem 3. Let the d largest eigenvalues of the graph be
positive,
 and let the d-th,(d + 1)-st largest have constant
 d 
ratio:  ŒªŒªd+1
 ‚â• C. Then, we can modify our algorithm
to output, with probability 1 ‚àí Œ¥, a k-subgraph with den
2
1
sity (1 ‚àí ) ¬∑ optd , in time O m ¬∑ log n + nd ¬∑ log ¬∑Œ¥
,
where m is the number of edges.
We found that the above spectral condition holds for all
d ‚â§ 5, in many real-world graphs that we tested.
Scalability. We develop two key scalability features that
allow us to scale up efficiently on massive graphs.
Vertex sparsification: We introduce a pre-processing step
that eliminates vertices that are unlikely to be part of

the densest k-subgraph. The elimination is based on the
vertices‚Äô weighted leverage scores (Mahoney & Drineas,
2009; Boutsidis et al., 2009) and admits a provable bound
on the introduced error. We empirically found that even
with a negligible additional error, the elimination dramatically reduced problem sizes in all tested datasets.
MapReduce implementation: We show that our algorithm
is fully-parallelizable and tailor it for the MapReduce
framework. We use our MapReduce implementation to
run experiments on Elastic MapReduce (EMR) on Amazon. In our large-scale experiments, we were able to scale
out to thousands of mappers and reducers in parallel over
800 cores, and find large dense subgraphs in graphs with
billions of edges.
1.1. Related work
DkS algorithms: One of the few positive results for DkS is
a 1 +  approximation for dense graphs where m = ‚Ñ¶(n2 ),
and in the linear subgraph setting k = ‚Ñ¶(n) (Arora et al.,
1995). For some values of m = o(n2 ) a 2 +  approximation was established by (Suzuki & Tokuyama, 2005).
Moreover, for any k = ‚Ñ¶(n) a constant factor approximation is possible via a greedy approach by (Asahiro
et al., 2000), or via semidefinite relaxations by (Srivastav
& Wolf, 1998) and (Feige & Langberg, 2001). Recently,
(Alon et al., 2013) established new approximation results
for graphs with small ‚Äú-rank,‚Äù using an approximate solver
for low-rank perturbed versions of the adjacency matrix.
There is a vast literature on algorithms for detecting communities and well-connected subgraphs:
greedy schemes (Ravi et al., 1994), optimization approaches (Jethava et al., 2012; d‚ÄôAspremont et al., 2010;
Ames, 2011), and the truncated power method (Yuan
& Zhang, 2011). We compare with various of these
algorithms in our evaluation section.
The Spannogram framework: We present an exact solver
for bilinear optimization problems on matrices of constant
rank, under {0, 1} and sparsity constraints on the variables.
Our theory is a generalization of the Spannogram framework, originally introduced in the foundational work of
(Karystinos & Liavas, 2010) and further developed in (Asteris et al., 2014; Papailiopoulos et al., 2013), that obtains
exact solvers for low-rank quadratic optimization problems
with combinatorial constraints, such as sparse PCA.
MapReduce algorithms for graphs: The design of MapReduce algorithms for massive graphs is an active research
area as Hadoop becomes one of the standards for storing
large data sets. The related work by Bahmani et al. (Bahmani et al., 2012) designs a novel MapReduce algorithm
for the densest subgraph problem. This densest subgraph
problem requires finding a subgraph of highest normalized density without enforcing a specific subgraph size k.

Finding Dense Subgraphs via Low-Rank Bilinear Optimization

Surprisingly, without a subgraph size restriction, the densest subgraph becomes polynomially solvable and therefore
fundamentally different from what we consider in this paper.

2. Proposed Algorithm
The density of a subgraph indexed by a vertex set S ‚äÜ
{1, . . . , n} is equal to the average degree of the vertices
within S:
1T A1S
den(S) = S
|S|
where A is the adjacency matrix (Ai,j = 1 if (i, j) is an
edge, else Ai,j = 0) and the indicator vector 1S has 1s
in the entries
Pindexed by S and 0 otherwise. Observe that
1TS A1S = i,j‚ààS Ai,j is twice the number of edges in the
subgraph with vertices in S.
For a fixed subgraph size |S| = k, we can express DkS as
a quadratic optimization:
DkS :

opt = (1/k) ¬∑ max 1TS A1S
|S|=k

where |S| = k denotes that the optimization variable is a
k-vertex subset of {1, . . . , n}.
The bilinear relaxation of DkS. We approximate DkS via
approximating its bipartite version. This problem can be
expressed as a bilinear maximization:
DBkS :

optB = (1/k) ¬∑ max max 1TX A1Y .
|X |=k |Y|=k

As we see in the following lemma, the two problems are
fundamentally related: a good solution for the bipartite version of the problem maps to a ‚Äúhalf as good‚Äù solution for
DkS. The proof is given in the Supplemental Material.
Lemma 1. A œÅ-approximation algorithm for DBkS implies
a 2œÅ-approximation algorithm for DkS.
2.1. DkS through low rank approximations
At the core of our approximation lies a constant rank
solver: we show that DBkS can be solved in polynomial
time on constant rank matrices. We solve constant rank
instances of DBkS instead of DkS due to an important implication: DkS is NP-hard even for rank-1 matrices with 1
negative eigenvalue, as we show in the Supplemental Material.
The exact steps of our algorithm are given in the pseudocode tables referred to as Algorithms 1-3.2 The output of
our algorithm is a k-subgraph Zd that has density optd that
2
In the pseudocode of Algorithm 2, topk (v), denotes the indices of the k largest signed elements of v.

comes with provable guarantees. We present our theoretical guarantees in the next subsection.
Our main algorithmic innovation, the constant rank solver
for DBkS (Algorithms 2-3), is called many times: in lines
5, 8, and 15 of our general DkS approximation, shown as
Algorithm 1. We describe its steps subsequently.
Algorithm 1 low-rank approximations for DkS
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

[Vd , Œõd ] = EVD(A, d)
if G is bipartite then
B = bi-adjacency of G
[Vd , Œ£d , Ud ] = SVD(B, d)
{Xd , Yd } = arg max|X |+|Y|=k 1TX Vd Œ£d UTd 1Y .
Zd = Xd ‚à™ Yd
else if The first d eigenvalues of A are positive then
{Xd , Xd } = arg max|X |=|Y|=k 1TX Vd Œõd VdT 1Y .
Zd = Xd
else
for i = 1 : logŒ¥ n do
draw n fair coins and assign them to vertices
L = vertices with heads; R = {1, . . . , n} ‚àí L
Bid = [Vd Œõd VdT ]L,R
{X i , Y i } = arg max|X |+|Y|=k 1TX Bid 1Y .
end for
{X i , Y i } = arg max1‚â§i‚â§n 1TX i Bid 1Y i
Zd = Xd ‚à™ Yd
end if
Output: Zd

Constant rank solver for DBkS. In the following we
present an exact solver for DBkS on constant rank approximations of A. Our DkS algorithm makes a number of calls
to the DBkS low-rank solver on slightly different (some
times rectangular) matrices. The details of the general lowrank solver are in the Supplemental Material.
Pd
Step 1: Obtain Ad = i=1 Œªi vi viT , a rank-d approximation of A. Here, Œªi is the i-th largest in magnitude eigenvalue and vi the corresponding eigenvector.
Step 2: Use Ad to obtain O(nd ) candidate subgraphs. For
any matrix A we can solve DBkS by exhaustively checking
2
all nk pairs (X , Y) of k-subsets of vertices. Surprisingly,
if we want to find the X , Y pairs that maximize 1TX Ad 1Y ,
i.e., the bilinear problem on the rank-d matrix Ad , then we
show that only O(nd ) candidate pairs need to be examined.
Step 3: Check all k-set pairs {X , Y} obtained by Step 2,
and output the one with the largest density on the low-rank
weighted adjacency Ad .
In the next section, we derive the constant rank-solver using two key facts. First, for each fixed vertex set Y, we
show that it is easy to find the optimal set X that maximizes
1TX Ad 1Y for that Y. Since this turns out to be easy, then
the challenge is to find the number of different vertex sets
Y thatwe need to check. Do we need to exhaustively check
all nk k-sets Y? We show that this question is equivalent

Finding Dense Subgraphs via Low-Rank Bilinear Optimization

to searching the span of the first d eigenvectors of A, and
collecting in a set Sd the top-k coordinates of all vectors
in that d-dimensional space. By modifying the Spannogram theory of (Karystinos & Liavas, 2010; Asteris et al.,
2014), we show how this set has size O(nd ) and can be
constructed in time O(nd+1 ). This will imply that DBkS
can be solved in time O(nd+1 ) on Ad .
Computational Complexity. The worst-case time complexity of the constant-rank DBkS solver on Ad is O(Td +
nd+1 ), where Td is the time to compute the first d eigenvectors of A. Under conditions satisfied by many real world
graphs, we show that we can modify our algorithm and obtain a randomized one that succeeds with probability Œ¥ and
is  far from the optimal rank-d solver, while its complexity reduces to nearly linear in the number
 of edges m of the
1
.
graph G: O m ¬∑ log n + nd ¬∑ log ¬∑Œ¥
Algorithm 2 lowrankDBkS(k, d, A)
1:
2:
3:
4:
1:
2:
3:

[Vd , Œõd ] = EVD(A, d)
Sd = Spannogram(k, Vd )
{Xd , Yd } = arg max|X |=k maxY‚ààSd 1TX Vd Œõd VdT 1Y
Output: {Xd , Yd }
Spannogram(k, Vd )
Sd = {topk (v) : v ‚àà span(v1 , . . . , vd )}
Output: Sd .

2.2. Approximation Guarantees
We approximate DBkS by finding a solution to the constant
rank problem
max max

|X |=k |Y|=k

1TX Ad 1Y .

We output a pair of vertex sets, Xd , Yd , which we refer to
as the rank-d optimal solution, that has density
optBd = (1/k) ¬∑ 1TXd A1Yd .
Our approximation guarantees measure how far optBd is
from optB , the optimal density for DBkS. Our bounds
capture a simple core idea: the loss in our approximation
comes due to solving the problem on Ad instead of solving
it on the full rank matrix A. This loss is quantified in the
next lemma. The detailed proofs of the following results
are in the supplemental material.
Lemma 2. For any matrix A: optBd ‚â• optB ‚àí 2 ¬∑ |Œªd+1 |,
where Œªi is the ith largest eigenvalue of A.
Using an appropriate pre-processing step and then running
Algorithm 2 as a subroutine on a sub-sampled and low-rank
version of A, we output a k-subgraph Zd that has density
optd . By essentially combining Lemmata 1 and 2 we obtain
the following bounds.

 d+2
Theorem 1. Algorithm 1 outputs in time O n Œ¥¬∑log n a
k-subgraph that has density
optd = den(Zd ) ‚â• 0.5 ¬∑ (1 ‚àí Œ¥) ¬∑ opt ‚àí 2 ¬∑ |Œªd+1 |,

with probability 1 ‚àí n1 , where Œªi is the ith largest, in magnitude, eigenvalue of the adjacency matrix of the graph.
If the graph is bipartite, or if the largest d eigenvalues of
the graph are positive, then our algorithm runs in time
O nd+1 + Td , and outputs a k-subgraph with density
optd ‚â• opt ‚àí 2 ¬∑ |Œªd+1 |, where Td is the time to compute the d leading eigenvectors of the adjacency matrix of
the graph.
Using bounds on eigenvalues of graphs, Theorem 1 translates to the following approximation guarantees.
Theorem 2. If the densest-k-subgraph contains
a constant
‚àö
fraction of all the edges, and k = Œò( E), then we can
2
approximate DkS within a factor of 2 + , in time nO(1/ ) .
If additionally the graph is bipartite, then we can approximate DkS within a factor of 1 + .
Remark 1. The above results are similar to the 1 +  ratio
of (Arora et al., 1995), which holds for graphs where the
densest-k-subgraph contains ‚Ñ¶(n2 ) edges.
Graph dependent bounds. For any given graph, after
running our constant rank solver on Ad , we can compute
an upper bound to the optimal density opt via bounds on
optB , since it is easy to see that optB ‚â• opt. Our graphdependent bound is the minimum of three upper bounds on
the unknown optimal density:
Lemma 3. The optimal density of DkS can be bounded as

	
opt ‚â§ min (1/k) ¬∑ 1TXd Ad 1Yd + |Œªd+1 |, k ‚àí 1, Œª1 .
In our experimental section, we plot the above upper
bounds, and show that for most tested graphs our algorithm
performs provably within 70% from the upper bound on the
optimal density. These are far stronger guarantees than the
best available a priori bounds.

3. The Spannogram Framework
In this section, we describe how our constant rank
solver operates by examining candidate vectors in a lowdimensional span of A.
Here, we work on a rank-d matrix Ad = v1 uT1 + . . . +
vd uTd where ui = Œªi vi , and we wish to solve:
max

|X |=|Y|=k


1TX v1 uT1 + . . . + vd uTd 1Y .

(1)

Observe that we can rewrite (1) in the following way
max

|X |=|Y|=k

1TX


v1 ¬∑

(uT1 1Y ) + . . .
| {z }
c1


= max

|Y|=k

max 1TX vY

|X |=k

+ vd ¬∑

(uTd 1Y )



| {z }
cd


,

(2)

Finding Dense Subgraphs via Low-Rank Bilinear Optimization

where vY = v1 ¬∑ c1 + . . . + vd ¬∑ cd is an n-dimensional
vector generated by the d-dimensional subspace spanned
by v1 , . . . , vd .

Let an auxiliary angle œÜ ‚àà Œ¶ = [0, œÄ) and let
h
i
sin œÜ
3
c = [ cc12 ] = cos
œÜ .

We will now make a key observation: for every fixed vector
vY in (2), the index set X that maximizes 1TX vY can be easily computed. It is not hard to see that for any P
fixed vector
vY , the k-subset X that maximizes 1TX vY = i‚ààX [vY ]i
corresponds the set of k largest signed coordinates of vY .
That is, the locally optimal k-set is topk (vY ).

Then, we re-express c1 ¬∑ v1 + c2 ¬∑ v2 in terms of œÜ as

We now wish to find all possible locally optimal sets X . If
we could possibly check all vectors vY , then we could find
all locally optimal index sets topk (vY ).
Let us denote as Sd the set of all k-subsets X that are the
optimal solutions of the inner maximization of (2) for any
vector v in the span of v1 , . . . , vd

v(œÜ) = sin œÜ ¬∑ v1 + cos œÜ ¬∑ v2 .
This means that we can rewrite the set S2 as:
S2 = {topk (¬±(v(œÜ)), œÜ ‚àà [0, œÄ)}.

Observe that each element of v(œÜ) is a continuous spectral curve in œÜ: [v(œÜ)]i = [v1 ]i sin(œÜ) + [v2 ]i cos(œÜ).
Consequently, the top/bottom-k supports of v(œÜ) (i.e.,
topk (¬±v(œÜ))) are themselves a function of œÜ. How can
we find all possible supports?
8

Sd = {topk ([v1 ¬∑ c1 + . . . + vd ¬∑ cd ]) : c1 , . . . , cd ‚àà R}.

6

Clearly, this set contains all possible locally optimal X sets
of the form topk (vY ). Therefore, we can rewrite DBkS on
Ad as
max max 1TX Ad 1Y .
(3)

2

Due to the above, the problem of solving DBkS on Ad
is equivalent to constructing the set of k-supports Sd , and
then finding the optimal solution in that set. How large can
Sd be and can we construct it in polynomial time? Initially one could expect that the set Sd could have size as
big as nk . Instead, we show that the set Sd will be tremendously smaller, as in (Karystinos & Liavas, 2010) and (Asteris et al., 2014).
Lemma 4. The set Sd has size at most O(nd ) and can be
built in time O(nd+1 ) using Algorithm 2.
3.1. Constructing the set Sd
We build up to the general rank-d algorithm by explaining
special cases that are easier to understand.
Rank-1 case. We start with the d = 1 case, where we
have S1 = {topk (c1 ¬∑ v1 ) : c1 ‚àà R}. It is not hard to see
that there are only two supports to include in S1 : topk (v1 )
and topk (‚àív1 ). These two sets can be constructed in time
in time O(n), via a partial sorting and selection algorithm
(Cormen et al., 2001). Hence, S1 has size 2 and can be
constructed in time O(n).
Rank-2 case. This is the first non-trivial d which exhibits
the details of the Spannogram algorithm.

[v (œÜ)] 1
[v (œÜ)] 2
[v (œÜ)] 3
[v (œÜ)] 4
[v (œÜ)] 5

4

0
‚àí2
‚àí4
‚àí6
‚àí8

|Y|=k X ‚ààSd

The above problem can now be solved in the following
way: for every set X ‚àà Sd find the locally optimal set Y
that maximizes 1TX Ad 1Y , that is, this will be topk (Ad 1X ).
Then, we simply need to test all such X , Y pairs on Ad and
keep the optimizer.

(4)

‚àí10
0

0.5

1

1.5
œÜ

2

2.5

3

Figure 1. A rank d = 2 spannogram for n = 5 and two random
vectors v1 , v2 . Observe that every two curves intersect in exactly
one point. These intersection points define intervals in which a
top-k set is invariant.

The Spannogram. In Fig. 1, we draw an example plot
of five curves [v(œÜ)]i , i = 1, . . . , 5, which we call a
spannogram. From the spannogram in Fig. 1, we can
see that the continuity of these sinusoidal curves implies
a ‚Äúlocal invariance‚Äù property of the top/bottom k supports
topk (¬±v(œÜ)), in a small neighborhood around a fixed œÜ.
So, when does a top/bottom-k support change? The index
sets topk (¬±v(œÜ)) change if and only if two curves cross,
i.e., when the ordering of two elements [v(œÜ)]i ,[v(œÜ)]j
changes.
Finding all supports: There are n curves and each pair in4
tersects at exactly one
 point in the Œ¶ domain . Therefore,

n
there are exactly 2 intersection
points. These n2 inter
section points define n2 + 1 intervals. Within an interval
the top/bottom k supports topk (¬±v(œÜ))
 remain the same.
Hence, it is now clear that |S2 | ‚â§ 2 n2 = O(n2 ).
A way to find all supports in S2 is to compute the v(œÜi,j )
vectors on the intersection points of two curves i, j, and
3

Observe that when we scan œÜ, the vectors c, ‚àíc express all
possible unit norm vectors on the circle.
4
Here we assume that the curves are in general position. This
can be always accomplished by infinitesimally perturbing the
curves as in (Papailiopoulos et al., 2013).

Finding Dense Subgraphs via Low-Rank Bilinear Optimization

then the supports in the two adjacent intervals of such intersection point. The v(œÜi,j ) vector on an intersection point
of two curves i and j can be easily computed by first solving a set of linear equations [v(œÜi,j )]i = [v(œÜi,j )]j ‚áí
(ei ‚àí ej )T [v1 v2 ]ci,j = 02√ó1 for the unknown vector
ci,j , where ei is the i-th column of the n √ó n identity matrix, i.e., ci,j = nullspace((ei ‚àí ej )T [v1 v2 ]). Then, we
compute v(œÜi,j ) = [v1 v2 ]ci,j . Further details on breaking ties in topk (v(œÜi,j )) can be found in the supplemental
material.

Computational cost: We have n2 intersection points,
where we calculate the top/bottom k supports for each
v(œÜi,j ). The top/bottom k elements of every v(œÜi,j ) can
be computed in time O(n) using a partial sorting and selection algorithm (Cormenet al., 2001). Since we perform
this routine a total of O( n2 ) times, the total complexity of
our rank-2 algorithm is O(n3 ).
General Rank-d case. The algorithm generalizes to arbitrary dimension d, as we show in the supplemental material; its pseudo-code is given as Algorithm 3.
Remark 2. Observe that the computation of each loop under line 2 of Algorithm 3 can be computed in parallel. This
will allow us to parallelize the Spannogram.
Algorithm 3 Spannogram(k,Vd )
1: Sd = ‚àÖ
2: for all (i1 , . . . , id ) ‚àà
{1, . . . , n}d and s ‚àà
{‚àí1, 1} do
Ô£´Ô£Æ
Ô£πÔ£∂
[(Vd ]i1 ,: ‚àí[Vd ]i2 ,:

3:

c = s ¬∑ nullspace Ô£≠Ô£∞

..
.

Ô£ªÔ£∏

[Vd ]i1 ,: ‚àí[Vd ]i ,:
d

4: v = VdT c
5: S = topk (v)
6: T = S ‚àí {i1 ,. . . , id }
d
7: for all k‚àí|T
subsets J of (i1 , . . . , id ) do
S|
8:
Sd = Sd (T ‚à™ J )
9: end for
10: end for
11: Output: Sd .

3.2. An approximate Sd in nearly-linear time
In our exact solver, we solve DBkS on Ad in time
O(nd+1 ). Surprisingly, when Ad has only positive eigenvalues, then we can tightly approximate DBkS on Ad in
nearly linear time.
Theorem 3. Let the d largest eigenvalues of the graph be
positive,
 and let the d-th,(d + 1)-st largest have constant
 d 
ratio:  ŒªŒªd+1
 ‚â• C. Then, we can output, with probability
2

1 ‚àí Œ¥, a k-subgraph with density
(1 ‚àí ) ¬∑ optd , in time

1
O m ¬∑ log n + nd ¬∑ log ¬∑Œ¥
.
The main idea is that instead of checking all O(nd ) possible k sets in Sd , we can approximately solve the
problem
1
by randomly sampling M = O ‚àíd ¬∑ log ¬∑Œ¥
vectors
in the span of v1 , . . . , vd . Our proof is based on the fact
that we can ‚Äúapproximate‚Äù the surface of the d-dimensional

sphere with M randomly sampled vectors from the span of
v1 , . . . , vd . This allows us to identify, probability 1 ‚àí Œ¥,
near-optimal candidates in Sd . The modified algorithm is
very simple and is given below; its analysis can be found in
the supplemental material.
Algorithm 4 Spannogram approx(k, Vd , Œõd )

1
1: for i = 1 : O ‚àíd ¬∑ log ¬∑Œ¥
do
1/2
2:
v = (Œõd ¬∑ Vd )T ¬∑ randn(d, 1)
3:
Sd = Sd ‚à™ topk (v) ‚à™ topk (‚àív)
4: end for
5: Output: Sd .

4. Scaling up
In this section, we present the two key scalability features
that allow us to scale up to graphs with billions of edges.
4.1. Vertex Sparsification
We introduce a very simple and efficient pre-processing
step for discarding vertices that are unlikely to appear in
a top k set in Sd . This step runs after we compute Ad and
Pd
uses the leverage score, `i = j=1 [Vd ]2i,j |Œªj |, of the i-th
vertex to decide whether we will discard it or not. We show
in the supplemental material, that by appropriately setting a
threshold, we can guarantee a provable bound on the error
introduced. In our experimental results, the above elimination is able to reduce n to approximately nÃÇ ‚âà 10 ¬∑ k for
a provably small additive error, even for data sets where
n = 108 .
4.2. MapReduce Implementation
A MapReduce implementation allows scaling out to a large
number of compute nodes that can work in parallel. The
reader can refer to (Meng & Mahoney, 2013; Bahmani
et al., 2012)) for a comprehensive treatment of the MapReduce paradigm. In short, the Hadoop/MapReduce infrastructure stores the input graph as a distributed file spread
across multiple machines; it provides a tuple streaming abstraction, where each map and reduce function receives
and emits tuples as (key, value) pairs. The role of the keys
is to ensure information aggregation: all the tuples with the
same key are processed by the same reducer.
For the spectral decomposition step of our scheme we
design a simple implementation of the power method in
MapReduce. The details are beyond the scope of this work;
high-performance implementations are already available in
the literature, e.g. (Lin & Schatz, 2010). We instead focus
on the novel implementation of the Spannogram.
Our MapReduce implementation of the rank-2 Spannogram is outlined in Algorithm 4. The Mapper is responsible
for the duplication and dissemination of the eigenvectors,
V2 , U2 = V2 Œõ2 , to all reducers. Line 3 emits the j-th
row of V2 and U2 once for every node i. Since i is used as
the key, this ensures that every reducer receives V2 , U2 in

Finding Dense Subgraphs via Low-Rank Bilinear Optimization

From the breakdown of the Spannogram in Section 3, it is
understood that, for the rank-2 case, it suffices to solve a
simple system of equations for every pair of nodes. The
Reducer for node i receives the full eigenvectors V2 , U2
and is responsible for solving the problem for every pair
(i, j), where j > i. Then, Line 6 emits the best candidate
computed at Reducer i. A trivial final step, not outlined
here, collects all n2 candidate sets and keeps the best one
as the final solution.
The basic outline in Algorithm 4 comes with heavy communication needs and was chosen here for ease of exposition. The more efficient version that we implement, does
not replicate V2 , U2 n times. Instead, the number of reducers ‚Äì say R = nŒ± ‚Äì is fine-tuned to the capabilities of
the cluster. The mappers emit V2 , U2 R times, once for
every reducer. Then, reducer r is responsible for solving
for node pairs (i, j), where i ‚â° r (mod R) and j > i. Depending on the performance bottleneck, different choices
for Œ± are more appropriate. We divide the construction of
the O(n2 ) candidate sets in S2 to O(nŒ± ) reducers and each
of them computes O(n2‚àíŒ± ) candidate subgraphs. The total communication cost for this parallelization scheme is
O(n1+Œ± ): nŒ± reducers need to have access to the entire
V2 , U2 that has 2 ¬∑ 2 ¬∑ n entries. Moreover, the total computation cost for each reducer is O(n3‚àíŒ± ).
Algorithm 5 SpannogramMR(V2 , U2 )
1:
2:
3:
4:
1:
2:
3:
4:

Map({[V2 ]j,: , [U2 ]j,: , j}):
for i = 1 : n do
emit: hi, {[V2 ]j,1 , [V2 ]j,2 , [U2 ]j,1 , [U2 ]j,2 , j}i
end for
Reducei (hi, {[V2 ]j,1 , [V2 ]j,2 , [U2 ]j,1 , [U2 ]j,2 , j}i , ‚àÄj):
for each j ‚â• i + 1 do
c = nullspace([V]i,: ‚àí [V]j,: )
[denj , {Xj , Yj }] =
max
1X V2 UT2 1Y
|Y|=k,X ‚ààtopk (¬±V2 c)

5: end for



6: emit: i, {Xi , Yi } = maxj 1Xj V2 UT2 1Yj

5. Experimental Evaluation
We experimentally evaluate the performance of our algorithm and compare it to the truncated power method
(TPower) of (Yuan & Zhang, 2011), a greedy algorithm by
(Feige et al., 2001) (GFeige) and another greedy algorithm
by (Ravi et al., 1994) (GRavi). We performed experiments
on synthetic dense subgraphs and also massive real graphs
from multiple sources. In all experiments we compare the
density of the subgraph obtained by the Spannogram to the
density of the output subgraphs given by the other algorithms.
Our experiments illustrate three key points: (1) for all
tested graphs, our method outperforms ‚Äì some times significantly ‚Äì all other algorithms compared; (2) our method is

Subgr aph dens ity

1000

800

!
‚àö "
G n, 12 , k = 3 n

Running times on 2.5 Billion Edges

G -Feige
G -Ravi
TPower
Spannogr am

Power method
Spannogr am
800

Number of cor es

their entirety.

600

400

400

200
240

0

4

10

6

10

8

10

10

10

|E |

(a) Densities of the recovered
subgraph v.s. the expected
number of edges.

0

100

200

300

400

500

600

Total time in minutes

(b) Running times of the
Spannogram and power iteration for two top eigenvectors.

Figure 2. Planted clique experiments for random graphs.

highly scalable, allowing us to solve far larger problem instances; (3) our data-dependent upper bound in many cases
provide a certificate of near-optimality, far more accurate
and useful, than what a priori bounds are able to do.
Planted clique. We first consider the so-called (and now
much studied) Planted Clique problem: we seek to find a
clique of size k that has been planted in a graph where all
other edges are drawn independently with probability 1/2.
We scale our randomized experiments from n = 100 up‚àöto
105 . In all cases we set the size of the clique to k = 3 ¬∑ n
‚Äì close to what is believed to be the critical computability threshold. In all our experiments, GRavi, TPower, and
the Spannogram successfully recovered the hidden clique.
However, as can be seen in Fig. 2, the Spannogram algorithm is the only one able to scale up to n = 105 ‚Äì a massive dense graph with about 2.5 billion edges. The reason is
that this graph does not fit in the main memory of one machine and caused all centralized algorithms to crash after
several hours. Our MapReduce implementation scales out
smoothly, since it splits the problem over multiple smaller
problems solved in parallel.
Specifically, we used Amazon Wireless Services‚Äô Elastic
MapReduce framework (aws). We implemented our map
and reduce functions in Python and used the MRJob class
(mrj). For our biggest experiments we used a 100-machine
strong cluster, consisting of m1.xlarge AWS instances (a
total of 800 cores).
The running times of our experiments over MapReduce are
shown in Fig. 2(b). The main bottleneck is the computation
of the first two eigenvectors which is performed by repeating the power iteration for few (typically 4) iterations. This
step is not the emphasis of this work and has not been optimized. The Spannogram algorithm is significantly faster
and the benefits of parallelization are clear since it is CPU
intensive.
In principle, the other algorithms could be also implemented over MapReduce, but that requires non-trivial dis-

Finding Dense Subgraphs via Low-Rank Bilinear Optimization

Figure 3. Subgraph density vs. subgraph size (k). We compare our DkS Spannogram algorithm with the algorithms from (Feige et al.,
2001) (GFeige), (Ravi et al., 1994) (GRavi), and (Yuan & Zhang, 2011) (tPM). Across all subgraph sizes k, we obtain higher subgraph
densities using Spannograms of rank d = 2 or 5. We also obtain a provable data-dependent upper bound (solid black line) on the
objective. This proves that for these data sets, our algorithm is typically within 80% from optimality, for all sizes up to k = 250, and
indeed for small subgraph sizes we find a clique which is clearly optimal. Further experiments on multiple other data sets are shown in
the supplemental material.

tributed algorithm design. As is well-known, e.g., (Meng
& Mahoney, 2013), implementing iterative machine learning algorithms over MapReduce can be a significant task
and schemes which perform worse in standard metrics can
be highly preferable for this parallel framework. Careful MapReduce algorithmic design is needed especially for
dense graphs like the one in the hidden clique problem.
Real Datasets. Next, we demonstrate our method‚Äôs performance in real datasets and also illustrate the power of
our data-dependent bounds. We run experiments on large
graphs from different applications and our findings are presented in Fig. 3. The figure compares the density achieved
by the Spannogram algorithm for rank 1, 2 and 5 to the
performance of GFeige, GRavi and TPower. The figure
shows that the rank-2 and rank-5 versions of our algorithm,
improve ‚Äì sometimes significantly ‚Äì over the other techniques. Our novel data-dependent upper-bound shows that
our results on these data sets are provably near-optimal.
The experiments are performed for two community graphs
(com-LiveJournal and com-DBLP), a web graph (webNotreDame), and a subset of the Facebook graph. A larger
set of experiments is included in the supplemental material.

Note that the largest graph in Figure 3 contains no more
than 35 million edges; these cases fit in the main memory
of a single machine and the running times are presented
in the supplemental material, all performed on a standard
Macbook Pro laptop using Matlab. In summary, rank-2
took less than one second for all these graphs while prior
work methods took approximately the same time, up to a
few seconds. Rank-1 was significantly faster than all other
methods in all tested graphs and took fractions of a second. Rank-5 took up to 1000 seconds for the largest graph
(LiveJournal).
We conclude that our algorithm is an efficient option for
finding dense subgraphs. Different rank choices give a
tradeoff between accuracy and performance while the parallel nature allows scalability when needed. Further, our
theoretical upper-bound can be useful for practitioners investigating dense structures in large graphs.

6. Acknowledgments
The authors would like to acknowledge support from NSF
grants CCF 1344364, CCF 1344179, DARPA XDATA, and
research gifts by Google, Docomo and Microsoft.

Finding Dense Subgraphs via Low-Rank Bilinear Optimization

References
Amazon Web Services, Elastic Map Reduce. URL http://
aws.amazon.com/elasticmapreduce/.
MRJob. URL http://pythonhosted.org/mrjob/.
Alon, Noga, Lee, Troy, Shraibman, Adi, and Vempala, Santosh.
The approximate rank of a matrix and its algorithmic applications: approximate rank. In Proceedings of the 45th annual
ACM symposium on Symposium on theory of computing, pp.
675‚Äì684. ACM, 2013.
Ames, Brendan PW. Convex relaxation for the planted clique,
biclique, and clustering problems. PhD thesis, University of
Waterloo, 2011.
Arora, Sanjeev, Karger, David, and Karpinski, Marek. Polynomial time approximation schemes for dense instances of nphard problems. In STOC, 1995.
Asahiro, Yuichi, Iwama, Kazuo, Tamaki, Hisao, and Tokuyama,
Takeshi. Greedily finding a dense subgraph. Journal of Algorithms, 34(2):203‚Äì221, 2000.
Asteris, Megasthenis, Papailiopoulos, Dimitris S, and Karystinos,
George N. The sparse principal component of a constant-rank
matrix. IEEE Trans. IT, 60(4):228‚Äì2290, 2014.
Bahmani, Bahman, Kumar, Ravi, and Vassilvitskii, Sergei. Densest subgraph in streaming and mapreduce. Proceedings of the
VLDB Endowment, 5(5):454‚Äì465, 2012.
Bhaskara, Aditya, Charikar, Moses, Chlamtac, Eden, Feige, Uriel,
and Vijayaraghavan, Aravindan. Detecting high log-densities:
an O(n1/4 ) approximation for densest k-subgraph. In STOC,
2010.
Boutsidis, Christos, Mahoney, Michael W, and Drineas, Petros.
An improved approximation algorithm for the column subset
selection problem. In Proceedings of the twentieth Annual
ACM-SIAM Symposium on Discrete Algorithms, pp. 968‚Äì977.
Society for Industrial and Applied Mathematics, 2009.
Cormen, Thomas H, Leiserson, Charles E, Rivest, Ronald L, and
Stein, Clifford. Introduction to algorithms. MIT press, 2001.
d‚ÄôAspremont, Alexandre et al. Weak recovery conditions using
graph partitioning bounds. 2010.
Dourisboure, Yon, Geraci, Filippo, and Pellegrini, Marco. Extraction and classification of dense communities in the web. In
WWW, 2007.
Feige, Uriel and Langberg, Michael. Approximation algorithms
for maximization problems arising in graph partitioning. Journal of Algorithms, 41(2):174‚Äì211, 2001.
Feige, Uriel, Peleg, David, and Kortsarz, Guy. The dense ksubgraph problem. Algorithmica, 29(3):410‚Äì421, 2001.

Gibson, David, Kumar, Ravi, and Tomkins, Andrew. Discovering
large dense subgraphs in massive graphs. In PVLDB, 2005.
Hu, Haiyan, Yan, Xifeng, Huang, Yu, Han, Jiawei, and Zhou,
Xianghong Jasmine. Mining coherent dense subgraphs across
massive biological networks for functional discovery. Bioinformatics, 21(suppl 1):i213‚Äìi221, 2005.
Jethava, Vinay, Martinsson, Anders, Bhattacharyya, Chiranjib,
and Dubhashi, Devdatt. The lovasz theta function, svms and
finding large dense subgraphs. In NIPS, 2012.
Karystinos, George N and Liavas, Athanasios P. Efficient computation of the binary vector that maximizes a rank-deficient
quadratic form. IEEE Trans. IT, 56(7):3581‚Äì3593, 2010.
Khot, Subhash. Ruling out ptas for graph min-bisection, densest
subgraph and bipartite clique. In FOCS, 2004.
Lin, Jimmy and Schatz, Michael. Design patterns for efficient
graph algorithms in mapreduce. In Proceedings of the Eighth
Workshop on Mining and Learning with Graphs, pp. 78‚Äì85.
ACM, 2010.
Mahoney, Michael W and Drineas, Petros. Cur matrix decompositions for improved data analysis. Proceedings of the National
Academy of Sciences, 106(3):697‚Äì702, 2009.
Meng, Xiangrui and Mahoney, Michael. Robust regression on
mapreduce. In Proceedings of The 30th International Conference on Machine Learning, pp. 888‚Äì896, 2013.
Miller, B, Bliss, N, and Wolfe, P. Subgraph detection using eigenvector l1 norms. In NIPS, 2010.
Papailiopoulos, Dimitris, Dimakis, Alexandros, and Korokythakis, Stavros. Sparse pca through low-rank approximations. In Proceedings of the 30th International Conference on
Machine Learning (ICML-13), pp. 747‚Äì755, 2013.
Ravi, Sekharipuram S, Rosenkrantz, Daniel J, and Tayi, Giri K.
Heuristic and special case algorithms for dispersion problems.
Operations Research, 42(2):299‚Äì310, 1994.
Saha, Barna, Hoch, Allison, Khuller, Samir, Raschid, Louiqa, and
Zhang, Xiao-Ning. Dense subgraphs with restrictions and applications to gene annotation graphs. In Research in Computational Molecular Biology, pp. 456‚Äì472. Springer, 2010.
Srivastav, Anand and Wolf, Katja. Finding dense subgraphs with
semidefinite programming. Springer, 1998.
Suzuki, Akiko and Tokuyama, Takeshi. Dense subgraph problems
with output-density conditions. In Algorithms and Computation, pp. 266‚Äì276. Springer, 2005.
Yuan, Xiao-Tong and Zhang, Tong. Truncated power method for
sparse eigenvalue problems. arXiv preprint arXiv:1112.2679,
2011.

