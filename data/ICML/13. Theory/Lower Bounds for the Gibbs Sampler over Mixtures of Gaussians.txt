Lower Bounds for the Gibbs Sampler
over Mixtures of Gaussians
Christopher Tosh
CTOSH @ CS . UCSD . EDU
Sanjoy Dasgupta
DASGUPTA @ CS . UCSD . EDU
Department of Computer Science and Engineering, UCSD, 9500 Gilman Drive, La Jolla, CA 92093-0404

Abstract
The mixing time of a Markov chain is the minimum time t necessary for the total variation
distance between the distribution of the Markov
chain’s current state Xt and its stationary distribution to fall below some ✏ > 0. In this paper,
we present lower bounds for the mixing time of
the Gibbs sampler over Gaussian mixture models
with Dirichlet priors.

1. Introduction
Inferring the parameters of a mixture model based on observed data is a classical problem in machine learning
that has received much attention from computer scientists
and statisticians alike. One of the first computational approaches to this problem was given by Dempster, Laird,
and Rubin (1977) in the form of the popular EM algorithm. The goal of their algorithm was to find the parameters which maximized the likelihood of the observed
data. While their algorithm is only guaranteed to converge to a local maximum (Wu, 1983), others have demonstrated efficient algorithms that recover the true parameters of mixtures of various distributions (Moitra & Valiant,
2010; Belkin & Sinha, 2010; Hsu & Kakade, 2013).
Alternatively, the Bayesian approach to the problem views
the parameters to be inferred not as fixed quantities, but
as random variables generated by some underlying distribution. By placing a prior distribution on the unknown
quantities and using Bayes’ rule to update the prior with
the likelihood of the observed data, a posterior distribution is obtained for the desired parameters. Since in many
cases of interest, the normalizing factor of this posterior
distribution is not available in closed form, the typical approach to inference is via Monte Carlo methods, specifically Markov chain Monte Carlo methods (MCMC) (GelProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

man et al., 1995). One of the most popular MCMC methods
is Gibbs sampling (Geman & Geman, 1984).
Gibbs sampling, often referred to as Glauber dynamics
(Levin et al., 2008) or alternating conditional sampling
(Gelman et al., 1995), is a generic MCMC method that
relies on knowing only the conditional marginal probabilities of the unknown parameters. More concretely, if
X1 , . . . , Xk are the random variables of interest and the
current state of the Gibbs sampler is (x1 , . . . , xk ), then the
Gibbs sampler takes a step by choosing an index i and updating the value of Xi according to P r(Xi = x | X i =
x i ), where the negative subscript refers to the vector of
values missing that index. There are two ways of selecting the indices to be updated. One can choose the indices
sequentially, the systematic scan method, or update them
randomly, the random scan method. It is an open question
by how much these two methods can differ in their rates of
convergence (Diaconis, 2013). In this paper, we will only
consider the random scan Gibbs sampler.
Regardless of whether the systematic or random scan
method is used, it is well known that in the limit, as the
number of updates goes to infinity, the distribution of the
state of the Gibbs sampler converges to the desired posterior distribution (Geman & Geman, 1984; Diebolt &
Robert, 1994). What is less understood is how long it takes
for the distribution of the Gibbs sampler’s state to be within
a reasonable distance of the posterior, the so-called mixing
time of the Markov chain (Levin et al., 2008), sometimes
referred to as the burn-in period (Brooks, 1998). The mixing rate of the Gibbs sampler can vary wildly depending
on the application, from nearly linear (Jerrum, 1995; Luby
& Vigoda, 1999) to exponential (Randall, 2006; Galvin &
Randall, 2007). Thus, to get meaningful bounds on the
mixing rate, we need to consider the specific application
we have in mind. Here, it is learning mixture models.
The Bayesian approach to specifying mixture models is
to provide a generative process by which the observable
quantities are created. We follow a widely used generative model seen, for example, in Neal (2000). For

Lower Bounds on Gibbs Sampling

a mixture model of k components with density function
P (x) = w1 P1 (x) + . . . + wk Pk (x), we assume that our
mixing weights (w1 , . . . , wk ) are drawn from a symmetric k-dimensional Dirichlet distribution with single parameter ↵ > 0. Each of the Pi is parameterized by ✓i 2 ⇥
which is drawn from a prior distribution with parameter
. We call this Q( ) The label zi for each data point
is drawn from the categorical distribution with parameter
w = (w1 , . . . , wk ) 2 k . Finally, the point xi is drawn
from the mixture component parameterized by ✓zi . We call
this P(✓zi ). This is summarized in (1).
(w1 , . . . , wk ) ⇠ Dirichlet(↵, . . . , ↵)
✓1 , . . . , ✓k ⇠ Q( )

zi ⇠ Categorical(w1 , . . . , wk )

(1)

xi ⇠ P(✓zi )

Diebolt and Robert (1994) showed that given the sequence
(x1 , . . . , xn ), the number of generating distributions k, the
prior distribution Q, and the likelihood distribution P, the
Gibbs sampler could be used to sample from the posterior
distribution P r(w, ✓, z | x). To do this, their Gibbs sampler
alternated between sampling the distributional parameters
w and ✓ and the labellings z. This is known as the naı̈ve
Gibbs sampler. Often it is sufficient to sample only the label variables z. In the case where the prior is conjugate
to the likelihood, it is possible to integrate out the distributional parameters w and ✓ and to only sample the labellings
z. This is called the collapsed Gibbs sampler.
Both of the above Gibbs samplers on mixture models suffer
from the issue of identifiability. That is, any permutation
of the label variable values zi identifies to exactly the same
underlying structure, but the Gibbs sampler still considers
them to be distinct. Thus, any grouping of points into k
clusters is represented in the space of labellings k! times,
which can lead to slow mixing. In the statistics literature,
this is sometimes referred to as the ‘label switching problem’ (Jasra et al., 2005). Since all the permutations contain
the same basic information, the particular labellings are not
important to us. Can something be said about how well the
Gibbs sampler performs when we only consider the structure underlying all permutations of the labellings?
In this paper, we show that the collapsed Gibbs sampler induces another Markov chain over the space of equivalence
classes of the labelling permutations. This space contains
a unique representative for each of the equivalence classes
and therefore does not suffer from the issue of identifiability. Further, the induced Markov chain has as its stationary
distribution the exact posterior distribution that we wish to
sample from. However, even with these advantages, the
induced Markov chain is not guaranteed to mix rapidly.
We provide lower bounds on the rate of convergence for
the Gibbs sampler over two instances. In both examples,

the clusters look Gaussian. In one of the cases the number
of Gaussians is misspecified, and we show that the Markov
chain is still very far from stationarity after exponentially
many steps. In the other case the number of Gaussians
is correctly specified, and we show that the Markov chain
must take at least n⌦(↵) steps before it approaches stationarity, where ↵ > 0 is a sparsity parameter of the prior distribution on the mixing weights.
1.1. High-Level Overview
We are particularly interested in mixtures of spherical
Gaussians with fixed variance 2 . To generate our
data point sequence x = (x1 , . . . , xn ), where each
xi 2 Rd , we follow the generative process described
in (1). Here, our prior distribution is a spherical Gaussian N (µ0 , 02 Id ). From this distribution, we draw means
µ1 , . . . , µk , which are the parameters of our likelihood distribution, N (·, 2 Id ). The process is summarized in (2).
(w1 , . . . , wk ) ⇠ Dirichlet(↵, . . . , ↵)
µ1 , . . . , µk ⇠ N (µ0 ,

2
0 Id )

zi ⇠ Categorical(w1 , . . . , wk )

(2)

xi ⇠ N (µzi , 2 Id )
The above process has several variables that need to be
specified: the number of clusters k, the sparsity of the
Dirichlet distribution ↵, the mean and variance of the Gaussian prior (µ0 , 02 ), and the variance of the mixing Gaussians 2 . This flexibility means that the above process can
be used to describe a wide variety of data sets.
Given a setting of these variables, the collapsed Gibbs
sampler is a random walk on the space of labellings
(z1 , . . . , zn ). At each step, it randomly chooses an index
i and sets zi to label j with probability P r(zi = j | z i , x)
where z i indicates the vector z minus the ith coordinate.
Starting with an initial configuration z (0) 2 {1, . . . , k}n ,
this process generates successive labellings z (0) , z (1) , . . .
that distributionally approach the probability distribution
P r(z | x). That is, if the distribution of the sample z (t) is
given by ⇡t , then lim ⇡t (·) = P r(· | x).
t!1

However, the above does not tell us about the rate at which
convergence occurs. For this, we need a notion of distance
for probability distributions. If ⌫ and µ are probability measures over a space ⌦, the total variation distance is
1X
|µ(!) ⌫(!)|.
kµ ⌫kT V := max|µ(A) ⌫(A)|=
A⇢⌦
2
!2⌦

If ⇡t is defined as above, then the mixing rate of the Gibbs
sampler is the minimum number of step ⌧mix to lower the
total variation distance between ⇡t and P r(· | x) below 1/4.
To establish lower bounds on the mixing rate of the Gibbs
sampler, it is sufficient to establish the result for any family

Lower Bounds on Gibbs Sampling

of point sets. However, such results are particularly interesting when the data points really do approximately conform to the model. As an approximation to a Gaussian, we
will consider a cluster of radius r centered about x 2 Rd
to be a collection of points within distance r of their mean,
x. We consider two cases of point sets and Gibbs samplers
and provide lower bounds on the mixing rate of both.
In the first case, we consider a collection of 6 identical
clusters of n points each lying in d-dimensional space with
d 3. The clusters are positioned such that no two cluster
means are within distance r of each other and the diameter
of each cluster is bounded above by r. However, when we
specify the number of Gaussians for the Gibbs sampler, we
use 3 instead of 6. In Section 6.1, we prove the following.
Theorem 6.1 For a proper setting of , the mixing time of
the induced Gibbs sampler with a misspecified number of
mixtures is bounded as ⌧mix (1/24) exp(r2 /8 2 ).
It is worth mentioning that the ratio r/ can be arbitrarily large. In fact, when the points comprising the individual clusters are drawn from a Gaussian with variance
2
, a larger value for r/ actually corresponds to a more
well-separated instance, which should be easier to learn,
not harder. It is interesting that the Gibbs sampler should
perform worse as the problem instance gets more tractable.
In the second case, where we correctly specify the number
of clusters, we consider a collection of 3 identical clusters
of n points each lying in d-dimensional space with d 2.
The means of these clusters lie at a distance r from each
other and the diameter of each cluster is bounded above by
r. In Section 6.2, we prove the following lower bound.
Theorem 6.2 For a proper setting of and ↵, the mixing
time of the induced Gibbs sampler with a correctly specified number of mixtures is bounded below as
0
⇣ ⌘d
⇣
⌘1
↵ ↵2
⇣ 2 ⌘ n↵ d/2
exp
r
n
1
0
B1
⇣ ⌘ C
min @ e 96 2 ,
⌧mix
A.
r2
8
6
3(↵
1/2)
2
(↵) exp 2
0

If the means are drawn from a d-dimensional normal distribution with variance 02 , then with high probability r2 ⇡
d 02 . In this case, the dominant term in Theorem 6.2
is the minimum of exp(r2 /96 2 ) ⇡ exp(d 02 / 2 ) and
d
n↵ d/2 ( / 0 ) . Additionally, to get clusters that are even,
or nearly even, in cardinality like the ones we consider, the
parameter ↵ must be relatively large. Thus, even though
we are taking the minimum of two quantities, we can still
expect the lower bound to be fairly large for reasonable n.
The key idea in the proofs of both of these lower bounds
is that there exists a subset of states of the Gibbs sampler
that has relatively low probability mass but is difficult for
the Gibbs sampler to transition out of. In Section 7 we

experimentally verify that this is the case by showing that
most runs of the Gibbs sampler spend the majority of their
time in local optima with relatively low probability mass.

2. Preliminaries
In this section, we give a more general review of the theory
of Markov chains. A Markov chain is a sequence of random
variables X0 , X1 , . . . taking values in a state space ⌦ s.t.
P r(Xt = x | Xt

1 , . . . , X0 )

= P r(Xt = x | Xt

1)

for all x 2 ⌦ and t 1. We can view the transition probability as a matrix P indexed by elements of ⌦ s.t.
P (x, y) = P r(Xt = y | Xt

1

= x).

Note that if X0 is some fixed state x 2 ⌦, the distribution
of Xt is P t (x, ·). A Markov chain is said to be irreducible
or strongly connected if, for all x, y 2 ⌦, there exists a
t > 0 s.t. P t (x, y) > 0. It is aperiodic if for all x, y 2 ⌦,
gcd({t : P t (x, y) > 0}) = 1.
A distribution ⇡ over ⌦ is said to be a stationary distribution of P if, when ⇡ and P are viewed as matrices indexed
by ⌦, then ⇡ is a left eigenvector of P with corresponding
eigenvalue 1 or, equivalently, ⇡ = ⇡P . The natural interpretation of this is that if the current state of the Markov
chain is distributed according to ⇡, then the next state will
also be distributed according to ⇡. A sufficient, but not
necessary, condition for a distribution to be stationary with
respect to a Markov chain is reversability. A Markov chain
P is reversible with respect to a distribution ⇡ if for all
x, y 2 ⌦, we have ⇡(x)P (x, y) = ⇡(y)P (y, x).
How do we measure the ‘closeness’ of two distributions?
One measure is the total variation distance. The total variation distance between two distributions µ, ⌫ over ⌦ is
1X
|µ(!) ⌫(!)|.
kµ ⌫kT V := max|µ(A) ⌫(A)|=
A⇢⌦
2
!2⌦

For ✏ > 0, the mixing time of a Markov chain P with
unique stationary distribution ⇡ is
⌧ (✏) = min{t : maxkP t (x, ·)
x2⌦

⇡kT V < ✏}.

Taking ✏ to be any constant less than 1/2 gives us nontrivial
results, but by convention ✏ is often taken to be 1/4. Thus,
where it will cause no confusion, we refer to the mixing
time interchangeably with the quantity ⌧mix := ⌧ (1/4).
Given a Markov chain P , its stationary distribution ⇡, and
a subset S ⇢ ⌦, the conductance of S is
X
1
(S) :=
⇡(x)P (x, y)
⇡(S)
c
x2S,y2S

Lower Bounds on Gibbs Sampling

and the conductance of P , ⇤ , is the minimum conductance
of any set S with ⇡(S)  1/2. The following theorem
relates the mixing time and conductance of a Markov chain.
Theorem 2.1 (Sinclair (1988)). Let P be an aperiodic, irreducible, and reversible Markov chain with conductance
1
⇤
and mixing time ⌧mix . Then, ⌧mix
4 ⇤.

3. Mixture Models and Gibbs Sampling
In this paper, we are concerned with Bayesian inference of
the parameters of a mixture distribution. Generally speaking, a mixture distribution is specified by k probability density functions P1 , . . . , Pk : ⌦ ! R and k corresponding
weights w1 , . . . , wk s.t. w1 + · · · + wk = 1 and has probability density function P (x) = w1 P1 (x) + . . . + wk Pk (x).
We are particularly interested in a certain class of mixture
models: finite mixtures of exponential families of distributions. For this section and the next, we will work in this
generality before addressing Gaussian mixture models.
3.1. Generative Model

Suppose that we produce a sequence x = (x1 , . . . , xn )
from the above generative process. Then for any z =
(z1 , . . . , zn ) 2 {1, . . . , k}n , ✓ = (✓1 , . . . , ✓k ) 2 ⇥k , and
w = (w1 , . . . , wk ) 2 k , we have the joint distribution
k
n
Y
(k↵) Y ↵ 1
P r(x, z, ✓, w) =
w
Q
(✓
)
(wzi P✓zi (xi )).
j
j
(↵)k j=1
i=1
We denote by Cj (z) the set of indices i for which zi = j
and by n(z) the vector whose jth element is |Cj (z)|. Here
we think about Cj (z) as being the jth ‘cluster.’
For a subset S of {1, . . . , n}, we let P✓ (S) denote the probability of S under the specific model ✓ 2 ⇥:
Y
P✓ (S) :=
P✓ (xi )
i2S

We now summarize the generative model we consider in
this paper, which can be seen, for example, in Neal (2000).
For a mixture model of k components, we assume that our
mixing weights (w1 , . . . , wk ) are drawn from a symmetric
k-dimensional Dirichlet distribution with single parameter
↵ > 0. This is a distribution over the k-simplex,
(
)
k
X
(w1 , . . . , wk ) 2 Rk
wi = 1 ,
k =
and has probability density function

i=1
k

D↵ (w1 , · · · , wk ) =

Algorithm 1 The collapsed Gibbs sampler, P .
Initialize z1 , . . . , zn 2 {1, . . . , k}
while true do
Choose i u.a.r. from {1, . . . , n}
Update zi according to P r(zi = j | z i , x1 , . . . , xn )
end while

(k↵) Y ↵
w
(↵)k i=1 i

1

.

Here, (·) is the gamma function. Each of the parameters, ✓i 2 ⇥, of the mixing distributions is drawn from
the same prior distribution parameterized by some vector
2 Rs . Call this Q( ) and its probability density function Q : ⇥ ! R. The label zi for each data point
is drawn from the categorical distribution with parameter
w = (w1 , . . . , wk ). The k-dimensional categorical distribution is defined over the discrete set {1, . . . , k} and has
probability mass function
Cw (i) = wi for i 2 {1, . . . , k}.

Finally, the point xi is drawn from the distribution parameterized by ✓zi . Call this P(✓zi ) and its probability density
function P✓zi . This can be summarized as the following.
(w1 , . . . , wk ) ⇠ Dirichlet(↵, . . . , ↵)
✓1 , . . . , ✓k ⇠ Q( )

zi ⇠ Categorical(w1 , . . . , wk )

xi ⇠ P(✓zi )

(1)

and let q(S) denote the probability of S given ✓ ⇠ Q( ):
Z
q(S) :=
Q (✓)P✓ (S) d✓.
⇥

In many contexts, we are interested in the probability of a
labelling z given a data sequence x. By Bayes’ theorem,
◆
k ✓
Y
(nj (z) + ↵)
P r(z|x) /
q(Cj (z)) .
(3)
(↵)
j=1
Denote P r(z|x) by ⇡(z). Even when q is computable in
closed form, there are no known exact methods for computing the normalizing factor of ⇡. However, it is often enough
to approximately sample from ⇡ with the Gibbs sampler.
3.2. The Collapsed Gibbs Sampler
The traditional collapsed Gibbs sampler is shown in Algorithm 1. The following lemma establishes that Algorithm 1
does indeed converge to the desired distribution.
Lemma 3.1. Let P denote the collapsed Gibbs sampler, ⇡
denote the conditional probability distribution in (3), and
assume that P(✓) > 0 everwhere. Then P is irreducible,
aperiodic, and reversible with respect to ⇡. In particular, ⇡
is the unique stationary distribution of P .
We still need to compute P r(zi = j | z i , x). Let S be a
subset of indices and i be an index. Then we define
q(S [ {i})
.
q(S \ {i})
With this notation, we can prove the following lemma.
(S, i) :=

Lower Bounds on Gibbs Sampling

Algorithm 2 The projected Gibbs sampler, P [ .
Initialize a clustering C 2 ⌦k (x)
while true do
Choose i u.a.r. from {1, . . . , n}
Move i to S 2 C with probability proportional to (↵+
|S \ {i}|) (S, i)
Move i to own set with probability proportional to
(k |C|) · ↵ · q({i})1
end while
Lemma 3.2. P r(zi = j | z i , x) is proportional to (↵ +
nj (z i )) (Cj (z), i).

4. Markov Chains and Equivalence Classes
Identifiability makes it difficult to analyze the mixing time
of P . If is a permutation over {1, . . . , k}, then z and
(z) = ( (z1 ), . . . , (zn )) hold the same information for
us. We are interested in the clustering of the points, not
the specific number assigned to each cluster. However, P
views z and (z) as separate states. Thus, mixing results
proved over the labelling space may not hold true for the
space we care about. We will now see how to factor out
this extraneous information by a suitable projection.
4.1. Equivalence Classes of Markov Chains
Consider the following setting: we have a state space ⌦
and an equivalence relation ⇠ on ⌦. Let (X1 , X2 , . . .) be
a Markov chain and consider the sequence over the equivalence classes ([X1 ], [X2 ], . . .). Under what conditions is
this a Markov chain? The following lemma (Levin et al.,
2008) answers this question.
Lemma 4.1 (Levin, Peres, Wilmer - Lemma 2.5). Let
(X1 , X2 , . . .) be a Markov chain with state space ⌦ and
transition matrix P and let ⇠ be an equivalence relation
over ⌦ with equivalence classes ⌦] = {[x] : x 2 ⌦}.
0
0
Assume P satisfies P
P(x, [y]) = P (x , [y]) for all x ⇠ x ,
where P (x, [y]) := y0 ⇠y P (x, y). Then ([X1 ], [X2 ], . . .)
is a Markov chain with state space ⌦] and transition function P ] ([x], [y]) = P (x, [y]).
What is the form of the stationary distribution for P ] ?
Lemma 4.2. Let P , P ] , ⌦, ⌦] , and ⇠ be as in Lemma 4.1.
If P is reversible with respect to ⇡,P
then P ] is reversible
]
with respect to ⇡ ([x]) = ⇡([x]) := x0 ⇠x ⇡(x).
4.2. Induced Clusterings

Let x be a point sequence and consider the equivalence re1

This is ambiguous if i is already its own cluster. In this case,
the probability we keep i as its own set is proportional to (k
|C|+1) · ↵ · q({i}).

lation ⇠ over labellings such that z ⇠ z 0 if there exists a
permutation s.t. (z) = (z 0 ). Let P denote the Gibbs
sampler from Algorithm 1. What does the corresponding
Markov chain over the equivalence classes, P ] , look like?
Given a labelling z, we know from (3) that z 0 , z 00 2 [z]
have the same probability mass under ⇡. Thus, one way
to describe P ] is that if the current state is [z], it chooses
any labelling z 0 2 [z], moves to a neighboring labelling z 00
according to P , and sets the new state to be [z 00 ].
While this is a concise way of describing P ] , it offers little
intuition on what the state space looks like. An alternative
view is to consider the following notion of clustering.
Given an index set S, a t-partition or t-clustering of S, is a
set of t nonempty, disjoint subsets whose union is S. Now
let S = {1, · · · , n} and define ⌦t (x) to be the set of all
t-partitions of S and ⌦k (x) = [kt=1 ⌦t (x).
Lemma 4.3. The state space ⌦k (x) is isomorphic to the
set of equivalence classes induced by ⇠ over {1, . . . , k}n ,
⌦] . Furthermore, the P [ specified in Algorithm 2 is the
induced Markov chain of P on ⌦k (x), P ] . Finally, P [ is
reversible with respect to
⇡ [ (C) /

Y
1
(k |C|)!

S2C

(|S|+↵)
q(S).
(↵)

The 1/(k |C|)! term appears because C has k! /(k |C|)!
counterparts in the labelling space. The upshot of Lemma
4.3 is that P ] and P [ are the same Markov chain.

5. Mixtures of Gaussians
In this paper, we are particularly interested in mixtures of
d-dimensional spherical Gaussians with known variance 2
and conjugate prior. One convenient conjugate prior of
such a distribution is itself a d-dimensional spherical Gaussian. We will consider the following generative process.
(w1 , . . . , wk ) ⇠ Dirichlet(↵, . . . , ↵)
µ1 , . . . , µk ⇠ N (µ0 ,

2
0 Id )

zi ⇠ Categorical(w1 , . . . , wk )

xi ⇠ N (µzi ,

2

(2)

Id )

The following lemma seen, for example, in (Murphy, 2012)
establishes the conjugacy of the prior and posterior in (2)
and gives an explicit form for the posterior.
Lemma 5.1 ((Murphy, 2012)). Suppose P(✓) is a family of spherical Gaussians with fixed variance 2 and
mean ✓, and our prior on ✓ is another spherical Gaussian with mean µ0 and variance 02 . If we observe data
y = (y1 , . . . , yn ) and let S = {1, . . . , n}, then our posterior is also a spherical Gaussian with mean µS and variance S2 where

Lower Bounds on Gibbs Sampling
2

µS = µ0 ·
2
S

=

2
0

2

+
2

·

2

+
P

2
0 |S|

+ µ(S) ·

2
0 |S|
2 + 2 |S|
0

2
0 |S|

1
where µ(S) = |S|
i2S yi is the mean of y. Note that S
2
2
only depends on the cardinality of S. Further,
h if 2 0 2 i ,
2
the second equality immediate implies S 2 |S|+1 , |S| .

Recall that for a set of indices S, q(S) is the expected probability of S under ✓ ⇠ Q( ). In the case of Gaussians, we
can work out q in closed form.
Lemma 5.2. Let 2 , µ0 , 02 , Q , P✓ , x be as given above.
Then for any set of indices S ⇢ {1, . . . , n}, we have
q(S) = L(S)R(S) where L(S) is the probability assigned
to S by the max-likelihood model,
!
✓
◆|S|d/2
1
1 X
L(S) =
exp
kxi µ(S)k2 ,
2⇡ 2
2 2

ratio of the intercluster distances and the variance. It is
worth noting that the larger this ratio is, the more wellseparated the clusters are.
The second case is the more natural case where the number
of Gaussians is correctly specified. We show the mixing
time of the Gibbs sampler in this case is lower bounded by
the minimum of two quantities, an exponential term much
like the first case and a term of the form n⌦(↵) where ↵ is
the sparsity parameter of the Dirichlet prior.
6.1. Misspecified Number of Clusters
The sequence of points we consider corresponds to 6 spherical clusters, T1 , . . . , T6 , of n points each with diameter
r whose means are located at the vertices of a triangular
prism whose edge lengths are identically r. Figure 1 displays our point configuration XM when we project to R3 .

i2S

and R(S) penalizes how far µ(S) is from µ0 :
✓
◆d/2
✓
◆
2
|S|kµ0 µ(S)k2
R(S) =
exp
.
2 + |S| 2
2( 2 + |S| 02 )
0
The above derivation also gives us a nice expression for
(·, ·), which is one of the factors in the transition probabilities from Lemma 3.2.
Lemma 5.3. Let x be as above and let S ⇢ {1, . . . , n} and
i 2 {1, . . . , n} \ S, then
✓
◆d/2
✓
◆
1
1 kxi µS k2
(S, i) =
exp
·
.
2+ 2
2⇡( 2 + S2 )
2
S
In the Bayesian setting, we typically set 02 to be large, allowing flexibility in the placement of means. To enforce
this, we will require that 0
. Additionally, µ0 is typically set to be the origin. This simplifies the form of µS :
2
|S|
µS = µ(S) · 2 0 2 .
+ 0 |S|
If the size of S is variable, then with an appropriate choice
of |S|, the leading term of µS can be made arbitrarily close
to the origin. To simplify things, however, we will only
consider the case where µ0 is the origin.

6. Mixing Rates
We analyze the mixing time of Algorithm 2 for two cases.
In the first case, the number of Gaussians is misspecified.
Even though we cannot expect the Gibbs sampler to recover the correct Gaussians in this case, it still makes sense
to consider the samples generated by the Markov chain and
evaluate how quickly these approach the stationary distribution. The lower bound we achieve is exponential in the

Figure 1. The sequence of points XM projected to R3 .

We let Sk denote the indices of the points in cluster Tk and
let our state space be ⌦ = ⌦3 (XM ). Then we have the
following result for the Gibbs sampler P over ⌦.
Theorem 6.1. Let 0 <  1/32, ↵ > 0, 0 <  0 , and
k = 3. Then there is a constant n0 = ⌦(max{↵, 2 , d})
s.t. for n n0 the mixing rate of the induced Gibbs sampler P with parameters ↵, , 0 , and k over ⌦ is bounded
below as ⌧mix

1
24

r2

· e8 2 .

Let A = S3 [ . . . [ S6 . Then we bound the conductance
of the singleton set V whose only element is the partition
C = {S1 , S2 , A}. Because of the symmetric nature of ⌦,
we have that ⇡(V )  1/2.
Note two properties of C. First, the number of points in
each cluster of C is within a constant fraction of any other
cluster of C. Second, all the points in a cluster of C are
closer to that cluster’s mean than to any other cluster’s
mean by a constant fraction.

To bound the conductance of V , we will bound the probability that we transition out of V . This can happen in one of
three ways: we can move an index in A to one of S1 or S2 ,
we can move an index in S1 or S2 to A, or we can move an
index between S1 and S2 .
Recalling the transition probabilities from Algorithm 2 and

Lower Bounds on Gibbs Sampling

the form of (·, ·) from Lemma 5.3, we can see the likelihood of moving a point i in a cluster S in C to another
cluster T in C is roughly of the form
P r(move i to T ) = P

T 0 2C

(↵ + |T |) (T, i)
(↵ + |T 0 \ {i}|) (T 0 , i)

(↵ + |T |) (T, i)
(↵ + |S \ {i}|) (S, i)
!d/2
✓
◆
2
2
+ S\{i}
↵ + |T |
⇡
2+ 2
↵ + |S \ {i}|
T
✓
◆
2
kxi µ(S)k
kxi µ(T )k2
exp
.
2
2


Note that since the sizes of S and T are within a constant
fraction of each other, we have by Lemma 5.1 that the first
two terms in the last line approach constants as the number
of points grows. Since all the points are closer to their own
cluster’s mean than to any other cluster’s mean by a constant fraction, the last term in the above is exponential in
r2 / 2 . Theorem 6.1 follows by applying Theorem 2.1.
The details of this proof are left to the Appendix.
6.2. Correctly Specified Number of Clusters
The sequence of points we consider corresponds to 3 spherical clusters, T1 , T2 , and T3 , of n points each with diameter
r whose means are located at the vertices of an equilateral triangle of edge length r and centered about the origin.
Figure 2(a) displays our point configuration XG in R2 .

such that S1 and S2 are clustered together and their cluster contains no indices from S3 . A typical element of V is
shown in Figure 2(b). Because of the symmetric nature of
⌦, we know ⇡(V )  1/2. Thus we can use the conductance of V to bound the mixing time.
Ideally, we would like to proceed in the same manner as
Section 6.1. However, there is a special case to consider. V
contains a special clustering where the number of clusters
is 2: C := {S1 [ S2 , S3 }. The probability of transitioning
from C to a clustering in V c cannot be bounded from above
in the same manner as before since we can choose a point in
S1 [ S2 and make it a singleton cluster with relatively high
probability. Thus, to analyze (V ), we will consider V as
the disjoint union of two sets A = {C} and B = V \ A.
Then by the definition of conductance,
(V ) 

⇡(A)
1
+
⇡(V ) ⇡(V )

6.2.1. T HE T WO C LUSTER C ASE
Our goal is to show A has small probability mass in comparison with the rest of V , giving us the following.
23(↵
⇡(A)

⇡(V )

(b) Typical clustering in V .

Figure 2. The setup for Section 6.2.

Letting ⌦ = ⌦3 (XG ) be our state space, we have the
following result about the mixing time of P over ⌦.
⇣q
⌘
7
3
Theorem 6.2. For < 14
1, 0 <  0 ,
3
2 ,↵

and k = 3, there exists n0 = ⌦(max{↵, 2 , d}) s.t.
n
n0 implies that the mixing rate of the induced Gibbs
sampler P with parameters ↵, , 0 , and k over ⌦ is
bounded below as
0
⇣ ⌘d
⇣
⌘1
↵ ↵2
⇣ 2 ⌘ n↵ d/2
exp
r
n
1
0
B1
⇣ ⌘ C
⌧mix
min @ e 96 2 ,
A.
2
8
6
23(↵ 1/2) (↵) exp r 2
0

To establish this result, we consider the partitions V ⇢ ⌦

(4)

⇡(x)P (x, y).

x2B,y2V c

Thus, it will be sufficient to consider bounding the two
right-hand side terms separately. Our approach to the first
term, described in Section 6.2.1, will be to bound the relative probability mass of A under ⇡ against the entire set
V . Our approach to the second term, described in Section
6.2.2, is similar to our approach in Section 6.1: we bound
the probability of transitioning from B to V c .

Lemma 6.3. For n

(a) XG projected to R2 .

X

2 and ↵
1/2)

1,
⇣ 2
(↵) exp ↵ n ↵ +
d n↵ d/2

r2

2
0

⌘

d
0

.

Unfortunately, it is possible to partition S3 into clusters C1
and C2 such that the quantity q(C1 )q(C2 ) is smaller than
q(S3 ). How much smaller this quantity can be is controlled
by the following lemma.
Lemma 6.4. Let {C1 , C2 } be a 2-partition of S3 and suppose n 2 and ↵ 1, then
✓ 2 ◆d/2
✓ 2◆
r
q(C1 )q(C2 )
exp
q(S3 ).
2
n 02
0
The proofs of these lemmas are left to the Appendix.
6.2.2. T HE T HREE C LUSTER C ASE
Bounding the probability that we move from B to V c is
done in the exact same way as the proof of Theorem 6.1.
We prove the following lemma in the Appendix.
⇣q
⌘
7
3
Lemma 6.5. For  14
3
2 , there exists an n0 =
⌦(max{↵,

2

, d}) s.t. for n

n0 ,

Lower Bounds on Gibbs Sampling

(a) d = 10, ↵ = 0.5,

0

(b) d = 3, ↵ = 1.5,

= 0.5

0

= 10.0

(c) d = 3, ↵ = 1.5,

0

= 1000.0

Figure 3. In all the above graphs, the dashed line represents the log-proportional probability of the generating clustering.

1
⇡(V )

X

C2B,C0 2V c

⇡(C)P (C, C0 )  6 exp

✓

r2
96 2

◆

.

To complete the proof of Theorem 6.2, we simply use (4)
with Lemmas 6.3 and 6.5.

7. Experimental Results
For each experiment, we generated the point sequence by
taking k = 10 draws from a d-dimensional spherical Gaussian N (0, 02 Id ) to get means µ1 , . . . , µ10 . For each mean
µi , we took n = 50 draws from N (µi , 2 Id ) with = 0.5.

Recalling Algorithm 2, the Gibbs sampler requires parameters k, ↵, 2 , 02 and an initial clustering. For each set of
experiments, we used the same k, 2 , and 02 that generated
the point sequence over which the sampler was run. We
then fixed an ↵ and performed 10 separate runs with different initial clusterings of the points. To generate our initial
configurations, we randomly chose k centers and clustered
the points together that were closest to a particular center.
Each run of the Gibbs sampler was done for 1,000,000
steps, and we plotted at each step the log of the relative
probability of the current state C.
In the experiments of Figure 3, we can see the importance
of the ratio 02 / 2 . Figures 3(b) and 3(c) demonstrate that
when all else is held constant, a higher value for 02 / 2 will
result in slower convergence times. Additionally, Figure
3(a) shows us that when ↵ and 02 / 2 are small, the Gibbs
sampler will converge to a high probability state.

Figure 4. Above, d = 10, ↵ = 1.0,
↵ = 0.5, 0 = 5.0

0

= 5.0. Below, d = 10,

rent clustering of the Gibbs sampler to the generating clustering.
Acknowledgements
The authors are grateful to the National Science Foundation
for support under grant IIS-1162581 and the Graduate Research Fellowship Program under grant DGE-1144086. We
are also appreciative of the feedback given by the anonymous reviewers.

In the experiments of Figure 4, we can see the importance
of ↵. There are many more phase changes when the value
of ↵ is lower. This is possibly due to the observation in
Lemma 6.3 that the relative probability mass of an empty
clustering is larger when ↵ is smaller. This makes it possible for the Gibbs sampler to create empty clusters more
often and thus to make more phase transitions.
Finally, Figure 5 gives us an idea of what these phase transitions look like. The confusion matrices compares the cur-

Figure 5. The confusion matrices of one of the runs from Figure
4 before and after a phase transition.

Lower Bounds on Gibbs Sampling

References
Belkin, Mikhail and Sinha, Kaushik. Polynomial learning
of distribution families. In FOCS 2010: Proceedings
of the 51st Annual IEEE Symposium on Foundations of
Computer Science, pp. 103–112, 2010.
Brooks, Stephen P. Markov chain sampling methods for
Dirichlet process mixture models. Journal of the Royal
Statistical Society. Series D (The Statistician), 47(1):69–
100, 1998.
Dempster, A.P., Laird, N. M., and Rubin, D. B. Maximumlikelihood from incomplete data via the EM algorithm.
Journal of Royal Statist. Soc. Ser. B, 39:1–38, 1977.
Diaconis, Persi. Some things we’ve learned (about Markov
chain Monte Carlo). Bernoulli, 19(4):1294–1305, 2013.
Diebolt, Jean and Robert, Christian P. Estimation of finite
mixture distributions through Bayesian sampling. Journal of the Royal Statistical Society. Series B (Methodological), 56(2):363–375, 1994.
Galvin, David and Randall, Dana. Torpid mixing of local
Markov chains on 3-colorings of the discrete torus. In
Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 376–384, 2007.
Gelman, Andrew, Robert, Christian, Chopin, Nicolas, and
Rousseau, Judith. Bayesian data analysis, 1995.
Geman, Stuart and Geman, Donald. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of images. In IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 721–741, 1984.
Hsu, Daniel and Kakade, Sham M. Learning mixtures of
spherical Gaussians: Moment methods and spectral decompositions. In Proceedings of the 4th Conference on
Innovations in Theoretical Computer Science, ITCS ’13,
pp. 11–20, 2013.
Jasra, A., Holmes, C. C., and Stephens, D. A. Markov chain
Monte Carlo methods and the label switching problem in
Bayesian mixture modeling. Statist. Sci., 20(1):50–67,
2005.
Jerrum, Mark. A very simple algorithm for estimating the
number of k-colorings of a low-degree graph. Random
Struct. Alg., 7(2):157–165, 1995.
Levin, David A., Peres, Yuval, and Wilmer, Elizabeth L.
Markov Chains and Mixing Times. American Mathematical Society, 2008.
Luby, Michael and Vigoda, Eric. Fast convergence of the
Glauber dynamics for sampling independent sets. Random Struct. Alg., 15:229–241, 1999.

Moitra, Ankur and Valiant, Gregory. Settling the polynomial learnability of mixtures of gaussians. In FOCS
2010: Proceedings of the 51st Annual IEEE Symposium
on Foundations of Computer Science, pp. 93–102, 2010.
Murphy, Kevin P. Machine learning: a probabilistic perspective. Cambridge, MA, 2012.
Neal, Radford M. Markov chain sampling methods for
Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9(2):249–265, 2000.
Randall, Dana. Slow mixing of Glauber dynamics via topological obstructions. In Proceedings of the 17th Symposium on Discrete Algorithms (SODA), pp. 870–879,
2006.
Wu, C. F. Jeff. On the convergence properties of the EM
algorithm. The Annals of Statistics, 11(1):95–103, 1983.

