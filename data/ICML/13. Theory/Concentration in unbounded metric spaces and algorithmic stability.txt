Concentration in unbounded metric spaces and algorithmic stability

Aryeh Kontorovich
Department of Computer Science, Ben-Gurion University, Beer Sheva 84105, ISRAEL

Abstract
We prove an extension of McDiarmid’s inequality for metric spaces with unbounded diameter. To this end, we introduce the notion of the
subgaussian diameter, which is a distributiondependent refinement of the metric diameter. Our
technique provides an alternative approach to
that of Kutin and Niyogi’s method of weakly
difference-bounded functions, and yields nontrivial, dimension-free results in some interesting cases where the former does not. As an application, we give apparently the first generalization bound in the algorithmic stability setting that
holds for unbounded loss functions. This yields a
novel risk bound for some regularized metric regression algorithms. We give two extensions of
the basic concentration result. The first enables
one to replace the independence assumption by
appropriate strong mixing. The second generalizes the subgaussian technique to other Orlicz
norms.

1. Introduction
Concentration of measure inequalities are at the heart of
statistical learning theory. Roughly speaking, concentration allows one to conclude that the performance of a (sufficiently “stable”) algorithm on a (sufficiently “close to iid”)
sample is indicative of the algorithm’s performance on future data. Quantifying what it means for an algorithm to be
stable and for the sampling process to be close to iid is by
no means straightforward and much recent work has been
motivated by these questions. It turns out that the various
notions of stability are naturally expressed in terms of the
Lipschitz continuity of the algorithm in question (Bousquet
& Elisseeff, 2002; Kutin & Niyogi, 2002; Rakhlin et al.,
2005; Shalev-Shwartz et al., 2010), while appropriate relaxations of the iid assumption are achieved using various
kinds of strong mixing (Karandikar & Vidyasagar, 2002;
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

KARYEH @ CS . BGU . AC . IL

Gamarnik, 2003; Rostamizadeh & Mohri, 2007; Mohri
& Rostamizadeh, 2008; Steinwart & Christmann, 2009;
Steinwart et al., 2009; Zou et al.; Mohri & Rostamizadeh,
2010; London et al., 2012; 2013; Shalizi & Kontorovich,
2013).
Many of the aforementioned results are based on McDiarmid’s inequality (McDiarmid, 1989):


2t2
,
(1)
P(|ϕ − Eϕ| > t) ≤ 2 exp − Pn
2
i=1 wi
where ϕ is a real-valued function of the sequence of independent random variables X = (X1 , . . . , Xn ), such that
|ϕ(x) − ϕ(x0 )| ≤ wi

(2)

whenever x and x0 differ only in the ith coordinate.
Aside from being instrumental in proving PAC bounds
(Boucheron et al., 2005), McDiarmid’s inequality has also
found use in algorithmic stability results (Bousquet & Elisseeff, 2002). Non-iid extensions of (1) have also been considered (Marton, 1996; Rio, 2000; Chazottes et al., 2007;
Kontorovich & Ramanan, 2008).
The distribution-free nature of McDiarmid’s inequality
makes it an attractive tool in learning theory, but also
imposes inherent limitations on its applicability. Chief
among these limitations is the inability of (1) to provide
risk bounds for unbounded loss functions. Even in the
bounded case, if the Lipschitz condition (2) holds not everywhere but only with high probability — say, with a
much larger constant on a small set of exceptions — the
bound in (1) still charges the full cost of the worst-case
constant. To counter this difficulty, Kutin (2002); Kutin &
Niyogi (2002) introduced an extension of McDiarmid’s inequality to weakly difference-bounded functions and used
it to analyze the risk of “almost-everywhere” stable algorithms. This influential result has been invoked in a number
of recent papers (El-Yaniv & Pechyony, 2006; Mukherjee
et al., 2006; Hush et al., 2007; Agarwal & Niyogi, 2009;
Shalev-Shwartz et al., 2010; Rubinstein & Simma, 2012).
However, the approach of Kutin & Niyogi entails some difficulties as well. These come in two flavors: analytical
(complex statement and proof) and practical (conditions are

Concentration in unbounded metric spaces

still too restrictive in some cases); we will elaborate upon
this in Section 3. In this paper, we propose an alternative
approach to the concentration of “almost-everywhere” or
“average-case” Lipschitz functions. To this end, we introduce the notion of the subgaussian diameter of a metric
probability space. The latter may be finite even when the
metric diameter is infinite, and we show that this notion
generalizes the more restrictive property of bounded differences.
Main results. This paper’s principal contributions include defining the subgaussian diameter of a metric probability space and identifying its role in relaxing the bounded
differences condition. In Theorem 1, we show that the subgaussian diameter can essentially replace the far more restrictive metric diameter in concentration bounds. This result has direct ramifications for algorithmic stability (Theorem 2), with applications to regularized regression. We
furthermore extend our concentration inequality to nonindependent processes (Theorem 3) and to other Orlicz
norms (Theorem 4).

used for all sequences, and sequence concatenation is denoted multiplicatively: xji xkj+1 = xki . We will frequently
Qj
use the shorthand P(xji ) = k=i P(Xk = xk ). Standard
order of magnitude notation such as O(·) and Ω(·) will be
used.
A function ϕ : X → R is L-Lipschitz if
|ϕ(x) − ϕ(x0 )| ≤ Lρ(x, x0 ),

x, x0 ∈ X .

Let (Xi , ρi , µi ), i = 1, . . . , n be a sequence of metric probability spaces. We define the product probability space
X n = X1 × X2 × . . . × Xn
with the product measure
µn = µ1 × µ2 × . . . × µn
and `1 product metric
ρn (x, x0 ) =

n
X

ρi (xi , x0i ),

x, x0 ∈ X n .

(3)

i=1

Motivation. The concentration properties of unbounded
functions become important in settings related to regression, such as sample bias correction, domain adaptation,
and boosting (Cortes & Mohri, 2014; Dasgupta & Long,
2003; Ben-David et al., 2006; Dudı́k et al., 2005; Mansour
et al., 2009). Subgaussian distributions occur in many practical applications, such as the histogram features in computer vision (Torralba et al., 2008; Deng et al., 2009). This
class of distributions subsumes the Gaussian random variables, as well as all the bounded ones (such as Bernoulli,
uniform, and multinomial).

We will denote partial products by
Xij = Xi × Xi+1 × . . . × Xj .
We write Xi ∼ µi to mean that Xi is an Xi -valued random variable with law µi — i.e., P(Xi ∈ A) = µi (A) for
all Borel A ⊂ Xi . This notation extends naturally to sequences: X1n ∼ µn . We will associate to each (Xi , ρi , µi )
the symmetrized distance random variable Ξ(Xi ) defined
by
Ξ(Xi ) = i ρi (Xi , Xi0 ),

Outline of paper. In Section 2 we define the subgaussian
diameter and relate it to (weakly) bounded differences in
Section 3. We state and prove the concentration inequality
based on this notion in Section 4 and give an application
to algorithmic stability in Section 5. We then give an extension to non-independent data in Section 6 and discuss
other Orlicz norms in Section 7. Conclusions and some
open problems are presented in Section 8.

2. Preliminaries
A metric probability space (X , ρ, µ) is a measurable space
X whose Borel σ-algebra is induced by the metric ρ, endowed with the probability measure µ. Our results are
most cleanly presented when X is a discrete set but they
continue to hold verbatim for general metric probability
spaces.
In particular, it will be convenient to write Eϕ =
P
P(x)ϕ(x)
even when the latter is an integral. Ranx∈X
dom variables are capitalized (X), specified sequences are
written in lowercase, the notation Xij = (Xi , . . . , Xj ) is

(4)

where Xi , Xi0 ∼ µi are independent and i = ±1 with
probability 1/2, independent of Xi , Xi0 . We note right
away that Ξ(Xi ) is a centered random variable:
E[Ξ(Xi )] = 0.

(5)

A real-valued random variable X is said to be subgaussian
if it admits a σ > 0 such that
EeλX ≤ eσ

2

λ2 /2

,

λ ∈ R.

(6)

The smallest σ for which (6) holds will be denoted by
σ ∗ (X). We define the subgaussian diameter ∆SG (Xi ) of
the metric probability space (Xi , ρi , µi ) in terms of its symmetrized distance Ξ(Xi ):
∆SG (Xi ) = σ ∗ (Ξ(Xi )).

(7)

In words, Ξ(Xi ) is the signed distance between two points
independently drawn from Xi and σ ∗ is the subgaussian

Concentration in unbounded metric spaces

moment of that random variable. If a metric probability
space (X , ρ, µ) has finite diameter,
diam(X ) := sup ρ(x, x0 ) < ∞,
x,x0 ∈X

then its subgaussian diameter is also finite:
∆SG (X ) ≤ diam(X ).

δ = exp(−Ω(n))
(8)

The bound in (8) is nearly tight in the sense that for every ε > 0 there is a metric probability space (X , ρ, µ) for
which
diam(X ) < ∆SG (X ) + ε

(9)

(see the Appendix for proofs of (8) and (9), and related
discussion).
On the other hand, there exist unbounded metric probability spaces with finite subgaussian diameter. A simple
example is (X , ρ, µ) with X = R, ρ(x, x0 ) = |x − x0 |
and µ the standard Gaussian probability measure dµ =
2
(2π)−1/2 e−x /2 dx. Obviously, diam(X ) = ∞. Now the
symmetrized distance Ξ = Ξ(X ) is distributed as the difference (=sum) of two standard Gaussians: Ξ ∼ N (0, 2).
2
Since EeλΞ = eλ , we have
√
∆SG (X ) = 2.
(10)
More generally, the subgaussian distributions on R are precisely those for which ∆SG (R) < ∞.

3. Related work
McDiarmid’s inequality (1) suffers from the limitations
mentioned above: it completely ignores the distribution and
is vacuous if even one of the wi is infinite.1 In order to address some of these issues, Kutin (2002); Kutin & Niyogi
(2002) proposed an extension of McDiarmid’s inequality to
“almost everywhere” Lipschitz functions ϕ : X n → R. To
formalize this, fix an i ∈ [n] and let X1n ∼ µn and Xi0 ∼ µi
be independent. Define X̃1n = X̃1n (i) by

Xj , j 6= i
X̃j (i) =
(11)
Xi0 , j = i.
Kutin & Niyogi define ϕ to be weakly difference-bounded
by (b, c, δ) if


P |ϕ(X) − ϕ(X̃(i))| > b = 0
(12)
and


P |ϕ(X) − ϕ(X̃(i))| > c < δ

for all 1 ≤ i ≤ n. The precise result of Kutin (2002, Theorem 1.10) is somewhat unwieldy to state — indeed, the
present work was motivated in part by a desire for simpler
tools. Assuming that ϕ is weakly difference-bounded by
(b, c, δ) with
(14)

and c = O(1/n), their bound states that
P(|ϕ − Eϕ| ≥ t) ≤ exp(−Ω(nt2 ))

(15)

for a certain range of t and n. As noted by Rakhlin et al.
(2005), the exponential decay assumption (14) is necessary
in order for the Kutin-Niyogi method to yield exponential
concentration. In contrast, the bounds we prove here
(i) do not require |ϕ(X) − ϕ(X̃)| to be everywhere
bounded as in (12)
(ii) have a simple statement and proof, and generalize to
non-iid processes with relative ease.
We defer the quantitative comparisons between (15) and
our results until the latter are formally stated in Section 4.
The entropy method (Boucheron et al., 2003) may also be
used to obtain concentration for unbounded functions but
typically requires more detailed structural information. In
a different line of work, Antonov (1979) gave inequalities for sums of independent random variables in the Orlicz spaces; these were recently improved by Rio (2013b).
Bentkus (2008) considered an extension of Hoeffding’s inequality to unbounded random variables. Rio (2013a) gave
an Lp extension of McDiarmid’s inequality. Kim & Vu
(2000); Vu (2002) gave concentration inequalities for some
classes of non-Lipschitz functions. An earlier notion of “effective” metric diameter in the context of concentration is
that of metric space length (Schechtman, 1982). Another
distribution-dependent refinement of diameter is the spread
constant (Alon et al., 1998). Lecué & Mendelson (2013)
gave minimax bounds for empirical risk minimization over
subgaussian classes. More recently, Mendelson (2014)
presented a framework for learning without concentration,
which allows for unbounded loss functions and fat-tailed
distributions. Cortes et al. (2013) gave relative bounds for
unbound losses under moment assumptions. A result of
van de Geer & Lederer (2013) interpolates between subgaussian and subexponential tails via a new Orlicz-type
norm. Perhaps closest in spirit to the present work is the
paper of Meir & Zhang (2003), whose Theorem 3 essentially expresses a subgaussian condition.

(13)

4. Concentration via subgaussian diameter
1

Note, though, that McDiarmid’s inequality is sharp in
the sense that the constants in (1) cannot be improved in a
distribution-free fashion.

McDiarmid’s inequality (1) may be stated in the notation
of Section 2 as follows. Let (Xi , ρi , µi ), i = 1, . . . , n be a

Concentration in unbounded metric spaces

sequence of metric probability spaces and ϕ : X n → R a
1-Lipschitz function. Then


2t2
P(|ϕ − Eϕ| > t) ≤ 2 exp − Pn
(16)
2
i=1 diam(Xi )
(the equivalence of (1) and (16) is proved in the Appendix).
We defined the subgaussian diameter ∆SG (Xi ) in Section 2,
having shown in (9) that it never exceeds the metric diameter. We also showed by example that the former can be
finite when the latter is infinite. The main result of this section is that diam(Xi ) in (16) can essentially be replaced by
∆SG (Xi ):
Theorem 1. If ϕ : X n → R is 1-Lipschitz and ∆SG (Xi ) <
∞ for all i ∈ [n], then Eϕ < ∞ and


t2
.
P(|ϕ − Eϕ| > t) ≤ 2 exp − Pn
2 i=1 ∆2SG (Xi )
Our constant in the exponent is worse than that of (16) by
a factor of 4. This appears to be an inherent artifact of our
method, and we do not know whether (16) holds verbatim
with diam(Xi ) be replaced by ∆SG (Xi ).
Proof. The strong integrability of ϕ — and in particular,
finiteness of Eϕ — follow from exponential concentration
(Ledoux, 2001). The rest of the proof will proceed via the
Azuma-Hoeffding-McDiarmid method of martingale differences. Define Vi = E[ϕ | X1i ] − E[ϕ | X1i−1 ] and expand

1-Lipschitz with respect to ρi . Since et + e−t = 2 cosh(t)
and cosh(t) ≤ cosh(s) for all |t| ≤ s, we have2
0

=

X

E[ϕ | X1i−1 ] =

X



X
X
0
0
≤ 12 
P(y)P(y 0 )eλρi (y,y ) +
P(y)P(y 0 )e−λρi (y,y ) 
y,y 0

y,y 0

= EeλΞ(Xi ) ≤ exp(λ2 ∆2SG (Xi )/2),
where Ξ(Xi ) is the symmetrized distance (4) and the
last inequality holds by definition of subgaussian diameter
(6,7). It follows that
E[eλVi | X1i−1 ] ≤ exp(λ2 ∆2SG (Xi )/2).

i=1

"
≤ e−λt E
"
−λt

−λt

= exp
Let us write Ṽi to denote Vi as a function of X1i−1 with Xi
integrated out:
X
P(xni+1 )·
Ṽi =
xn
i+1


P(xi )P(x0i ) ϕ(X1i−1 xi xni+1 ) − ϕ(X1i−1 x0i xni+1 ) .

xi ,x0i

Hence, by Jensen’s inequality, we have
X
E[eλVi | X1i−1 ] ≤
P(xni+1 )·

y,y

P(y)P(y 0 )e

i−1 0 n
λ(ϕ(X1i−1 yxn
y xi+1 ))
i+1 )−ϕ(X1

E

i=1
n
Y

i=1
" n
Y

#
eλVi
#
λVi

E[e

| X1i−1 ]
#

2

2

exp(λ ∆SG (Xi )/2)

!
n
1 2X 2
λ
∆ (Xi ) − λt .
2 i=1 SG

.

0

n
For fixed X1i−1 ∈ X1i−1 and xni+1 ∈ Xi+1
, define F :
i−1
n
Xi → R by F (y) = ϕ(X1 yxi+1 ), and observe that F is

(19)

Optimizing over λ and applying the same argument to Eϕ−
ϕ yields our claim.
Let us see how Theorem 1 compares to previous results on
n
some examples.
P Consider R 0 equipped with the `1 metric
n
0
ρ (x, x ) =
i∈[n] |xi − xi | and the standard Gaussian
product measure µn = N (0, In ). Let ϕ : Rn → R be 1/nLipschitz. Then Theorem 1 yields (recalling the calculation
in (10))
P(|ϕ − Eϕ| > ε) ≤ 2 exp(−nε2 /4),

xn
i+1

X

E

n
Y

i=1

n
xn
i ∈Xi

X

(18)

Applying the standard Markov’s inequality and exponential
bounding argument, we have
!
n
X
P(ϕ − Eϕ > t) = P
Vi > t

≤e

P(xni )ϕ(X1i−1 xni ).

0

y,y 0 ∈Xi

P(xni+1 )ϕ(X1i xni+1 )

n
xn
i+1 ∈Xi+1

0

Now for every term in the sum of the form exp(λ(F (y) −
F (y 0 ))) there is a matching term with the opposite sign in
the exponent, and hence
X
0
P(y)P(y 0 )eλ(F (y)−F (y ))
(17)

=e
E[ϕ | X1i ]

0

eλ(F (y)−F (y )) + eλ(F (y )−F (y)) ≤ eλρi (y,y ) + e−λρi (y,y ) .

ε > 0,

(20)

whereas the inequalities of McDiarmid (1) and KutinNiyogi (15) are both uninformative since the metric diameter is infinite.
2
An analogous symmetrization technique is employed in Tao
(2009) as a variant of the “square and rearrange” trick.

Concentration in unbounded metric spaces

For our next example, fix an n ∈ N and put Xi =
{±1, ±n} with the metric ρi (x, x0 ) = |x − x0 | and the dis2
tribution µi (x) ∝ e−x . One may verify
√ via a calculation
analogous to (10) that ∆SG (Xi ) ≤ 2. For
Pnindependent
Xi ∼ µi , i = 1, . . . , n, put ϕ(X1n ) = n−1 i=1 Xi . Then
Theorem 1 implies that in this case the bound in (20) holds
verbatim. On the other hand, ϕ is easily seen to be weakly
difference-bounded by (1, 1/n, e−Ω(n) ) and thus (15) also
yields subgaussian concentration, albeit with worse constants. Applying (1) yields the much cruder estimate
P(|ϕ − Eϕ| > ε) ≤ 2 exp(−2ε2 ).

We define the algorithm A to be β-totally Lipschitz stable if the function ϕ : Z n+1 → R given by ϕ(z1n+1 ) =
L(Az1n , zn+1 ) is β-Lipschitz with respect to the `1 product
metric on Z n+1 :
∀z, z 0 ∈ Z n+1 : |ϕ(z) − ϕ(z 0 )| ≤ β

n+1
X

ρ(zi , zi0 ).

(23)

i=1

Note that total Lipschitz stability is stronger than uniform
stability since it requires the algorithm to respect the metric
of Z.
Let us bound the bias of stable algorithms.

5. Application to algorithmic stability
We refer the reader to (Bousquet & Elisseeff, 2002; Kutin
& Niyogi, 2002; Rakhlin et al., 2005) for background on
algorithmic stability and supervised learning. Our metric
probability space (Zi , ρi , µi ) will now have the structure
Zi = Xi × Yi where Xi and Yi are, respectively, the instance and label space of the ith example. Under the iid
assumption, the (Zi , ρi , µi ) are identical for all i ∈ N (and
so we will henceforth drop the subscript i from these). A
training sample S = Z1n ∼ µn is drawn and a learning algorithm A inputs S and outputs a hypothesis f : X → Y.
The hypothesis f = A(S) will be denoted by AS . In line
with the previous literature, we assume that A is symmetric (i.e., invariant under permutations of S). The loss of a
hypothesis f on an example z = (x, y) is defined by
L(f, z) = `(f (x), y),
where ` : Y × Y → [0, ∞) is the cost function. To
our knowledge, all previous work required the loss to be
bounded by some constant M < ∞, which figures explicitly in the bounds; we make no such restriction. In the algorithmic stability setting, the empirical risk R̂n (A, S) is
typically defined as

Lemma 1. Suppose A is a symmetric, β-totally Lipschitz
stable learning algorithm over the metric probability space
(Z, ρ, µ) with ∆SG (Z) < ∞. Then
E[R(A, S) − R̂n (A, S)] ≤

1 2 2
2 β ∆SG (Z).

We now turn to Lipschitz continuity.
Lemma 2. Suppose A is a symmetric, β-totally Lipschitz stable learning algorithm and define the function
ϕ : Z n → R by ϕ(z) = R(A, z) − R̂n (A, z). Then ϕ
is 3β-Lipschitz.
Combining Lemmas 1 and 2 with our concentration inequality in Theorem 1 yields the main result of this section:
Theorem 2. Suppose A is a symmetric, β-totally Lipschitz
stable learning algorithm over the metric probability space
(Z, ρ, µ) with ∆SG (Z) < ∞. Then, for training samples
S ∼ µn and ε > 0, we have


P R(A, S) − R̂n (A, S) > 21 β 2 ∆2SG (Z) + ε


ε2
≤ exp −
.
18β 2 ∆2SG (Z)n

n

1X
L(AS , zi )
R̂n (A, S) =
n i=1

(21)

and the true risk R(A, S) as
R(A, S) = Ez∼µ [L(AS , z)].

(22)

The goal is to bound the true risk in terms of the empirical
one. To this end, a myriad of notions of hypothesis stability have been proposed. A variant of uniform stability in
the sense of Rakhlin et al. (2005) — which is slightly more
general than the homonymous notion in Bousquet & Elisseeff (2002) — may be defined as follows. The algorithm A
is said to be β-uniform stable if for all z̃ ∈ Z, the function
ϕz̃ : Z n → R given by ϕz̃ (z) = L(Az , z̃) is β-Lipschitz
with respect to the Hamming metric on Z n :
∀z̃ ∈ Z, ∀z, z 0 ∈ Z n : |ϕz̃ (z) − ϕz̃ (z 0 )| ≤ β

n
X
i=1

1{zi 6=z0 } .
i

As in Bousquet & Elisseeff (2002) and related results on algorithmic stability, we require β = o(n−1/2 ) for nontrivial
decay. Bousquet & Elisseeff showed that this is indeed the
case for some popular learning algorithms, albeit in their
less restrictive definition of stability. Below we show that a
natural metric regression algorithm is stable in our stronger
sense.
5.1. Stability of regularized nearest-neighbor
regression
In the regression setting, we take the label space Y to be
all of R (and note that many existing approaches require
Y to be a compact subst of R). Gottlieb et al. (2013) proposed an efficient algorithm for regression in general metric spaces via Lipschitz extension, which is algorithmically
realized by 1-nearest neighbors. Aside from efficiency,

Concentration in unbounded metric spaces

the nearest-neighbor approach also facilitates risk analysis. To any metric space (X , ρ) we associate the metric
space (Z, ρ̄), where Z = X × R and ρ̄((x, y), (x0 , y 0 )) =
ρ(x, x0 ) + |y − y 0 |. Suppose that (Z, ρ̄) is endowed with
a measure µ such that ∆SG = ∆SG (Z, ρ̄, µ) < ∞. Write
Fλ to denote the collection of all λ-Lipschitz hypotheses
f : X → R. The learning algorithm A maps the sample
S = Zi∈[n] , with Zi = (Xi , Yi ) ∈ X ×R, to the hypothesis
fˆ ∈ Fλ by minimizing the empirical risk

exists, though it may not be unique. Another elementary
property of couplings is that for any two f, g : X → R and
any coupling π ∈ Π(µ, µ0 ), we have
Eµ f − Eµ0 g = E(X,X 0 )∼π [f (X) − g(X 0 )].

(25)

It is possible to refine the total variation distance (24) between µ and µ0 so as to respect the metric of X . Given a
space equipped with probability measures µ, µ0 and metric
ρ, define the transportation cost3 distance Tρ (µ, µ0 ) by

n

1X
|f (Xi ) − Yi |
fˆ = argmin
f ∈Fλ n i=1

Tρ (µ, µ0 ) =

over all f ∈ Fλ , where we have chosen the absolute loss
`(y, y 0 ) = |y − y 0 |. We will give a heuristic argument
for the stability of 1-NN regression regularized by Lipschitz continuity λ. This argument will be fleshed out formally in the full version of the paper. Since the value of a
Lipschitz extension at a point is determined by its nearest
neighbors (Bousquet & Elisseeff, 2002), it suffices to ensure that none of the n + 1 points (n sample and 1 test)
is too isolated from the rest. The subgaussian assumption implies (see Rivasplata (2012, Theorem 3.1)) that with
probability 1 − n exp(−Ω(n)), each of the n + 1 points
is within distance O(∆SG ) of another point. Since a λLipschitz function can vary by at most O(λD) over a ball
of diameter D, this implies that the regression algorithm is
β = O(λ∆SG /n)-stable. Thus, Theorem 2 yields the risk
bound


P R(A, S) − R̂n (A, S) > (λ∆SG /n)2 + ε

 2 
ε n
+ n exp(−Ω(n)).
≤ exp −Ω
λ2 ∆2SG
Note that the subgaussian assumption implies risk bounds
not depending on any dimensions (doubling or otherwise)
of the metric space (cf. Gottlieb et al. (2013)).

6. Relaxing the independence assumption
In this section we generalize Theorem 1 to strongly mixing processes. To this end, we require some standard facts
concerning the probability-theoretic notions of coupling
and transportation (Lindvall, 2002; Villani, 2003; 2009).
Given the probability measures µ, µ0 on a measurable space
X , a coupling π of µ, µ0 is any probability measure on
X × X with marginals µ and µ0 , respectively. Denoting
by Π = Π(µ, µ0 ) the set of all couplings, we have
X

	
|µ(x) − µ0 (x)|
inf π( (x, y) ∈ X 2 : x 6= y ) = 21
π∈Π

x∈X

= kµ − µ0 kTV

(24)

where k·kTV is the total variation norm. An optimal coupling is one that achieves the infimum in (24); one always

inf

π∈Π(µ,µ0 )

E(X,X 0 )∼π ρ(X, X 0 ).

It is easy to verify that Tρ is a valid metric on probability measures and that for ρ(x, x0 ) = 1{x6=x0 } , we have
Tρ (µ, µ0 ) = kµ − µ0 kTV .
As in Section 4, we consider a sequence of metric spaces
(Xi , ρi ), i = 1, . . . , n and their `1 product (X n , ρn ). Unlike the independent case, we will allow nonproduct probability measures ν on (X n , ρn ). We will write X1n ∼ ν to
mean that P(X1n ∈ A) = ν(A) for all Borel A ⊂ X n . For
1 ≤ i ≤ j < k ≤ l ≤ n, we will use the shorthand


P(xlk | xji ) = P Xkl = xlk | Xij = xji .
The notation P(Xij ) means the marginal distribution of Xij .
Similarly, P(Xkl | Xij = xji ) will denote the conditional distribution. For 1 ≤ i < n, and xi1 ∈ X1i , x0i ∈ Xi define
n
n
0
τi (xi1 ,x0i ) = Tρni+1(P(Xi+1
|X1i = xi1 ),P(Xi+1
|X1i = xi−1
1 xi )),

where ρni+1 is the `1 product of ρi+1 , . . . ρn as in (3), and
τ̄i =

sup
xi1 ∈X1i ,x0i ∈Xi

τi (xi1 , x0i ),

with τ̄n ≡ 0. In words, τi (xi1 , x0i ) measures the transportation cost distance between the conditional distributions inn
duced on the “tail” Xi+1
given two prefixes that differ in
th
the i coordinate, and τ̄i is the maximal value of this quantity. Kontorovich (2007); Kontorovich & Ramanan (2008)
discuss how to handle conditioning on measure-zero sets
and other technicalities. Note that for product measures the
conditional distributions are identical and hence τ̄i = 0.
We need one more definition before stating our main result.
For the prefix x1i−1 , define the conditional distribution

i−1
νi (xi−1
= xi−1
1 ) = P Xi | X1
1
3
This fundamental notion is also known as the Wasserstein,
Monge-Kantorovich, or earthmover distance; see Villani (2003;
2009) for an encyclopedic treatment. The use of coupling and
transportation techniques to obtain concentration for dependent
random variables goes back to Marton (1996); Samson (2000);
Chazottes et al. (2007).

Concentration in unbounded metric spaces

and consider the corresponding metric probability space
(Xi , ρi , νi (xi−1
1 )). Define its conditional subgaussian diameter by
i−1
∆SG (Xi | xi−1
1 ) = ∆SG (Xi , ρi , νi (x1 ))

and the maximal subgaussian diameter by
¯ SG (Xi ) =
∆

sup
x1i−1 ∈X1i−1

∆SG (Xi | xi−1
1 ).

(26)

Note that for product measures, (26) reduces to the former definition (7). With these definitions, we may state
the main result of this section.
Theorem 3. If ϕ : X n → R is 1-Lipschitz with respect to
ρn , then
!
P
(t − i≤n τ̄i )2
P(|ϕ − Eϕ| > t) ≤ 2 exp − P
¯ 2SG (Xi ) , t > 0.
2
∆

We define the p-Orlicz diameter of a metric probability
space (X , ρ, µ), denoted ∆OR(p) (X ), as the smallest a > 0
that verifies (27) for the symmetrized distance Ξ(X ). In
light of (28), Theorem 1 extends straightforwardly to finite
p-Orlicz metric diameters:
Theorem 4. Let (Xi , ρi , µi ), i = 1, . . . , n be a sequence
of metric probability spaces and equip X n with the usual
product measure µn and `1 product metric ρn . Suppose
that for some p > 1 and all i ∈ [n] we have ∆OR(p) (Xi ) <
∞, and define the vector ∆ ∈ Rn by ∆i = ∆OR(p) (Xi ). If
ϕ : X n → R is 1-Lipschitz then for all t > 0,

!p/(p−1) 
t
p
−
1
.
P(|ϕ − Eϕ| > t) ≤ 2 exp −
p
k∆kp

8. Discussion

i≤n

Observe that we recover Theorem 1 as a special P
case. Since
typically we will take t = εn, it suffices that i≤n τ̄i =
P
¯ 2 (Xi ) = O(n) to ensure an exponeno(n) and i≤n ∆
SG
tial bound with decay rate exp(−Ω(nε2 )). (Note that our
functions are scaled to be O(1)-Lipschitz as opposed to
the usual O(1/n)-Lipschiz condition.) The appearance of
the mixing coefficients τ̄i in the numerator is non-standard,
and is mainly an artifact of our inability to obtain nontrivial bounds on this quantity. Elucidating its structure is an
active research direction.

7. Other Orlicz diameters
Let us recall the notion of an Orlicz norm kXkΨ of a real
random variable X (see, e.g., Rao & Ren (1991)):
kXkΨ = inf {t > 0 : E[Ψ(X/t)] ≤ 1} ,
where Ψ : R → R is a Young function — i.e., nonnegative,
even, convex and vanishing at 0. In this section, we will
consider the Young functions
p

ψp (x) = e|x| − 1,

p > 1,

and their induced Orlicz norms. A random variable X is
subgaussian if and only if kXkψ2 < ∞ (Rivasplata, 2012).
For p 6= 2, kXkψp < ∞ implies that
EeλX ≤ e(a|λ|)

p

/p

,

λ ∈ R,

(27)

for some a > 0, but the converse implication need not hold.
An immediate consequence of Markov’s inequality is that
any X for which (27) holds also satisfies
 p/(p−1) !
p−1 t
P(|X| ≥ t) ≤ 2 exp −
.
(28)
p
a

We have given a concentration inequality for metric spaces
with unbounded diameter, showed its applicability to algorithmic stability with unbounded losses, and gave an extension to non-independent sampling processes. Some fascinating questions remain:
(i) How tight is Theorem 1? First there is the vexing matter of having a worse constant in the exponent (i.e., 1/2) than McDiarmid’s (optimal) constant
2. Although this gap is not of critical importance,
one would like a bound that recovers McDiarmid’s
in the finite-diameter case. More importantly, is it
the case that finite subgaussian diameter is necessary
for subgaussian concentration of all Lipschitz functions? That is, given the metric probability spaces
(Xi , ρi , µi ), i ∈ [n], can one always exhibit a 1Lipschitz ϕ : X n → R that achieves a nearly matching lower bound?
(ii) We would like to better understand how Theorem 1
compares to the Kutin-Niyogi bound (15). We conjecture that for any (X n , µn ) and ϕ : X n → R that
satisfies (12) and (13),
P one can construct a product
metric ρn for which i∈[n] ∆2SG (Xi ) < ∞ and ϕ is 1Lipschitz. This would imply that whenever the KutinNiyogi bound is nontrivial, so is Theorem 1. We have
already shown by example (20) that the reverse does
not hold.
(iii) The quantity τ̄i defined in Section 6 is a rather complicated object; one desires a better handle on it in terms
of the given distribution and metric.
(iv) We have argued in Section 5.1 that a natural regularized metric regression is totally Lipschitz stable under our definition (23). The next order of business
would be to show that some other common learning

Concentration in unbounded metric spaces

algorithms, such as kernel SVM, are also stable in our
strong sense.
ACKNOWLEDGEMENTS
John Lafferty encouraged me to seek a distribution-dependent refinement of McDiarmid’s inequality. Thanks also to Gideon Schechtman, Shahar Mendelson, Assaf
Naor, Iosif Pinelis and Csaba Szepesvári for helpful correspondence, and to Roi
Weiss for carefully proofreading the manuscript. This work was partially supported
by the Israel Science Foundation (grant No. 1141/12) and a Yahoo Faculty award.

References
Agarwal, S. and Niyogi, P. Generalization bounds for ranking algorithms via algorithmic stability. JMLR, 10:441–
474, June 2009.
Alon, N., Boppana, R., and Spencer, J. An asymptotic
isoperimetric inequality. GAFA, 8(3):411–436, 1998.
Antonov, S. Probability inequalities for a series of independent random variables. Teor. Veroyatnost. i Primenen.,
24(3):632–636, 1979.
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F.
Analysis of representations for domain adaptation. In
NIPS, pp. 137–144, 2006.
Bentkus, V.. An extension of the Hoeffding inequality to
unbounded random variables. Lith. Math. J., 48(2):137–
157, 2008.
Boucheron, S., Lugosi, G., and Massart, P. Concentration
inequalities using the entropy method. Ann. Probab., 31
(3):1583–1614, 2003.
Boucheron, S., Bousquet, O., and Lugosi, G. Theory
of classification: a survey of some recent advances.
ESAIM: Probability and Statistics, 9:323–375, 2005.
Bousquet, O. and Elisseeff, A. Stability and generalization.
JMLR, 2:499–526, 2002.
Chazottes, J.-R., Collet, P., Külske, C., and Redig, F. Concentration inequalities for random fields via coupling.
Prob. Theory Rel. Fields, 137(1-2):201–225, 2007.
Cortes, C. and Mohri, M. Domain adaptation and sample bias correction theory and algorithm for regression.
Theor. Comput. Sci., 519:103–126, 2014.
Cortes, C., Greenberg, S., and Mohri, M.. Relative deviation learning bounds and generalization with unbounded
loss functions (arxiv:1310.5796). 2013.
Dasgupta, S. and Long, P. M. Boosting with diverse base
classifiers. In COLT, 2003.

Deng, J., Dong, Wei, S., Richard, L., Li-Jia, L., Kai, and Li,
F. Imagenet: A large-scale hierarchical image database.
In CVPR, pp. 248–255, 2009.
Dudı́k, M., Schapire, R., and Phillips, S. Correcting sample
selection bias in maximum entropy density estimation.
In NIPS, 2005.
El-Yaniv, R. and Pechyony, D. Stable transductive learning.
In COLT, 2006.
Gamarnik, D. Extension of the PAC framework to finite and
countable Markov chains. IEEE Trans. Inform. Theory,
49(1):338–345, 2003.
Gottlieb, Lee-Ad, Kontorovich, Aryeh, and Krauthgamer,
Robert. Efficient regression in metric spaces via approximate Lipschitz extension. In SIMBAD, 2013.
Hush, D., Scovel, C., and Steinwart, I. Stability of unstable
learning algorithms. Machine Learning, 67(3):197–206,
2007.
Karandikar, R. and Vidyasagar, M. Rates of uniform
convergence of empirical means with mixing processes.
Statist. Probab. Lett., 58(3):297–307, 2002.
Kim, J. H. and Vu, Van H. Concentration of multivariate
polynomials and its applications. Combinatorica, 20(3):
1439–6912, 2000.
Kontorovich, A. Measure Concentration of Strongly Mixing Processes with Applications. PhD thesis, CMU,
2007.
Kontorovich, A. and Ramanan, K. Concentration Inequalities for Dependent Random Variables via the Martingale
Method. Ann. Probab., 36(6):2126–2158, 2008.
Kutin, S. Extensions to McDiarmid’s inequality when differences are bounded with high probability. Technical
Report TR-2002-04, University of Chicago, 2002.
Kutin, S. and Niyogi, P. Almost-everywhere algorithmic
stability and generalization error. In UAI, 2002.
Lecué, G. and Mendelson, S. Learning subgaussian
classes: Upper and minimax bounds, arxiv:1305.4825.
2013.
Ledoux, M. The Concentration of Measure Phenomenon.
Mathematical Surveys and Monographs Vol. 89, 2001.
Lindvall, T. Lectures on the Coupling Method. 2002.
London, B., Huang, B., and Getoor, L. Improved generalization bounds for large-scale structured prediction.
In NIPS Workshop on Algorithmic and Statistical Approaches for Large Social Networks, 2012.

Concentration in unbounded metric spaces

London, B., Huang, B., Taskar, B., and Getoor, L. Collective stability in structured prediction: Generalization
from one example. In ICML, 2013.
Mansour, Y., Mohri, M., and Rostamizadeh, A. Domain
adaptation: Learning bounds and algorithms. In COLT,
2009.
¯
Marton, K. Bounding d-distance
by informational divergence: a method to prove measure concentration. Ann.
Probab., 24(2):857–866, 1996.

Rubinstein, B. and Simma, A. On the stability of empirical
risk minimization in the presence of multiple risk minimizers. IEEE Trans. Inform. Theory, 58(7):4160–4163,
2012.
Samson, P.-M. Concentration of measure inequalities for
Markov chains and Φ-mixing processes. Ann. Probab.,
28(1):416–461, 2000.

McDiarmid, C. On the method of bounded differences. In
Siemons, J. (ed.), Surveys in Combinatorics, volume 141
of LMS Lecture Notes Series, pp. 148–188, 1989.

Schechtman, G. Lévy type inequality for a class of finite
metric spaces. In Chao, Jia-Arng and Woyczyński, Wojbor A. (eds.), Martingale Theory in Harmonic Analysis and Banach Spaces, volume 939 of Lecture Notes in
Mathematics, pp. 211–215. Springer Berlin Heidelberg,
1982.

Meir, R. and Zhang, T. Generalization error bounds for
bayesian mixture algorithms. JMLR, 4:839–860, December 2003.

Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K. Learnability, stability and uniform convergence.
JMLR, 11:2635–2670, 2010.

Mendelson, Shahar. Learning without concentration. In
COLT, 2014.

Shalizi, C. and Kontorovich, A. Predictive PAC learning
and process decompositions. In NIPS, 2013.

Mohri, M. and Rostamizadeh, A. Rademacher complexity
bounds for non-i.i.d. processes. In NIPS, 2008.

Steinwart, I. and Christmann, A. Fast learning from noni.i.d. observations. In NIPS, 2009.

Mohri, M. and Rostamizadeh, A. Stability bounds for stationary phi-mixing and beta-mixing processes. JMLR,
11:789–814, 2010.

Steinwart, I., Hush, D., and Scovel, C. Learning from dependent observations. J. Multivar. Anal., 100(1):175 –
194, 2009.

Mukherjee, S., Niyogi, P., Poggio, T., and Rifkin, R. Learning theory: stability is sufficient for generalization and
necessary and sufficient for consistency of empirical risk
minimization. Advances in Computational Mathematics,
25(1-3):161–193, 2006.

Tao, T. Talagrand’s concentration inequality, 2009.

Rakhlin, A., Mukherjee, S., and Poggio, T. Stability results
in learning theory. Anal. Appl., 3(4):397–417, 2005.

van de Geer, S. and Lederer, J. The bernstein-orlicz norm
and deviation inequalities. Prob. Theory Rel. Fields, 157
(1-2):225–250, 2013.

Rao, M. M. and Ren, Z. D. Theory of Orlicz spaces, volume
146 of Monographs and Textbooks in Pure and Applied
Mathematics, 1991.
Rio, E. Inégalités de Hoeffding pour les fonctions lipschitziennes de suites dépendantes. C. R. Acad. Sci. Paris
Sér. I Math., 330(10):905–908, 2000.
Rio, E. On McDiarmid’s concentration inequality. Electron. Commun. Probab., 18:no. 44, 11, 2013a.
Rio, E. Extensions of the Hoeffding-Azuma inequalities.
Electron. Commun. Probab., 18:no. 54, 6, 2013b.
Rivasplata, O. Subgaussian random variables: An expository note. 2012.
Rostamizadeh, A. and Mohri, M. Stability bounds for noni.i.d. processes. In NIPS, 2007.

Torralba, A., Fergus, R., and Freeman, W. T. 80 million
tiny images: A large data set for nonparametric object
and scene recognition. IEEE Trans. Pattern Anal. Mach.
Intell., 30(11):1958–1970, 2008.

Villani, C. Topics in optimal transportation, volume 58 of
Graduate Studies in Mathematics. American Mathematical Society, Providence, RI, 2003.
Villani, C. Optimal Transport: Old and New, volume 338
of Grundlehren der Mathematischen Wissenschaften.
2009.
Vu, V. H. Concentration of non-Lipschitz functions and applications. Random Struct. Algorithms, 20(3):262–316,
2002.
Zou, B., Xu, Z., and Xu, J. Generalization bounds of ERM
algorithm with Markov chain samples. Acta Mathematicae Applicatae Sinica, pp. 1–16.

