Learnability of the Superset Label Learning Problem

Li-Ping Liu
LIULI @ EECS . OREGONSTATE . EDU
Thomas G. Dietterich
TGD @ EECS . OREGONSTATE . EDU
School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon 97331, USA

Abstract
In the Superset Label Learning (SLL) problem,
weak supervision is provided in the form of a superset of labels that contains the true label. If
the classifier predicts a label outside of the superset, it commits a superset error. Most existing SLL algorithms learn a multiclass classifier
by minimizing the superset error. However, only
limited theoretical analysis has been dedicated to
this approach. In this paper, we analyze Empirical Risk Minimizing learners that use the superset error as the empirical risk measure. SLL data
can arise either in the form of independent instances or as multiple-instance bags. For both
scenarios, we give the conditions for ERM learnability and sample complexity for the realizable
case.

1. Introduction
In multiclass supervised learning, the task is to learn a
classifier that maps an object to one of several candidate
classes. When each training example is labeled with one label, many successful multiclass learning methods can solve
this problem (Aly, 2005; Mukherjee and Schapire, 2013).
In some applications, however, we cannot obtain training
examples of this kind. Instead, for each instance we are
given a set of possible labels. The set is guaranteed to contain the true label as well as one or more distractor labels.

Several learning algorithms for the superset label learning
problem have shown good experimental results. Most of
these algorithms seek an hypothesis that explicitly minimizes superset errors on the training instances (possibly including a regularization penalty). An exception is
the ECOC-based algorithm proposed by Zhang (2014).
Though it does not minimize superset errors explicitly, this
algorithm generally predicts a class label of an training instance from its superset and thus are also minimizing superset errors. In this paper, we define the superset error
as the empirical risk in the SLL problem, and we analyze
the performance of learners that minimize the empirical
risk (ERM learners). We only investigate the realizable
case where the true multiclass classifier is in the hypothesis space.
The key to SLL learnability is that any hypothesis with nonzero classification error must have a significant chance of
making superset errors. This in turn depends on the size
and distribution of the label supersets. Small supersets, for
example, are more informative than large ones. Precisely
speaking, a sufficient condition for learnability is that the
classification error can be bounded by the superset error.
The SLL problem arises in two settings. In the first setting, which we call SSL-I, instances are independent of
each other, and the superset is selected independently for
each instance. The second setting, which we call SLL-B,
arises from the multi-instance multi-label learning (MIML)
problem (Zhou & Zhang, 2006; Zhou et al., 2012), where
the training data are given in the form of MIML bags.

Despite these distractor labels, we still wish learn a multiclass classifier that has low error when measured according to the traditional 0/1 misclassification loss. This learning problem has been given several names, including the
â€œmultiple label problemâ€, the â€œpartial label problemâ€ and
the â€œsuperset label learning problemâ€ (Jin & Ghahramani,
2002; Nguyen & Caruana, 2008; Cour et al., 2011; Liu &
Dietterich, 2012). In this paper, we adopt the last of these.

For SLL-I, Cour, Sapp, and Taskar (2011) proposed the
concept of ambiguity degree. This bounds the probability
that a specific distractor label appears in the superset of a
given instance. When the ambiguity degree is less than 1,
Cour, et al. give a relationship between the classification
error and the superset error. With the same condition, we
show that the sample complexity of SLL-I with ambiguity
degree zero matches the complexity of multiclass classification.

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

For SLL-B, the training data have the form of independent
MIML bags. Each MIML bag consists of a bag of instances

Learnability of Superset Label Learning Problem

and a set of labels (the â€œbag labelâ€). Each instance has
exactly one (unknown) label, and the bag label is exactly
the union of the labels of all of the instances. (See Zhou
et al. (2012) for discussion of other ways in which MIML
bags can be generated.) The learner only observes the bag
label and not the labels of the individual instances.
It is interesting to note that several previous papers test their
algorithms on synthetic data corresponding to the SSL-I
scenario, but then apply them to a real application corresponding to the SSL-B setting.
To show learnability, we convert the SLL-B problem to
a binary classification problem with the general condition
that the classification error can be bounded by the superset error times a multiplicative constant. We then provide a
concrete condition for learnability: for any pair of class labels, they must not always co-occur on a bag label. That is,
there must be non-zero probability of observing a bag that
contains an instance of only one of the two labels. Given
enough training data, we can verify with high confidence
whether this condition holds.
The success of weakly supervised learning depends on the
degree of correlation between the supervision information
and the classification error. We show that superset learning
exhibits a strong correlation between these because a superset error is always caused by a classification error. Our
study of the SLL problem can be seen as a first step toward
the analysis of more general weakly-supervised learning
problems.

2. Related Work
Different superset label learning algorithms have been proposed by Jin and Ghahramani (2002); Nguyen and Caruana
(2008); Cour, Sapp, and Taskar (2011); Liu and Dietterich
(2012); and Zhang (2014). All these algorithms employ
some loss to minimize superset errors on the training set.
Cour, Sapp, and Taskar (2011) conducted some theoretical analysis of the problem. They proposed the concept of
â€œambiguity degreeâ€ and established a relationship between
superset error and classification errors. They also gave a
generalization bound for their algorithm. In their analysis,
they assume instances are independent of each other.
Sample complexity analysis of multiclass learning provides
the basis of our analysis of SLL problem with independent
instances. The Natarajan dimension (Natarajan, 1989) is
an important instrument for characterizing the capacity of
multiclass hypothesis spaces. Ben-David et al. (1995) and
(Daniely et al., 2011) give sample complexity bounds in
terms of this dimension.
The MIML framework was proposed by Zhou & Zhang
(2006), and the instance annotation problem is raised by

Briggs et al. (2012). Though the Briggs, et al. algorithm
explicitly uses bag information, it is still covered by our
analysis of the SLL-B problem. There is some theoretical
analysis of multi-instance learning (Blum & Kalai, 1998;
Long & Tan, 1998; Sabato & Tishby, 2009), but we only
know of one paper on the learnability of the MIML problem (Wang & Zhou, 2012). In that work, no assumption is
made for the distribution of instances within a bag, but the
labels on a bag must satisfy some correlation conditions.
In our setting, we assume the distribution of the labels of
instances in a bag is arbitrary, but that these instances are
independent of each other given their labels.

3. Superset Label Learning Problem with
Independent Instances (SLL-I)
Let X be the instance space and Y = {1, 2, . . . , L} be the
finite label space. The superset space S is the powerset
of Y without the empty set: S = 2Y âˆ’ {âˆ…}. A â€œcompleteâ€ instance (x, y, s) âˆˆ X Ã— Y Ã— S is composed of its
features x, its true label y, and the label superset s. We
decompose D into the standard multiclass distribution Dxy
defined on X Ã— Y and the label set conditional distribution Ds (x, y) defined over S given (x, y). We assume that
P rsâˆ¼Ds (x,y) (y âˆˆ s) = 1, that is, the true label is always in
the label superset. Other labels in the superset will be called
distractor labels. Let Âµ(Â·) denote the probability measure
of a set; its subscript indicates
Denote a
 the distribution.
	n
sample of instances by z = (xi , yi , si ) i=1 , where each
instance is sampled from D independently. The size of the
sample is always n. Although the true label is included
in the training set in our notation, it is not visible to the
learner. Let I(Â·) denote the indicator function, which has
the value 1 when its argument is true and 0 otherwise.
The hypothesis space is denoted by H, and each h âˆˆ H :
X â†’ Y is a multiclass classifier. The expected classification error of h is defined as
ErrD (h) = E(x,y,s)âˆ¼D I(h(x) 6= y).

(1)

We use H to denote the set of hypotheses with error at
least , H = {h âˆˆ H : ErrD (h) â‰¥ }. The superset error
is defined as the event that the predicted label is not in the
superset: h(x) âˆˆ
/ s. The expected superset error and the
average superset error on set z are defined as
s
ErrD
(h)

=

Errzs (h)

=

E(x,y,s)âˆ¼D I(h(x) âˆˆ
/ s)
n
X
1
I(h(xi ) âˆˆ
/ si )
n i=1

(2)
(3)

It is easy to see that the expected superset error is always
no greater than the expected classification error.
For conciseness we often omit the word â€œexpectedâ€ or â€œaverageâ€ when referring these errors defined above. The

Learnability of Superset Label Learning Problem

meaning should be clear from how the error is calculated.
An Empirical Risk Minimizing (ERM) learner A for H is
n
a function, A : âˆªâˆž
n=0 (X Ã— S) 7â†’ H. We define the empirical risk as the average superset error on the training set.
The ERM learner for hypothesis space H always returns an
hypothesis h âˆˆ H with minimum superset error for training
set z.

Then when n > n0 (H, , Î´), ErrD (A(z)) <  with probability 1 âˆ’ Î´.
We follow the method of proving learnability of binary
classification to prove this theorem (Anthony & Biggs,
1997). Let Rn, be the set of all n-samples for which there
exists an -bad hypothesis h with zero empirical risk:
Rn, = {z âˆˆ (X Ã—YÃ—S)n : âˆƒh âˆˆ H , Errzs (h) = 0}.

A(z) = arg min Errzs (h)
hâˆˆH

Since the learning algorithm A can only observe the superset label, this definition of ERM is different from that of
multiclass classification. In the realizable case, there exists
h0 âˆˆ H such that ErrD (h0 ) = 0.
3.1. Small ambiguity degree condition
On a training set with label supersets, an hypothesis will
not be rejected by an ERM learner as long as its predictions are contained in the superset labels of the training
instances. If a distractor label always co-occurs with one
of the true labels under distribution Ds , there will be no
information to discriminate the true label from the distractor. On the contrary, if for any instance all labels except
the true label have non-zero probability of being missing
from the superset, then the learner always has some probability of rejecting an hypothesis if it predicts this instance
incorrectly. The ambiguity degree, proposed by Cour et al.
(2011), is defined as
Î³=

sup

P rsâˆ¼Ds (x,y) (` âˆˆ s).

(4)

(x,y)âˆˆX Ã—Y, `âˆˆY :
p(x,y)>0, `6=y

This is the maximum probability that some particular distractor label ` co-occurs with the true label y. If Î³ = 0, then
with probability one there are no distractors. If Î³ = 1, then
there exists at least one pair y and ` that always co-occur.
If a problem exhibits ambiguity degree Î³, then a classification error made on any instance will be detected (i.e., lie
outside the bag label) with probability at least 1 âˆ’ Î³:
P r(h(x) âˆˆ
/ s|h(x) 6= y, x, y) â‰¥ 1 âˆ’ Î³.
If an SSL-I problem exhibits Î³ < 1, then we say that it satisfies the small ambiguity degree condition. We prove that
this is sufficient for ERM learnability of the SSL-I problem.
Theorem 3.1 Suppose an SLL-I problem has ambiguity
2
, and suppose the
degree Î³, 0 â‰¤ Î³ < 1. Let Î¸ = log 1+Î³
Natarajan dimension of the hypothesis space H is dH . Define
n0 (H, , Î´) =
 


4
1
1
dH log(4dH ) + 2 log L + log
+ log + 1 ,
Î¸
Î¸
Î´

Essentially we need to show that P r(Rn, ) â‰¤ Î´.
The proof is composed of the following two lemmas.
Lemma 3.2 We introduce a testing set z0 of size n with
each instance drawn independently from distribution D,
and define the set Sn, to be the event that there exists a
hypothesis in H that makes no superset errors on training
set z but makes at least 2 classification errors on testing
set z0 .
n
Sn, = (z, z0 ) âˆˆ (X Ã— Y Ã— S)2n :
o
.
âˆƒh âˆˆ H , Errzs (h) = 0, Errz0 (h) â‰¥
2

Then P r (z, z0 ) âˆˆ Sn, â‰¥ 12 P r(z âˆˆ Rn, ) when n > 8 .
Proof This lemma is used in many learnability proofs.
Here we only give a proof sketch. Sn, is a subevent
of Rn, . We can apply the
 chain rule of probability to
write P r (z, z0 ) âˆˆ Sn, = P r (z, z0 ) âˆˆ Sn, |z âˆˆ
Rn, P r z âˆˆ Rn, . Let H(z) = {h âˆˆ H : Errzs (h) =
0} be the set of hypotheses with zero empirical risk on the
training sample. Then we have
P r ((z, z0 ) âˆˆ Sn, |z âˆˆ Rn, )
n

 o
= P r âˆƒh âˆˆ H âˆ© H(z), Errz0 (h) â‰¥
z âˆˆ Rn,
2


 
â‰¥ P r {h âˆˆ H âˆ© H(z), Errz0 (h) â‰¥ }z âˆˆ Rn,
2
In the last line, h is a particular hypothesis in the intersection. Since h has error at least , we can bound the probability via the Chernoff bound. When n > 8 , we obtain
P r (z, z0 ) âˆˆ Sn, | z âˆˆ Rn, > 12 which completes the
proof.
With Lemma 3.2, we can bound the probability of Rn, by
bounding the probability of Sn, . We will do this using
the technique of swapping training/testing instance pairs,
which is used in various proofs of learnability.
In the following proof, the sets z and z0 are expanded to
(x, y, s) and (x0 , y0 , s0 ) respectively when necessary. Let
the training and testing instances form n training/testing
pairs by arbitrary pairing. The two instances in each pair
are respectively from the training set and the testing set,

Learnability of Superset Label Learning Problem

and these two instances are both indexed by the pair index, which is indicated by a subscript. Define a group G
of swaps with size |G| = 2n . A swap Ïƒ âˆˆ G has an index
set JÏƒ âŠ† {1, . . . , n}, and it swaps the training and testing
instances in the pairs indexed by JÏƒ . We write Ïƒ as a superscript to indicate the result of applying Ïƒ to the training
Ïƒ
and testing sets, that is, Ïƒ(z, z0 ) = (zÏƒ , z0 ).
Lemma 3.3 If the hypothesis space H has Natarajan dimension dH and Î³ < 1, then
dH

P r(Sn, ) â‰¤ (2n)

2dH

L



nÎ¸
exp âˆ’
2

2n P r ((z, z0 ) âˆˆ Sn, )
X
=
E [P r ((z, z0 ) âˆˆ Sn, |x, y, x0 , y0 )]
ÏƒâˆˆG

X

Ïƒ

(x , y )
Ïƒ
Ïƒ
(x0 , y0 )

vÏƒ = 1

u u u e u
e e e e e
u u u u e u u e e e
| {z } | {z } | {z }
u1
u2
u3
Ïƒ

Ïƒ

Figure 1. A situation of (xÏƒ , yÏƒ , x0 , y0 ). Black dots represent
instances misclassified by h and circles represent instances correctly classified by h.

h
Sn,
,

Proof Since the swap does not change the measure of
(z, z0 ),

=

Ïƒ

E [P r (Ïƒ(z, z0 ) âˆˆ Sn, |x, y, x0 , y0 )]


h
P r Ïƒ(z, z0 ) âˆˆ Sn,
| x, y, x0 , y0

Â·
= I Errz0 Ïƒ (h) â‰¥
2

P r h(xÏƒi ) âˆˆ sÏƒi , 1 â‰¤ i â‰¤ n|xÏƒ , yÏƒ
n

Y
= I Errz0 Ïƒ (h) â‰¥
P r h(xÏƒi ) âˆˆ sÏƒi |xÏƒi , yiÏƒ .
2 i=1

ÏƒâˆˆG

"
= E

#
X

0

0

0

P r (Ïƒ(z, z ) âˆˆ Sn, |x, y, x , y )

(5)

ÏƒâˆˆG

The expectations are taken with respect to (x, y, x0 , y0 ).
The probability in the expectation comes from the randomness of (s, s0 ) given (x, y, x0 , y0 ).
Let H|(x, x0 ) be the set of hypothesis making different
h
for each hypothclassifications for (x, x0 ). Define set Sn,
esis h âˆˆ H as
n
o
h
Sn,
= (z, z0 ) : Errzs (h) = 0, Errz0 (h) â‰¥
2

There are u1 + vÏƒ wrongly-predicted instances in the training set. Since the true label is in the superset with probability one, while the wrong label appears in the superset
no greater
than Î³ by (4), we have

Qn with probability
Ïƒ
Ïƒ Ïƒ
u1 +vÏƒ
P
r
h(x
)
âˆˆ
s
|x
â‰¤
Î³
.
i
i
i
i=1

By the union bound, we have
X

P r (Ïƒ(z, z0 ) âˆˆ Sn, |x, y, x0 , y0 )

Now for a single swap Ïƒ, we have the bound

ÏƒâˆˆG

â‰¤

X

X



h 
P r Ïƒ(z, z0 ) âˆˆ Sn,
x, y, x0 , y0

For a pair of training/testing sets (x, y, x0 , y0 ), let u1 , u2
and u3 represent the number of pairs for which h classifies
both incorrectly, one incorrectly, and both correctly. Let
vÏƒ , 0 â‰¤ vÏƒ â‰¤ u2 , be the number of wrongly-predicted
instances swapped into the training set (xÏƒ , yÏƒ ). One such
situation is shown in Figure 1. There are u1 + u2 âˆ’ vÏƒ
wrongly-predicted instances in the testing set. The error
condition Errz0 Ïƒ (h) â‰¥ 2 is equivalent to u1 + u2 âˆ’ vÏƒ â‰¥


2 n, which always indicates u1 + u2 â‰¥
 2 n. So we have


I Errz0 Ïƒ (h) â‰¥ 2 â‰¤ I u1 + u2 â‰¥ 2 n .

(6)

hâˆˆH|(x,x0 ) ÏƒâˆˆG



By Natarajan (1989), H|(z, z0 ) â‰¤ (2n)dH L2dH .
P
0
The only work left is to bound
ÏƒâˆˆG P r Ïƒ(z, z ) âˆˆ
h
0
0
Sn, |x, y, x , y , and this part is our contribution.
Here is our strategy. We first fix (x, y,x0 , y0 ) and Ïƒ and
h
bound P r Ïƒ(z, z0 ) âˆˆ Sn,
| x, y, x0 , y0 with the ambiguity degree assumption. Then we find an upper bound of
the summation over Ïƒ. Start by expanding the condition in

h
P r Ïƒ(z, z0 ) âˆˆ Sn,
| x, y, x0 , y0



â‰¤ I u1 + u2 â‰¥

  u1 +vÏƒ
n Î³
2

(7)

Let us sum up (7) over Ïƒ. Any swap Ïƒ can freely switch
instances in u1 + u3 without changing the bound in (7),
and choose from the u2 pairs vÏƒ to switch. For each value
0 â‰¤ j â‰¤ u2 , there are 2u1 +u3 uj2 swaps that have vÏƒ = j.

Learnability of Superset Label Learning Problem

Therefore,
X

I u1 + u2 â‰¥

ÏƒâˆˆG

attention in the construction: how to relate the classification errors of the two tasks, and how to specify the hypothesis space of the binary classification task and obtain its
VC-dimension.

  u1 +vÏƒ
n Î³
2

u2  
  u1 +u3 X
u2 u1 +j
n 2
Î³
2
j
j=0
 
= I u1 + u2 â‰¥ n 2nâˆ’u2 Î³ u1 (1 + Î³)u2
2

u
  n u1 1 + Î³ 2
= I u1 + u2 â‰¥ n 2 Î³
2
2

â‰¤ I u1 + u2 â‰¥

When (x, y, x0 , y0 ) and h make u1 = 0 and u2 = 2 n,
n/2
the right side reaches its maximum 2n 1+Î³
, which is
2
2n eâˆ’nÎ¸/2 with the definition of Î¸ in Theorem 3.1. Applying this to (6) and (5), completes the proof.
Proof of Theorem 3.1. By
combining
the
results of the two lemmas, we have P (Rn, ) â‰¤
2(dH +1) ndH L2dH exp(âˆ’ nÎ¸
Bound this with Î´ on
2 ).
a log scale to obtain
(dH + 1) log 2 + dH log n + 2dH log L âˆ’

Î¸n
â‰¤ log Î´
2

By bounding log n with (log( 4dÎ¸H ) âˆ’ 1) + 4dÎ¸H n, we get a
linear form for n. Then we solve for n to obtain the result.

Remark Theorem 3.1 includes the multiclass problem as
a special case. When Î³ = 0 and Î¸ = log 2, we get the same
sample complexity as the multiclass classification problem.
(See Theorem 6 in Daniely et al. (2011).)
The small ambiguity degree condition is strong. A good
aspect of our result is that we only require that H has finite Natarajan dimension to ensure that an ERM learning
finds a good hypothesis. However, the result is not general enough. Here is an example where the small ambiguity degree condition is not satisfied but the problem is
still learnable. Consider a case where the true label is `1 ,
but the label superset always contains a distractor label `2 ;
otherwise the superset contains only the true label. Such
a distribution of superset labels certainly does not satisfy
the small ambiguity condition, but if no hypothesis in H
classifies `1 as `2 , then the problem is still learnable by an
ERM learner. Though the example is not very realistic, it
suggests that there should be a more general condition for
ERM learnability.
3.2. A general condition for learnability of SLL-I
We can obtain a more general condition for ERM learnability by constructing a binary classification task from the
superset label learning task. Two key points need special

We first construct a binary classification problem from the
SLL-I problem. Given an instance (x, s), the binary classifier needs to predict whether the label of x is outside of
s, that is, predict the value of I(yx âˆˆ
/ s). If (x, s) is from
the distribution D, then â€œ0â€ is always the correct prediction. Let FH be the space of these binary classifiers. FH is
induced from H as follows:
FH = {fh : fh (x, s) = I(h(x) âˆˆ
/ s), h âˆˆ H}.
Though the inducing relation is a surjection from H to FH ,
we assume the subscript h in the notation fh can be any
h âˆˆ H inducing fh . The binary classification error of fh is
the superset error of h in the SLL-I problem.
Denote the ERM learner of the binary classification problem by As . The learner As actually calls A, which returns
an hypothesis h? and then As returns the induced hypothesis fh? . In the realizable case, both h? and fh? have zero
training error on their respective classification tasks.
A sufficient condition for learnability of an SLL-I problem
is that any hypothesis with non-zero classification error will
cause a superset error with relatively large probability Let
Î· be defined by
Î·=

s
ErrD
(h)
.
hâˆˆH:ErrD (h)>0 ErrD (h)

inf

(8)

Define the tied error condition as Î· > 0. Then we can
bound the multiclass classification error by bounding the
superset error when this condition holds.
We need to find the VC-dimension of FH first. It can be
bounded by the Natarajan dimension of H.
Lemma 3.4 Denote the VC-dimension of FH as dF and
the Natarajan dimension of H is dH , then
dF

< 4 dH (log dH + 2 log L)

Proof There is a set of dF instances that can be shattered
by FH â€”that is, there are functions in FH that implement
each of the 2dF different ways of classifying these dF instances. Any two different binary classifications must be
the result of two different label predictions for these dF instances. According to Natarajanâ€™s (1989) original result on
Natarajan dimension, there are at most ddFH L2dH different
multiclass classifications on these instances. Therefore,
2dF â‰¤ ddFH L2dH .
Taking the logarithm of both sides gives us
dF log 2 â‰¤ dH log dF + 2dH log L.

Learnability of Superset Label Learning Problem

By bounding log dF above by log dH +
dF log 2 â‰¤ dH log dH +

dF
edH ,

we get

dF
+ 2dH log L.
e

By observing that (log 2 âˆ’ eâˆ’1 )âˆ’1 < 4, we obtain the result.
Now we can bound the multiclass classification error by
bounding the superset error.
Theorem 3.5 Assume Î· > 0 and assume H has a finite
Natarajan dimension dH , then A returns an hypothesis
with error less than  with probability at least 1 âˆ’ Î´ when
the training set has size n > n1 (H, Î´, ), which is defined
as
n1 (H, Î´, ) =

 
 
12
2
4
4dH (log dH + 2 log L) log
+ log
.
Î·
Î·
Î´
Proof Learner A returns hypothesis h? âˆˆ H with
s
ErrD
(h? ) = 0. The corresponding binary hypothesis
fh? âˆˆ FH makes no superset error on the training data.
By Lemma (3.4), n1 (H, Î´, ) â‰¥ ns1 (FH , Î´, Î·), where
ns1 (H, Î´, Î·)

4
=
Î·


 
 
12
2
dF log
+ log
.
Î·
Î´

When n > n1 (H, Î´, ) â‰¥ ns1 (FH , Î´, Î·), by Theorem 8.4.1
s
in Anthony & Biggs (1997), ErrD
(fh? ) < Î· with probability at least 1 âˆ’ Î´. Therefore ErrD (h? ) <  with probability at least 1 âˆ’ Î´.
The necessary condition for the learnability of the SLL
problem is that no hypothesis have non-zero multiclass
classification error but have zero superset error. Otherwise,
no training data can reject such an incorrect hypothesis.
Theorem 3.6 If there exists an hypothesis h âˆˆ H such that
s
ErrD (h) > 0 and ErrD
(h) = 0, then the SLL-I problem
is not learnable by an ERM learner.
The gap between the general sufficient condition and the
necessary condition is small.
Letâ€™s go back and check the small ambiguity degree condition against the tied error condition. The condition Î³ < 1
indicates P r(h(x) âˆˆ
/ s|x, y) â‰¥ (1 âˆ’ Î³)P r(h(x) 6= y|x, y).
By taking the expectation of both sides, we obtain the tied
error condition Î· â‰¥ 1âˆ’Î³ > 0. This also shows that the tied
error condition is more general. The tied error condition is
less practical, but it can be used as a guideline to find more
sufficient conditions.

4. Superset Label Learning Problem with
Bagged Training Data (SLL-B)
The second type of superset label problem arises in multiinstance multilabel learning (Zhou & Zhang, 2006). The
data are given in the form of i.i.d. bags. Each bag contains
multiple instances, which are generally not independent of
each other. The bag label set consists of the labels of these
instances. In this form of superset label learning problem,
the bag label set provides the label superset for each instance in the bag. An ERM learning algorithm for SSL-I
can naturally be applied here regardless of the dependency
between instances. In this section, we will show learnability of this SSI-B problem.
We assume each bag contains r instances, where r is fixed
as a constant. The space of labeled instances is still X Ã— Y.
The space of sets of bag labels is also S. The space of
bags is B = (X Ã— Y)r Ã— S. Denote a bag of instances
as B = (X, Y, S), where (X, Y ) âˆˆ (X Ã— Y)r and S âˆˆ S.
Note that although (X, Y ) is written as a vector of instances
for notational convenience, the order of these instances is
not essential to any conclusion in this work. We assume
that each bag B = (X, Y, S) is sampled from the distribution DB . The label set S consists of labels in Y in
the MIML setting, but the following analysis still applies
when S contains extra labels. The learner can only observe
(X, S) during training, whereas the learned hypothesis is
tested on (X, Y ) during testing. The boldface B always
denotes a set with m independent bags drawn from the distribution DB .
The hypothesis space is still denoted by H. The expected
classification error of hypothesis h âˆˆ H on bagged data is
defined as
ErrDB (h)

= EBâˆ¼DB

r
h1 X

r

i
I(h(Xi ) 6= Yi ) (9)

i=1

The expected superset error and the average superset error
on the set B are defined as
s
ErrD
B (h)

s
ErrB
(h)

= EBâˆ¼DB
=

r
h1 X

r

i
I(h(Xi ) âˆˆ
/ Si ) (10)

i=1

r
1 XX
I(h(Xi ) âˆˆ
/ Si ).
mr
i=1

(11)

BâˆˆB

The ERM learner A for hypothesis space H returns the hypothesis with minimum average superset error.
4.1. The general condition for learnability of SLL-B
Using a technique similar to that employed in the last section, we convert the SLL-B problem into a binary classification problem over bags. By bounding the error on the

Learnability of Superset Label Learning Problem

binary classification task, we can bound the multiclass classification error of the hypothesis returned by A. Let GH be
the hypothesis space for binary classification of bags. GH
is induced from H as follows:
GH = {gh : gh (X, S) =

probability at least 1 âˆ’ Î´ when the training set B has size
m and m > m0 (H, Î´, ),
m0 (H, Î´, ) =

 
 
4
12
2
4dH log(dH rL2 ) log
+ log
Î»
Î»
Î´

max I(h(Xi ) âˆˆ
/ Si ), h âˆˆ H}.

1â‰¤iâ‰¤r

Every hypothesis gh âˆˆ GH is a binary classifier for bags
that predicts whether any instance in the bag has its label
outside of the bag label set. Since 0 is the correct classification for every bag from the distribution DB , we define
the expected and empirical bag error of gh as
B
= EBâˆ¼DB max I(h(Xi ) âˆˆ
/ Si ) (12)
ErrD
B (gh )
1â‰¤iâ‰¤r

B
ErrB
(gh )

=

1 X
max I(h(Xi ) âˆˆ
/ Si ). (13)
1â‰¤iâ‰¤r
m
BâˆˆB

It is easy to check the following relation between
B
s
ErrD
B (gh ) and ErrD B (h).
s
B
s
ErrD
B (h) â‰¤ ErrD B (gh ) â‰¤ rErrD B (h).

Proof With the relation in (14), we have ErrDB (h) â‰¤
1
B
?
Î» ErrD B (gh ). The hypothesis h returned by A has zero
superset error on the training bags, thus gh? has zero bag
B
error. When m > m0 , we have ErrD
B (gh ) < Î» with
probability at least 1 âˆ’ Î´ by Theorem 8.4.1 in Anthony &
Biggs (1997). Hence, we have ErrDB (h) <  with probability at least 1 âˆ’ Î´.
4.2. The no co-occurring label condition
We now provide a more concrete sufficient condition for
learnability with a principle similar to the ambiguity degree. It generally states that any two labels do not always
co-occur in bag label sets.
First we make an assumption about the data distribution.

(14)
Assumption 4.3 (Conditional independence assumption)

Denote by AB the ERM learner for this binary classification problem. In the realizable case, if the hypothesis gh? is
returned by AB , then gh? has no binary classification error
on the training bags and h? makes no superset error on any
instance in training data.
To bound the error for the binary classification problem, we
need to bound the VC-dimension of GH .
Lemma 4.1 Denote the VC-dimension of GH by dG and
the Natarajan dimension of H by dH , then
dG

<

4 dH (log dH + log r + 2 log L).

The proof of this lemma is almost the same as Lemma
3.4. The only difference is that different classifications of
dG bags are caused by different classifications of rdG instances.
The tied error condition for the SLL-B problem is Î» > 0,
Î»=

s
ErrD
B (h)
.
hâˆˆH:ErrD (h)>0 ErrD B (h)

inf

With the tied error condition, we can give the sample
complexity of learning a multiclass classifier with multiinstance bags.
Theorem 4.2 Suppose the tied error condition holds, Î» >
0, and assume H has finite Natarajan dimension dH , then
A(B) returns an hypothesis with error less than  with

P r(X|Y ) =

r
Y

P r(Xi |Yi )

i=1

This assumption states that the covariates of an instance are
determined by its label only. With this assumption, a bag is
sampled in the following way. Instance labels Y of the bag
are first drawn from the marginal distribution D(Yi ), then
for each Yi , 1 â‰¤ i â‰¤ r, Xi is sampled from distribution
Dx (Yi ) independently.
Remark We can compare our assumption of bag distribution with assumptions in previous work. Blum & Kalai
(1998) assume that all instances in a bag are independently
sampled from the instance distribution. As stated by Sabato
& Tishby (2009), this assumption is too strict for many applications. In their work as well as in Wang & Zhou (2012),
the instances in a bag are assumed to be dependent, and
they point out that a further assumption is needed to get a
low error classifier. In our assumption above, we also assume dependency among instances in the same bag. The
dependency only comes from the instance labels. This assumption is roughly consistent with human descriptions of
the world. For example, in the description â€œa tall person
stands by a red vehicleâ€, the two labels â€œpersonâ€ and â€œvehicleâ€ capture most of the correlation, while the detailed descriptions of the person and the vehicle are typically much
less correlated.

Learnability of Superset Label Learning Problem

The distribution DB of bags can be decomposed into DY
and Dx (`), ` âˆˆ Y. Here DY is a distribution over Y r . For
each class ` âˆˆ Y, Dx (`) is a distribution over the instance
space X . As a whole, the distribution of X is denoted as
DX (Y ).
With Assumption 4.3, we propose the no co-occurring label condition in the following theorem.

Î±B =

Theorem 4.4 Define
Î±=

inf

(`,`0 )âˆˆI

1
min
m (`,`0 )âˆˆI

X

I(` âˆˆ S)I(`0 âˆˆ
/ S),

(X,Y,S)âˆˆB

0

E(X,Y,S)âˆ¼DB [I(` âˆˆ S)I(` âˆˆ
/ S)]

where I is the index set {(`, `0 ) : `, `0 âˆˆ Y, ` 6= `0 }. Suppose Assumption 4.3 holds and Î± > 0, then
s
ErrD
Î±
B (h)
â‰¥ .
r
hâˆˆH: ErrDB (h)>0 ErrD B (h)

inf

Proof Denote the row-normalized confusion matrix of
each hypothesis h âˆˆ H as Uh âˆˆ [0, 1]LÃ—L .
Uh (`, `0 ) = P rxâˆ¼Dx (`) (h(x) = `0 ), 1 â‰¤ `, `0 â‰¤ L
The entry (`, `0 ) of Uh means a random instance from class
` is classified as class `0 by h with probability Uh (`, `0 ).
Each row of Uh sums to 1. Denote k(Y, `) as the number
of occurrences of label ` in Y .
Then the error of h can be expressed as
ErrDB (h) =
=

r X
i
hX
1
EY
Uh (Yi , `0 ) | Y
r
i=1 `0 6=Yi
h X
i
1
EY
k(Y, `)Uh (`, `0 ) | Y
r
0
(`,` )âˆˆI

â‰¤
=

1 X
rUh (`, `0 )
r
(`,`0 )âˆˆI
X
Uh (`, `0 ).
(`,`0 )âˆˆI

The expected superset error of h can be computed as
s
ErrD
B (h)
r X
h
i
X
1
=
EY
I(`0 âˆˆ
/ Si )Uh (Yi , `0 ) | Y
r
i=1 `0 6=Yi
h X
i
1
=
EY
k(Y, `)I(`0 âˆˆ
/ Si )Uh (`, `0 ) | Y
r
0
(`,` )âˆˆI

â‰¥

With the no co-occurring label condition that Î± > 0, the
remaining learnability requirement is that the hypothesis
space H have finite Natarajan dimension. A merit of the
condition of Theorem 4.4 is that it can be checked on the
training data with high confidence. Suppose we have a
training data B. Then the empirical estimate of Î± is

1 X
Î±Uh (`, `0 ).
r
0
(`,` )âˆˆI

These two inequalities hold for any h âˆˆ H, so the theorem
is proved.

If Î±B > 0, then by the Chernoff bound, we can obtain a
lower bound on Î± > 0 with high confidence. Conversely,
if Î±B = 0 for a large training set, then it is quite possible
that Î± is very small or even zero.

5. Conclusion and Future Work
In this paper, we analyzed the learnability of an ERM
learner on two superset label learning problems: SLL-I (for
independent instances) and SLL-B (for bagged instances).
Both problems can be learned by the same learner regardless the (in)dependency among the instances.
For both problems, the key to ERM learnability is that the
expected classification error of any hypothesis in the space
can be bounded by the superset error. If the tied error condition holds, then we can construct a binary classification
problem from the SLL problem and bound the expected error in the binary classification problem. By constructing a
relationship between the VC-dimension and the Natarajan
dimension of the original problem, the sample complexities
can be given in terms of the Natarajan dimension.
For the SLL-I problem, the condition of small ambiguity
degree guarantees learnability for problems with hypothesis spaces having finite Natarajan dimension. This condition leads to lower sample complexity bounds than those
obtained from the general tied error condition. The sample complexity analysis with this condition generalizes the
analysis of multiclass classification. For the SLL-B problem, we propose a reasonable assumption for the distribution of bags. With this assumption, we identify a practical
condition stating that no two labels always co-occur in bag
label sets.
There is more to explore in theoretical analysis of the superset label learning problem. Analysis is needed for the
agnostic case. Another important issue is to allow noise in
the training data by removing the assumption that the true
label is always in the label superset.

Acknowledgments
We thank Qi Lou, Wei Wang and Shell Hu for useful discussions.

Learnability of Superset Label Learning Problem

References
Aly, M.
2005.

Survey on multi-class classification methods,

Anthony, M. and Biggs, N. Computational Learning Theory. Cambridge University Press, 1997.
Ben-David, S., Cesa-Bianchi, N., Haussler, D., and Long,
P. Characterizations of learnability for classes of {0, . . . ,
n}-valued functions. Journal of Computer and System
Sciences, 50(1):74â€“86, 1995.
Blum, A. and Kalai, A. A note on learning from multipleinstance examples. Machine Learning, 30(1):23â€“29,
1998.
Briggs, F., Fern, X. Z., and Raich, R. Rank-loss support instance machines for MIML instance annotation. In Proceedings of the 18th International Conference on Knowledge Discovery and Data Mining (KDDâ€™12), pp. 534â€“
542, 2012.
Cour, T., Sapp, B., and Taskar, B. Learning from partial
labels. Journal of Machine Learning Research, 12(May):
1501â€“1536, 2011.
Daniely, A., Sabato, S., Ben-David, S., and ShalevShwartz, S. Multiclass learnability and the ERM principle. In The 24rd Annual Conference on Learning Theory
(COLTâ€™11), pp. 207â€“232, 2011.
Jin, R. and Ghahramani, Z. Learning with multiple labels.
In Advances in Neural Information Processing Systems
15 (NIPSâ€™02), pp. 897â€“904, 2002.
Liu, L.-P. and Dietterich, T. G. A conditional multinomial
mixture model for superset label learning. In Advances
in Neural Information Processing Systems 25 (NIPSâ€™12),
pp. 557â€“565, 2012.
Long, P. M. and Tan, L. PAC learning axis-aligned rectangles with respect to product distributions from multipleinstance examples. Machine Learning, 30(1):7â€“21,
1998.
Mukherjee, I. and Schapire, R. E. A theory of multiclass
boosting. Journal of Machine Learning Research, 14
(Feb):437â€“497, 2013.
Natarajan, B.K. On learning sets and functions. Machine
Learning, 4(1):67â€“97, 1989.
Nguyen, N. and Caruana, R. Classication with partial labels. In Proceedings of the 14th International
Conference on Knowledge Discovery and Data Mining(KDDâ€™08), pp. 551â€“559, 2008.

Sabato, S. and Tishby, N. Homogeneous multi-instance
learning with arbitrary dependence. In Proceeding
of the 22nd Annual Conference on Learning Theory
(COLTâ€™09), pp. 93â€“104, 2009.
Wang, W. and Zhou, Z.-H. Learnability of multi-instance
multi-label learning. Chinese Science Bulletin, 57(19):
2488â€“2491, 2012.
Zhang, M.-L. Disambiguation-free partial label learning.
In Proceedings of the 14th SIAM International Conference on Data Mining (SDMâ€™14), pp. 37â€“45, 2014.
Zhou, Z.-H. and Zhang, M.-L. Multi-instance multilabel
learning with application to scene classification. In Advances in Neural Information Processing Systems 19
(NIPSâ€™06), pp. 1609â€“1616, 2006.
Zhou, Z.-H., Zhang, M.-L., Huang, S.-J., and Li, Y.-F.
Multi-instance multi-label learning. Artificial Intelligence, 176(1):2291â€“2320, 2012.

