Convex Calibrated Surrogates for Hierarchical Classification

Harish G. Ramaswamy
Indian Institute of Science, Bangalore, INDIA

HARISH GURUP @ CSA . IISC . ERNET. IN

Ambuj Tewari
University of Michigan, Ann Arbor, USA

TEWARIA @ UMICH . EDU

Shivani Agarwal
Indian Institute of Science, Bangalore, INDIA

SHIVANI @ CSA . IISC . ERNET. IN

Abstract
Hierarchical classification problems are multiclass supervised learning problems with a predefined hierarchy over the set of class labels. In
this work, we study the consistency of hierarchical classification algorithms with respect to
a natural loss, namely the tree distance metric
on the hierarchy tree of class labels, via the usage of calibrated surrogates. We first show that
the Bayes optimal classifier for this loss classifies an instance according to the deepest node
in the hierarchy such that the total conditional
probability of the subtree rooted at the node is
greater than 21 . We exploit this insight to develop
new consistent algorithm for hierarchical classification, that makes use of an algorithm known
to be consistent for the “multiclass classification
with reject option (MCRO)” problem as a subroutine. Our experiments on a number of benchmark datasets show that the resulting algorithm,
which we term OvA-Cascade, gives improved
performance over other state-of-the-art hierarchical classification algorithms.

1. Introduction
In many practical applications of the multiclass classification problem the class labels live in a pre-defined hierarchy.
For example, in document classification the class labels are
topics and they form topic hierarchies; in computational biology the class labels are protein families and they are also
best organized in a hierarchy. See Figure 1 for an example hierarchy used in mood classification of speech. Such
problems are commonly known in the machine learning literature as hierarchical classification.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

Speech

Active

Anger

Non-Active

Median

Gladness

Fear

Neutral

Passive

Sadness

Boredom

Figure 1. Speech based mood classification hierarchy in the
Berlin dataset (Burkhardt et al., 2005) used by Xiao et al. (2007)

Hierarchical classification has been the subject of many
studies (Wang et al., 1999; Sun & Lim, 2001; Cai & Hofmann, 2004; Dekel et al., 2004; Rousu et al., 2006; CesaBianchi et al., 2006a;b; Wang et al., 2011; Gopal et al.,
2012; Babbar et al., 2013; Gopal & Yang, 2013). For a
detailed review and more references we refer the reader to
a survey on hierarchical classification by Silla Jr. & Freitas
(2011).
The label hierarchy has been incorporated into the problem
in various ways in different approaches. The most prevalent and technically appealing approach is to involve the
hierarchy in the final evaluation metric and design an algorithm that does well on this evaluation metric. We shall
work in the setting where class labels are single nodes in a
tree, and use the very natural evaluation metric that penalizes predictions according to the tree-distance between the
prediction and truth (Sun & Lim, 2001; Cai & Hofmann,
2004; Dekel et al., 2004).
While hierarchical classification problems are actively
studied, there is a gap between theory and practice – even
basic statistical properties of hierarchical classification algorithms have not been examined in depth. This paper addresses this gap and its main contributions are summarized
below:

Convex Calibrated Surrogates for Hierarchical Classification

• We show that the Bayes optimal classifier for the treedistance loss classifies an instance according to the
deepest node in the hierarchy such that the total conditional probability of the subtree rooted at the node is
greater than 21 .
• We reduce the problem of finding the Bayes optimal classifier for the tree-distance loss to the problem
of finding the Bayes optimal classifier for multiclass
classification with reject option (MCRO) problem.
• We construct a convex optimization based consistent
algorithm for the tree-distance loss based on the above
reduction and observe that in one particular instantiation called the OvA-cascade, this optimization problem can be solved only using binary SVM solvers.
• We run the OvA-cascade algorithm on several benchmark datasets and demonstrate improved performance.

2. Preliminaries
Let the instance space be X , and let Y = [n] = {1, . . . , n}
be a finite set of class labels. Let H = ([n], E, W ) be a tree
over the class labels, with edge set E, and positive, finite
edge lengths for the edges in E given by W . Let the root
node be r ∈ [n]. Let loss function `H : [n] × [n]→R+ be
`H (y, y 0 ) = Shortest path length in H between y and y 0 .
We call this the H-distance loss (or simply tree-distance
loss). Given training examples (X1 , Y1 ), . . . , (Xm , Ym )
drawn i.i.d. from a distribution D on X × Y, the goal is
to learn a prediction model g : X →[n] with low expected
`H -regret defined as
H
R`D [g]

H

= E[` (Y, g(X))] −

inf

H

g 0 :X →[n]

inf

H

R`D [Υ ◦ f ] ≤ ξ · Rψ
D [f ],

E[ψ(Y, f 0 (X))] .

(1)

where ξ > 0 is a constant. A surrogate and a predictor (ψ, Υ), satisfying such a bound, which we call a
(ψ, `H , Υ)-excess risk transform,2 would immediately give
an algorithm consistent w.r.t. `H from an algorithm consistent w.r.t. ψ. We also say that such a (ψ, Υ) is calibrated
(Zhang, 2004; Ramaswamy & Agarwal, 2012) w.r.t. `H .
2.1. Conventions and Notations
∆n denotes
the probability simplex in Rn : ∆n = {p ∈
P
n
R+ : i pi = 1}.
For the tree H = ([n], E, W ) with root r we define the
following several objects. For every y ∈ [n] define the sets
D(y), C(y), U (y) as follows:

E[` (Y, g (X))] ,

However, minimizing the discrete `H -regret directly is
computationally difficult; therefore one uses instead a surrogate loss function ψ : [n] × Rd →R+ for some d ∈
Z+ and learns a model f : X →Rd by minimizing (approximately, based on the training sample) the ψ-error
E(X,Y )∼D [ψ(Y, f (X))]. Predictions on new instances x ∈
X are then made by applying the learned model f and mapping back to predictions in the target space [n] via some
mapping Υ : Rd →[n], giving g(x) = Υ(f (x)). Let the
ψ-regret of a function f : X →Rd be
f 0 :X →Rd

We seek a surrogate ψ : [n] × Rd →R+ for some d ∈ Z+
and a predictor Υ : Rd →[n] such that ψ is convex in its second argument and satisfies a bound of the following form
holding for all f : X →Rd and distributions D

0

where expectations are over (X, Y ) ∼ D. Ideally, one
wants the `H -regret of the learned model to be close to zero.
An algorithm which when given a random training sample
as above produces a (random) model hm : X →T is said to
be consistent w.r.t. `H if the `H -regret of the learned model
gm converges in probability to zero.

Rψ
D [f ] = E[ψ(Y, f (X))] −

Under suitable conditions, algorithms that approximately
minimize the ψ-error based on a training sample are known
to be consistent with respect to ψ, i.e. the ψ-regret of
the learned model f approaches zero with larger training
data.1 Also, if ψ is convex in its second argument, the ψerror minimization problem becomes a convex optimization problem and can be solved efficiently.

D(y)

= Set of descendants of y including y

P (y)

= Parent of y

C(y)

= Set of children of y

U (y)

= Set of ancestors of y, not including y.

For all y ∈ [n], define the level of y denoted by lev(y), and
the mapping Sy : ∆n →[0, 1] as follows:
lev(y)
Sy (p)

= |U (y)|
X
=
pi .
i∈D(y)

Let the height of the tree be h = maxy∈[n] lev(y). Define
1

For example, an algorithm consistent w.r.t. ψ can be obtained
by minimizing the regularized empirical ψ-risk over an RKHS
function class with Gaussian kernel and a regularization parameter approaching 0 with increasing sample size.
2
An inequality which upper bounds the `H -regret in terms of
a function ξ of the ψ-regret, with ξ(0) = 0 and ξ continuous at 0
would also qualify to be an excess risk transform.

Convex Calibrated Surrogates for Hierarchical Classification

the sets N=j , N≤j and scalars αj , βj for 0 ≤ j ≤ h as:
N=j

= {y ∈ [n] : lev(y) = j}

N≤j

= {y ∈ [n] : lev(y) ≤ j}

αj

=

βj

=

max `H (y, P (y)).

y∈N=j

For integers 0 ≤ j ≤ h define the function ancj :
[n]→N≤j and Aj : ∆n →∆nj such that for all y ∈
[n], y 0 ∈ [nj ],
=

Ajy0 (p)

=

(
y
ancestor of y at level j
X
pi

if lev(y) ≤ j
otherwise

i∈[n]:ancj (i)=y 0

=

(
py
if lev(y 0 ) < j
.
Sy (p) if lev(y 0 ) = j

Note that in all the above definitions the only terms that
depend on the edge lengths W are the scalars αj and βj .

3. Bayes Optimal Classifier for the
Tree-Distance Loss
In this section we characterize the Bayes optimal classifier minimizing the expected tree-distance loss. We show
that such a predictor can be viewed as a ‘greater than 12
conditional probability subtree detector’. We then design a
scheme for computing this prediction based on this observation.
The following theorem is the key result of this section. Figure 2 gives an illustration for this theorem.
Theorem 1. Let H = ([n], E, W ) and let `H : [n] ×
[n]→R+ be the tree-distance loss for the tree H. For
x ∈ X , let p(x) ∈ ∆n be the conditional probability of the
label given the instance x. Then there exists a g ∗ : X →[n]
such that for all x ∈ X the following holds:
(a) Sg∗ (x) (p(x)) ≥

p2 = 0.2
S2 (p) = 0.3

1

2

3

p3 = 0
S3 (p) = 0.7

max `H (y, y 0 )

y,y 0 ∈N=j

By reordering the classes we ensure that lev is a nondecreasing function and hence we always have that N≤j =
[nj ] for some integers nj and r = 1.

ancj (y)

p1 = 0
S1 (p) = 1

1
2

(b) Sy (p(x)) ≤ 12 , ∀y ∈ C(g ∗ (x)) .
Also, g ∗ is a Bayes optimal classifier for the tree distance
loss, i.e.
H
R`D [g ∗ ] = 0 .

p6 = 0
S6 (p) = 0.1
p5 = 0
p4 = 0.1
S4 (p) = 0.1 S5 (p) = 0

4

5

8

6

7

9

10

p7 = 0.2
S7 (p) = 0.6

11

p5 = 0
p11 = 0.2
p10 = 0.2
p4 = 0.1
S4 (p) = 0.1 S5 (p) = 0 S10 (p) = 0.2 S11 (p) = 0.2

Figure 2. An example tree and an associated conditional probability vector p(x) for some instance x, along with S(p(x)). The
Bayes optimal prediction is shaded here.

For any instance x, with conditional probability p ∈ ∆n ,
Theorem 1 says that predicting y ∈ [n] that has the largest
level and has Sy (p) ≥ 21 is optimal. Surprisingly, this does
not depend on the edge lengths W .
Theorem 1 suggests the following scheme to find the optimal prediction for a given instance, with conditional probability p:
1. For each j ∈ {1, 2, . . . , h} create a multiclass problem instance with the classes being elements of
N≤j = [nj ], and the probability associated with each
class in y ∈ N≤j is equal to Ajy (p), i.e. py if
lev(y) < j and equal to Sy (p) if lev(y) = j.
2. For each multiclass problem j ∈ {1, 2, . . . , h}, if
there exists a class with probability mass at least 12
assign it to vj∗ , otherwise let vj∗ = ⊥.
3. Find the largest j such that vj∗ 6= ⊥ and return the
corresponding vj∗ , or return the root 1 if vj∗ = ⊥ for
all j ∈ [h].
We will illustrate the above procedure for the example in
Figure 2.
Example 1. From Figure 2 we have that h = 3. The three
induced multiclass problems are given below.
1. n1 = 3, and the class probabilities are given as
1
∗
10 [0, 3, 7]. Clearly, v1 = 3.
2. n2 = 7, and the class probabilities are given as
1
∗
10 [0, 2, 0, 1, 0, 1, 6]. Clearly v2 = 7.
3. n3 = 11, and the class probabilities are given as
1
∗
10 [0, 2, 0, 1, 0, 0, 2, 1, 0, 2, 2]. Clearly, v3 = ⊥.
And hence the largest j such that vj∗ 6= ⊥ is 2, and the
scheme returns v2∗ = 7.

Convex Calibrated Surrogates for Hierarchical Classification

The reason such a scheme as the one above is of interest to
us is that the second step in the above scheme exactly corresponds to the Bayes optimal classifier for the abstain loss,
the evaluation metric used in the MCRO problem, which
we briefly explain in the next section.

4. Multiclass Classification with Reject
Option
In some multiclass problems like medical diagnosis, it is
better to abstain from predicting on instances where the
learner is uncertain rather than predicting the wrong class.
This feature can be incorporated via an evaluation metric
called the abstain loss, and designing algorithms that perform well on this evaluation metric instead of the standard
zero-one loss. The n-class abstain loss `?,n : [n] × ([n] ∪
{⊥})→R+ , (Ramaswamy et al., 2015) is defined as

0
0

1 if y 6= y and y 6= ⊥
?,n
0
.
` (y, y ) = 21 if y 0 = ⊥


0
0 if y = y
It can be seen that the Bayes optimal risk for the abstain
loss is attained by the function g ∗ : X →([n] ∪ {⊥}) given
by
(
argmaxy∈[n] py (x) if maxy∈[n] py (x) ≥ 12
,
g ∗ (x) =
⊥
otherwise

Theorem 2 ((Ramaswamy et al., 2015)). Let n ∈ N and
τ ∈ (−1, 1). Let D be any distribution over X × [n]. Then,
for all f : X →Rn
?,n

`
RD
[ΥOvA,n
◦ f] ≤
τ

In the next section we use such surrogates calibrated with
the abstain loss as a black box to construct calibrated surrogates for the tree-distance loss.

5. Cascade Surrogate for Hierarchical
Classification
In this section we construct a template surrogate ψ cas and
template predictor Υcas based on the scheme in Section 3,
and is constituted of simpler surrogates ψ j and predictors
Υ?j . We then give a (ψ cas , `H , Υcas )-excess risk transform
assuming the existence of abstain loss excess risk transforms for the component surrogates and predictors, i.e.
(ψ j , `?,nj , Υ?j )-excess risk transforms.
For all j ∈ {1, 2, . . . , h}, let the surrogate ψ j : [nj ] ×
Rdj →R+ and predictor Υ?j : Rdj →([nj ] ∪ {⊥}) be such
that they are calibrated w.r.t. the abstain loss with nj
Pj
classes for some integers dj . Let d = i=1 dj . Let any
d
>
>
u ∈ R be decomposed as u = [u1 , . . . , u>
h ] , with each
dj
uj ∈ R . The template surrogate, that we call the cascade
surrogate ψ cas : [n] × Rd →R+ , is defined in terms of its
constituent surrogates as follows:

where py (x) = P(Y = y|X = x).
?,n

The `

-regret of a function g : X →([n] ∪ {⊥}) is

OvA,n
1
Rψ
[f ] .
D
2(1 − |τ |)

ψ cas (y, u)

=

h
X

ψ j (ancj (y), uj ) .

(2)

j=1
?,n

R`D [g] = E[`?,n (Y, g(X))] − inf0 E[`?,n (Y, g 0 (X))] .
g

Ramaswamy et al. (2015) give three different surrogates
and predictors with excess risk transforms relating the surrogate regret to the `?,n -regret, one of which we give below.
Define the surrogate ψ OvA,n : [n] × Rn →R+ and predictor
ΥτOvA,n : Rn →([n] ∪ ⊥) as
ψ OvA,n (y, u) =

n
X

1(y = i)(1−ui )+ +1(y 6= i)(1+ui )+

i=1

(
ΥOvA,n
(u)
τ

=

argmaxi∈[n] ui
⊥

if maxj uj > τ
,
otherwise

where (a)+ = max(a, 0) and τ ∈ (−1, 1) is a threshold
parameter, and ties are broken arbitrarily, say, in favor of
the label y with the smaller index.
The following theorem by Ramaswamy et al. (2015), gives
an (ψ OvA,n , `?,n , ΥOvA,n
)-excess risk transform.
τ

The template predictor, Υcas , is defined via the function
d1
× . . . × Rdj →[nj ] which is defined recursively
Υcas
j : R
as follows:
Υcas
j (u1 , . . . , uj )
(
Υ?j (uj )
if Υ?j (uj ) 6= ⊥
. (3)
=
Υcas
j−1 (u1 , . . . , uj−1 ) otherwise
The function Υcas
0 takes no arguments and simply returns 1
(the root node). Occasionally we abuse notation by reprecas
senting Υcas
j (u1 , . . . , uj ) simply as Υj (u).
The template predictor, Υcas : Rd →[n] is simply defined as
Υcas (u) = Υcas
h (u1 , . . . , uh ) .
The lemma below, captures the essence of the reduction
from the hierarchical classification problem to the MCRO
problem. A proof outline is also provided.
Lemma 3. Let H = ([n], E, W ) be a tree with height h.
For all j ∈ [h], let αj = maxy,y0 ∈N=j `H (y, y 0 ). For any
distribution D over X × [n], let Aj (D) be the distribution

Convex Calibrated Surrogates for Hierarchical Classification

over X ×[nj ] given by the distribution of (X, ancj (Y )) with
(X, Y ) ∼ D. For all j ∈ [h], let fj : X →Rdj be such that
f (x) = [f1 (x)> , . . . , fh (x)> ]> . Then for all distributions
D over X × [n] and all functions f : X →Rd
H

R`D [Υcas ◦ f ] ≤

h
X

?,nj

2αj · R`Aj (D) [Υ?j ◦ fj ] .

j=1

Proof. (Outline:)
Due to linearity of expectation, it is sufficient to fix a singleton X , and give proofs for all distributions p ∈ ∆n over
class labels, instead of all distributions D over X × Y.
For a given conditional probability vector p ∈ ∆n , and
vector u ∈ Rd , the analysis is based on whether the abstain
loss predictor at the deepest level (level farthest from root)
abstains or not.

Lemma 3 bounds the `H regret on distribution D, by a
weighted sum of abstain loss regrets, each over a modified
distribution derived from D. Each of the components of the
surrogate ψ cas is exactly designed to minimize the abstain
loss for the corresponding modified distribution. Assuming
a (ψ j , `?,nj , Υ?j )-excess risk transform for all j ∈ [h], one
can easily derive (ψ cas , `H , Υcas )-excess risk transform as
in Equation 1. This is done in the theorem below.
Theorem 4. Let H = ([n], E, W ) be a tree with height h.
For all j ∈ [h], let ψ j : [nj ] × Rdj →R+ and Υ?j : Rdj →nj
be such that for all fj : X →Rdj , and all distributions D
over X × [nj ] we have
?,nj

R`D

j

[Υ?j ◦ fj ] ≤ C · Rψ
D [fj ],

for some constant C > 0. Then for all f : X →Rd and
distributions D over X × [n],
cas

H

R`D [Υcas ◦ f ] ≤ 2C · max
`H (y, y 0 ) · Rψ
D [f ] .
0
y,y ∈[n]

1. If the abstain loss predictor at the deepest level does
not abstain (Case 1 in the proof), then the tree-distance
regret is bounded by the maximum distance between
any two nodes at the deepest level αh , with a discount
factor depending on the conditional probability of the
predicted class. This can be simply be bounded by
2αh times the abstain loss regret.
2. If the abstain loss predictor at the deepest level does
abstain and the optimal prediction is not in the deepest
level (Case 2a in the proof), then prediction for the
deepest level is ‘correct’ and hence one can show that
the tree-distance regret is simply bounded by the treedistance regret for the modified problem where all the
probability mass associated with the nodes in deepest
level are absorbed by their parents.
3. If the abstain loss predictor at the deepest level does
abstain and the optimal prediction is in the deepest
level (Case 2b in the proof), then one can bound the
tree-distance regret by the sum of two terms –
(a) The abstain loss regret, weighted by twice the
largest distance between any node at the deepest
level and its parent βh . This captures the error
made by choosing to predict at a shallower level
than the level of optimal prediction.
(b) The tree-distance regret on the modified problem
mentioned in case 2a. This captures the error
made on shallower levels.
In all cases, the tree-distance regret can be bounded by the
sum of the tree-distance regret on the modified problem and
2αh times the abstain loss regret. Applying this bound recursively gets our desired bound.

Hence one just needs to plug in an appropriate surrogate ψ j
to get concrete consistent algorithms for hierarchical classification. The results of Ramaswamy et al. (2015) give
three such surrogates, but we will focus on the one vs all
hinge surrogate here, as the resulting algorithm can be easily parallelized and gives the best empirical results.

6. OvA-Cascade Algorithm
OvA,n

When ψ j = ψ OvA,nj and Υ?j = Υτj j for some τj ∈
(−1, 1), we call the resulting cascade surrogate ψ cas and
predictor Υcas together as OvA-Cascade. In this case we
have dj = nj . In the surrogate minimizing algorithm
for OvA-cascade, one solves h one-vs-all SVM problems.
Problem j has nj classes, with the classes corresponding
to the nj−1 nodes in the hierarchy at level less than j,
and nj − nj−1 ‘super-nodes’ in the hierarchy at level j
which also absorb the nodes of its descendants. The resulting training and prediction algorithms can thus be simplified and they are presented in Algorithms 1 and 2. The
training phase requires an SVM optimization sub-routine,
SVM-Train, which takes in a binary dataset and a regularization parameter C and returns a real valued function over
the instance space minimizing the regularized hinge loss
over an appropriate function space.
Theorems 2 and 4 immediately give the following corollary.
Corollary 5. Let H = ([n], E, W ) be a tree with height
h. Let the component surrogates and predictors of ψ cas and
OvA,n
Υcas be ψ j = ψ OvA,nj and Υj = Υτj j . Then, for all
distributions D and functions f : X →Rd ,
H

R`D [Υcas ◦ f ] ≤

cas
maxy,y0 ∈[n] `H (y, y 0 )
· Rψ
D [f ] .
1 − maxj |τj |

Convex Calibrated Surrogates for Hierarchical Classification

and functions f : X →Rd ,

Algorithm 1 OVA-Cascade Training
Input: S = ((x1 , y1 ), . . . , (xm , ym )) ∈ (X × [n])m ,
H = ([n], E).
Parameters: Regularization parameter C > 0

Algorithm 2 OVA-Cascade Prediction
Input: x ∈ X , H = ([n], E), trained models fi , fi0 for
all i ∈ [n]
Parameters: Scalars τ1 , . . . , τh in (−1, 1)
for j = h down to 1
nj
Construct
( u ∈ R such that,
fi (x) if lev(i) = j
ui =
fi0 (x) if lev(i) < j
if maxi ui > τj
return argmaxi ui
end if
end for
return 1

Thus setting τj = 2j−1
2j+1 gives almost a factor 2 improvement over setting τj = 0. This threshold setting is also
intuitively satisfying as it says to use a higher threshold
and predict conservatively (abstain more often) in deeper
levels and to use a lower threshold and predict aggressively
in levels nearer to the root. In practice, the optimal thresholds are distribution dependent and are best obtained via
cross-validation.

7. Experiments
We run our cascade surrogate based algorithm for hierarchical classification on some standard document classification tasks with a class hierarchy and compare the results
against other standard algorithms. We use the unweighted
tree-distance loss as the evaluation metric. The details of
the datasets and the algorithms are given below.

To get the best bound from Corollary 5, one must set τj = 0
for all j ∈ [h]. However, using a slightly more intricate
version of Theorem 2 and Lemma 3 one can give a better
upper bound for the `H -regret than in Theorem 4, and this
tighter upper bound is minimized for a different τj . This
observation is captured by the Theorem below.
Theorem 6. Let H = ([n], E, W ) be a tree with height
h. For all j ∈ [h], let αj = maxy,y0 ∈N=j `H (y, y 0 )
and let βj = maxy∈N=j `H (y, P (y)). For j ∈ [h], let
α −β
τj = αjj +βjj . Let the component surrogates and predictors
OvA,nj

of ψ cas and Υcas be ψ j = ψ OvA,nj and Υj = Υτj
Then, for all distributions D and functions f : X →Rd ,
H

cas

b. For all j ∈ [h] let τj = 2j−1
2j+1 , then, for all distributions D and functions f : X →Rd ,


cas
1
`H
cas
RD [Υ ◦ f ] ≤ h +
· Rψ
D [f ] .
2

for i = 1 : n
Let tj = 2 · 1(yj ∈ D(i)) − 1, ∀j ∈ [m]
Ti = ((x1 , t1 ), . . . , (xm , tm )) ∈ (X × {+1, −1})m .
fi =SVM-Train(Ti , C)
Let t0j = 2 · 1(yj = i) − 1, ∀j ∈ [m]
Ti0 = ((x1 , t01 ), . . . , (xm , t0m )) ∈ (X × {+1, −1})m .
fi0 =SVM-Train(Ti0 , C)
end for

R`D [Υcas ◦ f ] ≤

H

R`D [Υcas ◦ f ] ≤ 2h · Rψ
D [f ] .

7.1. Datasets
We used several standard multiclass document classification datasets, all of which have one class label per example.
The basic statistics of the datasets is given in Table 1.
• CLEF (Dimitrovski et al., 2011) Medical X-ray images organized according to a hierarchy.
• IPC 3 Patents organized according to the International
Patent Classification Hierarchy.

.

cas
1
max(αj + βj ) · Rψ
D [f ] .
2 j∈[h]

One can clearly see the effect of improved bounds given by
setting τj as in Theorem 6 for the unweighted hierarchy, in
which case αj = 2j and βj = 1.
Corollary 7. Let the hierarchy H be an unweighted tree
with all edges having length 1. Let the component surrogates and predictors of ψ cas and Υcas be ψ j = ψ OvA,nj and
OvA,n
Υj = Υτj j .

• LSHTC-small, DMOZ-2010 and DMOZ-2012 4
Web-pages, from the LSHTC (Large-Scale Hierarchical Text Classification) challenges 2010-12, organized
according to a hierarchy.
We used the standard train-test splits wherever available
and possible. For the DMOZ-2010 and 2012 datasets however we created our own train-test splits because the given
test sets do not contain class labels and the oracle for evaluating submissions does not accept interior nodes as predictions.
3

a. For all j ∈ [h] let τj = 0, then, for all distributions D

4

http://www.wipo.int/classifications/ipc/en/support/
http://lshtc.iit.demokritos.gr/node/3

Convex Calibrated Surrogates for Hierarchical Classification
Table 1. Dataset Statistics

Dataset
CLEF
LSHTC-small
IPC
DMOZ-2010
DMOZ-2012

#Train
9,000
4,463
35,000
80,000
250,000

#Validation
1,000
1,860
11,324
13,805
50,000

#Test
1,006
1,858
28,926
34,905
83,408

#Labels
97
2,388
553
17,222
13,347

#Leaf-Labels
63
1,139
451
12,294
11,947

Depth
3
5
3
5
5

#Features
89
51,033
541,869
381,580
348,548

Table 2. Average tree-distance loss on the test set. Runs that failed due to memory issues are denoted by a ‘-’.

Root
CLEF
LSHTC-small
IPC
DMOZ-2010
DMOZ-2012

3.00
4.77
2.97
4.65
4.75

OVA
1.10
4.12
2.29
3.96
2.83

HSVMmargin
0.98
3.47
-

HSVMslack
1.00
3.54
-

CSCascade
0.91
3.20
-

OVACascade
0.95
3.19
2.06
3.12
2.46

Plug-in
0.97
3.26
2.05
3.16
2.48

Table 3. Training times (not including validation) in hours (h) or seconds (s). Runs that failed due to memory issues are denoted by a ‘-’.

Root
CLEF
LSHTC-small
IPC
DMOZ-2010
DMOZ-2012

0s
0h
0h
0h
0h

OVA
35s
0.24 h
2.6 h
36 h
201 h

HSVMmargin
50 s
2.1 h
-

7.2. Algorithms
We run a variety of algorithms on the above datasets. The
details of the algorithms are given below.
Root: This is a simple baseline method where the returned
classifier always predicts the root of the hierarchy.
OVA: This is the standard One vs All algorithm which
completely ignores the hierarchy information and treats the
problem as one of standard multiclass classification.
HSVM-margin and HSVM-slack : These algorithms are
Struct-SVM like (Tsochantaridis et al., 2005) algorithms
for the tree-distance loss as proposed in Cai & Hofmann
(2004). HSVM-margin and HSVM-slack use margin and
slack rescaling respectively, and are considered among the
state-of-the-art algorithms for hierarchical classification.
OVA-Cascade: This is the algorithm in which we minimize the surrogate ψ cas with the component surrogates being ψ j = ψ OvA,nj and is detailed as Algorithms 1 and 2.
All the datasets in Table 1 have the property that all instances are associated only with a leaf-label (note however
that we can still predict interior nodes), and hence the step
of computing fi0 in Algorithm 1 can be skipped, and fi0
can be set to be identically equal to negative infinity for

HSVMslack
45 s
1.8 h
-

CSCascade
20 s
1.7 h
-

OVACascade
50 s
0.3 h
2.9 h
59 h
220 h

Plug-in
66 s
0.5 h
4.2 h
146 h
361 h

all i ∈ [n]. Note that, in this case, the training phase is very
similar to the ‘less-inclusive policy’ using the ‘local node
approach’ (Silla Jr. & Freitas, 2011). We use LIBLINEAR (Fan et al., 2008) for the SVM-train subroutine and
use the simple linear kernel. The regularization parameter
C is chosen via a separate validation set. The thresholds τj
for j ∈ [h] are also chosen via a coarse grid search using
the validation set.
Plug-in classifier: This algorithm is based on estimating
the conditional probabilities using a logistic loss. Specifically, it estimates Sy (p) for all non-root nodes y. This is
done by creating a binary dataset for each y, with instances
having labels which are the descendants of y being positive
and the rest being negative, and running a logistic regression algorithm on this dataset. The final predictor is simply
based on Theorem 1, it chooses the deepest node y such
that the estimated value of Sy (p) is greater than 21 .
CS-Cascade: This algorithm also minimizes the cascade
surrogate ψ cas , but with the component surrogates ψ j being
the Crammer-Singer surrogate (Crammer & Singer, 2001).
From the results of Ramaswamy et al. (2015), one can derive excess risk transforms for the resulting cascade surrogate as well. As all instances have labels which are leaf
nodes, the h subproblems all turn out to be multiclass learn-

Convex Calibrated Surrogates for Hierarchical Classification

ing problems with nj classes for each of which we use the
Crammer-Singer algorithm. We optimize the CrammerSinger surrogate over the standard multiclass linear function class using the LIBLINEAR software. Once again we
use the same regularization parameter C for all the h problems which we choose using the validation set. We also use
a threshold vector tuned on the validation set over a coarse
grid.
The three algorithms OvA-Cascade, Probability estimation
and CS-cascade are all motivated by our analysis and would
form consistent algorithms for the tree-distance loss if used
with an appropriate function class.
7.3. Discussion of Results
Table 2 gives the average tree-distance loss incurred by
various algorithms on some standard datasets and Table
3 gives the times taken for running these algorithms on
a 4-core CPU.5 Some of the algorithms, like HSVM, and
CS-cascade could not be run on the larger datasets due
to memory issues. In the smaller datasets of CLEF and
LSHTC-small where all the algorithms could be run,
the algorithms motivated by our analysis – OvA-cascade,
Plug-in and CS-cascade – perform the best. In the bigger
datasets, only the OvA-cascade, plug-in and the flat OvA
algorithms could be run, and both OvA-cascade and Plugin perform significantly better than the flat OvA. While
both OvA-cascade and Plug-in give comparable error performance, the OvA-cascade only takes about half as much
time as the Plug-in and hence is more preferable.

8. Conclusion
The reduction of the hierarchical classification problem to
the problem of multiclass classification with a reject option
gives an interesting and powerful family of algorithms. Extending such results to other related settings, such as the
case where there is a graph over the set of class labels, or
where a subset of the label set is allowed to be predicted
instead of a single label, are interesting future directions.

References
Babbar, R., Partalas, I., Gaussier, E., and Amin, M.-R. On
flat versus hierarchical classification in large-scale taxonomies. In Advances in Neural Information Processing
Systems 26, 2013.
Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier,
W., and Weiss, B. A database of german emotional
speech. In Proceedings of the 9th European conference
on speech communication and technology, 2005.
Cai, L. and Hofmann, T. Hierarchical document categorization with support vector machines. In International Conference on Information and Knowledge Management,
2004.
Cesa-Bianchi, N., Gentile, C., and Zaniboni, L. Hierarchical classification: combining Bayes with SVM. In
International Conference on Machine Learning, 2006a.
Cesa-Bianchi, N., Gentile, C., and Zaniboni, L. Incremental algorithms for hierarchical classification. Journal of
Machine Learning Research, 7:31–54, 2006b.
Crammer, K. and Singer, Y. On the learnability and design of output codes for multiclass problems. Machine
Learning, 2001.
Dekel, O., Keshet, J., and Singer, Y. Large margin hierarchical classification. In International Conference on
Machine Learning, 2004.
Dimitrovski, I., Kocev, D., Suzana, L., and Dzeroski, S. Hierchical annotation of medical images. Pattern Recognition, 2011.
Fan, R., Chang, K., Hsieh, C., Wang, X., and Lin, C. Liblinear: A library for large linear classification. Journal
of Machine Learning Research, 9:1871–1874, 2008.
Gopal, S. and Yang, Y. Recursive regularization for largescale classification with hierarchical and graphical dependencies. In International Conference on Knowledge
Discovery and Data Mining, 2013.

Acknowledgements
HGR and AT gratefully acknowledge the support of NSF
via grant CCF-1422157. HGR also acknowledges support from TCS via a PhD Fellowship and a grant from
the Indo-US virtual institute for mathematical and statistical sciences (VIMSS) . SA acknowledges support from
the Department of Science & Technology (DST) of the
Indian Government under a Ramanujan Fellowship, from
the Indo-US Science & Technology Forum (IUSSTF), and
from Yahoo in the form of an unrestricted grant.
5

HSVM, and CS-cascade effectively only use a single core due
to lack of parallelization.

Gopal, S., Bai, B., Yang, Y., and Niculescu-Mizil, A.
Bayesian models for large-scale hierarchical classification. In Advances in Neural Information Processing Systems 25, 2012.
Ramaswamy, H. G. and Agarwal, S. Classification calibration dimension for general multiclass losses. In Advances in Neural Information Processing Systems 25,
2012.
Ramaswamy, H. G., Tewari, A., and Agarwal, S. Consistent algorithms for multiclass classification with a reject
option. arXiv:1505.04137, 2015.

Convex Calibrated Surrogates for Hierarchical Classification

Rousu, J., Saunders, C., Szedmak, S., and Shawe-Taylor, J.
Kernel-based learning of hierarchical multilabel classification models. Journal of Machine Learning Research,
7:1601–1626, 2006.
Silla Jr., C. N. and Freitas, A. A. A survey of hierarchical
classification across different application domains. Data
Mining and Knowledge Discovery, 2011.
Sun, A. and Lim, E.-P. Hierarchical text classification and
evaluation. In International Conference on Data Mining,
2001.
Tsochantaridis, I., Joachims, T., Hoffman, T., and Altun,
Y. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484, 2005.
Wang, H., Shen, X., and Pan, W. Large margin hierarchical
classification with mutually exclusive class membership.
Journal of Machine Learning Research, 12:2721–2748,
2011.
Wang, K., Zhou, S., and Liew, S. C. Building hierarchical
classifiers using class proximity. In International Conference on Very Large Data Bases, 1999.
Xiao, Z., Dellandréa, E., Dou, W., and Chen, L. Hierarchical classification of emotional speech. IEEE Transactions on Multimedia, 2007.
Zhang, T. Statistical analysis of some multi-category
large margin classification methods. Journal of Machine
Learning Research, 5:1225–1251, 2004.

