Surrogate Functions for Maximizing Precision at the Top

Purushottam Kar
Microsoft Research, INDIA

T- PURKAR @ MICROSOFT. COM

Harikrishna Narasimhan∗
Indian Institute of Science, Bangalore, INDIA

HARIKRISHNA @ CSA . IISC . ERNET. IN

Prateek Jain
Microsoft Research, INDIA

Abstract
The problem of maximizing precision at the
top of a ranked list, often dubbed Precision@k
(prec@k), finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalance.
However, despite its popularity, there exist significant gaps in our understanding of this problem and its associated performance measure.
The most notable of these is the lack of a convex upper bounding surrogate for prec@k. We
also lack scalable perceptron and stochastic gradient descent algorithms for optimizing this performance measure. In this paper we make key
contributions in these directions. At the heart
of our results is a family of truly upper bounding surrogates for prec@k. These surrogates are
motivated in a principled manner and enjoy attractive properties such as consistency to prec@k
under various natural margin/noise conditions.
These surrogates are then used to design a class
of novel perceptron algorithms for optimizing
prec@k with provable mistake bounds. We also
devise scalable stochastic gradient descent style
methods for this problem with provable convergence bounds. Our proofs rely on novel uniform
convergence bounds which require an in-depth
analysis of the structural properties of prec@k
and its surrogates. We conclude with experimental results comparing our algorithms with stateof-the-art cutting plane and stochastic gradient
algorithms for maximizing prec@k.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).
∗
Part of the work was done while H.N. was an intern at MSR India.

PRAJAIN @ MICROSOFT. COM

1. Introduction
Ranking a given set of points or labels according to their
relevance forms the core of several real-life learning systems. For instance, in classification problems with a rareclass as is the case in spam/anomaly detection, the goal is
to rank the given emails/events according to their likelihood
of being from the rare-class (spam/anomaly). Similarly, in
multi-label classification problems, the goal is to rank the
labels according to their likelihood of being relevant to a
data point (Tsoumakas & Katakis, 2007).
The ranking of items at the top is of utmost importance
in these applications and several performance measures,
such as Precision@k, Average Precision and NDCG have
been designed to promote accuracy at top of ranked lists.
Of these, the Precision@k (prec@k) measure is especially
popular in a variety of domains. Informally, prec@k counts
the number of relevant items in the top-k positions of a
ranked list and is widely used in domains such as binary
classification (Joachims, 2005), multi-label classification
(Prabhu & Varma, 2014) and ranking (Le & Smola, 2007).
Given its popularity, prec@k has received attention from
algorithmic, as well as learning theoretic perspectives.
However, there remain specific deficiencies in our understanding of this performance measure. In fact, to the best of
our knowledge, there is only one known convex surrogate
function for prec@k, namely, the struct-SVM surrogate due
to (Joachims, 2005) which, as we reveal in this work, is not
an upper bound on prec@k in general, and need not recover
an optimal ranking even in strictly separable settings.
Our aim in this paper is to develop efficient algorithms for
optimizing prec@k for ranking problems with binary relevance levels. Since the intractability of binary classification
in the agnostic setting (Guruswami & Raghavendra, 2009)
extends to prec@k, our goal would be to exploit natural
notions of benign-ness usually observed in natural distributions to overcome such intractability results.

Surrogate Functions for Maximizing Precision at the Top

1.1. Our Contributions
We make several contributions in this paper that both, give
deeper insight into the prec@k performance measure, as
well as provide scalable techniques for optimizing it.
Precision@k margin: motivated by the success of marginbased frameworks in classification settings, we develop a
family of margin conditions appropriate for the prec@k
problem. Recall that the prec@k performance measure
counts the number of relevant items at the top k positions
of a ranked list. The simplest of our margin notions, that
we call the weak (k, γ)-margin, is said to be present if a
privileged set of k relevant items can be separated from all
irrelevant items by a margin of γ. This is the least restrictive margin condition that allows for a perfect ranking w.r.t
prec@k. Notably, it is much less restrictive than the binary
classification notion of margin which requires all relevant
items to be separable from all irrelevant items by a certain
margin. We also propose two other notions of margin suited
to our perceptron algorithms.
Surrogate functions for prec@k: we design a family of
three novel surrogates for the prec@k performance measure. Our surrogates satisfy two key properties. Firstly
they always upper bound the prec@k performance measure
so that optimizing them promotes better performance w.r.t
prec@k. Secondly, these surrogates satisfy conditional
consistency in that they are consistent w.r.t. prec@k under
some noise condition. We show that there exists a one-one
relationship between the three prec@k margin conditions
mentioned earlier and these three surrogates so that each
surrogate is consistent w.r.t. prec@k under one of the margin conditions. Moreover, our discussion reveals that the
three surrogates, as well as the three margin conditions, lie
in a concise hierarchy.
Perceptron and SGD algorithms: using insights gained
from the previous analyses, we design two perceptron-style
algorithms for optimizing prec@k. Our algorithms can be
shown to be a natural extension of the classical perceptron algorithm for binary classification (Rosenblatt, 1958).
Indeed, akin to the classical perceptron, both our algorithms enjoy mistake bounds that reduce to crisp convergence bounds under the margin conditions mentioned earlier. We also design a mini-batch-style stochastic gradient
descent algorithm for optimizing prec@k.
Learning theory: in order to prove convergence bounds
for the SGD algorithm, and online-to-batch conversion
bounds for our perceptron algorithms, we further study
prec@k and its surrogates and prove uniform convergence
bounds for the same. These are novel results and require an
in-depth analysis into the involved structure of the prec@k
performance measure and its surrogates. However, with
these results in hand, we are able to establish crisp conver-

gence bounds for the SGD algorithm, as well as generalization bounds for our perceptron algorithms.
Paper Organization: Section 2 presents the problem formulation and sets up the notation. Section 3 introduces
three novel surrogates and margin conditions for prec@k
and reveals the interplay between these with respect to consistency to prec@k. Section 4 presents two perceptron algorithms for prec@k and their mistake bounds, as well as
a mini-batch SGD-based algorithm. Section 5 discusses
uniform convergence bounds for our surrogates and their
application to convergence and online-to-batch conversion
bounds for our the perceptron and SGD-style algorithms.
We conclude with empirical results in Section 6.
1.2. Related Work
There has been much work in the last decade in designing
algorithms for bipartite ranking problems. While the earlier methods for this problem, such as RankSVM, focused
on optimizing pair-wise ranking accuracy (Herbrich et al.,
2000; Joachims, 2002; Freund et al., 2003; Burges et al.,
2005), of late, there has been enormous interest in performance measures that promote good ranking performance at
the top portion of the ranked list, and in ranking methods
that directly optimize these measures (Clémençon & Vayatis, 2007; Rudin, 2009; Agarwal, 2011; Boyd et al., 2012;
Narasimhan & Agarwal, 2013a;b; Li et al., 2014).
In this work, we focus on one such evaluation measure –
Precision@k, which is widely used in practice. The only
prior algorithms that we are aware of that directly optimize
this performance measure are a structural SVM based cutting plane method due to (Joachims, 2005), and an efficient
stochastic implementation of the same due to (Kar et al.,
2014). However, as pointed out earlier, the convex surrogate used in these methods is not well-suited for prec@k.
It is also important to note that the bipartite ranking setting considered in this work is different from other popular
forms of ranking such as subset or list-wise ranking settings, which arise in several information retrieval applications, where again there has been much work in optimizing performance measures that emphasize on accuracy at
the top (e.g. NDCG) (Valizadegan et al., 2009; Cao et al.,
2007; Yue et al., 2007; Le & Smola, 2007; Chakrabarti
et al., 2008; Yun et al., 2014). There has also been some
recent work on perceptron style ranking methods for listwise ranking problems (Chaudhuri & Tewari, 2014), but
these methods are tailored to optimize the NDCG and MAP
measures, which are different from the prec@k measure
that we consider here. Other less related works include online ranking algorithms for optimizing ranking measures in
an adversarial setting with limited feedback (Chaudhuri &
Tewari, 2015).

Surrogate Functions for Maximizing Precision at the Top

2. Problem Formulation and Notation
We will be presented with a set of labeled points
(xi , yi ), . . . , (xn , yn ), where xi ∈ X and yi ∈ {0, 1}. We
shall use X to denote the entire dataset, X+ and X− to denote the set of positive and negatively (null) labeled points,
and y ∈ {0, 1}n to denote the label vector. z = (x, y)
shall denote a labeled data point. Our results readily extend
to multi-label and ranking settings but for sake of simplicity, we focus only on bipartite ranking problems, where the
goal is to rank (a subset of) positive examples above the
negative ones.
Given n labeled data points z1 , . . . , zn and a scoring function s : X → R, let σs ∈ Sn be the permutation that sorts
points according to the scores given by s i.e. s(xσs (i) ) ≥
s(xσs (j) ) for i ≤ j. The Precision@k measure for this
scoring function can then be expressed as:
k
X
(1 − yσs (i) ).
prec@k(s; z1 , . . . , zn ) =

(1)

i=1

Note that the above is a “loss” version of the performance
measure which penalizes any top-k ranked data points that
have a null label. For simplicity, we will use the abbreviated notation prec@k(s) := prec@k(s; z1 , . . . , zn ). We
will also use the shorthand si = s(xi ). For any label vectors y0 , y00 ∈ {0, 1}n , we define

Motivated by the above requirements, we develop a family
of surrogates which upper bound the prec@k loss function
and are consistent to it under certain margin/noise conditions. We note that the results of (Calauzènes et al., 2012)
that negate the possibility of consistent convex surrogates
for ranking performance measures do not apply to our results since they are neither stated for prec@k, nor do they
negate the possibility of conditional consistency.
It is notable that the seminal work of (Joachims, 2005) did
propose a convex surrogate for prec@k, that we refer to as
`struct
prec@k (·). However, as the discussion below shows, this
surrogate is not even an upper bound on prec@k let alone
be consistent to it. Understanding the reasons for the failure
of this surrogate would be crucial in designing our own.
3.1. The Curious Case of `struct
prec@k (·)
The `struct
prec@k (·) surrogate is a part of a broad class of surrogates called struct-SVM surrogates that are designed for
structured output prediction problems that can have exponentially large output spaces (Joachims, 2005). Given a set
of n labeled data points, `struct
prec@k (·) is defined as
(
)
n
X
max n ∆(y, ŷ) +
(ŷi − yi ) si .
(3)
ŷ∈{0,1}
kŷk1 =k

i=1

3. A Family of Novel Surrogates for prec@k

The above surrogate penalizes a scoring function if there
exists a set of k points with large scores (i.e. the second term is large) which are actually negatives (i.e. the
first term is large). However, since the candidate labeling ŷ is restricted to labeling just k points as positive
whereas the true label vector y has n+ positives, in cases
where n+ > k, a non-optimal candidate labeling ŷ can
exploit the remaining n+ − k labels to hide the high scoring negative points, thus confusing the surrogate function.
This indicates that this surrogate may not be an upper
bound to prec@k. We refer the reader to Appendix A
for an explicit example where, not only does this surrogate not upper bound prec@k, but more importantly, minimizing `struct
prec@k (·) does not produce a model that is optimal
for prec@k, even in separable settings where all positives
points are separated from negatives by a margin.

As prec@k is a non-convex loss function that is hard to
optimize directly, it is natural to seek surrogate functions
that act as a good proxy for prec@k. There will be two
properties that we shall desire of such a surrogate:

In the sequel, we shall propose three surrogates, all
of which are consistent with prec@k under various
noise/margin conditions. The surrogates, as well as the
noise conditions, will be shown to form a hierarchy.

∆(y0 , y00 ) =

n
X

(1 − yi0 )yi00 ,

i=1
0

00

K(y , y ) =

n
X

(2)
yi0 yi00 .

i=1
0

0

0

Let n+ (y ) = K(y , y ) = ky0 k1 denote the number of
positives in the label vector y0 and n+ = n+ (y) denote
the number of actual positives. Let y(s,k) be the label vector that assigns the label 1 only to the top k ranked items
(s,k)
according to the scoring function s. That is, yi
= 1 if
−1
if σs (i) ≤ k and 0 otherwise. It is easy to verify that for
any scoring function s, ∆(y, y(s,k) ) = prec@k(s).

1. Upper Bounding Property: the surrogate should upper bound the prec@k loss function, so that minimizing the surrogate promotes small prec@k loss.
2. Conditional Consistency: under some regularity assumptions, optimizing the surrogate should yield an
optimal solution for prec@k as well.

3.2. The Ramp Surrogate `ramp
prec@k (·)
The key to maximizing prec@k in a bipartite ranking setting is to select a subset of k relevant items and rank them at
the top k positions. This can happen iff the top ranked k relevant items are not outranked by any irrelevant item. Thus,
a surrogate must penalize a scoring function that assigns

Surrogate Functions for Maximizing Precision at the Top

scores to irrelevant items that are higher than those of the
top ranked relevant items. Our ramp surrogate `ramp
prec@k (s)
implicitly encodes this strategy:
(
)
n
n
X
X
max ∆(y, ŷ) +
ŷi si − max
ỹi si . (4)
kŷk1 =k

kỹk1 =k
i=1
K(y,ỹ)=k

i=1

{z

|

}

(P )

where ỹ  y implies that yi = 0 ⇒ ỹi = 0. Thus, to convexify the surrogate `ramp
prec@k (·), we need to design a convex
upper bound on (Q). Notice that the term (Q) contains the
sum of the scores of the n+ − k lowest ranked positive data
points. This can be readily upper bounded in several ways
which give us different surrogate functions.
3.3. The Max Surrogate `max
prec@k (·)

The term (P ) contains the sum of scores of the k highest scoring positives. Note that `ramp
prec@k (·) is similar to the
“ramp” losses for binary classification (Do et al., 2008).
We now show that `ramp
prec@k (·) is indeed an upper bounding
surrogate for prec@k.
Claim 1. For any k ≤ n+ and scoring function s, we have
ramp
`ramp
prec@k (s) ≥ prec@k(s). Moreover, if `prec@k (s) ≤ ξ for a
given scoring function s, then there necessarily exists a set
S ⊂P
[n] of size atPmost k such that for all kŷk1 = k, we
n
have i∈S si ≥ i=1 ŷi si + ∆(y, ŷ) − ξ.

An immediate convex upper bound on (Q) is obtained by
replacing the sum of scores of the n+ − k lowest ranked
positives with those of the highest
Pn ranked ones as follows: (Q) ≤ max ỹ(1−ŷ)·y
i=1 ỹi si , which gives us

Proofs for this section are deferred to Appendix B. We can
show that this surrogate is conditionally consistent as well.
To do so, we introduce the notion of weak (k, γ)-margin.
Definition 2 (Weak (k, γ)-margin). A set of n labeled
data points satisfies the weak (k, γ)-margin condition if for
some scoring function s and set S+ ⊆ X+ of size k,

The above surrogate, being a point-wise maximum over
convex functions, is convex, as well as an upper bound on
prec@k(s) since it upper bounds `ramp
prec@k (s). This surrogate
can also be shown to be consistent w.r.t. prec@k under the
strong γ-margin condition defined below for γ = 1.

min si − max sj ≥ γ.

i∈S+

j:yj =0

Moreover, we say that the function s realizes this margin.
We abbreviate the weak (k, 1)-margin condition as simply
the weak k-margin condition.
Clearly, a dataset has a weak (k, γ)-margin iff there exist
some k positive points that substantially outrank all negatives. Note that this notion of margin is strictly weaker than
the standard notion of margin for binary classification as it
allows all but those k positives to be completely mingled
with the negatives. Moreover, this seems to be one of the
most natural notions of margin for prec@k. The following
lemma establishes that `ramp
prec@k (·) is indeed consistent w.r.t.
prec@k under the weak k-margin condition.
Claim 3. For any scoring function s that realizes the weak
k-margin over a dataset, `ramp
prec@k (s) = prec@k(s) = 0.
`ramp
prec@k (·)

This suggests that
is not only a tight surrogate,
but tight at the optimal scoring function, i.e. prec@k(s) =
0; this along with upper bounding property implies consistency. However, it is also a non-convex function due to the
term (P ). To obtain convex surrogates, we perform relaxations on this term by first rewriting it as follows:
(P ) =

n
X
i=1

yi si −

n
X

min

ỹy
kỹk1 =n+ −k i=1

|

{z

(Q)

ỹi si ,
}

(5)

kỹk1 =n+ −k

the `max
prec@k (s) surrogate defined below:
max





kŷk1 =k 


n
X
∆(y, ŷ) +
(ŷi − yi )si +
i=1

max

n
X

ỹ(1−ŷ)·y
kỹk1 =n+ −k i=1

ỹi si





.



(6)

Definition 4 (Strong γ-margin). A set of n labeled data
points satisfies the γ-strong margin condition if for some
scoring function s, mini:yi =1 si − maxj:yj =0 sj ≥ γ.
We notice that the strong margin condition is actually the
standard notion of binary classification margin and hence
much stronger than the weak (k, γ)-margin condition. It
also does not incorporate any elements of the prec@k problem. This leads us to look for tighter convex relaxations to
the term (Q) that we do below.
3.4. The Avg Surrogate `avg
prec@k (·)
A tighter upper bound on (Q) can be obtained by replacing
(Q) by the average score of the false negatives. Define
and consider the relaxation (Q) ≤
C(ŷ) = n+ −K(y,ŷ)
n+ −k
Pn
1
i=1 (1 − ŷi )yi si . Combining this with (4), we get
C(ŷ)
a new convex surrogate `avg
prec@k (s) defined as:
(
max

kŷk1 =k

)
n
1 X
∆(y, ŷ) +
si (ŷi − yi ) +
(1 − ŷi )yi si .
C(ŷ) i=1
i=1
(7)
n
X

We refer the reader to Appendix B.4 for a proof that
`avg
prec@k (·) is an upper bounding surrogate. It is notable that
for k = n+ (i.e. for the PRBEP measure), the surrogate
struct
`avg
prec@k (·) recovers Joachims’ original surrogate `prec@k (·).
To establish conditional consistency of this surrogate, consider the following notion of margin:
Definition 5 ((k, γ)-margin). A set of n labeled data points
satisfies the (k, γ)-margin condition if for some scoring

Surrogate Functions for Maximizing Precision at the Top
avg
max
prec@k(s) ≤ `ramp
prec@k (s) ≤ `prec@k (s) ≤ `prec@k (s)

⇑
weak (k, γ)
⊇
margin

⇑
(k, γ)
margin

⇑
strong γ

⊇ margin

Figure 1. A hierarchy among the three surrogates for prec@k and
the corresponding margin conditions for conditional consistency.

function s, we have, for all sets S+ ⊆ X+ of size n+ −k+1,
X
1
si − max sj ≥ γ.
j:yj =0
n+ − k + 1
i∈S+

Moreover, we say that the function s realizes this margin.
We abbreviate the (k, 1)-margin condition as simply the kmargin condition.
We can now establish the consistency of `avg
prec@k (·) under
the k-margin condition. See Appendix B.5 for a proof.
Claim 6. For any scoring function s that realizes the kmargin over a dataset, `avg
prec@k (s) = prec@k(s) = 0.
We note that the (k, γ)-margin condition is strictly weaker
than the strong γ-margin condition (Definition 4) since it
still allows a non negligible fraction of the positive points
to be assigned a lower score than those assigned to negatives. On the other hand, the (k, γ)-margin condition
is strictly stronger than the weak (k, γ)-margin condition
(Definition 2). The weak k-margin condition only requires
one set of k-positives to be separated from the negatives,
whereas the above margin condition at least requires the
average of all positives to be separated from the negatives.
As Figure 1 demonstrates, the three surrogates presented
above, as well as their corresponding margin conditions,
fall in a neat hierarchy. We will now use these surrogates to
formulate two perceptron algorithms with mistake bounds
with respect to these margin conditions.

4. Perceptron & SGD Algorithms for prec@k
We now present perceptron-style algorithms for maximizing the prec@k performance measure in bipartite ranking
settings. Our algorithms work with a stream of binary labeled points and process them in mini-batches of a predetermined size b. Mini-batch methods have recently gained
popularity and have been used to optimize ranking loss
functions such as `struct
prec@k (·) as well (Kar et al., 2014). It
is useful to note that the requirement for mini-batches goes
away in ranking and multi-label classification settings, for
our algorithms can be applied to individual data points in
those settings (e.g. individual queries in ranking settings).
At every timeinstant t, our
 algorithms receive a batch of b
points Xt = x1t , . . . , xbt and rank these points using the

Algorithm 1 P ERCEPTRON @ K - AVG
Input: Batch length b
1: w0 ← 0, t ← 0
2: while stream not exhausted do
3:
t←t+1


4:
Receive b data points Xt = x1t , . . . , xbt , yt ∈ {0, 1}b
5:
Calculate st = wt−1 Xt and let ŷt = y(st ,k)
6:
∆t ← ∆(yt , ŷt )
7:
if ∆t = 0 then
8:
wt ← wt−1
9:
else
∆t
10:
Dt ← kyt k −K(y
1
Pt ,ŷt )
11:
wt ← wt−1 − i∈[b] (1 − yi )ŷi · xit
{false pos.}
P
12:
wt ← wt + Dt · i∈[b] (1 − ŷi )yi · xit {false neg.}
13:
end if
14: end while
15: return wt

Algorithm 2 P ERCEPTRON @ K - MAX
10:
11:
12:

St ← FN(s, ∆tP
)
{false pos.}
wt ← wt−1 − i∈[b] (1 − yi )ŷi · xit
P
{top ranked false neg.}
wt ← wt + i∈St xit

existing model. Let ∆t denote the prec@k loss (equation 1)
at time t. If ∆t = 0 i.e. all top k ranks are occupied by
positive points, then the model is not updated. Otherwise,
the model is updated using the false positives and negatives.
For sake of simplicity, we will only look at linear models
in this paper. Depending on the kind of updates we make,
we get two variants of the perceptron rule for prec@k.
Our first algorithm, P ERCEPTRON @ K - AVG, updates the
model using a combination of all the false positives and
negatives (see Algorithm 1). The effect of the update is a
very natural one – it explicitly boosts the scores of the positive points that failed to reach the top ranks, and attenuates
the scores of the negative points that got very high scores.
It is interesting to note that in the limiting case of k = 1 and
unit batch length (i.e. b = 1), the P ERCEPTRON @ K - AVG
update reduces to that of the standard perceptron algorithm
(Rosenblatt, 1958; Minsky & Papert, 1988) for the choice
ŷt = sign(st ). Thus, our algorithm can be seen as a natural
extension of the classical perceptron algorithm.
The next lemma establishes that, similar to the classical
perceptron (Novikoff, 1962), P ERCEPTRON @ K - AVG also
enjoys a mistake bound. Our mistake bound is stated in the
most general agnostic setting with the hinge loss function
replaced with our surrogate `avg
prec@k (s). All proofs in this
section are deferred to Appendix C.
 
Theorem 7. Suppose xit  ≤ R for all t, i. Let ∆C
T =
PT
∆
be
the
cumulative
mistake
value
observed
when
t=1 t
Algorithm 1 is executed for T batches. Also, for any w, let

Surrogate Functions for Maximizing Precision at the Top

Algorithm 3 SGD@ K - AVG
Input: Batch length b, step lengths ηt , feasible set W
Output: A model w̄ ∈ W
1: w0 ← 0, t ← 0
2: while stream not exhausted do
3:
t←t+1


4:
Receive b data points Xt = x1t , . . . , xbt , yt ∈ {0, 1}b
{See Algorithm 4}
5:
Set gt ∈ ∂w `avg
prec@k (wt−1 ; Xt , yt )
6:
wt ← ΠW [wt−1 − ηt · gt ]
{project onto set W}
7: end while
P
8: return w̄ = 1t tτ =1 wτ

Algorithm 4 Subgradient calculation for `avg
prec@k (·)
Input: A model win , n data points X, y, parameter k
Output: A subgradient g ∈ ∂w `avg
prec@k (win ; X, y)
1: Sort pos. and neg. points separately in dec. order of scores
−
−
+
assigned by win i.e. s+
1 ≥ . . . ≥ sn+ and s1 ≥ . . . ≥ sn−
0
2: for k = 0 → k0do
3:
Dk0 ← nk−k
0
+ −k
Pk−k0 −
Pn+
+
0
4:
∆k ← k − k0 − Dk0 i=k
0 +1 si +
i=1 si
Pk−k0 −
Pn+
5:
gk0 ← i=1 xi − Dk0 i=k0 +1 x+
i
6: end for
∗
7: k ← arg maxk0 ∆k0
8: return gk∗

L̂avg
T (w)

=

∆C
T

PT

avg
t=1 `prec@k (w; Xt , yt ).

Then we have


2
q
√
avg
≤ min kwk · R · 4k + L̂T (w) .
w

Similar to the classical perceptron mistake bound
(Novikoff, 1962), the above bound can also be reduced to a
simpler convergence bound in separable settings.
∗

Corollary 8. Suppose a unit norm w exists such that the
scoring function s : x 7→ x> w∗ realizes the (k, γ)-margin
condition for all the batches, then Algorithm 1 guarantees
4kR2
the mistake bound: ∆C
T ≤ γ2 .
The above result assures that, as datasets become “easier” in the sense that their (k, γ)-margin becomes larger,
P ERCEPTRON @ K - AVG will converge to an optimal hyperplane at a faster rate. It is important to note there that the
(k, γ)-margin condition is strictly weaker than the standard classification margin condition. Hence for several
datasets, P ERCEPTRON @ K - AVG might be able to find a
perfect ranking while at the same time, it might be impossible for standard binary classification techniques to
find any reasonable classifier in poly-time (Guruswami &
Raghavendra, 2009).
We note that P ERCEPTRON @ K - AVG performs updates
with all the false negatives in the mini-batches. This raises
the question as to whether sparser updates are possible
as such updates would be slightly faster as well as, in
high dimensional settings, ensure that the model is sparser.

To this end we design the P ERCEPTRON @ K - MAX algorithm (Algorithm 2). P ERCEPTRON @ K - MAX differs from
P ERCEPTRON @ K - AVG in that it performs updates using
only a few of the top ranked false negatives. More specifically, for any scoring function s and m > 0, define:

X
(s,k)
FN(s, m) = arg max
1 − yi
yi si
S⊂X+
t ,|S|=m i∈S

as the set of the m top ranked false negatives.
P ERCEPTRON @ K - MAX makes updates only for false positives in the set FN(s, ∆t ). Note that ∆t can significantly
smaller than the total number of false negatives if k  n+ .
P ERCEPTRON @ K - MAX also enjoys a mistake bound but
with respect to the `max
prec@k (·) surrogate.
 
Theorem 9. Suppose xit  ≤ R for all t, i. Let ∆C
T =
PT
t=1 ∆t be the cumulative observed mistake value when
Algorithm 2 P
is executed for T batches. Also, for any w, let
T
max
L̂max
T (w) =
t=1 `prec@k (w; Xt , yt ). Then we have
∆C
T ≤ min
w

2

q
√
(w)
.
kwk · R · 4k + L̂max
T

Similar to P ERCEPTRON @ K - AVG, we can give a simplified
mistake bound in situations where the separability condition specified by Definition 4 is satisfied.
Corollary 10. Suppose a unit norm w∗ exists such that
the scoring function s : x 7→ x> w∗ realizes the strong
γ-margin condition for all the batches, then Algorithm 2
4kR2
guarantees the mistake bound: ∆C
T ≤ γ2 .
As the strong γ-margin condition is exactly the same
as the standard notion of margin for binary classification, the above bound is no stronger than the one for
the classical perceptron. However, in practice, we observe that P ERCEPTRON @ K - MAX at times outperforms
even P ERCEPTRON @ K - AVG, even though the latter has a
tighter mistake bound. This suggests that our analysis of
P ERCEPTRON @ K - MAX might not be optimal and fails to
exploit latent structures that might be present in the data.
Stochastic Gradient Descent for Optimizing prec@k.
We now extend our algorithmic repertoire to include
a stochastic gradient descent (SGD) algorithm for the
prec@k performance measure. SGD methods are known to
be very successful at optimizing large-scale empirical risk
minimization (ERM) problems as they require only a few
passes over the data to achieve optimal statistical accuracy.
However, SGD methods typically require access to cheap
gradient estimates which are difficult to obtain for nonadditive performance measures such as prec@k. This has
been noticed before by (Kar et al., 2014; Narasimhan et al.,
2015) who propose to use mini-batch methods to overcome this problem (Kar et al., 2014). By combining the

Surrogate Functions for Maximizing Precision at the Top

`avg
prec@k (·) surrogate with mini-batch-style processing, we
design SGD@ K - AVG (Algorithm 3), a scalable SGD algorithm for optimizing prec@k. The algorithm uses minibatches to update the current model using gradient descent
steps. The subgradient calculation for this surrogate turns
out to be non-trivial and is detailed in Algorithm 4.
The task of analyzing this algorithm is made non-trivial by
the fact that the gradient estimates available to SGD@ K AVG via Algorithm 4 are far from being unbiased. The
luxury of having unbiased gradient estimates is crucially
exploited by standard SGD analyses but unfortunately, unavailable to us. To overcome this hurdle, we propose a uniform convergence based proof that, in some sense, bounds
the bias in the gradient estimates.
In the following section, we present this, and many other
generalization and online-to-batch conversion bounds with
applications to our perceptron and SGD algorithms.

5. Generalization Bounds
In this section, we discuss novel uniform convergence
(UC) bounds for our proposed surrogates. We will use
these UC bounds along with the mistake bounds in Theorems 7 and 9 to prove two key results – 1) online-tobatch conversion bounds for the P ERCEPTRON @ K - AVG
and P ERCEPTRON @ K - MAX algorithms and, 2) a convergence guarantee for the SGD@ K - AVG algorithm.
To better present our generalization and convergence
bounds, we use normalized versions of prec@k and the surrogates. To do so we write k = κ · n+ for some κ ∈ (0, 1]
and define, for any scoring function s, its prec@κ loss as:
prec@κ(s; z1 , . . . , zn ) =

1
∆(y, y(s,κn+ ) ).
κn+

We will also normalize the surrogate functions `ramp
prec@κ (·),
avg
`max
(·),
and
`
(·)
by
dividing
by
k
=
κ
·
n
+.
prec@κ
prec@κ
Definition 11 (Uniform Convergence). A performance
measure Ψ : W × (X × {0, 1})n 7→ R+ exhibits uniform
convergence with respect to a set of predictors W if for
some α(b, δ) = poly 1b , log 1δ , for a sample ẑ1 , . . . , ẑb of
size b chosen i.i.d. (or uniformly without replacement) from
an arbitrary population z1 , . . . , zn , we have w.p. 1 − δ,
sup |Ψ(w; z1 , . . . , zn ) − Ψ(w; ẑ1 , . . . , ẑb )| ≤ α(b, δ)
w∈W

Recently, (Kar et al., 2014) also established a similar result for the `struct
prec@k (·) surrogate. However, a very different
proof technique is required to establish similar results for
avg
`max
prec@κ (·) and `prec@κ (·), partly necessitated by the terms
in these surrogates which depend, in a complicated manner, on the positives predicted by the candidate labeling ŷ.
Nevertheless, the above results allow us to establish strong
online-to-batch conversion bounds for P ERCEPTRON @ K AVG and P ERCEPTRON @ K - MAX , as well as convergence
rates for the SGD@ K - AVG method. In the following we
shall assume that our data streams are composed of points
chosen i.i.d. (or u.w.r.) from some fixed population Z.
Theorem 13. Suppose an algorithm, when fed a random
stream of data points, in T batches of length b each, generates an ensemble of models w1 , . . . , wT which together
suffer a cumulative mistake value of ∆C
T . Then, with probability at least 1 − δ, we have
!
r
T
∆C
T
1
1X
t
T
prec@κ(w ; Z) ≤
+O
log
.
T t=1
bT
b
δ
The proof of this theorem follows from Theorem 12 which
t
guarantees
that
q
 w.p. 1 − δ, prec@κ(w ; Z) ≤ ∆t /b +

1
1
O
b log δ for all t. Combining this with the mistake
bound from Theorem 7 ensures the following generalization guarantee for the ensemble generated by Algorithm 1.
Corollary 14. Let w1 , . . . , wT be the ensemble of classifiers returned by the P ERCEPTRON @ K - AVG algorithm on
a random stream of data points and batch length b. Then,
with probability at least 1 − δ, for any w∗ we have
T
q
2
1X
∗ ; Z) + C
prec@κ(wt ; Z) ≤
`avg
(w
,
prec@κ
T t=1
q
q


where C = O kw∗ k R T1 + 4 1b log Tδ .

A similar statement holds for the P ERCEPTRON @ K - MAX algorithm with respect to the `max
prec@κ (·) surrogate as well. Using the results from Theorem 12, we can also establish the
convergence rate of the SGD@ K - AVG algorithm.
Theorem 15. Let w̄ be the model returned by Algorithm 3
when executed on a stream with T batches of length b. Then
with probability at least 1 − δ, for any w∗ ∈ W, we have
`avg
prec@κ (w̄; Z)

≤

∗
`avg
prec@κ (w ; Z)+O

r

1
T
log
b
δ

!

r

+O

We now state our UC bounds for prec@κ and its surrogates.
We refer the reader to Appendix D for proofs.

The proof of this Theorem can be found in Appendix E.

Theorem 12. The loss function prec@κ(·), as well as the
avg
max
surrogates `ramp
prec@κ (·), `prec@κ (·) and `prec@κ (·), all exhibit
q

1
1
uniform convergence at the rate α(b, δ) = O
log
b
δ .

6. Experiments

1
T

!

We shall now evaluate our methods on several benchmark
datasets for binary classification problems with a rare-class.

Surrogate Functions for Maximizing Precision at the Top

0.2
−2

10

0

0.8
0.6
0.4
0.2
0
−2
10

2

10
10
Training time (secs)

(a) PPI

−1

Average Prec@0.25

0.4

Average Prec@0.25

Average Prec@0.25

Average Prec@0.25

1
0.6

0.8
0.6
0.4
0.2

0

−2

10
10
Training time (secs)

0

10

0.6
0.4
0.2
−2

10
Training time (secs)

(b) Letter

0.8

0

10

2

10
10
Training time (secs)

(c) Adult

(d) IJCNN

0.2
k=0.05

k=0.5

(a) KDD08

k=0.75

0.6
0.4
0.2
0

0.8
0.6
0.4

Perceptron@k−avg
0

2

10
k=0.05

k=0.5

(b) PPI

10
Epoch length

k=0.75

(c) KDD08

4

10

0.85
0.8
0.75
0.7

Perceptron@k−max
0

10

2

10
Epoch length

(d) KDD08

4

10

Average Prec@0.25

0.4

0.8

Average Prec@0.25

0.6

0

SVMPerf
Perceptron@k−max
Perceptron@k−avg
SGD@k−avg

1
Average Prec@k

Average Prec@k

1
0.8

Average Prec@0.25

Figure 2. A comparison of the proposed perceptron and SGD based methods with baseline methods (SVMPerf and 1PMB) on prec@0.25
maximization tasks. P ERCEPTRON @ K - AVG and SGD@ K - AVG (both based on `avg
prec@k (·)) are the most consistent methods across tasks.
0.85
0.8
0.75
SGD@k−avg
0

10

2

10
Epoch length

(e) KDD08

Figure 3. (a), (b): A comparison of different methods on optimizing prec@κ for different values of κ. (c), (d), (e): The performance of
the proposed perceptron and SGD methods on prec@0.25 maximization tasks with varying batch lengths b.

Datasets: We evaluated our methods on 7 publicly available benchmark datasets: a) PPI, b) KDD Cup 2008, c) Letter, d) Adult, e) IJCNN, f) Covertype, and g) Cod-RNA. All
datasets exhibit moderate to severe label imbalance with
the KDD Cup dataset having just 0.61% positives.
Methods: We compared both perceptron algorithms,
SGD@ K - AVG, as well as an SGD solver for the `max
prec@k (·)
surrogate, with the cutting plane-based SVMPerf solver of
(Joachims, 2005), and the stochastic 1PMB solver of (Kar
et al., 2014). The perceptron and SGD methods were given
a maximum of 25 passes over the data with a batch length
of 500. All methods were implemented in C. We used 70%
of the data for training and the rest for testing. All results
are averaged over 5 random train-test splits.
Our experiments reveal three interesting insights into the
problem of prec@k maximization – 1) using tighter surrogates for optimization routines is indeed beneficial, 2) the
presence of a stochastic solver cannot always compensate
for the use of a suboptimal surrogate, and 3) mini-batch
techniques, applied with perceptron or SGD-style methods,
can offer rapid convergence to accurate models.
We first timed all the methods on prec@κ maximization
tasks for κ = 0.25 on various datasets (see Figure 2). Of
all the methods, the cutting plane method (SVMPerf) was
found to be the most expensive computationally. On the
other hand, the perceptron and stochastic gradient methods,
which make frequent but cheap updates, were much faster
at identifying accurate solutions.

We also observed that P ERCEPTRON @ K - AVG and
SGD@ K - AVG, which are based on the tight `avg
prec@k (·)
surrogate, were the most consistent at converging to
accurate solutions whereas P ERCEPTRON @ K - MAX and
SGD@ K - MAX, which are based on the loose `max
prec@k (·)
surrogate, showed large deviations in performance across
tasks. Also, 1PMB and SVMPerf, which are based on the
non upper-bounding `struct
prec@k (·) surrogate, were frequently
found to converge to suboptimal solutions.
The effect of working with a tight surrogate is also clear
from Figure 3 (a), (b) where the algorithms working with
our novel surrogates were found to consistently outperform
the SVMPerf method which works with the `struct
prec@k (·) surrogate. For these experiments, SVMPerf was allowed a
runtime of up to 50× of what was given to our methods
after which it was terminated.
Finally, to establish the stability of our algorithms, we ran,
both the perceptron, as well as the SGD algorithms with
varying batch lengths (see Figure 3 (c)-(e)). We found the
algorithms to be relatively stable to the setting of the batch
length. To put things in perspective, all methods registered
a relative variation of less than 5% in accuracies across
batch lengths spanning an order of magnitude or more. We
present additional experimental results in Appendix F.

Acknowledgments
HN thanks support from a Google India PhD Fellowship.

4

10

Surrogate Functions for Maximizing Precision at the Top

References
Agarwal, S. The Infinite Push: A new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list. In 11th SIAM International Conference on Data Mining (SDM), pp. 839–850, 2011.

Guruswami, Venkatesan and Raghavendra, Prasad. Hardness of learning halfspaces with noise. SIAM J. Comput.,
39(2):742–765, 2009.

Boucheron, Stphane, Lugosi, Gbor, and Bousquet, Olivier.
Concentration inequalities. In Advanced Lectures in Machine Learning, pp. 208–240. Springer, 2004.

Herbrich, R., Graepel, T., and Obermayer, K. Large margin rank boundaries for ordinal regression. In Smola,
A., Bartlett, P., Schoelkopf, B., and Schuurmans, D.
(eds.), Advances in Large Margin Classifiers, pp. 115–
132. MIT Press, 2000.

Boyd, Stephen, Cortes, Corinna, Mohri, Mehryar, and
Radovanovic, Ana. Accuracy at the top. In 26th Annual
Conference on Neural Information Processing Systems
(NIPS), pp. 953–961, 2012.

Joachims, T. Optimizing search engines using clickthrough
data. In 8th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD), pp.
133–142, 2002.

Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M.,
Hamilton, N., and Hullender, G. Learning to rank using
gradient descent. In 22nd International Conference on
Machine Learning (ICML), pp. 89–96, 2005.

Joachims, Thorsten. A Support Vector Method for Multivariate Performance Measures. In 22nd International
Conference on Machine Learning (ICML), 2005.

Calauzènes, Clément, Usunier, Nicolas, and Gallinari,
Patrick. On the (Non-)existence of Convex, Calibrated
Surrogate Losses for Ranking. In 26th Annual Conference on Neural Information Processing Systems (NIPS),
2012.
Cao, Zhe, Qin, Tao, Liu, Tie-Yan, Tsai, Ming-Feng, and
Li, Hang. Learning to rank: from pairwise approach to
listwise approach. In 24th International Conference on
Machine learning (ICML), pp. 129–136. ACM, 2007.
Chakrabarti, Soumen, Khanna, Rajiv, Sawant, Uma, and
Bhattacharyya, Chiru. Structured Learning for NonSmooth Ranking Losses. In 14th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD), 2008.
Chaudhuri, Sougata and Tewari, Ambuj. Perceptron-like
algorithms and generalization bounds for learning to
rank. CoRR, abs/1405.0591, 2014.
Chaudhuri, Sougata and Tewari, Ambuj. Online ranking
with top-1 feedback. In 18th International Conference
on Artificial Intelligence and Statistics (AISTATS), 2015.
Clémençon, Stéphan and Vayatis, Nicolas. Ranking the
best instances. The Journal of Machine Learning Research, 8:2671–2699, 2007.
Do, Chuong B., Le, Quoc, Teo, Choon Hui, Chapelle,
Olivier, and Smola, Alex. Tighter Bounds for Structured
Estimation. In 22nd Annual Conference on Neural Information Processing Systems (NIPS), 2008.
Freund, Y., Iyer, R., Schapire, R. E., and Singer, Y.
An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933–
969, 2003.

Kar, Purushottam, Narasimhan, Harikrishna, and Jain, Prateek. Online and stochastic gradient methods for nondecomposable loss functions. In 28th Annual Conference on Neural Information Processing Systems (NIPS),
pp. 694–702, 2014.
Le, Quoc V. and Smola, Alexander J. Direct optimization
of ranking measures. arXiv preprint arXiv:0704.3359,
2007.
Li, Nan, Jin, Rong, and Zhou, Zhi-Hua. Top rank optimization in linear time. In 28th Annual Conference on Neural
Information Processing Systems (NIPS), pp. 1502–1510,
2014.
Minsky, Marvin Lee and Papert, Seymour. Perceptrons:
An Introduction to Computational Geometry. MIT Press,
1988. ISBN 0262631113.
Narasimhan, Harikrishna and Agarwal, Shivani. A Structural SVM Based Approach for Optimizing Partial AUC.
In 30th International Conference on Machine Learning
(ICML), 2013a.
Narasimhan, Harikrishna and Agarwal, Shivani. SVMtight
pAUC :
A New Support Vector Method for Optimizing Partial
AUC Based on a Tight Convex Upper Bound. In 19th
ACM SIGKDD Conference on Knowledge, Discovery
and Data Mining (KDD), 2013b.
Narasimhan, Harikrishna, Kar, Purushottam, and Jain, Prateek. Optimizing Non-decomposable Performance Measures: A Tale of Two Classes. In 32nd International
Conference on Machine Learning (ICML), 2015.
Novikoff, A.B.J. On convergence proofs on perceptrons.
In Proceedings of the Symposium on the Mathematical
Theory of Automata, volume 12, pp. 615–622, 1962.

Surrogate Functions for Maximizing Precision at the Top

Prabhu, Yashoteja and Varma, Manik. Fastxml: a fast, accurate and stable tree-classifier for extreme multi-label
learning. In 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD,
pp. 263–272, 2014.
Rosenblatt, Frank. The perceptron: A probabilistic model
for information storage and organization in the brain.
Psychological Review, 65(6):386–408, 1958.
Rudin, C. The p-norm push: A simple convex ranking algorithm that concentrates at the top of the list. Journal
of Machine Learning Research, 10:2233–2271, 2009.
Tsoumakas, Grigorios and Katakis, Ioannis. Multi-Label
Classification: An Overview. International Journal of
Data Warehousing and Mining, 3(3):1–13, 2007.
Valizadegan, Hamed, Jin, Rong, Zhang, Ruofei, and Mao,
Jianchang. Learning to rank by optimizing NDCG measure. In 26th Annual Conference on Neural Information
Processing Systems (NIPS), pp. 1883–1891, 2009.
Yue, Y., Finley, T., Radlinski, F., and Joachims, T. A support vector method for optimizing average precision. In
30th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval (SIGIR), pp. 271–278, 2007.
Yun, Hyokun, Raman, Parameswaran, and Vishwanathan,
S. Ranking via robust binary classification. In 28th Annual Conference on Neural Information Processing Systems (NIPS), pp. 2582–2590, 2014.
Zhang, Tong. Covering Number Bounds of Certain Regularized Linear Function Classes. Journal of Machine
Learning Research, 2:527–550, 2002.

