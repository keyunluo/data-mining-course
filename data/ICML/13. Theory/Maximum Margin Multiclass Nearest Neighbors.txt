Maximum Margin Multiclass Nearest Neighbors

Aryeh Kontorovich
Department of Computer Science, Ben-Gurion University, Beer Sheva 84105, ISRAEL

KARYEH @ CS . BGU . AC . IL

Roi Weiss
Department of Computer Science, Ben-Gurion University, Beer Sheva 84105, ISRAEL

ROIWEI @ CS . BGU . AC . IL

Abstract
We develop a general framework for marginbased multicategory classification in metric
spaces. The basic work-horse is a marginregularized version of the nearest-neighbor classifier. We prove generalization bounds that
match the state of the art in sample size n and significantly improve the dependence on the number
of classes k. Our point of departure is a nearly
Bayes-optimal finite-sample risk bound independent of k. Although k-free, this bound is unregularized and non-adaptive, which motivates
our main result: Rademacher and scale-sensitive
margin bounds with a logarithmic dependence on
k. As the best previous
√ risk estimates in this setting were of order k, our bound is exponentially sharper. From the algorithmic standpoint,
in doubling metric spaces our classifier may be
trained on n examples in O(n2 log n) time and
evaluated on new points in O(log n) time.

1. Introduction
Whereas the theory of supervised binary classification is by
now fairly well developed, its multiclass extension continues to pose numerous novel statistical and computational
challenges. On the algorithmic front, there is the basic
question of how to adapt the hyperplane and kernel methods — ideally suited for two classes — to three or more.
A host of new problems also arise on the statistical front.
In the binary case, the VC-dimension characterizes the
distribution-free sample complexity (Anthony & Bartlett,
1999) and tighter distribution-dependent bounds are available via Rademacher techniques (Bartlett & Mendelson,
2002; Koltchinskii & Panchenko, 2002). Characterizing
the multiclass distribution-free sample complexity is far
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

less straightforward, though impressive progress has been
recently made (Daniely et al., 2011).
Following von Luxburg & Bousquet (2004); Gottlieb et al.
(2010), we adopt a proximity-based approach to supervised
multicategory classification in metric spaces. The principal
motivation for this framework is two-fold:
(i) Many natural metrics, such as L1 , earthmover, and
edit distance cannot be embedded in a Hilbert space
without a large distortion (Enflo, 1969; Naor &
Schechtman, 2007; Andoni & Krauthgamer, 2010).
Any kernel method is thus a priori at a disadvantage
when learning to classify non-Hilbertian objects, since
it cannot faithfully represent the data geometry.

(ii) Nearest neighbor-based classification sidesteps the issue of k-to-binary reductions — which, despite voluminous research, is still the subject of vigorous
debate (Rifkin & Klautau, 2004; El-Yaniv et al.,
2008). In terms of time complexity, the reductions
approach faces an Ω(k) information-theoretic lower
bound (Beygelzimer et al., 2009), while nearest neighbors admit solutions whose runtime does not depend
on the number of classes.
Main results. Our contributions are both statistical and
algorithmic in nature. On the statistical front, we open
with the observation that the nearest-neighbor classifier’s
expected risk is at most twice the Bayes optimal plus a term
that decays with sample size at a rate not dependent on the
number of classes k (and continues to hold for k = ∞,
Theorem 1). Although of interest as apparently the first
“k-free” finite-sample result, it has the drawback of being
non-adaptive in the sense of depending on properties of
the unknown sampling distribution and failing to provide
the learner with a usable data-dependent bound. This difficulty is overcome in our main technical contribution (Theorems 4 and 5), where we give a margin-based multiclass

Maximum Margin Multiclass Nearest Neighbors

bound of order
( 

 1
1 )
log k 2
1 log k D+1 1
min
, D
,
γ
n
n
γ2

(1)

where k is the number of classes, n is sample size, D is
the doubling dimension of the metric instance space and
0 < γ ≤ 1 is the margin. This matches the state of the
art asymptotics in n for metric spaces and significantly improves
the dependence on k, which hitherto was of order
√
k (Zhang, 2002; 2004) or worse. The exponential dependence on some covering dimension (such as D) is in
general inevitable, as shown by a standard no-free-lunch argument (Ben-David & Shalev-Shwartz, 2014), but whether
(1) is optimal remains an open question.
On the algorithmic front, using the above bounds, we show
how to efficiently perform Structural Risk Minimization
(SRM) so as to avoid overfitting. This involves deciding
how many and which sample points one is allowed to err
on. We reduce this problem to minimal vertex cover, which
admits a greedy 2-approximation. Our algorithm admits
a significantly faster ε-approximate version in doubling
spaces with a graceful degradation in ε of the generalization bounds, based on approximate nearest neighbor techniques developed by Gottlieb et al. (2010; 2013a). For doubling dimension D and ε, our runtime is O(2O(D) n2 log n)
for learning and O(2O(D) log n) for evaluation on a test
point. (Exact nearest neighbor requires Θ(n) evaluation
time.) Finally, our generalization bounds and algorithm can
be made adaptive to the intrinsic dimension of the data via a
recent metric dimensionality-reduction technique (Gottlieb
et al., 2013b).
Related work. Due to space constraints, we are only
able to mention the most directly relevant results — and
even these, not in full generality but rather with an eye
to facilitating comparison to the present work. Supervised k-category classification approaches follow two basic paradigms: (I) defining a score function on point-label
pairs and classifying by choosing the label with the optimal score and (II) reducing the problem to several binary
classification problems. Regarding the second paradigm,
the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclassto-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < ∞ (and hence does not
apply to nearest neighbors or infinite-dimensional
Hilbert
q


dVC
log k
. Langford &
spaces) and is of the form Õ γ
n
Beygelzimer (2005); Beygelzimer et al. (2009) gave k-free
and O(log k) regret bounds, but these are conditional on
the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper.

As for the first paradigm, proximity is perhaps the most
natural score function — and indeed, a formal analysis
of the nearest neighbor classifier (Cover & Hart, 1967)
much predated the first multiclass extensions of SVM (Weston & Watkins, 1999). Crammer & Singer (2002a;b)
considerably reduced the computational complexity of
the latter approach and gave a risk bound decaying as
Õ(k 2 /nγ 2 ), for the separable case with margin γ. In an
alternative approach based on choosing q prototype examples, Crammer
√ et al. (2002) gave a risk bound with
rate Õ(q k/2 /γ n). Ben-David et al. (1995) characterized the PAC learnability of k-valued functions in terms
of combinatorial dimensions, such as the Natarajan dimension dNat . Guermeur (2007; 2010) gave scale-sensitive analogues ofthese dimensions.
 He gave a risk bound decayp
ing as Õ logγ k dγ Nat /n , where dγ Nat is a scale-sensitive
Natarajan dimension — essentially replacing the finite VC
dimension dVC in Allwein et al. (2001) by dγ Nat . He further
showed that for linear function classes in Hilbert spaces,
dγ Nat is bounded by Õ(k 2 /γ 2 ), resulting in a risk bound
√
decaying as Õ(k/γ 2 n). To the best of our knowledge,
the sharpest current estimate on the Natarajan dimension
(for some special function classes) is dNat = Õ(k) with
a matching lower bound of Ω(k) (Daniely et al., 2011).
A margin-based Rademacher analysis of score functions
√
(Mohri et al., 2012) yields a bound of order Õ(k 2 /γ n),
and this is also the k-dependence obtained by Cortes et al.
(2013) in a recent paper proposing a multiple kernel approach to multiclass learning. Closest in spirit to our work
are the results of Zhang (2002; 2004), who used the chaining technique to q
achieve
 a Rademacher complexity with
asymptotics Õ

1
γ

k
n

.

Besides the dichotomy of score functions vs. multiclassto-binary reductions outlined above, multicategory risk
bounds may also be grouped by the trichotomy of (a) combinatorial dimensions (b) Hilbert spaces (c) metric spaces
(see Table 1). Category (a) is comprised of algorithmindependent results that give generalization bounds in
terms of some combinatorial dimension of a fixed concept class (Allwein et al., 2001; Ben-David et al., 1995;
Guermeur, 2007; 2010; Daniely et al., 2011). Multiclass
extensions of SVM and related kernel methods (Weston
& Watkins, 1999; Crammer & Singer, 2002a;b; Crammer et al., 2002; Cortes et al., 2013) fall into category
(b). Category (c), consisting of agnostic1 metric-space
methods is the most sparsely populated. The pioneering
asymptotic analysis of Cover & Hart (1967) was cast in
a modern, finite-sample version by Ben-David & ShalevShwartz (2014), but only for binary classification. Unlike Hilbert spaces, which admit dimension-free margin
bounds, we are not aware of any metric space risk bound
1

in the sense of not requiring an a priori fixed concept class

Maximum Margin Multiclass Nearest Neighbors

Paper

decay rate Õ(·)
q
dVC
n

log k
γ

‡

Allwein et al. (2001)

Daniely et al. (2011)∗†‡
‡

Guermeur (2010)

dNat log k
nq
dγ Nat
log k
γ
n

group
(II,a)
(I,a)
(I,a)

2

Crammer & Singer (2002b)†

k
γ2n

Cortes et al. (2013)

k
√
γ n

(I,b)

k
√
2

(I,b)

γ qn
1
γ

Zhang (2004)

k
n

1
γ D/2

current paper

1
γ

current paper



In line with paradigm (I) outlined in the Introduction,
our classification procedure consists of optimizing a score
function. In hindsight, the score at a test point will be determined by its labeled neighbors, but for now, we consider an
unspecified collection F of functions mapping X × Y to R.
A score function f ∈ F induces the classifier gf : X → Y
via

(I,b)

2

Guermeur (2010)

The multiclass learning framework. Let (X , d) be a
metric instance space with diam(X ) = 1, ddim(X ) =
D < ∞, and Y ⊆ N an at most countable label set. We
n
observe a sample S = (Xi , Yi )i=1 ∈ {X × Y}n drawn iid
from an unknown distribution P over X × Y.

q

log k
n

(I,b)
log k
n

(I,c)

1
 1+D

gf (x) = argmax f (x, y),

Table 1. Comparing various multiclass bounds. (∗ ) Not margin-based.
(† ) Only for the separable case. (‡ ) Combinatorial dimension depends on k.

that does not explicitly depend on some metric dimension D or covering numbers. The bounds in Ben-David
& Shalev-Shwartz (2014); Gottlieb et al. (2013b) exhibit
a characteristic “curse of dimensionality” decay rate of
O(n−1/(D+1) ), but more optimistic asymptotics can be obtained (Guermeur, 2007; 2010; Zhang, 2002; 2004; Gottlieb et al., 2010). Although some sample lower bounds
for proximity-based methods are known (Ben-David &
Shalev-Shwartz, 2014), the optimal dependence on D and
k is far from being fully understood.

Metric Spaces. Given two metric spaces (X , d) and
(Z, ρ), a function f : X → Z is called L-Lipschitz
if ρ(f (x), f (x0 )) ≤ Ld(x, x0 ) for all x, x0 ∈ X . (The
real line R is always considered with its Euclidean metric |·|.) The Lipschitz constant of f , denoted kf kLip , is
the smallest L for which f is L-Lipschitz. The distance
between two sets A, B ⊂ X is defined by d(A, B) =
inf x∈A,x0 ∈B d(x, x0 ). For a metric space (X , d), let λ be
the smallest value such that every ball in X can be covered
by λ balls of half the radius. The doubling dimension of X
is ddim(X ) := log2 λ. A metric is doubling when its doubling dimension is bounded. The ε-covering number of a
metric space (X , d), denoted N (ε, X , d), is defined as the
smallest number of balls of radius ε that suffices to cover
X . It can be shown (e.g., Krauthgamer & Lee (2004)) that

N (ε, X , d) ≤

breaking ties arbitrarily. The margin of f ∈ F on (x, y) is
defined by
!
1
f (x, y) − sup f (x, y 0 ) .
(4)
γf (x, y) =
2
y 0 6=y
Note that gf misclassifies (x, y) precisely when γf (x, y) <
0. One of our main objectives is to upper-bound the generalization error P(gf (X) 6= Y ) = E[1{γf (X,Y )<0} ]. To this
end, we introduce surrogate loss functions L : R → R+ ,
Lcutoff (u)

= 1{u<1}

Lmargin (u)

=

T[0,1] (1 − u),

where
T[a,b] (z) = max {a, min {b, z}}

2. Preliminaries



2 diam(X )
ε

ddim(X )
,

(2)

where diam(X ) = sup d(x, x0 ) is the diameter of X .
x,x0 ∈X

(3)

y∈Y

(I,c)

(5)

b
is the truncation operator. The empirical loss E[L(γ
f )]
induced
by
any
of
the
loss
functions
above
is
Pn
1
L(γ
(X
,
Y
)).
All
probabilities
P(·)
and
exf
i
i
i=1
n
pectations E[·] are with respect to the sampling distribution
P . We will write ES to indicate expectation over a sample
(i.e., over P n ).

3. Risk bounds
In this section we analyze the statistical properties of
nearest-neighbor multicategory classifiers in metric spaces.
In Section 3.1, Theorem 1, we record the observation that
the 1-nearest neighbor classifier is nearly Bayes optimal,
with a risk decay that does not depend on the number of
classes k. Of course, the naive 1-nearest neighbor is wellknown to overfit. This is reflected in the non-adaptive nature of the analysis: the bound is stated in terms of properties of the unknown sampling distribution, and fails to
provide the learner with a usable data-dependent bound.
To achieve the latter goal, we develop a margin analysis in
Section 3.2. Our main technical result is Lemma 2, from

Maximum Margin Multiclass Nearest Neighbors

which the logarithmic dependence on k claimed in (1) follows. Although not k-free like the Bayes excess risk bound
of Theorem 1, O(log k) is exponentially sharper than the
current state of the art (Zhang, 2002; 2004). Whether a kfree metric entropy bound is possible is currently left as an
open problem.
The metric entropy bound of Lemma 2 facilitates two approaches to bounding the risk: via Rademacher complexity (Section 3.2.2) and via scale-sensitive techniques in the
spirit of Guermeur (2007) (Section 3.2.3). In Section 3.2.4
we combine these two margin bounds by taking their minimum. The resulting bound will be used in Section 4 to
perform efficient Structural Risk Minimization.

yf∗

1

γf∗
1

2

-γf∗

⇒

3
-γf∗

(yf∗ , γf∗ )
3

2
h̃(x)

hf (x, y)

Figure 1. The mapping in Lemma 2 with |Y| = 3.

Note that the bound is independent of the number of classes
k and holds even for k = ∞. The proof is deferred to
Appendix A.

3.1. Multiclass Bayes near-optimality
In this section, (X , d) is a metric space and Y is an at
most countable (possibly infinite) label set. A sample
n
S = (Xi , Yi )i=1 is drawn iid from an unknown distribution P over X × Y. For x ∈ X let (Xπ1 (x) , Yπ1 (x) ) be its
nearest neighbor in S:
π1 (x) = argmin d(Xi , x).
i∈[n]

Thus, the nearest-neighbor classifier gNN is given by
gNN (x) = Yπ1 (x) .

Here again (X , d) is a metric space, but now the label
set Y is assumed finite: |Y| = k < ∞. As before,
n
S = (Xi , Yi )i=1 with (Xi , Yi ) ∼ P iid. It will be convenient to write S y = {Xi : Yi = y, i ∈ [n]} for the subset
of examples with label y. The metric induces the natural
score function fNN (x, y) = −d(x, S y ) with corresponding
nearest-neighbor classifier
gNN (x) = argmax fNN (x, y),

η(x) = P(Y = · | X = x).
The Bayes optimal classifier g ∗ — i.e., one that minimizes
P(g(X) 6= Y ) over all measurable g ∈ Y X — is wellknown to have the form
g ∗ (x) = argmax ηy (x),
y∈Y

where ties are broken arbitrarily. Our only distributional
assumption is that η is L-Lipschitz with respect to the supnorm. Namely, for all x, x0 ∈ X , we have
kη(x) − η(x0 )k∞ ≡ sup |ηy (x) − ηy (x0 )| ≤ Ld(x, x0 ).
y∈Y

This is a direct analogue of the Lipschitz assumption for
the binary case (Cover & Hart, 1967; Ben-David & ShalevShwartz, 2014). We make the additional standard assumption that X has a finite doubling dimension: ddim(X ) =
D < ∞. The Lipschitz and doubling assumptions are sufficient to extend the finite-sample analysis of binary nearest neighbors (Ben-David & Shalev-Shwartz, 2014) to the
multiclass case:
Theorem 1.
4L
.
n1/(D+1)

(7)

y∈Y

(6)

Define the function η : X → RY by

ES [P(gNN (X) 6= Y )] ≤ 2P(g ∗ (X) 6= Y ) +

3.2. Multiclass margin bounds

easily seen to be identical to the one in (6). At this point we
make the simple but crucial observation that the function
fNN (·, y) : X → R is 1-Lipschitz. This will enable us to
generalize the powerful Lipschitz extension framework of
von Luxburg & Bousquet (2004) to |Y| > 2.
We will need a few definitions. Let FL be the collection
of all L-Lipschitz functions from X to R and put FL =
FL × Y. Since each f ∈ FL maps X × Y to R, the margin
γf (x, y) is well-defined via (4). Putting
yf∗ (x)

=

argmax f (x, y),

γf∗ (x)

=

γf (x, yf∗ (x)),

y∈Y

we define the projection Φf :
(
γf∗ (x), if y = yf∗ (x)
Φf (x, y) =
−γf∗ (x), otherwise.
Finally, we define HL as the truncated (as in (5)) projections of functions in FL :

	
HL = (x, y) 7→ T[-1,1] (Φf (x, y)) : f ∈ FL .
(8)
Thus, HL is the set of functions hf : X × Y →
[−1, 1], where each hf (·, y) is L-Lipschitz and hf (x, y) =
±T[-1,1] (γf∗ (x)), depending upon whether y = yf∗ (x), see
Figure 1 (left).

Maximum Margin Multiclass Nearest Neighbors
1

1

b ⊂ H̃2L as follows. At every point xi ∈ N
Construct H
select one of the classes y ∈ Y and set b
h(xi ) = (y, γ(xi ))
with γ(xi ) some multiple of 2Lε0 = ε/4, while maintaining kb
hkLip ≤ 2L. Construct a 2L-Lipschitz extension for
b
h from N to all over X (such an extension always exists,
(McShane, 1934; Whitney, 1934)). We claim that every
classifier in HL , via its twin h̃ ∈ H̃L , is close to some
b
b in the sense that ρ∞ (h̃, b
h ∈ H,
h) ≤ ε. Indeed, every point
x ∈ X is 2ε0 -close to some point xi ∈ N , and since h̃ is
L-Lipschitz and b
h is 2L-Lipschitz,

h̃0 (x)

0

h̃ (x)
h̃(x)

h̃(x)
3

y = y0

2

3

y 6= y 0

2

Figure 2. The metric ρ(h̃(x), h̃0 (x)) with |Y| = 3.

3.2.1. B OUNDING THE METRIC ENTROPY
Our main technical result is a bound on the metric entropy
of HL , which will be used to obtain error bounds (Theorems 4 and 5) for classifiers derived from this function
class. The analysis differs from previous bounds (see Table
1) by explicitly taking advantage of the mutually exclusive
nature of the labels, obtaining an exponential improvement
in terms of the number of classes k. Endow HL with the
sup-norm
k · k∞ = sup max | · | .
x∈X y∈Y

Lemma 2. For any ε > 0,

log N (ε, HL , k · k∞ ) ≤

16L
ε

D


log

5k
ε



ρ(h̃(x), b
h(x))

≤ ρ(h̃(x), h̃(xi )) + ρ(h̃(xi ), b
h(xi ))
+ ρ(b
h(xi ), b
h(x))
≤ Ld(x, xi ) + ε/4 + 2Ld(x, xi ) ≤ ε.

b provides an ε-cover for H̃L (and hence for HL ).
Thus, H
b ≤ (d4k/εe + 1)|N | , since by construction,
Note that |H|
functions b
h are determined by their values on N , which at
a given point can take one of d4k/εe + 1 possible values.
D
Since by (2) we have |N | = N (ε/8L, X , d) ≤ 16L
the
ε
bound follows.
A tighter bound is possible when the metric space (X , d)
possesses two additional properties:

.

Proof. By the definition of HL , for all hf ∈ HL and x ∈ X
there is at most one y ∈ Y such that hf (x, y) > 0. In
addition, if hf (x, y) = c > 0, then hf (x, y 0 ) = −c for all
y 0 6= y. Since γf∗ (x) ≥ 0, we may reparametrize hf (x, y)
by (yf∗ (x), γf∗ (x)) ∈ Y × [0, 1], see Figure 1. To complete
the mapping hf 7→ (yf∗ , γf∗ ), define the following star-like
metric ρ over Y × [0, 1] (see Figure 2):

|γ − γ 0 | y = y 0
0
0
ρ((y, γ), (y , γ )) =
.
γ + γ 0 y 6= y 0

Lemma 3. If (X , d) is connected and centered, then

Let H̃L be the collection of functions h̃ : X → Y × [0, 1]
that are L-Lipschitz:

 D
 !
L
1
log N (ε, HL , k · k∞ ) = O
log k + log
.
ε
ε

ρ(h̃(x), h̃(x0 )) ≤ Ld(x, x0 ),

x, x0 ∈ X .

It is easily verified that the metric space (HL , k · k∞ ) is
isometric to (H̃L , ρ∞ ) with
ρ∞ (h̃, h̃0 ) = sup ρ(h̃(x), h̃0 (x)).
x∈X

Thus, N (ε, HL , k · k∞ ) = N (ε, H̃L , ρ∞ ), and we proceed to bound the latter.2 Fix a covering of X consisting
of |N | = N (ε/8L, X , d) balls {U1 , . . . , U|N | } of radius
|N |

ε0 = ε/8L and choose |N | points N = {xi ∈ Ui }i=1 .
2

The remainder of the proof is based on a technique communicated to us by R. Krauthgamer, a variant of the classic Kolmogorov & Tikhomirov (1959) method.

1. (X , d) is connected if for all x, x0 ∈ X and all
ε > 0, there is a finite sequence of points x =
x1 , x2 , . . . , xm = x0 such that d(xi , xi+1 ) < ε for
all 1 ≤ i < m.
2. (X , d) is centered if for all r > 0 and all A ⊂ X with
diam(A) ≤ 2r, there exists a point x ∈ X such that
d(x, a) ≤ r for all a ∈ A.

Proof. With the additional assumptions on X we follow
the proof idea in Kolmogorov & Tikhomirov (1959) and
b ≤ (d4k/εe + 1)(2k +
demonstrate the tighter bound |H|
|N |−1
|N |
b is constructed as in the
1)
= O((2k) /ε). Here H
proof for Lemma 2 but now each xi ∈ N is taken to be a
“center” of Ui , as furnished by Property 2 above. Let xj ∈
N . Since X is connected, we may traverse a path from x1
to xj via the cover points x1 = xi1 , xi2 , . . . , xim = xj ,
such that the distance between any two successive points
(xil , xil+1 ) is at most 2ε0 = ε/4L. Since b
h is 2L-Lipschitz,
b
on any two such points the value of h can change by at most
ε/2. Thus, given the value b
h(xil ), the value of b
h(xil+1 ) can
take one of at most 2k + 1 values (as Figure 2 shows, at the
star’s hub, b
h(xil+1 ) can take one of 2k + 1 values, while

Maximum Margin Multiclass Nearest Neighbors

at one of the spokes only 5 values are possible). So we are
left to choose the value of b
h on the point x1 to be one from
b and the
the d4k/εe + 1 possible values. The bounds on |H|
metric entropy follow.

The Rademacher complexity of the set of functions HL is
defined by
"
#
n
1X
Rn (HL ) = E sup
σi h(Xi , Yi ) ,
(9)
h∈HL n i=1
where the σi are n independent random variables with
P(σi = +1) = P(σi = −1) = 1/2. In Appendix B,
we invoke Lemma 2 to derive the bound
1/(D+1)

log 5k
Rn (HL ) ≤ cD L
,
(10)
n
(cD is a constant depending only on D), which in turn implies “half” of our main risk estimate (1):
Theorem 4. With probability at least 1 − δ, for all L > 0
and every f ∈ FL with its projected version hf ∈ HL ,

D = 5,
D = 5,
D = 5,
D = 5,
∆ Rad
∆ fat

0.8

∆(m, L, δ)

3.2.2. R ADEMACHER ANALYSIS

1

L=1
L=2
L=4
L=8

0.6
0.4
0.2
0 6
10

9

12

10

10

m

15

10

Figure 3. The combined complexity bounds (k = 10, δ = 0.01).

n
o
1
1
D
of order min L (log(k)/n) D+1 , L 2 (log(k)/n) 2 , as
claimed in (1). In terms of the number of classes k, our
bound compares favorably to those in Allwein et al. (2001);
Guermeur (2007; 2010), and more recently in Daniely
et al. (2011), which have a k-dependence of O(dNat log k),
where dNat is the (scale-sensitive, k-dependent) Natarajan
dimension of the multiclass hypothesis class. The optimal
dependence of the risk on k is an intriguing open problem.

b [L(hf )] + ∆Rad (n, L, δ),
P(gf (X) 6= Y ) ≤ E
where gf is the classifier defined in (3), L is any of the loss
functions defined in Section 2 and ∆Rad (n, L, δ) is at most
s
s

 1

log 2δ
log log2 2L
log 5k D+1
+
.
cD L
+
n
n
2n
+
3.2.3. S CALE - SENSITIVE ANALYSIS
The following Theorem, proved in Appendix C, is an adaptation of Guermeur (2007, Theorem 1), using Lemma 2.
Theorem 5. With probability at least 1 − δ, for all L > 0
and every f ∈ FL with its induced hf ∈ HL ,
b [Lcutoff (hf )] + ∆fat (n, L, δ),
P(gf (X) 6= Y ) ≤ E
where ∆fat (n, L, δ) is at most
s 
 
2
2L
1
D
2 (16L) log (20k) + ln
+ .
n
δ
n
3.2.4. C OMBINED B OUND
Taking L = Lcutoff in Theorem 4 we can merge
the above two bounds by taking their minimum.
Namely, Theorem 5 holds with ∆(n, L, δ)
=
min {∆Rad (n, L, δ), ∆fat (n, L, δ)}
in
place
of
∆fat (n, L, δ), see Figure 3. The resulting risk decay rate is

4. Algorithm
Theorems 4 and 5 yield generalization bounds of the
schematic form
b
P(g(X) 6= Y ) ≤ E[L]
+ ∆(n, L, δ).

(11)

The free parameter L in (11) controls (roughly speaking)
the bias-variance tradeoff: for larger L, we may achieve a
b
smaller empirical loss E[L]
at the expense of a larger hypothesis complexity ∆(n, L, δ). Our Structural Risk Minimization (SRM) consists of seeking the optimal L — i.e.,
one that minimizes the right-hand side of (11) — via the
following high-level procedure:
b
1. For each L > 0, minimize E[L(h
f )] over f ∈ FL .
2. Choose the optimal L∗ and its corresponding classifier
gf with f ∈ FL∗ .
n

Minimizing the empirical loss. Let S = (Xi , Yi )i=1 be
the training sample and L > 0 a given maximal allowed
Lipschiz constant. We will say that a function h ∈ HL
is inconsistent with a sample point (x, y) if h(x, y) < 1
(i.e., if the margin of h on (x, y) is less than one). Denote
by m(L)
b
the smallest possible number of sample points on
which a function h ∈ HL may be inconsistent:
b cutoff (h)].
m(L)
b
= min E[L
h∈HL

Maximum Margin Multiclass Nearest Neighbors

5. Extensions

Thus, our SRM problem consists of finding
∗

L = argmin {m(L)
b
+ ∆(n, L, δ)} .
L>0

For k = 2, Gottlieb et al. (2010) reduced the problem of
computing m(L)
b
to one of finding a minimal vertex cover
in a bipartite graph (by König’s theorem, the latter is efficiently computable as a maximal matching). We will extend this technique to k > 2 as follows. Define the k-partite
graph GL = ({V y }ky=1 , E), where each vertex set V y corresponds to the sample points S y with label y. Now in
order for h ∈ HL to be consistent with the points (Xi , Yi )
and (Xj , Yj ) for Yi 6= Yj , the following relation must hold:
Ld(Xi , Xj ) ≥ 2.

(12)

Hence, we define the edges of GL to consist of all point
pairs violating (12):
(Xi , Xj ) ∈ E

⇐⇒

(Yi 6= Yj ) ∧ (d(Xi , Xj ) < 2/L).

Since removing either of Xi , Xj in (12) also deletes the
violating edge, m(L)
b
is by construction equivalent to the
size of the minimum vertex cover for GL . Although
minimum vertex cover is NP-hard to compute (and even
hard to approximate within a factor of 1.3606, (Dinur &
Safra, 2005)), a 2-approximation may be found in O(n2 )
time (Papadimitriou & Steiglitz, 1998). This yields a 2approximation m̃(L) for m(L).
b
Optimizing over L. Equipped with an efficient routine
for computing m̃(L) ≤ 2m(L),
b
we now seek an L > 0
that minimizes
Q(L) := m̃(L) + ∆(n, L, δ).

(13)

Since the Lipschitz
 constant induced by the data is determined by the n2 distances among the sample points, we
need only consider O(n2 ) values of L. Rather a brute-force
searching all of these values, Theorem 7 of Gottlieb et al.
(2010) shows that using an O(log n) time binary search
over the values of L, one may approximately minimize
Q(L), which in turn yields an approximate solution to (11).
The resulting procedure has runtime O(n2 log n) and guarantees an L̃ for which
Q(L̃) ≤ 4 [m(L
b ∗ ) + ∆(n, L∗ , δ)] .

(14)

Classifying test points. Given the nearly optimal Lipschitz constant L̃ computed above we construct the approximate (within a factor of 4) empirical risk minimizer
h∗ ∈ HL̃ . The latter partitions the sample into S = S0 ∪S1 ,
where S1 consists of the points on which h∗ is consistent
and S0 = S \ S1 . Evaluating h∗ on a test point amounts to
finding its nearest neighbor in S1 . Although in general metric spaces, nearest-neighbors search requires Ω(n) time, for
doubling spaces, an exponential speedup is available via
approximate nearest neighbors (see Section 5).

In this section, we discuss two approaches that render the
methods presented above considerably more efficient in
terms of runtime and generalization bounds. The first is
based on the fact that in doubling spaces, hypothesis evaluation time may be reduced from O(n) to O(log n) at the
expense of a very slight degradation of the generalization
bounds. The second relies on a recent metric dimensionality reduction result. When the data is “close” to being D̃dimensional, with D̃ much smaller than the ambient metric
space dimension D, both the evaluation runtime and the
generalization bounds may be significantly improved —
depending essentially on D̃ rather than D.
5.1. Exponential speedup via approximate NN
If (X , d) is a metric space and x∗ ∈ E ⊂ X is a minimizer
of d(x, x0 ) over x0 ∈ E, then x∗ is a nearest neighbor of
x in E. A simple information-theoretic argument shows
that the problem of computing an exact nearest neighbor in
general metric spaces has Ω(n) time complexity. However,
an exponential speedup is possible if (i) X is a doubling
space and (ii) one is willing to settle for approximate nearest neighbors. A (1 + η) nearest neighbor oracle returns an
x̃ ∈ E such that
d(x, x∗ ) ≤ d(x, x̃) ≤ (1 + η)d(x, x∗ ).

(15)

We will use the fact that in a doubling space, one may
precompute a (1 + η) nearest neighbor data structure in
(2O(ddim(X )) log n + η −O(ddim(X )) )n time and evaluate it
on a test point in 2O(ddim(X )) log n + η −O(ddim(X )) time
(Cole & Gottlieb, 2006; Har-Peled & Mendel, 2006).3
The approximate nearest neighbor oracle induces an ηapproximate version of gNN in defined (7). After performing SRM as described in Section 4, we are left with a subset S1 ⊂ S of the sample, which will be used to label test
points. More precisely, the predicted label of a test point
will be determined by its η-nearest neighbor in S1 .
The exponential speedup afforded by approximate nearest
neighbors comes at the expense of mildly degraded generalization guarantees. The modified generalization bounds
are derived in three steps, whose details are deferred to Appendix D:
(i) We cast the evaluation of h ∈ HL in (8) as a nearest
neighbor calculation with a corresponding h̃ induced by the
(1 + η) approximate nearest neighbor oracle. The nearestneighbor formulation of h is essentially the one obtained
3
Note that an alternative approximate nearest-neighbor search
technique based on Locality Sensitive Hashing (Andoni & Indyk,
2008) is only applicable to vector-represented data and not in general metric spaces.

Maximum Margin Multiclass Nearest Neighbors

by von Luxburg & Bousquet (2004):

1
h(x, y) =
min {ξ(y, y 0 ) + Ld(x, x0 )}
2 S1

(16)


+ max {ξ(y, y 0 ) − Ld(x, x0 )} ,
S1

0

0

where (x , y ) ∈ S1 and ξ(y, y 0 ) = 2 · 1{y=y0 } − 1.
(ii) We observe a simple relation between h and h̃:
kh − h̃k∞ ≡

sup

|h(x, y) − h̃(x, y)| ≤ 2η.

x∈X ,y∈Y

(iii) Defining the 2η-perturbed function class
HL,2η = {T[-1,1] (h0 ) : kh0 − hk∞ ≤ 2η, h ∈ HL },
we relate its metric entropy to that of HL :
Lemma 6. For ε > 2η > 0, we have
N (ε, HL,2η , k · k∞ ) ≤

N (ε − 2η, HL , k · k∞ ).

The metric entropy estimate for HL,2η readily yields ηperturbed versions of Theorems 4 and 5. From the
standpoint of generalization bounds, the effect of the ηperturbation on HL amounts, roughly speaking, to replacing L by L(1 + O(η)), which constitutes a rather benign
degradation.
5.2. Adaptive dimensionality reduction
The generalization bound in (1) and the runtime of our
sped-up algorithm in Section 5.1 both depend exponentially on the doubling dimension of the metric space.
Hence, even a modest dimensionality reduction could lead
to dramatic savings in algorithmic and sample complexities. The standard Euclidean dimensionality-reduction
tool, PCA, until recently had no metric analogue — at least
not with rigorous performance guarantees. The technique
proposed in Gottlieb et al. (2013b) may roughly be described as a metric analogue of PCA.
A set X = {x1 , . . . , xn } ⊂ X inherits the metric d of
X and hence ddim(X) ≤ ddim(X ) is well-defined. We
say that X̃
Pn= {x̃1 , . . . , x̃n } ⊂ X is an (α, β)-perturbation
of X if i=1 d(xi , x̃i ) ≤ α and ddim(X̃) ≤ β. Intuitively, the data is “essentially” low-dimensional if it admits
an (α, β)-perturbation with small α, β, which leads to improved Rademacher estimates. The empirical Rademacher
complexity of HL on a sample S = (X, Y ) ∈ X n × Y n is
given by
 #
"
n

1X

b
σi h(Xi , Yi )  S
Rn (HL ; S) = E sup

h∈HL n
i=1

and is related to Rn defined in (9) via
h
i
b n (HL ; S)
= ES R

Rn (HL )




b n  ≥ ε
P Rn − R
≤ 2 exp(−ε2 n/2),

where the identity is obvious and the inequality is a simple consequence of measure concentration (Mohri et al.,
2012). Hence, up to small changes in constants, the two
may be used in generalization bounds such as Theorem 4
b n lets us
interchangeably. The data-dependent nature of R
exploit essentially low-dimensional data (see Appendix E):
Theorem 7. Let S = (X, Y ) ∈ X n × Y n be the training
sample and suppose that X admits an (α, β)-perturbation
X̃. Then
1 !!

 1+β
log
k
α
b n (HL ; S) = O L
+
.
(17)
R
n
n
A pleasant feature of the bound above is that it does not depend on ddim(X ) (the dimension of the ambient space) or
even on ddim(X) (the dimension of the data). Note the inherent tradeoff between the distortion α and dimension β,
with some non-trivial (α∗ , β ∗ ) minimizing the right-hand
side of (17). Although computing the optimal (α∗ , β ∗ )
seems computationally difficult, Gottlieb et al. (2013b)
were able to obtain an efficient (O(1), O(1))-bicriteria approximation. Namely, their algorithm computes an α̃ ≤
c0 α∗ and β̃ ≤ c1 β ∗ , with the corresponding perturbed
set X̃, for universal constants c0 , c1 , with a runtime of
2O(ddim(X)) n log n + O(n log5 n).
The optimization routine over (α, β) may then be embedded inside our SRM optimization over the Lipschitz constant L in Section 4. The end result will be a nearly optimal
(in the sense of (14)) Lipschitz constant L̃, which induces
the partition S = S0 ∪ S1 , as well as (α̃, β̃), which induce the perturbed set S̃1 . To evaluate our hypothesis on a
test point, we may invoke the (1 + η)-approximate nearestneighbor routine from Section 5.1. This involves a precomputation of time complexity (2O(β̃) log n + η −O(β̃) )n, after
which new points are classified in 2O(β̃) log n + η −O(β̃)
time. Note that the evaluation time complexity depends
only on the “intrinsic dimension” β̃ of the data, rather than
the ambient metric space dimension.
ACKNOWLEDGEMENTS
A.K. was partially supported by the Israel Science Foundation (grant No. 1141/12)
and a Yahoo Faculty award. R.W. was partially supported by the Lynne and William
Frankel Center for Computer Science and a Kreitman Negev Fellowship.

References
Allwein, Erin L, Schapire, Robert E, and Singer, Yoram.
Reducing multiclass to binary: A unifying approach for
margin classifiers. JMLR, 1:113–141, 2001.
Andoni, A. and Krauthgamer, R. The computational hardness of estimating edit distance. SICOMP, 39(6), 2010.
Anthony, M. and Bartlett, P. Neural network learning: theoretical foundations, 1999.

Maximum Margin Multiclass Nearest Neighbors

Bartlett, P. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. JMLR,
3:463-482, 2002.
Ben-David, S. and Shalev-Shwartz, S. Understanding machine learning: From theory to algorithms, 2014.
Ben-David, S., Cesa-Bianchi, N., Haussler, D., and Long,
P.
Characterizations of learnability for classes of
{0, · · · , n}-valued functions. J. CSS., 50(1), 1995.
Beygelzimer, A., Langford, J., and Ravikumar, P. Errorcorrecting tournaments. ALT, 2009.

Gottlieb, L., Kontorovich, A., and Krauthgamer, R. Adaptive metric dimensionality reduction. ALT, 2013b.
Guermeur, Y. VC theory of large margin multi-category
classifiers. JMLR, 8:2551-2594, 2007.
Guermeur, Y. Sample complexity of classifiers taking values in RQ , application to multi-class SVMs. Comm.
Statist. Theory Methods, 39(3):543-557, 2010.
Har-Peled, S. and Mendel, M. Fast construction of nets in
low-dimensional metrics and their applications. SIAM J.
Comp., 35(5):1148-1184, 2006.

Cole, R. and Gottlieb, L. Searching dynamic point sets in
spaces with bounded doubling dimension. STOC, 2006.

Andoni, A. and Indyk, P. Near-optimal hashing algorithms
for approximate nearest neighbor in high dimensions.
Commun. ACM, 51(1):117–122, 2008.

Cortes, C., Mohri, M., and Rostamizadeh, A. Multiclass classification with maximum margin multiple kernel. ICML, 2013.

Kolmogorov, A. and Tikhomirov, V. ε-entropy and εcapacity of sets in function spaces. Uspekhi Matematicheskikh Nauk, 14(2):3-86, 1959.

Cover, T. and Hart, P. Nearest neighbor pattern classification. IEEE Trans. Info. Theo., 13(1):21-27, 1967.

Koltchinskii, V. and Panchenko, D. Empirical margin distributions and bounding the generalization error of combined classifiers. Ann. Statist., 30(1):1-50, 2002.

Crammer, K. and Singer, Y. On the algorithmic implementation of multiclass kernel-based vector machines.
JMLR, 2:265-292, 2002a.
Crammer, K. and Singer, Y. On the learnability and design
of output codes for multiclass problems. Mach. Learn.,
47(2-3):201-233, 2002b.
Crammer, K., Gilad-Bachrach, R., Navot, A., and Tishby,
N. Margin analysis of the lvq algorithm. NIPS, 2002.
Daniely, A., Sabato, S., Ben-David, S., and ShalevShwartz, S. Multiclass learnability and the erm principle.
JMLR - Proceedings Track, 19:207-232, 2011.

Krauthgamer, R. and Lee, J. Navigating nets: Simple algorithms for proximity search. SODA, 2004.
Langford, J. and Beygelzimer, A. Sensitive error correcting
output codes. COLT, 2005.
McShane, E. J. Extension of range of functions. Bull. Amer.
Math. Soc., 40(12):837-842, 1934.
Mohri, M., Rostamizadeh, A., and Talwalkar, A. Foundations Of Machine Learning. The MIT Press, 2012.
Naor, A. and Schechtman, G. Planar earthmover is not in
l1 . SICOMP, 37:804-826, June 2007.

Dinur, I. and Safra, S. On the hardness of approximating
minimum vertex cover. Ann. Math., 162(1), 2005.

Papadimitriou, C. and Steiglitz, K. Combinatorial optimization : algorithms and complexity. 1998.

Dudley, R.M. The sizes of compact subsets of hilbert space
and continuity of gaussian processes. J. Func. Anal., 1
(3):290-330, 1967.

Rifkin, R. and Klautau, A. In defense of one-vs-all classification. JMLR, 5:101-141, 2004.

El-Yaniv, R., Pechyony, D., and Yom-Tov, E. Better multiclass classification via a margin-optimized single binary
problem. Patt. Rec. Lett., 29(14):1954-1959, 2008.
Enflo, P. On the nonexistence of uniform homeomorphisms
between Lp -spaces. Ark. Mat., 8:103-105, 1969.
Gottlieb, L., Kontorovich, A., and Krauthgamer, R. Efficient classification for metric data. COLT, 2010.
Gottlieb, L., Kontorovich, A., and Krauthgamer, R. Efficient regression in metric spaces via approximate lipschitz extension. SIMBAD, 2013a.

von Luxburg, U. and Bousquet, O. Distance-based classification with lipschitz functions. JMLR, 5:669-695, 2004.
Weston, J. and Watkins, C. Support vector machines for
multi-class pattern recognition. ESANN 99, 61-72, 1999.
Whitney, H. Analytic extensions of differentiable functions
defined in closed sets. Trans. Amer. Math. Soc., 36(1):
63-89, 1934.
Zhang, T. Covering number bounds of certain regularized
linear function classes. JMLR, 2:527-550, 2002.
Zhang, T. Statistical analysis of some multi-category large
margin classification methods. JMLR, 5, 2004.

