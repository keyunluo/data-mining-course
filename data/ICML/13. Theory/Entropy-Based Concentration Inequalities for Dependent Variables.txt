Entropy-Based Concentration Inequalities for Dependent Variables

Liva Ralaivola
LIVA . RALAIVOLA @ LIF. UNIV- MRS . FR
QARMA, LIF, CNRS, Aix-Marseille University, F–13288 Marseille cedex 9, France
Massih-Reza Amini
MASSIH - REZA . AMINI @ IMAG . FR
AMA, LIG, CNRS, University Grenoble Alpes, Centre Equation 4, BP 53, F–38041 Grenoble Cedex 9, France

Abstract
We provide new concentration inequalities for
functions of dependent variables. The work extends that of Janson (2004), which proposes concentration inequalities using a combination of
the Laplace transform and the idea of fractional
graph coloring, as well as many works that derive concentration inequalities using the entropy
method (see, e.g., (Boucheron et al., 2003)). We
give inequalities for fractionally sub-additive and
fractionally self-bounding functions. In the way,
we prove a new Talagrand concentration inequality for fractionally sub-additive functions of dependent variables. The results allow us to envision the derivation of generalization bounds for
various applications where dependent variables
naturally appear, such as in bipartite ranking.

1. Introduction
We present new concentration inequalities for specific
functions of possibly dependent random variables. The approach that we advocate is based on the entropy method
and the idea of breaking up the dependencies between random variables thanks to a graph coloring approach. Having
these results at hand allows us to envision the study of the
generalization properties of predictors trained over interdependent data for which a suitable dependency structure
exist. As discussed by Amini & Usunier (2015), this structure could be naturally related to the dependency graph of
the data, or it could be obtained a posteriori from a transformation that reduces a general learning problem to a more
simple case, e.g. some reductions of multiclass classification problems to binary classification problems.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

Related Works. Learning with interdependent data is a
topic that has received quite interest over the past few years;
from a theoretical point of view, it ultimately pertains to
the availability of concentration inequalities designed to account for the dependencies at hand. Among the prominent
works that address this problem are a series of contributions on learning from mixing processes, where the dependencies within a sequence of random variables decreases
over time (Yu, 1994; Karandikar & Vidyasagar, 2002; Kontorovich & Ramanan, 2008; Mohri & Rostamizadeh, 2008;
2009; Samson, 2000; Steinwart & Christmann, 2010). Another line of research within this field, is based on the
idea of graph coloring, designed to divide a graph into
sets of independent sets, and considers subsets of independent random variables deduced from the graph, linking these variables. By mixing the idea of graph coloring with the Laplace transform, Hoeffding-like concentration inequalities for the sum of dependent random variables
were proposed by Janson (2004). Usunier et al. (2006)
later extended this result to provide a generalization of the
bounded differences inequality of McDiarmid (1989) to
the case of interdependent random variables. This extension then paved the way for the definition of the fractional
Rademacher complexity that generalizes the idea of Rademancher complexity and allows one to derive generalization bounds for scenarios where the training data are made
of dependent data. The Chromatic PAC-Bayes bound proposed by Ralaivola et al. (2009; 2010) is another instance of
a generalization bound that builds upon the coloring principle; London et al. (2014) later provided another PACBayesian result for dependent inputs. However, one important issue that has not been explored in these studies, is
the use of second-order (i.e. variance) information: such
information is pivotal to get generalization bounds with
fast learning rates as outlined for instance in (Boucheron
et al., 2005). To this aim, we here consider the entropy
method (Boucheron et al., 2003) that is a central technique to obtain concentration inequalities for certain types
of functions (namely, sub-additive and self-bounding) and
it is at the core of a proof of the well-known Talagrand

Entropy-Based Concentration Inequalities for Dependent Variables

concentration inequality for empirical processes (Bousquet, 2002; Ledoux, 1996; Massart, 2000). This inequality makes it possible then to derive generalization bounds
based on Local Rademacher Complexities (Bartlett et al.,
2005; Koltchinskii, 2006) that may induce fast convergence
rates. To the best of our knowledge, the question of pairing
the entropy method together with the coloring approach has
not yet been studied and, we propose to address it in this
paper.
Contributions. The main theoretical results of the
present paper essentially are of three different kinds. First,
we show that, according to the idea of fractional coloring, it is possible to extend the applicability of concentration of certain types of sub-abdditive and self-bounding
functions, namely fractionally sub-additive and fractionally self-bounding functions to the case of dependent variables; the new Bernstein’s type concentration inequalities
we propose reduce to the usual concentration inequalities
when the random variables at hand are independent. Second, thanks to the derived concentration inequality, we introduce the notion of local fractional Rademacher complexity. Finally, we show how these technical results can be
instantiated for the learning scenario of bipartite ranking.

where D+ (resp. D− ) is the conditional distribution of the
positive or relevant (resp. negative or irrelevant) examples.
To this end, it is natural to consider some empirical risk
R̂` (f, S) on S related to the AUC and defined as
X
X
1
.
1If (Ti )<f (Tj ) , (2)
R̂` (f, S) =
n+ n−
i:Yi =+1 j:Yj =−1

where 1Iπ is the indicator function that is equal to 1 if
the predicate π holds and 0 otherwise. The optimization
of (2) can be carried out by finding a classifier of the form
cf (T, T 0 ) = sgn(f (T ) − f (T 0 )) that minimizes the classification error over the pairs (T, +1) and (T 0 , −1) (Agarwal
& Niyogi, 2005; Clémençon et al., 2008).
.
Yet, if we consider the random variables Xij = (Ti+ , Tj− )
made of pairs of positive Ti+ and negative Tj− examples
in S, then each pair Xij is dependent to another pair Xkl
whenever i = k or j = l, and the empirical classification
error over these pairs is a function of dependent variables.
In this work, we are interested in deriving concentration
inequalities for some functions of dependent variables.

3. Notation and Background Results
3.1. Notation

Organization of the paper. Section 2 states the general
problem we are interested in. In Section 3, we give the formal definition of our framework and explicit the progression of our analysis over the paper. Section 4 presents new
entropy-based concentration inequalities which will allow
to extend several inequalities proposed for empirical processes to the case of dependent variables, and, in Section 5,
we prove a generalization bound for bipartite ranking.

2. Statement of the Problem
Many learning problems deal with interdependent training
data. The study of the consistency of the ERM principle
requires in this case, the availability of concentration inequalities tailored to handle general functions of dependent
random variables. A common example is the reduction of
learning problems to classification of pairs of examples like
in the bipartite ranking or in multiclass classification with
the all-pairs approach (Amini & Usunier, 2015). The former problem deals with the search of a scoring function
over a class F = {f : X → R} of real-valued functions
using a training set S = {(Ti , Yi )}ni=1 where the observations (Ti , Yi ) are supposed to be identically and independently distributed according to some distribution D, in such
a way that P(T,Y ),(T 0 ,Y 0 )∼D ((Y −Y 0 )(f (T )−f (T 0 )) ≤ 0)
is as small as possible. Without loss of generality, we will
preferrably term the problem as that of controlling
PT + ∼D+ ,T − ∼D− (f (T + ) < f (T − )),

(1)

Throughout, we use the following notation. For any pos.
itive integer N , [N ] denotes the set [N ] = {1, . . . , N }.
For a sequence (U1 , . . . , UN ) of elements, that will later
refer to sequences of real values or sequence of random
variables, and any subset C of [N ], UC is the subsequence
.
UC = (Ui )i∈C and, therefore U[N ] = (U1 , . . . , UN ). For
\k .
\k
k ∈ C, the sequence UC is given by UC = (Ui )i∈C\{k}
.
We assume that X[N ] = (X1 , . . . , XN ) is a sequence
of (not necessarily independent) random variables taking
value in some space X ; A denotes the σ-algebra generated
by X[N ] and, for n ∈ [N ], An the σ-algebra generated by
\n .
X[N ] = (X1 , . . . , Xn−1 , Xn+1 , . . . , XN )
Further, f : X N → R is some A-measurable function
which allows us to define the random variable Z as:
.
Z = f (X[N ] )
.
0
0
0
If X[N
] = (X1 , . . . , XN ) is an independent copy of X[N ] ,
(n)

then, for each n, X[N ] and Z (n) are respectively defined as
(n) .
X[N ] = (X1 , . . . , Xn0 , . . . , XN )
.
(n)
Z (n) = f (X[N ] ).

(3)
(4)

Given a subset C = {n1 , . . . , n|C| } of [N ] and some nk ∈
(nk )

C, XC

is defined as
(n ) .
XC k = (Xn1 , . . . , Xn0 k , . . . , Xn|C| )

(5)

Entropy-Based Concentration Inequalities for Dependent Variables

Finally, the expectations taken with respect to An and the
σ-algebra genrated by XC are denoted by En and EC ; when
the context is clear, the former is simply denoted by E.
3.2. Concentration of Sub-Additive and Self-Bounding
Functions
Essential to some of our results are the notions of subadditive functions and self-bounding functions.
Definition 1 (Sub-additive functions). A function f :
X N → R of N variables is sub-additive if there exists a
sequence (fn )n∈[N ] of functions of N − 1 variables such
that for all x[N ] = (x1 , . . . , xN ),
N
X

\n

(f (x[N ] ) − fn (x[N ] )) ≤ f (x[N ] ).

(6)

We recall the following result which provides a bound on
the expectation of the Laplace transform of self-bounding
functions due to McDiarmid & Reed (2006). Note that similar results for weakly self-bounding—a slightly weaker notion than self-bounding— functions are given by Maurer
(2006) and this set of results were refined (with better constants) by Boucheron et al. (2009). We decide to refer to
the result of McDiarmid & Reed (2006) because it implies
upper and lower tail bound in a more compact way—but
the extensions to the dependent case that we propose also
apply to the more recent versions of the results.
Theorem 2 ((McDiarmid & Reed, 2006)). Let f : X N →
R be a (a, b)-self-bounding function. Assume X[N ] is a
.
sequence of independent random variables. Let µ = EZ.
The following holds.

n=1

Definition 2 (Self-bounding functions). A function f :
X → R of N variables is (a, b)-self-bounding if there exists a sequence (fn )n∈[N ] of functions of N − 1 variables
such that for all x[N ] = (x1 , . . . , xN ),
\n

0 ≤ f (x[N ] ) − fn (x[N ] ) ≤ 1, ∀n ∈ [N ],
N
X

\n

(f (x[N ] ) − fn (x[N ] )) ≤ af (x[N ] ) + b.

∀λ ≤ 0

(12)

0 ≤ λ ≤ 1/a.

(13)

3.3. Dependency Graph
(7a)
(7b)

n=1

The concentration inequalities for sub-additive and selfbounding functions (Boucheron et al., 2009; Bousquet,
2003) are based on bounding the log-Laplace transform
function G : R → R defined as
.
G(λ) = log E[exp(λ(Z − EZ))].
(8)
Using Markov’s inequality with the bound exhibited for
G(λ) together with a clever setting of λ is the traditional
way to get concentration inequality per se.
As it will shortly appear, it is convenient (and usual) in the
setting of concentration results to introduce functions ψ :
R → R and ϕ : [0; +∞) → R defined as
.
ψ(x) = exp(−x) + x − 1
(9)
.
ϕ(x) = (1 + x) log(1 + x) − x.
(10)
Theorem 1 ((Bousquet, 2002; 2003)). Let f : X N → R
be a sub-additive function. Assume X[N ] is a sequence
of independent random variables and that (Yn )n∈[N ] is
a sequence of real-valued A-measurable random variables such that P(Yn ≤ Z − Z (n) ≤ 1) = 1 and
2
P(E
 n [Yn ]P≥ 0) = 1.
 Let σ ∈ R be such that
N
P σ 2 ≥ n=1 En Yn2 = 1, If there exists b > 0 such
that for all n ∈ [N ], P(Yn ≤ b) = 1 then, for all λ ≥ 0
G(λ) ≤ ψ(−λ)v
.
where v = (1 + b)EZ + σ 2 .

G(λ) ≤ (aµ + b)ψ(λ),
aµ + b 2
λ ,
G(λ) ≤
2(1 − aλ)

(11)

Specific graphs, namely dependency graphs, are also at the
core of the present study: a graph G = (V, E) is made of
a finite set V of vertices and a set E ⊆ V × V of edges
that connect the vertices. We have the following definition
of an exact proper fractional cover of a graph that we will
make intensive use of afterwards.
Definition 3 (Exact proper fractional cover of G). Let G =
(V, E) be a graph. C = {(Cj , ωj )}j∈[J] , for some positive
integer J, with Cj ⊆ V and ωj ∈ [0, 1] is an exact proper
fractional cover of G, if:
1. it is proper: ∀j, Cj is an independent set, i.e., there is
no connections between vertices in Cj ;
2. it is
P an exact fractional cover of G: ∀v ∈
V, j:v∈Cj ωj = 1.
. P
The weight W (C) of C is given by: W (C) =
j∈[J] ωj
and the minimum weight χ∗ (G) = minC∈K(G) W (C) over
the set K(G) of all exact proper fractional covers of G is
the fractional chromatic number of G.
Note that, as observed by Janson (2004), Lemma 3.2, we
may restrict ourselves to working
P with exact fractional covers, which requires ∀v ∈ V, j:v∈Cj ωj = 1 instead of the
P
weaker condition ∀v ∈ V,
j:v∈Cj ωj ≥ 1 for non-exact
fractional covers, without loss of generality, since any fractional covers induces an exact fractional cover.
From now on, it must be understood that we refer to exact
proper fractional cover when using the simpler term of fractional cover. The reader that is not familiar with this notion
of fractional covers may regard them as generalization of
graph coloring, where the question is to assign the smallest

Entropy-Based Concentration Inequalities for Dependent Variables

number of colors to nodes of a graph so that no two connected nodes share the same color. When colored this way,
the set of points that have the same color are necessarily
independent sets and the coloring might be thought of an
exact proper fractional coloring with every Cj corresponding to color j and every ωj being equal to 1. Given some
graph G, the smallest number of colors χ(G) is its chromatic number and the following holds: χf (G) ≤ χ(G).
.
Definition 4 (Dependency Graph). Let X[N ]
=
(X1 , . . . , XN ) be a sequence of random variables.
.
We may associate the dependency graph GX = (VX , EX )
to X[X] so that i) VX = [N ] and ii) (i, j) ∈ EX if and only
if Xi and Xj are dependent random variables.
Note that there are other notions of dependency graphs that
can be envisioned (see (Janson, 2004)). The present notion
of dependency graph will however suffice to our purpose.
As we shall see, computing an exact proper fractional cover
{(Cj , ωj )}j∈[J] of GX allows one to decompose X[N ] in
sets of independent variables XCj . This will make it possible to have the usual concentration inequalities for independent variables to carry over to the dependent case.

4. New Concentration Inequalities
As stated above, we build upon the works on the entropy method (see, for a somewhat exhaustive overview the
method the work of Boucheron et al. (2003)) and that of
Janson (2004) to provide new concentration inequalities for
functions of dependent random variables.
4.1. Fractionally Colorable Functions
We aim at establishing concentration results for functions
that are more complex than sums of dependent random
variables. To this end, we introduce the notions of fractionally colorable functions, colorable sub-additive functions and colorable self-bounding functions; they refine the
definitions given in the previous section.
Definition 5 (Fractionally colorable function). Let G =
([N ], E) be a graph. A function f : X N → R is fractionally colorable with respect to G if there exists a decomposition DG (f ) = {(fj , Cj , ωj )}j∈[J] of J triplets, such that:
1. C = {(Cj , ωj )}j∈[J] is an exact proper fractional
cover of G;
2. for all j, fj : X |Cj | → R is a function of |Cj | variables
and f decomposes as
X
∀x[N ] ∈ X N , f (x[N ] ) =
ωj fj (xCj )
(14)
j

The decomposition DG (f ) of f is optimal if the weight
of the cover C = {(ωj , Cj )}j∈[J] is the smallest over all
decompositions of f . In that case, the chromatic decom-

position number χf of f is the weight of such an optimal
decomposition.
In the sequel, and without loss of generality, we will always
consider optimal decompositions of fractionally colorable
functions. Also, we will assume that the graph G at hand is
the dependency graph of the sequence X[N ] under study.
We may now assume that we are working with a fractionally colorable function f and we may recall/introduce notation: as before, Z and Z (n) are defined as



.
.
(n)
Z = f X[N ] ,
Z (n) = f X[N ] ,
(n)

and, for j ∈ [J], n ∈ Cj , Zj and Zj

.
Zj = fj XCj ,

(n)

Zj

are defined as:


.
(n)
= fj XCj ,

(n)

where XCj is defined as in equation (5). Hence,
X
Z=
ωj Zj .

(15)

j

Let ΠJ be the family of discrete probability distributions
over J-sets:
.
ΠJ =

(
(p1 , . . . , pJ ) :

J
X

)
pj = 1 and pj > 0, ∀j

(16)

j=1

We then have the following central lemma .
Lemma 1 (Central Lemma). If f is fractionally colorable
then ∀(p1 , . . . , pJ ) ∈ ΠJ , ∀λ ∈ R,

 
X
ωj
(17)
G(λ) ≤ log
pj exp Gj λ
pj
j∈[J]

where


.
Gj (λ) = log ECj exp λ Zj − ECj [Zj ] .

(18)

Proof.
G(λ) = log E[exp(λ(Z − E[Z]))]



X

= log E exp λ
ωj Zj − ECj [Zj ] 
j∈[J]







ω
j
= log E exp λ
pj
Zj − ECj [Zj ] 
pj
j∈[J]




X

ω
j
≤ log E 
pj exp λ
Zj − ECj [Zj ] 
pj
X

j∈[J]

(convexity of x 7→ ex and the Jensen inequality)



X

ωj
= log
pj ECj exp λ
Zj − ECj [Zj ]
pj
j∈[J]

Entropy-Based Concentration Inequalities for Dependent Variables

Remark that the functions Gj ’s are the counterparts of G
for the random variables Zj , which are defined with respect
to the set Cj of independent variables.
4.2. Concentration of Fractionally Sub-Additive
Functions
Definition 6 (Fractionally Sub-Additive function). Let
G = ([N ], E) be some graph. A function f : X N →
R is fractionally sub-additive if it is fractionally colorable with respect to G with decomposition DG (f ) =
{(fj , Cj , ωj )}j∈[J] and each fj is sub-additive.
Proposition 1. Suppose the following assumptions are
true. f is fractionally sub-additive with decomposition
D(f ) = {(fj , Cj , ωj )}j∈[J] . Assume that for all j ∈ [J]:
• (Yj,n )n∈Cj is a sequence of real-valued σ(XCj )measurable random variables such that ∀n ∈ Cj ,
(n)

P(Yj,n ≤ Zj − Zj

≤ 1) = 1,

Theorem 3 (Bennett’s Inequality for Dependent Variables).
Suppose the assumptions of Proposition 1 hold with b1 =
.
. . . = bJ = b and define the constants
. X
.
.
σ2 =
ωj σj2 , v = (1 + b)E[Z] + σ 2 , c = 25χf /16.
j∈[J]

The following results hold:
• for all t ≥ 0

 
v
4t
;
P(Z ≥ E[Z] + t) ≤ exp − ϕ
χf
5v
where ϕ is defined as in (10).
• for all t ≥ 0


√
ct
P Z ≥ E[Z] + 2cvt +
≤ e−t .
3

(20)

(21)

Proof. To get the results, we start from Proposition 1 and
we follow the steps of Janson (2004) (Theorem 3.4). Set

P(Ej,n [Yj,n ] ≥ 0) = 1,

. X
. X
v=
ωj vj , W =
ωj ,

where Ej,n denotes the expectation with respect to the
σ-algebra generated by (XCj \{n} );
• there exists σj2 ∈ R so that

j∈[J]

(22)

j∈[J]



. X
1/2
U=
ωj max 1, vj W 1/2 v −1/2 ,

(23)

j∈[J]


P σj2

≥

X


 2 
Ej,n Yj,n  = 1;

n∈Cj

• there exists a positive bj ∈ R such that ∀n ∈
Cj , P(Yj,n ≤ bj ) = 1;
.
• vj ∈ R denotes the real vj = (1 + bj )E[Zj ] + σj2 .
The following result holds: for all λ ≥ 0 and for all
(p1 , . . . , pJ ) ∈ ΠJ ,



X
ωj
,
(19)
G(λ) ≤ log
pj exp vj ψ −λ
pj
j
where ψ is defined as in (9).



.
1/2
pj = ωj max 1, vj W 1/2 v −1/2 /U.

With these choices, we observe that each summand
of the sum in (19) is such that vj ψ(−λU ωj /pj ) ≤
1/2
vψ(−λU )/W . Indeed, if vj W 1/2 v −1/2 ≤ 1, then
pj = ωj /U , vj ≤ v/W , and


v
ωj
= vj ψ(λU ) ≤
ψ(−λU ).
vj ψ −λ
pj
W
1/2

Otherwise (i.e. vj W 1/2 v −1/2 > 1)
1/2

pj = ωj vj W 1/2 v −1/2 /U
and

(Lemma 1)

!


ωj
v 1/2
vj ψ −λ
= vj ψ −λU 1/2
pj
vj W 1/2

2
1
vj
≤ vj
ψ(−λU ) =
ψ(−λU ),
W
W 1/2

(Theorem 1)

where the inequality comes from a property of ψ that is
recalled in Proposition 5 (in Appendix A).

Proof. Let (p1 , . . . , pJ ) ∈ ΠJ and λ > 0 (the proof is
trivially true for λ = 0).
 

ωj
pj exp Gj λ
pj
j∈[J]



X
ωj
≤ log
pj exp vj ψ −λ
pj

G(λ) ≤ log

X

(24)

j∈[J]

This bounding of vj ψ(−λU ωj /pj ), and the fact that x 7→
ex is an increasing function give
The following result extends Bennett’s inequality presented
by (Bousquet, 2002) to the dependent case.

X
j∈[J]




 v

ωj
pj exp vj ψ −λU
≤ exp
ψ(−λU ) .
pj
W

Entropy-Based Concentration Inequalities for Dependent Variables

Using Markov’s inequality (cf. Theorem 6, Appendix A),
P(Z − E[Z] ≥ t) = P (exp (λ(Z − E[Z])) ≥ exp(λt))
 v

≤ exp
ψ(−λU ) − λt .
W

The upper bound of this inequality is minimized for λ =
ln(1 + tW/vU )/U ; plugging in this value yields



tW
v
.
P(Z − E[Z] ≥ t) ≤ exp − ϕ
W
Uv

Now using the fact that ∀x ∈ R, x ≤ 1 + x2 /4, we get
U≤

X


ωj

1+

j

vj W
4v


=W +

5
vW
= W.
4v
4

Since x 7→ ϕ(t/x) is decreasing for t > 0, we readily have
the following upper bound

P(Z − E[Z] ≥ t) ≤ exp −

v
ϕ
W



4t
5v


.

As said before, we consider only optimal decompositions
of f and the total weight W may be readily replaced by the
chromatic number χf . Finally, we observe that:
v=

X

ωj vj =

j

X

ωj ((1 + b)E[Zj ] + σj2 )

j

= (1 + b)E[Z] + σ 2 .

Proof. (Sketch.) The proof is similar to the one of Bousquet (2003) (Theorem 7.3) and it hinges on the fact that, by
Lemma 2, Appendix A, the Zj ’s are indeed sub-additive
functions and by studying the random variables Yj,n de. n
j
fined
n ), where fn is such that
P
P for n ∈ nCj as: Yj,n = fj (X
k∈Cj f (Xk ) which also
k∈Cj \{n} fj (Xk ) = supf ∈F
yields that b = 1 in the definition of v.
We may now introduce the Local Fractional Rademacher
Complexity which, combined with the previous inequality,
is useful to get generalization bounds (see Section 5).
Definition 7. The Local Fractional Rademacher Complexity R(F, r) is defined as
X
. 2
R(F, r) = Eξ
ωj EXCj
N
j∈[J]

sup

X

ξi f (Xn )

f ∈F :Vf ≤r n∈C

j

(27)
where ξ = (ξ1 , . . . , ξN ) is a sequence of N independent
Rademacher variables: P(ξn = 1) = P(ξn = −1) = 1/2.
This is a generalization of the fractional Rademacher complexity of Usunier et al. (2006). The following holds:
Proposition 2. For all r > 0,
EX[N ]

X
j

ωj

X

sup
f ∈F :Vf ≤r

[Ef (Xn ) − f (Xn )] ≤ N R(F, r).

n∈Cj

Inequality (21) is deduced from (20) and the fact that x ≥
0, ϕ(x) ≥ x2 /(2(1 + x/3).

Proof. A simple symmetrization argument carefully used
in combination with the fractional decomposition of f
gives the result.

This, in turn, gives the following Talagrand’s type inequality for empirical processes in the dependent case.

4.3. Concentration of Fractionally Self-Bounding
Functions

Theorem 4. Let F be a set of functions from X to R
and assume all functions in F are measurable, squareintegrable and satisfy E[f (Xn )] = 0, ∀n ∈ [N ] and
supf ∈F kf k∞ ≤ 1. Assume that C = {(Cj , ωj )}
is a cover
. P
of the dependency graph of X[N ] and let χf = j ωj .

We provide concentration inequalities for a generalization of self-bounding functions, namely, fractionally selfbounding functions. Such results may have some use to
problems that naturally make self-bounding functions appear (Boucheron et al., 2009; McDiarmid & Reed, 2006).

Let us define:
X
. X
Z=
ωj sup
f (Xn )
j∈[J]

f ∈F n∈C
j



P
Let σj be so that σj2 ≥ n∈Cj supf ∈F E f 2 (Xn ) .
. P
Let v = j ωj σj2 + 2E[Z]. For any t ≥ 0,

 
v
4t
P(Z ≥ E[Z] + t) ≤ exp − ϕ
(25)
χf
5v
.
Also, if c = 25χf /16.


√
ct
P Z ≥ E[Z] + 2cvt +
≤ e−t
(26)
3

Definition 8 (Fractionally Self-Bounding Function). Let
G = ([N ], E) be some graph. A function f : X N →
R with decomposition DG (f ) = {(fj , Cj , ωj )}j∈[J] is
({aj }j , {bj }j )-fractionally self-bounding if each fj is
(aj , bj )-self-bounding.
Proposition 3. Let GX be the dependency graph associated with X[N ] . Let f : X N → R be ({aj }j , {bj }j )fractionally self-bounding. The following holds for all
(p1 , . . . , pJ ) ∈ ΠJ ,
• for all λ ≤ 0
G(λ) ≤ log

X
j∈[J]




ωj
pj exp (aj µj + bj )ψ λ
(28)
pj

Entropy-Based Concentration Inequalities for Dependent Variables

• for all 0 ≤ λ < min(pj /(aj ωj )), with µj = ECj [Zj ],
X

G(λ) ≤ log

aj µj + bj
2(1 − λaj ωj /pj )

pj exp

j∈[J]



λωj
pj

It is easy to verify that for all λ ≥ 0 we have

2 !

θj (λ) ≤

.
(29)

1/2 1/2

−1/2
Indeed, if a−1
≤ 1 then θj is bounded as
j γj χf γ

Proof. A combination of Lemma 1 and Theorem 2.
θj (λ) =

This proposition entails the following concentration inequalities for the upper and lower tails of Z.
N

Theorem 5. Let f : X
→ R be ({aj }j , {bj }j )fractionally self-bounding, with decomposition DGX (f ) =
{(fj , Cj , ωj )}j∈[J] . Define

1/2 1/2

θj (λ) =

The following results hold: for all t > 0,

.
where ρ =

1
χf

P

j∈[J]

ωj aj +

1
4γ

(30)
(31)

γ/χf

 (λU )2
1/2 −1/2 −1/2 1/2
2 1 − λU · χf γ
χf
γ

=

γ/χf
(λU )2 ,
2 (1 − λU )

j∈[J]

ωj ajj .

.
If a = a1 = . . . = aJ , then ρ simplifies to ρ = a +
and (31) takes the more convenient form
γ
P(Z ≥ E[Z] + t) ≤ exp − ϕ
χf



where the upper-bounding is possible because λ ≥ 0.
Therefore, from the bound (29) it comes

exp G(λ) = E [exp (λ(Z − E[Z]))] ≤ exp


γ/χf
(λU )2 .
2 (1 − λU )

γ

P



γ/χf

 (λU )2
−1/2 −1/2 1/2
2 1 − λU · aj γj
χf
γ

≤

j

 

4t
γ
,
P(Z ≤ E[Z] − t) ≤ exp − ϕ
χf
5γ

 
γ
t
P(Z ≥ E[Z] + t) ≤ exp − ϕ
,
χf
γρ

γj /a2j
γ/χf
(λU )2 ≤
(λU )2 .
2 (1 − λU )
2 (1 − λU )

−1/2
And if, a−1
> 1, then
j γj χf γ

.
. X
. X
γj = (aj µj + bj ), γ =
ωj γj , χf =
ωj .
j∈[J]

γ/χf
(λU )2
2 (1 − λU )

4at
(4a2 + 1)γ

1
4a


,

(32)

which is similar to Equation (30) for a = 1.
Proof. The proof of Equation (30) follows exactly the same
steps as the proof of Theorem 3 after noticing that the starting point of the former, namely Equation (28), is similar to
the Equation (19), the starting point of the latter, with λ in
place of −λ and γj in place of vj .
The proof of Equation (31) hinges on the following
choices:


. X
1/2 1/2 −1/2
U=
aj ωj max 1, a−1
χf γ
,
j γj

Using Markov’s inequality, and the fact that pj /(aj ωj ) ≥
1/U for all j ∈ [J], we get
P(Z − E[Z] ≥ t) = P(exp (λ(Z − E[Z])) ≥ exp(λt))


γ/χf
(λU )2 − λt
≤ exp
inf
0≤λ<1/U 2 (1 − λU )



γ
χf t
= exp − ϕ
,
χf
γU

where we used Lemma 3 (Appendix A) to get the last line.
Using the inequality ∀x ∈ R, x ≤ 1 + x2 /4 once again, we
may bound U as
U≤

X
j∈[J]


aj ωj

1+

χf γ j
4a2j γ


=

X
j∈[J]

ωj aj +

γj
χf X
ωj
4γ
aj
j∈[J]

and use the fact that x 7→ ϕ(t/x) is decreasing for t >
0.

j∈[J]



.
1/2 1/2 −1/2
pj = aj ωj max 1, a−1
χf γ
/U,
j γj

which allow the argument of the exponential in the righthand side of Equation (29) to be rewritten as, for all j ∈ [J]
.
θj (λ) =

γj
2(1 − λaj ωj /pj )



λωj
pj

2



1/2 1/2 −1/2
γj /a2j · U 2 / max2 1, a−1
χf γ
j γj

 λ2 .
= 
−1 1/2 1/2 −1/2
2 1 − λU/ max 1, aj γj χf γ

5. Induced Generalization Bounds for
Bipartite Ranking
In this section, we show how the concentration inequalities
we established in the previous section can be of some use
to derive generalization bounds for predictors trained on
interdependent data. We will more precisely take advantage of the concentration inequality given by Theorem 3
and provide a generalization bound for the problem of bipartite ranking that can be reduced to the classification of
pairs of examples (see Section 2). To do so, our proof will

Entropy-Based Concentration Inequalities for Dependent Variables

rest on the notion of local fractional Rademacher complexity, a generalization of both notions of local Rademacher
complexity (Bartlett et al., 2005; Koltchinskii, 2006) and
fractional Rademacher complexity (Usunier et al., 2006).
.
If we assume that n+ ≤ n− , it is easy to see that C =
.
{(Cj , ωj = 1)}j∈[n− ] with
.
Cj = {(i, (j + i − 2

mod n− ) + 1) : i = 1, . . . , n+ }

is an exact cover of the dependency graph of (XijP
)ij . The
chromatic number of this cover is therefore χf = j ωj =
n− and R̂` (f, S) decomposes as
R̂` (f, S) =

1 X
N

X

`(f, Xkl ),

(33)

j∈[n− ] (k,l)∈Cj

where, abusing notation, `(f, Xkl ) = `(f, Ti+ , Tj− ) and
.
N = n+ n− . This is a colorable function with respect to
.
the sequence X = (Xkl )kl and the tools we have developed
will help us derive a generalization bound for f .
Given a family of functions F, and r > 0, we define the
parameterized family F`,r which, for r > 0, is given by
.
F`,r = {f : f ∈ F, VX1,1 `(f, X1,1 ) ≤ r},
where V denotes the variance (recall that all the Xkl are
identically distributed). Now denote the function Φ as
h
i
.
Φ(X, r) = N sup EX 0 [R̂` (f, X 0 )] − R̂` (f, X) ,
f ∈F`,r

0

where X is a copy X and where we have used the notation EX 0 [R̂` (f, X 0 )] for ES R̂` (f, S) to make explicit the
dependence on the sequence of dependent variables X 0 . It
is easy to see that
Φ(X, r) ≤

X
j∈[n− ]

sup
f ∈F`,r

X h

0
EX 0 [`(f, Xkl
)] − `(f, Xkl )

i

kl

(k,l)∈Cj

.
= Z.

(34)

When ` takes values in the interval [0, 1] then Theorem 4
readily applies to upper bound the right hand side of (34).
Therefore, for t > 0, the following holds with probability
at least 1 − e−t :
Φ(X, r) ≤ E[Z] +

√

2cvt +

ct
3

where c = 25χf /16 = √
25n− /16√and v ≤ N r + 2E[Z].
√
√
Using a + b ≤ a + b and 2 ab ≤ ua + b/u for all
u > 0, we further have, for all α > 0


√
1
1
Φ(X, r) ≤ (1 + α)E[Z] + 2cN rt +
+
ct,
3 α
and, using Proposition 2, we get, the following proposition.

Proposition 4. With probability 1 − e−t , for all α > 0
Φ(X, r) ≤ (1 + α)N R(F`,r , r) +

√
2cN rt +



1
1
+
3
α


ct,

or, using χf = n− , N = n+ n− , with probability at least
1 − e−t , for all f ∈ F`,r
ES [R̂(f, S)] − R̂(f, S)
r




25 1
1
t
5 2rt
.
+
+
≤ inf (1 + α)R(F`,r , r) +
α>0
4 n+
16 3
α n+

As is common with generalization bounds for bipartite
ranking, the convergence rate is governed by the least represented class, i.e. the positive class here. Note this result
is only the starting point of a wealth of results that may be
obtained using the concentration inequalities studied here.
In particular, it might be possible to sutdy how arguments
based on star hulls and subroot functions may help us to get
fast-rate-like results akin to (Clémençon et al., 2008).

6. Conclusion
We have proposed new concentration inequalities for functions of dependent variables. From these, we derived
a new Talagrand concentration inequality for fractionally
sub-additive functions and fractionally self-bounding functions of dependent variables. An instance of a generalization bounds based on Fractional Local Rademacher Complexity for bipartite ranking exemplifies the usefulness of
our concentration results.
Acknowledgments. This work is partially supported by
the French GIP ANR under contract ANR GRETA 12BS02-004-01 Greediness: theory and algorithms, and the
LabEx PERSYVAL- Lab ANR-11-LABX-0025.

A. Technical Results
Theorem 6 (Markov Inequality). Let X be a nonnegative
random variable. For all a > 0 P(X > a) ≤ E[X]
a .
Proposition 5 (Lemma A.3 of Bousquet (2003)). Let gλ be
.
as gλ = ψ(−λx)/x2 . If λ ≥ 0 then gλ is non-decreasing
on R. If λ ≤ 0 then gλ is non-increasing on R.
Lemma 2 (Lemma C.1 of Bousquet P
(2003)). Let F be a
.
n
set of functions and let P
Z = supf ∈F k=1 f (Xk ). Then,
.
defining Zk = supf ∈F i6=k f (Xi ), Z is sub-additive. The
Pn
.
.
same is true if Z =  supf ∈F | k=1 f (Xk )| and Zk =
P

supf ∈F  i6=k f (Xi ).
Lemma 3 (Lemma 11 of Boucheron et al. (2003)). Let C
and a denote two positive real numbers. Then




2C
at
Cλ2
= 2ϕ
,
supλ∈[0,1/a) λt −
1 − aλ
a
2C

and the supremum is at λ =

1
a



1− 1+


at −1/2
C



.

Entropy-Based Concentration Inequalities for Dependent Variables

References
Agarwal, S. and Niyogi, P. Stability and Generalization
of Bipartite Ranking Algorithms. In COLT, pp. 32–47,
2005.
Amini, M.-R. and Usunier, N. Learning with Partially Labeled and Interdependent Data. Springer, 2015.
Bartlett, P., Bousquet, O., and Mendelson, S. Local
Rademacher complexities. Annals of Statistics, 33(4):
1497–1537, 2005.

London, B., Huang, B., Taskar, B., and Getoor, L. PACBayesian Collective Stability. In Proc. of the 17th Int.
Conf. on Artificial Intelligence and Statistics (AISTATS
14), 2014.
Massart, P. About the constants in Talagrand’s concentration inequalities for empirical processes. The Annals of
Probability, 28(2):863–884, 2000.
Maurer, A. Concentration inequalities for functions of
independent variables. Random Structures and Algorithms, 29:121–138, 2006.

Boucheron, S., Lugosi, G., and Massart, P. Concentration
inequalities using the entropy method. The Annals of
Probability, 31(3):1583–1614, 2003.

McDiarmid, C. On the method of bounded differences.
Survey in Combinatorics, pp. 148–188, 1989.

Boucheron, S., Bousquet, O., and Lugosi, G. Theory
of classification : A survey of some recent advances.
ESAIM. P&S, 9:323–375, 2005.

McDiarmid, C. and Reed, B. Concentration for Selfbounding Functions and an Inequality of Talagrand.
Random Structures and Algorithms, 29(4):549–557,
2006.

Boucheron, S., Lugosi, G., and Massart, P. On concentration of self-bounding functions. Electronic Journal of
Probability, 14(64):1884–1899, 2009.
Bousquet, O. A Bennett Concentration Inequality and its
Application to Suprema of Empirical Processes. CRAS,
Serie I, 334:495–500, 2002.
Bousquet, O. Concentration Inequalities for Sub-Additive
Functions Using the Entropy Method. In Giné, E.,
Houdré, C., and Nualart, D. (eds.), Stochastic Inequalities and Applications, volume 56 of Progress in Probability, pp. 213–247. Birkhäuser Basel, 2003.
Clémençon, S., Lugosi, G., and Vayatis, N. Ranking and
empirical minimization of u -statistics. The Annals of
Statistics, 36(2):844–874, April 2008. ISSN 0090-5364.
Janson, S. Large Deviations for Sums of Partly Dependent
Random Variables. Random Structures and Algorithms,
24(3):234–248, 2004.
Karandikar, R. L. and Vidyasagar, M. Rates of uniform
convergence of empirical means with mixing processes.
Statistics and Probability Letters, 58(3):297307, 2002.
Koltchinskii, V. Local Rademacher complexities and oracle
inequalities in risk minimization. The Annals of Statistics, 34(6):2593–2656, 12 2006.
Kontorovich, L. and Ramanan, K. Concentration inequalities for dependent random variables via the martingale
method. The Annals of Probability, 36(6):2126–2158,
2008.
Ledoux, M. Talagrand deviation inequalities for product
measures. ESAIM: Probabilty and Statistics, 1:63–87,
1996.

Mohri, M. and Rostamizadeh, A. Stability Bounds for Noni.i.d. Processes. In Adv. in Neural Information Processing Systems 20, pp. 1025–1032, 2008.
Mohri, M. and Rostamizadeh, A. Rademacher Complexity
Bounds for Non-I.I.D. Processes. In Adv. in Neural Information Processing Systems 21, pp. 1097–1104, 2009.
Ralaivola, L., Szafranski, M., and Stempfel, G. Chromatic PAC-Bayes Bounds for non-IID Data. In AISTATS
09: JMLR Workshop and Conference Proceedings, volume 5, pp. 416–423, 2009.
Ralaivola, L., Szafranski, M., and Stempfel, G. Chromatic
PAC-Bayes Bounds for Non-IID Data: Applications to
Ranking and Stationary β-Mixing Processes. JMLR,
Journal of Machine Learning Research, pp. 1–30, 2010.
Samson, P.-M. Concentration of measure inequalities for
Markov chains and Φ-mixing processes. Annals of Probability, 28(1):416–461, 2000.
Steinwart, I. and Christmann, A. Fast learning from noni.i.d. observations. In Adv. in Neural Information Processing Systems 22, pp. 1768–1776, 2010.
Usunier, N., Amini, M.-R., and Gallinari, P. Generalization Error Bounds for Classifiers Trained with Interdependent Data. In Adv. in Neural Information Processing
Systems 18, pp. 1369–1376, 2006.
Yu, B. Rates of Convergence for Empirical Processes of
Stationary Mixing Sequences. The Annals of Probability, 22(1):94–116, 1994.

