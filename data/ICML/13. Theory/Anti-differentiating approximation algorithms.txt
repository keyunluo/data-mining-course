Anti-differentiating approximation algorithms:
A case study with min-cuts, spectral, and flow

David F. Gleich
Computer Science, Purdue University, West Lafayette, IN 47906

dgleich@purdue.edu

Michael W. Mahoney
mmahoney@icsi.berkeley.edu
International Computer Science Institute and Dept. of Statistics, University of California at Berkeley, Berkeley, CA 94720

Abstract
We formalize and illustrate the general concept of
algorithmic anti-differentiation: given an algorithmic procedure, e.g., an approximation algorithm
for which worst-case approximation guarantees
are available or a heuristic that has been engineered to be practically-useful but for which a precise theoretical understanding is lacking, an algorithmic anti-derivative is a precise statement of an
optimization problem that is exactly solved by that
procedure. We explore this concept with a case
study of approximation algorithms for finding
locally-biased partitions in data graphs, demonstrating connections between min-cut objectives,
a personalized version of the popular PageRank
vector, and the highly effective “push” procedure
for computing an approximation to personalized
PageRank. We show, for example, that this latter algorithm solves (exactly, but implicitly) an
`1 -regularized `2 -regression problem, a fact that
helps to explain its excellent performance in practice. We expect that, when available, these implicit optimization problems will be critical for
rationalizing and predicting the performance of
many approximation algorithms on realistic data.

1. Introduction
In an ideal setting, one begins with a well-defined objective function and one develops (or calls as a subroutine) an
algorithm to solve that problem “exactly.” In many applications, however, the appropriate objective function might
not be known in advance and/or one might have only an
approximation algorithm available for that objective. In addition, in practical settings, one might heuristically modify
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

the algorithm—e.g, stop at fewer than the theoreticallyappropriate number of iterations, truncate very small entries
of a large dense vector to zero, etc.—in order to achieve
improved performance. In these cases, it can be difficult
to have a precise theoretical understanding of what these
heuristic modifications are doing, even though anecdotal
evidence suggests that these modifications can be the prime
determinants of the practical performance of many machine
learning algorithms on realistic data.
To address these issues, we would like to introduce the
term algorithmic anti-differentiation for the following activity: analyzing a given algorithmic procedure (typically,
an approximation algorithm or a heuristic) and reinterpreting it as a scheme that exactly solves a different (but, of
course, related) optimization problem (implicitly, in the
sense that the actual problem is never written down and
may not even be known a priori). We have chosen the
term “anti-differentiation” because many algorithmic procedures arise as a means to solve exactly the optimality
conditions of some optimization problem. If, instead, we
begin with a practically-useful algorithmic procedure, the
idea behind algorithmic anti-differentiation is to find an optimization problem for which that procedure exactly solves
the the optimality conditions. Here, we are drawing an
analogy between the optimality conditions as the derivative
of the optimization problem; hence, reversing the procedure is “anti-differentiation.” In a simple unconstrained
optimization problem, this analogy is precise, as the optimality conditions are statements about the derivative of the
objective function.1
In this paper, we present a “case study” of algorithmic antidifferentiation, with a focus on two related problems having
1
The analogy also suggests that, just as computing closed-form
anti-derivatives of general functions is much harder than computing
derivatives, one should expect that finding a “nice” expression
for the algorithmic anti-derivative of an arbitrary approximation
heuristic should be much harder than computing an optimum of a
function.

Anti-differentiating Approximation Algorithms

to do with finding locally-biased graph partitions. We start
by showing (in Section 3.1) that the solution of a PageRank
problem (Page et al., 1999) is a 2-norm variation of a 1norm formulation of a min-cut linear program related to
the so-called FlowImprove procedure (Andersen & Lang,
2008). Although a similar fact was known for personalized
PageRank and a locally-biased spectral partitioning problem (Mahoney et al., 2012), our new result permits us to
reverse the relationships and demonstrate (in Section 3.2)
a min-cut/max-flow problem that relaxes to the standard
PageRank problem with uniform teleportation.
We then show (in Section 3.3) that a particularly efficient procedure for solving a personalized PageRank problem (Andersen et al., 2006) implicitly corresponds to adding a 1norm regularization term to the 2-norm PageRank objective.
By examining the optimality conditions for this regularized
problem, we will be able to understand why this procedure
gives rise to both sparse solutions and a sparse truncated
residual; and our analysis also makes a hidden tolerance
parameter in the algorithmic procedure an explicit feature
of our approach.
The utility of these results is that we can begin to understand more precisely the implicit side-effects of heuristic
design decisions that are often made in implementing algorithms. We illustrate this in Section 4. For example,
we compare these procedures to illustrate subtle but important differences between using max-flow/min-cut programs,
their spectral relatives, and the “truncated” versions of their
spectral relatives; and we point out as a remark that we
can also use this same setup to understand other related
diffusion-based methods that have been popular recently in
machine learning.
Far from inventing the concept of algorithmic antidifferentiation, our contribution is to make it precise and to
provide a detailed case study of it for a class of algorithms
that has received a great deal of interest recently in machine
learning and data analysis. For instance, exploiting algorithmic anti-differentiation ideas has been very fruitful for
understanding the relationships between various iterative
methods for solving linear systems of equations in linear
algebra and scientific computing. As an example of this,
Saunders (1995) (in his Results 8 and 9) demonstrates an
equivalence relationship between the iterates of two widelyused algorithms. Although these problems are shown to
be mathematically equivalent in exact arithmetic, the numerical properties—and thus the performance in practical
implementations—of these two methods differ substantially
in the presence of “noise” introduced by roundoff error; and
thus one variant is much preferable in practice. (For this and
related reasons alluded to below, we expect that precisely
understanding algorithmic anti-differentiation will help in
the development of better variants of many popular machine

learning algorithms in very large-scale settings.)
We conclude this introduction with a brief survey of several prior examples of studying approximation algorithms
to elicit surprising optimization properties. While not exhaustive, these are the examples that most informed our
approach; and they are most strongly related to instances
of spectral methods, cuts, and flows. First, Dhillon et al.
(2007) show that the kernel k-means algorithm is equivalent
to a particular type of trace minimization, and they used this
insight to produce a scalable graph clustering method based
on the normalized cut objective. Subsequently, Kulis & Jordan (2012) construct a Bayesian algorithmic anti-derivative,
which gives rise to the Dirichlet process-means (DP-means)
algorithm. Second, Koutra et al. (2011) show how an algorithmic approximation to a set of Belief Propagation equations corresponds to solving a linear system that is closely
related to a diffusion process. Third, Chin et al. (2013)
use the relationship between LASSO-style objectives and
max-flow/min-cut problems (of the form of Prob. (2) below) and shortest paths problems in order to derive new
runtime bounds for the LASSO problems that occur in machine learning (Tibshirani, 1994). Finally, the work that is
perhaps most-closely related to ours shows how algorithmic
decisions such as early stopping (of an iterative algorithm)
can be recast as linear operators that solve particular regularized SDPs (Orecchia & Mahoney, 2011). This has the
interpretation that approximate computation in and of itself
can implicitly implement regularization (e.g., in the “space”
of approximation algorithms (Mahoney, 2012)), whereas
our results below establish similar results for the solutions
of those approximation algorithms.

2. Notation and background
To draw precise relationships between various formulations
of min-cut/max-flow problems on graphs, spectral variations
of these problems, and related PageRank problems, we will
require an unusually high degree of precision in our notation.
Let G = (V, E, w) be a connected, undirected graph with
positive edge weights, and let n = |V| be the number of
vertices. Fix an indexing of the vertices from 1 to n, and
then the adjacency matrix is the n × n matrix A where



w(i, j) (i, j) ∈ E
Ai, j = 

0
otherwise.
The degrees of each node in G are given by the row-sums of
the matrix A. Others have called these the weighted degrees,
but since all of our graphs are weighted, we won’t make
this distinction. The matrix D = diag(Ae), i.e., D is a n × n
square diagonal matrix with the degree of each vertex on
the diagonal; the vector d = De is the vector of degrees.
The vector e will denote the all-ones vector; and ei will
denote the vector with a one in the ith position and zeros

Anti-differentiating Approximation Algorithms

P
everywhere else. In addition, for a set S , eS = i∈S ei ; IS
are columns of the identity matrix for vertices in S in a fixed
order; and vol(S ) is eTS d, the sum of degrees of nodes in S .
Let m be the number of undirected edges in G (i.e., where
each edge is only counted once), and fix an ordering of
these m edges. The edge-node incidence matrix of G is an
m × n matrix B where each row is of the form (ei − e j )T
for an edge (i, j). Note that the incidence matrix does not
include any effect of the edge weights and includes only the
combinatorial structure of the graph. We let the diagonal
matrix C hold the information on the edge weights, where
the order of edges is the same. In particular, we use the
notation C(i, j) to denote a diagonal of this matrix for the
edge (i, j). In this case, the combinatorial Laplacian matrix
is given by L = BT CB = D − A; and the uniform randomwalk transition matrix on G is given by P = D−1 A.
As an aside, it is tempting to define the incidence matrix
as “C−1/2 B” so as to preserve the relationship: “L = BT B”.
This choice would end up causing confusion when we begin
to discuss how PageRank and related diffusion computations
are related to the 1-norm min-cut problems. We have found
it most convenient to treat the edge weights via a weighted
norm, such that if x is a binary vector, then
X
k Bx kC,1 =
C(i, j) |xi − x j |

All of these relationships are straightforward to derive from
the original PageRank equation. We will primarily use the
first and third; but we note that the second arises in a setup
of semi-supervised learning on a graph (Zhou et al., 2004),
a topic upon which we will comment below.

3. Main theoretical results
In this section, we present two algorithmic anti-derivatives.
The first (in Section 3.1) relates the PageRank problem on
an undirected graph to a particular minimum s, t-cut construction. Under certain conditions, this problem is a 2-norm
minorant of a 1-norm formulation of the min-cut objective.
We also (in Section 3.2) establish the reverse relationship
that extracts a cut/flow problem from any PageRank problem. The second (in Section 3.3) relates an approximation
algorithm for the personalized PageRank problem to a 1norm regularized version of the 2-norm objective.
3.1. Relating min-cut to PageRank
Recall the 1-norm formulation of a linear program for the
min-s, t-cut problem:
minimize

k Bx kC,1

subject to

x s = 1, xt = 0, x ≥ 0.

(2)

(i, j)∈E

= cut(S ),

where

S = {i : xi = 1}.

The spectral problem will then replace the 1-norm with a
2-norm, but it will preserve the weighting so that:
X
2
k Bx kC,2
=
C(i, j) (xi − x j )2 .
(i, j)∈E

The PageRank problem for an undirected graph can be defined as the solution of the linear equation (Arasu et al.,
2002; Langville & Meyer, 2006):
(I − βPT )x = (1 − β)v,

(1)

where P is the random walk matrix for the graph (as defined
above), β is the teleportation parameter which satisfies 0 <
β < 1, and v is the non-negative teleportation distribution
P
vector with vi ≥ 0 and i vi = eT v = 1. This formulation is
entirely equivalent to the standard definition as the stationary
distribution of a random walk with restart or the PageRank
Markov chain (Langville & Meyer, 2006).
Recall also that, for an undirected graph, the following
formulations of PageRank are all equivalent:
1. (I − βAD−1 )x = (1 − β)v;
2. (I − βA)y = (1 − β)D−1/2 v,
where A = D−1/2 AD−1/2 and x = D1/2 y; and
3. [αD + L]z = αv where β = 1/(1 + α) and x = Dz.

When the weights are integers, these problems are usually solved using an efficient max-flow routine in order to
compute a strictly-integral solution. Here, we consider a
specific s, t-cut problem inspired by the FlowImprove procedure (Andersen & Lang, 2008), which was also used in
recent work on a local version of this objective (Orecchia
& Zhu, 2014). In order to state this problem, we must fix a
set of vertices S . These roughly correspond to the teleportation vector in the PageRank problem, and we make this
analogy precise below. Once we have this set, we define a
new weighted graph based on the original graph that we call
the localized cut graph. This graph consists of the original
graph, with two additional vertices, s and t, that connect to
S and S̄ , respectively, with weights equal to α multiplied by
the degree of each connected vertex. We illustrate a localized cut graph in Figure 1. We now state the formal version
for completeness.
Definition 1 Let G = (V, E) be a graph, let S be a set of
vertices, possibly empty, let S̄ be the complement set, and
let α be a non-negative constant. Then the localized cut
graph is the weighted, undirected graph with adjacency
matrix:


αdST
0 
 0


A αdS̄  ,
AS = αdS


0
αdTS̄
0
where dS = DeS is a degree vector localized on the set S , A
is the adjacency matrix of the original graph G, and α ≥ 0

Anti-differentiating Approximation Algorithms

Specifically, if x(α, S ) is the solution of Prob. (4), then


 1 


x(α, S ) = vol(S )z .


0
Proof This result is mainly algebraic. The key idea is that
the 2-norm problem corresponds with a quadratic objective,
which PageRank solves. The quadratic objective for the
2-norm approximate cut is:
2
k B(S )x kC(α),2

= xT B(S )T C(α)B(S )x

αvol(S ) −αdTS
T 
L + αD
= x  −αdS

0
−αdS̄

Figure 1. An illustration of a localized cut graph used in our framework. The vertices in the set S are circled. We will use this graph
to illustrate our optimization problems.

is a non-negative weight. Note that the first vertex is s and
the last vertex is t.
In the remainder of the section, we’ll use the α and S parameter to denote the matrices for the localized cut graph.
For example, B(S ) is the incidence matrix of the localized
cut graph, which depends on the set S :


e −IS 0


B(S ) = 0 B 0 ,


0 −IS̄ e
where, recall, the variable IS are the columns of the identity
matrix corresponding to vertices in S . The edge-weights
of the localized cut graph are given by the diagonal matrix
C(α), which depends on the value α. In this case, the minimum weighted s, t cut in the flow graph solves the linear
program:
minimize

k B(S )x kC(α),1

subject to

x s = 1, xt = 0, x ≥ 0.

(3)

Theorem 1 Let B(S ) be the incidence matrix for the localized cut graph, and C(α) be the edge-weight matrix. The
PageRank vector z that solves
(αD + L)z = αv
with v = dS /vol(S ) is a renormalized solution of the 2-norm
cut computation:
k B(S )x kC(α),2

subject to

x s = 1, xt = 0.

If we apply the constraints that x s = 1 and xt = 0 and
let xG be the free set of variables, then we arrive at the
unconstrained objective:
h
1

xGT


T
i αvol(S ) −αdS

0  −αdS
L + αD

0
−αdS̄

 
0   1 
 
−αdS̄  xG 
 
αvol(S̄ ) 0

= xGT (L + αD)xG − 2αxGT dS + αvol(S ).
Here, the solution xG solves the linear system
(αD + L)xG = αdS .
The vector xG = vol(S )z, where z is the solution of the
PageRank problem defined in the theorem, which concludes
the proof.
3.2. Relating PageRank to min-cut

The following theorem is our first algorithmic antiderivative. In it, we show that PageRank implicitly solves a
2-norm variation of the 1-norm formulation of the s, t-cut
problem, as given in Prob. (3). (Recall that the 2-norm is a
minorant of the 1-norm.)

minimize


0 

−αdS̄  x.

αvol(S̄ )

(4)

The result of the previous subsection gives only one “direction” of the relationship between the PageRank problem and
the min-cut problem. That is, there is a cut/flow problem
that gives rise to a PageRank problem. In this subsection,
we establish the reverse relationship that extracts a cut/flow
problem from any PageRank problem. This result is of interest by itself, but it is also of interest as a precursor to our
main result in the next subsection.
Notice that the reason the proof of Theorem 1 works is
that the edges we added had weights proportional to the
degree of the node, and hence the increase to the degree
of the nodes was proportional to their current degree. This
property, in turn, causes the diagonal of the Laplacian matrix
of the localized cut graph to become αD + D. This idea
forms the basis of our subsequent analysis. For a general
PageRank problem, however, we require a slightly more
general definition of the localized cut graph, which we call
a PageRank cut graph.

Anti-differentiating Approximation Algorithms

Definition 2 Let G = (V, E) be a graph, and let s ≥ 0 be a
vector such that d − s ≥ 0. Let s connect to each node in
G with weights given by the vector αs, and let t connect to
each node in G with weights given by α(d − s). Then the
PageRank cut graph is the weighted, undirected graph with
adjacency matrix:


 0
αsT
0 


A
α(d − s) .
A(s) = αs


T
0 α(d − s)
0
We use B(s) to refer to the incidence matrix of this PageRank
cut graph; and note that when s = dS , then we return to the
localized cut graph definition.
With this, we state the following theorem, which is a sort of
converse to Theorem 1, as well as a corollary of independent
interest. Due to its similarity with the proof of Theorem 1,
the proof of this theorem is omitted.
Theorem 2 Consider any PageRank problem that fits the
framework of Prob. (1). The PageRank vector z that solves
(αD + L)z = αv
is a renormalized solution of the 2-norm cut computation:
minimize

k B(s)x kC(α),2

subject to

x s = 1, xt = 0

(5)

s = v. Specifically, if x(α, S ) is the solution of the 2-norm
cut, then
 
1
 
x(α, s) = z .
 
0
Corollary 1 If s = e, then the solution of a 2-norm cut
is a reweighted, renormalized solution of PageRank with
v = e/n.
That is, as a corollary of our framework, the standard PageRank problem with v = e/n gives rise to a cut problem
where s connects to each node with weight α and t connects
to each node v with weight α(dv − 1).
Remark 1 With respect to solving an objective, e.g., of the
form of Prob. (3), note that the weights C(α) will not in
general be integer-valued. In theory, any max-flow/min-cut
solver will compute the correct solution for these cases; but
we have observed poor behavior from most implementations
of the efficient push-relabel method (Goldberg & Tarjan,
1988). The most reliable way to solve these problems for
general non-integer weights is to use a commercial linear
programming package. Another alternative is to scale and
round the weights back to integers. This latter approach
introduces an arbitrary small error and may demand many
bits of precision in the integers.

Remark 2 This machinery we have introduced will produce other diffusion equations as well, depending on the
details of the setup, which may be of independent interest.
For example, consider the equally natural setting where the
edges from s to the graph and the edges from the graph to
t are not degree-weighted, but we’ve reweighted the graph
instead:


 0 eTS
0 
eS θA e  .
S̄ 


0 eS̄ 0
In this case, the 2-norm approximate cut solution on vertices
of the graph solves:
(I + θL)x = eS .
Remark 3 These ideas are likely applicable much more
generally in diffusion-based machine learning. Recall, e.g.,
that the procedure of Zhou et al. (2004) for semi-supervised
learning on graphs solves the following:
(I − βA)−1 Y.
This is exactly a PageRank equation for a degree-based scaling of the labels, and thus our construction from Theorem 2
is directly applicable.
3.3. Relating approximate PageRank and exact 1-norm
regularization
Next, we show that the Andersen, Chung, Lang (ACL) procedure for approximating a personalized PageRank vector (Andersen et al., 2006) (of the form considered in Section 3.1) exactly computes a hybrid 1-norm 2-norm variant
of the min-cut problem. The balance between these two
terms has the effect of producing sparse PageRank solutions
that also have sparse truncated residuals, and it also provides
an interesting connection with `1 -regularized `2 -regression
problems. We start by reviewing the ACL method.
Consider the personalized PageRank problem (I −
βAD−1 )x = (1 − β)v, where v = ei is localized onto a
single node. If A is a connected, undirected graph, then
x a strictly positive solution vector. ACL use an algorithmic procedure to approximate this personalized PageRank
vector with a bounded amount of work. In addition to the
PageRank parameter β, the procedure has two parameters:
τ > 0 is a accuracy parameter that determines when to
stop, and 0 < ρ ≤ 1 is an additional approximation term
that we introduce. As τ → 0, the computed solution x
goes to the personalized PageRank vector that is non-zero
everywhere. The value of ρ has been 1/2 in most previous implementations of the procedure. Here, we present a
modified procedure that differs slightly from their original
algorithm that makes the effect of ρ explicit.2
2

The procedure we describe is based on an implementation
provided by Reid Andersen and matches the procedure used in the
large-scale empirical study (Leskovec et al., 2009).

Anti-differentiating Approximation Algorithms

1. x(1) = 0, r(1) = (1 − β)ei , k = 1
2. while any r j > τd j
d j is the degree of node j
3.
x(k+1) = x(k) + (r j − τd j ρ)e j


τd j ρ
i= j



 (k)
(k+1)
4.
ri
=
ri + β(r j − τd j ρ)/d j i ∼ j




r(k)
otherwise
i
5.
k ←k+1

Consider the optimality conditions of this quadratic problem
(where s are the Lagrange multipliers):
0 = (αD + L)zG − αdS̄ + κd − s
s≥0
zG ≥ 0
zGT s = 0.

One of the important properties of this procedure is that the
algorithm maintains the invariant r = (1−β)v−(I−βAD−1 )x
throughout.3 This method is closely related to the GaussSeidel method, which is a coordinate descent method for
solving the personalized PageRank linear system, except
for a few key differences. First, the algorithm only takes a
partial step along the gradient in each coordinate. Second,
the algorithm does not cycle through all the rows like the
standard Gauss-Seidel procedure; instead, it just picks any
row with a large residual (the original method used a queue
of valid vertices).
For any 0 ≤ ρ ≤ 1, this algorithm converges because the sum
of entries in the residual always decreases monotonically.
At the solution we will have
0 ≤ r ≤ τd,
which provides an ∞-norm style worst-case approximation
guarantee to the exact PageRank solution.
Our main result in this section establishes a precise algorithmic anti-derivative for the ACL procedure, in the case
that ρ = 1. In the same way that Theorem 2 establishes that
a PageRank vector can be interpreted as optimizing an `2
objective involving the edge-incidence matrix, the following
theorem establishes that the ACL procedure to approximate
this vector can be interpreted as solving an `1 -regularized
`2 objective.
Theorem 3 Fix a subset of vertices S . Let x be the output
from the Andersen, Chung, Lang procedure with ρ = 1,
0 < β < 1, v = dS /vol(S ), and τ fixed. Set α = 1−β
β ,
κ = τvol(S )/β, and let zG be the solution on graph vertices
of the sparsity-regularized cut problem:
minimize

2
1
2 k B(S )z kC(α),2

+ κk Dz k1

subject to z s = 1, zt = 0, z ≥ 0
1
where z = zG as above. Then x = DzG /vol(S ).

,

(6)

0

Proof If we expand the objective function and apply the
constraint z s = 1, zt = 0, Prob. (6) becomes:
minimize
subject to
3

1 T
2 zG (αD
2

+ L)zG − αzGT dS

+ α vol(S ) + κdT zG
zG ≥ 0

.

In our description, we assumed the graph had no self-loops
and fixed v = ei . It is easy to adapt to the case of self-loops or
general v by maintaining this invariant.

These are both necessary and sufficient because (αD + L) is
positive definite. In addition, and for the same reason, the
solution is unique.
In the remainder of the proof, we demonstrate that vector
x produced by the ACL method satisfies these conditions.
To do so, we first translate the optimality conditions to the
equivalent PageRank normalization:
0 = (I − βAD−1 )DzG /vol(S )−
(1 − β)dS /vol(S ) + βκ/vol(S )d − βs/vol(S )
s≥0

zG ≥ 0

zGT s = 0.

When the ACL procedure finishes with β, ρ, and τ as in the
theorem, the vectors x and r satisfy:
r = (1 − β)v − (I − βAD−1 )x
x≥0
0 ≤ r ≤ τd = βκ/vol(S )d.
Thus, if we set s such that βs/vol(S ) = βκ/vol(S )d − r, then
we satisfy the first condition with x = DzG /vol(S ). All of
these transformations preserve x ≥ 0 and zG ≥ 0. Also,
because τd ≥ r, we also have s ≥ 0. What remains to be
shown is zGT s = 0.
Here, we show xT (τd − r) = 0, which is equivalent to the
condition zGT s = 0 because the non-zero structure of the
vectors is identical. Orthogonal non-zero structure suffices
because zG s = 0 is equivalent to either xi = 0 or τdi − ri = 0
(or both) for all i. If xi , 0, then at some point in the
execution, the vertex i was chosen at the step r j > τd j . In
that iteration, we set ri = τdi . If any other step increments
ri , we must revisit this step and set ri = τdi again. Then at
a solution, xi , 0 requires ri = τdi . For such a component,
si = 0, using the definition above. For xi = 0, the value of
si is irrelevant, and thus, we have xT (τd − r) = 0.
This proof makes the nature of ρ immediately clear. If ρ < 1,
then the output from ACL is not equivalent to the solution of
Prob. (6). That is, the renormalized solution will not satisfy
zGT s = 0. Setting ρ < 1, however, will compute a solution
much more rapidly. This leaves open the question of the
precise form of the anti-derivative when ρ < 1. We believe
that a precise characterization of these solutions exists, but
have not been able to obtain a simple form.

Anti-differentiating Approximation Algorithms

4. Illustrative empirical results
In this section, we present several empirical results that
illustrate our theory from Section 3. We begin with a few
pedagogical examples in order to state solutions of these
problems that are correctly normalized. These precise values
are sensitive to small differences and imprecisions in solving
the equations. We state them here so that others can verify
the correctness of their future implementations—although
the codes underlying our computations are also available for
download.4 We then illustrate how the 2-norm PageRank
vector (Prob. (4)) and 1-norm regularized vector (Prob. (6))
approximate the 1-norm min-cut problem (Prob. (2)) for a
small but widely-studied social graph.
4.1. Pedagogical examples
We start by solving numerically the various formulations
and variants (min-cut, PageRank, and ACL) of our basic
problem for the example graph illustrated in Figure 1. A
summary of these results may be found in Table 1. To orient
the reader about the normalizations (which can be tricky),
we present the three PageRank vectors that satisfy the relationships xpr = Dz and vol(S )z = x(α, S ), as predicted
by the theory from Section 2 and Theorem 1. Note that
z, x(α, S ) and zG round to the discrete solution produced
by the exact cut if we simply threshold at about half the
largest value. Thus, and not surprisingly, all these formulations reveal—to a first approximation—similar properties
of the graph. Importantly, though, note also that the vector
zG has the same sparsity as the true cut-vector. This is a
direct consequence of the implicit `1 regularization that we
characterize in Theorem 3. Understanding these “details”
matters critically as we proceed to apply these methods to
larger graphs where there may be many small entries in the
PageRank vector z.5
4.2. Newman’s netscience graph
Consider, for instance, Newman’s netscience graph (Newman, 2006), which has 379 nodes and 914 undirected edges.
We considered a set S of 16 vertices, illustrated in Figure 2.
In this case, then the solution of the 1-norm min-cut problem
(Prob. (2)) has 15 non-zeros; the solution of the PageRank
problem (which we have shown in Prob. (4) implicitly solves
an `2 regression problem) has 284 non-zeros;6 and the solution from the ACL procedure (which we have shown in
4
http://www.cs.purdue.edu/homes/dgleich/codes/
l1pagerank
5
The reason is both statistical, in that we encourage sparsity to
find sparse partitions when the original set S is small, as well as
algorithmic, in that the algorithm does not in general touch most of
the nodes of a large graph in order to find small clusters (Mahoney,
2012). These properties were critically exploited in (Leskovec
et al., 2009).
6
The PageRank solution actually has 379 non-zero entries.
However, only 284 of them are non-zero above a 10−5 threshold.

Table 1. Numerical results of solving each problem (Prob. (2),
Prob. (4), and Prob. (6)) for the graph G and set S from Figure 1.
Here, we set α = 1/2, τ = 10−2 , ρ = 1, β = 2/3, and κ = 0.315.
The vector xpr , z, and x(α, S ) are the PageRank vectors from Theorem 1, where x(α, S ) solves Prob. (4) and the others are from the
problems at the end of Section 2. The vector xcut solves the cut
Prob. (2), and zG solves Prob. (6).

Deg.

xpr

z

x(α, S )

xcut

zG

2
4
7
4
4
7
3
2
4
3

0.0788
0.1475
0.2362
0.1435
0.1297
0.1186
0.0385
0.0167
0.0487
0.0419

0.0394
0.0369
0.0337
0.0359
0.0324
0.0169
0.0128
0.0083
0.0122
0.0140

0.8276
0.7742
0.7086
0.7533
0.6812
0.3557
0.2693
0.1749
0.2554
0.2933

1
1
1
1
1
0
0
0
0
0

0.2758
0.2437
0.2138
0.2325
0.1977
0
0
0
0
0

Prob. (6) solves an `1 -regularized `2 regression problem)
has 24 non-zeros. The true “min-cut” set is large in both
the 2-norm PageRank problem and the regularized problem.
Thus, we identify the underlying graph feature correctly;
but the implicitly regularized ACL procedure does so with
many fewer non-zeros than the vanilla PageRank procedure.

5. Discussion and Conclusion
We have shown that the PageRank linear system corresponds
to a 2-norm variation on a 1-norm formulation of a min-cut
problem, and we have also shown that the ACL procedure
for computing an approximate personalized PageRank vector exactly solves a 1-norm regularized version of the PageRank 2-norm objective. Both of these results are examples
of algorithmic anti-differentiation, which involves extracting a precise optimality characterization for the output of an
approximate or heuristic algorithmic procedure.
While a great deal of work has focused on deriving new
theoretical bounds on the objective function quality or runtime of an approximation algorithm, our focus is somewhat
different: we have been interested in understanding what
are the precise problems that approximation algorithms and
heuristics solve exactly. Prior work has exploited these ideas
less precisely in a much larger-scale application to draw very
strong downstream conclusions (Leskovec et al., 2009), and
our empirical results have illustrated this more precisely in
much smaller-scale and more-controlled applications.
Our hope is that one can use these insights in order to combine simple heuristic/algorithmic primitives in ways that
will give rise to principled scalable machine learning algorithms more generally. We have early results along these
lines that involve semi-supervised learning.

Anti-differentiating Approximation Algorithms
16 nonzeros

15 nonzeros

284 nonzeros

24 nonzeros

Figure 2. Examples of the different cut vectors on a portion of the netscience graph. In the left subfigure, we show the set S highlighted
with its vertices enlarged. In the other subfigures, we show the solution vectors from the various cut problems (from left to right, Probs. (2),
(4), and (6), solved with min-cut, PageRank, and ACL) for this set S . Each vector determines the color and size of a vertex, where high
values are large and dark. White vertices with outlines are numerically non-zero (which is why most of the vertices in the fourth figure are
outlined, in contrast to the third figure). The true min-cut set is large in all vectors, but the implicitly regularized problem achieves this
with many fewer non-zeros than the vanilla PageRank problem.

References
Andersen, Reid and Lang, Kevin. An algorithm for improving
graph partitions. In Proceedings of the 19th annual ACM-SIAM
Symposium on Discrete Algorithms, pp. 651–660, 2008.
Andersen, Reid, Chung, Fan, and Lang, Kevin. Local graph partitioning using PageRank vectors. In Proceedings of the 47th
Annual IEEE Symposium on Foundations of Computer Science,
pp. 475–486, 2006.
Arasu, Arvind, Novak, Jasmine, Tomkins, Andrew, and Tomlin,
John. PageRank computation and the structure of the web:
Experiments and algorithms. In Proceedings of the 11th international conference on the World Wide Web, 2002.
Chin, Hui Han, Ma̧dry, Aleksander, Miller, Gary L., and Peng,
Richard. Runtime guarantees for regression problems. In Proceedings of the 4th Conference on Innovations in Theoretical
Computer Science, pp. 269–282, 2013.
Dhillon, Inderjit S., Guan, Yuqiang, and Kulis, Brian. Weighted
graph cuts without eigenvectors: A multilevel approach. IEEE
Trans. Pattern Anal. Mach. Intell., 29(11):1944–1957, 2007.
Goldberg, Andrew V. and Tarjan, Robert E. A new approach to the
maximum-flow problem. Journal of the ACM, 35(4):921–940,
1988.
Koutra, Danai, Ke, Tai-You, Kang, U., Chau, Duen Horng, Pao,
Hsing-Kuo Kenneth, and Faloutsos, Christos. Unifying guilt-byassociation approaches: Theorems and fast algorithms. In Proceedings of the 2011 European conference on Machine learning
and knowledge discovery in databases, pp. 245–260, 2011.
Kulis, Brian and Jordan, Michael I. Revisiting k-means: New
algorithms via Bayesian nonparametrics. In Proceedings of the
29th International Conference on Machine Learning, 2012.
Langville, Amy N. and Meyer, Carl D. Google’s PageRank and
Beyond: The Science of Search Engine Rankings. Princeton
University Press, 2006.

Leskovec, Jure, Lang, Kevin J., Dasgupta, Anirban, and Mahoney,
Michael W. Community structure in large networks: Natural
cluster sizes and the absence of large well-defined clusters.
Internet Mathematics, 6(1):29–123, 2009.
Mahoney, Michael W. Approximate computation and implicit
regularization for very large-scale data analysis. In Proceedings
of the 31st Symposium on Principles of Database Systems, pp.
143–154, 2012.
Mahoney, Michael W., Orecchia, Lorenzo, and Vishnoi,
Nisheeth K. A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs
locally. Journal of Machine Learning Research, 13:2339–2365,
2012.
Newman, M. E. J. Finding community structure in networks using
the eigenvectors of matrices. Phys. Rev. E, 74:036104, 2006.
Orecchia, Lorenzo and Mahoney, Michael W. Implementing regularization implicitly via approximate eigenvector computation.
In Proceedings of the 28th International Conference on Machine
Learning, pp. 121–128, 2011.
Orecchia, Lorenzo and Zhu, Zeyuan Allen. Flow-based algorithms
for local graph clustering. In Proceedings of the 25th ACMSIAM Symposium on Discrete Algorithms, pp. 1267–1286, 2014.
Page, Lawrence, Brin, Sergey, Motwani, Rajeev, and Winograd,
Terry. The PageRank citation ranking: Bringing order to the
web. Technical Report 1999-66, Stanford University, November
1999.
Saunders, Michael A. Solution of sparse rectangular systems
using LSQR and CRAIG. BIT Numerical Mathematics, 35(4):
588–604, 1995.
Tibshirani, Robert. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society, Series B, 58:
267–288, 1994.
Zhou, Dengyong, Bousquet, Olivier, Lal, Thomas Navin, Weston,
Jason, and Schölkopf, Bernhard. Learning with local and global
consistency. In Advances in Neural Information Processing
Systems 16, pp. 321–328, 2004.

