Heavy-tailed regression with a generalized median-of-means

Daniel Hsu
Department of Computer Science, Columbia University

DJHSU @ CS . COLUMBIA . EDU

Sivan Sabato
Microsoft Research New England, 1 Memorial Drive, Cambridge, MA 02446

Abstract
This work proposes a simple and computationally efficient estimator for linear regression, and
other smooth and strongly convex loss minimization problems. We prove loss approximation
guarantees that hold for general distributions, including those with heavy tails. All prior results only hold for estimators which either assume bounded or subgaussian distributions, require prior knowledge of distributional properties, or are not known to be computationally
tractable. In the special case of linear regression with possibly heavy-tailed responses and
with bounded and well-conditioned covariates in
d-dimensions, we show that a random sample
of size OÌƒ(d log(1/Î´)) suffices to obtain a constant factor approximation to the optimal loss
with probability 1âˆ’Î´, a minimax optimal sample
complexity up to log factors. The core technique
used in the proposed estimator is a new generalization of the median-of-means estimator to arbitrary metric spaces.

1. Introduction
Many standard methods for estimation and statistical learning are designed for optimal behavior in expectation, yet
they may be suboptimal for high-probability guarantees.
For instance, the population mean of a random variable
can be estimated by the empirical mean, which is minimaxoptimal with respect to the expected squared error. However, the deviations of this estimator from the true mean
may be large with constant probability unless higher-order
moments are controlled in some way, such as a subguassianity assumption (Catoni, 2012); similar issues arise in
multivariate and high-dimensional estimation problems,
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

SIVAN . SABATO @ MICROSOFT. COM

such as linear regression and convex loss minimization. In
many practical applications, distributions are heavy-tailed
and thus are not subgaussianâ€”they may not even have finite high-order moments. Thus, standard techniques such
as empirical averages may be inappropriate, in spite of their
optimality guarantees under restrictive assumptions.
A case in point is the classical problem of linear regression, where the goal is to estimate a linear function of a
random vector X (the covariate) that predicts the response
(label) Y with low mean squared error. The common approach for this problem is to use ordinary least squares or
ridge regression, which minimize the loss on a finite labeled sample (with regularization in the case of ridge regression). The analyses of Srebro et al. (2010) and Hsu
et al. (2012) for these estimators give sharp rates of convergence of the mean squared error of the resulting predictor
to the optimal attainable loss, but only under assumptions
of boundedness. Audibert & Catoni also analyze these estimators using PAC-Bayesian techniques, and manage to
remove the boundedness assumptions, but they only provide asymptotic guarantees or guarantees which hold only
if n â‰¥ â„¦(1/Î´). The failure of these estimators for general unbounded distributions may not be surprising given
their inherent non-robustness to heavy-tailed distributions
as discussed later in this work.
To overcome the issues raised above, we propose simple
and computationally efficient estimators for linear regression and other convex loss minimization problems. The estimators have near-optimal approximation guarantees, even
when the data distributions are heavy-tailed. Our estimator for the linear regression of a response Y on a ddimensional covariate vector X converges to the optimal
loss at an optimal rate with high probability, with only an
assumption of bounded constant-order moments for X and
Y (see Theorem 1). For comparison, the only previous result with a comparable guarantee is based on an estimator which requires prior knowledge about the response distribution and which is not known to be computationally
tractable (Audibert & Catoni, 2011). Furthermore, in the

Heavy-tailed regression with a generalized median-of-means

case where X is bounded and well-conditioned (but the
distribution of Y may still be heavy-tailed), our estimator achieves, with probability â‰¥ 1 âˆ’ Î´, a multiplicative
constant approximation of the optimal squared loss, with
a sample size of n â‰¥ O(d log(d) Â· log(1/Î´)) (see Theorem 2). This improves on the previous work of Mahdavi
& Jin (2013), whose estimator, based on stochastic gradient descent, requires under the same conditions a sample
sq
size of n â‰¥ O(d5 log(1/(Î´Lsq
? ))), where L? is the optimal squared loss. We also prove an approximation guarantee in the case where X has a bounded distribution in
an infinite-dimensional Hilbert space, as well as general
results for other loss minimization problems with smooth
and strongly-convex losses.
Our estimation technique is a new generalization of the
median-of-means estimator used by Alon et al. (1999) and
many others (see, for instance, Nemirovsky & Yudin, 1983,
p. 243). The basic idea is to repeat an estimate several times
by splitting the sample into several groups, and then selecting a single estimator out of the resulting list of candidates with an appropriate criterion. If an estimator from
one group is good with better-than-fair chance, then the
selected estimator will be good with probability exponentially close to one. Our generalization provides a new simple selection criterion which yields the aforementioned improved guarantees. We believe that our new generalization
of this basic technique will be applicable to many other
problems with heavy-tailed distributions. Indeed, the full
version of this paper (Hsu & Sabato, 2013) reports additional applications to sparse linear regression and low-rank
matrix approximation. In an independent work, Minsker
(2013) considers other variations of the original medianof-means estimator.
We begin by stating and discussing the main results for linear regression in Section 2. We then explain the core technique in Section 3. The application of the technique for
smooth and convex losses is analyzed in Section 4. Section 5 provides the derivations of our main results for regression.

2. Main results
In this section we state our main results for linear regression, which are specializations of more general results given in Section 4. Unlike standard high-probability
bounds for regression, the bounds below make no assumption on the range or the tails of the response distribution
other than a trivial requirement that the optimal squared
loss be finite. We give different bounds depending on conditions on the covariate distributions.
Let [n] := {1, 2, . . . , n} for any natural number n âˆˆ N.
Let Z be a data space, X a parameter space, D a distri-

bution over Z, and Z a Z-valued random variable with
distribution D. Let ` : Z Ã— X â†’ R+ be a non-negative
loss function, and for w âˆˆ X, let L(w) := E(`(Z, w))
be the expected loss. Also define the empirical loss with
respect to a finite P
sample T âŠ‚ Z (where T is a multiset),
LT (w) := |T |âˆ’1 zâˆˆT `(z, w). Let Id be the identity operator on X, and L? := minw L(w). Set w? such that
L? = L(w? ).
For regression, we assume the parameter space X is a
Hilbert space with inner product hÂ·, Â·iX , and Z := X Ã—
R. The loss is the squared loss ` = `sq , defined as
`sq ((x, y), w) := 12 (x> w âˆ’ y)2 . The regularized squared
loss, for Î» â‰¥ 0, is `Î» ((x, y), w) := 21 (hx, wiX âˆ’ y)2 +
1
0
sq
2 Î»hw, wiX ; note that ` = ` . We analogously define
sq
sq
Î»
sq
L , LT , L? , L , etc. as above.
Let X âˆˆ X be a random vector drawn according to
the marginal of D on X, and let Î£ : X â†’ X
be the second-moment operator a 7â†’ E(XhX, aiX ).
For a finite-dimensional X, Î£ is simply the (uncentered) covariance matrix E[XX > ]. For a sample T :=
{X 1 , X 2 , . . . , X m } of m independent copies of X, denote by Î£T : X
X the empirical second-moment operaPâ†’
m
tor a 7â†’ mâˆ’1 i=1 X i hX i , aiX .
The proposed algorithm for regression (Algorithm 1) is
as follows. First, draw k independent random samples i.i.d. from D, and perform linear regression with Î»regularization on each sample separately, to obtain k linear
regressors. Then, use several independent estimations of
the covariance matrix Î£ from i.i.d. samples to select a single regressor from the k regressors at hand. The variant in
Step 5 may be used to obtain tighter bounds in some cases
discussed below.
Algorithm 1 Regression for heavy-tails
input Î» â‰¥ 0, sample sizes n, n0 , confidence Î´ âˆˆ (0, 1).
output Approximate predictor wÌ‚ âˆˆ X.
1: Set k := dC ln(1/Î´)e.
2: Draw k random i.i.d. samples S1 , . . . , Sk from D, each
of size bn/kc.
3: For each i âˆˆ [k], let w i âˆˆ argminwâˆˆX LÎ»
Si (w).
4: Draw a random i.i.d sample T of size n0 , and split it to
k samples {Tj }jâˆˆ[k] of equal size.
5: For each i âˆˆ [k], let ri be the median of the values in
{hwi âˆ’ wj , (Î£Tj + Î» Id)(wi âˆ’ wj )i | j âˆˆ [k] \ {i}}.
[Variant: Use Î£T instead of Î£Tj ].
6: Set i? := arg miniâˆˆ[k] ri .
7: Return wÌ‚ := w i? .
First, consider the finite-dimensional case, where X = Rd ,
and assume Î£ is not singular. In this case we obtain a guarantee for ordinary least squares with Î» = 0. The guarantee
holds whenever the empirical estimate of Î£ is close to the

Heavy-tailed regression with a generalized median-of-means

true Î£ in expectation, a mild condition that requires only
bounded low-order moments. For concreteness, we assume
the following condition.1
Condition 1 (Srivastava & Vershynin 2013). There exists
c, Î· > 0 such that
h
i
Pr kÎ Î£ âˆ’1/2 Xk22 > t â‰¤ ctâˆ’1âˆ’Î· , for t > c Â· rank(Î )
for every orthogonal projection Î  in Rd .
Under this condition, we show the following guarantee for
least squares regression.
Theorem 1. Assume Î£ is not singular. If X satisfies Condition 1 with parameters c and Î·, then there is a constant C = C(c, Î·) such that Algorithm 1 with Î» = 0,
n â‰¥ Cd log(1/Î´), and n0 â‰¥ C log(1/Î´), with probability
at least 1 âˆ’ Î´,
Lsq (wÌ‚) â‰¤


EkÎ£ âˆ’1/2 X(X > w? âˆ’ Y )k22 log(1/Î´)
sq
.
L? + O
n
Define the following finite fourth-moment conditions:
p
p
EkÎ£ âˆ’1/2 Xk42
EkÎ£ âˆ’1/2 Xk42
=
< âˆž and
Îº1 :=
2
âˆ’1/2
d
EkÎ£
Xk2
p
p
E(X > w? âˆ’ Y )4
E(X > w? âˆ’ Y )4
=
< âˆž.
Îº2 :=
>
E(X w? âˆ’ Y )2
Lsq
?
Under these conditions, EkÎ£ âˆ’1/2 X(X > w? âˆ’ Y )k22 â‰¤
Îº1 Îº2 dLsq
? (via Cauchy-Schwartz); if Îº1 and Îº2 are constant, then we obtain the bound



d log(1/Î´)
sq
L (wÌ‚) â‰¤ 1 + O
Lsq
?
n
with probability â‰¥ 1 âˆ’ Î´. In comparison, the recent work
of Audibert & Catoni (2011) proposes an estimator for
linear regression based on optimization of a robust loss
function (see also Catoni, 2012) which achieves essentially
the same guarantee as Theorem 1 (with only mild differences in the moment conditions, see the discussion following their Theorem 3.1). However, that estimator depends on prior knowledge about the response distribution,
and removing this dependency using Lepskiâ€™s adaptation
method (Lepski, 1991) may result in a suboptimal convergence rate. It is also unclear whether that estimator can be
computed efficiently.
1

As shown by Srivastava & Vershynin (2013), Condition 1
holds for various heavy-tailed distributions (e.g., when X has a
product distribution with bounded 4+ moments for some  > 0).
Condition 1 may be easily substituted with other moment conditions, yielding similar results, at least up to logarithmic factors.

Theorem 1 can be specialized for other specific cases of
interest. For instance, suppose X is bounded and wellconditioned in the sense that there exists R < âˆž such that
Pr[X > Î£ âˆ’1 X â‰¤ R2 ] = 1, but Y may still be heavy-tailed
(and, here, we do not assume Condition 1). Then, the following result can be derived using Algorithm 1, with the
variant of Step 5 for slightly tighter guarantees.
Theorem 2. Assume Î£ is not singular. Let wÌ‚ be the output
of the variant of Algorithm 1 with Î» = 0. With probability
at least 1 âˆ’ Î´, for n â‰¥ O(R2 log(R) log(1/Î´)) and n0 â‰¥
O(R2 log(R/Î´)),


 2
R log(1/Î´)
sq
Lsq
L (wÌ‚) â‰¤ 1 + O
? .
n
Note that E(X > Î£ âˆ’1âˆš
X) = E tr(X > Î£ âˆ’1 X) âˆš
= tr(Id) =
d, therefore R = â„¦( d). If indeed R = Î˜( d), then a
total sample size of O(d log(d) log(1/Î´)) suffices to guarantee a constant factor approximation to the optimal loss.
This is minimax optimal up to logarithmic factors (see, e.g.,
Nussbaum, 1999). We also remark that the boundedness
assumption can be replaced by a subgaussian assumption
on X, in which case the sample size requirement becomes
O(d log(1/Î´)).
In recent work of Mahdavi & Jin (2013), an algorithm
based on stochastic gradient descent obtains multiplicative approximations to L? , for general smooth and strongly
convex losses `, with a sample complexity scaling with
log(1/LÌƒ). Here, LÌƒ is an upper bound on L? , which must be
known by the algorithm. The specialization of Mahdavi &
Jinâ€™s main result to square loss implies a sample complexity
sq
of OÌƒ(dR8 log(1/(Î´Lsq
? )) if L? is known. In comparison,
2
Theorem 2 shows that OÌƒ(R log(1/Î´)) suffice when using
our estimator.
It is interesting to note that here we achieve a constant
factor approximation to L? with a sample complexity that
does not depend on the value of L? . This contrasts with
other parametric learning settings, such as classification,
where constant approximation requires â„¦(1/L? ) samples,
and even active learning can only improve the dependence
to â„¦(log(1/L? )) (see, e.g., Balcan et al., 2006).
Finally, we also consider the case where X is a general,
infinite-dimensional Hilbert space, Î» > 0, the norm of X
is bounded, and Y again may be heavy-tailed.
Theorem 3. Let V > 0 such that Pr[hX, XiX â‰¤
V 2 ] = 1. Let wÌ‚ be the output of the variant of Algorithm 1 with Î» > 0. With probability
at least 1 âˆ’ Î´, as
âˆš
2
soon as n â‰¥ O((V âˆš
/Î») log(V / Î») log(1/Î´)) and n0 â‰¥
O((V 2 /Î») log(V /(Î´ Î»)),



(1 + V 2 /Î») log(1/Î´)
LÎ» (wÌ‚) â‰¤ 1 + O
LÎ»? .
n

Heavy-tailed regression with a generalized median-of-means

If the optimal unregularized squared loss Lsq
? is achieved
by wÌ„ âˆˆ p
X with hwÌ„, wÌ„iX â‰¤ B 2 , the choice
2
2
Î» = Î˜( Lsq
? V log(1/Î´)/(B n)) yields that as
0
2 2
â‰¥
soon as n â‰¥ OÌƒ(B V log(1/Î´)/Lsq
? ) and n
sq
2 2
OÌƒ(B V log(1/Î´)/L? ),
Lsq (wÌ‚) â‰¤ Lsq
(1)
?
r sq 2 2

sq
L? B V log(1/Î´) (L? + B 2 V 2 ) log(1/Î´)
+O
+
.
n
n
By this analysis, a constant factor approximation for Lsq
?
is achieved with a sample of size OÌƒ(B 2 V 2 log(1/Î´)/Lsq
? ).
As in the finite-dimensional setting, this rate is known to be
optimal up to logarithmic factors (Nussbaum, 1999).

3. The core technique
In this section we present the core technique from which
Algorithm 1 is derived. We first demonstrate the underlying principle via the median-of-means estimator, and then
explain the generalization to arbitrary metric spaces.
3.1. Warm-up: median-of-means estimator
Algorithm 2 Median-of-means estimator
input Sample S âŠ‚ R of size n, number of groups k âˆˆ N
which divides n.
output Population mean estimate ÂµÌ‚ âˆˆ R.
1: Randomly partition S into k groups S1 , S2 , . . . , Sk ,
each of size n/k.
2: For each i âˆˆ [k], let Âµi âˆˆ R be the sample mean of Si .
3: Return ÂµÌ‚ := median{Âµ1 , Âµ2 , . . . , Âµk }.
We first motivate our procedure for approximate loss minimization by considering the special case of estimating a
scalar population mean using a median-of-means estimator,
given in Algorithm 2. This estimator, heavily used in the
streaming algorithm literature (Alon et al., 1999, though
a similar technique also appears in the textbook by Nemirovsky & Yudin, 1983 as noted by Levin, 2005), partitions a sample into k equal-size groups, and returns the
median of the sample means of each group. The input parameter k is a constant determined by the desired confidence level (i.e., k = log(1/Î´) for confidence Î´ âˆˆ (0, 1)).
The following result is well known.
Proposition 1. Let x be a random variable with mean Âµ
and variance Ïƒ 2 < âˆž, and let S be a set of n independent
copies of x. Assume k divides n. With probability at least
1âˆ’eâˆ’k/4.5 , the estimate ÂµÌ‚ returned
by Algorithm 2 on input
p
(S, k) satisfies |ÂµÌ‚ âˆ’ Âµ| â‰¤ Ïƒ 6k/n.
Proof. Pick any i âˆˆ [k], and observe that Si is an i.i.d. sample of size n/k. Therefore, by Chebyshevâ€™s inequality,

p
2 k/n] â‰¥ 5/6. For each i âˆˆ [k], let
Pr[|Âµi âˆ’ Âµ| â‰¤ 6Ïƒp
bi := 1{|Âµi âˆ’ Âµ| â‰¤ 6Ïƒ 2 k/n}. The bi are independent
indicator random variables, each with E(bi ) â‰¥ 5/6. By
Pk
Hoeffdingâ€™s inequality, Pr[ i=1 bi > k/2] â‰¥ 1 âˆ’ eâˆ’k/4.5 .
Pk
In the event
p { i=1 bi > k/2}, at least half of the Âµi are
within 6Ïƒ 2 k/n of Âµ, so the same holds for the median of
the Âµi .
Remark
âˆš 1. It is remarkable that the estimator has
O(Ïƒ/ n) convergence with exponential probability tails,
even though the random variable x may have heavy-tails
(e.g., no bounded moments beyond the variance). Catoni
(2012) also presents mean estimators with these properties
and also asymptotically optimal constants, although the estimators require Ïƒ as a parameter.
Remark 2. Catoni (2012) shows that the empirical mean
cannot provide a qualitatively similar guarantee: for any
Ïƒ > 0 and Î´ âˆˆ (0, 1/(2e)), there is a distribution with
mean zero and variance Ïƒ 2 such that the empirical average
ÂµÌ‚emp of n i.i.d. draws satisfies

nâˆ’1 
2eÎ´  2
Ïƒ 
â‰¥ 2Î´.
(2)
1âˆ’
Pr |ÂµÌ‚emp | â‰¥ âˆš
n
2nÎ´
Therefore the âˆš
deviation of thepempirical mean necessarily
scales with 1/ Î´ rather than log(1/Î´) (with probability
â„¦(Î´)).
3.2. Generalization to arbitrary metric spaces
We now consider a generalization of the median-of-means
estimator for arbitrary metric spaces, with a metric that can
only be crudely estimated. Let X be the parameter (solution) space, w? âˆˆ X be a distinguished point in X (the
target solution), and Ï a metric on X (in fact, a pseudometric suffices). Let BÏ (w0 , r) := {w âˆˆ X : Ï(w0 , w) â‰¤ r}
denote the ball of radius r around w0 .
The first abstraction captures the generation of candidate
solutions obtained from independent subsamples. We assume there is an oracle APPROXÏ,Îµ which, upon querying, returns a random w âˆˆ X satisfying
h
i
Pr Ï(w? , w) â‰¤ Îµ â‰¥ 2/3.
(3)
We assume that the responses of APPROXÏ,Îµ are generated independently. Note that the 2/3 could be replaced
by another constant larger than half; we have not made any
attempt to optimize constants.
To second abstraction captures the limitations in calculating the metric. We assume there is an oracle DISTÏ which,
if queried with any x, y âˆˆ X, returns a random number
DISTÏ (x, y) satisfying
h
i
Pr Ï(x, y)/2 â‰¤ DISTÏ (x, y) â‰¤ 2Ï(x, y) â‰¥ 8/9. (4)

Heavy-tailed regression with a generalized median-of-means

Algorithm 3 Robust approximation with random distances
input Number of candidates k, query access to
APPROXÏ,Îµ , query access to DISTÏ .
output Approximate solution wÌ‚ âˆˆ X.
1: For each i âˆˆ [k], let w i be the response from querying
APPROXÏ,Îµ ; set W := {w1 , w2 , . . . , wk }.
2: For each i âˆˆ [k], let ri := median{DISTÏ (w i , w j ) :
j âˆˆ [k]}; set i? := arg miniâˆˆ[k] ri .
3: Return wÌ‚ := w i? .

Pk
on the event i=1 bi > 3k/5, i.e., that more than 3/5 of
the wi are contained in BÏ (w? , Îµ).

We assume that the responses of DISTÏ are generated independently (and independent of APPROXÏ,Îµ ). Note that
the responses need not correspond to a metric. Moreover, we will only query DISTÏ for the pairwise distances of k fixed points (the candidate parameters W =
{w1 , w2 , . . . , wk }), and it will suffice for the responses
within each set {DISTÏ (wi , wj )}jâˆˆ[k]\{i} for any fixed i
to be mutually independent.

for such wj , i.e., E(yi,j ) â‰¥ 8/9.
Therefore
Pk
P
E( j=1 yi,j ) â‰¥
jâˆˆ[k]:wj âˆˆBÏ (w? ,Îµ) Eyi,j â‰¥ 8k/15 >
Pk
k/2. By Hoeffdingâ€™s inequality, Pr[ i=1 yi,j â‰¤ k/2] â‰¤
eâˆ’k/45 . Thus, with probability at least 1 âˆ’ eâˆ’k/45 , ri =
median{DISTÏ (wi , wj ) : j âˆˆ [k]} â‰¤ 4Îµ.

The proposed procedure, given in Algorithm 3, generates k
candidate solutions by querying APPROXÏ,Îµ k times, and
then selects a single candidate using a randomized generalization of the median. Specifically, for each i âˆˆ [k], the
radius of smallest ball centered at wi that contains more
than half of {w1 , w2 , . . . , wk } is approximated using calls
to DISTÏ ; the wi with the smallest such approximation is
returned. Again, the number of candidates k determines
the resulting confidence level. The following theorem provides a guarantee for Algorithm 3. The idea of the proof is
illustrated in Figure 1. A similar technique was proposed
by Nemirovsky & Yudin (1983), however their formulation
relies on knowledge of  and the metric.

Îµ

ri?
w?

wÌ‚

Suppose wi
âˆˆ
BÏ (w? , Îµ), and let yi,j
:=
1{DISTÏ (wi , wj ) â‰¤ 4Îµ}.
Observe that for every
wj âˆˆ BÏ (w? , Îµ), Ï(wi , wj ) â‰¤ 2Îµ by the triangle
inequality, and thus
h
i
Pr DISTÏ (wi , wj ) â‰¤ 4Îµ
h
i
â‰¥ Pr DISTÏ (wi , wj ) â‰¤ 2Ï(wi , wj ) â‰¥ 8/9

Now suppose wi 6âˆˆ BÏ (w? , 9Îµ).
Let zi,j :=
1{DISTÏ (wi , wj ) > 4Îµ}. Observe that for every wj âˆˆ
BÏ (w? , Îµ), Ï(wi , wj ) â‰¥ Ï(w? , wi ) âˆ’ Ï(w? , wj ) > 8Îµ by
the triangle inequality, and thus
h
i
Pr DISTÏ (wi , wj ) > 4Îµ
h
i
â‰¥ Pr DISTÏ (wi , wj ) â‰¥ (1/2)Ï(wi , wj ) â‰¥ 8/9
for such wj , i.e., E(zi,j ) â‰¥ 8/9. Therefore, as bePk
fore E( j=1 zi,j ) â‰¥ 8k/15 > k/2. By Hoeffdingâ€™s
inequality, with probability at least 1 âˆ’ eâˆ’k/45 , ri =
median{DISTÏ (wi , wj ) : j âˆˆ [k]} > 4Îµ.
Now take a union bound over the up to k events described
above (at most one for each wi âˆˆ W ) to conclude that with
probability at least 1âˆ’(k+1)eâˆ’k/45 , (i) |W âˆ©BÏ (w? , Îµ)| â‰¥
3k/5 > 0, (ii) ri â‰¤ 4Îµ for all wi âˆˆ W âˆ© BÏ (w? , Îµ),
and (iii) ri > 4Îµ for all wi âˆˆ W \ BÏ (w? , 9Îµ). In this
event the wi âˆˆ W with the smallest ri must satisfy wi âˆˆ
BÏ (w? , 9Îµ).

4. Minimizing strongly convex losses
Figure 1. The main argument in the proof of Theorem 4, illustrated on the Euclidean plane. With probability at least 1 âˆ’ Î´, at
least 3k/5 of the wi (depicted by full circles) are within Îµ of w?
(the empty circle). Therefore, with high probability, wÌ‚ is within
Îµ + ri? â‰¤ 9Îµ of w? .
âˆ’k/45

Theorem 4. With probability at least 1 âˆ’ (k + 1)e
Algorithm 3 returns wÌ‚ âˆˆ X satisfying Ï(w? , wÌ‚) â‰¤ 9Îµ.

,

Proof. For each i âˆˆ [k], let bi := 1{Ï(w? , wi ) â‰¤ Îµ}.
Note that the bi are independent indicator random variables, each with E(bi ) â‰¥ 2/3. By Hoeffdingâ€™s inequality,
Pk
Pr[ i=1 bi > 3k/5] â‰¥ 1 âˆ’ eâˆ’k/45 . Henceforth condition

In this section, we apply our core technique to the problem of approximately minimizing strongly convex losses,
which includes least squares linear regression as a special
case.
We employ the definitions for a general loss ` : Z Ã— X â†’
R+ given in Section 2. To simplify the discussion throughout, we assume ` is differentiable, which is anyway our
primary case of interest. We assume that L has a unique
minimizer w? := arg minwâˆˆX L(w).2
Suppose (X, k Â· k) is a Banach space. Denote by k Â· kâˆ— the
dual norm, so kykâˆ— = sup{hy, xi : x âˆˆ X, kxk â‰¤ 1} for
2

This holds, for instance, if L is strongly convex.

Heavy-tailed regression with a generalized median-of-means

y âˆˆ Xâˆ— . Also, denote by BkÂ·k (c, r) := {x âˆˆ X : kxâˆ’ck â‰¤
r} the ball of radius r â‰¥ 0 around c âˆˆ X.
The derivative of a differentiable function f : X â†’ R at
x âˆˆ X in direction u âˆˆ X is denoted by hâˆ‡f (x), ui. We
say f is Î±-strongly convex with respect to k Â· k if
f (x) â‰¥ f (x0 ) + hâˆ‡f (x0 ), x âˆ’ x0 i +

Î±
kx âˆ’ x0 k2
2

for all x, x0 âˆˆ X; it is Î²-smooth with respect to k Â· k if for
all x, x0 âˆˆ X
f (x) â‰¤ f (x0 ) + hâˆ‡f (x0 ), x âˆ’ x0 i +
We say k Â· k is Î³-smooth if x 7â†’
respect to k Â· k.

1
2
2 kxk

Î²
kx âˆ’ x0 k2 .
2
is Î³-smooth with

Fix a norm k Â· k on X with a dual norm k Â· kâˆ— . The metric Ï used by Algorithm 3 is defined by Ï(w1 , w2 ) =
kw1 âˆ’ w2 k. We denote Ï by k Â· k as well. We implement APPROXkÂ·k,Îµ based on loss minimization over subsamples, as follows: Given a sample S âŠ† Z, randomly
partition S into k equal-size groups S1 , S2 , . . . , Sk , and let
the response to the i-th query to APPROXkÂ·k,Îµ be the loss
minimizer on Si , i.e., arg minwâˆˆX LSi (w). We call this
implementation subsampled empirical loss minimization.
We further assume that there exists some sample size nk
that allows DISTkÂ·k to be correctly implemented using any
i.i.d. sample of size n0 â‰¥ nk . Clearly, if S is an i.i.d. sample from D, and DISTkÂ·k is approximated using a separate
sample, then the queries to APPROXkÂ·k,Îµ are independent
from each other and from DISTkÂ·k . Thus, to apply Theorem 4, it suffices to show that Eq. (3) holds.
We assume k Â· kâˆ— is Î³-smooth for some Î³ > 0. Let nÎ± denote the smallest sample size such that the following holds:
With probability â‰¥ 5/6 over the choice of an i.i.d. sample
T of size |T | â‰¥ nÎ± from D, for all w âˆˆ X,
Î±
LT (w) â‰¥ LT (w? )+hâˆ‡LT (w? ), wâˆ’w? i+ kwâˆ’w? k2 .
2
(5)
In other words, the sample T induces a loss LT which is
Î±-strongly convex around w? . We assume that nÎ± < âˆž
for some Î± > 0.
The following lemma proves that Eq. (3) holds under these
assumptions with
r
6Î³kEkâˆ‡`(Z, w? )k2âˆ—
Îµ := 2
.
(6)
nÎ±2
Lemma 1. Assume k divides n, and that S is an i.i.d. sample from D of size n â‰¥ k Â· nÎ± . Then subsampled empirical
loss minimization using the sample S is a correct implementation of APPROXkÂ·k,Îµ for up to k queries.

Proof. It is clear that w1 , w2 , . . . , wk are independent by
the assumption. Fix some i âˆˆ [k]. Observe that âˆ‡L(w? ) =
E(âˆ‡`(Z, w? )) = 0, and therefore, since k Â· k is Î³-smooth,
Ekâˆ‡LSi (w? )k2âˆ— â‰¤ Î³(k/n)Ekâˆ‡`(Z, w? )k2âˆ— (see Juditsky
& Nemirovski, 2008). By Markovâ€™s inequality,


6Î³k
5
2
2
Pr kâˆ‡LSi (w? )kâˆ— â‰¤
E(kâˆ‡`(Z, w? )kâˆ— ) â‰¥ .
n
6
Moreover, the assumption that n/k â‰¥ nÎ± implies that with
probability at least 5/6, Eq. (5) holds for T = Si . By a
union bound, both of these events hold simultaneously with
probability at least 2/3. In the intersection of these events,
letting wi := arg minwâˆˆX LSi (w),
(Î±/2)kwi âˆ’ w? k2
â‰¤ âˆ’hâˆ‡LSi (w? ), wi âˆ’ w? i + LSi (wi ) âˆ’ LSi (w? )
â‰¤ kâˆ‡LSi (w? )kâˆ— kwi âˆ’ w? k,
where the last inequality follows from the definition of the
dual norm, and the optimality of wi on LSi . Rearranging
and combining with the above probability inequality implies Pr[kwi âˆ’ w? k â‰¤ Îµ] â‰¥ 2/3.
Combining Lemma 1 and Theorem 4 gives the following
theorem.
Theorem 5. Assume k := Cdlog(1/Î´)e (for some universal constant C > 0) divides n, S is an i.i.d. sample from
D of size n â‰¥ k Â· nÎ± , and S 0 is an i.i.d. sample from
D of size n0 â‰¥ nk . Further, assume Algorithm 3 uses
the subsampled empirical loss minimization to implement
APPROXkÂ·k,Îµ , where Îµ is as in Eq. (6), as well as implementation of DISTkÂ·k using S 0 . Then with probability at
least 1 âˆ’ Î´, the parameter wÌ‚ returned by Algorithm 3 satisfies, (for some universal constant C)
r
Î³dlog(1/Î´)eEkâˆ‡`(Z, w? )k2âˆ—
kwÌ‚ âˆ’ w? k â‰¤ C
.
nÎ±2
We give an easy corollary of Theorem 5 for the case where
` is smooth.
Corollary 1. Assume the same conditions as Theorem 5,
and also that: (i) w 7â†’ `(z, w) is Î²-smooth with respect to
k Â· k for all z âˆˆ Z, and (ii) w 7â†’ L(w) is Î²Ì„-smooth with
respect to k Â· k. Then with probability at least 1 âˆ’ Î´, (for
some universal constant C > 0)


CÎ² Î²Ì„Î³dlog(1/Î´)e
L(wÌ‚) â‰¤ 1 +
L(w? ).
nÎ±2
Proof. Due to the smoothness assumption on `,
kâˆ‡`(z, w? )k2âˆ— â‰¤ 4Î²`(z, w? ) for all z âˆˆ Z (Srebro et al.,
2010, Lemma 2.1). Thus, E[kâˆ‡`(Z, w? )k2âˆ— ] â‰¤ 4Î²L(w? ).
The result follows using Theorem 5 and since
L(wÌ‚) âˆ’ L(w? ) â‰¤ Î²Ì„2 kwÌ‚ âˆ’ w? k2 , due to the strong
smoothness of L and the optimality of L(w? ).

Heavy-tailed regression with a generalized median-of-means

Corollary 1 implies that for smooth losses, Algorithm 3
provides a constant factor approximation to the optimal
loss with a sample size max{nÎ± , Î³Î² Î²Ì„/Î±2 } Â· O(log(1/Î´))
(with probability at least 1 âˆ’ Î´). In subsequent sections,
we exemplify cases where the two arguments of the max
are roughly of the same order, and thus imply a sample size requirement of O(Î³ Î²Ì„Î²/Î±2 log(1/Î´)). Note that
there is no dependence on the optimal loss L(w? ) in the
sample size, and the algorithm has no parameters besides
k = O(log(1/Î´)).
Remark 3. The problem of estimating a scalar population
mean is a special case of the loss minimization problem,
where Z = X = R, and the loss function of interest is the
square loss `(z, w) = (z âˆ’ w)2 . The minimum population
loss in this setting is the variance Ïƒ 2 of Z, i.e., L(w? ) =
Ïƒ 2 . Moreover, in this setting, we have Î± = Î² = Î²Ì„ = 2,
so the estimate wÌ‚ returned by Algorithm 3 satisfies, with
probability at least 1 âˆ’ Î´,

 log(1/Î´) 
L(w? ).
L(wÌ‚) = 1 + O
n
In Remark 2 a result from Catoni (2012) is quoted which
implies that if n = o(1/Î´), then thePempirical mean
wÌ‚emp := arg minwâˆˆR LS (w) = |S|âˆ’1 zâˆˆS z (i.e., empirical risk (loss) minimization for this problem) incurs loss
L(wÌ‚emp ) = Ïƒ 2 + (wÌ‚emp âˆ’ w? )2 = (1 + Ï‰(1))L(w? )
with probability at least 2Î´. Therefore empirical risk minimization cannot provide a qualitatively similar guarantee as
Corollary 1. It is easy to check that minimizing a regularized objective also does not work, since any non-trivial regularized objective necessarily provides an estimator with a
positive error for some distribution with zero variance.

5. Least squares linear regression
We now show how to apply our analysis for squared loss
minimization using an appropriate norm and an upper
bound on nÎ± . Assume X is a Hilbert space with inner product hÂ·, Â·iX , and that LT is twice-differentiable (which is the
case for square loss). By Taylorâ€™s theorem, for any w âˆˆ X,
there exist t âˆˆ [0, 1] and wÌƒ = tw? + (1 âˆ’ t)w such that
LT (w) =LT (w? ) + hâˆ‡LT (w? ), w âˆ’ w? iX
1
+ hw âˆ’ w? , âˆ‡2 LT (wÌƒ)(w âˆ’ w? )iX ,
2
for any sample T âŠ† Z. Therefore, to establish a bound on
nÎ± , it suffices to find a size of T such that for an i.i.d. sample T from D,


hÎ´, âˆ‡2 LT (wÌƒ)Î´iX
Pr
inf
â‰¥
Î±
â‰¥ 5/6. (7)
kÎ´k2
Î´âˆˆX\{0},wÌƒâˆˆRd

For ease of exposition, we start with analysis for the case
where Y is allowed to be heavy-tailed, but X is assumed to
be light-tailed. The analysis is provided in Section 5.1 and
Section 5.2. The analysis for the case where X can also be
heavy tailed is provided in Section 5.3.
Recall that for a sample T := {X 1 , X 2 , . . . , X m } of m
independent copies of a random vector X âˆˆ X, Î£T is the
empirical second-moment operator based on T . The following result bounds the spectral norm deviation of Î£T
from the population second moment operator Î£ under a
boundedness assumption on X.
Lemma 2 (Specialization of Lemma 1 in Oliveira 2010).
Fix any Î» â‰¥ 0, and assume hX, (Î£ + Î» Id)âˆ’1 XiX â‰¤ rÎ»2
almost surely. For any Î´ âˆˆ (0, 1), if m â‰¥ 80rÎ»2 ln(4m2 /Î´),
then with probability at least 1 âˆ’ Î´, for all a âˆˆ X,
1
ha, (Î£ + Î» Id)aiX â‰¤ ha, (Î£T + Î» Id)aiX
2
â‰¤ 2ha, (Î£ + Î» Id)aiX .
We use the boundedness assumption on X for sake of
simplicity; it is possible to remove the boundedness assumption, and the logarithmic dependence on the cardinality of T , under different conditions on X (e.g., assuming
Î£ âˆ’1/2 X has subgaussian projections, as in Litvak et al.
2005).
5.1. Finite-dimensional ordinary least squares
Consider first ordinary least squares in the finitedimensional case. In this case X = Rd and Algorithm 1
can be used with Î» = 0. It is easy to see that Algorithm 1
is a specialization of Algorithm 3 with subsampled empirical loss minimization when ` = `sq . We now prove Theorem 2. Recall that in this theorem we assume the variant of
Algorithm 1, in which step 5 uses the covariance matrix of
the entire T sample, Î£T , instead of separate matrices Î£Ti,j .
Thus the âˆš
norm we use in Algorithm 3 is k Â· kT , defined as
kakT = a> Î£T a, with the oracle DISTkÂ·k = DISTkÂ·kT
that always provides the correct distance.
Proof of Theorem 2. The proof is derived from Corollary 1
as follows. First, it is easy to check that the dual of k Â· kT
is 1-smooth. Let the norm k Â· kÎ£ be defined by kakÎ£ =
âˆš
a> Î£a. By Lemma 2, if n0 â‰¥ O(R2 log(R/Î´)), with
probability at least 1 âˆ’ Î´, (1/2)kak2Î£ â‰¤ kak2T â‰¤ 2kak2Î£
for all a âˆˆ Rd . Denote this event E and assume for the
rest of the proof that E occurs. Since `sq is R2 -smooth
with respect to k Â· kÎ£ , and Lsq is 1-smooth with respect to
k Â· kÎ£ , the same holds, up to constant factors, for k Â· kT .
Moreover, for any sample S,
Î´ > âˆ‡2 LS (wÌƒ)Î´
Î´ > Î£S Î´
Î´ > Î£S Î´
=
â‰¥
.
kÎ´k2T
Î´ > Î£T Î´
2Î´ > Î£Î´

Heavy-tailed regression with a generalized median-of-means

By Lemma 2 with Î» = 0, if |S| â‰¥ 80R2 log(24|S|2 )
then with probability at least 5/6, âˆ€Î´ âˆˆ Rd \ {0},
Î´ > Î£S Î´/Î´ > Î£Î´ â‰¥ 1/2. Therefore Eq. (7) holds for k Â· kT
with Î± = 1/4 and n1/4 = O(R2 log R). We can thus apply Corollary 1 with Î± = 1/4, Î² = 4R2 , Î²Ì„ = 4, Î³ = 1,
and n1/4 = O(R2 log R), so with probability at least 1 âˆ’ Î´,
the parameter wÌ‚ returned by Algorithm 1 (with the variant)
satisfies


 2
R log(1/Î´)
L(w? ),
(8)
L(wÌ‚) â‰¤ 1 + O
n
as soon as n â‰¥ O(R2 log(R) log(1/Î´)). A union bound
over the probability that E also occurs finishes the proof.

5.2. Ridge regression
In a general, possibly infinite-dimensional, Hilbert space
X, the variant of Algorithm 1 can be used with Î» > 0. In
this case, the algorithm is a specialization of Algorithm 3
with subsampled empirical loss minimization
when ` = `Î» ,
p
>
with the norm defined by kakT,Î» = a (Î£T + Î» Id)a.
Proof of Theorem 3. First, it is easy to check that the dual
of k Â· kT,Î» is 1-smooth. As in the proofâˆšof Theorem 2,
by Lemma 2 if n0 â‰¥ O((V 2 /Î») log(V /(Î´ Î»))) then with
probability p
1 âˆ’ Î´ the norm kakT,Î» is equivalent to the norm
k Â· kÎ£,Î» = a> (Î£ + Î» Id)a up to constant factors. Moreover, since we assume that Pr[hX, XiX â‰¤ V 2 ] = 1, we
have hx, (Î£ + Î»I)âˆ’1 xiX â‰¤ hx, xiX /Î» for all x âˆˆ X, so
Pr[hX, (Î£ + Î»I)âˆ’1 XiX â‰¤ V 2 /Î»] = 1. Therefore `Î» is
(1 + V 2 /Î»)-smooth with respect to k Â· kÎ£,Î» . In addition,
LÎ» is 1-smooth with respect to k Â· kÎ£,Î» . Using Lemma 2
with rÎ» = V /Î», we have, similarly
âˆš to the proof of Theorem 2, n1/4 = O((V 2 /Î») log(V / Î»)). Setting Î± = 1/4,
Î² = 4(1 + V 2 /Î»), Î²Ì„ = 4, Î³ = 1, and n1/4 as above, to
match the actual norm k Â· kT,Î» , we have with probability
1 âˆ’ Î´,



(1 + V 2 /Î») log(1/Î´)
LÎ» (wÌ‚) â‰¤ 1 + O
LÎ» (w? ),
n
âˆš
as soon as n â‰¥ O((V 2 /Î») log(V / Î») log(1/Î´)).
We are generally interested in comparing to the minimum
sq
square loss Lsq
? := inf wâˆˆX L (w), rather than the minimum regularized square loss inf wâˆˆX LÎ» (w). Assuming
the minimizer is achieved
p by some wÌ„ âˆˆ X with hwÌ„, wÌ„iX â‰¤
2
2
B 2 , the choice Î» = Î˜( Lsq
? V log(1/Î´)/(B n)) yields
Lsq (wÌ‚) + Î»hwÌ‚, wÌ‚iX â‰¤ Lsq
?
r sq 2 2

2 2
L? B V log(1/Î´) (Lsq
? + B V ) log(1/Î´)
+O
+
n
n
as soon as n â‰¥ OÌƒ(B 2 V 2 log(1/Î´)/Lsq
? ).

5.3. Heavy-tailed covariates
In this section we prove Theorem 1. When the regression
covariates are not bounded or subgaussian as in the two previous sections, the empirical second-moment matrix may
deviate significantly from its population counterpart with
non-negligible probability. In this case we use Algorithm 1
with the original step 5 so that for any i âˆˆ [k], the responses
{DISTkÂ·k (wi , wj )}jâˆˆ[k]\{i} are mutually independent.
For simplicity, we work in finite-dimensional Euclidean
space X := Rd and consider Î» = 0. The analysis shows
that Algorithm 1 is an instance of subsampled âˆš
empirical
loss minimization for `sq with the norm kakÎ£ = a> Î£a.
Recall that we assume Condition 1 given in Section 2. The
following lemma shows that under this condition, O(d)
samples suffice so that the expected spectral norm distance
between the empirical second-moment matrix and Î£ is
bounded.
Lemma 3 (Corollary 1.2 from Srivastava & Vershynin
2013, essentially). Let X satisfy Condition 1, and let
b
XP
1 , X 2 , . . . , X n be independent copies of X. Let Î£ :=
n
>
1
i=1 X i X i . For fixed Î·, c > 0, there is a constant
n
Î¸, such that for any  âˆˆ (0, 1), if n â‰¥ Î¸âˆ’2âˆ’2/Î· d, then
b âˆ’1/2 âˆ’ Id k2 â‰¤ .
EkÎ£ âˆ’1/2 Î£Î£
Lemma 3 implies that for the norm k Â· kÎ£ , n1/2 = O(c0Î· d)
where c0Î· = Î¸ Â·2O(1+1/Î·) . Therefore, for k = O(log(1/Î´)),
subsampled empirical loss minimization requires n â‰¥ k Â·
n1/2 = O(c0Î· d log(1/Î´)) samples to correctly implement
APPROXkÂ·kÎ£ ,Îµ for Îµ as in Eq. (6).
Step 5 in Algorithm 1 implements DISTkÂ·kÎ£ such that for
every i, {DISTkÂ·kÎ£ (wi , wj )}jâˆˆ[k]\{i} are estimated using
independent samples Tj . We now need to show that this
implementation satisfies Eq. (4). By Lemma 3, for every
i, j âˆˆ [k] an i.i.d. sample Tj of size O(c0Î· ) suffices so that
with probability at least 8/9,
1/2

(1/2)kÎ£ 1/2 (wi âˆ’ wj )k2 â‰¤ kÎ£Tj (wi âˆ’ wj )k2
â‰¤ 2kÎ£ 1/2 (wi âˆ’ wj )k2 .
Thus for k = O(log(1/Î´), the total size of the sample T
in Algorithm 1 needs to be n0 = O(c0Î· log(1/Î´)). Setting
Î± = 1/2, Î³ = 1 and nÎ± = O(c0Î· d), Theorem 1 is now
derived from Theorem 5, by applying the identity
kâˆ‡`sq ((X, Y ), w? )kÎ£,âˆ— = 2kÎ£ âˆ’1/2 X(X > w? âˆ’ Y )k2 .

References
Alon, Noga, Matias, Yossi, and Szegedy, Mario. The space
complexity of approximating the frequency moments.
Journal of Computer and System Sciences, 58:137â€“147,
1999.

Heavy-tailed regression with a generalized median-of-means

Audibert, Jean-Yves and Catoni, Olivier. Robust linear
least squares regression. Ann. Stat., 39(5):2766â€“2794,
2011.

Srebro, Nathan, Sridharan, Karthik, and Tewari, Ambuj.
Smoothness, low noise and fast rates. In Advances in
Neural Information Processing Systems 23, 2010.

Balcan, M.-F., Beygelzimer, A., and Langford, J. Agnostic
active learning. In Twenty-Third International Conference on Machine Learning, 2006.

Srivastava, N. and Vershynin, R. Covariance estimation for
distributions with 2 +  moments. Annals of Probability,
41:3081â€“3111, 2013.

Catoni, Olivier. Challenging the empirical mean and empirical variance: a deviation study. Ann. Inst. H. Poincar
Probab. Statist., 48(4):1148â€“1185, 2012.
Hsu, Daniel and Sabato, Sivan.
Approximate loss
minimization with heavy tails.
ArXiv e-prints,
arXiv:1307.1827, 2013. Arxiv preprint.
Hsu, Daniel, Kakade, Sham M., and Zhang, Tong. Random design analysis of ridge regression. In Twenty-Fifth
Conference on Learning Theory, 2012.
Juditsky, Anatoli and Nemirovski, Arkadii S. Large deviations of vector-valued martingales in 2-smooth normed
spaces. ArXiv e-prints, arXiv:0809.0813, 2008.
Lepski, O. V. Asymptotically minimax adaptive estimation
I: Upper bounds. optimally adaptive estimates. Theory
Probab. Appl., 36(4):682â€“697, 1991.
Levin, Leonid A. Notes for miscellaneous lectures. CoRR,
abs/cs/0503039, 2005.
Litvak, Alexander E., Pajor, Alain, Rudelson, Mark, and
Tomczak-Jaegermann, Nicole. Smallest singular value
of random matrices and geometry of random polytopes.
Adv. Math., 195(2):491â€“523, 2005. ISSN 0001-8708.
doi: 10.1016/j.aim.2004.08.004. URL http://dx.
doi.org/10.1016/j.aim.2004.08.004.
Mahdavi, Mehrdad and Jin, Rong. Passive learning with
target risk. In Twenty-Sixth Conference on Learning Theory, 2013.
Minsker, Stanislav. Geometric median and robust estimation in banach spaces. arXiv e-prints, arXiv:1308.1334,
2013.
Nemirovsky, A. S. and Yudin, D. B. Problem Complexity and Method Efficiency in Optimization. WileyInterscience, 1983.
Nussbaum, M. Minimax risk: Pinsker bound. In Kotz, S.
(ed.), Encyclopedia of Statistical Sciences, Update Volume 3, pp. 451â€“460. Wiley, New York, 1999.
Oliveira, Roberto. Sums of random Hermitian matrices and
an inequality by Rudelson. Electron. Commun. Probab.,
15(19):203â€“212, 2010.

