Classification with Low Rank and Missing Data

Elad Hazan
Princeton University and Microsoft Research, Herzliya

EHAZAN @ CS . PRINCETON . EDU

Roi Livni
The Hebrew University of Jerusalem and Microsoft Research, Herzliya
Yishay Mansour
Microsoft Research, Hertzelia and Tel Aviv University

Abstract
We consider classification and regression tasks
where we have missing data and assume that the
(clean) data resides in a low rank subspace. Finding a hidden subspace is known to be computationally hard. Nevertheless, using a non-proper
formulation we give an efficient agnostic algorithm that classifies as good as the best linear
classifier coupled with the best low-dimensional
subspace in which the data resides. A direct implication is that our algorithm can linearly (and
non-linearly through kernels) classify provably
as well as the best classifier that has access to
the full data.

1. Introduction
The importance of handling correctly missing data is a
fundamental and classical challenge in machine learning.
There are many reasons why data might be missing. For
example, consider the medical domain, some data might be
missing because certain procedures were not performed on
a given patient, other data might be missing because the
patient choose not to disclose them, and even some data
might be missing due to malfunction of certain equipment.
While it is definitely much better to have always complete
and accurate data, this utopian desire is, in many cases, unfulfilled. For this reason we need to utilize the available
data even if some of it is missing.
Another, very different motivation for missing data are recommendations. For example, a movie recommendations
dataset might have users opinions on certain movies, which
is the case, for example, in the Netflix motion picture
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

ROI . LIVNI @ MAIL . HUJI . AC . IL

MANSOUR . YISHAY @ GMAIL . COM

dataset. Clearly, no user has seen or reviewed all movies,
or even close to it. In this respect recommendation data is
an extreme case: the vast majority is usually missing (i.e.,
it is sparse to the extreme).
Many times we can solve the missing data problem since
the data resides on a lower dimension manifold. In the
above examples, if there are prototypical users (or patients)
and any user is a mixture of the prototypical users, then this
implicitly suggests that the data is low rank. Another way
to formalize this assumption is to consider the data in a matrix form, say, the users are rows and movies are columns,
then our assumption is that the true complete matrix has a
low rank.
Our starting point is to consider the low rank assumption,
but to avoid any explicit matrix completion, and instead
directly dive in to the classification problem. At the end of
the introduction we show that matrix completion is neither
sufficient and/or necessary.
We consider perhaps the most fundamental data analysis
technique of the machine learning toolkit: linear (and kernel) classification, as applied to data where some (or even
most) of the attributes in an example might be missing.
Our main result is an efficient algorithm for linear and
kernel classification that performs as well as the best
classifier that has access to all data, under low rank assumption with natural non-degeneracy conditions.
We stress that our result is worst case, we do not assume
that the missing data follows any probabilistic rule other
than the underlying matrix having low rank. This is a clear
contrast to most existing matrix completion algorithms. We
also cast our results in a distributional setting, showing that
the classification error that we achieve is close to the best
classification using the subspace of the examples (and with
no missing data). Notably, many variants of the problem
of finding a hidden subspace are computationally hard (see
e.g. (Berthet & Rigollet, 2013)), yet as we show, learn-

Classification with Low-Rank and Missing Data

ing a linear classifier on a hidden subspace is non-properly
learnable.
At a high level, we assume that a sample is a triplet
(x, o, y), where x ∈ Rd is the complete example, o ⊂
{1, . . . , d} is the set of observable attributes and y ∈ Y
is the label. The learner observes only (xo , y), where xo
omits any attribute not in o. Our goal is given a sample
(i)
S = {(xo , y (i) )}m
i=1 to output a classifier fS such that
w.h.p.:
E [`(fS (xo ), y)] ≤ min E [`(w · x, y)] + ,
w∈Rd

kwk≤1

where ` is the loss function. Namely, we like our classifier
fS to compete with the best linear classifier for the completely observable data.
Our main result is achieving this task (under mild regularity conditions) using a computationally efficient algorithm
for any convex Lipschitz-bounded loss function. Our basic result requires a sample size which is quasi-polynomial,
but we complement it with a kernel construction which can
guarantee efficient learning under appropriate large margin
assumptions. Our kernel depends only on the intersection
of observable values of two inputs, and is efficiently computable. (We give a more detailed overview of our main
results in Section 2.)
We complement our theoretical contributions with experimental findings that show superior classification performance both on synthetic data and on publicly-available recommendation data.
Previous work. Classification with missing data is a well
studied subject in statistics with numerous books and papers devoted to its study, (see, e.g., (Little & Rubin, 2002)).
The statistical treatment of missing data is broad, and to
a fairly large extent assumes parametric models both for
the data generating process as well as the process that creates the missing data. One of the most popular models for
the missing data process is Missing Completely at Random
(MCAR) where the missing attributes are selected independently from the values.
We outline a few of the main approaches handling missing data in the statistics literature. The simplest method is
simply to discard records with missing data, even this assumes independence between the examples with missing
values and their labels. In order to estimate simple statistics, such as the expected value of an attribute, one can use
importance sampling methods, where the probability of an
attribute being missing can depend on it value (e.g., using
the Horvitz-Thompson estimator (Horvitz & Thompson,
1952)). A large body of techniques is devoted to imputation procedures which complete the missing data. This can
be done by replacing a missing attribute by its mean (mean

imputation), or using a regression based on the observed
value (regression imputation), or sampling the other examples to complete the missing value (hot deck).1 The imputation methodologies share a similar goal as matrix completion, namely reduce the problem to one with complete data,
however their methodologies and motivating scenarios are
very different. Finally, one can build a complete Bayesian
model for both the observed and unobserved data and use
it to perform inference (e.g. (Vishwanathan et al., 2005)).
As with almost any Bayesian methodology, its success depends largely on selecting the right model and prior, this is
even ignoring the computational issues which make inference in many of those models computationally intractable.
In the machine learning community, missing data was considered in the framework of limited attribute observability
(Ben-David & Dichterman, 1998) and its many refinements
(Dekel et al., 2010; Cesa-Bianchi et al., 2010; 2011; Hazan
& Koren, 2012). However, to the best of our knowledge,
the low-rank property is not captured by previous work,
nor is the extreme amount of missing data. More importantly, much of the research is focused on selecting which
attributes to observe or on missing attributes at test or train
time (see also (Eban et al., 2014; Globerson & Roweis,
2006)). In our case the learner has no control which attributes are observable in an example and the domain is
fixed. The latter case is captured in the work of (Chechik
et al., 2008), who rescale inner-products according to the
amount of missing data. Their method, however, does not
entail theoretical guarantees on reconstruction in the worst
case, and gives rise to non-convex programs.
A natural and intuitive methodology to follow is to treat
the labels (both known and unknown) as an additional column in the data matrix and complete the data using a matrix
completion algorithm, thereby obtaining the classification.
Indeed, this exactly was proposed in the innovative work
of (Goldberg et al., 2010), who connected the methodology of matrix completion to prediction from missing data.
Although this is a natural approach, we show that completion is neither necessary nor sufficient for classification. Furthermore, the techniques for provably completing a low rank matrix are only known under probabilistic
models with restricted distributions (Srebro, 2004; Candes
& Recht, 2009; Lee et al., 2010; Salakhutdinov & Srebro,
2010; Shamir & Shalev-Shwartz, 2011). Agnostic and nonprobabilistic matrix completion algorithms such as (Srebro
et al., 2004; Hazan et al., 2012) we were not able to use for
our purposes.
Is matrix completion sufficient and/or necessary? We
demonstrate that classification with missing data is prov1

We remark that our model implicitly includes meanimputation or 0-imputation method and therefore will always outperform them.

Classification with Low-Rank and Missing Data

ably different from that of matrix completion. We start by
considering a learner that tries to complete the missing entries in an unsupervised manner and then performs classification on the completed data, this approach is close akin
to imputation techniques, generative models and any other
two step – unsupervised/supervised algorithm. Our example shows that even under realizable assumptions, such an
algorithm may fail. We then proceed to analyze the approach previously mentioned – to treat the labels as an additional column.
To see that unsupervised completion is insufficient for prediction, consider the example in Figure 1: the original data
is represented by filled red and green dots and it is linearly
separable. Each data point will have one of its two coordinates missing (this can even be done at random). In the
figure the arrow from each instance points to the observed
attribute. However, the rank-one completion of projection
onto the pink hyperplane is possible, and admits no separation. The problem is clearly that the mapping to a low
dimension is independent from the labels, and therefore we
should not expect that properties that depend on the labels,
such as linear separability, will be maintained.

First note that there is no 1-rank completion of this matrix. On the other hand, there is more than one 2-rank
completion each lead to a different classification of the test
point. The first two possible completions are to complete
odd columns to a constant one vector, and even column
vectors to a constant −1 vector. Then complete the labeling whichever way you choose. We can also complete the
first and last rows to a constant 1 vector, and the second row
to a constant −1 vector. All possible completions lead to
an optimal solution w.r.t Problem G but have different outcome w.r.t classification. We stress that this is not a sample
complexity issue. Even if we observe abundant amount of
data, the completion task is still ill-posed.
Finally, matrix completion is also not necessary for prediction. Consider movie recommendation dataset with two
separate populations, French and Chinese, where each population reviews a different set of movies. Even if each population has a low rank, performing successful matrix completion, in this case, is impossible (and intuitively it does
not make sense in such a setting). However, linear classification in this case is possible via a single linear classifier,
for example by setting all non-observed entries to zero. For
a numerical example, return to the matrix M in Eq. 1. Note
that we observe only three instances hence the classification task is easy but does not lead to reconstruction of the
missing entries.

2. Problem Setup and Main Result

Figure 1. Linearly separable data, for which certain completions
make the data non-separable.

Next, consider a learner that treats the labels as an additional column. (Goldberg et al., 2010) Considered the following problem:
minimize
Z

(G)

where Ω is the set of observed attributes (or observed labels for the corresponding columns). Now assume that we
always see one of the following examples: [1, ∗, 1, ∗],
[∗, − 1, ∗, − 1], or [1, , −1, 1, − 1]. The observed labels
are respectively 1,−1 and 1. A typical data matrix with one
test point might be of the form:
1
 ∗
M =
 1
1

(
xi
(xo )i =
∗

i∈o
else

rank(Z)

subject to: Zi,j = xi,j , (i, j) ∈ Ω , .



We begin by presenting the general setting: A vector with
missing entries can be modeled as a tuple x × o, where
x ∈ Rd and o ∈ 2d is a subset of indices. The vector
x represents the full data and the set o represents the observed attributes. Given such a tuple, let us denote by xo a
vector in (R ∪ {∗})d such that

∗ 1
∗
−1 ∗ −1
−1 1 −1
∗ 1
∗


1
−1 

1 
∗

(1)

The task of learning a linear classifier with missing data
is to return a target function over xo that competes with
best linear classifier over x. Specifically, a sequence of
triplets {(x(i) , o(i) , yi )}m
i=1 is drawn iid according to some
distribution D. An algorithm is provided with the sample
S = {(xioi , yi )}m
i=1 and should return a target function fS
over missing data such that w.h.p:
E [`(fS (xo ), y))] ≤

min E [`(w · x, y))] + ,

w∈Bd (1)

(2)

where ` : R×R 7→ R is the loss function and
√Bd (r) denotes
the Euclidean ball in dimension d of radius r. For brevity,
we will say that a target function fS is -good if Eq. 2
holds.

Classification with Low-Rank and Missing Data

Without any assumptions on the distribution D, the task is
ill-posed. One can construct examples where the learner
over missing data does not have enough information to
compete with the best linear classifier. Such is the case
when, e.g., yi is some attribute that is constantly concealed
and independent of all other features. Therefore, certain
assumptions on the distribution must be made.

Pγ
γ+1
such that Γ = k=1 dk = d d−1−d , and the scalar product between two samples φγ (x1o1 ) and φγ (x2o2 ) can be efficiently computed, specifically it is given by the formula:

One reasonable assumption is to assume that the marginal
distribution D over x is supported on a small dimensional
linear subspace E and that for every set of observations, we
can linearly reconstruct the vector x from the vector Po x,
where Po : Rd → R|o| is the projection on the observed
attributes. In other words, we demand that the mapping
Po|E : E → Po E, which is the restriction of Po to E, is
full-rank. As the learner doesn’t have access to the subspace E, the learning task is still far from trivial.

In addition, let ` be an L-Lipschitz loss function and S =
{(xioi , yi )}m
i=1 a sample drawn iid according to a distribution D. We make the assumption that kPo xk ≤ 1 a.s. The
followings hold:

We give a precise definition of the last assumption in Assumption 1. Though our results hold under the low rank
assumption the convergence rates we give depend on a certain regularity parameter. Roughly, we parameterize the
“distance” of Po|E from singularity, and our results will
quantitatively depend on this distance. Again, we defer all
rigorous definitions to Section 3.2.
2.1. Main Result
Our first result is a an upper bound on the sample complexity of the problem. We then proceed to a more general statement that entails an efficient kernel-based algorithm. Proofs are available in the supplamentary material
(see (Hazan et al., 2015) for a full version).
Theorem 1 (Main Result). Assume that ` is a L-Lipschitz
convex loss function, let D be a λ-regular distribution (see
and
Definition 1), let γ() ≥ log 2L/(λ)
λ
Γ() =

dγ()+1 − d
.
d−1

There exists an algorithm (independent of D) that receives
a sampleS = {(xioi , yi )}m
i=1 of size m ∈

Ω

L2 Γ()2 log 1/δ
2

kγ (x1o1 , x2o2 ) :=

|o(1) ∩ o(2) |γ − 1
|o(1) ∩ o(2) | − 1

(1)

X

xi

(2)

· xi .

i∈o(1) ∩o(2)

1. At each iteration of Alg. 1 we can efficiently compute
vt> φγ (xtot ) for any new example xtot . Specifically it
is given by the formula
vt> φγ (xtot ) :=

t−1
X

(t−1)

αi

k(xioi , xtot ).

i=1

Hence Alg. 1 runs in poly(T ) time and sequentially
produces target functions ft (xo ) = vt> φγ (xo ) that
can be computed at test time in poly(T ) time.
2. Run Alg. 1 with fixed T > 0, 0 < ρ <
PT
Let v̄ = T1 t=1 vt . For every v let

1
4

and ηt =

1
ρt .

m

1 X
ρ
`(v> φγ (xioi ), yi )
F̂ρ (v) = kvk2 +
2
m i=1
then with probability (1 − δ):

F̂ρ (v̄) ≤ min F̂ρ (v) + O

L2 Γ ln T /δ
ρT


.

(3)

3. For any  > 0, if D is a λ-regular distribution and
γ ≥ log 2L/(λ)
then for some v∗ ∈ BΓ (Γ)
λ
E [`(v∗ · φγ (xo ), y)] ≤

min E [`(w · x, y)] + .

w∈Bd (1)

and returns a target function fS that

is -good with probability at least (1 − δ). The algorithm
runs in time poly(|S|).
As the sample complexity in Theorem 1 is quasipolynomial, the result has limited practical value in many situations. However, as the next theorem states, fS can actually be computed by applying a kernel trick. Thus, under
further large margin assumptions we can significantly improve performance.
Theorem 2. For every γ ≥ 0, there exists an embedding
over missing data
φ γ : x o → RΓ ,

To summarize, Theorem 2 states that we can embed the
sample points with missing attributes in a high dimensional, finite, Hilbert space of dimension Γ, such that:
• The scalar product between embedded points can be
computed efficiently. Hence, due to the conventional
representor argument, the task of empirical risk minimization is tractable.
• Following the conventional analysis of kernel methods: Under large margin assumptions in the ambient
space, we can compute a predictor with scalable sample complexity and computational efficiency.

Classification with Low-Rank and Missing Data

• Finally, the best linear
predictor over embedded sam√
ple points in a Γ–ball is comparable to the best
linear predictor over fully observed data. Taken together, we can learn a predictor with sample complexity Ω(Γ2 ()/2 log 1δ ) and Theorem 1 holds.
For completeness we present the method together with an
efficient algorithm that optimizes the RHS of Eq. 3 via
an SGD method. The optimization analysis is derived in a
straightforward manner from the work of (Shalev-Shwartz
et al., 2011). Other optimization algorithms exist in the literature, and we chose this optimization method as it allows
us to also derive regret bounds which are formally stronger
(see Section 2.2). We point out that the main novelty of this
paper is in the introduction of a new kernel and our guarantees do not depend on a specific optimization algorithm.
Finally, note that φ1 induces the same scalar product as a
0-imputation. In that respect, by considering different γ =
1, 2, . . . and using a holdout set we can guarantee that our
method will outperform the 0-imputation method.
2.2. Regret minimization for joint subspace learning
and classification
A significant technical contribution of this manuscript is
the agnostic learning of a subspace coupled with a linear
classifier. A subspace is represented by a projection matrix
Q ∈ Rd×d , which satisfies Q2 = Q. Denote the following
class of target functions
F0 = {fw,Q : w ∈ Bd , Q ∈ Md×d , Q2 = Q}
where fw,Q (xo ) is the linear predictor defined by w over
subspace defined by the matrix Q, as formally defined in
definition 2.
Given the aforementioned efficient kernel mapping φγ , we
consider the following kernel-gradient-based online algorithm for classification called KARMA (Kernelized Algorithm for Risk-minimization with Missing Attributes):
Our main result for the fully adversarial online setting is
given next, and proved in the supplamentary material and
in the full version. Notice that the subspace E ∗ and associated projection matrix Q∗ are chosen by an adversary and
unknown to the algorithm.
Theorem 3. For any γ > 1, λ > 0, X > 0, ρ > 0, B > 0,
L-Lipschitz convex loss function `, and λ-regular sequence
{(xt , ot , yt )} w.r.t subspace E ∗ and associated projection
matrix Q∗ such that kxt k∞ < X, Run Algorithm 1 with
1
}, sequentially outputs {vt ∈ Rt } that satisfy
{ηt = ρt
X
X
`(vt> φγ (xtot ), yt ) − min
`(fw,Q∗ (xtot ), yt ) ≤
kwk≤1

t

t

2L X Γ
ρ
e−λγ
(1 + log T ) + T · B +
LT
ρ
2
λ
2

2

Algorithm 1 KARMA: Kernelized Algorithm for Riskminimization with Missing Attributes
1: Input: parameters γ > 1, {ηt > 0}, 0 < ρ < 1
2: Initialize: v1 = 0, α(0) = 0.
3: for t = 1 to T do
4:
Observe
example
(xtot , yt ),
suffer
loss
>
t
`(vt φγ (xot ), yt )
5:
Update (`0 denotes the derivative w.r.t. the first argument)

(t−1)

i<t
(1 − ηt ρ) · αi
(t)
αi = −ηt `0 (vt> φγ (xtot ), yt ) i = t


0
else
vt+1 =

t
X

(t)

αi φγ (xioi )

i=1

6: end for

In particular, taking ρ =
for every kwk ≤ 1:

X

√
LX
√ Γ,
BT

γ =

1
λ

log T we obtain

√
`(vt> φγ (xtot ), yt )−`(fw,Q∗ (xtot ), yt ) ∈ O(XL ΓBT )

t

3. Preliminaries and Notations
3.1. Notations
As discussed, we consider a model where a distribution D
is fixed over Rd × O × Y, where O = 2d consists of all
subsets of {1, . . . , d}. We will generally denote elements
of Rd by x, w, v, u and elements of O by o. We denote√by
Bd the unit ball of Rd , and by Bd (r) the ball of radius r.
Given a subset o we denote by Po : Rd → R|o| the projection onto the indices in o, i.e., if i1 ≤ i2 ≤ · · · ≤ ik are
the elements of o in increasing order then (Po x)j = xij .
Given a matrix A and a set of indices o, we let
Ao,o = Po APo> .
Finally, given a subspace E ⊆ Rd we denote by PE :
Rd → Rd the projection onto E.
3.2. Model Assumptions
Definition 1 (λ-regularity). We say that D is λ-regular
with associated subpsace E if the following happens with
probability 1 (w.r.t the joint random variables (x, o)):
1. kPo xk ≤ 1.
2. x ∈ E.

Classification with Low-Rank and Missing Data

3. ker(Po PE ) = ker(PE )
4. If λo > 0 is a strictly positive singular value of the
matrix Po PE then λo ≥ λ.
Assumption 1 (Low Rank Assumption). We say that D
satisfies the low rank assumption with associated subspace
E if it is λ-regular with associated subspace E for some
λ > 0.
Let o be a set of probable observables, and let Xo be the
matrix obained by taking only columns in o from the data
matrix X. If rank(Xo ) < rank(X) then Assumption 1
is not satisfied. Since rank(Xo ) < |o| we have that assumption 1 is met only if rank(X) is bounded by minimal
number of observations. As in previous example, without
some further restrictive assumptions, if the rank was larger
than is required in the assumption, the problem of finding
an -good function becomes ill-posed.

Lemma 2. Let (x, o) be a sample drawn according to a
λ-regular distribution D with associated subspace E. Let
Q = PE and kwk ≤ 1 then a.s:
(1 − λ)γ
.
λ
Corollary 1. Let ` be a L-Lipschitz function. Under λthe class F γ contains an
regularity, for every γ ≥ log L/λ
λ
-good target function.
γ
kfw,I−Q
(xo ) − fw,Q (xo )k ≤

4.2. Improper learning of F γ and a kernel trick
Let G be the set of all finite, non empty, sequences of length
at most γ over d. For each s ∈ G denote |s|– the length
of the sequence and send the last element of the sequence.
Given a set of observations o we write s ⊆ o if all elements
of the sequence s belong to o. We let
Γ=

4. Learning under low rank assumption and
λ-regularity.
Definition 2 (The class F0 ). We define the following class
of target functions
F0 = {fw,Q : w ∈ Bd (1), Q ∈ Md×d , Q2 = Q}

γ
X

dj = |G| =

j=1

and we index the coordinates of RΓ by the elements of G:
Definition 4. We let φγ : (Rd × O) → RΓ be the embedding:
(

where

(φγ (xo ))s =

fw,Q (xo ) = (Po w) · Q†o,o · (Po x).
(Here M † denotes the pseudo inverse of M .)

Lemma 1. Let D be a distribution that satisfies the low
∗
∈ F0
rank assumption. For every w∗ ∈ Rd there is fw,Q
such that a.s:
∗
fw,Q
(xo ) = w∗ · x.

γ
(xo ) =
fw,Q

We next define a surrogate class of target functions that approximates F0
Definition 3 (The classes F γ ). For every γ we define the
following class
: w ∈ Bd (1), Q ∈ R

d×d

γ−1
X
j=0

ws1 xs1 +

X

ws1 · Qs1 ,s2 · Qs2 ,s3 · · · Qs|s|−1 ,send · xsend

{s:s⊆o, 2≤|s|≤t}
γ
Corollary 2. For every fw,Q
∈ F γ there is v ∈ BΓ (Γ),
such that:

j

As a corollary, for every loss function ` and distribution D
we have that:

min E [`(v · φ(xo ), y)] ≤

v∈BΓ (Γ)

min

γ
fw,Q
∈F γ

h
i
γ
E `(fw,Q (xo ), y)

2

, Q = Q}

where,
γ
fw,Q
(xo ) = (Po w) ·

X

γ
fw,Q
(xo ) = v · φγ (xo ).

4.1. Approximating F0 under regularity

F =

s⊆o
else

s1 ∈o

In particular Q = PE and w = PE> w∗ .

γ
{fw,Q

xsend
0

Lemma 3. For every Q and w we have:

The following Lemma states that, under the low rank assumption, the problem of linear learning with missing data
is reduced to the problem of learning the class F0 , in the
sense that the hypothesis class F0 is not less-expressive.

γ

dγ+1 − d
d−1

(Qo,o ) · (Po x)

Due to Corollary 2, learning F γ can be improperly done
via learning a linear classifier over the embedded sample
Γ
set {φγ (xo )}m
i=1 . The ambient space R may be very large.
However, as we next show, the scalar product between two
embedded points can be computed efficiently:

Classification with Low-Rank and Missing Data

Theorem 4.

5.1. Toy Examples
γ

(2)
φγ (x(1)
o1 ) · φγ (xo2 ) =

|o1 ∩ o2 | − 1
|o1 ∩ o2 | − 1

(We use the convention that

γ

1 −1
1−1

(1) (2)

X

xk xk .

k∈o1 ∩o2

= limx→1

γ

x −1
x−1

= γ)

5. Experiments
We’ve conducted both toy and real-data experiments to
compare the performance of our new method vs. other
existing methods: 0-imputation, mcb, mc1 and geom.
We’ve conducted experiments on binary classification
tasks, multi-class classification and regression tasks. We
begin by describing the implementation for each scenario

5.1.1. MCAR- M ODEL
The first toy data we’ve experimented on was a regression
task. We randomly picked m = 103 examples (normal
distribution), in an r = 10 dimensional subspace embedded
in a d = 20-dimensional linear space. We then randomly
picked a classifier w and let yˆi = w · x(i) . The labels
where normalized to have standard deviation 1 and mean 0
yˆi −Ei [yˆi ]
). We then randomly chose a
(i.e., yi = √ 1 P
2
i (yˆi −Ei [yˆi ])

m

karma
0−imputation
mc1/mcb

0.8
0.7
Sq. loss

0.6

• karma: We’ve applied our method in all experiments.
We evaluated the loss with γ = {1, 2, 3, 4} and C =
{10−5 , 10−4 , . . . , 104 , 105 }. We chose γ and λ
using a holdout set. A constant feature was added to
allow bias and the data was normalized. For binary
classification we let ` be the Hinge loss, for multiclass we used the multiclass Hinge loss as described
in (Crammer & Singer, 2002) and finally for regression tasks we used squared loss (which was also used
at test time).

0.5
0.4
0.3
0.2
0.1
0

0.2

0.4

0.6
p

0.8

1

Figure 2. Missing Completely At Random features.

• 0-imputation: 0-imputation is simply reduced to
γ = 1 . A constant feature was added to allow bias
and data was normalized so the mean of each feature
was 0.

subset of observed features using iid Bernoulli distribution.
Each feature had probability p = 0.1, 0.2, . . . , 0.9, 1 to be
observed. The results appear in Figure 2: Square loss vs.
fraction of observed features.

• mcb/ mc1: mcb and mc1 are methods suggested by
(Goldberg et al., 2010). They are inspired by a convex
relaxation of Problem G and differ by the way a bias
term is introduced.

5.1.2. T OY DATA WITH L ARGE M ARGIN

When applying mcb and mc1 in binary classification tasks, we used the algorithm as suggested there.
We ran their iterative algorithm until the incremental
change was smaller than 1e−5 and used the logistic
loss function to fit the entries. In multi-class tasks the
label matrix was given by a {0, 1} matrix with a 1 entry at the correct label. In regression tasks we’ve used
the squared loss to fit the labels (as this is the loss we
tested the results against).
• geom: Finally we‘ve applied geom (Chechik et al.,
2008). This is an algorithm that tries to maximize the
margin of the classifier but the margin is computed
with respect to the revealed entries subspace. We’ve
applied this method only for binary classification as
this method was designed specifically to correct the
hinge loss methods. Again we used the iterative algorithm as suggested in there with 5 iterations. Regularization parameters were chosen using a holdout set.

As discussed earlier, under large margin assumption our
algorithm enjoys strong guarantees. For this we considered a different type of noise model – one that guarantees
large margin. First we constructed fully observed sample
of size m = 104 : The data resides in r = 20 dimension
subspace in a d = 200 dimension linear space. We then
divided the features into three types- {A1 , A2 , A3 }. Each
subset contained “enough information” and we made sure
that for each subset Ai there is a classifier wAi with large
margin that correctly classify xo when o = Ai (in fact a
random division of the features led to this). Thus, if at each
turn o = Ai for some i, the 0-imputation method would
guarantee to perform well. However, in our noise model,
for each sample, each type had probability 1/3 to appear
(independently, conditioned on the event that at least one
type of feature appears). Thus there is no guarantee that
0-imputation will work well.
Such a noise distribution can model, for example, a scenario where we collect our data by letting people answer a
survey. A person is likely to answer a certain type of question by and not answer a different type (say for example, he

Classification with Low-Rank and Missing Data

is disinclined to reveal information on his financial matters
but care less about his recreational preferences).
Denote:
(

1 A⊆o
. One can show that the class
0 else
{v · φγ (xo ) : v ∈ RΓ } can express the following target
function whenever γ ≥ 3:
χ(o; A) =

h(xo ) =

X

χ(o; Ai )wAi xo −

i

1 X
χ(o; Ai1 ∪ Ai2 )(wAi1 + wAi2 ) · x̄+
2
i1 6=i2

χ(o; A1 ∪ A2 ∪ A3 )(wA1 + wA2 + wA3 ) · x̄
Where x̄ is the vector received by filling zeros at non observed features. One can also verify that h(xo ) will have
zero expected loss. Also we can
sure that the correP make
sponding v will have norm 73
kwAi k.

Table 1. Experiment Results
(a) Multiclass Labeling
name
karma 0–imp
Cleveland
0.44
0.42
0.03
0.04
dermatology
marketing
0.70
0.71
movielens(occupation)
0.81
0.87

name
jester
books
movielens (age)

name
mammographic
bands
hepatitis
Wisconsin
horses
movielens (gender)

(b) Regression
karma 0–imp
0.23
0.24
0.25
0.25
0.16
0.22
(c) Binary Labeling
karma 0–imp
0.17
0.17
0.24
0.34
0.23
0.17
0.03
0.03
0.35
0.36
0.22
0.26

mcb
0.48
0.04
0.70
0.86

mcb
0.27
0.25
0.25

mcb
0.17
0.41
0.23
0.03
0.55
0.28

mc1
0.42
0.04
0.70
0.87

mc1
0.27
0.25
0.25

mc1
0.18
0.40
0.21
0.04
0.37
0.28

geom
0.17
0.35
0.22
0.04
0.36
0.25

1
karma
0−imputation
mcb/mc1

0.8

0.6

0.4

0.2

0

200

400

600

800

almost all users was used as a label (specifically joke number 5). The movielens data set includes users who rated
various movies, and includes met-data such as age, gender and occupation. We used the meta-data to construct
three different tasks. In the appendix we add the details as
to data set sizes and percentage of missing values in each
task. Throughout regression tasks were normalized to have
mean zero and standard deviation 1.

1000

6. Discussion and future work
Figure 3. Block type revealed entries. Mean square error, Vs.
sample size

Our method, indeed, picks a good classifier with a comparably small sample size. Also, due to higher expressiveness
it significantly outperforms 0-imputation.
5.2. Real Data
Finally, we tested our method on real data sets. We emphasize that the missing data was inherent in the real data.
The comparison appears in Table 1. The data sets were collected primarily from (Alcal-Fdez et al., 2011). The Jester
data set was collected from (Goldberg et al., 2001), The
books dataset was collected from (Ziegler et al., May 1014, 2005) and we’ve also used the MovieLens data set2 . We
took from the Jester data set around 1000 users that rated
at least 36 Jokes out of 100. One joke that was rated by
2
available here: http://grouplens.org/datasets/
movielens/

We have described the first theoretically-sound method to
cope with low rank missing data, giving rise to a classification algorithm that attains competitive error to that of the
optimal linear classifier that has access to all data. Our nonproper agnostic framework for learning a hidden low-rank
subspace comes with provable guarantees, whereas heuristics based on separate data reconstruction and classification
are shown to fail for certain scenarios.
Our technique is directly applicable to classification with
low rank missing data and polynomial kernels via kernel
(polynomial) composition. General kernels can be handled
by polynomial approximation, but it is interesting to think
about a more direct approach.
It is possible to derive all our results for a less stringent condition than λ-regularity: instead of bounding the smallest
eigenvalue of the hidden subspace, it is possible to bound
only the ratio of largest-to-smallest eigenvalue. This results
in better bounds in a straightforward plug-and-play into our
analysis, but was omitted for simplicity.

Classification with Low-Rank and Missing Data

Acknowledgements: EH: This research was supported in
part by the European Research Council project SUBLRN
and the Israel Science Foundation grant 810/11.
RL is a recipient of the Google Europe Fellowship in
Learning Theory, and this research is supported in part by
this Google Fellowship.
YM: This research was supported in part by The Israeli
Centers of Research Excellence (I-CORE) program, (Center No. 4/11), by a grant from the Israel Science Foundation, by a grant from United States-Israel Binational Science Foundation (BSF).

References
Alcal-Fdez, J., Fernandez, A., Luengo, J., Derrac, J.,
Garca, S., Snchez, L., and Herrera., F. Keel data-mining
software tool: Data set repository, integration of algorithms and experimental analysis framework. Journal of
Multiple-Valued Logic and Soft Computing, 17:2-3:255–
287, 2011. URL http://sci2s.ugr.es/keel/
missing.php.
Ben-David, S. and Dichterman, E. Learning with restricted
focus of attention. Journal of Computer and System Sciences, 56(3):277–298, 1998.
Berthet, Q. and Rigollet, P. Complexity theoretic lower
bounds for sparse principal component detection. J.
Mach. Learn. Res., W&CP, 30:1046–1066 (electronic),
2013.
Candes, E. and Recht, B. Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9:717–772, 2009.
Cesa-Bianchi, N., Shalev-Shwartz, S., and Shamir, O. Efficient learning with partially observed attributes. In Proceedings of the 27th international conference on Machine learning, 2010.
Cesa-Bianchi, N., Shalev-Shwartz, S., and Shamir, O. Online learning of noisy data. IEEE Transactions on Information Theory, 57(12):7907 –7931, dec. 2011. ISSN
0018-9448. doi: 10.1109/TIT.2011.2164053.
Chechik, Gal, Heitz, Geremy, Elidan, Gal, Abbeel, Pieter,
and Koller, Daphne. Max-margin classification of data
with absent features. J. Mach. Learn. Res., 9:1–21, 2008.
ISSN 1532-4435.
Crammer, Koby and Singer, Yoram. On the algorithmic
implementation of multiclass kernel-based vector machines. The Journal of Machine Learning Research, 2:
265–292, 2002.

Dekel, Ofer, Shamir, Ohad, and Xiao, Lin. Learning to
classify with missing and corrupted features. Mach.
Learn., 81(2):149–178, November 2010. ISSN 08856125.
Eban, Elad, , Mezuman, Elad, and Globerson, Amir. Discrete chebyshev classifiers. In Proceedings of the 27th
international conference on Machine learning, 2014.
Globerson, Amir and Roweis, Sam. Nightmare at test time:
robust learning by feature deletion. In Proceedings of the
23rd international conference on Machine learning, pp.
353–360. ACM, 2006.
Goldberg, Andrew B., Zhu, Xiaojin, Recht, Ben, Xu, JunMing, and Nowak, Robert D. Transduction with matrix
completion: Three birds with one stone. In Proceedings
of the 24th Annual Conference on Neural Information
Processing Systems 2010., pp. 757–765, 2010.
Goldberg, Ken, Roeder, Theresa, Gupta, Dhruv, and
Perkins., Chris. Eigentaste: A constant time collaborative filtering algorithm. Information Retrieval, 4(2):133–
151, 2001. URL http://www.ieor.berkeley.
edu/˜goldberg/jester-data/.
Hazan, Elad and Koren, Tomer. Linear regression with
limited observation. In Proceedings of the 29th International Conference on Machine Learning, ICML 2012,
Edinburgh, Scotland, UK, June 26 - July 1, 2012, 2012.
Hazan, Elad, Kale, Satyen, and Shalev-Shwartz, Shai.
Near-optimal algorithms for online matrix prediction. In
COLT, pp. 38.1–38.13, 2012.
Hazan, Elad, Livni, Roi, and Mansour, Yishay. Classification with low rank and missing data. arXiv preprint
arXiv:1501.03273, 2015.
Horvitz, D. G. and Thompson, D. J. A generalization
of sampling without replacement from a finite universe.
Journal of the American Statistical Association, 47:663–
685, 1952.
Lee, J., Recht, B., Salakhutdinov, R., Srebro, N., and
Tropp, J. A. Practical large-scale optimization for maxnorm regularization. In NIPS, pp. 1297–1305, 2010.
Little, Roderick J. A. and Rubin, Donald B. Statistical Analysis with Missing Data, 2nd Edition. WileyInterscience, 2002.
Salakhutdinov, R. and Srebro, N. Collaborative filtering in
a non-uniform world: Learning with the weighted trace
norm. In NIPS, pp. 2056–2064, 2010.
Shalev-Shwartz, Shai, Singer, Yoram, Srebro, Nathan, and
Cotter, Andrew. Pegasos: Primal estimated sub-gradient

Classification with Low-Rank and Missing Data

solver for svm. Mathematical programming, 127(1):3–
30, 2011.
Shamir, O. and Shalev-Shwartz, S. Collaborative filtering
with the trace norm: Learning, bounding, and transducing. JMLR - Proceedings Track, 19:661–678, 2011.
Srebro, Nathan. Learning with Matrix Factorizations. PhD
thesis, Massachusetts Institute of Technology, 2004.
Srebro, Nathan, Rennie, Jason, and Jaakkola, Tommi S.
Maximum-margin matrix factorization. In Advances in
neural information processing systems, pp. 1329–1336,
2004.
Vishwanathan, Alex Smola, Smola, Alex J, and Vishwanathan, SVN. Kernel methods for missing variables.
In In Proceedings of the Tenth International Workshop
on Artificial Intelligence and Statistics, 2005.
Ziegler, Cai-Nicolas, McNee, Sean M., Konstan,
Joseph A., and Lausen., Georg.
Improving recommendation lists through topic diversification.
Proceedings of the 14th International World Wide Web
Conference (WWW ’05), 17:2-3, May 10-14, 2005. URL
http://www2.informatik.uni-freiburg.
de/˜cziegler/BX/.

