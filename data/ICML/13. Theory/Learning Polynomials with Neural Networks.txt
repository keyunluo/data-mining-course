Learning Polynomials with Neural Networks

Alexandr Andoni
Microsoft Research

ANDONI @ MICROSOFT. COM

Rina Panigrahy
Microsoft Research

RINA @ MICROSOFT. COM

Gregory Valiant
Stanford University

GREGORY. VALIANT @ GMAIL . COM

Li Zhang
Microsoft Research

LZHA @ MICROSOFT. COM

Abstract
We study the effectiveness of learning low degree
polynomials using neural networks by the gradient descent method. While neural networks have
been shown to have great expressive power, and
gradient descent has been widely used in practice for learning neural networks, few theoretical
guarantees are known for such methods. In particular, it is well known that gradient descent can
get stuck at local minima, even for simple classes
of target functions. In this paper, we present several positive theoretical results to support the effectiveness of neural networks. We focus on twolayer neural networks where the bottom layer is
a set of non-linear hidden nodes, and the top
layer node is a linear function, similar to Barron (1993). First we show that for a randomly
initialized neural network with sufficiently many
hidden units, the generic gradient descent algorithm learns any low degree polynomial, assuming we initialize the weights randomly. Secondly,
we show that if we use complex-valued weights
(the target function can still be real), then under suitable conditions, there are no “robust local minima”: the neural network can always escape a local minimum by performing a random
perturbation. This property does not hold for
real-valued weights. Thirdly, we discuss whether
sparse polynomials can be learned with small
neural networks, with the size dependent on the
sparsity of the target function.
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

1. Introduction
Neural networks have drawn signification attention from
the machine learning community, in part due to the recent empirical successes (see the surveys Bengio (2009;
2013)). Neural networks have led to state-of-the-art systems for crucial applications such as image recognition,
speech recognition, natural language processing, and others; see, e.g., (Krizhevsky et al., 2012; Goodfellow et al.,
2013; Wan et al., 2013). There are several key concepts
that have been instrumental in the success of learning the
neural networks, including gradient descent, parallel implementations, carefully-designed network structure (e.g.,
convolutional networks), unsupervised pre-training (e.g.,
auto-encoders, RBMs) (Hinton et al., 2006; Ranzato et al.,
2006; Bengio et al., 2007), among others.
With the flurry of empirical successes and anecdotal evidence suggesting (sometimes conflicting) principles in the
design and training of neural networks, there seems to be
a need for a more solid theoretical understanding of neural networks. Perhaps the most natural starting point for
such investigations is the question of when a neural network provably learns a given target function. A classical
result is that, if the target function is linear, the perceptron
algorithm will learn it. This is equivalent to saying that
gradient descent on a neural network with a single node (or
layer) can successfully learn a linear function.
But what can we prove about gradient descent on networks
with more than one layer? In full generality, this question
is nearly hopeless: standard complexity-theoretic results
strongly suggest there are no efficient algorithms for learning arbitrary functions (let alone gradient descent), even for
target functions representable by very low-depth networks
(circuits) (Applebaum et al., 2006). Nevertheless, we may
hope to prove concrete results in restricted settings whose

structure is reminiscent of the structure present in practical
settings.
In this work, we consider learning bounded degree polynomials by neural networks. Polynomials are an expressive class of functions since they can be used to approximate many “reasonable” functions, for example, any Lipshitz function can be well-approximated by a bounded degree polynomial (see, e.g., Andoni et al. (2014)). We establish provable guarantees for gradient descent on two-layer
neural networks (i.e., networks with one hidden layer of
neurons) for learning bounded degree polynomials. Our
main result shows this setting can learn any bounded degree
polynomial, provided the neural network is sufficiently
large. Specifically, suppose the target function f is a degree
d polynomial over an n-dimensional variable x ∈ Cn , with
x distributed according to a product distribution (for our
main results, the distribution can be Gaussian or uniform
distributions over the reals). The two-layer
network with m
Pm
hidden units outputs a function g(x) = i=1 αi φ(wi · x),
where αi ∈ C, wi ∈ Cn are network parameters, and φ
is a non-linear activation function (e.g., a sigmoid). It has
been known (Barron, 1993) that, when m ≈ nd , there exists some choice of parameters wi , αi so that the network g
closely approximates the function f . We show that, in fact,
with high probability, even if the bottom layer (wi ’s) is set
to be random, there is a choice for top layer (αi ’s) such
that the neural network approximates the target function f .
Hence, the perceptron algorithm, when run on only the top
layer while keeping the bottom layer fixed, will learn the
correct weights αi .
Learning polynomials. Analyzing gradient descent on the
entire network turns out to be more challenging as, unlike when the bottom layer is fixed, the error function is
no longer convex. We show that even if gradient descent
is run on the entire network, i.e. on both top and bottom
layers, the neural network will still find the network parameters αi and wi , for which the network approximates
the target function f . This can be interpreted as saying that
the effect of learning the bottom layer does not negatively
affect the overall learning of the target function. Indeed,
we show that the learning of the top layer (αi ’s) happens
sufficiently fast, in particular before the bottom layer (wi ’s)
significantly changes. This insight may be useful in analyzing gradient descent for the neural networks with multiple
layers.
We further explore the landscape of the gradient descent
by showing a conceptually compelling, though incomparable result: we show that for sufficiently large networks,
in a large region of the parameter space, there are no robust local optima. That is, for any point in the parameter
space that satisfies some mild conditions on the weights,
with constant probability, a random perturbation will re-

duce the error by a non-negligible factor. Hence even when
the gradient descent is stuck at a local minimal, a random
perturbation would take it out. Because of the conditions
on the weights of the neural network, we can not use this
theorem to conclude that gradient descent plus random perturbation will always converge to the global optima from
any initial neural network initialization; nevertheless, this
provides a rigorous, and conceptually compelling explanation for why the existence of local optima do not deter the
usage of neural networks in practice.
We stress that the random perturbation on the weights is
complex-valued but the input can still be real valued. In
contrast, the same “robustness” result does not hold when
the random perturbation is strictly real; in this case there are
robust local minima such that a random perturbation will
increase the error. Intuitively, the complex-valued weights
give extra dimensions, and hence nicer geometry, to the local structure of the space of neural networks, which allows
it to escape from a local minimum. We find this result intriguing and hope it can further motivate the construction
of new types of neural networks.
Learning sparse polynomials. It is natural to ask whether,
in the case of a target function that is “simpler” (e.g., because it can be represented by a small number of hidden
units), the gradient descent can actually learn it more efficiently. “More efficiently” here can mean both: 1) with a
smaller network, 2) at a faster convergence rate. To study
this general question, we suggest the following concrete
open problem. Define the sparsity of a polynomial as the
number of its nonzero monomial terms. We know, thanks
to Barron (1993), a k-sparse degree-d polynomial can be
represented by a neural network with nk · dO(d) hidden
nodes, but can gradient descent learn such a polynomial by
a neural network with only (nk · dd )O(1) hidden nodes, and
in a similar amount of time?
We note here we require the target function to be a k-sparse
degree-d polynomial, more restrictive than requiring the
target to be representable by a small network as in Barron
(1994). This might be an easier problem, especially given
that recent work has shown that one can indeed learn, for
Gaussian or uniform distribution, any sparse polynomial in
nk · dO(d) time (Andoni et al., 2014), albeit via an algorithm that does not resemble gradient descent. We should
emphasize that it is important to consider well-behaved distribution such as Gaussian or uniform distributions. Otherwise, if we allow arbitrary distribution, the sparsity assumption may not be helpful. For example, in the boolean
case (where x is distributed over the boolean cube), this is
at least as hard as ”learning parity with noise” and ”learning
juntas” – problems that we suspect do not admit any efficient algorithms and are even used as cryptographic primitives (Alekhnovich, 2003; Regev, 2005; Peikert, 2009; Ap-

plebaum et al., 2010).
While we do not have the answer to this natural but challenging question, we give some indications why this may
indeed be possible. In particular we show that, if the target
function depends only on k  n variables, then the neural
network will learn a function that also depends on these k
variables. Additionally, we provide some strong empirical
evidence that such small networks are capable of learning
sparse polynomials.
With the recent success of neural networks in practice,
there is an increasing demand of insights and tools for analyzing gradient descent for learning neural networks. Our
work represents one such effort. We note that Saxe et al.
(2013) is another recent work in this space.
We will only sketch proof ideas in this abstract. The full
details can be found in the supplementary material.

2. Preliminaries
We will mostly use complex-valued inputs and network parameters, to simplify the description. Most results generalize to other distributions.
For a complex number c√∈ C, write c as its conjugate, and
define its norm as |c| = cc. For a matrix (or a vector) P ,
write P ∗ as the adjoint of P , i.e., (P ∗ )ij = Pji .
We denote by C(r) the uniform distribution of complex
number with norm exactly r, N(σ 2 ) the real Gaussian distribution with mean 0 and variance σ 2 , and U(r) the uniform distribution on [−r, r]. For a distribution P, denote
by Pn the n-fold product distribution. For any given distribution D and two functions f, g : Cn 7→ C, define the
inner product hf, giD p
= Ex∼D [f (x)g(x)] and the norm
p
kf kD = hf, f iD = Ex∼D |f (x)|2 .
Polynomials. For any J = (J1 , · · · , Jn ) ∈ P
Nn , write a
J1
J
Jn
monomial x = x1 ·P
· · xn . Define |J| =
k Jk . For
a polynomial p(x) = J bJ xJ where bJ ∈ C, its degree
∆

deg(p) = maxbJ 6=0 |J|. All the degree d polynomials form
an nd -dimensional linear space. For any given distribution
D, a polynomial basis P1 , P2 , . . . is orthonormal w.r.t. D if
hPi , Pj iD = 1 if i = j and is zero otherwise.
We will use the following basic but important fact that
monomials form an orthonormal basis for D = C(1)n .
0
Fact 2.1. hwJ , wJ iC(r)n = r2|J| if J = J 0 and 0 otherwise.
For Gaussian and uniform distributions, the corresponding
orthonormal bases are the Hermite and Legendre polynomials respectively.
Neural networks. Assume φ : C 7→ P
C is an analytic funcj
tion without poles. Write φ(z) =
j≥0 aj z . For any

w ∈ Cn , define φw : Cn 7→ C as
!
X
X
φw (x) = φ
wk xk =
aj
k

:=

X

aJ wJ xJ .

j

!j
X

wk xk

k

(2.1)

J

The (two-layer) neural
network is defined as a function of
P
the form g(x) = i αi φwi (x). Here φ is called the activation function, and each φwi — a hidden unit. We refer to
αi ’s as the “upper layer” and wi ’s as the “lower layer”.
In this paper, we will consider activation functions φ(z) =
Pd
ez and its truncated version φd (z) = k=0 z k /k!, i.e. ez
with higher than degree d terms truncated (in this case, we
call it a truncated neural network). While a more common
φ is the sigmoid function, the particular form of φ is not
that important as long as it has “good” coverage of all the
degrees in its Taylor series (see, for example, the discussion in Barron (1993)), and for analysis convenience, it is
defined everywhere on the complex plane. Our particular
choice is just for the sake of a cleaner analysis.
Learning. Here learning is defined to construct the representation of an unknown (or target) function, from some
function family F, by given random samples. In this paper, we focus on learning low degree polynomials, where
the input distribution is drawn from C(1)n . We will comment when the results can be extended to the other distributions. Learning the neural network via gradient descent
proceeds by minimizing the error kf − gk2D as a function
of weights ws and αs ’s of φ. To focus on the main ideas,
we will assume that there are enough samples such that the
empirical estimate of gradient is sufficiently accurate. So
we will be working with the model where we do have the
access to the gradient.

3. Random Initialization of Neural Network
In this section we prove that any polynomial can be represented using a linear combination of a sufficient number of
hidden units with weights wi initialized randomly. This is
a strengthening of (Barron, 1993), who showed that there
exist some weights wi , for which a linear combination of
the hidden units yields a given polynomial. In addition,
we show that the weights at the top node has small norm.
Compared to the representability result in Barron (1993),
we require more units, O(n2d ), rather than O(nd ) there.
Theorem 3.1. [Representation Theorem] Let φ(z) = ez
and the distribution D = C(1)n . For m ≥ O(n2d /2 ),
we choose√m random w1 , w2 , . . . , wm from the distribution C(1/ n)n . Then, with high probability, for any polynomial P
p of degree d and norm 1, there existP
α1 , . . . , αm
2
2d
wi
where
|α
|
=
O(n
/m)
such
that
k
−
i
i
i αi φ

pkD ≤ .
The result also holds whenever D is Gaussian distribution
N(1), or the uniform distribution U(1).
To prove the theorem, we consider general φ and D first
and then instantiate them by estimating the relevant parameters. The following is immediate from (2.1) and Fact 2.1.
n

n

Observation 3.2. For any x ∈ C and any J ∈ N ,
Ew∼C(r)n wJ φw (x) = aJ r2|J| xJ .
In the following, we will show, constructively, that for
the given set of units, how to pick the top layer weights
to approximate
any given p. Define for any polynomial
P
p(x) = J bJ xJ , r ≥ 0, and weight vector w, Tp,r (w) =
P
J
2|J|
). By ObservaJ cJ (r)bJ w , where cJ (r) , 1/(aJ r
tion 3.2, for any x,
Ew∼C(r)n [Tp,r (w)φw (x)] = p(x) .

(3.1)

Hence Tp,r (w)φw is centered around p in the functional
space. We can apply standard concentration bound to
show that the average over m such terms, with w chosen independently and m large enough, one can approximate p(x) arbitrarily close. More precisely, suppose that
w1 , . . . , P
wm are sampled from C(r)n . Let η(x) denote the
m
1
wi
error m
i=1 Tp,r (w)φ (x) − p(x). We obtain,
Ew1 ,...,wm kηk2D ≤

1
Ew∼C(r)n |Tp,r (w)|2 kφw k2D .
m

(3.2)

P
Let a(d) = min|J|≤d |aJ |, and define kpk1 =
J |bJ |.
When w is from C(r)n for r ≤ 1, we have
X
X
|Tp,r (w)| ≤
|cJ (r)bJ wJ | =
|bJ wJ /(aJ r2|J| )|
J

J

by deg(p) = d
X
X
=
|bJ /(aJ r|J| )| ≤ 1/(a(d)rd )
|bJ |
J

= kpk1 /(a(d)rd ) .

J

(3.3)

Denote by βD (d, r) = Ew∼C(r)n kφw k2D /r2d and γD (d) =
maxkpkD =1 kpk1 . Plugging (3.3) into (3.2), we have
Lemma 3.3. Whenever m ≥ γD (d)2 βD (d, r)/(a(d))2 ,
with high probability Ew1 ,...,wm kηkD ≤  for any p where
deg(p) ≤ d and kpkD = 1.
Note that the number of required hidden units is determined
by various parameters dependent on φ and D.
Now consider φ(z) = ez . Then a(d) ≥
√ 1/d!. Suppose
n
2 nr
/r2d ). Taking
that D = C(1)
.
Then
β
(d,
r)
=
O(e
D
√
√
r = O(1/ n), we have βD (d, 1/ √
n) = O(nd ). Further,
P
2
kpkD = J |bJ | , so γD (d) = O( nd ). Plugging these

parameters into Lemma 3.3, we have m = O(n2d /2 ). In
addition αi = Tp,r (wi )/m, so
m
X
i=1

|αi |2 =

1
m2

m
X

|Tp,r (wi )|2 ≤

kpk21
ma(d)r 2d

= O(n2d /m) .

i=1

The same bounds can be derived for the distributions such
as standard Gaussian distribution in Rn and the uniform
distribution in [0, 1]n . This proves Theorem 3.1.

4. Gradient Descent for Polynomials
In this section we show that gradient descent on a twolayer neural network can learn a polynomial function, given
enough hidden units and small enough learning rate. The
statement relies on the fact that the weights have been initialized randomly. The formal statement is in Theorem 4.2.
First we prove a warm up (simpler) statement: if we run
gradient descent only in the upper layer (and leave intact
the weights in the lower layer), then the gradient descent
will converge to a network approximating the polynomial
up to a small error. We use the representation theorem from
the previous section. In this simpler statement, we also assume that we have access to the exact value of the gradient.
Then we show a more general statement, which reaches the
same conclusion even if we run the generic gradient descent (i.e., on the entire network), assuming that the number of hidden units is sufficiently large. The main intuition
behind the result is that the top-layer weights converge to
a good state (as in the simplified theorem) before the modifications in the lower layer change enough to affect the
overall representability. In particular, one step of a gradient
descent will update the weights αi of all the hidden units
at once. Hence the “total speed” of convergence of weights
αi is essentially proportional to the number of hidden units,
whereas the speed of change of each wi is not. Once we
have enough hidden units, namely nO(d) , the speed of convergence of αi is sufficiently high to overtake the changes
to the weight wi of any particular hidden unit.
Theorem 4.1. Fix some degree-d polynomial f of norm 1,
and desired error  > 0. Consider a two-layer neural network with m = Ω(n2d /2 ) hidden units. We initialize the
αi ’s such that kαk ≤ 1 (e.g., α = 0√
would do), and choose
the weights wi randomly from C(1/ n)n . Consider the algorithm where we run the gradient descent on the weights
in the upper layer (keeping weights in lower layer fixed),
with access to exact gradient. Then, for a learning rate
λ < 1/m, the algorithm
to a network g such
 will converge

that kg − f k ≤  in O

n2d
λ2 m

steps.

Proof. As in preliminaries, g denotesP
the function prom
duced by the neural network: g(x) = i=1 αi φ(wit · x).

Abusing notation, we will think of all functions in the base
of monomials xJ . In particular, g in the function space is
wi
a sum of m vectors
Pmp1 . . . pm , where pi = φ depends on
vector wi : g = i=1 αi pi .
The gradient descent minimizes the quantity E = he, ei =
e∗ · e, for the error function e = f − g. We would like to
take gradient with respect to αi , but E is not analytic with
respect to e. Instead we rely on Wirtinger calculus, and
consider the following gradient:1
∗

∂e
· e = −pi ∗ e = −hpi , ei.
∂αi
In one step of the gradient descent the new function g 0 is
X
g0 =
pi · (αi + λhpi , ei),
i

and the new error function:
!
0

0

e =f −g =e−λ

X
i

pi hpi , ei =

I −λ

X

pi p∗i

e,

i

where I is the identity matrix.
Let P be the matrix whose
P
columns are pi ’s. Then i pi p∗i = P P ∗ .

(0) 2

Finally, note that we can have at most kaλ2k steps, as
ka(l) k ≥ 0 always. Thus, after that many steps, we
must have that kP a(l) k ≤ , and hence ke(l) k ≤ krk +
kP a(l) k ≤ 2. Since
ka(0) k2 ≤ O(n2d /m), we obtain a

total of O

n2d
2 λm

steps.

We now continue to our main result, which shows that the
gradient descent on the entire network will converge as
well, given enough hidden units and small enough learning
rate. The proof of the theorem appears in the supplementary material.
Theorem 4.2 (General gradient descent). Fix target error
 > 0 and degree d ≥ 1. Suppose the weights
α are initial√
ized to zero and wi ’s are random C(1/ n)n . Assume the
number of hidden units is m = Ω(n6d /3 ) and the learning rate is λ ≤ 1/4m. Then, given a degree-d polynomial
f (x) of unit norm, the gradient descent will converge to
a net, which approximates
 2d f up to error . The number
of steps required is O λn2 m , and the number of samples
required is M = mO(1) .

5. Random Perturbation at Local Minima

The results of the previous section show that for a sufficiently large neural network, with high probability over the
e(l) = (I − λP P ∗ )l e(0) ,
random initialization of the weights, the gradient descent
will learn a degree d polynomial. In this section, we prove
where e(0) is the starting error function.
a conceptually compelling, though incomparable result: we
(0)
show that for sufficiently large networks, in a large region
We apply the representation Theorem 3.1 to e to obtain
(0)
of the parameter space, while there may exist local mineP = x+r, where krk ≤ , and x can be expressed as x =
2
2d
t
ima, there are no robust local minima. That is, for any
i ai pi , with kak ≤ O(n /m) for a , (a1 , . . . am ) .
point in the specified parameter space, as long as the error
Note that x = P a.
is not vanishingly small, with constant probability a ranThen we can rewrite the above as:
dom perturbation will reduce the error by at least 1/nO(d)
factor (see Theorem 5.1). Because of the conditions on the
e(l) = (I−λP P ∗ )l (r+P a) = (I−λP P ∗ )l r+(I−λP P ∗ )l P a. parameters, we can not use this theorem to conclude that
gradient descent will always converge to the global optima
Let’s see what happens after one iteration to the second
from any initial neural network initialization. Nevertheless,
term: (I −λP P ∗ )P a = P a−λP P ∗ P a = P (I −λP ∗ P )a.
this provides a rigorous explanation for why local optima
Hence, (I − λP P ∗ )l P a = P (I − λP ∗ P )l a.
may not be so damaging for neural networks in practice.
Let a(l) , (I − λP ∗ P )l a, i.e., e(l) = P a(l) . Suppose
Furthermore, this perspective may prove useful for going
kP a(l) k ≥ . Then, we have that
beyond the results of the previous section, for example for
addressing the harder questions posed in the later Section 6.
ka(l+1) k2 = (a(l+1) )∗ a(l+1)
We stress that this result relies crucially on the fact that we
= ((I − λP ∗ P )a(l) )∗ (I − λP ∗ P )a(l)
allow complex valued weights. In fact we also show such a
result is not true when the weights are real-valued.
= ka(l) k2 − 2λkP a(l) k2 + λ2 kP ∗ P a(l) k2 . (4.1)
After l iterations of the gradient descent, the error is

√
As we have that kP k ≤ m and λkP k2 ≤ 1, we conclude
that ka(l+1) k2 ≤ ka(l) k2 − λkP a(l) k2 ≤ ka(l) k2 − λ2 .
1

We essentially consider the variables and their conjugates as
independent variables, and then take derivative with respect to αi∗ .

We now state the main
Presult of this section. Define the
fourth-norm kαk4 = ( i |αi |4 )1/4 .
Theorem 5.1. There exist constant c1 , c2 , c3 ,P
c4 > 0
wi
such that for a truncated neural network g =
i αi φd
c1 d
where m = Ω(n ), kwi k = O(log n), and kαk4 =

c3 d
O(kαk/nc2 d ), if kekD = Ω(kαkn
), then a perturbation
√
of each wsj drawn from C(1/ n) reduces the total error
by a factor of at least 1 + 1/nc4 d , with constant probability.

Note that by Theorem 3.1, when m is large enough, m =
nΩ(d) , the conditions in the theorem can all be met.
We first sketch the proof idea. Under a given distribution
D, for a target function f and a neural network g, consider the error function kek2D = kg − f k2D . For a local
perturbation g + ∆g of g, kg + ∆g − f k2D = kek2D +
2 Re(h∆g, eiD ) + k∆gk2D . Hence the change in error is
∆kek2D = 2 Re(h∆g, eiD ) + k∆gk2D . We shall show that,
with constant probability, the linear term is negative and
overwhelms the quadratic term if the perturbation is sufficiently small. The proof consists of two steps. First
we consider a single hidden unit. We show that a local
perturbation can create non-negligible correlation with any
bounded degree polynomial. Secondly we show that, by
the anti-concentration inequality2 , when we perturb many
hidden units independently, the aggregated correlation is
still large and, when the number of hidden units is large
enough, exceeds the quadratic term, which can be bounded
by the standard concentration bound.
Our claim applies when the error function e = g − f has
bounded degree d, which is the reason the result applies
to networks
P with a truncated activation function φd where
φd (z) = 0≤j≤d aj z j . For the simplicity of notation, we
will simply write it as φ(z).
Here it is important that w is complex. The distribution D
on x can be over the reals, for example, D can be standard Gaussian distribution N(1)n in Rn or the uniform
distribution U(1)n over [−1, 1]n . We first show our statement for D = C(1)n . Then we extend it to the case when
D = N(1)n or U(1)n . We will only sketch the main steps
of proofs in this section.

Proof sketch. Clearly for any x, ∆δ φw (x) can be written as a polynomial in δ without constant term. By
Fact 2.1, Eδ∈C(r)n [∆δ φw (x)] = 0. This implies that
Eδ∈C(r)n [h∆δ φw , ηiD ] = 0,
Write B(w) = hφw , ηiD . As η is a polynomial with degree
d, so is B(w). By the above, we have
Eδ∈C(r)n [B(w + δ)] = B(w) .

(5.1)

We lower bound Eδ∈C(r)n [|B(w +δ)−B(w)|2 ] as follows.
We first show the case when w = 0. Then we apply the
“shifting” lemma (Lemma 5.4) to complete the proof.
Lemma 5.3. For 0 ≤ r ≤ 1, Eδ∈C(r)n [|B(δ) − B(0)|2 ] ≥
r2d a(d)2 .
Lemma 5.4. Suppose that f is a degree d polynomial on n
variables. Let v = (v1 , . . . , vn ) such that kvk∞ ≤ L. Let
fv (x) = f (v + x). Then kfv k2 ≤ nd (L + 1)2d kf k2 .
By the above two lemmas, we can show that

Eδ∈C(r)n [|h∆δ φw , ηiD |2 ] = Ω r2d a(d)2 /(nd (L + 1)2d ) ,
and further transfer this bound to (Re h∆δ φw , ηiD )2 , to
complete the proof.
We note that the above theorem holds for large range of r
and w. But to suppress the second order term, we √
will only
need the theorem in the range where r = O(1/ n) and
kwk = O(log n).
5.2. Random perturbation of many hidden units
Pm
wi
Now consider a neural network g(x) =
i=1 αi φ (x),
0
where
kwi k = O(log n).
Let g (x) =
Pm each
from
αi φwi +δi (x), where each δi is i.i.d.
i=1√
C(1/ n)n .

We give further details of the proof; the missing proofs are
in the supplementary material.

ke0 k2D − kek2D = k(g 0 − g) + ek2D − kek2D

5.1. Random perturbation of one hidden unit

Pm
First consider kg 0 − gk2D = k i=1 αi ∆δi φwi k2D . We
can view ∆δ φw as a vector in the functional space,
so each ∆δi φwi is a random vector. We have shown
that Eδi [∆δi √
φwi ] = 0, and for kwi k = O(log n) and
r = O(1/ n), Eδi ∼C(r)n k∆δi φwi k2D = O(nO(1) ).
Since ∆δi φwi ’s are independent, by standard concentration
bound, with high probability

We first show that for a single hidden unit, a random perturbation will create large correlation with P
any bounded
J J
degree polynomial. Recall that φw (x) =
J aJ w x ,
n
and a(d) = min|J|≤d |aJ |. For x ∈ C , we define
kxk∞ = maxj |xj |. We denote by ∆δ φw = φw+δ − φw as
the perturbation of a hidden unit φw by δ. We have
Theorem 5.2. For any x ∈ Cn , Eδ∈C(r)n [∆δ φw (x)] = 0.
For any η such that deg(η) ≤ d, and kηkD ≥ 1, we have
that for any 0 < r ≤ 1 and kwk∞ ≤ rL,

 2d


a(d)2
.
Eδ∈C(r)n Re(h∆δ φw , ηiD )2 = Ω nrd (L+1)
2d
2

This is where we need the condition on kαk4 .

= kg 0 − gk2D + 2 Re(hg 0 − g, eiD ) . (5.2)

kg 0 − gk2D = O(nO(1) kαk2 ) .

(5.3)

For the linear term Re(hg 0 − g, eiD ), by using Theorem 5.2
and anti-concentration inequality, we can show that when
kαk4 ≤ kαk/ncd , with constant probability, say 1/4,
Re(hg 0 − g, eiD ) = −Ω(kαkkekD /nO(d) ) .

(5.4)

Combining (5.2,5.3,5.4), we have whenever kαk4 ≤
ncd kαk and kαk ≤ kekD /nO(d) , we have that ke0 k2D ≤
kek2D − kαkkekD /nO(d) with constant probability. Hence,
we have proved the main theorem.
5.3. Extension to distributions on reals
The above proof can also be extended to the case where the
x is not complex but chosen from a Gaussian distribution
N(1)n in Rn or uniform distribution U(1)n on [−1, 1]n .
This follows from the following observation that relates the
norm of a polynomial under different distributions.
Observation 5.5. Let P (x) be a degree d polynomial.
Then kP kD = Ω(1/dd/2 )kP kC(1)n and
d/2
O(d kP kC(1)n ), where D = N(1)n or U(1)n .
The above observation implies that if we replace C(1)n by
N(1)n or U(1)n , the bound in Theorem 5.2 is only affected
by a factor dependent on d only (dd or 2d ). Since we assume d to be constant, we have:
Corollary 5.6. The same statement in Theorem 5.1 holds
when D = N(1)n or D = U(1)n .

change in the weights δij . Fix one particular hidden unit
i, and fixed j ∈ [n], and consider the term corresponding
to second Legendre polynomial in xj , L2 (xj ) (remember
that the Legendre polynomials are the basis for our input
distribution, so we are considering one “coordinate” in the
polynomial basis). We claim that its coefficient is positive:
in fact it is a (positive) combination of even powers of δi,j 0
for all j 0 ∈ [n]. In particular, say in a term al (δi x)l , we
have contribution to L2 (xj ) only from terms of the form
δ J xJ , where vector J is even (any other term has correlation 0 with L2 (xj )).
We can now choose e = g −f
P so that he, ∆gi is positive by
choosing f (x) = g(0) − j L2 (xj ). Then the change in
he, ei is: ∆he, ei = he0 , e0 i−he, ei = 2he, ∆gi+h∆g, ∆gi.
The error strictly increases with probability 1. It is also
∂g
, when all wi = 0, is
clear why the gradient is zero: ∂w
ij
composed only of a linear in x terms, whereas e = g − f is
has no constant or linear terms.
We remark that the above holds even if we perform a small
random perturbation on the weights αi as well (it suffices
that αi and perturbed versions remain positive).

5.4. Robust local minima for real weights
The perturbation theorem 5.1 uses random perturbation in
the complex plane to escape a local minimum. It is natural
to ask whether real-valued perturbation would be sufficient
instead. We show that this is not the case: there are examples, where a real-valued perturbation does not improve the
error. This suggest that using complex perturbations may
be useful.
LemmaP
5.7. Consider a network with activation function
d
φ(z) = l=0 al z l , where al ≥ 0, on a neural network with
one layer of hidden units, with real weights; the input distribution is x ∈ U(1)n . There exist a set of network parameters where the gradient is zero and a random perturbation
on the weights {wi }i goes in the direction away from the
target function, i.e., ke0 kD > kekD with high probability.
Proof. We will give a construction of a set of parameters,
which are a local minimum for the gradient descent, and a
real-valued random perturbation of weights is expected to
increase the error. This point is where all hidden units have
identical weights
P wi = 0 and αi = 1/m (for m hidden
units), so that αi = 1.
We show why local perturbation does not reduce error, except with a very small probability. Let δi be the perturbation in weight wi ; for concreteness, suppose each perturbation is uniform from [−λ, λ] for some λ  1/m. Let
∆g = g 0 − g denote the change in the output function of
the neural net. We argue that Eδi [∆g] is non-zero. Note
that the change ∆g can be written as a polynomial in the

6. Learning sparse polynomials
In this section we study whether smaller neural networks
are sufficient for learning sparse polynomials (containing
few monomials). As an intermediary step towards this goal,
we will also consider the setting where the polynomial only
depends on a small subset of the n variables (and hence
is also sparse). These questions can be viewed as clean
and potentially theoretically tractable special cases of the
general question of understanding the relation between the
representation complexity and the learning complexity of
neural networks for some given class of functions.
6.1. Learning n-sparse polynomials
Can a neural network with O(n) hidden units learn a
quadratic or cubic polynomial that has ≈ n monomials?
We provide strong empirical evidence (see Fig. 1) suggesting that, for the case of n-sparse polynomials over n variables, a neural network with O(n) hidden units can learn
the function. We train the net using 5n hidden units while
varying n through the values 10, 20, 40, and 80. The polynomial is constructed using randomly chosen n monomials.
The plots show that the training error drops significantly after a reasonable number of iterations that depends on n.
6.2. Learning polynomials over few variables
As an intermediary step towards the sparse polynomial
case, we investigate whether a small neural network suffices to learn a sparse polynomial which also depends only

Training Error

Learning n−Sparse Quadratic Polynomials Over n Variables
0.8
n=10
n=20
0.6
n=40
n=80
0.4
0.2

Training Error

0

0

500

1500 2000 2500 3000
Iteration
Learning n−Sparse Cubic Polynomials Over n Variables
0.5
n=10
0.4
n=20
n=40
0.3
n=80

polynomial ψ and gathering terms that are dependent on
x1 and others
q(x2 , x3 , .., xn ) + h(x) where
P we
Pget e =
J J
h(x) =
i αi
J cJ wi x where each J has non zero
degree in x1 (that is J1 is non zero) and q does not depend
on
x1 (or its weights wi,1 ). All we need to show that
P either
∂e
2
i |h ∂wi,1 , ei| is non-zero. So the partial derivative of the
error with respect to wi,1 is
X X
X
J J
∂e
∂
=
(
α
c
w
x
)
=
α
cJ J1 wiJ xJ /wi,1 .
l
J
i
k
∂wi,1
∂wi,1

0.2

J

l

1000

J

Therefore
X

∂e
=
wi,1 ∂w
i,1

X

αi

i

i

X

cJ J1 wiJ xJ =

J

X
J

J1

X

αi cJ wiJ xJ

i

and hence
X
∂e
, ei
h
wi,1 ∂w
i,1
i

0.1
0

=h
0

0.5

1
Iteration

1.5

2
4

x 10

=

X
J

i

X

X

J

Figure 1. In the above plots the neural networks had 5n hidden
units, and the polynomials were chosen by selecting each of the n
monomials uniformly at random from the O(n2 ) (in the quadratic
case), or O(n3 ) (in the cubic case) possible monomials.

on k variables. Here, a simpler goal may be to prove that
gradient descent learns the polynomial using only k O(d)
hidden units instead of nO(d) . We are unable to prove this
but provide some evidence in this direction. First we show
(Lemma 6.1) that assuming x ∈ C(1)n , at the termination of the gradient descent, the final function output by
the net will depend on the k relevant variables only. We
show a similar result for the (more realistic) case where
x ∈ N(1)n , for a specific transfer function ψ, built using
Hermite polynomials. We will use HJ (x) to denote the
Hermite polynomial over x corresponding to a vector J of
degrees in the n variables in x. (Some details are deferred
to full version.)
Lemma 6.1. If x ∈ C(1)n and the target function f does
not depend on a variable xi , then: whenever the gradient
descent converges to a point with zero gradient, the output g does not depend on xi . This also holds for the input
distribution x ∈ N(1)n , provided
one uses a special actiP
vation function ψ w (x) = J aJ HJ xJ wJ .
Proof. The main idea is that if a variable, say x1 , is not
used in f , then the error e can be written as a sum of two
polynomials, one that does not involve x1 and another that
has x1 in every monomial and these two are orthonormal.
P
Let e = f − g = f − s αi ψ(wi · x). By expanding the

X
XX
J1 (
αi cJ wiJ )xJ ,
(
αi cJ wiJ )xJ i

J1 |

J

i

J 2

αi cJ w | ,

i

P
P
J J
which
as h =
=
i αi
J cJ wi x
P Pmust be Jnon-zero
J
J(
i αi cJ wi )x is non-zero (each J1 is non-zero).
P
This means that i wi,1 h ∂w∂i,1 e, ei is non-zero. Thus at
least one of the h ∂w∂i,1 e, ei is non-zero, which means that
the gradient descent has not converged yet.
A similar proof works for the case when x is real and we
instead use a modified polynomial ψ where each monomial
is replaced by a Hermite polynomial. The proof is based on
the orthonormality of these polynomials over N(1)n .
Further we point out that neural networks are able to learn
polynomials based on their best possible sparsity under any
orthonormal transform; i.e., the notion of sparsity is independent of the chosen coordinate system. This is because
the neural net works with dot products of the input point.
Observation 6.2. If a neural network can learn a k-sparse
polynomial in time T (k, d, n), then it can learn also learn
any polynomial that is k-sparse under any orthonormal
transform in the same time complexity.
Proof. This follows from the rotational invariance of the
gradient descent process when g is a function of wit .x over
the different hidden units i. That is rotating the coordinate
system does not change the gradient descent process.
In order to show the gradient descent succeeds with k O(d)
units, we need to assume a “High-Rank Condition” (a variant of the condition in Theorem 3.1) for similar analysis to
Section 4 to hold. But we are currently unable to prove the
“high-rank condition” and leave it as an open question.

References
Alekhnovich, Michael. More on average case vs approximation complexity. In Proceedings of the Symposium on
Foundations of Computer Science (FOCS), 2003.
Andoni, Alexandr, Panigrahy, Rina, Valiant, Gregory, and
Zhang, Li. Learning sparse polynomial functions. In
Proceedings of the ACM-SIAM Symposium on Discrete
Algorithms (SODA), 2014.
Applebaum, Benny, Ishai, Yuval, and Kushilevitz, Eyal.
Cryptography in ncˆ0. SIAM Journal on Computing, 36
(4):845–888, 2006.
Applebaum, Benny, Barak, Boaz, and Wigderson, Avi.
Public-key cryptosystem from different assumptions. In
Proceedings of the Symposium on Theory of Computing
(STOC), 2010.
Barron, Andrew R. Universal approximation bounds for
superpositions of a sigmoidal function. IEEE Transactions on Information Theory, 39(3):930–945, 1993.
Barron, Andrew R. Approximation and estimation bounds
for artificial neural networks. Machine Learning, 14:
115–133, 1994.
Bengio, Y. Learning deep architectures for ai. Foundations
and Trends in Machine Learning, 2009.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.
Greedy layer-wise training of deep networks. In NIPS,
2007.
Bengio, Yoshua. Deep learning of representations: Looking forward. arXiv preprint arXiv:1305.0445, 2013.
Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville,
A., and Bengio, Y. Maxout networks. In ICML, 2013.
Hinton, G. E., Osinderoand, S., and Teh, Y. A fast learning
algorithm for deep belief nets. Neural Computation, 18:
1527–1554, 2006.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classification with deep convolutional neural networks.
In NIPS, 2012.
Peikert, Chris. Public-key cryptosystems from the worstcase shortest vector problem. In Proceedings of the Symposium on Theory of Computing (STOC), 2009.
Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. Efficient learning of sparse representations with an energybased model. NIPS, 2006.

Regev, Oded. On lattices, learning with errors, random
linear codes, and cryptography. In STOC ’05: Proceedings of the thirty-seventh annual ACM symposium
on Theory of computing, pp. 84–93, New York, NY,
USA, 2005. ACM. ISBN 1-58113-960-8. doi: http:
//doi.acm.org/10.1145/1060590.1060603.
Saxe, Andrew M, McClelland, James L, and Ganguli,
Surya. Dynamics of learning in deep linear neural networks. NIPS Workshop on Deep Learning, 2013.
Wan, Li, Zeiler, Matthew, Zhang, Sixin, LeCun, Yann, and
Fergus, Rob. Regularization of neural networks using
dropconnect. In ICML, 2013.

