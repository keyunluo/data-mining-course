A Convergence Rate Analysis for LogitBoost, MART and Their Variant

Peng Sun1
Tong Zhang2
Jie Zhou1

SUNP 08@ MAILS . TSINGHUA . EDU . CN
TZHANG @ STAT. RUTGERS . EDU
JZHOU @ TSINGHUA . EDU . CN

1

Tsinghua National Laboratory for Information Science and Technology(TNList), Department of Automation, Tsinghua
University, Beijing 100084, China
2
Baidu Inc., Beijing, China and Department of Statistics, Rutgers University, NJ, USA

Abstract
LogitBoost, MART and their variant can be
viewed as additive tree regression using logistic loss and boosting style optimization. We analyze their convergence rates based on a new
weak learnability formulation. We show that it
has O( T1 ) rate when using gradient descent only, while a linear rate is achieved when using
Newton descent. Moreover, introducing Newton
descent when growing the trees, as LogitBoost
does, leads to a faster linear rate. Empirical results on UCI datasets support our analysis.

1. Introduction
Boosting is a successful machine learning algorithm for binary classification. After the introduction of the first practical boosting method – AdaBoost (Freund & Schapire,
1995), many variations have been proposed. It was shown
boosting can be regarded as gradient descent in function space (Mason et al., 1999). Along this line, Friedman (2001)
equipped boosting with tree base learner and formulated it
as Multiple Additive Regression Tree (MART), where at
each iteration a tree is added to fit the gradient using least
squares. Plugging the logistic loss for binary classification,
MART becomes a concrete classification method described
in (Friedman, 2001). The procedure differs from the one in
(Friedman et al., 2000) where Hessian was employed in the
so-called LogitBoost procedure, leading to Newton steps
instead of gradient descent.
One natural question is whether it Is beneficial to introduce
Hessian instead of using only gradient. In (Friedman et al.,
2000; Friedman, 2001), where the Hessian was firstly introduced into the Boosting framework, this question was
not answered. Recently, Li (2009b; 2010a); Saberian et al.
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

Table 1. LogitBoost and its variants

GBoost
MART
LogitBoost

Gain for Tree Growth

Leaf Value

gradient (12)
gradient (12)
Newton (16)

gradient (11)
Newton (15)
Newton (15)

(2011) presented empirical results showing that using Hessian does lead to lower test errors; furthermore, Li (2009a;
2010b) presented empirical results that LogitBoost can decrease training loss faster than MART. However, a convergence rate analysis was absent in previous studies. In this
paper, we attempt to fill this gap by theoretically showing
how fast the training loss decreases, with or without the
Hessian.
Looking inside the tree fitting process, we observe that gradient/Hessian descent can be used either when growing the
trees or when fitting the leaf values. The combinations give
rise to LogitBoost and its two variants (GBoost and MART)
as shown in Table 1. MART and LogitBoost have already
seen many successful real world applications (e.g., Web
Page Ranking (Burges, 2010), Click-Through Rate estimation for web advertisements (Trofimov et al., 2012), Optical
Digits Recognition (Shalev-Shwartz et al., 2011)). GBoost
is a different procedure considered in this paper, which can
serve as a baseline for the purpose of theoretical analysis.
Our main convergence results are: 1) GBoost only achieves
O( T1 ) rate; 2) Both MART and LogitBoost achieve linear
rates; 3) LogitBoost has a faster linear rate than MART.
It is worth mentioning that there are two main difficulties
of the analysis. 1) The logistic loss is not strongly convex.
2) The regression tree is hard to deal with. Our analysis
begins with a one-instance toy example (Section 3), which
could be seen as a technical device to be utilized for the
analysis of other Boosting algorithms. We then generalize the simple analysis to the many-instance case by viewing the tree growth process as block coordinate descent. In

LogitBoost Convergence Rate

this work we assume that the weak learnability assumption
holds, and introduce a novel weak learnability formulation
(i.e., lemma 8) that is more appropriate for the analysis of
LogitBoost than previous formulations.
1.1. Related Work
Convergence Rate. Due to the special property of exponential loss, it is well known that AdaBoost has linear convergence rate (Schapire & Freund, 2012). For logistic loss,
(Bickel et al., 2006) showed a sub-linear rate. Very recently, (Telgarsky, 2012; 2013) proved linear convergence for
boosting with logistic loss.
The linear convergence rate has also been established
for boosting with strongly convex losses in the literature (Rätsch et al., 2001; Grubb & Bagnell, 2011). Unfortunately logistic loss is not strongly convex. To address this
difficulty, Telgarsky (2012) proposed a technique to compare the gradient of the loss and its Hessian. Inspired by
this idea, a similar technique tailored to Newton descent is
presented in this paper.
Weak Learnability. We employ a weak learnability assumption in this work1 . In the AdaBoost literature (Schapire &
Freund, 2012), the weak learnability is formulated with respect to weighted training error. In gradient boosting (Mason et al., 1999), (Grubb & Bagnell, 2011) connects the
weighted error formulation to the gradient least square fitting described in (Friedman et al., 2000; Friedman, 2001).
A similar weak learnability formulation for gradient fitting
was used in (Telgarsky, 2012; 2013). However, the weak
learnability assumptions in earlier work involved only gradient, making them unsuitable for our Hessian based Newton descent analysis.
Results most related to this paper are those of (Telgarsky,
2012; 2013). However, there are important differences: 1)
The linear rate in (Telgarsky, 2012; 2013) relies on ad hoc
step size selection which excludes the Newton descent; in
particular, the analysis there doesn’t cover MART and LogitBoost. In this work, we directly study Newton descent
that is adopted by MART and LogitBoost. 2) The weak
learnability assumption in (Telgarsky, 2012; 2013) is defined on gradient only, which is unsuitable to the analysis
of Newton descent in the sense that it cannot reflect the convergence rate observed in our experiments. In contrast, we
1
This assumption seems very strong. However, it is realistic
in many real world applications. It is shown that weak learnability is equivalent to linear separability over the base learners (see
a recent survey (Shalev-Shwartz & Singer, 2010) for this issue),
which indeed covers many classification tasks. Fox example, in
Optical Character Recognition the data with different labels cannot overlap(say, an image cannot be both ’4’ and ’9’), thus the
raw image data can be linearly separated over trees that are deep
enough.

propose a new weak learnability assumption that is more
appropriate for Newton descent analysis, and the assumption is justified both theoretically and empirically.
The rest of this paper is organized as follows. In Section
2 we review the problem setup. In Section 3 we introduce
the main techniques via a toy example. Section 4 gives the
formal analysis. Finally, in Section 5 we present empirical
results on UCI datasets to support our claims.

2. Review of LogitBoost and Its Variants
In this section we review the problem setup posed in (Friedman et al., 2000), including three key ingredients: the loss,
the function model and the optimization method.
The definition of Logistic loss. Given a set of training data {xi , yi }N
i=1 where the feature xi ∈ X and the label
yi ∈ {±1}, a classifier learns a predictor function F =
F (x) ∈ R by minimizing the total loss over the training
PN
dataset i=1 `(yi , F (xi )). In (Friedman et al., 2000), the
instance wise loss `(·, ·) adopts the so-called logistic loss:


 
1
1
+ (1 − r) log
, (1)
`(y, F ) = r log
p
1−p
where the 0/1 response r = 1 if y = +1, r = 0 otherwise;
and the probability estimate p ∈ [0, 1] is a “surrogate” of F
via the so-called link function:
p = ψ(F ) =

eF
,
eF + e−F

(2)

which is a sigmoid-like function on F .
To carry out the numerical optimization, we need the gradient and the Hessian of `(·, F ) w.r.t. F . Simple calculus
gives them as:
g(y, F ) , ∇F `(y, F ) = 2(p − r),

(3)

h(F ) , ∇2F `(y, F ) = 4p(1 − p),

(4)

by noting the derivative of the link function is ψ 0 (F ) =
2p(1 − p).
Note that the above formulae are all defined on the implicit
variables r and p, rather than defined directly on y and F .
This choice turns out to be very convenient for our later
development.
The Additive Tree Model. F (x) is expressed as sum of rePT
gression trees: F (x) =
t=1 ut (x). Omitting the subscript t, a regression tree u(x) with J leaves can be written
PJ
as u(x) = j=1 sj I(x ∈ Rj ), where sj ∈ R is the fitted value on the j-th leaf, Rj is the region on feature space
corresponding to the j-th leaf and the indicator function

LogitBoost Convergence Rate

I(·) = 1 if its argument is true and 0 otherwise. In the
AdaBoost literature, regression tree is a domain-partition
based weak learner with confidence-rated output (Schapire
& Singer, 1999).
The Greedy Stage Wise Optimization. At each boosting iteration only one tree is added and the other trees previously learned are fixed. What’s special here is that when
updating F = F (x) a real value ν ∈ (0, 1] is multiplied:
F (x) ← F (x) + νu(x). The fixed step size ν, designated
in advance as a tuning parameter, is called shrinkage factor controlling the learning rate (Friedman et al., 2000). In
later literature, it is found to have the effect of L1 regularization (Rosset et al., 2004). In several up-to-date implementations, this choice leads to satisfactory classification
performance, e.g., in (Li, 2010a) a stable and good result
is observed by always setting ν = 0.1. The corresponding
pseudo-code in given in Algorithm 1.
Algorithm 1 LogitBoost, MART and Variant.
input {xi , yi }N
i=1 : the training set; ν: the shrinkage factor;
T : the maximum number of iterations.
output the Additive Tree Model F = F (x).
1: Fi = 0, i = 1, . . . , N
2: for t = 1 to T do
3:
pi = ψ(Fi ) as in Eq (2) and clap on pi as in (17),
i = 1, . . . , N .
4:
Grow a tree with domain partition {Rj }Jj=1 .
5:
Fit a value sj for the j-th leaf, j = 1, . . . , J.
PJ
6:
Fi ← Fi + ν j=1 sj I(xi ∈ Rj ), i = 1, . . . , N .
7: end for

2.1. Tree Growth and Leaf Value Fitting
In Algorithm 1, it needs to specify how to grow a tree
(line 4) and how to fit the leaf value (line 5), giving
rise to the difference among GBoost, MART and LogitBoost. We provide greater details here. At some
iteration, denote by u = (u1 , . . . , uN )> ∈ RN the fitted
leaf values
PN over the N instances. Define the total loss
L(u) , i=1 `(yi , Fi +ui ). Consider its tractable quadratb
b
ic approximation at 0: L(u)
=L(0)
+ g > u + 12 u> Hu,
P
N
b
where L(0)
=L(0) = i=1 `(yi , Fi ) and the instance wise
gradient and Hessian
g = (g1 , . . . , gN )> (5), H = diag(h1 , . . . , hN ) (6)
are respectively calculated by (3) and (4) with p replaced
by pi at each instance i. Since typically J < N , each
component of u cannot vary independently: the instances falling into the same leaf must share a common
fitted value. We can thus write down u = V s, where
RJ 3 s = (s1 , . . . , sj , . . . , sJ )> are the fitted values on

the J leaves and V ∈ RN ×J is a projection matrix:
V = [v 1 , . . . , v j , . . . , v J ],

(7)

where v j,i = 1 if the j-th leaf contains the i-th instance
and 0 otherwise, j = 1, . . . , J, i = 1, . . . , N . Substituting
b
u = V s back to the quadratic L(u)
and reload the notation
J
b
L(·) for s ∈ R we have
1
b
b
L(s)
= L(0)
+ (g > V )s + s> (V > HV )s. (8)
2
To this extent, the leaf value can be defined as the minib
mizer s∗ = arg mins L(s),
while the total split gain2 for
growing a tree can be defined
as the maximum
loss reduc

∗
b
b
tion gain(s ) = maxs L(0) − L(s) , both boiling down
to minimizing (8). There are two methods.
Gradient Descent. Because the Hessian (4) is bounded
above by h ≤ 1, we can replace H with the upper bound
I, i.e., the identity matrix. Solving the consequent quadratic problem using matrix calculus (Magnus & Neudecker,
2007), we have:
s∗ = −(V > V )−1 V > g,
∗

gain(s ) = kV

s∗ k22

>

(9)
>

>

−1

= (V g) (V V )

>

(V g).
(10)

Note that V > V = diag(n1 , . . . , nj , . . . , nJ ) ∈ RJ×J
where nj denotes the number of instances falling into the
j-th leaf. Moreover, (9) and (10) can be rewritten in the
“scalar form”:
s∗j = −

gj
nj

gain(s∗ ) =

(11)

J
X
g 2j
n
j=1 j

(12)

P
where g j , i∈Ij gi should be understood as “node wise
gradient”. The geometrical meaning is straightforward: in
the subspace span(V ) we find a vector u = V s ∈ RN
that is as close to −g as possible, which in (Friedman,
2001) is described as fitting the negative gradient {−gi }N
i=1
using a least squares regression tree.
Newton Descent. Directly solving (8) we have
s∗ = −(V > HV )−1 V > g,
∗

gain(s ) = kV

s∗ k2H

>

>

(13)
>

−1

= (V g) (V HV )

>

(V g),
(14)

which can be rewritten in “scalar form” as:
s∗j

gj
=− ,
~j

(15)

∗

gain(s ) =

J
X
g 2j
j=1

,

(16)

~j

2
While it is difficult to directly maximize the total gain, (Friedman et al., 2000) suggests to grow a binary tree top-down until
J leaves are encountered, maximizing a binary split gain once a
time.

LogitBoost Convergence Rate

P
where ~j , i∈Ij hi should be understood as “node wise
Hessian”. The geometrical meaning is similar to the case
of gradient descent, but the Euclidean norm should be
replaced by matrix norm (the Hessian matrix H) when
talking about “close”. This is described in (Friedman
et al., 2000) as fitting the Newton step {−gi /hi }N
i=1 with
weights {hi }N
i=1 using a weighted least square regression
tree.
2.2. Clapping on pi
Normally (15) is numerically stable. When pi → ri for all
i ∈ Ij , it can be verified that sj → 12 although both g j → 0
and ~j → 0. However, (15) is still occasionally very large
when only ~j → 0, causing numerical problems. To tackle
this issue, (Friedman et al., 2000) suggests the Newton step
(15) to be clapped in a range, say, between [−5, +5]. In
this work, however, we implement the idea in a slightly
different way. Instead of directly clapping the Newton step
(15), we clap the pi , i = 1, . . . , N :


1 − ρ, (ri = 0) ∧ (pi > 1 − ρ)
(17)
pi = ρ, (ri = 1) ∧ (pi < ρ)


pi , otherwise

Loss

Loss

0

0

10

10

−1

−5

10

10

−2

−10

10

10

−3

−15

10

10

0

100

200

300

0

100

200

300

Figure 1. How the loss decreases with iteration for the one instance toy example. The vertical axis is in log scale. Left: Gradient Descent; Right: Newton Descent.

1: F = 0, p = 1/2.
2: for t = 1 to T do
3:
Update: F ← F + f .
4:
Compute the implicit variable via (2): p = ψ(F ).
5:
Compute the loss at iteration t: Lt = − log(p).
6: end for

Obviously, the optimal loss value, 0, is achieved when F =
+∞. The step f in line 3 can be either gradient descent or
Newton descent times a shrinkage factor. Now let’s take a
closer look.

for some small constant ρ in line 4 of Algorithm 1. From
(1), we observe that the clapping (17) is to prevent large
penalty of wrong probability estimate (or negative margin
in regards of F ∈ R). In this way, we can show that the
Newton step is finite as in the theorem below. The proof is
provided in the supplement. Although motivated by making the Newton step numerically stable, we also apply the
clapping on pi in the case of gradient descent.

Gradient Descent. The step f = −νg, where g = 2(p − 1)
as in (3). With ν = 0.1, the result of how Lt decreases with
iteration t is shown on the left of Figure 1.

Theorem 1 (Bounded Newton Step). With the value clapping on pi , i = 1, . . . , N as in (17), the Newton step (15)
is bounded such that g j /~j  ≤ 1/(2ρ).

From Figure 1, we can clearly see that the gradient descent
shows a sub-linear convergence rate consistent with O( T1 ),
while the Newton descent shows a linear rate, i.e., O(e−T ).
Next we will provide a quick theoretic explanation for this
phenomenon.

3. One-Instance Toy Example

Newton Descent. The step f = −ν hg , where g = 2(p − 1),
h = 4p(1 − p) as in (3) and (4), respectively. With ν =
0.1, the result of how Lt decreases is shown on the right of
Figure 1.

Having introduced the problem setup, we are ready for the
analysis. As will be seen shortly in Section 4, the convergence rate heavily depends on how the leave values are
fitted, i.e., whether the gradient (11) or the Newton (15) is
adopted. In this section we present the main results and the
underlying proof techniques via a toy example.

3.2. Theoretical Analysis

3.1. Algorithm 1 with One Instance

The first part is vital to our analysis. We begin with several
related theorems.

Suppose there is only one training instance with label, say,
y = +1. In this case it is unnecessary to grow a tree in
Algorithm 1, and only the leaf value fitting (line 5) matters.
Consequently, Algorithm 1 boils down to performing some
numerical descent method on the function − log(p), as in
the following pseudo code:

We follow the standard techniques in convex optimization
to derive the desired bound. First, we bound the one step
loss reduction; then we derive the recurrence relation from
iteration t − 1 to iteration t; finally we obtain the convergence rate in t from the recurrence relation.

Theorem 2 (Smoothness). Let `(·) be the shorthand of
`(y, ·) and g be the shorthand of gradient (3). Then `(·)
is 1-smooth and the one step loss reduction satisfies
1
`(F + f ) ≤ `(F ) + gf + f 2 .
2

(18)

LogitBoost Convergence Rate
2

|g|
L

2

6

1

4

0.5

2

0.2

0.4

0.6

0.8

g /h
L

8

1.5

0

for theorem 6 is obvious and we omit it here. Next we are
ready to analyze the toy example.

10

2.5

1

0

3.2.1. G RADIENT D ESCENT
Denote the loss at current iteration by L and that at next
iteration by L+ . From Theorem 2, we have
0.2

0.4

0.6

0.8

1

Figure 2. Left: |g| v.s. ` as in Theorem 3 when the label y = +1;
2
Right: gh v.s. ` as in Theorem 4. In both graphs, the horizontal
axis is p.

We omit the proof of Theorem 2, which follows from the
standard argument in convex optimization textbooks, e.g.,
(Boyd & Vandenberghe, 2004) and the fact that the Hessian
in (4) is upper-bounded by h ≤ 1.
Theorem 3 (Comparison I). Let g and ` be the shorthand
for gradient (3) and loss (1), respectively. When p ≥ ρ in
the case of y = +1 (or p ≤ 1 − ρ in the case of y = −1),
there exists a constant α = α(ρ) > 0 such that |g| ≥ α`.
Theorem 4 (Comparison II). Let g, h and ` be the shorthand for gradient (3), Hessian (4) and loss (1), respective2
ly. Then ∀p (and thus ∀F ), the inequality gh ≥ ` always
holds.
See the supplement for the proofs of the above two theorems. The loss family with the properties |g| ≥ aL and
h ≤ bL for some constants a, b was studied in (Telgarsky,
2012). This includes but not restricted to the logistic loss
2
2
(1). The properties imply the inequality gh ≥ ab L, which
is similar to theorem 4. Obviously, the results in (Telgarsky, 2012) are more general, while Theorem 4 in this work
is specific to the logistic loss (1). As will be seen shortly,
Theorem 4 is useful in the analysis of Newton step.
In the left of Figure 2 we demonstrate Theorem 3 with α =
1 (corresponding to ρ ≈ 0.2) when y = +1; While in the
right of Figure 2 we demonstrate Theorem 4 when y = +1.
The following two theorems establish convergence rates
from the corresponding recurrence relationships.
Theorem 5 (Recurrence to O( T1 ) rate). If a sequence 0 ≥
. . . ≥ t−1 ≥ t ≥ . . . ≥ T > 0 has the recurrence
relation t ≤ t−1 − c2t−1 for a small constant c > 0, then
0
.
we have O( T1 ) convergence rate: T ≤ 1+c
0T
Theorem 6 (Recurrence to linear rate). If a sequence 0 ≥
. . . ≥ t−1 ≥ t ≥ . . . ≥ T > 0 has the recurrence
relation t ≤ ct−1 for a small constant 0 < c < 1, then
we have linear convergence: T ≤ 0 cT .
See the supplement for the proof of theorem 5. The proof

1
L+ ≤ L + gf + f 2 .
(19)
2
Substituting the gradient descent times a shrinkage factor
f = −νg in the right hand side, we have:
1
(20)
L+ ≤ L − ν(1 − ν)g 2 .
2
Noting the initial value F = 0, we apply Theorem 3 with
α = 1 and get:
1
L+ ≤ L − ν(1 − ν)L2 .
(21)
2
This leads to a recurrence relationship from iteration t − 1
to t. Setting a proper ν such that ν(1 − 21 ν) > 0, (21)
implies a O( T1 ) convergence rate due to Theorem 5.
Remark. If the loss is m-strongly convex, we can show
g 2 ≥ (2m)L, leading to a linear convergence rate, which
well known in convex optimization, e.g., (Boyd & Vandenberghe, 2004; Nesterov, 2004). However, this argument cannot be applied to logistic loss because the minimum
of the Hessian (4) is 0, implying that the strong convexity
doesn’t hold.
3.2.2. N EWTON D ESCENT
For f ≥ 0, we use the Mean Value Theorem in calculus
and have
1
L+ = L + gf + hξ f 2 ,
(22)
2
ξ ∈ [0, f ] and hξ denotes the Hessian at ξ. Substituting the
Newton Step times a shrinkage factor f = −ν hg ≥ 0, we
have
g2
1 hξ g 2
L+ = L − ν + ν 2
.
(23)
h
2 h h
Since the initial value F = 0 and f ≥ ξ ≥ 0, it holds that
hξ
≤1
(24)
h
by checking the form of the Hessian (4) and the form of the
2
link (2). Noticing that gh ≥ 0, the inequality simplifies to
1 g2
L+ ≤ L − ν(1 − ν) .
(25)
2 h
By applying Theorem 4, we obtain the recurrence relation
1
L+ ≤ L − ν(1 − ν)L
2
1
= (1 − ν(1 − ν))L.
2

(26)

LogitBoost Convergence Rate

By setting a proper value for ν such that 0 < (1 − ν(1 −
1
2 ν)) < 1, we get the linear convergence rate according to
Theorem 6.
Remark. If the loss is strong convex and the Hessian is
strictly Lipschitz continuous, we can show that the Newton
Step leads to quadratic convergence rate, which is again a standard result in convex optimization textbook, e.g., (Boyd
& Vandenberghe, 2004; Nesterov, 2004).
We make the following remarks concerning the oneinstance example of this section. 1) The toy example becomes realistic if we allow a very large J such that at each
leaf there is only one training instance. In general, when
J < N we intuitively cannot expect a faster convergence
rate than what is established here. 2) The proof for the toy
example can be adapted to handle the general case, as we
shall illustrate in the next section.

4. The General Analysis
In this section we provide the full analysis for N instances
and J leaves. Basically, we extend the one-instance analysis to the one-leaf analysis and sum up for the totally J
leaves using matrix notation. The analysis in Section 4.1
covers GBoost, while that in Section 4.2 covers MART and
LogitBoost.
4.1. Gradient Descent
In Section 3.2.1 we have established convergence rates for
the toy example, and the key step is the application of Theorem 3 to obtain (21) from (20). To do the same in the J-leaf
case, we need to convert “node wise gradient” to “instance
wise gradient”. This idea is captured by Definition 1 and
Lemma 7.
Definition 1 (Weak Learnability). For a set of N instances with labels {±1}N and non-negative weights
{w1 , . . . , wN }, there exists a J-leaf classification tree (i.e.,
outputting ±1 at each leaf) such that the weighted error rate
at each leaf is strictly less than 12 by at least δ > 0.
This definition is the weak learnability assumption in the
AdaBoost literature (Schapire & Freund, 2012). The J-leaf
tree can be seen as a so-called domain-partition weak learner (Schapire & Singer, 1999), where each leaf, an atomic
domain with output value ±1, can be regarded as a weak
classifier. Definition 1 assumes that each leaf classifier is
better than random guessing by δ > 0.
Lemma 7 (Weak Learnability I). Assume that the weak
learnability assumption as in Definition 1 holds, then for
any gradient g ∈ RN as defined in (5) and gi 6= 0,
i = 1, . . . , N , there exists a J-leaf regression tree whose
projection matrix V ∈ RN ×J satisfies
(V > g)> (V > V )−1 (V > g) ≥ γ 2 g > g

(27)

for some constant γ > 0.
See the supplement for its proof. Recall the gain (10) and
its geometric meaning, Lemma 7 says that we can always
find a subspace span(V ) such that the projected g has big
enough norm compared with the original g. The proof of
Lemma 7 was due to (Grubb & Bagnell, 2011) which contains a very similar result. In the following we will use the
second part of Lemma 7 to derive our bounds.
At some iteration, consider the stepsize vector f =
(f1 , . . . , fN )> ∈ RN , and denote the total current loss
PN
by L , i=1 `(yi , Fi ) and the total loss at next iteration
PN
by L+ , i=1 `(yi , Fi + fi ). Using Theorem 2, we can
bound the loss reduction as
1
(28)
L+ ≤ L + g > f + f > f .
2
Replacing f by the gradient descent in (9) times the shrinkage factor ν:
f = −νV s = −νV (V > V )−1 g,

(29)

we have:
L+ ≤ L − ν(V > g)> (V > V )−1 (V > g)
1
+ ν 2 (V > g)> (V > V )−1 (V > V )(V > V )−1 (V > g).
2
(30)
Noting that V > V is invertible
(V > V )> (V > V )−1 , we obtain

and

eliminating

1
L+ ≤ L − ν(1 − ν)(V > g)> (V > V )−1 (V > g). (31)
2
With the help of Lemma 7, it yields
1
L+ ≤ L − ν(1 − ν)γ 2 kgk22 .
2

(32)

1
2
N kgk1

and multiply

Recall the norm inequality kgk22 ≥
both sides by N1 , we obtain
L+
L
1
≤
− ν(1 − ν)γ 2
N
N
2



kgk1
N

2
.

(33)

Recall the clapping operation (17) and Theorem 3, we can
compare kgk1 to L as
kgk1 ≥ αL

(34)

for some α = α(ρ). Substituting back, we obtain the recurrence relation:
 2
L+
L
1
L
≤
− ν(1 − ν)γ 2 α2
.
(35)
N
N
2
N
+

L
Viewing LN as t , N
as t−1 and setting proper ν so that
1
2 2
ν(1 − 2 ν)γ α > 0, we establish the O( T1 ) convergence
rate with the help of Theorem 5.

LogitBoost Convergence Rate
3

4.2. Newton Descent

We are now ready to present lemmas needed for our subsequent analysis.
Lemma 8 (Weak Learnability II). Let g ∈ RN (gi 6= 0, i =
1, . . . , N ) be defined in (5) and H ∈ RN ×N be defined in
(6), where pi is clapped as in (17). If the weak learnability
assumption 1 holds, then there exists a J-leaf regression
tree whose projection matrix V ∈ RN ×J satisfies
(36)

for some constant γ∗ > 0.
See the supplement for its proof. Lemma 8 is a new weak
learnability result that can be used to analyze Newton descent. It says that with the same weak learnability assumption in Lemma 7, we can find a subspace span(V ) such
that the projection of g in the subspace has a sufficiently
large matrix norm induced by the Hessian matrix H. The
geometric meaning is given in (14).
Lemma 9 (Change of Hessian). For the current F ∈ R
and a step f ∈ R, the change of the Hessian (4) can be
bounded in terms of step size |f |:
h(F + f )
≤ e2|f | .
h(F )

(37)

We are now ready to derive our general results similar to
what we have shown in Section 3.2.2. We define L+ , L,
and f with the same meanings of the corresponding quantities in Section 4.1. Using second order Taylor expansion,
we obtain
(38)

where ξ ∈ RN such that each of its component ξi lies within Fi and Fi + fi (i = 1, . . . , N ), and H ξ is a shorthand of
the Hessian matrix at ξ. Replacing f with the Newton step
(13) times a shrinkage factor ν:
f = −νV s = −νV (V > HV )−1 V > g,

2

0

10

10

1

−5

10

10

0

−10

10

0

200

400

600

800

1000

10

0

200

400

600

800

1000

Figure 3. A typical convergence pattern on UCI dataset #3. The
horizontal axis indicates iterations, and the vertical axis indicates
logistic loss on training data in log scale. Left: GBoost; Right:
MART and LogitBoost.

we have:
L+ = L − ν(V > g)> (V > HV )−1 V > g
1
+ ν 2 (V > g)> (V > HV )−1 (V > H ξ V )(V > HV )−1 V > g,
2
(40)
where we can bound the term (V > H ξ V )(V > HV )−1 using Theorem 9 as follows.
Corollary 10 (Node wise Hessian). Assume that the Newton step (39) is used with the value clapping (17). Then
(V > H ξ V )(V > HV )−1 ≤ µI, where µ = exp(ν/ρ).
The proof is given in the supplement. For now we can apply
Corollary 10 to obtain the inequality
1
L+ ≤ L − ν(1 − νµ)(V > g)> (V > HV )(V > g). (41)
2
Using Lemma 8, we convert the node wise Newton decrement to instance wise Newton decrement in the sense that
(V > g)> (V > HV )(V > g) ≤ γ∗2 g > H −1 g.

(42)

Substituting back, the inequality simplifies as:

See the supplement for its proof. The role of Lemma 9
is similar to that of equation (24) in the toy example of
Section 3.2.2.

1
L+ = L + g > f + f > H ξ f ,
2

10

LogitBoost
MART

In Section 3.2.2 we have established convergence rate for
the toy example by using the following two ideas:
• The application of the comparison theorem 4 so as to
transit from (25) to (26). To do the same in the J-leaf N instance case, we need to convert the “node wise Newton
Decrement” to “instance wise Newton Decrement”. This
idea is captured by Lemma 8 below.
h
• The upper bound of the term hξ given by (24). In the
general J-leaf N -instance case, a similar upper bound is
available as in Lemma 9 and Corollary 10 below.

(V > g)> (V > HV )−1 (V > g) ≥ γ∗2 g > H −1 g,

5

10

(39)

1
L+ ≤ L − ν(1 − νµ)γ∗2 g > H −1 g.
2

(43)

Finally, we connect Newton objective reduction and loss
reduction using Theorem 4 as follows
g > H −1 g ≥ L,

(44)

which, together with (43), yields the recurrence relation:
1
L+ ≤ (1 − ν(1 − νµ)γ∗2 )L.
2

(45)

With proper step size ν ensuring 0 < ν(1 − 21 νµ)γ∗2 < 1,
we can establish the desired linear convergence rate from
Theorem 6.
Remark 1. Our analysis for Newton descent is based on the
new weak learnability result in Lemma 8. From (45), the

LogitBoost Convergence Rate

quality of our bound relies on the constant γ∗ defined in
(36): the bigger the γ∗ , the faster the convergence. It is
worth mentioning that the bound in the related work (Telgarsky, 2012) is similar to (27), a choice that is unsuitable
for Newton descent. We confirm this assertion by experiments in Section 5, where we show that large γ doesn’t
lead to faster convergence while large γ∗ does.
Remark 2. According to the above discussion, we should
prefer LogitBoost to MART in order to achieve faster convergence. This is because when growing a tree, MART
does not search for good γ∗ .

5. Experiments
In this section we present empirical results on five binary
datasets: #1: optdigits05, #2: pendigits49, #3: zipcode38,
#4: letter01, #5: mnist10k05. These are synthesized from
the corresponding UCI datasets; e.g., “optdigits05” means
we pick class 0 and class 5 from the multiclass datasets
“optdigits”. In all of the following experiments we set ν =
0.1, J = 8 and the clapping ρ = 0.05.
Convergence rate We note that the empirical results of
(Li, 2009a; 2010b) have already demonstrated that the
training loss of LogitBoost decreases faster than that of
MART, e.g., Figure 2 in (Li, 2010b). For completeness, we
perform a similar experiment here, focusing on binary classification and adding GBoost in our comparisons. In the
supplement, we plot the convergence for GBoost, MART
and LogitBoost on datasets #1 to #5. Figure 3 shows the
typical convergence pattern. As in our analysis, GBoost
(i.e., the gradient descent) shows a sub-linear convergence
rate of O( T1 ), while MART and LogitBoost (i.e., the Newton descent) show linear rates.

Table 2. The weak learnability for LogitBoost and MART
MART
#1
#2
#3
#4
#5

LogitBoost

γ∗

γ

T0

γ∗

γ

T0

0.565
0.157
0.056
0.078
0.042

0.317
0.047
0.047
0.029
0.037

217
500
865
518
1043

0.817
0.466
0.219
0.263
0.203

0.400
0.034
0.039
0.027
0.021

206
269
568
345
582

row in Table 2 and the right graph in Figure 3). This shows
that the analysis of Newton descent should depend on γ∗
instead of γ.
Acknowledgments The authors would like to thank the
reviewers for their helpful comments that improved the
paper. This work is supported by the National Natural
Science Foundation of China under Grants 61225008 and
61020106004, the National Basic Research Program of
China under Grant 2014CB349304, the Ministry of Education of China under Grant 20120002110033, and the Tsinghua University Initiative Scientific Research Program.

References
Bickel, Peter J, Ritov, Ya’acov, and Zakai, Alon. Some
theory for generalized boosting algorithms. The Journal
of Machine Learning Research, 7:705–732, 2006.
Boyd, Stephen Poythress and Vandenberghe, Lieven. Convex optimization. Cambridge university press, 2004.
Burges, Chris. From ranknet to lambdarank to lambdamart: An overview. Microsoft Research Technical Report MSR-TR-2010-82, 2010.

Weak Learnability and Faster Linear Rate In this work,
our analysis for Newton descent relies on the constant γ∗ defined in (36): a bigger γ∗ indicates faster convergence. To study this issue empirically, we look at the ra>
>
>
HV )−1 (V > g)
tio (V g) (V
over all iterations and pick the
g > H −1 g
minimum value as γ∗ . Similarly, we can set γ defined in
>
>
>
−1
>
(36) to be the minimum value of (V g) (Vg>Vg ) (V g) —
this choice is employed by (Telgarsky, 2012) where the resulting bound contains a quantity being similar to γ. Table
2 lists the values γ∗ and γ for the two algorithms LogitBoost and MART, and it gives the number of iterations T 0
needed to achieve 10−6 accuracy for logistic loss.

Freund, Y. and Schapire, R. A desicion-theoretic generalization of on-line learning and an application to boosting. In Computational learning theory, pp. 23–37.
Springer, 1995.

As can be seen in Table 2 and Figure 3, LogitBoost converges faster than MART. Checking Table 1, we also find
that LogitBoost has bigger γ∗ than MART. However, the
faster convergence of LogitBoost cannot be captured by γ.
For example, on dataset #3 LogitBoost has a smaller γ value than that of MART while still converges faster (the third

Grubb, Alexander and Bagnell, Drew. Generalized boosting algorithms for convex optimization. In Proceedings
of the 28th International Conference on Machine Learning (ICML-11), pp. 1209–1216, 2011.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The
annals of statistics, 28(2):337–407, 2000.
Friedman, Jerome H. Greedy function approximation: a
gradient boosting machine. Annals of Statistics, pp.
1189–1232, 2001.

Li, Ping. Abc-boost: Adaptive base class boost for multiclass classification. In Proceedings of the 26th Annu-

LogitBoost Convergence Rate

al International Conference on Machine Learning, pp.
625–632. ACM, 2009a.
Li, Ping. Abc-logitboost for multi-class classification. arXiv preprint arXiv:0908.4144, 2009b.
Li, Ping. Robust logitboost and adaptive base class (abc)
logitboost. In Uncertainty in Artificial Intelligence,
2010a.
Li, Ping. An empirical evaluation of four algorithms for multi-class classification: Mart, abc-mart, robust
logitboost, and abc-logitboost. arXiv preprint arXiv:1001.1020, 2010b.
Magnus, Jan R and Neudecker, Heinz. Matrix differential
calculus with applications in statistics and econometrics. John Wiley & Sons, 3rd edition, 2007.
Mason, Llew, Baxter, Jonathan, Bartlett, Peter, and Frean, Marcus. Boosting algorithms as gradient descent in
function space. In Advances in Neural Information Processing Systems. NIPS, 1999.
Nesterov, Yurii. Introductory lectures on convex optimization: A basic course, volume 87. Springer, 2004.
Rätsch, Gunnar, Mika, Sebastian, and Warmuth, Manfred K. On the convergence of leveraging. In Advances
in Neural Information Processing Systems, pp. 487–494,
2001.
Rosset, Saharon, Zhu, Ji, and Hastie, Trevor. Boosting
as a regularized path to a maximum margin classifier.
The Journal of Machine Learning Research, 5:941–973,
2004.
Saberian, Mohammad J, Masnadi-Shirazi, Hamed, and
Vasconcelos, Nuno. Taylorboost: First and second-order
boosting algorithms with explicit margin control. In
Computer Vision and Pattern Recognition (CVPR), 2011
IEEE Conference on, pp. 2929–2934. IEEE, 2011.
Schapire, R. and Singer, Y. Improved boosting algorithms
using confidence-rated predictions. Machine learning,
37(3):297–336, 1999.
Schapire, Robert E and Freund, Yoav. Boosting: Foundations and Algorithms. The MIT Press, 2012.
Shalev-Shwartz, Shai and Singer, Yoram. On the equivalence of weak learnability and linear separability: New
relaxations and efficient boosting algorithms. Machine
learning, 80(2-3):141–163, 2010.
Shalev-Shwartz, Shai, Wexler, Yonatan, and Shashua, Amnon. Shareboost: Efficient multiclass learning with feature sharing. In NIPS, 2011.

Telgarsky, Matus. A primal-dual convergence analysis of
boosting. The Journal of Machine Learning Research,
13:561–606, 2012.
Telgarsky, Matus. Margins, shrinkage, and boosting. In
Proceedings of the 30th International Conference on
Machine Learning (ICML2013), pp. 307–315, 2013.
Trofimov, Ilya, Kornetova, Anna, and Topinskiy, Valery.
Using boosted trees for click-through rate prediction for
sponsored search. In Proceedings of the Sixth International Workshop on Data Mining for Online Advertising
and Internet Economy, pp. 2. ACM, 2012.

