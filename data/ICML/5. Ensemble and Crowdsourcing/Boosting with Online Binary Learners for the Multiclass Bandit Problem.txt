Boosting with Online Binary Learners for the Multiclass Bandit Problem

Shang-Tse Chen
School of Computer Science, Georgia Institute of Technology, Atlanta, GA

SCHEN 351@ GATECH . EDU

Hsuan-Tien Lin
Department of Computer Science and Information Engineering
National Taiwan University, Taipei, Taiwan

HTLIN @ CSIE . NTU . EDU . TW

Chi-Jen Lu
Institute of Information Science, Academia Sinica, Taipei, Taiwan

CJLU @ IIS . SINICA . EDU . TW

Abstract
We consider the problem of online multiclass
prediction in the bandit setting. Compared with
the full-information setting, in which the learner
can receive the true label as feedback after making each prediction, the bandit setting assumes
that the learner can only know the correctness of
the predicted label. Because the bandit setting
is more restricted, it is difficult to design good
bandit learners and currently there are not many
bandit learners. In this paper, we propose an approach that systematically converts existing online binary classifiers to promising bandit learners with strong theoretical guarantee. The approach matches the idea of boosting, which has
been shown to be powerful for batch learning
as well as online learning. In particular, we establish the weak-learning condition on the online
binary classifier, and show that the condition allows automatically constructing a bandit learner
with arbitrary strength by combining several of
those classifiers. Experimental results on several real-world data sets demonstrate the effectiveness of the proposed approach.

1. Introduction
Recently, machine learning problems that involve partial
feedback have received an increasing amount of attention
(Auer et al., 2002; Flaxman et al., 2005). These problems
occur naturally in many modern applications, such as online advertising and recommender systems (Li et al., 2010).
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

For instance, in a recommender system, the partial feedback represents whether the user likes the content recommended by the system, whereas the user’s preference for
the other contents that have not been displayed remain unknown. In this paper, we consider one particular learning
problem related to partial feedback: the online multiclass
prediction problem in the bandit setting (Kakade et al.,
2008). In the problem, the learner iteratively interacts with
the environment. In each iteration, the learner observes an
instance and is asked to predict its label. The main difference between the traditional full-information setting and
the bandit setting is the feedback received after each prediction. In the full-information setting, the true label of
the instance is revealed, whereas in the bandit setting, only
whether the prediction is correct is known. That is, in the
bandit setting, the true label remains unknown if the prediction is incorrect. Our goal is to make as few errors as
possible in the harsh environment of the bandit setting.
With more restricted information available, it becomes
harder to design good learning algorithms in the bandit setting, except for the case of online binary classification, in
which the bandit setting and full-information setting coincide. Thus, it is desirable to find a systematic way to transform the existing online binary classification algorithms,
or combine many of them, to get an algorithm that effectively deals with the bandit setting. The motivation calls for
boosting (Schapire, 1990), which is one of the most popular and well-developed ensemble methods implemented in
the traditional batch supervised classification framework.
While most studies on boosting focus on the batch setting
(Freund & Schapire, 1997; Schapire & Singer, 1999), some
works have extended the success of boosting to the online
setting (Oza & Russell, 2001; Chen et al., 2012). However,
to the best of our knowledge, there is no boosting algorithm yet for the problem of online multiclass prediction in
the bandit setting. In this paper, we study the possibility of

Boosting with Online Binary Learners for the Multiclass Bandit Problem

extending the promising theoretical and empirical results
of boosting to the bandit setting.
As in the design and analysis of boosting algorithms in
other settings, we need an appropriate assumption on weak
learners in order for boosting to work. A stronger assumption makes the design of a boosting algorithm easier, but
at the expense of more restricted applicability. To weaken
the assumption, we consider binary full-information weak
learners, instead of multiclass bandit ones, with the given
binary examples constructed through the one-versus-rest
decomposition from the multiclass examples (Chen et al.,
2009). Following (Chen et al., 2012), we propose a similar assumption which requires such binary weak learners to perform better than random guessing with respect
to “smooth” weight distributions over the binary examples. Then we prove that boosting is possible under this
assumption by designing a strong bandit algorithm using
such binary weak learners. Our bandit algorithm is extended from the full-information one of (Chen et al., 2012),
which provides a method to generate such smooth example
weights for updating weak learners, as well as some appropriate voting weights for combining the predictions of
weak learners.
Nevertheless, our extension in this paper is non-trivial. To
compute these weights exactly in (Chen et al., 2012), one
needs the full-information feedback, which is not available
in our bandit setting. With the limited information of bandit feedback, we show how to find good estimators for the
example weights as well as for the voting weights, and
we prove that they can in fact be used to replace the true
weights to make boosting work in the bandit setting.
Our proposed bandit boosting algorithm enjoys nice theoretical properties similar to those of its batch counterpart.
In particular, the proposed algorithm can achieve a small
error rate if the performance of each weak learner is better
than that of random guessing with respect to the carefully
generated weight distributions. In addition, the algorithm
reaches promising empirical performance on real-world
data sets, even when using very simple full-information
weak learners.
Finally, let us stress the difference between our work and
existing ones on the bandit problem. Unlike existing works,
our goal is not to construct one specific bandit algorithm
and analyze its regret. Instead, our goal is to study the
possibility of a general paradigm for designing bandit algorithms in a systematic way. Note that there are currently
only a very small number of bandit algorithms for the multiclass prediction problem, and most seem to be based on
linear models (Kakade et al., 2008; Hazan & Kale, 2011).
With the limited power of such linear models, a high error
rate is unavoidable in general, so the focus of these works
was to reduce the regret, regardless of whether the actual

error rate is high. Our result, on the other hand, works for
a broader class of classifiers beyond linear ones. We show
how to construct a strong bandit algorithm with an error
rate close to zero, when we have weak learners which can
perform slightly better than random guessing. Here we allow any weak learners, not just linear ones, that only need
to work in the simpler full-information setting rather than
in the more challenging bandit setting. Constructing such
weak learners may look much less daunting, but we show
that they in fact suffice for constructing strong bandit algorithms. We hope that this could open more possibilities for
designing better bandit algorithms in the future.

2. Boosting in different settings
Before formally describing our boosting framework in the
online bandit setting, let us first review the traditional batch
setting as well as the online full-information setting.
In the batch setting, the boosting algorithm has the whole
training set S = {(x1 , y1 ), . . . , (xT , yT )} available at the
beginning, where each xt is the feature vector from some
space X ⊆ Rd and yt is its label from some space Y.
For the case of binary classification, we assume Y =
{−1, +1}, and the boosting algorithm repeatedly calls the
batch weak learner for a number of rounds as follows. In
round i, it feeds S as well as a probability distribution p(i)
over S to the weak learner, which then returns a weak hypothesis h(i) after seeing the whole S and p(i) . It stops
at some round N when the strong hypothesis H(x) =
PN
sign( i=1 α(i) h(i) (x)), with α(i) ∈ R being the voting
weight of h(i) , achieves a small error rate over S, defined
as |{t : H(xt ) 6= yt }|/T .
For the case of multiclass classification, we assume Y =
{1, . . . , K}, and for simplicity we adopt the one-versusrest approach to reduce the multiclass problem to a binary
one. More precisely, each multiclass example (xt , yt ) is
decomposed into K binary examples ((xt , k), ytk ), k =
1, . . . , K, where ytk is 1 if yt = k and −1 otherwise. One
can then apply the boosting algorithm to such binary exP (i)
amples and use H(x) = arg maxk i αk h(i) (x, k) as the
strong hypothesis for the original multiclass problem.
In the online full-information setting, the examples of S
are usually considered as chosen adversarially and they arrive one at a time. The online boosting algorithm must decide on some number N of online weak learners to start
with. At step t, the boosting algorithm receives xt and it
P (i) (i)
(i)
predicts H(xt ) = arg maxk i αtk ht (xt , k), where ht
is the weak hypothesis provided by the i’th weak learner
(i)
and αtk is its voting weight. After the prediction, the true
label yt is revealed, and to update each weak learner, we
would like to feed it with a probability measure on each
binary example, as in the batch boosting. However, in the

Boosting with Online Binary Learners for the Multiclass Bandit Problem

online setting it is hard to determine a good measure of an
example without seeing the remaining examples, so we in(i)
stead only generate a weight wtk for ((xt , k), ytk ), which
(i)
after normalization corresponds to the measure ptk , for the
i-th weak learner. The goal is again to achieve a small error
rate over S, given that each weak learner has some positive
P
(i)
(i)
advantage, defined as t,k ptk ytk ht (xt , k). Chen et al.
(2012) proposed an online boosting algorithm that achieves
this goal in the binary case, which can be easily adapted to
the multiclass case here.
In this paper, we consider the online multiclass prediction
problem in the bandit setting. The setting is similar to the
full-information one, except that at step t the boosting algorithm only receives the bandit information of whether
its prediction is correct or not. The goal is essentially the
same—to achieve a small error rate, given that each weak
learner has some positive advantage.
Several issues arise in designing such a bandit boosting algorithm. The standard approach in designing a bandit algorithm is to use a full-information algorithm as a black
box, with its needed information replaced by some estimated one. Usually, the only information needed by a fullinformation algorithm is the gradient of the loss function
at each step, and this information is used only once, for
updating its next strategy or action. As a result, the performance (regret) of such a bandit algorithm can be easily
analyzed based on that of the full-information one, as it
is usually expressed as a simple function of the gradients.
For our boosting problem, we would also like to follow this
approach, and the only available full-information boosting
algorithm with theoretical guarantee is that of (Chen et al.,
2012). However, it is not obvious what to estimate now
since that algorithm involves three online processes which
all need the information yt , but for different purposes. First,
the boosting algorithm needs yt to compute the example
(i)
weights wtk ’s. Second, the boosting algorithm needs yt to
(i)
compute the voting weights αtk ’s. Third, the weak learn(i)
ers also need yt , in addition to wtk ’s, to update its next hypothesis. Can one single bit of bandit information about yt
be used to get good estimators for all the three processes?
Furthermore, as yt is used in several places and in a more
involved way, the bandit algorithm may not be able to use
the full-information one as a simple black box, and its performance (error rate) may not be easily based on that of the
full-information one. Finally, it is not clear what the appropriate assumption one should make on the weak learners in
order for boosting to work in the bandit setting. In fact, it is
not even clear what type of weak learners one should use.
Perhaps the most natural choice is to use multiclass bandit
algorithms. That is, starting from weak multiclass bandit
algorithms, we “boost” them into strong multiclass bandit
ones. Surprisingly, we will show that it suffices to use bi-

nary full-information algorithms with a positive advantage
as weak learners. This not only gives us a stronger result in
theory, as a weaker assumption on weak learners is needed,
but also provides us more possibilities of designing weak
learners (and thus strong bandit algorithms) in practice, as
most existing multiclass bandit algorithms are linear ones.
We will use the following notation and convention. For a
positive integer n, we let [n] denote the set {1, . . . , n}. For
a condition π, we use the notation 1[π] which gives the
value 1 if π holds and 0 otherwise. For simplicity, we assume that each xt has length kxt k2 ≤ 1 and each hypothesis ht comes from some family H with ht (xt , k) ∈ [−1, 1].

3. Online weak learners
In this section, we study reasonable assumptions on weak
learners for allowing boosting to work in the bandit setting.
As mentioned in the previous section, instead of using multiclass bandit algorithms as weak learners, we will use binary full-information ones. A natural assumption to make
is for such a binary full-information algorithm to achieve
a positive advantage with respect to any example weights.
However, as noted in (Chen et al., 2012), this assumption is
too strong to achieve, as one cannot expect an online algorithm to achieve a positive advantage in extreme cases, such
as when only the first example has a nonzero weight. Thus,
some constraints must be put on the example weights.
To identify an appropriate constraint, let us follow (Chen
et al., 2012) and consider the case that each hypothesis ht
consists of K linear functions with ht (x, k) = hhtk , xi, the
inner product of two vectors htk and x, with khtk k2 ≤ 1.
When given an example (xt , k), the weak learner uses htk
to predict the binary label ytk . After that, it receives ytk
as well as the example weight wtk , and uses them to update htk into a new h(t+1)k . We can reduce the task of
such a weak learner to the well-known online linear optimization problem, by using the reward function rtk (htk ) =
wtk ytk hhtk , xt i, which is linear in htk . Then we can apply
the online gradient descent algorithm of (Zinkevich, 2003)
to generate htk at step t, and a standard regret analysis
shows that for some constant c > 0,
s
X
X
X
2
wtk
wtk ytk hhtk , xt i ≥
wtk ytk hh∗k , xt i − c
t

t

t

for any h∗k with kh∗k k2 ≤ 1. Summing over k ∈ [K] and
using Cauchy-Schwarz inequality, we get
s
X
X
X
2 .
wtk ytk hhtk , xt i ≥
wtk ytk hh∗k , xt i− cK
wtk
t,k

t,k

t,k

P
tk
Let |w| denote the total weight t,k wtk , so that ptk = w|w|
is the measure of example (xt , k). Then by dividing both

Boosting with Online Binary Learners for the Multiclass Bandit Problem

sides of the inequality above by |w|, we obtain
v
u
X w2
X
X
u
tk
.
ptk ytk hhtk , xt i ≥
ptk ytk hh∗k , xt i−tcK
|w|2
t,k

t,k

t,k

P
Note that t,k ptk ytk hh∗k , xt i is the advantage of the offline learner, and suppose that it is at least 3γ > 0. Moreover, suppose the example weights are large, in the sense
that they satisfy the following condition:

setting. More precisely, at step t we do the following after
receiving the feature vector xt . For each class k ∈ [K],
a new feature vector (xt , k) is created, we obtain a binary
(i)
(i)
weak hypothesis htk (x) = ht (x, k) from the i’th weak
learner, for i ∈ [N ], and we form the strong hypothesis
Ht (x) = arg max ftk (x), with ftk (x) =
k∈[K]

N
X

(i) (i)

αtk htk (x),

i=1

(i)

2

|w| ≥ cKB/γ ,

(1)

where B ≥ maxt,k wtk is a constant that will be fixed later.
Then the advantage of the online weak learner becomes
s
X
B|w|
≥ 3γ − γ = 2γ.
ptk ytk hhtk , xt i ≥ 3γ − cK
|w|2
t,k

This motivates us to propose the following assumption on
weak learners, which need not be linear ones.
Assumption 1. There is an online full-information weak
learner which can achieve an advantage 2γ > 0 for any
sequence of examples and weights satisfying condition (1).
From the discussion above, we have the following.
Lemma 1. Suppose for any sequence of examples and
weights satisfying condition (1), there exists an offline linear hypothesis with an advantage 3γ > 0. Then Assumption 1 holds.
Let us make two remarks on Assumption 1. First, the
assumption that a weak learner has a positive advantage
is just the assumption that it predicts better than random
guessing, which is the standard assumption used by (almost) all previous batch boosting algorithms. Second, the
condition (1) on example weights actually makes our assumption weaker, which in turn makes the boosting task
harder and our boosting result in the next section stronger.
More precisely, we only require the weak learner to perform well (having a positive advantage) when the weights
are large, and we do not care how bad it may perform with
small weights. In fact, we will make our boosting algorithm
call the weak learner with large weights.

where αtk is some voting weight for the i’th weak learner.
Then we make our prediction ŷt based on Ht (xt ) in some
way and receive the feedback 1[ŷt = yt ]. Using the feed(i)
back, we prepare some example weight wtk to update the
i’th weak learner, as well as to compute the next voting
(i)
weight α(t+1)k , for i ∈ [N ]. It remains to show how to set
the example weights and the voting weights, as well as how
to choose ŷt , which we describe and analyze in detail next.
The complete algorithm is given in Algorithm 1.
Algorithm 1 Bandit boosting algorithm with online weak
learner WL
Input: Streaming examples (x1 , y1 ), . . . , (xT , yT ).
Parameters: 0 < δ < 1, 0 ≤ θ < γ < 21 .
(i)
(i)
Choose α1k = N1 and random h1k for k ∈ [K], i ∈ [N ].
for t = 1 to T do
P
(i) (i)
Let Ht (x) = arg maxk∈[K] i∈[N ] αtk htk (x).
δ
Let pt (k) = (1 − δ)1[k = Ht (xt )] + K
for k ∈ [K].
Predict ŷt according to the distribution pt .
Receive the information 1[ŷt = yt ].
for k = 1 to K and i = 1 to N do
(i)
Update wtk according to (4).
(i)
(i)
If ŷt = k, call WL(htk , (xt , k), ytk , wtk ) to obtain
(i)
(i)
(i)
h(t+1)k ; otherwise, let h(t+1)k = htk .
(i)

Update α(t+1)k according to (6).
end for
end for
The example weight of (xt , k) for the i’th weak learner
used by the full-information algorithm of (Chen et al.,
2012) is
n
o
(i−1)
(i)
w̄tk = min (1 − γ)ztk /2 , 1 ,
(2)

4. Our bandit boosting algorithm
(0)

In this section we show how to design a bandit boosting
algorithm under Assumption 1. Let WL be such an online
full-information weak learner and we will run N copies of
WL, for some N to be determined later. We follow the approach of reducing the multiclass problem to the binary one
as described in Section 2, and we base our bandit boosting
algorithm on the full-information one of (Chen et al., 2012)
that works for binary classification in the full-information

(i−1)

where ztk = 0 and ztk
(i−1)

ztk

=

, for i − 1 ≥ 1, is defined as

i−1
X
(j)
(ytk htk (xt ) − θ), with θ = γ/(2 + γ), (3)
j=1

which depends on the information ytk . As we are in the
bandit setting, we do not have ytk to compute such weights
in general. Thus, we balance exploitation with exploration

Boosting with Online Binary Learners for the Multiclass Bandit Problem

by independently predicting Ht (xt ) with probability 1 − δ
and a random label with probability δ, with δ ≤ γ; let
ŷt denote our prediction. For k ∈ [K], let pt (k) denote
the probability that ŷt = k. Then we replace the example
(i)
weight w̄tk by the estimator
(i)



wtk =

(i)
w̄tk /pt (k)

0

if ŷt = k,
otherwise,

(4)

which we can compute, because when ŷt = k, we do have
(i)
ytk to compute w̄tk . As pt (k) ≥ δ/K, we can choose
(i)
(i)
B = K/δ and have wtk ≤ B for any t, k, i. Note that wtk
(i)
and w̄tk are random variables, and the following shows that
(i)
(i)
wtk ’s are in fact good estimators for w̄tk ’s.
Claim 1. For any t, k, i,
Pr

"
X

(i)
w̄tk

−

(i)
E[w̄tk ]

(i)
wtk



=

(i)
E[wtk ].

Then when T ≥ c0 (K 2 /δ 4 ) log(K/δ) for a large enough
constant c0 ,



Pr {(t, k) : ytk f¯tk (xt ) < θ} > 2δKT ≤ δ,
for the parameter θ = γ/(2 + γ) introduced in (3)
Proof. Note that according to the definition, for any t and
(m+1)
(m+1)
k, w̄tk
≥ 0, and w̄tk
= 1 if ytk f¯tk (xt ) < θ, as
(m)
¯
ztk = m(ytk ftk (xt ) − θ). This implies that

 X (m+1)
{(t, k) : ytk f¯tk (xt ) < θ} ≤
w̄
.
tk

t,k

As

P

t,k

For any k, i, λ,
2

T /B 2 )

< δKT by the definition of m, we have




Pr {(t, k) : ytk f¯tk (xt ) < θ} > 2δKT


X (m+1) X (m+1)
≤ Pr 
w̄tk
−
wtk
> δKT  ,

#
> λT ≤ 2−Ω(λ

(m+1)

wtk

.

t

t,k

Proof. Observe that any fixing of the randomness up to
(i)
(i)
step t − 1 leaves w̄tk and wtk with the same conditional
(i)
(i)
expectation. Thus, E[w̄tk ] = E[wtk ]. Moreover, as the
(i)
(i)
random variables Mt = w̄tk − wtk , for t ∈ [T ], form a
martingale difference sequence, with |Mt | ≤ B, the probability bound follows from Azuma’s inequality.
(i)

This claim allows us to use wtk as the example weight of
(xt , k) to update the i’th weak learner. However, as each
weak learner is assumed to be a full-information one, it also
needs the label ytk to update which we may not know. One
may try to take a similar approach as before to feed the
weak learner with an estimator which is ytk /pt (k) when
ŷt = k and 0 otherwise, but this does not work as it does
not take a value in {−1, 1} needed by the binary weak
learner. Instead, we take a different approach: we only
call the weak learner to update when ŷt = k so that we
know ytk . That is, when ŷt = k, we call the i’th weak
(i)
learner with wtk and ytk , which can then update and re(i)
turn the next weak hypothesis h(t+1)k ; otherwise, we do
not call the i’th weak learner to update and we simply let
(i)
(i)
the next hypothesis h(t+1)k be the current htk . Another
issue is that a weak learner is only assumed to work well
when given large example weights satisfying condition (1),
and even then, it only works well on those examples which
are given to it to update. This is dealt by the following.
Lemma 2. Let δ ∈ [0, 1], let m be the largest number such
P
(i)
that t,k wtk ≥ δKT for every i ≤ m, and let

which by a union bound and Claim 1 is at most
2
2
K2−Ω(δ T /B ) ≤ δ.
The following lemma gives an upper bound on the parameter m defined in Lemma 2.
Lemma 3. Suppose Assumption 1 holds and T ≥
cK/(δ 2 γ 2 ) for the constant c in the condition (1). Then
the parameter m in Lemma 2 is at most O(K/(δ 2 γ 2 )).
P
(i)
Proof. Note that for any i ∈ [m], t,k wtk ≥ δKT ≥
cKB/γ 2 , with B = K/δ, and the condition (1) is satisfied.
Thus from Assumption 1, we have
X (i)
X (i)
(i)
wtk ytk htk (xt ) ≥ 2γ
wtk ,
(5)
i,t,k

i,t,k

with the sums over i above, as well as in the rest of the
proof, being taken over i ∈ [m]. On the other hand, we
have the following claim.
P
(i)
(i)
Claim 2.
≤ O(BKT /γ) +
i,t,k wtk ytk htk (xt )
P
(i)
γ i,t,k wtk .
We omit its proof here as it is very similar to that for
Lemma 5 in (Servedio, 2003).1 Combining the bound in
Claim 2 with the inequality (5), we have
X (i)
γ
wtk ≤ O(BKT /γ).
i,t,k
1

m
X
1 (i)
f¯tk (x) =
htk (x).
m
i=1

t,k

(i)

Although that lemma is for w̄tk ’s, its proof can be easily
(i)
modified to work for wtk ’s, but with an additional factor of B
appearing in the term O(BKT /γ) here.

Boosting with Online Binary Learners for the Multiclass Bandit Problem

P
(i)
Since t,k wtk ≥ δKT for i ∈ [m] and B = K/δ, we get
γmδKT ≤ O(K 2 T /(δγ)). From this, the required bound
on m follows, and we have the lemma.
Let us suppose that T ≥ c0 (K 2 /δ 4 ) log(K/δ) for a large
enough constant c0 so that both lemmas apply. Then
Lemma 2 shows that one can obtain a strong learner by
combining the first m weak learners. However, one cannot determine the number m before seeing all the examples, and in fact in our online setting, we need to decide
the number N of weak learners even before seeing the first
example. Following (Chen et al., 2012), we set N to be the
upper bound given by Lemma 3. Then at step t, for each
k ∈ [K], we consider the function
ftk (x) =

N
X

(i) (i)

and reduce the task of finding such αtk =
to the Online Convex Programming problem. More precisely, we use the N -dimensional probability simplex, denoted by PN , as the feasible set and define the loss function
as
(
)
N
X
(i) (i)
Ltk (α) = max 0, θ − ytk
α htk (xt ) ,
i=1

which is a convex function of α. However, unlike in (Chen
et al., 2012), we are in the bandit setting and thus may not
know ytk . To overcome this, we use a similar idea as before
to estimate a subgradient ∇Ltk (αtk ) by

∇Ltk (αtk )/pt (k) if ŷt = k,
`tk =
0
otherwise.
(N )

One can then use `tk = (`tk , . . . , `tk ) to perform gradient descent to update αtk as in (Chen et al., 2012). However, to get a better theoretical bound, here we choose to
perform a multiplicative update on αtk to get α(t+1)k =
(N )

(α(t+1)k , . . . , α(t+1)k ) for step t + 1, with
(i)

(i)

α(t+1)k = αtk · e

(i)
−η`tk

/Z(t+1)k ,

t

O(δKT ),

Ltk (ᾱk )

(i)

1
m

for i ≤ m and

max{0, θ − ytk f¯tk (xt )}
≤ (1 + θ)1[ytk f¯tk (xt ) < θ].
=

Then we know from Lemma 2 that


X
Pr 
Ltk (ᾱk ) ≤ (1 + θ)2δKT  ≥ 1 − δ.
t,k

Combining the two probability bounds together, we have
the lemma.
Finally, recall that to predict each yt , we independently output Ht (xt ) = arg maxk∈[K] ftk (xt ) with probability 1 − δ
and a random label with probability δ. Thus, by a Chernoff bound, our algorithm makes at most |{t : Ht (xt ) 6=
2
yt }| + 2δT errors with probability 1 − 2−Ω(δ T ) ≥ 1 − δ.
On the other hand, as
X
1[Ht (xt ) 6= yt ] ≤
1[ytk ftk (xt ) < 0]
≤

(6)

Proof. Following the standard analysis, one can show that
for any k ∈ [K] and any ᾱk ∈ PN ,
X
X
h`tk , αtk − ᾱk i ≤ O((log N )/η) + η
k`tk k2∞
t

(N )

Let ᾱk = (ᾱk , . . . , ᾱk ), with ᾱk =
(i)
ᾱk = 0 for i > m, so that

k

where Z(t+1)k is the normalization factor and η is the learning rate which we set to δ 3 /K. Then we have the following.
hP
i
Lemma 4. Pr
L
(α
)
≤
O(δKT
)
≥ 1 − 2δ.
tk
tk
t,k

≤

Then using the bound in (7) and applying a similar martingale analysis as before, one can show that for any ᾱk ∈
PN ,


X
Pr 
(Ltk (αtk ) − Ltk (ᾱk )) ≤ O(δKT ) ≥ 1 − δ.
(1)

(1)
(N )
(αtk , . . . , αtk )

(1)

E [Ltk (αtk ) − Ltk (ᾱk )] ≤ E[h`tk , αtk − ᾱk i].

t,k

αtk htk (x)

i=1

(1)

since k`tk k2∞ ≤ B 2 = K 2 /δ 2 , η = δ 3 /K and N ≤
O(K/δ 4 ). Now note that for any t and k, E[h`tk , αtk −
ᾱk i] = E[h∇Ltk (αtk ), αtk − ᾱk i] because given the randomness up to step t − 1, αtk is fixed and the conditional expectation of `tk equals ∇Ltk (αtk ). Moreover, as
Ltk (αtk ) − Ltk (ᾱk ) ≤ h∇Ltk (αtk ), αtk − ᾱk i for convex
Ltk , we have

(7)

X

Ltk (αtk )/θ,

k

Lemma 4 implies that
Pr [|{t : Ht (xt ) 6= yt }| ≤ O(δKT /θ)] ≥ 1 − 2δ.
Consequently, for θ = γ/(2 + γ), we can conclude that our
algorithm makes at most
O(KδT /θ) + 2δT ≤ O(KδT /γ)
errors with probability at least 1 − 3δ. Therefore, we have
the following, which is the main result of our paper.

Boosting with Online Binary Learners for the Multiclass Bandit Problem

Theorem 1. Suppose Assumption 1 holds and T ≥
c0 (K 2 /δ 4 ) log(K/δ) for a large enough constant c0 . Then
our bandit algorithm uses O(K/(δ 2 γ 2 )) weak learners
and makes O(KδT /γ) errors with probability 1 − 3δ.
Note that the error rate of our algorithm is O(Kδ/γ),
which can be made to any ε by setting δ = O(εγ/K), with
the requirement on T and the number of weak learners adjusted accordingly. We remark that we did not attempt to
optimize our bounds (which we believe can be improved)
as our focus was on establishing the possibility of boosting
in the bandit setting. Moreover, it does not seem appropriate to compare our error bound with the regret bounds of
existing bandit algorithms. This is because existing algorithms are usually based on linear classifiers, which may
have large error rates even though their regrets are small.
On the other hand, our boosting algorithm works for any
type of classifiers and achieves a small error rate as long as
we have weak learners which satisfy Assumption 1.

5. Experiments
In this section, we validate the empirical performance of
the proposed algorithm on several real-world data sets. We
compare with two representative algorithms. The first one
is Banditron (Kakade et al., 2008), which is one of the
first proposed algorithms for the bandit setting. It is modified from a multiclass variant of the well-known Perceptron
algorithm (Rosenblatt, 1962) using the so-called Kesler’s
construction (Duda & Hart, 1973). By doing some random exploration, it can accurately construct the estimation
of the update step for the full-information multiclass Perceptron. The algorithm has good theoretical guarantee, especially when the data is linearly separable. The algorithm
can be viewed as a direct modification of a full-information
learner (Perceptron) for the bandit setting, without combining the learners for boosting.
The second one is Conservative OVA (C-OVA) (Chen
et al., 2009), which uses the one-versus-all multiclass to
binary decomposition similar to our algorithm. But unlike
most of the bandit algorithms, it does not do random exploration at all. Instead, it conservatively updates using whatever it gets from the partial feedback, and hence the name.
Note that although it embeds an online binary learning algorithm as its “base learner”, it does not perform boosting
by combining several base learners like our algorithm does.
Also, C-OVA performs a margin-based decoding of the binary classification results, and hence may not work well
with non-margin-based base learners.
To demonstrate the boosting ability of our proposed algorithm, we choose two completely different types of online
binary classifiers as our weak learners. The first one is Perceptron, a standard margin-based linear classifier. Note that
in (Chen et al., 2009) they use a similar but more com-

Table 1. The data sets used in our experiments.
Data set
Car
DNA Nursery Connect4 Reuters4
#classes
4
3
5
3
4
#features
6
180
8
126
346,810
#examples 1,728 3,186 12,960
67,557
673,768

plex Online Passive-Aggressive Algorithm (PA) (Crammer
et al., 2006) as its internal learner. Since we found little difference in performance on the data sets we tested between
the PA algorithm and the Perceptron algorithm, we only
report the results using the simpler and more famous Perceptron algorithm to compare fairly with Banditron. The
second weak learner we use is Naive Bayes, a simple statistical classifier that estimates the posterior probability for
each class using Bayes theorem and the assumption of conditional independence between features.
5.1. Results
We test our algorithm on 5 public real-world data sets from
various domains with different sizes: C AR, N URSERY,
and C ONNECT 4 from the UCI machine learning repository
(Frank & Asuncion, 2010); D NA from the Statlog project
(Michie et al., 1994); R EUTERS 4 from the paper of Banditron (Kakade et al., 2008). Basic information of these
data sets are summarized in Table 1. As described previously, each example is first used for prediction before the
disclosure of its label, and the error rate is the number of
prediction errors divided by the total number of examples.
All the experiments are repeated 10 times with different
random orderings of the examples.
For fairness of comparison to Banditron, we do not tune
the parameters other than the exploration rate δ. We fix the
number of weak learners to be 100 and the assumed weak
learner advantage γ to be 0.1 as in the full-information online boosting algorithm (Chen et al., 2012). For the exploration rate δ, we test a wide range of values to see the
effect of random exploration. The results are shown in Figure 1. Note that C-OVA is not included in this figure since
C-OVA does not perform random exploration at all and is
parameter-free. One can see that for reasonable range of
values of δ (around 0.01 to 0.1), the performance of our algorithm is quite strong and relatively stable, while setting
it too high or too low results in worse performance as expected. Table 2 summarizes the average error rate and the
standard deviation when the best choices of δ are used in
Banditron and in our algorithm.
Let us first focus on the case when Perceptron is used as
the weak learner. Here, the categorical features are transformed into numerical ones by decomposition into binary
vectors. We can see that the proposed bandit boosting algorithm consistently outperforms Banditron on all the data
sets, and is also comparable to C-OVA, especially on larger

Boosting with Online Binary Learners for the Multiclass Bandit Problem
Banditron
BanditBoost + Perceptron
BanditBoost + NaiveBayes

Banditron
BanditBoost + Perceptron
BanditBoost + NaiveBayes

0.7

0.6

0.5

0.5

0.3

error rate

0.6

0.5
0.4

0.4
0.3

0.4
0.3

0.2

0.2

0.2

0.1

0.1

0.1

0 -3
10

-2

10

-1

0 -3
10

0

10

δ

10

-2

10

(a) C AR

-1

0 -3
10

0

10

δ

Banditron
BanditBoost + Perceptron
BanditBoost + NaiveBayes

0.7

0.6

error rate

error rate

0.7

10

-2

-1

10

(b) D NA

0

10

δ

10

(c) N URSERY
0.8

Banditron
BanditBoost + Perceptron
BanditBoost + NaiveBayes

0.6

0.5

0.5

0.4
0.3

0.3
0.2

0.1

0.1

-2

10

-1

10

δ

0

10

(d) C ONNECT 4

0.6

0.4

0.2

Banditron
C-OVA + Perceptron
BanditBoost + Perceptron

0.7

error rate

0.6

0 -3
10

Banditron
BanditBoost + Perceptron

0.7

error rate

error rate

0.7

0.5
0.4
0.3
0.2
0.1

0 -3
10

-2

10

-1

δ

10

0

10

(e) R EUTERS 4

0 2
10

3

10

4

10

# of examples

5

10

6

10

(f) Learning Curve of R EUTERS 4

Figure 1. (a)-(d): Error rate using different values of exploration rate δ. (f): learning curve of R EUTERS 4 using the best δ

Table 2. Average (over 10 trials) error rate (%) and standard deviation comparison
Data set

Banditron

C AR
D NA

29.4 ± 0.9
26.8 ± 9.0
28.8 ± 1.4
39.8 ± 0.4
16.7 ± 0.5

N URSERY
C ONNECT 4
R EUTERS 4

C-OVA
(Perceptron)
22.8 ± 1.1
13.5 ± 0.5
17.9 ± 3.0
28.1 ± 0.2
8.5 ± 4.2

data sets. To take a closer look at the performance of these
algorithms, we plot the learning curve for the largest data
set (R EUTERS 4) in Figure 1 (f). One can see that our algorithm begins to outperform the other algorithms when the
number of examples is sufficiently large. This is due to the
more complex model we use and the need for random exploration as opposed to the deterministic C-OVA algorithm.
Note that it is in accordance to our analysis in Theorem 1,
as the error bound only holds when the number of rounds T
is large.
Next, let us see the situation when the weak learner is
switched to Naive Bayes. Note that here we did not test on
the R EUTERS 4 data set due to the slow inference of Naive
Bayes for high dimensional data. It can be seen that our
algorithm consistently reaches the best on all the data sets.
Moreover, we see a large difference between C-OVA and
our algorithm, especially in D NA and N URSERY data sets.
The superiority echoes the earlier conjecture that C-OVA
may not work well with non-margin-based base learners.

BanditBoost
(Perceptron)
26.9 ± 2.4
18.6 ± 0.6
16.0 ± 1.1
30.8 ± 0.2
6.3 ± 0.1

C-OVA
(Naive Bayes)
30.0 ± 0.1
42.9 ± 8.3
59.3 ± 8.2
34.9 ± 2.4
N/A

BanditBoost
(Naive Bayes)
25.1 ± 1.8
25.1 ± 2.6
28.9 ± 2.1
34.2 ± 0.4
N/A

On the other hand, the proposed bandit boosting algorithm
enjoys a stronger theoretical guarantee and works well with
various types of weak learners.

6. Conclusion
We propose a boosting algorithm to efficiently generate
strong multiclass bandit learners by exploiting the abundance of existing online binary learners. The proposed
algorithm can be viewed as a careful combination of the
online boosting algorithm for binary classification (Chen
et al., 2012) and some key estimation techniques in the bandit algorithms. While the proposed algorithm is simple,
we show some non-trivial theoretical analysis that leads to
sound theoretical guarantee. To the best of our knowledge,
our proposed boosting algorithm is the first one that comes
with such theoretical guarantee. In addition, experimental
results on real-world data sets show that the proposed bandit boosting algorithm can be easily coupled with different
weak binary learners to reach promising performance.

Boosting with Online Binary Learners for the Multiclass Bandit Problem

References
Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E.
The non-stochastic multi-armed bandit problem. SIAM
Journal of Computing, 32:48–77, 2002.
Chen, G., Chen, G., Zhang, J., Chen, S., and Zhang, C.
Beyond banditron: A conservative and efficient reduction for online multiclass prediction with bandit setting
model. In Proceedings of ICDM, pp. 71–80, 2009.
Chen, S.-T., Lin, H.-T., and Lu, C.-J. An online boosting
algorithm with theoretical justifications. In Proceedings
of ICML, pp. 1007–1014, July 2012.
Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S.,
and Singer, Y. Online passive-aggressive algorithms. J.
Mach. Learn. Res., 7:551–585, December 2006.
Duda, R. O. and Hart, P. E. Pattern Classification and
Scene Analysis. Wiley, 1973.
Flaxman, A. D., Kalai, A. T., and McMahan, H. B. Online convex optimization in the bandit setting: gradient
descent without a gradient. In Proceedings of SODA, pp.
385–394, Philadelphia, PA, USA, 2005.
Frank, A. and Asuncion, A. UCI machine learning repository, 2010. URL http://archive.ics.uci.
edu/ml.
Freund, Y. and Schapire, R. E. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):
119–139, 1997.
Hazan, E. and Kale, S. Newtron: an efficient bandit algorithm for online multiclass prediction. In Proceedings of
NIPS, pp. 891–899, 2011.
Kakade, S. M., Shalev-Shwartz, S., and Tewari, A. Efficient bandit algorithms for online multiclass prediction.
In Proceedings of ICML, pp. 440–447, New York, NY,
USA, 2008.
Li, L., Chu, W., Langford, J., and Schapire, R. E. A
contextual-bandit approach to personalized news article
recommendation. In Proceedings of WWW, pp. 661–670,
New York, NY, USA, 2010. ACM.
Michie, D., Spiegelhalter, D. J., and Taylor, C. C. Machine
learning, neural and statistical classification, 1994.
Oza, N. C. and Russell, S. Online bagging and boosting. In
Proceedings of AISTATS, pp. 105–112, 2001.
Rosenblatt, F. Principles of Neurodynamics: Perceptrons
and the Theory of Brain Mechanisms. Spartan, 1962.

Schapire, R. E. The strength of weak learnability. Mach.
Learn., 5(2):197–227, July 1990.
Schapire, R. E. and Singer, Y. Improved boosting algorithms using confidence-rated predictions. Machine
Learning, 37(3):297–336, December 1999.
Servedio, R. A. Smooth boosting and learning with malicious noise. JMLR, 4:473–489, 2003.
Zinkevich, M. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of
ICML, pp. 928–936, 2003.

