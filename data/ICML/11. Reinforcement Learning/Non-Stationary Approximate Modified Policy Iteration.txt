Non-Stationary Approximate Modified Policy Iteration

Boris Lesner
BORIS . LESNER . DATEXIM @ GMAIL . COM
Bruno Scherrer
BRUNO . SCHERRER @ INRIA . FR
Inria, Villers-ls-Nancy, F-54600, France
Universit de Lorraine, LORIA, UMR 7503, Vanduvre-ls-Nancy, F-54506, France

Abstract
We consider the infinite-horizon γ-discounted
optimal control problem formalized by Markov
Decision Processes. Running any instance of
Modified Policy Iteration—a family of algorithms that can interpolate between Value and
Policy Iteration—with an error  at each iteration is known to lead to stationary policies that
2γ
are at least (1−γ)
2 -optimal. Variations of Value
and Policy Iteration, that build `-periodic nonstationary policies, have recently been shown to
2γ
display a better (1−γ)(1−γ
` ) -optimality guarantee. We describe a new algorithmic scheme,
Non-Stationary Modified Policy Iteration, a family of algorithms parameterized by two integers
m ≥ 0 and ` ≥ 1 that generalizes all the
above mentionned algorithms. While m allows
one to interpolate between Value-Iteration-style
and Policy-Iteration-style updates, ` specifies the
period of the non-stationary policy that is output. We show that this new family of algorithms
2γ
also enjoys the improved (1−γ)(1−γ
` ) -optimality
guarantee. Perhaps more importantly, we show,
by exhibiting an original problem instance, that
this guarantee is tight for all m and `; this tightness was to our knowledge only known in two
specific cases, Value Iteration (m = 0, ` = 1)
and Policy Iteration (m = ∞, ` = 1).

1. Introduction
Dynamic Programming (DP) is an elegant approach
for addressing γ-discounted infinite-horizon optimal control problems formalized as Markov Decision Processes
(MDP) (Puterman, 1994). The two most well-known DP
algorithms in this framework are Value Iteration (VI) and
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

Policy Iteration (PI). While the former has typically lighter
iterations, the latter usually converges much faster. Modified Policy Iteration (MPI), that interporlates between the
two, was introduced to improve the convergence rate of VI
while remaining lighter than PI (Puterman & Shin, 1978).
When the optimal control problem one considers is large,
an option is to consider approximate versions of these DP
algorithms, where each iteration may be corrupted with
some noise . An important question is the sensitivity
of such an approach to the noise. Bertsekas & Tsitsiklis
(1996) gather several results regarding approximate versions of VI and PI (thereater named AVI and API). It is
known that the policy output by such procedures is guaran2γ
teed to be (1−γ)
2 -optimal. In particular, when the perturbation  tends to 0, one recovers an optimal solution. This
analysis was recently generalized to an approximate implementation of MPI (AMPI) independently by Canbolat
& Rothblum (2012) and Scherrer et al. (2012). The bet2γ
ter guarantee, obtained by the latter— (1−γ)
2 -optimality—
exactly matches that of AVI and API. The algorithmic
scheme AMPI can be implemented in various ways, reducing the original control problem to a series of (more
standard) regression and classification problems (Scherrer et al., 2012), and lead to state-of-the-art results on
large benchmark problems, in particular on the Tetris domain (Gabillon et al., 2013).
An apparent weakness of these sensitivity analyses is that
the dependence with respect to the discount factor γ is bad:
since γ is typically close to 1, the denominator of the con2γ
stant (1−γ)
2 often makes the guarantee uninformative in
practice. Unfortunately, it turns out that it is not so much a
weakness of the analyses but a weakness of the very algorithmic approach since Bertsekas & Tsitsiklis (1996) and
2γ
Scherrer & Lesner (2012) showed that the bound (1−γ)
2
is tight respectively for API and AVI and thus cannot be
improved in general. Interestingly, the authors of the latter article described a trick for modifying AVI and API
so as to improve the guarantee: even though one knows
that there exists a stationary policy that is optimal, Scherrer & Lesner (2012) showed that variations of AVI and API

Non-Stationary Approximate Modified Policy Iteration

that compute `-periodic non-stationary policies (thereafter
named NS-AVI and NS-API) lead to an improved bound of
2γ
. For values of ` of the order of log1 1 —that is
(1−γ)(1−γ ` )
γ

1
equivalent to 1−γ
when γ is close to 1—the guarantee is
1
). With reimproved by a significant factor (of order 1−γ
spect to the standard AVI and API schemes, the only extra
algorithmic price to pay is memory that is then O(`) instead of O(1). As often in computer science, one gets a
clear trade-off between quality and memory.

To the best of our knowledge, it is not known whether the
non-stationary trick also applies to a modified algorithm
that would interpolate between NS-AVI and NS-API. Perhaps more importantly, it is not known whether the im2γ
proved bound (1−γ)(1−γ
` ) is tight for NS-AVI or NS-API,
2γ
and even whether the standard (1−γ)
2 bound is tight for
AMPI. In this article, we fill the missing parts of this topic
in the literature. We shall describe NS-AMPI, a new nonstationary MPI algorithm that generalizes all previously
mentioned algorithms—AVI, API, AMPI, NS-AVI and NS2γ
API—and prove that it returns a policy that is (1−γ)(1−γ
`) optimal. Furthermore, we will show that for any value of
the period ` and any degree of interpolation between NSAVI and NS-API, such a bound is tight. Thus, our analysis
not only unifies all previous works, but it provides a complete picture of the sensitivity analysis for this large class
of algorithms.

The paper is organized as follows. In Section 2 we describe the optimal control problem. Section 3 describes
the state-of-the-art algorithms AMPI, NS-AVI and NS-API
along with their known sensitivity analysis. In Section 4,
we describe the new algorithm, NS-AMPI, and our main
results: a performance guarantee (Theorem 3) and a matching lower bound (Theorem 4). Section 5 follows by providing the proof skteches of both results. Section 6 describes
a small numerical illustration of our new algorithm, which
gives some insight on the choice of its parameters. Section 7 concludes and mentions potential future research directions.

2. Problem Setting
We consider a discrete-time dynamic system whose state
transition depends on a control. Let X be a state space.
When at some state, an action is chosen from a finite action space A. The current state x ∈ X and action a ∈
A characterize through a homogeneous probability kernel
P (dx|x, a) the distribution of the next state x0 . At each
transition, the system is given a reward r(x, a, x0 ) ∈ R
where r : X×A×X → R is the instantaneous reward function. In this context, the goal is to determine a sequence of
actions (at ) adapted to the past of the process until time
t that maximizes the expected discounted sum of rewards

from any starting state x:

" ∞
#

X

k
E
γ r(xk , ak , xk+1 )  x0 = x, xt+1 ∼ P (·|xt , at ) ,

k=0

where 0 < γ < 1 is a discount factor. The tuple
hX, A, P, r, γi is called a Markov Decision Process (MDP)
and the associated optimization problem infinite-horizon
stationary discounted optimal control (Puterman, 1994;
Bertsekas & Tsitsiklis, 1996) .
An important result of this setting is that there exists at
least one stationary deterministic policy, that is a function
π : X → A that maps states into actions, that is optimal (Puterman, 1994). As a consequence, the problem is
usually recast as looking for the stationary deterministic
policy π that maximizes for every state x the quantity

#
" ∞

X

γ k r(xk , π(xk ), xk+1 )  x0 = x ,
vπ (x) := Eπ

k=0
(1)
called the value of policy π at state x. The notation
Eπ means that we condition on trajectories such that
xt+1 ∼ Pπ (·|xt ), where Pπ (dx|x) is the stochastic kernel P (dx|x, π(x)) that chooses actions according to policy
π. We shall similarly write rπ : X → R for the function
giving the immediate reward while following policy π:
∀x, rπ (x) = E [r(x0 , π(x0 ), x1 ) | x0 = x, x1 ∼ Pπ (·|x0 )] .
Two linear operators are associated with the stochastic kernel Pπ : a left operator on functions f ∈ RX
Z
∀x, (Pπ f )(x) = f (y)Pπ (dy|x)
= E [f (x1 ) | x0 = x, x1 ∼ Pπ (·|x0 )] ,
and a right operator on distributions µ
Z
(µPπ )(dy) = Pπ (dy|x)µ(dx).
In words, (Pπ f )(x) is the expected value of f after following policy π for a single time-step starting from x, and µPπ
is the distribution of states after a single time-step starting
from µ.
Given a policy π, it is well known that the value vπ is the
unique solution of the following Bellman equation:
vπ = rπ + γPπ vπ .
In other words, vπ is the fixed point of the affine operator
Tπ v := rπ + γPπ v. The optimal value starting from state
x is defined as
v∗ (x) := max vπ (x).
π

Non-Stationary Approximate Modified Policy Iteration

It is also well known that v∗ is characterized by the following Bellman equation:
v∗ = max(rπ + γPπ v∗ ) = max Tπ v∗ ,
π

π

where the max operator is componentwise. In other words,
v∗ is the fixed point of the nonlinear operator T v :=
maxπ Tπ v. Finally, for any function v ∈ RX , we say that
a policy π is greedy with respect to v if it satisfies:
π ∈ arg max
Tπ0 v
0
π

or equivalently Tπ v = T v. We write, with some abuse of
notation1 G(v) any policy that is greedy with respect to v.
The notions of optimal value function and greedy policies
are fundamental to optimal control because of the following standard property: any policy π∗ that is greedy with
respect to the optimal value is an optimal policy and its
value vπ∗ is equal to v∗ . Thus, the main problem amounts
to computing the optimal value function v∗ . The next section descibes algorithmic approaches from the literature.

3. State-of-the-Art Algorithms
We begin by describing the Approximate Modified Policy Iteration (AMPI) algorithmic scheme (Scherrer et al.,
2012). Starting from an arbitrary value function v0 , AMPI
generates a sequence of value-policy pairs
πk+1 = G(vk )
vk+1 = (Tπk+1 )

(greedy step)
m+1

vk +  k

(evaluation step)

where m ≥ 0 is a free parameter. At each iteration k, the
term k accounts for a possible approximation in the evaluation step. AMPI generalizes the well-known approximate
DP algorithms Value Iteration (AVI) and Policy Iteration
(API) for values m = 0 and m = ∞, respectively. In the
exact case (k = 0), MPI requires less computation per iteration than PI (in a way similar to VI) and enjoys the faster
convergence (in terms of number of iterations) of PI (Puterman & Shin, 1978; Puterman, 1994).
It was recently shown that controlling the errors k when
running AMPI is sufficient to ensure some performance
guarantee (Scherrer et al., 2012; Canbolat & Rothblum,
2012). For instance, we have the following performance
bound, that is remarkably independent of the parameter m.2
1
There might be several policies that are greedy with respect
to v.
2
Note that in practice, the term k will generally depend on
m. The exact dependence may strongly depend on the precise
implementation and we refer the reader to (Scherrer et al., 2012)
for examples of such analyses. In this paper, we only consider the
situation of a uniform error bound on the errors, all the more that
extensions to more complicated errors is straightforward.

Theorem 1 (Scherrer et al. (2012, Remark 2)). Consider
AMPI with any parameter m ≥ 0. Assume there exists
an  > 0 such that the errors satisfy kk k∞ <  for all
k. Then, the loss due to running policy πk instead of the
optimal policy π∗ satisfies
kv∗ − vπk k∞ ≤

2(γ − γ k )
2γ k
+
kv∗ − v0 k∞ .
2
(1 − γ)
1−γ

In the specific case corresponding to AVI (m = 0) and API
(m = ∞), this bound matches performance guarantees that
have been known for a long time (Singh & Yee, 1994; Bert2γ
sekas & Tsitsiklis, 1996). The asymptotic constant (1−γ)
2
can be very big, in particular when γ is close to 1. Unfortunately, it cannot be improved: Bertsekas & Tsitsiklis (1996,
Example 6.4) showed that the bound is tight for PI, Scherrer
& Lesner (2012) proved that it is tight for VI,3 and we will
prove in this article4 the—to our knowledge unknown—
fact that it is also tight for AMPI. In other words, improving
the performance bound requires to change the algorithms.
Even though the theory of optimal control states that there
exists a stationary policy that is optimal, Scherrer & Lesner
(2012) recently showed that the performance bound of Theorem 1 could be improved in the specific cases m = 0 and
m = ∞ by considering variations of AVI and API that
build periodic non-stationary policies (instead of stationary
policies). Surprisingly, the Non-Stationary AVI (NS-AVI)
algorithm proposed there works almost exactly like AVI: it
builds the exact same sequence of value-policy pairs from
any initialization v0 (compare with AMPI with m = 0):
πk+1 = G(vk )

(greedy step)

vk+1 = Tπk+1 vk + k

(evaluation step)

The only difference is in what is output: while AVI would
return the last policy, say πk after k iterations, NS-AVI returns the periodic non-stationary policy πk,` that loops in
reverse order on the last ` generated policies:
πk,` = πk πk−1 · · · πk−`+1 πk πk−1 · · · πk−`+1 · · ·
{z
} |
{z
}
|
last ` policies

last ` policies

Following the policy πk,` means that the first action is selected by πk , the second one by πk−1 , until the `th one by
πk−`+1 , then the policy loops and the next actions are selected by πk , πk−1 , so on and so forth. Note that when
` = 1, we recover the output of AVI: the last policy πk that
is used for all actions.
3

Though the MDP instance used to show the tightness of the
bound for VI is the same as that for PI (Bertsekas & Tsitsiklis,
1996, Example 6.4), Scherrer & Lesner (2012) seem to be the
first to argue about it in the literature.
4
Theorem 4 page 4 with ` = 1.

Non-Stationary Approximate Modified Policy Iteration

To describe the other algorithm proposed by Scherrer &
Lesner (2012), Non-Stationary API (NS-API), we shall introduce the linear Bellman operator Tπk,` associated with
πk,` :
∀v ∈ RX , Tπk,` v = Tπk Tπk−1 . . . Tπk−`+1 v.
It is indeed straightforward to show that the value vπk,`
obtained by following πk,` is the unique fixed point
of Tπk,` .
Then, from any initial set of ` policies
(π0 , π−1 , . . . , π−`+1 ), NS-API generates the following sequence of value-policy pairs:

4. Main results
We are now ready to present the first contribution of this
paper. We shall introduce a new algorithm, Non-Stationary
AMPI (NS-AMPI), that generalizes NS-AVI and NS-API
(in the same way the standard AMPI algorithm generalizes standard AVI and API) and AMPI (in the same way
NS-VI and NS-PI respectively generalize AVI and API).
Given some free parameters m ≥ 0 and ` ≥ 1, an arbitrary value function v0 and an arbitrary set of ` − 1 policies π0 , π−1 , π−`+2 , consider the algorithm that builds a
sequence of value-policy pairs as follows:
πk+1 = G(vk )

vk = vπk,` + k
πk+1 = G(vk )

(evaluation step)

On the one hand, using these non-stationary variants may
require more memory since one must store ` policies instead of one. On the other hand, the following result shows
that this extra memory allows us to improve the performance guarantee.
Theorem 2 (Scherrer & Lesner (2012, Theorems 2 and 4)).
Consider NS-AVI or NS-API with any parameter l ≥ 0.
Assume there exists an  > 0 such that the errors satisfy
kk k∞ <  for all k. Then, the loss due to running the
non-stationary policy πk,` instead of the optimal policy π∗
satisfies
2(γ − γ k )
 + γ k g0 .
(1 − γ)(1 − γ ` )

2
where g0 = 1−γ
=
` kv∗ − v0 k∞ for NS-AVI or g0


v∗ − vπ  for NS-API.
0,` ∞
1−γ
For any ` ≥ 1, it is a factor 1−γ
` better than in Theol
m
1
rem 1. Using ` = 1−γ
yields5 an asymptotic perfor-

mance bound of 3.164γ
1−γ . which constitutes an improve1
ment of order O( 1−γ
), which is significant in typical situations where γ is close to 1.
5

vk+1 = (Tπk+1,` ) Tπk+1 vk + k .

(evaluation step)

(greedy step)

While computing the value vk requires (approximately)
solving the fixed point equation vπk,` = Tπk,` vπk,` of the
non-stationary policy πk,` made of the last ` computed policies, the new policy πk+1 that is computed in the greedy
step is (as usual) a simple stationary policy. After k iterations, similarly to NS-AVI, the algorithm returns the periodic non-stationary policy πk,` . Here again, setting ` = 1
provides the standard API algorithm.



v∗ − vπ  ≤
k,` ∞

(greedy step)
m

Using the facts that 1 − γ ≤ − log γ and log γ ≤ 0, we
1
1
have log γ ` ≤ log γ 1−γ ≤ − log
log γ = −1 hence γ ` ≤ e−1 .
γ
2
2
Therefore 1−γ ` ≤ 1−e−1 < 3.164.

While the greedy step is identical to that of all algorithms,
the evaluation step involves the non-stationary Bellman operator Tπk+1,` (composed with itself m times) that we introduced in the previous section, composed with the standard
Bellman operator Tπk+1 . As in NS-AVI and NS-API, after
k iterations, the output of the algorithm is the periodic nonstationary policy πk,` . For the values m = 0 and m = ∞,
it is easy to see that one respectively recovers NS-AVI and
NS-API. When ` = 1, one recovers AMPI (that itself generalizes the standard AVI and API algorithms, obtained if
we further set respectively m = 0 and m = ∞).
At this point, a natural question is whether the previous
sensitivity results extend to this more general setting. As
the following original result states, the answer is yes.
Theorem 3. Consider NS-AMPI with any parameters m ≥
0 and ` ≥ 1. Assume there exists an  > 0 such that the
errors satisfy kk k∞ <  for all k. Then, the loss due to
running policy πk,` instead of the optimal policy π∗ satisfies


v∗ − vπ  ≤
k,` ∞

2γ k
2(γ − γ k )

+
kv∗ − v0 k∞ .
(1 − γ)(1 − γ ` )
1−γ

Theorem 3 asymptotically generalizes both Theorem 1 for
` > 1 (the bounds match when ` = 1) and Theorem 2
for m > 0 (the bounds are very close when m = 0 or
m = ∞). As already observed for AMPI, it is remarkable
that this performance bound is independent of m.
The second main result of this article is that the bound of
Theorem 3 is tight, in the precise sense formalized by the
following theorem.
Theorem 4. For all parameter values m ≥ 0 and ` ≥ 1,
for all discount factor γ, for all  > 0, there exists an MDP
instance, an initial value function v0 , a set of initial policies
π0 , π−1 , . . . , π−`+2 and a sequence of error terms (k )k≥1
satisfying kk k∞ ≤ , such that for all iterations k, the
bound of Theorem 3 is satisfied with equality.

Non-Stationary Approximate Modified Policy Iteration

This theorem generalizes the (separate) tightness results for
PI (m = ∞, ` = 1) (Bertsekas & Tsitsiklis, 1996) and
for VI (m = 0, ` = 1) (Scherrer & Lesner, 2012), which
are the only results we are aware of. To our knowledge,
this result is new even for the standard AMPI algorithm (m
arbitrary but ` = 1), and for the non-trivial instances of NSVI (m = 0, ` > 1) and NS-PI (m = ∞, ` > 1) proposed
by Scherrer & Lesner (2012).
Since it is well known that there exists an optimal policy
that is stationary, our result—as well as those of Scherrer
& Lesner (2012)—suggesting to consider non-stationary
policies may appear strange. There exists, however, a
very simple approximation scheme of discounted infinitehorizon control problems—that has to our knowledge never
been documented in the literature—that sheds some light
on the deep reason why non-stationary policies may be
an interesting option. Given an infinite-horizon problem,
consider approximating it by a finite-horizon discounted
control problem by “cutting the horizon” after some sufficiently big instant T (that is assume there is no reward after time T ). Contrary to the original infinite-horizon problem, the resulting finite-horizon problem is non-stationary,
and has therefore naturally a non-stationary solution that is
built by dynamic programming in reverse order. Moreover,
it can be shown (Kakade, 2003, by adapting the proof of
Theorem 2.5.1) that solving this finite-horizon with VI with
a potential error of  at each iteration, will induce at most
T
PT −1
)
a performance error of 2 i=0 γ t  = 2(1−γ
1−γ . If we add
max

|r(s,a)|

s,a
the error due to truncating the horizon (γ T
),

 1−γ
1
we get an overall error of order O 1−γ  for a mem

1
ory T of the order of6 Õ 1−γ
. Though this approximation scheme may require a significant amount of memory
1
(when γ is close to 1), it achieves the same O( 1−γ
) improvement over the performance bound of AVI/API/AMPI
as NS-AVI/NS-API/NS-AMPI do. In comparison, the nonstationary algorithms with a fixed period ` can be seen as a
more flexible way to make the trade-off between the memory and the quality.

5. Proof sketches
We begin by considering Theorem 3. While the performance guarantee was obtained through three independent
proofs for NS-VI, NS-PI and AMPI, the more general setting that we consider here involves a totally unified proof,
which we describe in the remaining of this section.
We write Pk (resp. P∗ ) for the transition kernel Pπk (resp.
6
log

We use the fact that γ T K <

(1−γ)K


1−γ

with K =


1−γ

maxs,a |r(s,a)|
.
1−γ

⇔ T >

log

(1−γ)K


1
log γ

Pπ∗ ) induced by the stationary policy πk (resp. π∗ ). We
will write Tk (resp. T∗ ) for the associated Bellman operator. Similarly, we will write Pk,` for the transition kernel associated with the non-stationary policy πk,` and Tk,`
for its associated Bellman operator. For k ≥ 0 we define
the following quantities: bk = Tk+1 vk − Tk+1,` Tk+1 vk ,
sk = vk −vπk,` −k , dk = v∗ −vk +k , and lk = v∗ −vπk,` .
The last quantity, the loss lk of using policy πk,` instead of
π∗ is the quantitiy we want to ultimately upper bound.
The core of the proof consists in deriving the following recursive relations.
Lemma 1. The quantities bk , sk and dk satisfy:

bk ≤ γPk+1 (γ ` Pk,` )m bk−1 + (I − γ ` Pk,` )k ,
dk = γP∗ dk−1 − γP∗ k−1 +

(γ ` Pk,` )i bk−1 ,

i=0
∞
X
sk = (γ ` Pk,` )m
(γ ` Pk,` )j (Tk vk−1 − Tk,` Tk vk−1 ) .
j=0

Since  is a uniform upper-bound on the pointwise absolute
value of the errors |k |, the first inequality implies that bk ≤
O(), and as a result, the second and third inequalities gives
us dk ≤ O() and sk ≤ O(). This means that the loss
lk = dk + sk will also satisfy lk ≤ O() and the result
is obtained by taking the norm k · k∞ . The actual bound
given in the theorem requires a careful expansion of these
three inequalities where we make precise what we have just
hidden in the O-notations. The details of this expansion are
tedious and deferred to Appendix B of the Supplementary
Material. We thus now concentrate on the proof of these
relations.
Proof of Lemma 1. We will repeatedly use the fact that
since policy πk+1 is greedy with respect to vk , we have
∀π 0 , Tk+1 vk ≥ Tπ0 vk .

(2)

For a non-stationary policy πk,` , the induced `-step transition kernel is Pk,` = Pk Pk−1 · · · Pk−`+1 . As a consequence, for any function f : S → R, the operator Tk,` may
be expressed as: Tk,` f = rk + γPk,1 rk−1 + γ 2 Pk,2 rk−2 +
· · · + γ ` Pk,` f and, for any function g : S → R, we have
Tk,` f − Tk,` g = γ ` Pk,` (f − g)

(3)

Tk,` (f + g) = Tk,` f + γ ` Pk,` (g).

(4)

and
Let us now bound bk . We have
bk = Tk+1 vk − Tk+1,` Tk+1 vk
Eq.(2)

'

m−1
X

≤

Tk+1 vk − Tk+1,` Tk−`+1 vk

= Tk+1 vk − Tk+1 Tk,` vk
= γPk+1 (vk − Tk,` vk )

Non-Stationary Approximate Modified Policy Iteration

= γPk+1 ((Tk,` )m Tk vk−1

Lesner, 2012) and PI (Bertsekas & Tsitsiklis, 1996, Example 6.4). Precisely, this MDP consists of states {1, 2, . . . },
two actions: left (←) and right (→); the reward function r
and transition kernel P are characterized as follows for any
state i ≥ 2:

+k − Tk,` ((Tk,` )m Tk vk−1 + k ))
Eq.(4)

=

γPk+1 ((Tk,` )m Tk vk−1
−(Tk,` )m+1 Tk vk−1 + (I − γ ` Pk,` )k

Eq.(3)

=



γPk+1 (γ ` Pk,` )m (Tk vk−1

r(i, ←) = 0,


−Tk,` Tk vk−1 ) + (I − γ ` Pk,` )k

(γ ` Pk,` )m bk−1 + (I − γ ` Pk,` )k .

= γPk+1

P (i|i + 1, ←) = 1,

dk = v∗ − vk + k
= v∗ − (Tk,` )m Tk vk−1
= v∗ − Tk vk−1
m−1
X

(Tk,` )i Tk vk−1 − (Tk,` )i+1 Tk vk−1

i=0
Eq.(3)

=

+

T∗ v∗ − Tk vk−1
m−1
X

(γ ` Pk,` )i (Tk vk−1 − Tk,` Tk vk−1 )

i=0
Eq.(2)

≤

T∗ v∗ − T∗ vk−1 +

m−1
X

(γ ` Pk,` )i bk−1

i=0
Eq.(3)

=

γP∗ (v∗ − vk−1 ) +

m−1
X

(γ ` Pk,` )i bk−1

i=0

= γP∗ dk−1 − γP∗ k−1 +

m−1
X

(γ ` Pk,` )i bk−1 .

i=0

Finally, we prove the relation for sk :
sk = vk − vπk,` − k
= (Tk,` )m Tk vk−1 − vπk,`
= (Tk,` )m Tk vk−1 − (Tk,` )∞ Tk,` Tk vk−1
∞
X
= (γ ` Pk,` )m
(γ ` Pk,` )j (Tk vk−1 − Tk,` Tk vk−1 ) .

1

−2(γ + γ 2 )

−2γ

0

2

0

3

P (i + ` − 1|i, →) = 1,

In such a case, one can prove that the sequence of policies π1 , π2 , . . . , πk that are generated up to iteration k is
such that for all i ≤ k, the policy πi takes ← in all
states but i, where it takes →. As a consequence, a nonstationary policy πk,` built from this sequence takes → in
k (as dictated by πk ), which transfers the system into state
k + ` − 1 incurring a reward of rk . Then the policies
πk−1 , πk−2 , . . . , πk−`+1 are followed, each indicating to
take ← with 0 reward. After ` steps, the system is again
is state k and, by the periodicity of the policy, must again
use the action πk (k) = →. The system is thus stuck in
a loop, where every ` steps a negative reward of rk is received. Consequently, the value of this policy from state k
is:
γ − γk
rk
=−
2.
vπk,` (k) =
`
1−γ
(1 − γ)(1 − γ ` )
As a consequence, we get the following lower bound,


v∗ − vπ  ≥ |vπ (k)| =
k,` ∞
k,`

j=0

0

...
0

0

`

0

`+1

0

`+2

γ − γi
,
1−γ

and r(1) = 0 and P (1|1)1 for state 1 (all the other transitions having zero probability mass). As a shortcut, we will
use the notation ri for the non-zero reward r(i, →) in state
i. Figure 1 depicts the general structure of this MDP. It is
easily seen that the optimal policy π∗ is to take ← in all
states i ≥ 2, as doing otherwise would incur a negative reward. Therefore, the optimal value v∗ (i) is 0 in all states
i. The proof of the above theorem considers that we run
AMPI with v0 = v∗ = 0, π0 = π−1 = · · · = π`+2 = π∗ ,
and the following sequence of error terms:

 − if i = k,

if i = k + `,
∀i, k (i) =

0
otherwise.

We now turn to the bound of dk :

+

r(i, →) = −2

...
0

Figure 1. The deterministic MDP matching the bound of Theorem 3.

We now turn to the tightness results given in Theorem 4.
The proof considers a generalization of the MDP instance
used to prove the tightness of the bound for VI (Scherrer &

γ − γk
2
(1 − γ)(1 − γ ` )

which exactly matches the upper bound of Theorem 3
(since v0 = v∗ = 0). The proof of this result involves
computing the values vk (i) for all states i, steps k of the algorithm, and values m and ` of the parameters, and proving
that the policies πk+1 that are greedy with respect to these
values satisfy what we have described above. Due to lack
of space, the complete proof is deferred to Appendix B of
the Supplementary Material; in Lemma 7 and the associated Figures 4 and 5 there, note the quite complex shape
of the value function that is induced by the cyclic nature of
the MDP and the NS-AMPI algorithm.

Non-Stationary Approximate Modified Policy Iteration

6. Empirical Illustration
In this section, we describe an empirical illustration of the
new algorithm NS-AMPI. Note that the goal here is not to
convince the reader that the new degrees of freedom for approximate dynamic programming may be interesting in difficult real control problems—we leave this important question to future work—but rather to give some insight, on
small and artificial well-controlled problems, on the effect
of the main parameters m and `.
The problem we consider, the dynamic location problem
from Bertsekas & Yu (2012), involves a repairman moving
between n sites according to some transition probabilities.
As to allow him do his work, a trailer containing supplies
for the repair jobs can be relocated to any of the sites at
each decision epoch. The problem consists in finding a relocation policy for the trailer according the repairman’s and
trailer’s positions which maximizes the discounted expectation of a reward function.
Given n sites, the state space has n2 states comprising the
locations of both the repairman and the trailer. There are n
actions, each one corresponds to a possible destination of
the trailer. Given an action a = 1, . . . , n, and a state s =
(sr , st ), where the repairman and the trailer are at locations
sr and st , respectively, we define the reward as r(s, a) =
−|sr − st | − |st − a|/2. At any time-step the repairman
moves from its location sr < n with uniform probability to
any location sr ≤ s0r ≤ n; when sr = n, he moves to site
1 with probability 0.75 or otherwise stays. Since the trailer
moves are deterministic, the transition kernel is

 n−s1r +1 if sr < n
0
P ((sr , a)|(sr , st ), a) =
0.75
if sr = n ∧ s0r = 1

0.25
if sr = n ∧ s0r = n
and 0 everywhere else.
We evaluated the empirical performance gain of using nonstationary policies by implementing the algorithm using
random error vectors k , with each component being uniformly random between 0 and some user-supplied value
. The adjustable size (with n) of the state and actions
spaces allowed to compute an optimal policy to compare
with the approximate ones generated by NS-AMPI for all
combinations of parameters ` ∈ {1, 2, 5, 10} and m ∈
{1, 2, 5, 10, 25, ∞}. Recall that the cases m = 1 and
m = ∞ correspond respectively to the NS-VI and NS-PI,
while the case ` = 1 corresponds to AMPI. We used n = 8
locations, γ = 0.98 and  = 4 in all experiments.
Figure 2 shows the average value of the error v∗ − vπk,`
per iteration for the different values of parameters m and `.
For each parameter combination, the results are obtained by
averaging over 250 runs. While higher values of ` impacts
computational efficiency (by a factor O(`)) it always re-

Figure 2. Average error of policy πk,` per iteration k of NSAMPI. Red lines for ` = 1, yellow for ` = 2, green for ` = 5 and
blue for ` = 10.

sults with better asymptotic performance. Especially with
the lower values of m, a higher ` allows for faster convergence. While increasing m, this trend fades to be finally
reversed in favor of faster convergence for small `. However, while small ` converges faster, it is with greater error
than with higher ` after convergence. It can be seen that
convergence is attained shortly after the `th iteration which
can be explained by the fact that the first policies (involving
π0 , π−1 , . . . , π−`+2 ), are of poor quality and the algorithm
must perform at least ` iterations to “push them out” of
πk,` .
We conducted a second experiment to study the relative influence of the parameters ` and m. From the observation
that, in the very setting we are considering, the time complexity of an iteration of NS-AMPI can be roughly summarized by the number `m + 1 of applications of a stationary
policy’s Bellman operator, we ran the algorithm for fixed
values of the product `m and measured the asymptotic policy error for varying values of ` after 150 iterations. These
results are depicted on Figure 3. This setting gives insight
on how to set both parameters for a given “time budget”
`m. While runs with a lower ` are slightly faster to converge, higher values always give the best policies after a
sufficient number of iterations, and greatly reduces the variance across all runs, showing that non-stationarity adds robustness to the approximation noise. Regarding asymptotic
quality, it thus appears that the best setting is to favor ` instead of m.
Overall, both experiments confirm our theoretical analysis
that the main parameter for asymptotic quality is `. Regarding the rate of convergence, the first experiments sug-

Non-Stationary Approximate Modified Policy Iteration
`m = 10

36

`m = 20

25

the errors k are controlled in max norm. In practice, the
evaluation step of dynamic programming algorithm is usually done through some regression scheme—see for instance (Bertsekas & Tsitsiklis, 1996; Antos et al., 2007a;b;
Scherrer et al., 2012)—and thusq
controlled through the
R
f (x)µ(dx). Munos
L2,µ norm, defined as kf k2,µ =
(2003; 2007) originally developed such analyzes for AVI
and API. Farahmand et al. (2010) and Scherrer et al. (2012)
later improved them. Using a technical lemma due to
Scherrer et al. (2012, Lemma 3), one can easily deduce7
from our analysis (developed in Appendix A of the Supplementary Material) the following performance bound.

24

35

23

Policy error

Policy error

34

33

22

21

32
20
31

30

19

0

2

4

6

8

18

10

0

5

10

`
20

15

20

`

`m = 50

`m = 100

20

18

18

16

Policy error

Policy error

16

14

14

12

12
10
10

8

8

0

10

20

30

`

40

50

6

0

20

40

60

80

100

`

Figure 3. Policy error and standard deviation after 150 iterations
for different different values of `. Each plot represents a fixed
value of the product `m. Data is collected over 250 runs with
n = 8.

gests that too big values of ` may be harmful. In practice,
a schedule where ` progressively grows while m decreases
may provide the best compromise. Confirming this, as well
as studying approximate implementations designed for real
problems constitutes a matter for future investigation.

7. Conclusion
We have described a new dynamic-programming scheme,
NS-AMPI, that extends and unifies several state-of-the-art
algorithms of the literature: AVI, API, AMPI, NS-VI, and
NS-PI. NS-AMPI has two integer parameters: m ≥ 0 that
allows to move from a VI-style update to a PI-style update, and ` ≥ 1 that characterizes the period of the nonstationary policy that it builds. In Theorem 3, we have
provided a performance guarantee for this algorithm that
is independent of m and that improves when ` increases;
since ` directly controls the memory of the process, this allows to make a trade-off between memory and quality. In
the literature, similar upper bounds were only known for
AMPI (Scherrer et al., 2012)—` = 1 and m arbitrary—
and NS-AVI/NS-API (Scherrer & Lesner, 2012)—` arbitrary but m ∈ {0, ∞}. For most settings—` > 1 and
1 ≤ m < ∞—the result is new. By exhibiting a specially
designed MDP, we argued (Theorem 4) that our analysis is
tight. Similar lower bounds were only known for AVI and
API—` = 1 and m ∈ {0, ∞}. In other words, we have
generalized the scarce existing bounds in a unified setting
and closed the gap between the upper and lower bounds for
all values of m ≥ 0 and ` ≥ 1.
A practical limitation of Theorem 3 is that it assumes that

Corollary 1. Consider AMPI with any parameters m ≥ 0
and ` ≥ 1. Assume there exists an  > 0 such that the
errors satisfy kk k2,µ <  for all k. Then, the expected
(with respect to some initial measure ρ) loss due to running
policy πk,` instead of the optimal policy π∗ satisfies
Es∼ρ [v∗ (s) − vπk,` (s)] ≤

2(γ − γ k )C1,k,`
+O
(1 − γ)(1 − γ ` )



γk
1−γ



k−1 ∞

where Cj,k,l =

(1 − γ)(1 − γ l ) X X i+ln
γ
c(i + ln)
γj − γk
i=j n=i

is a convex combination of concentrability coefficients based on Radon-Nikodym
derivatives c(j) =

 d(ρPπ1 Pπ2 ···Pπj ) 
maxπ1 ,··· ,πj 
 .
dµ
2,µ

With respect to the previous bound in norm k · · · k∞ , this
bound involves extra constants Cj,k,l ≥ 1. Each such coefficient Cj,k,l is a convex combination of terms c(i), that
each quantifies the difference between 1) the distribution µ
used to control the errors and 2) the distribution obtained
by starting from ρ and making k steps with arbitrary sequences of policies. Overall, this extra constant can be
seen as a measure of stochastic smoothness of the MDP
(the smoother, the smaller). Further details on these coefficients can be found in (Munos, 2003; 2007; Farahmand
et al., 2010).
We have shown on a small numerical study the significant
influence of the parameter ` on the asymptotic quality of
approximately optimal controllers, and suggested that optimizing the speed of convergence may require a fine schedule between ` and m. Instantiating and analyzing specific implementations of NS-AMPI as was done recently
for AMPI (Scherrer et al., 2012), and applying them on
large domains constitutes interesting future work.
7

Precisely, Lemma 3 of (Scherrer et al., 2012) should be applied to Equation (8) page 15 in Appendix A of the Supplementary
Material.

,

Non-Stationary Approximate Modified Policy Iteration

References
Antos, A., Munos, R., and Szepesvári, C. Fitted Q-iteration
in continuous action-space MDPs. In NIPS, 2007a.
Antos, A., Szepesvári, C., and Munos, R. Value-iteration
based fitted policy iteration: learning with a single trajectory. In Approximate Dynamic Programming and Reinforcement Learning, 2007. ADPRL 2007, pp. 330–337.
IEEE, 2007b.
Bertsekas, D.P. and Tsitsiklis, J.N. Neuro-dynamic programming. Athena Scientific, 1996.
Bertsekas, D.P. and Yu, H. Q-learning and enhanced policy
iteration in discounted dynamic programming. Mathematics of Operations Research, 37(1):66–94, 2012.
Canbolat, P. and Rothblum, U. (Approximate) iterated successive approximations algorithm for sequential decision
processes. Annals of Operations Research, pp. 1–12,
2012. ISSN 0254-5330.
Farahmand, A.M., Munos, R., and Szepesvári, Cs. Error
propagation for approximate policy and value iteration
(extended version). In NIPS, 2010.
Gabillon, Victor, Ghavamzadeh, Mohammad, and Scherrer, Bruno. Approximate Dynamic Programming Finally
Performs Well in the Game of Tetris. In Neural Information Processing Systems (NIPS) 2013, South Lake Tahoe,
United States, December 2013.
Kakade, S.M. On the Sample Complexity of Reinforcement
Learning. PhD thesis, University College London, 2003.
Munos, R. Error bounds for approximate policy iteration. In International Conference on Machine Learning
(ICML), pp. 560–567, 2003.
Munos, R. Performance bounds in Lp -norm for approximate value iteration. SIAM Journal on Control and Optimization, 46(2):541–561, 2007.
Puterman, M.L. Markov decision processes: Discrete
stochastic dynamic programming. John Wiley & Sons,
Inc., 1994.
Puterman, M.L. and Shin, M.C. Modified policy iteration
algorithms for discounted Markov decision problems.
Management Science, 24(11):1127–1137, 1978.
Scherrer, B. and Lesner, B. On the Use of Non-Stationary
Policies for Stationary Infinite-Horizon Markov Decision Processes. In Advances in Neural Information Processing Systems 25, pp. 1835–1843, 2012.

Scherrer, B., Ghavamzadeh, M., Gabillon, V., and Geist,
M. Approximate Modified Policy Iteration. In Proceedings of the 29th International Conference on Machine
Learning (ICML-12), pp. 1207–1214, July 2012.
Singh, S. and Yee, R. An upper bound on the loss from approximate optimal-value functions. Machine Learning,
16-3:227–233, 1994.

