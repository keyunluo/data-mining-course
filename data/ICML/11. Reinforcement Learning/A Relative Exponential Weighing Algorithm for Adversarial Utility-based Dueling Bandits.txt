A Relative Exponential Weighing Algorithm for Adversarial Utility-based
Dueling Bandits

Pratik Gajane
Tanguy Urvoy
Fabrice Clérot
Orange-labs, Lannion, France

Abstract
We study the K-armed dueling bandit problem
which is a variation of the classical Multi-Armed
Bandit (MAB) problem in which the learner receives only relative feedback about the selected
pairs of arms. We propose an efficient algorithm
called Relative Exponential-weight algorithm for
Exploration and Exploitation (REX 3) to handle
the adversarial utility-based formulation of this
problem. We prove a finite time
p expected regret upper bound of order O( K ln(K)T ) for
this√algorithm and a general lower bound of order
Ω( KT ). At the end, we provide experimental
results using real data from information retrieval
applications.

1. Introduction
The K-armed dueling bandit problem is a variation of
the classical Multi-Armed Bandit (MAB) problem introduced by Yue and Joachims (2009) to formalize the exploration/exploitation dilemma in learning from preference
feedback. In its utility-based formulation, at each time period, the environment sets a bounded value for each of K
arms. Simultaneously the learner selects two arms and wagers that one of the arms will be better than the other. The
learner only sees the outcome of the duel between the selected arms (i.e. the feedback indicates which of the selected arms has better value) and receives the average of
the rewards of the selected arms. The goal of the learner is
to maximize her cumulative reward. The difficulty of this
problem stems from the fact that the learning algorithm has
no way of directly observing the reward of its actions. This
is a perfect example of partial monitoring problem as defined in Piccolboni and Schindelhauer (2001).
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

PRATIK . GAJANE @ ORANGE . COM
TANGUY. URVOY @ ORANGE . COM
FABRICE . CLEROT @ ORANGE . COM

Relative feedback is naturally suited to many practical applications like user-perceived product preferences, where a
relative perception: “A is better than B” is easier to obtain than its absolute counterpart: “A value is 42, B is
worth 33”. Another important application of dueling bandits comes from information retrieval systems where users
provide implicit feedback about the provided results. This
implicit feedback is collected in various ways e.g. a click
on a link, a tap, or any monitored action of the user. In all
these ways however, this kind of feedback is often strongly
biased by the model itself (the user cannot click on a link
which was not proposed).
To remove this bias in search engines, Radlinski and
Joachims (2007) propose to interleave the outputs of different ranking models: the model which scores a click wins
the duel. The accuracy of this interleave filtering method
was highlighted in several experimental studies (Radlinski
and Joachims, 2007; Joachims et al., 2007; Chapelle et al.,
2012).
Main contribution
The main contribution of this article is an algorithm designed for the adversarial utility-based dueling bandit problem in contrast to most of the existing algorithms which
assume a stochastic environment.
Our algorithm, called Relative Exponential-weight algorithm for Exploration and Exploitation (REX 3), is
a non-trivial extension of the Exponential-weight algorithm for Exploration and Exploitation (EXP 3) algorithm
(Auer et al., 2002b) to the dueling bandit problem. We
prove
p a finite time expected regret upper bound of order
O( K ln(K)T ) and develop an argument initially proposed by Ailon et√al. (2014) to exhibit a general lower
bound of order Ω( KT ) for this problem.
These two bounds correspond to the original bounds of
the classical EXP 3 algorithm
√ and the upper bound strictly
improves from the Õ(K T ) obtained by existing generic

Relative Exponential Weighing for Dueling Bandits

partial monitoring algorithms.1

2.2. Previous work on stochastic dueling bandits

Our experiments on information retrieval datasets show
that the anytime version of REX 3 is a highly competitive algorithm for dueling bandits, especially in the initial phases
of the runs where it clearly outperforms the state of the art.

The dueling bandits problem is recent, although related to
previous works on computing with noisy comparison (see
for instance Karp and Kleinberg, 2007). This problem also
falls under the framework of preference learning (Freund
et al., 2003; Liu, 2009; Fürnkranz and Hüllermeier, 2010)
which deals with learning of (predictive) preference models from observed (or extracted) preference information i.e.
relative feedback which specifies which of the chosen alternatives is preferred. Most of the articles hitherto published
on dueling bandits consider the problem under a stochastic
assumption.

Outline
This article is organized as follows: In section 2, we give a
brief survey on dueling bandits with section 2.3 dedicated
to the adversarial case. Most notations and formalizations
are introduced in this section. Secondly, in section 3, we
formally describe REX 3 with its pseudo-code. Then, in section 4.1, we provide the upper bound on the expected regret
of REX 3. Furthermore, in section 4.2, we provide the lower
bound on the regret of any algorithm attempting to solve
the adversarial utility-based dueling bandit problem. Section 5 begins with an empirical study of the bound given
in section 4.1. It then provides comparisons of REX 3 with
state-of-the art algorithms on information retrieval datasets.
The conclusion is provided in section 6.

2. Previous Work and Notations
The conventional MAB problem has been well studied in
the stochastic setting as well as the (oblivious) adversarial
setting (see Cesa-Bianchi and Lugosi, 2006; Bubeck and
Cesa-Bianchi, 2012). These MAB algorithms are designed
to optimize exploration and exploitation in order to control
the cumulative regret which is the difference between the
gain of a reference strategy and the actual gain of the algorithm.
2.1. Exponential-weight algorithm for Exploration and
Exploitation
Of particular interest are the Exponential-weight algorithm
for Exploration and Exploitation (EXP 3) and its variants
presented by Auer et al. (2002b) for the adversarial bandit
setting. For a fixed horizon T and K arms, the EXP 3 algorithm provides
an expected cumulative regret bound of orp
der O( K ln(K)T ) against the best single-arm strategy.
This algorithm is indeed adversarial because it does not
require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of
the horizon T to run properly. A “doubling trick” solution
is proposed by Auer et al. (2002b) to preserve the regret
bound when T is unknown. It consists of running EXP 3 in
a carefully designed sequence of increasing epochs. Another elegant solution was later proposed by Seldin et al.
(2012) for the same purpose.
1

The notation Õ(·) hides logarithmic factors.

Yue and Joachims (2009) propose an algorithm called Dueling Bandit Gradient Descent (DBGD) to solve a version
of the dueling bandits problem where context information
is provided. They approach (contextual) dueling bandits as
an on-line convex optimization problem.
Yue et al. (2012) propose an algorithm called Interleaved
Flitering (IF). Their formulation is stochastic and matrixbased: for each pair (i, j) of arms, there is an unknown
probability Pi,j for i to win against j. This preference matrix P of size K × K must satisfy the following symetry
property:
∀i, j ∈ {1, . . . , K},

Pi,j + Pj,i = 1

(1)

Hence on the diagonal: Pi,i = 12 ∀i ∈ {1, . . . , K}. Let
i∗ be the “best arm” (as we will see later, this best arm
coincides with the notion of Condorcet winner). Yue et al.
(2012) define the regret incurred at the time instant t when
arms a and b are pulled as:
0
ra,b
=

Pi∗ ,a + Pi∗ ,b − 1
2

∈ (0,

1
)
2

(2)

We will call this regret a Condorcet regret.
For the IF algorithm to work, the P matrix is expected to
satisfy several strong assumptions: strict linear ordering,
stochastic transitivity, and stochastic triangular inequality.
Under these three assumptions IF is guaranteed to suffer an
expected cumulative regret of order O(K log T ).
Yue and Joachims (2011) introduce Beat The Mean (BTM),
an algorithm which proceeds by successive elimination of
arms. This algorithm is less constrained than IF as it also
applies to a relaxed setting where the preference matrix can
slighlty violate the stochastic transitivity assumption. Its
cumulative regret bound is of order O(γ 7 K log T ) where
γ is here a known parameter.
Urvoy et al. (2013) propose a generic algorithm called SAVAGE (for Sensitivity Analysis of VAriables for Generic Exploration) which does away with several assumptions made
in the previous algorithms e.g. existence of inherent values of arms, existence of a linear order amongst arms. In

Relative Exponential Weighing for Dueling Bandits

this general setting, the SAVAGE algorithm obtains a regret
bound of order O(K 2 log T ). The key notions they introduce for dueling bandits are the Copeland, Borda and Condorcet scores (Charon and Hudry, 2010).PThe Borda score
K
of an arm i on a preference matrix P is j=1 Pi,j and its
PK
Copeland score is j=1 JPi,j > 12 K (We use J . . .K to denote the indicator function). If an arm has a Copeland score
of K − 1, which means that it defeats all the other arms in
the long run, it is called a Condorcet winner. The existence of a Condorcet winner is the minimum assumption
required for the Condorcet regret as defined on equation
(2) to be applicable. There exists however some datasets
like MSLR30K (2012) where this Condorcet condition is
not satisfied.

Note that if µi > µj for some arms i and j, then Pi,j > 21 .
The best arm in the usual bandit sense hence coincides with
the Condorcet winner (which turns out to be the Borda winner too) on the matrix formulation and the expected bandit
regret is twice the Condorcet regret as defined in (2):

Zoghi et al. (2014a) extend the Upper Confidence Bound
(UCB) algorithm (Auer et al., 2002a) and propose an algorithm called Relative Upper Confidence Bound (RUCB)
provided that the preference matrix admits a Condorcet
winner. They retrieve an O(K log T ) bound under this sole
assumption. Unlike the previous algorithms, RUCB is an
anytime dueling bandits algorithm since it does not require
the time horizon T as input.

2.3. Adversarial dueling bandits

Ailon et al. (2014) propose three methods (D OUBLER,
M ULTISMB, and S PARRING) to reduce the stochastic
utility-based dueling bandits problem to the conventional
MAB problem. A stochastic dueling bandits problem is
utility-based if the preference is the result of comparisons
of the individual utility/reward of the arms. This is a strong
restriction from the general – matrix-based – formulation
of the problem.
More formally, there are K probability distributions
v1 , . . . , vK over [0, 1] associated respectively with arms
1, . . . , K. Let µ1 , . . . , µK be the respective means of
v1 , . . . , vK . When an arm a is pulled, its reward/utility
xa is drawn from the corresponding distribution va 2 . Let
i∗ ∈ arg max µi be an optimal arm. The regret incurred at
the time instant t when arms a and b are pulled is defined
as:
2xi∗ − xa − xb
(3)
ra,b (t) =
2
We will call this regret a bandit regret. With randomized
tie-breaking we can rebuild the preference matrix:
Pi,j = P(xi > xj ) +

1
P(xi = xj )
2

When all vi are Bernoulli laws, this reduces to:
Pi,j =
2

µi − µj + 1
2

(4)

Note that we frequently drop the time index when it is unnecessary or clear from context. For instance we simply write xa (t)
or simply xa instead of xat (t).

Era,b =

2µi∗ − µa − µb
0
= Pi∗ ,a + Pi∗ ,b − 1 = 2ra,b
2

Several other models and algorithms have been proposed
since to handle stochastic dueling bandits. We can cite
(Busa-Fekete et al., 2013; 2014; Zoghi et al., 2014b; 2015).
See also (Busa-Fekete and Hüllermeier, 2014) for an extensive survey of this domain.

The bibliography on stochastic dueling bandits is flourishing, but the results about adversarial dueling bandits remain
quite scarce.
A utility-based formulation of the problem is however proposed in Ailon et al. (2014, section 6). In this setting, as in
classical adversarial MAB, the environment chooses beforehand an horizon T and a sequence of utility/reward vectors
x(t) = (x1 (t), . . . , xK (t)) ∈ [0, 1]K for t = 1, . . . , T .
The learning algorithm aims at controling the bandit regret
against the best single-arm strategy, as defined in (3), by
choosing properly the pairs of arms (i, j) to be compared.
To tackle this problem, Ailon et al. (2014) suggest to apply the S PARRING reduction algorithm, although originally
designed for stochastic settings, with an adversarial bandit algorithm like EXP 3 as a black-box MAB. According√to the authors, the S PARRING reduction preserves the
O( KT ln K) upper bound of EXP 3. This algorithm uses
two separate MABs (one for each arm). As a consequence,
when it gets a relative feedback about a duel (i, j), the
left instantiation of EXP 3 only updates its weight for arm
i while the right instantiation only updates for j. The algorithm we propose improves from this solution by centralizing information for both arms on a single weight vector.
As mentionned earlier, the dueling bandits problem is a
special instance of a partial monitoring game (Piccolboni
and Schindelhauer, 2001; Cesa-Bianchi and Lugosi, 2009;
Bartók et al., 2014). A partial monitoring game is defined
by two matrices: a loss matrix L and a feedback matrix
H. These two matrices are known by the learner. At each
round, the learner chooses an action a while the environment simultaneously chooses an outcome (say x). The
learner receives a feedback H(a, x) and suffers (in a blind
manner) a loss L(a, x).
It is straightforward to encode the classical MAB as a finite partial monitoring game: the actions are arms indices a ∈ {1, . . . , K} while the environment outcomes

Relative Exponential Weighing for Dueling Bandits

are reward vectors x(t) = (x1 (t), . . . , xK (t)) ∈ [0, 1]K .
The loss and feedback matrices are respectivly defined by
L(a, x) = −xa and H(a, x) = xa . For the utility-based
dueling bandits problem, the learner’s actions are the duels (a, b) ∈ {1, . . . , K}2 and the environment outcomes
are reward vectors. If we constrain the rewards to be binary it turns out to be a finite partial monitoring game. The
Loss matrix is defined by L ((a, b), x) = −(xa + xb )/2
and the feedback by H ((a, b), x) = ψ(xa − xb ) where ψ is
a non-decreasing transfer function such such that ψ(0) = 0
(usually ψ(x) = Jx > 0K or ψ(x) = x).

Several generic partial monitoring algorithms were recently
proposed for both stochastic and adversarial settings (see
Bartók et al., 2014, for details). If we except GLOBAL EXP 3 (Bartók, 2013) which tries to capture more finely the
structure of the games, these algorithms only focus on the
time bound and perform inefficiently when the number of
actions grows.

In a dueling bandit the number of non-duplicate actions
is actually K(K + 1)/2 and these algorithms,
including
√ 
GLOBALEXP 3, provide at best a Õ K T regret guarantee. The dedicated algorithm that we propose is using the
preference feedback more efficiently.

3. Relative Exponential-weight Algorithm for
Exploration and Exploitation
The pseudo-code for the algorithm we propose is given in
Algorithm 1. As previously stated on Section 2.3, this algorithm is designed to apply for the adversarial utility-based
dueling bandits problem.
It is similar to the original EXP 3 from step 1 to step 6
where it computes a distribution p(t) = (p1 (t), . . . , pK (t))
which
Pis a mixture of a normalized weighing of the arms
wi / i wi and a uniform distribution 1/K. As in EXP 3,
this uniform probability is introduced to ensure a minimum
exploration of all arms.
At step 7, the algorithm draws two arms a and b independently according to p(t). At step 8, the algorithm gets
ψ(xa − xb ) as relative feedback . Note that, since arms
are drawn with replacement, we may have a = b, in which
case the algorithm will get no information. This event is
indeed expected to become frequent when the p(t) distribution becomes peaked around the best arms. This necessity for a regret-minimizing dueling bandits algorithm to
renounce getting information when confident about its decision is a structural bias toward exploitation that is not encountered in classical bandits.
Step 8 is the big difference from EXP 3; because we only
have access to the relative ψ(xa − xb ) value, we have no
mean to estimate the individual rewards xa or xb . There

is however a solution to circumvent this problem: the best
arm in expectation at time t is not only the one which maximizes the absolute reward. It is indeed the one which maximizes the regret of any fixed strategy π(t) against it:

arg max xi (t) = arg max xi (t) − Ea∼π(t) xa .
i

i

This reference strategy could be a single-arm or uniform
strategy but playing a suboptimal strategy to get a reference
has a cost in terms of regret. One of our contributions is
to show that the algorithm may use its own strategy as a
reference.
At step 9, the condition a 6= b is only a slight improvement
for matrix-based dueling bandits where the outcome of a
duel of an arm against itself is randomized as in (4).
At steps 10 and 11, the weights of the played arms are updated. This update process is the core of our algorithm, it
will be detailed in Section 4.
Step 13 is only required for the anytime version of the algorithm. It will be explained in section 5.2.
Algorithm 1 REX 3: Exp3 with relative feedback
1: Parameters: γ ∈ (0, 1]
2: Initialization: wi (1) = 1 for i = 1, . . . , K.
3: for t = 1, 2, . . . do
4:
for i = 1, . . . , K do
γ
5:
Set pi (t) ← (1 − γ) PKwi (t)
+K
w (t)
j=1

6:
7:

8:
9:
10:

j

end for
Pull two arms a and b chosen independently according to the distribution (p1 (t), . . . , pK (t)).
Receive relative feedback ψ(xa − xb ) ∈ [−1, +1]
if a 6= b then
γ ψ(xa −xb )
2pa

Set wa (t + 1) ← wa (t) · e K
−

γ ψ(xa −xb )
2pb

11:
Set wb (t + 1) ← wb (t) · e K
12:
end if
13:
Update γ (for anytime version)
14: end for

4. Analysis
For the analysis, we focus on the simple case where ψ is the
identity. It provides a ternary win/tie/loss feedback if we assume binary rewards. The main difference between EXP 3
and our algorithm is at steps 10 and 11 of Algorithm 1,
where we update the weights according to the duel outcome: the winning arm is gratified while the loser is penalized. This ‘punitive’ approach of exponential weighing departs from EXP 3 and other weighing algorithms which gratify the most rewarding arms while kindly ignoring the nonrewarding ones (Freund and Schapire, 1999; Cesa-Bianchi
and Lugosi, 2006).

Relative Exponential Weighing for Dueling Bandits

4.1. Upper bound for REX 3
In this section, we provide a finite-horizon non-stochastic
upper bound on the expected regret against the best single
action policy.
The steps 10-11 on Algorithm 1 are equivalent to operating
for each arm i an update of the form:
wi (t + 1) = wi (t) · e

γ
K

ĉi (t)

Hence:

Corollary 2. When γ = min

1
2,

q

K ln(K)
τ

pected regret of REX 3 (Algorithm 1) is O

where
ψ(xa − xb )
ψ(xb − xa )
ĉi (t) = Ji = aK
+ Ji = bK
2pa
2pb

Substituting γ in Corollary 1 with its optimal value from
eq. (6) we obtain:
p
Gmax − E(Galg ) ≤ 2 K ln(K) [eGmax − (4−e) Gmin ]

(5)

One big difference with EXP 3 is that ĉi (t) not an estimator
of the reward xi (t). We instead have:

p



, the ex
K ln(K)T .

The upper bound of REX 3 for adversarial utility-based dueling bandits is hence the same as the one of EXP 3 for aversarial MABs. For a high-number of arms or a short term
horizon, this bound
 is competitive against the O (K ln(T ))
or O K 2 ln(T ) existing bounds for stochastic dueling
bandits.

Lemma 1.
E [ĉi (t)|(a1 , b1 ), .., (at−1 , bt−1 )] = Ea∼p(t) ψ(xi (t)−xa (t))
If ψ is the identity then Eĉi (t) = xi (t) − Ea∼p(t) xa (t) in
which case we estimate the expected instantaneous regret
of the algorithm against arm i. If we rather take ψ(x) =
Jx > 0K, then Eĉi (t) = Pa∼p(t) (xi (t) > xa (t)), i.e. the
probability for the algorithm to select an arm defeated by i.
PT
Let Gmax = maxi t=1 xi (t) be the best single-arm gain,
P
T
and let Galg = 12 t=1 xa (t) + xb (t) be the gain of the alPT PK
1
gorithm. Let EGunif = K
t=1
i=1 xi (t) be the average value of the game (i.e. the expected gain of the uniform
sampling strategy).
Theorem 1. If the transfer function ψ is the identity and
γ ∈ (0, 12 ), then,
Gmax − E(Galg ) ≤

K
ln(K) + γτ
γ

where τ = e · EGalg − (4−e) · EGunif .
We give a sketch of proof for this result in section A.
Provided that EGalg ≤ Gmax and EGunif ≥ Gmin ,
PT
where Gmin = mini t=1 xi (t) is the gain of the worst
single-arm strategy, we can simplify the bound into:
Corollary 1.
Gmax − EGalg

≤

Kln K
+ γ (eGmax − (4−e) Gmin )
γ

As in (Auer et al., 2002b, section 3), since K
γ ln(K) + γτ
is convex, we can obtain the optimal γ on (0, 12 ):
( r
)
1
K ln(K)
γ∗ = min
,
(6)
2
τ

4.2. Lower bound for dueling bandits algorithms
To provide a lower bound on the regret of any dueling bandits algorithm, we use a reduction to the classical MAB
problem suggested by Ailon et al. (2014).
Algorithm 2 Reduction to classical MAB
1: DBA.init()
2: Set t = 1
3: repeat
4:
(at , bt+1 ) ← DBA.decide()
5:
xat ← CBE.get reward()
6:
xbt+1 ← CBE.get reward()
7:
DBA.update((at , bt+1 ), (xat − xbt+1 ))
8:
t=t+2
9: until t ≥ T
Algorithm 2 gives an explicit formulation of this reduction by using a generic dueling bandits algorithm (DBA)
as a black-box having the following public sub-routines:
init(), decide() and update(). The classical bandit environment (CBE) provides get reward() which
returns the reward of the input arm. The expected classicalbandit gain of Algorithm 2 will be twice the expected gain
of the black-box dueling bandits it uses.
It is important to note that this reduction only works for
stochastic settings where the expected reward of each arm
remains the same across time instants. According to Theorem 5.1 given by Auer et al. (2002b, section 5), for
K ≥√2, the expected regret in the classical bandit√setting
is Ω( KT ) (assuming T is large enough i.e. T ≥ KT ).
Since this result is obtained with a stationary stochastic distribution, by extension, the expected √
regret in the dueling
bandits setting cannot be less than Ω( KT ).
Theorem 2. For any number of actions
√ K ≥ 2 and large
enough time horizon T (i.e. T ≥ KT ), there exists a

Relative Exponential Weighing for Dueling Bandits

5. Experiments
To evaluate REX 3 and other dueling bandits algorithms,
we have applied them to the online comparison of rankers
for search engines by interleaved filtering (Radlinski and
Joachims, 2007). A search-engine ranker is a function that
orders a collection of documents according to their relevancy to a given user search query. By interleaving the output of two rankers and tracking on which ranker’s output
the user did click, we are able to get an unbiased feedback
about the relative quality of these two rankers. Given K
rankers, the problem of finding the best ranker is indeed a
K-armed dueling bandits.
In order to obtain reproducible and comparable results, we
adopted the stochastic matrix-based experiment setup already employed by Yue and Joachims (2011); Zoghi et al.
(2014a;b; 2015) with both the cumulative Condorcet regret
as defined by Yue et al. (2012) and
P the accuracy i.e. the
best arm hit-rate over the runs: N1 n J(a, b) = (i∗ , i∗ )K.

This experimental setup uses real search engines’ logs to
build empirical preference matrices. The duel outcomes are
then simulated on these matrices. We used several preference matrices issued from namely: ARXIV dataset (Yue and
Joachims, 2011), LETOR NP 2004 dataset (Liu et al., 2007),
and MSLR 30 K dataset. The last dataset distinguishes three
kinds of queries: informational, navigational and perfecthit navigational (MSLR30K, 2012). These matrices are
courtesy of Zoghi et al. (2014b)’s authors.
5.1. Empirical validation of Corollary 1
We have used LETOR NP 2004 and MSLR 30 K datasets
(resricted to 64 rankers) to compare the average Condorcet
regret of 100 runs of REX 3 with T = 105 to the corresponding halved3 theoretical bounds from Corollary 1 for various values of γ. The results of this experiment are sumarized in Figure 1. We plotted two theoretical curves: one
with a conservative Gmax = T /2, and a riskier one with
Gmax = T /4. This experiment illustrates the dual impact
of the γ parameter on the exploration/exploitation tradeoff:
a low value reduces both the exploration and the reactivity
of the algorithm to unexpected feedbacks and a high value
tends to uniformize exploration while increasing reactivity. It also shows that the theoretical optimal γ ∗ we obtain
with Equation (6) is a good guess even with a conservative
upper-bound for Gmax .
3
As mentioned at the end of section 2.2, the utility-based bandit regret is indeed twice the Condorcet regret as defined in (2).

20
Random on NP2004
Random on MSLR Inf. (K=64)
Random on MSLR Nav. (K=64)
Rex3 on NP2004
Rex3 on MSLR Inf (K=64)
Rex3 on MSLR Nav. (K=64)
(K ln (K)/(2γ) + γ · e · T /2) /2
(K ln (K)/(2γ) + γ · e · T /4) /2

cumulative Condorcet regret ×10−3

distribution over assignments of rewards such that the expected cumulative regret of any utility-based
dueling ban√
dits algorithm cannot be less than Ω( KT ).

15

10

5

0

∗
0 γ

0.2

0.4

0.6

0.8

1

γ

Figure 1. Empirical validation of Corollary 1. The colored areas
around the curves show the minimal and maximal values over 100
runs.

5.2. Interleave filtering simulations
For our experiments we have considered the following state
of the art algorithms: BTM (Yue and Joachims, 2011) with
γ = 1.1 and δ = 1/T (explore-then-exploit setting),
Condorcet-SAVAGE (Urvoy et al., 2013) with δ = 1/T ,
RUCB (Zoghi et al., 2014a) with α = 0.51, and S PAR RING coupled with EXP 3 (Ailon et al., 2014). We also took
the uniform sampling strategy R ANDOM as a baseline. We
considered three versions of REX 3: two non-anytime versions where the optimal γ ∗ is computed beforehand according to (6) with Gmax set respectively to T /2 and T /10 and
one anytime version where γ ∗ is recomputed at each time
step according to (6) (see Seldin et al., 2012, for details
about this form of “doubling trick”).
A point which makes the comparison difficult is that some
algorithms are anytime while others require the horizon as
input. For anytime algorithms, namely R ANDOM, RUCB
and REX 3 with adaptive γ, we displayed the average over
100 runs of the progressive accumulation of regret while for
non-anytime algorithms, namely BTM, CSAVAGE, S PAR RING and other versions of REX 3, we displayed the average
over 50 runs of the final cumulative regret for several fixed
and known horizons. This protocol is slightly favorable
to non-anytime algorithms which benefit from more information. However, for elimination algorithms like BTM and
CSAVAGE the difference between the anytime regret and the
non-anytime regret is small. For adversarial algorithms like
S PARRING and REX 3 the “doubling trick” can be applied
to make them anytime: the adaptive γ version of REX 3 is
an example of such a fixed-to-anytime transformation.

Relative Exponential Weighing for Dueling Bandits

cumulative Condorcet regret

105

104

103

105

1

104
1

0.8

0.8
accuracy

accuracy

cumulative Condorcet regret

106
Random Policy
BTM (γ = 1.1, fixed T )
Sparring+exp3 (fixed T )
CSAVAGE (fixed T )
Rex3 (g = 1/2, fixed T )
Rex3 (g = 1/10, fixed T )
RUCB (α = 0.51, anytime)
Rex3 (adaptive γ, anytime)

0.6
0.4
0.2
0

0.6
0.4
0.2

103

104

105

106

107

0

108

105

106

time

107

108

time

Figure 2. Regret and accuracy plots averaged over 100 runs (50 runs for fixed-horizon algorithms) respectively √
on ARXIV dataset (6
rankers) and LETOR NP 2004 dataset (64 rankers). On regret plots, both time and regret scales are logarithmic ( t hence appears as
t/2). The colored areas around the curves show the minimal and maximal values over the runs.

18
16
14

105

12
10
8

4

10
1

6

0.8
accuracy

Random
CSAVAGE
RUCB
Sparring+exp3
Rex3
Rex3 (adaptive)

cumulative Condorcet regret ×10−3 at T = 105

cumulative Condorcet regret

106

4

0.6
0.4

2

0.2
0

105

106

107
time

108

0

20

40

60
80
100
K (mslr rankers)

120

140

Figure 3. On the left: average regret and accuracy plots on MSLR 30 K with navigational queries (K = 136 rankers). On the right: same
dataset, average regrets for a fixed T = 105 and K varying from 4 to 136.

Relative Exponential Weighing for Dueling Bandits

The results of these experiments are summarized in Figure 2, and 3. Furthermore, similar experiments are given as
extended material.
As expected, the adversarial-setting
algorithms S PAR √
RING and REX 3 follow an O( T ) regret curve while
the stochastic-setting algorithms follow an O(ln T ) curve.
Among the adversarial-setting algorithms, R EX 3 is shown
to outperform S PARRING on all datasets. In the long run,
adversarial-setting algorithms continue exploring and cannot compete in terms of regret against stochastic-setting
algorithms, but the accuracy curves show that the cost of
this exploration is very small. Moreover, for small horizons
or high number of rankers, REX 3 is extremely competitive
against other algorithms.
This difference is clearly illustrated on the left-hand side
of Figure 3 where we show the evolution of the expected
cumulative regret at a fixed time horizon (T = 105 ) according to the number of arms. To obtain this plot we averaged the regret over 50 runs. For each K and each run we
sampled uniformly K dimensions of the original 136×136
MSLR30 K navigational preference matrix.

The inequality ex ≤ 1 + x + (e − 2)x2 is tight for x ∈ [0, 1]
but it remains valid for negative values, hence:


Wt+1
γ 2 /K
≤1−
Wt
1−γ


 X

1 K

ĉi (t)

K

 i=1
{z
}
|
=−M1




+

(e − 2)γ 2 /K
1−γ


 X

1 K
2

p
(t)ĉ
(t)
i
i

K

 i=1
|
{z
}
=M2

As in EXP 3 we take the logarithm and sum over t. We get
for any j:
T
X
γ
γ 2 /K
(e − 2)γ 2 /K
ĉj (t) − ln(K) ≤
M1 +
M2
K
1−γ
1−γ
t=1

By taking the expectation over the algorithm’s randomization, we obtain for any j:
T
X
γ
E∼p ĉj (t) − ln(K) ≤
K | {z }
t=1

6. Conclusion

(8)

We proposed REX 3, an exponential weighing algorithm for
adversarial utility-based dueling bandits. We provided both
an upper and a lower bound for its expected cumulative
regret. These two bounds match the original bounds of
the classical EXP 3 algorithm. A thorough empirical study
on several information retrieval datasets has confirmed the
validity of these theoretical results. It also showed that
REX 3 and especially its anytime version with adaptive γ
are competitive solutions for dueling bandits, even when
compared to stochastic-setting algorithms in a stochastic
environment.

2

γ /K
1−γ

T
X

E∼p M1 +
| {z }
i=t
(9)

T
(e − 2)γ 2 /K X
E∼p M2
1−γ
| {z }
i=t

(7)

(10)

From Lemma 1 we directly get the expected regret against
j on the left side of the inequality:
E∼p ĉj (t) = xj − E∼p (xa )

(8)

By averaging (8) over the arms, we obtain:
E∼p(t) M1 = −

K
K
1 X
1 X
E∼p ĉi (t) = E(xa ) −
xi
K i=1
K i=1

(9)

A. Proof Sketch for Theorem 1

The following result is detailled in the extended version:

The general structure of the proof is similar to the one of
(Auer et al., 2002b, section 3), but, as explained before,
the ĉi (t) estimator we use differs from the one of EXP 3
because it gives an instantaneous regret estimate instead of
an absolute reward estimate. As such, it may reach negative
values and the wi (t) weights may decrease with time. We
only give here a sketch of proof, stressing on the differences
from (Auer et al., 2002b). An unfamiliar reader may refer
to the extended version for step-by-step details.

From Lemma 1, (9), (10), and by definition of Gmax ,
EGalg , and EGunif , the inequality (7) rewrites into:

Proof. Let Wt = w1 (t)+w2 (t)+· · ·+wK (t). As in EXP 3
proof we consider:

Assuming γ ≤

K

X pi (t) − γ/K
Wt+1
=
e(γ/K)ĉi (t)
Wt
1
−
γ
i=1

E∼p(t) M2 ≤

Gmax − EGalg −
+

K
1 X
1
E(xa ) +
xi
2
2K i=1

(10)

K ln K
γ
≤
(EGalg − EGunif )
γ
1−γ

(e−2)γ
(EGalg + EGunif )
2(1 − γ)
1
2

we finally obtain:

Gmax − EGalg ≤

K ln K
+ γ (eEGalg − (4−e) EGunif )
γ

Relative Exponential Weighing for Dueling Bandits

Acknowledgments
We would like to thank the reviewers for their constructive
comments and Masrour Zoghi who kindly sent us his experiment datasets.

References
Ailon, N., Karnin, Z. S., and Joachims, T. (2014). Reducing dueling bandits to cardinal bandits. In ICML 2014,
volume 32 of JMLR Proceedings, pages 856–864.
Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002a). Finitetime analysis of the multiarmed bandit problem. Mach.
Learn., 47(2-3):235–256.
Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E.
(2002b). The nonstochastic multiarmed bandit problem.
SIAM Journal on Computing, 32(1):48–77.
Bartók, G. (2013). A near-optimal algorithm for finite
partial-monitoring games against adversarial opponents.
In Proc. COLT.
Bartók, G., Foster, D. P., Pál, D., Rakhlin, A., and
Szepesvári, C. (2014). Partial monitoring - classification, regret bounds, and algorithms. Math. Oper. Res.,
39(4):967–997.
Bubeck, S. and Cesa-Bianchi, N. (2012). Regret analysis of stochastic and nonstochastic multi-armed bandit
problems. Foundations and Trends in Machine Learning, 5(1):1–122.
Busa-Fekete, R. and Hüllermeier, E. (2014). A survey of
preference-based online learning with bandit algorithms.
In ALT 2014, pages 18–39. Springer.
Busa-Fekete, R., Hüllermeier, E., and Szörényi, B. (2014).
Preference-based rank elicitation using statistical models: The case of mallows. In ICML 2014, JMLR Proceedings, pages 1071–1079.
Busa-Fekete, R., Szörényi, B., Cheng, W., Weng, P., and
Hüllermeier, E. (2013). Top-k selection based on adaptive sampling of noisy preferences. In ICML 2013, volume 28 of JMLR Proceedings, pages 1094–1102.
Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction,
Learning, and Games. Cambridge University Press,
New York, NY, USA.

Charon, I. and Hudry, O. (2010). An updated survey on
the linear ordering problem for weighted or unweighted
tournaments. Annals OR, 175(1):107–158.
Freund, Y., Iyer, R., Schapire, R. E., and Singer, Y. (2003).
An efficient boosting algorithm for combining preferences. J. Mach. Learn. Res., 4:933–969.
Freund, Y. and Schapire, R. E. (1999). Adaptive game playing using multiplicative weights. Games and Economic
Behavior, 29(1):79–103.
Fürnkranz, J. and Hüllermeier, E., editors (2010). Preference Learning. Springer-Verlag.
Joachims, T., Granka, L., Pan, B., Hembrooke, H., Radlinski, F., and Gay, G. (2007). Evaluating the accuracy of
implicit feedback from clicks and query reformulations
in web search. ACM Trans. Inf. Syst., 25(2).
Karp, R. M. and Kleinberg, R. (2007). Noisy binary search
and its applications. In SODA 2007, SIAM Proceedings,
pages 881–890.
Liu, T.-Y. (2009). Learning to rank for information retrieval. Found. Trends Inf. Retr., 3(3):225–331.
Liu, T.-Y., Xu, J., Qin, T., Xiong, W., and Li, H. (2007).
LETOR: Benchmark dataset for research on learning to
rank for information retrieval. In SIGIR 2007. ACM.
MSLR30K (2012). Microsoft learning to rank dataset.
Piccolboni, A. and Schindelhauer, C. (2001). Discrete
prediction games with arbitrary feedback and loss. In
COLT/EuroCOLT, volume 2111 of LNCS, pages 208–
223. Springer.
Radlinski, F. and Joachims, T. (2007). Active exploration
for learning rankings from clickthrough data. In KDD
2007, pages 570–579. ACM.
Seldin, Y., Szepesvári, C., Auer, P., and Abbasi-Yadkori,
Y. (2012). Evaluation and analysis of the performance
of the exp3 Algorithm in stochastic environments. In
EWRL, volume 24 of JMLR Proceedings, pages 103–
116.
Urvoy, T., Clerot, F., Féraud, R., and Naamane, S. (2013).
Generic exploration and K-armed voting bandits. In
ICML 2013, volume 28 of JMLR Proceedings, pages 91–
99.

Cesa-Bianchi, N. and Lugosi, G. (2009). Combinatorial
bandits. In COLT 2009, pages 237–246. Omnipress.

Yue, Y., Broder, J., Kleinberg, R., and Joachims, T. (2012).
The k-armed dueling bandits problem. J. Comput. Syst.
Sci., 78(5):1538–1556.

Chapelle, O., Joachims, T., Radlinski, F., and Yue, Y.
(2012). Large-scale validation and analysis of interleaved search evaluation. ACM Trans. Inf. Syst., 30(1):6.

Yue, Y. and Joachims, T. (2009). Interactively optimizing
information retrieval systems as a dueling bandits problem. In ICML 2009, pages 1201–1208. Omnipress.

Relative Exponential Weighing for Dueling Bandits

Yue, Y. and Joachims, T. (2011). Beat the mean bandit. In
ICML 2011, pages 241–248. Omnipress.
Zoghi, M., Whiteson, S., and de Rijke, M. (2015).
MergeRUCB: A method for large-scale online ranker
evaluation. In WSDM 2015. ACM.
Zoghi, M., Whiteson, S., Munos, R., and de Rijke, M.
(2014a). Relative upper confidence bound for the karmed dueling bandit problem. In ICML 2014, volume 32 of JMLR Proceedings, pages 10–18.
Zoghi, M., Whiteson, S. A., de Rijke, M., and Munos, R.
(2014b). Relative confidence sampling for efficient online ranker evaluation. In WSDM 2014, pages 73–82.
ACM.

