Scaling Up Robust MDPs using Function Approximation

Aviv Tamar
AVIVT @ TX . TECHNION . AC . IL
Electrical Engineering Department, The Technion - Israel Institute of Technology, Haifa 32000, Israel
Shie Mannor
SHIE @ EE . TECHNION . AC . IL
Electrical Engineering Department, The Technion - Israel Institute of Technology, Haifa 32000, Israel
Huan Xu
MPEXUH @ NUS . EDU . SG
Mechanical Engineering Department, National University of Singapore, Singapore 117575, Singapore

Abstract
We consider large-scale Markov decision processes (MDPs) with parameter uncertainty, under the robust MDP paradigm. Previous studies
showed that robust MDPs, based on a minimax
approach to handling uncertainty, can be solved
using dynamic programming for small to medium
sized problems. However, due to the ‚Äúcurse of dimensionality‚Äù, MDPs that model real-life problems are typically prohibitively large for such approaches. In this work we employ a reinforcement learning approach to tackle this planning
problem: we develop a robust approximate dynamic programming method based on a projected
fixed point equation to approximately solve large
scale robust MDPs. We show that the proposed
method provably succeeds under certain technical conditions, and demonstrate its effectiveness
through simulation of an option pricing problem.
To the best of our knowledge, this is the first attempt to scale up the robust MDP paradigm.

1. Introduction
Markov decision processes (MDPs) are standard models
for sequential decision making problems in stochastic dynamic environments (Puterman, 1994; Bertsekas & Tsitsiklis, 1996). Given the parameters, namely, transition probability and reward, the strategy that achieves maximal expected accumulated reward is considered optimal. However, in practice, these parameters are typically estimated
from noisy data, or even worse, they may change during
the execution of a policy. It is thus not surprising that the
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

actual performance of the chosen strategy can significantly
degrade from the model‚Äôs prediction due to such parameter
uncertainty ‚Äì the deviation of the model parameters from
the true ones (see experiments in Mannor et al. 2007).
To mitigate performance deviation due to parameter uncertainty, the robust MDP framework (Iyengar, 2005; Nilim
& El Ghaoui, 2005; Bagnell et al., 2001) is now a common method. In this context, it is assumed that the uncertain parameters can be any member of a known set (termed
the ‚Äúuncertainty set‚Äù), and solutions are ranked based on
their performance under the (respective) worst parameter
realizations. Under mild technical conditions, the optimal
solution of a robust MDP can be obtained using dynamic
programming, at least for small to medium sized MDPs.
This paper considers planning in large robust MDPs, a setting largely untouched in literature. It is widely known
that, due to the ‚Äúcurse of dimensionality‚Äù, practical problems modeled as MDPs often have prohibitively large statespaces, under which dynamic programming becomes intractable. Many approximation schemes have been proposed to alleviate the curse of dimensionality of large scale
MDPs, among them approximate dynamic programming
(ADP) is a popular approach (Powell, 2011). ADP considers approximations of the optimal value function, for example, as a linear functional of some features of the state, that
can be solved efficiently using a sampling based approach.
Of course, selecting good features is an art by itself. However, ADP has been used successfully in large-scale problems with hundreds of state dimensions (Powell, 2011). Inspired by the empirical success of ADP, we adapt it to the
robust MDP setting, and develop and analyze methods that
handle large scale robust MDPs. From a high level, we
indeed solve a planning problem via a reinforcement learning (RL; Sutton & Barto 1998) approach: while the robust
MDP model, the parameters, and the uncertainty sets are all
known, and hence the optimal solution is well defined, we
still use an RL approach to approximately find the solution

Scaling Up Robust MDPs using Function Approximation

due to the scale of the problem.
Our specific contributions are a framework for approximate solution of large-scale robust MDPs; algorithms for
approximate robust policy evaluation and policy improvement, with convergence proofs and error bounds; and an
application of our framework to an option trading domain.

definition implicitly assumes a rectangularity of the uncertainty set (Iyengar, 2005). In robust MDPs, one is typically
interested in maximizing the worst case performance. Formally, we define the robust value function (Iyengar, 2005;
Nilim & El Ghaoui, 2005) for a policy œÄ as its worst-case
value function
V œÄ (x) = inf V œÄ,P (x),
P ‚ààP

2. Background
We describe our problem formulation and some preliminaries from robust MDPs and ADP.
2.1. Robust Markov Decision Processes
For a discrete set B, let M(B) denote the set of probability measures on B, and let |B| denote its cardinality. A
Markov Decision Process (MDP; Puterman 1994) is a tuple {X , Z, U, P, r, Œ≥} where X is a finite set of states, Z
is a (possibly empty) set of absorbing terminal states, and
U is a finite set of actions. Also, r : X √ó U ‚Üí R is a
deterministic and bounded reward function, Œ≥ is a discount
factor, and P : X √ó U ‚Üí M(X ‚à™ Z) denotes the probability distribution of next states, given the current state and
action. We assume zero reward at terminal states.
A stationary policy œÄ : X ‚Üí M(U) maps each state to
a probability distribution over the actions. The value of
a state x under policy œÄ and state transition model P is
denoted V œÄ,P (x) and represents the expected sum of discounted returns when starting from that state and executing
œÄ,

#
" ‚àû

X

œÄ,P
œÄ,P
t
V
(x) = E
Œ≥ r(xt , ut ) x0 = x ,

t=0

œÄ,P

where E
denotes expectation w.r.t. the state-action distribution induced by the transitions P and the policy œÄ.
Note that for any terminal state z ‚àà Z and all œÄ and P
we have V œÄ,P (z) = 0.
Typically in MDPs, one is interested in finding a policy
that maximizes the value of certain (or all) states. When
the state space is small enough, and all the parameters are
known, efficient methods exist (Puterman, 1994). In practice, however, the state transition probabilities may not be
exactly known. A widely-applied approach in this setting is
the Robust MDP (RMDP; Nilim & El Ghaoui 2005; Iyengar 2005, also termed Ambiguous MDP). In this framework, the unknown transition probabilities are assumed to
lie in some known uncertainty set. Such a set may be obtained, for example, from statistical confidence intervals
when the transition probabilities are estimated from data.
Mathematically, an RMDP is a tuple {X , Z, U, P, r, Œ≥}
where X , Z, U, r, and Œ≥ are as defined for MDPs. The uncertainty set P, where P(x, u) ‚äÇ M(X ‚à™ Z), denotes a
known uncertainty in the state transitions. Note that this

‚àó
and we
optimal
 seek for the
	 robust value function V (x) =
œÄ,P
supœÄ inf P ‚ààP V
(x) . Iyengar (2005) and Nilim & El
Ghaoui (2005) showed that similarly to the regular value
function, the robust value function is obtained by a deterministic policy, and satisfies a (robust) Bellman recursion
of the form


V ‚àó (x) = sup r(x, u) + Œ≥ inf EP [V ‚àó (x0 )|x, u] ,
P ‚ààP

u‚ààU

where x0 denotes the state following the state x and action
u. Thus, in the sequel we shall only consider deterministic
policies, and write œÄ(x) as the action prescribed by policy
œÄ at state x.
Iyengar (2005) proposed a policy iteration algorithm for
the robust MDP framework. This algorithm repeatedly improves a policy œÄ by choosing greedy actions with respect
to V œÄ . The key step in this approach is therefore policy
evaluation: calculating V œÄ , which satisfies
V œÄ (x) = r(x, œÄ(x)) + Œ≥ inf EP [V œÄ (x0 )|x, œÄ(x)] . (1)
P ‚ààP

The non-linear equation (1) may be solved for V œÄ using an
iterative method as follows. Let us first write (1) in vector
notation. For some x and u we define the operator œÉP(x,u) :
R|X | ‚Üí R as

	
.
œÉP(x,u) v = inf p> v : p ‚àà P(x, u)] ,
where v ‚àà R|X | and, slightly abusing notation, we ignore
transitions to terminal states in P(x, u). Also, for some
policy œÄ let the operator œÉœÄ : R|X | ‚Üí R|X | be defined such
.
that {œÉœÄ v} (x) = œÉP(x,œÄ(x)) v. Then (1) may be written as
V œÄ = rœÄ + Œ≥œÉœÄ V œÄ . Let T œÄ : R|X | ‚Üí R|X | denote the
robust Bellman operator for a fixed policy, defined by
.
T œÄ v = rœÄ + Œ≥œÉœÄ v.
(2)
We see that V œÄ is a fixed point of T œÄ , i.e., V œÄ = T œÄ V œÄ .
Furthermore, since T œÄ is known to be a contraction in the
sup norm (Iyengar, 2005), V œÄ may be found by iteratively
applying T œÄ to some vector v.
The robust Bellman operator T : R|X | ‚Üí R|X | is defined
by
.
T v(x) = sup T œÄ v(x),
œÄ

and was shown to be a contraction (Iyengar, 2005), with
V ‚àó as its fixed point.

Scaling Up Robust MDPs using Function Approximation

2.2. Projected Fixed Point Equation Methods
For MDPs, when the state space is large, dynamic programming methods become intractable, and one has to resort to
an approximation procedure. A popular approach involves
a projection of the value function onto a lower dimensional
subspace by means of linear function approximation (Bertsekas & Tsitsiklis, 1996), and solving the solution of a projected Bellman equation. We briefly review this approach.
Assume a standard MDP setting without uncertainty, where
the Bellman equation (1) for a fixed policy is reduced to
œÄ
V œÄ (x) = r(x, œÄ(x)) + Œ≥EP V œÄ (x0 ), and let Treg
denote
the corresponding fixed policy Bellman operator. When the
state space is large, calculating V œÄ (x) for every x is prohibitively computationally expensive, and a lower dimensional approximation of V œÄ is sought. Consider the linear
approximation given by a weighted sum of features
VÃÉ œÄ (x) = œÜ(x)> w,

where Œ† is a projection operator onto the subspace spanned
by Œ¶ with respect to a d-weighted Euclidean norm. At this
point we only assume that d ‚àà R|X | is positive. Since there
œÄ
is no uncertainty, Treg
is a linear mapping, and Equation
(3) may be written in matrix form as follows
Œ¶> DŒ¶w = Œ¶> Dr + Œ≥Œ¶> DP œÄ Œ¶w,

(4)

where D = diag(d), and P œÄ ‚àà R|X |√ó|X | is the Markov
transition matrix induced by policy œÄ. Given Œ¶> DŒ¶,
Œ¶> Dr, and Œ¶> DP œÄ Œ¶, Eq. (4) may be solved for w either
by matrix inversion (Boyan, 2002), or iteratively (known as
Projected Value Iteration; PVI; Bertsekas 2012)
‚àí1

3. Robust Policy Evaluation
In this section we propose an extension of ADP to the robust setting. We do this as follows. First, we consider policy evaluation, and extend the projected fixed point equation (3) to the robust case, with the robust T œÄ operator reœÄ
placing Treg
. We discuss the conditions under which this
equation has a solution, and how it may be obtained. We
then propose a sampling based procedure to solve the equation for large state spaces, and prove its convergence. Finally, in Section 4, we will use our policy evaluation procedure as part of a policy improvement algorithm in the
spirit of LSPI (Lagoudakis & Parr, 2003), for obtaining an
(approximately) optimal robust policy.

x ‚àà X,

where œÜ(x) ‚àà Rk , k < |X | contains the features of state
x and w ‚àà Rk are the approximation weights. Let Œ¶ ‚àà
R|X |√ók denote a matrix with the feature vectors in its rows.
We assume that the features are linearly independent, i.e.,
rank(Œ¶) = k. A popular approach for finding w is by
solving the projected Bellman equation (Bertsekas, 2012),
given by
œÄ
VÃÉ œÄ = Œ†Treg
VÃÉ œÄ ,
(3)

wk+1 = Œ¶> DŒ¶

using Least Squares Policy Iteration (LSPI; Lagoudakis &
Parr 2003), which extends policy iteration to the function
approximation setting.


Œ¶> Dr + Œ≥Œ¶> DP œÄ Œ¶wk .

(5)

When d corresponds to the steady state distribution over
states for policy œÄ, the iterative procedure in (5) can be
œÄ
shown to converge using contraction properties of Œ†Treg
(Bertsekas, 2012). For a large state space, the terms in (5)
cannot be calculated explicitly. However, the strength of
this approach is that these terms may be sampled efficiently,
using trajectories from the MDP (Bertsekas, 2012).
Recall that our ultimate goal is policy improvement. For
a regular MDP, the policy evaluation procedure described
above may be combined with a policy improvement step

3.1. A Projected Fixed Point Equation
Throughout this section we consider a fixed policy œÄ. For
some positive d, let the projection operator Œ† be defined
as above. Consider the following projected robust Bellman
equation for a fixed policy
VÃÉ œÄ = Œ†T œÄ VÃÉ œÄ .

(6)

œÄ

Note that here, as opposed to (3), T is not necessarily linear, and hence it is not clear whether Eq. (6) has a solution
at all. We now show that under suitable conditions the operator Œ†T œÄ is a contraction and Equation (6) has a unique
solution. We consider two different cases, depending on
the existence of terminal states Z. Let œÄÃÇ, PÃÇ , and ŒæÀÜ represent a given policy, state transition probabilities, and initial
ÀÜ
state distribution, respectively. We let Pr(xt = j|œÄÃÇ, PÃÇ , Œæ)
denote the probability that the state at time t is j, given that
the states evolve according to a Markov chain with tranÀÜ In the
sitions PÃÇ , policy œÄÃÇ, and initial state distribution Œæ.
ÀÜ
sequel, œÄÃÇ, PÃÇ , and Œæ will be used to represent the exploration policy of the MDP in an offline learning setting. We
ÀÜ which also
make the following assumption on œÄÃÇ, PÃÇ , and Œæ,
defines the projection weights d.
Assumption 1. Either Z = ‚àÖ, and there exists positive
numbers dj such that
dj = lim Pr(xt = j|x0 = i, œÄÃÇ, PÃÇ ) ‚àÄi, j ‚àà X ,
t‚Üí‚àû

or Z 6= ‚àÖ, and the policy œÄÃÇ is proper (Bertsekas, 2012),
that is, for tÃÑ = |X |
Pr(xtÃÑ ‚àà Z|x0 = i, œÄÃÇ, PÃÇ ) > 0

‚àÄi ‚àà X ,

and all states have a positive probability of being visited.
In this case we let
‚àû
X
ÀÜ ‚àÄj ‚àà X .
dj =
Pr(xt = j|œÄÃÇ, PÃÇ , Œæ)
t=0

Scaling Up Robust MDPs using Function Approximation

Put simply, Assumption 1 requires that every state has a
positive probability of being visited, and defines dj as a
suitable occupation measure of state j.
The following assumption relates the transitions of the exploration policy and the (uncertain) transitions of the policy
under evaluation.
Assumption 2. There exists Œ≤ ‚àà (0, 1) such that
Œ≥P (x0 |x, œÄ(x)) ‚â§ Œ≤ PÃÇ (x0 |x, œÄÃÇ(x)), ‚àÄP ‚àà P, x ‚àà
X , x0 ‚àà X .
Assumption 2 may appear restrictive, especially when the
discount factor Œ≥ approaches 1. Unfortunately, it is necessary in the sense that without it Œ†T œÄ is not necessarily a
contraction (see supplementary material). We note that a
similar difficulty arises in off-policy RL (Bertsekas & Yu,
2009; Sutton et al., 2009), and our Assumption 2 is in fact
similar to an assumption of Bertsekas & Yu 2009. Nevertheless, although our algorithms in the sequel are motivated
by the contraction property of Œ†T œÄ , we show empirically
that our approach works in cases where Assumption 2 is
severely violated, therefore in practice it is not a serious
limitation.
Let k ¬∑ kd denote the d-weighted Euclidean norm, which is
well-defined due to Assumption 1. Our key insight is the
following proposition, which shows that under Assumption
2, the robust Bellman operator is a Œ≤-contraction in k ¬∑ kd .
Proposition 3. Let Assumptions 1 and 2 hold.
kT œÄ y ‚àí T œÄ zkd ‚â§ Œ≤ky ‚àí zkd for all y, z ‚àà R|X |
œÄ

Then

d

where in last equality we used the well-known result that
the state transition matrix PÃÇ œÄÃÇ is contracting in the dweighted Euclidean norm (Bertsekas, 2012).
The projection operator Œ† is known to be non-expansive
in the d-weighted norm (Bertsekas, 2012). This fact, and
Lemma 6.9 of Bertsekas & Tsitsiklis (1996) lead to the
following contraction property and error bound for the approximate robust value function VÃÉ œÄ :
Corollary 4. Let Assumptions 1 and 2 hold. Then the projected robust Bellman operator Œ†T œÄ is a Œ≤-contraction in
the d-weighted Euclidean norm. Furthermore, Eq. (6) has
a unique solution, and


1

 œÄ
kŒ†V œÄ ‚àí V œÄ kd .
VÃÉ ‚àí V œÄ  ‚â§
1‚àíŒ≤
d
The contraction property in Corollary 4 also suggests a
straightforward procedure for solving Equation (6) which
we describe next.
3.2. Robust Projected Value Iteration
Consider the robust equivalent of PVI for solving Eq. (6):
Œ¶wk+1 = Œ†T œÄ (Œ¶wk ) .

œÄ

Proof. Fix x ‚àà X , and assume that T y(x) ‚â• T z(x).
Choose some  > 0, and Px ‚àà P such that
EPx [ z(x0 )| x, œÄ(x)] ‚â§ inf EP [ z(x0 )| x, œÄ(x)] + . (7)
P ‚ààP

Also, note that by definition
inf EP [ y(x0 )| x, œÄ(x)] ‚â§ EPx [ y(x0 )| x, œÄ(x)] .

P ‚ààP

arbitrary, we have that |T œÄ y(x) ‚àí T œÄ z(x)|
‚â§
Œ≤EPÃÇ [ |y(x0 ) ‚àí z(x0 )| | x, œÄÃÇ(x)] for all x, and therefore




kT œÄ y ‚àí T œÄ zkd ‚â§ Œ≤ PÃÇ œÄÃÇ |y ‚àí z| ‚â§ Œ≤ ky ‚àí zkd ,

(8)

Now, we have
0 ‚â§ T œÄ y(x) ‚àí T œÄ z(x)
‚â§ (Œ≥EPx [ y(x0 )| x, œÄ(x)]) ‚àí (Œ≥EPx [ z(x0 )| x, œÄ(x)] ‚àí Œ≥)
= Œ≥EPx [ y(x0 ) ‚àí z(x0 )| x, œÄ(x)] + Œ≥
‚â§ Œ≤EPÃÇ [ |y(x0 ) ‚àí z(x0 )| | x, œÄÃÇ(x)] + Œ≥,
where the second inequality is by (7) and (8), and the last
inequality is by Assumption 2. Conversely, if T œÄ z(x) ‚â•
T œÄ y(x), following the same procedure we obtain 0 ‚â§
T œÄ z(x) ‚àí T œÄ y(x) ‚â§ Œ≤EPÃÇ [ |y(x0 ) ‚àí z(x0 )| | x, œÄÃÇ(x)] + Œ≥,
and we therefore conclude that |T œÄ y(x) ‚àí T œÄ z(x)| ‚â§
Œ≤EPÃÇ [ |y(x0 ) ‚àí z(x0 )| | x, œÄÃÇ(x)] + Œ≥. Since  was

(9)

The algorithm (9) may be written explicitly in matrix form
(see Bertsekas 2012) as
‚àí1 >

wk+1 = Œ¶> DŒ¶
Œ¶ Dr + Œ≥Œ¶> DœÉœÄ (Œ¶wk ) . (10)
We refer to the algorithm in (10) as robust projected value
iteration (RPVI). Note that a matrix inversion approach
would not be applicable here, as (10) is not linear due to
non-linearity of œÉœÄ (¬∑).
Corollary 4 guarantees that under Assumptions 1 and 2, the
iterates of (9) converge to the fixed point of Œ†T œÄ , and the
RPVI algorithm converges to the corresponding weights.
We emphasize that Assumption 2 is only a sufficient condition for convergence. As we show empirically in Section 5, the algorithm works in cases where Assumption 2
is severely violated, and in fact, we have not encountered
convergence issues in any of our experiments. Nevertheless, Assumption 2 does point out where things may go
wrong. This is important in practice, especially if the uncertainty set may be controlled to satisfy it. Finally, note
that for averager type function approximations (Gordon,
1995), such as non-overlapping grid tiles, kernel smoothing, and k-nearest-neighbor, Œ† contracts in the sup-norm.

Scaling Up Robust MDPs using Function Approximation

Since T œÄ also contracts in the sup-norm (Iyengar, 2005),
Œ†T œÄ contracts regardless of Assumption 2, and convergence of RPVI is guaranteed.
For a large state space, computing the terms in (10) exactly
is intractable. For this case we propose a sampling procedure for estimating these terms, as described next.
3.3. A Sampling Based Approach
When the state space is too large for the terms in Equation
(6) to be computed exactly, one may resort to a sampling
based procedure. This approach is popular in the RL and
ADP literature, and has been used successfully on problems with very large state spaces (Powell, 2011). Here, we
describe how it may be applied for the robust MDP setting.
Assume that we have obtained a long trajectory from
an MDP with transition probabilities PÃÇ , while following policy œÄ.
We denote this trajectory by
x0 , u0 , r0 , x1 , u1 , r1 , . . . , xN , uN , rN . The terms in (10)
may be estimated from the data by1
Œ¶>DŒ¶ ‚àº

N ‚àí1
N ‚àí1
1X
1X
œÜ(xt )œÜ(xt )>, Œ¶>Dr ‚àº
œÜ(xt )r(xt ,ut ),
N t=0
N t=0

and
Œ¶> DœÉœÄ (Œ¶wk ) ‚àº

N ‚àí1
1 X
œÜ(xt )œÉP(xt ,ut ) (Œ¶wk ).
N t=0

(11)

Using the law of large numbers, it may be proved2 that
these estimates converge with probability 1 to their respective terms in (10) as N ‚Üí ‚àû. Together with Corollary 4
we have the following convergence result. The straightforward proof is omitted.
Proposition 5. Let Assumptions 1 and 2 hold. Consider
the RPVI algorithm with the terms in (10) replaced by their
sampled counterparts (11). Then as N ‚Üí ‚àû and k ‚Üí
‚àû, wk converges with probability 1 to w‚àó , and Œ¶w‚àó is the
unique solution of (6).
3.4. Solving the Inner Problem
In Eq. (11), the calculation of each œÉP(xt ,ut ) (Œ¶wk ) in the
sum requires the solution of the inner problem:
X
inf
p(x)œÜ(x)> wk ,
(12)
p‚ààP(x,u)

x‚ààXr (x,u)

where Xr (x, u) denotes the set of reachable states from
(x, u) under all transitions in the set P(x, u). Solving Eq.
1

These estimates are for the case Z = ‚àÖ in Assumption 1.
Modifying these estimates for the case Z 6= ‚àÖ is straightforward,
along the lines of Chapter 7.1 of Bertsekas (2012).
2
The proof is similar to the case without uncertainty, detailed
by Bertsekas (2012).

(12) clearly requires a model ‚Äì i.e., access to the state transitions in P(x, u). Also, depending on the uncertainty set,
it may be computationally demanding. We now discuss
specific uncertainty sets for which Eq. (12) is tractable.
A natural class of models is constructed from empirical
state transitions xt ‚Üí xt+1 . Let pÃÇ denote the empirical transition frequencies from state x and action u (obtained by, e.g., historical observations of the system), and
consider
sets on the support of pÃÇ ofothe form P(x, u) =
n
>

p : Dist(p, pÃÇ) ‚â§ , p 1 = 1, p ‚â• 0 , where Dist(¬∑, ¬∑) is
some distance function and  > 0. The distance function
and confidence parameter  are typically related to statistical confidence regions about pÃÇ (Nilim & El Ghaoui, 2005).
For the case of the L1 distance, Strehl & Littman (2005)
solve Eq. (12) with complexity O(|pÃÇ| log |pÃÇ|). Iyengar
(2005) and Nilim & El Ghaoui (2005) propose efficient solutions for the Kullback-Liebler distance, and also for interval and ellipsoidal models. All of these methods scale
at least linearly with the number of elements in pÃÇ, which
in most practical scenarios is small compared to the cardinality of the state space, as it is bounded by the sample
size used to create pÃÇ. In the case of binary transitions, as
in our option pricing example of Section 5, performing the
minimization in (12) is trivial.
Nonetheless, some problems may involve very large, or
even continuous sets of reachable states. A natural model
for these cases is a set of parametric distributions. Let
pŒ∏ (x) denote a distribution on X parameterized by Œ∏.
We consider uncertainty sets of the form P(x, u) =
{pŒ∏ : Œ∏ ‚àà Œò}, where Œò is some convex set3 , and our goal
is solving


inf EpŒ∏ œÜ(x)> wk .
(13)
Œ∏‚ààŒò

We assume that we have access to a distribution pÃÉ(x) such
that pŒ∏ (x)/pÃÉ(x) is well defined for all x ‚àà X and Œ∏ ‚àà Œò.
Now, observe that (13) hmay be written as
i a Stochastic PropŒ∏ (x)
>
gram (SP): inf Œ∏‚ààŒò EpÃÉ pÃÉ(x) œÜ(x) wk . A standard solution to this SP is via the Sample Average Approximation (SAA; Shapiro & Nemirovski 2005), where Ns i.i.d.
samples xi ‚àº pÃÉ are drawn, and the following determinPNs pŒ∏ (xi )
>
istic problem is solved: inf Œ∏‚ààŒò N1s i=1
pÃÉ(xi ) œÜ(xi ) wk .
When the objective of the SP is convex, and under additional technical conditions on pÃÉ, pŒ∏ , and œÜ, efficient solution of (13) is guaranteed4 (Shapiro & Nemirovski, 2005).
An alternative to the SAA is to optimize (13) directly using stochastic mirror descent (Nemirovski et al., 2009), by
noting that an unbiased estimate of the gradient may be ob3
As a concrete example, consider a Gaussian distribution pŒ∏ =
N (Œ∏, 1), where Œò = [Œ∏‚àí , Œ∏+ ], is a confidence interval for the
maximum likelihood estimate of Œ∏ from historical data.
4
See the supplementary material for an explicit result.

Scaling Up Robust MDPs using Function Approximation

tained by sampling, using the likelihood ratio trick:




‚àáŒ∏ EpŒ∏ œÜ(x)> wk = EpŒ∏ ‚àáŒ∏ log pŒ∏ (x)œÜ(x)> wk .
An in-depth analysis of this approach is deferred to the full
version of this paper. In the supplementary material we
present a successful application of our method to a domain
with continuous state transitions, using the SAA method
described above.

4. Robust Approximate Policy Iteration
In this section we propose a policy improvement algorithm,
driven by the RPVI method of the previous section.
First,
let
us
introduce
the
stateœÄ
action
value
function
Q
(x,
u)
=
P‚àû
inf P ‚ààP EœÄ,P [ t=0 Œ≥ t r(xt , ut )| x0 = x, u0 = u] , which
is more convenient for applying the optimization step of
policy iteration than V œÄ (x). We assume linear function
approximation of the form QÃÉœÄ (x, u) = œÜ(x, u)> w, where
œÜ(x, u) ‚àà Rk is a state-action feature vector and w ‚àà Rk
is a parameter vector. Note that QœÄ (x, u) may be seen
as the value function of an equivalent RMDP with states
in X √ó U, therefore the policy evaluation algorithm of
Section 3 applies. Also, note that given some w, a greedy
‚àó
policy œÄw
(x) at state x with respect to that approximation
may be computed by
‚àó
œÄw
(x) = arg max œÜ(x, u)> w,

(14)

u

‚àó
(x)), and let Œ¶‚àów denote a
and we write œÜ‚àów (x) = œÜ(x, œÄw
matrix with œÜ‚àów (x) in its rows.

The Approximate Robust Policy Iteration (ARPI) algorithm is initialized with an arbitrary parameter vector w0 .
At iteration i + 1, we estimate the parameter wi+1 of the
greedy policy with respect to wi as follows. We initialize
Œ∏0 ‚àà Rk to some arbitrary value, and then iterate on Œ∏:
‚àí1 >

Œ∏j+1 = Œ¶>DŒ¶
Œ¶ Dr+Œ≥Œ¶>DœÉœÄ (Œ¶‚àówi Œ∏j ) , (15)
where the terms in (15) are estimated from
data
Eq.
11) according to Œ¶> DŒ¶ ‚àº
PN(cf.
‚àí1
1
>
Œ¶> Dr
‚àº
N Pt=0 œÜ(xt , ut )œÜ(xt , ut ) ,
N
‚àí1
1
>
>
‚àó
N Pt=0 œÜ(xt , ut )r(xt , ut ) , and Œ¶ DœÉœÄ (Œ¶wi Œ∏j ) ‚àº
N ‚àí1
1
‚àó
Note
that,
t=0 œÜ(xt , ut )œÉP(xt ,ut ) (Œ¶wi Œ∏j ).
N
similarly to Eq. (11), each term in the last sum
requires the
problem
P solution of the ‚àó following
>
inf p‚ààP(x,u) x‚ààXr (x,u) p(x)œÜ(x, œÄw
(x))
Œ∏
,
which
j
i
may be solved efficiently for the uncertainty sets discussed
above. After Œ∏ has converged, we set wi+1 to its final
value. In practice, we only iterate (15) for a few iterations5
and set wi+1 to the last value of Œ∏.
5
Due to the fast convergence of (15) in practice, we didn‚Äôt
employ more sophisticated stopping conditions.

For comparison, in standard LSPI (Lagoudakis & Parr,
2003) the iteration on Œ∏ is not needed, as the policy evaluation equation (3) is linear, and may be solved using a least
squares approach (LSTD; Boyan 2002). Computationally,
the contraction property of Corollary 4 guarantees a linear
convergence rate for the Œ∏ iteration, therefore the addition
of this step should not impact performance significantly.
Also, note that the computation of Œ¶> DŒ¶ and Œ¶> Dr only
needs to be done once.
For standard approximate policy iteration, a classical result
(Bertsekas, 2012) bounds the error (closeness to optimality) of the resulting policy by errors in policy evaluation
and policy improvement. We now extend this result to robust approximate policy iteration.
Consider a general approximate robust policy iteration
method that generates a sequence of policies {œÄi } and corresponding robust value functions {Vi } that satisfy
kVi ‚àí V œÄi k‚àû ‚â§ Œ¥, kT œÄi+1 Vi ‚àí T Vi k‚àû ‚â§ . (16)
The following extension of Proposition 2.5.8 of Bertsekas
(2012) bounds the error kV œÄi ‚àí V ‚àó k‚àû . The proof is based
on the contraction and monotonicity properties of T œÄ and
T , and detailed in the supplementary material.
Proposition 6. The sequence {œÄi } generated by the general approximate robust policy iteration algorithm (16) satisfies
 + 2Œ≥Œ¥
.
lim sup kV œÄi ‚àí V ‚àó k‚àû ‚â§
(1 ‚àí Œ≥)2
i‚Üí‚àû
Note that in the ARPI algorithm, since we are working with
state-action values, and solve the maximization in (14) explicitly, there are no errors in the policy improvement step.
We therefore have the following corollary
Corollary 7. Consider the ARPI algorithm (15), and de‚àó
note Qi (x, u) = œÜ(x, u)> wi and œÄi = œÄw
. If the sei‚àí1
œÄi
quence of value functions satisfy kQi ‚àí Q k‚àû ‚â§ Œ¥ for all
2Œ≥Œ¥
i, then lim supi‚Üí‚àû kQœÄi ‚àí Q‚àó k‚àû ‚â§ (1‚àíŒ≥)
2.
Corollary 7 suggests that the ARPI algorithm is fundamentally sound. We note that more general L2 -norm bounds
for approximate policy iteration were proposed by Munos
(2003), and extending them to the robust case requires further work. In addition, Kaufman & Schaefer (2012) provide bounds for robust policy iteration without function
approximation, but with errors in the calculation of the
œÉP(x,u) operator.

5. Applications
In this section we discuss applications of robust ADP. We
start with a discussion of optimal stopping problems. Then,
we present an empirical evaluation on an option trading domain ‚Äì a finite horizon continuous state space optimal stopping problem, for which an exact solution is intractable.

Scaling Up Robust MDPs using Function Approximation

An optimal stopping problem is an RMDP where the only
choice is when to terminate the process. Formally, the action set is binary U = {0, 1}, and executing u = 1 from any
state always transitions to a terminal state with probability
1 (and no uncertainty). Let œÄÃÇ denote a policy that never
chooses to terminate, i.e., œÄÃÇ(x) = 0, ‚àÄx. In the supplementary material we show that if Assumption 2 is satisfied for
œÄ = œÄÃÇ, then it is immediately satisfied for all other policies.
While this does not ease the conditions that Assumptions 2
places on the uncertainty set and discount factor, it simplifies the design of a suitable exploration policy.
5.1. Option Trading
In this section we apply ARPI to the problem of trading American-style options. An American-style put (call)
option (Hull, 2006) is a contract which gives the owner
the right, but not the obligation, to sell (buy) an asset at a specified strike price K on or before some maturity time T . Letting the state xt represent the price
of the asset at time t ‚â§ T , the immediate payoff of
executing a put option at that time is gput (xt ), where
.
gput (x) = max (0, K ‚àí x), whereas for a call option we
.
have gcall (x) = max (0, x ‚àí K). Assuming Markov state
transitions, an optimal execution policy may be found by
solving a finite horizon optimal stopping problem; however, since the state space is typically continuous, an exact
solution is infeasible. Even calculating the value of a given
policy, an important goal by itself, is challenging. Previous
studies (Tsitsiklis & Van Roy, 2001; Li et al., 2009) have
proposed RL solutions for these tasks, and shown their utility. Here we extend this approach.
One challenge of option investments is that the underlying model is never truly known, but only accessed through
historical data, in the form of state trajectories (e.g., stock
prices over time). Catering for risk-averse traders, we plan
policies based on the worst-case model that fits the data.
In the following we show that option trading may be formulated as an RMDP, and then present our results of applying
the ARPI algorithm to the problem. We consider three different scenarios: a simple put option, a combination of a
put and a call, and a case of model misspecification.
5.1.1. A N RMDP F ORMULATION
The option pricing problem may be formulated as an
RMDP as follows. To account for the finite horizon, we
include time explicitly in the state, thus, the state at time
t is {xt , t}. The action is binary, where 1 stands for executing the option and 0 for continuing to hold it. Once an
option is executed, or when t = T , a transition to a terminal state takes place. Otherwise, the state transitions to
{xt+1 , t + 1} where xt+1 is determined by a stochastic kernel PÃÇ (x0 |x, t). The reward for executing u = 1 at state x

is g(x) and zero otherwise. We have g(x) = gput (x) for
a put option, g(x) = gcall (x) for a call option, or some
combination of them for a mixed investment.
Note that the state-action values for execution is known
in advance, for we have Q({x, t}, u = 1) = g(x) by
definition. Therefore, we only need to estimate the value
of not exercising the option. We use linear function approximation QÃÉœÄ ({x, t}, u = 0) = œÜ({x, t})> w, and the
ARPI update equation (15) in this case may be written as
‚àí1

Œ∏j+1 = Œ¶> DŒ¶
Œ≥Œ¶> DœÉœÄ (ŒΩ) , where ŒΩ(x, t) equals
g(x) if g(x) > œÜ({x, t})> wi , and equals œÜ({x, t})> Œ∏j otherwise. As our features we chose 2-dimensional (for x and
t) radial basis functions (RBF).6
The parameters for the experiments are provided in the supplementary material, and were chosen to balance the different factors in the problem. Most importantly, we chose
Œ≥ = 0.98 and a large uncertainty set such that Assumption
2 is severely violated. We did not, however, encounter any
convergence problems, indicating that our method works
well beyond the limits of Assumption 2. The Matlab code
for these results is provided in the supplementary material.
5.1.2. T RADING WITH A P UT O PTION
Here we consider a simple put option, where K is equal
to the initial price x0 . Our price fluctuation model M fol7
lows
( a Bernoulli distribution (Cox et al., 1979), xt+1 =
fu xt , w.p. p
, where the up and down factors, fu
fd xt , w.p. 1 ‚àí p
and fd , are constant. Our empirical evaluation proceeds as
follows. In each experiment, we generate Ndata trajectories of length T from the true model M . From these trajectories we form the maximum likelihood estimate of the
up probability pÃÇ, and the 95% confidence intervals pÃÇ‚àí and
pÃÇ+ using the Clopper-Pearson method (Clopper & Pearson,
1934), which constructs our uncertain model Mrobust . We
also build a model without uncertainty Mnominal by setting
pÃÇ‚àí = pÃÇ+ = pÃÇ. Using pÃÇ, we then simulate Nsim trajectories
of length T (this corresponds to a policy that never executes the option), where x0 = K + , and  is uniformly
distributed in [‚àíŒ¥, Œ¥]. These trajectories are used as input
data for the ARPI algorithm of Section 4.
Let œÄrobust and œÄnominal denote the policies found by
ARPI using Mrobust and Mnominal , respectively. We evaluate the performance of œÄrobust and œÄnominal using Ntest
6
In comparison, Li et al. (2009) used Laguerre polynomials for
x and several monotone functions for t. We observed significantly
better performance with the RBFs. We attribute this to the nonseparable (in x and t) nature of the value function, a property that
is not captured by the representation of Li et al. (2009).
7
Similar results were obtained with a geometric Brownian motion model, using the SAA method for solving the inner problem.
These results are provided in the supplementary material.

Scaling Up Robust MDPs using Function Approximation

Figure 1. Performance of robust vs. nominal policies. A,C,E: The tail distribution (complementary cumulative distribution function) of
the total reward R for the put option (A), put and call (C) and model misspecification (E) scenarios. Note that a higher value for some
a indicates a higher chance of guaranteeing a reward of at least a, therefore the plots (A) and (C) display a risk-sensitive behavior of
the robust policies. The results were obtained from 100 independent experiments. B: The nominal and robust policies for the put option
scenario, represented by the exercise boundary for each t. D: The reward g(x) and value function QÃÉ(x, t = 5) from a typical experiment
of the put and call option scenario.

trajectories obtained from the true model M . Recall that
we seek risk-averse policies; thus, the advantage of œÄrobust
should reflect in the least favorable outcomes. In Figure 1A
we plot the tail distribution of the total reward R (from 100
experiments) obtained by œÄrobust and œÄnominal . It may be
seen that œÄrobust has a lower probability of obtaining a low
payoff (or losing the investment). This, however, comes at
a cost of a smaller probability for a high payoff. To the risksensitive investor, such results are important. In Figure 1B
we further illustrate the policies œÄrobust and œÄnominal by
plotting the exercise boundary (the lowest price for which
the policy decides to exercise) for each t. The conservative
behavior of œÄrobust is evident.
5.1.3. T RADING WITH A P UT AND A C ALL
We now consider a more complicated scenario, where
the trader has bought both a put option, with strike price
Kput < x0 , and a call option, with strike Kcall > x0 .
The reward is given by g(x) = gput (x) + gcall (x), and the
models and experimental procedure are the same as in the
previous scenario. In Figure 1C we plot the tail distribution
of the total reward (from 100 independent experiments) obtained by œÄrobust and œÄnominal . Notice that the risk-averse
policy has a significantly smaller chance of losing the investment. In Figure 1D we display the reward g(x) and
the (approximate) value functions QÃÉœÄrobust and QÃÉœÄnominal
from a typical experiment, for t = 5. The robust value
function is important by itself, as it holds valuable information about the expected future profit.

p(x) = p1 1 {x ‚â§ Œ±} + p2 1 {x > Œ±}, where the threshold
Œ± is (Kput + Kcall )/2. However, let the estimated models
Mrobust and Mnominal , and the experimental procedure remain as before. We consider again the case of both a put
and a call option, as in Section 5.1.3. In Figure 1E we plot
the tail distribution of the total reward (from 100 independent experiments) obtained by œÄrobust and œÄnominal . Observe that in this case, the misspecification of the nominal
model led to a policy that is dominated by the robust policy,
which was less affected by this problem.

6. Conclusion and Future Work
We presented a novel framework for solving large-scale
uncertain Markov decision processes. To the best of our
knowledge, such problems are beyond the capabilities of
previous studies, which focused on exact solutions and
hence suffer from the ‚Äúcurse of dimensionality‚Äù. We presented both formal guarantees and empirical evidence to
the usefulness of our approach. As we demonstrated, uncertain MDPs are suitable for both risk-averseness and mitigation of model misspecification, indicating their importance for decision making under uncertainty.
Interestingly, as was recognized by Iyengar (2005), results
on robust MDPs may also be extended to their ‚Äòbest-case‚Äô
counterpart, known as optimistic MDPs8 . Such are useful for efficient exploration, as in the UCRL2 algorithm
(Jaksch et al., 2010), suggesting a future extension of our
work.

5.1.4. ROBUSTNESS TO M ODEL M ISSPECIFICATION

Acknowledgments

In the previous scenarios we assumed that our estimated
models, Mrobust and Mnominal , are the same as the true
model M . In practice, this is rarely the case, and one
has to consider the possibility of model misspecification.
An RMDP model provides some robustness against model
misspecification, as we now demonstrate. Let the probability p in the true model M depend on the state according to

The research leading to these results has received funding
from the European Research Council under the European
Union‚Äôs Seventh Framework Program (FP/2007-2013) /
ERC Grant Agreement n. 306638. H. Xu is partially supported by the Ministry of Education of Singapore through
AcRF Tier Two grant R-265-000-443-112.
8

See the supplementary material for more details.

Scaling Up Robust MDPs using Function Approximation

References
Bagnell, A., Ng, A., and Schneider, J. Solving uncertain
Markov decision problems. Technical Report CMU-RITR-01-25, Carnegie Mellon University, August 2001.
Bertsekas, D. P. Dynamic Programming and Optimal Control, Vol II. Athena Scientific, fourth edition, 2012.
Bertsekas, D. P. and Tsitsiklis, J. N. Neuro-Dynamic Programming. Athena Scientific, 1996.
Bertsekas, D. P. and Yu, H. Projected equation methods for
approximate solution of large linear systems. Journal of
Computational and Applied Mathematics, 227(1):2750,
2009.
Boyan, J. A. Technical update: Least-squares temporal
difference learning. Machine Learning, 49(2):233‚Äì246,
2002.
Clopper, C. J. and Pearson, E. S. The use of confidence
or fiducial limits illustrated in the case of the binomial.
Biometrika, 26(4):404‚Äì413, 1934.
Cox, J. C., Ross, S. A., and Rubinstein, M. Option pricing:
A simplified approach. Journal of financial Economics,
7(3):229‚Äì263, 1979.
Gordon, G. J. Stable function approximation in dynamic
programming. In Proceedings of the 12th International
Conference on Machine Learning, 1995.

Munos, R. Error bounds for approximate policy iteration.
In Proceedings of the 20th International Conference on
Machine Learning, pp. 560‚Äì567, 2003.
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
Robust stochastic approximation approach to stochastic
programming. SIAM Journal on Optimization, 19(4):
1574‚Äì1609, 2009.
Nilim, A. and El Ghaoui, L. Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780‚Äì798, 2005.
Powell, W. B. Approximate Dynamic Programming. John
Wiley and Sons, 2011.
Puterman, M. L. Markov decision processes: discrete
stochastic dynamic programming. John Wiley & Sons,
Inc., 1994.
Shapiro, A. and Nemirovski, A. On complexity of stochastic programming problems. In Continuous optimization,
pp. 111‚Äì146. Springer, 2005.
Strehl, A. L. and Littman, M. L. A theoretical analysis of
model-based interval estimation. In Proceedings of the
22nd international conference on Machine learning, pp.
856‚Äì863. ACM, 2005.
Sutton, R. S. and Barto, A. G. Reinforcement Learning: An
Introduction. MIT Press, 1998.

Iyengar, G. N. Robust dynamic programming. Mathematics of Operations Research, 30(2):257‚Äì280, 2005.

Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S.,
Silver, D., Szepesvari, C., and Wiewiora, E. Fast
gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings
of the 26th Annual International Conference on Machine
Learning, 2009.

Jaksch, T., Ortner, R., and Auer, P. Near-optimal regret
bounds for reinforcement learning. Journal of Machine
Learning Research, 11:1563‚Äì1600, 2010.

Tsitsiklis, J. N. and Van Roy, B. Regression methods for
pricing complex American-style options. Neural Networks, IEEE Transactions on, 12(4):694‚Äì703, 2001.

Hull, J. C. Options, Futures, and Other Derivatives (6th
edition). Prentice Hall, 2006.

Kaufman, D. L. and Schaefer, A. J. Robust modified policy
iteration. INFORMS Journal on Computing, 2012.
Lagoudakis, M. G. and Parr, R. Least-squares policy iteration. The Journal of Machine Learning Research, 4:
1107‚Äì1149, 2003.
Li, Y., Szepesvari, C., and Schuurmans, D. Learning exercise policies for American options. In Proc. of the 12th
International Conference on Artificial Intelligence and
Statistics, JMLR: W&CP, volume 5, pp. 352‚Äì359, 2009.
Mannor, S., Simester, D., Sun, P., and Tsitsiklis, J. N. Bias
and variance approximation in value function estimates.
Management Science, 53(2):308‚Äì322, 2007.

