GeNGA: A Generalization of Natural Gradient Ascent with Positive and
Negative Convergence Results

Philip S. Thomas
School of Computer Science, University of Massachusetts, Amherst, MA 01003 USA

Abstract
Natural gradient ascent (NGA) is a popular optimization method that uses a positive definite metric tensor. In many applications the metric tensor is only guaranteed to be positive semidefinite
(e.g., when using the Fisher information matrix
as the metric tensor), in which case NGA is not
applicable. In our first contribution, we derive
generalized natural gradient ascent (GeNGA),
a generalization of NGA which allows for positive semidefinite non-smooth metric tensors. In
our second contribution we show that, in standard settings, GeNGA and NGA can both be divergent. We then establish sufficient conditions
to ensure that both achieve various forms of convergence. In our third contribution we show how
several reinforcement learning methods that use
NGA without positive definite metric tensors can
be adapted to properly use GeNGA.

1. Introduction
Natural gradient ascent (NGA) is a popular method for
finding local maxima of a smooth function. According to
Google Scholar, the paper introducing NGA (Amari, 1998)
has over 1,600 citations from a wide range of fields, which
hints at the popularity and impact of NGA. With its breadth
of applications, it is not surprising that the assumptions
made in the original derivation of NGA are not always
satisfied. One example of this is that NGA assumes that
the domain of the function being optimized is a Riemannian manifold. In some applications the domain is a semiRiemannian manifold, but not necessarily a Riemannian
manifold. Our first contribution is generalized natural gradient ascent (GeNGA), which relaxes the assumptions of
NGA to allow for domains that are (possibly non-smooth)
semi-Riemannian manifolds.
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

PTHOMAS @ CS . UMASS . EDU

Despite the popularity of NGA, its convergence properties
have not been well studied. In our second contribution, we
provide positive and negative convergence results for NGA
and GeNGA.
Finally, in our third contribution, we consider the application of NGA to reinforcement learning. This is an example
of a field where the domain of the function to be optimized
is a semi-Riemannian manifold, but not necessarily a Riemannian manifold. The derivations of the existing methods
therefore, by using NGA, make an implicit assumption that
is not satisfied. We remedy this by showing how reinforcement learning algorithms can use GeNGA in place of NGA.
The body of the paper is organized as follows. In Section
2 we show that NGA can diverge when ordinary gradient
ascent converges. In Section 3 we derive GeNGA, and in
Section 4 we provide convergence guarantees for NGA and
GeNGA. In Section 5 we show how some reinforcement
learning algorithms can be updated to use GeNGA. Finally,
in Section 6 we present examples that illustrate the different assumptions and convergence guarantees before concluding in Section 7.

2. Divergence of Natural Gradient Ascent
Consider maximizing a function, f : Rn → R.
Assumption 1. f is continuously differentiable.

•

The gradient of f at x, ∇f (x), is the direction of change
to x that causes f to increase most rapidly, assuming that
x resides in Euclidean space. The natural gradient generalizes the gradient to allow x to lie on a Riemannian manifold with metric tensor G(x) (Amari, 1998). On this manifold, at x, theplength of a vector, ∆x ∈ Rn , is given by
k∆xkG(x) := ∆x| G(x)∆x.1
For G to describe a Riemannian manifold, it must vary
smoothly from point to point and must be positive definite
1
We use A| to denote the transpose of a matrix, A, and A+
to denote the Moore-Penrose pseudoinverse of A. When vector
norms are applied to matrices, they denote the induced matrix
norm. We assume that vectors are column vectors.

GeNGA

for all x, in which case k·kG(x) are norms. Often, G(x) is
taken to be the Fisher information matrix for a parameterized distribution (Amari, 1998). NGA produces a sequence
of points (xi )∞
i=1 by ascending the natural gradient from an
initial point, x1 , using a step size schedule (αi )∞
i=1 and the
update
xi+1 = xi + αi G(xi )−1 ∇f (xi ).

These are both satisfied when i = 1 since x1 = 2.

Despite claims that NGA converges to a local maximum
(Peters & Schaal, 2008), without non-standard restrictions,
it does not. Specifically, if Assumption 1 holds, and
Assumption 2 (Lipschitz Assumption). There exists a finite constant L such that ∀x, z ∈ Rn ,

For the inductive step for (2):

k∇f (x) − ∇f (x − z)k2 ≤ Lkzk2 ,
•
Assumption
3. All αi are positive,
P∞ 2
α
<
∞,
i=1 i




x3i 

|xi+1 | = xi −  ≥ |2xi |,
i
by (2). Then by (1): |xi+1 | ≥ |4i| > 2(i + 1).


2
|xi+1 |3
x3i 
1 
xi −  |xi+1 |
=
i+1
i+1
i
1
≥
|2xi |2 |xi+1 |,
i+1
by (2). By (1) we have

P∞

i=1 αi = ∞, and
•

then ordinary gradient ascent causes either f (xi ) → ∞
or limi→∞ k∇f (xi )k2 = 0 (Bertsekas & Tsitsiklis, 2000).
However, the same is not true for NGA. In Theorem 1 we
give an example where NGA oscillates and diverges when
ordinary gradient ascent would converge to the global maximum.
Theorem 1. If Assumptions 1, 2, and 3 hold, then it can occur that the sequence, (xi )∞
i=1 , produced by NGA diverges
when ordinary gradient ascent would converge to a finite
value.
2

Proof. We provide a counterexample. Let f (x) := −x ,
where x ∈ R. Notice that f is continuously differentiable
and its derivative is Lipschitz. Consider the application of
1
and x1 = 2. Ordinary gradient
NGA to f with αi = 2i
ascent causes xi → 0 in this setting (Bertsekas & Tsitsiklis,
2000), and x = 0 is a global maximum of f . For NGA, let
(
−x2 + 2 if x ∈ (−1, 1)
G(x) :=
x−2
otherwise.
This G meets all of the requirements to describe a Riemannian manifold.
When xi 6∈ (−1, 1), the NGA update is
x3i
.
i
We show that this sequence diverges without entering the
(−1, 1) interval. We show this with an inductive proof that
|xi | ≥ 2i.
xi+1 =xi + αi G(xi )−1 ∇f (xi ) = xi −

We have two inductive hypotheses:
|xi | ≥2i,
 3
 xi 
  ≥3|xi |.
 i 

For the inductive step for (1):

(1)

1
|xi+1 |3
≥
|4i|2 |xi+1 | ≥ 3|xi+1 |.
i+1
i+1

In Section 6.5 we give an example to show that divergence
can still occur even if the metric tensor is the Fisher information matrix. In the following section we introduce
GeNGA, a generalization of NGA, before providing convergence proofs that apply to both methods.

3. Generalized Natural Gradient Ascent
Although the Fisher information matrix is often chosen as a
metric tensor for NGA, it is only guaranteed to be positive
semidefinite. In these cases where G(x) is only positive
semidefinite, G describes a semi-Riemannian manifold and
k·kG(x) is a seminorm. Before deriving an expression for
the directions of steepest ascent in this setting, we require:
Assumption 4. There exists at least one solution, ∆x, to
the equality
G(x)∆x = ∇f (x),
for all x ∈ Rn .

•

It can be shown that Assumption 4 implies that if there is
a direction of change, ∆x, to the current x that incurs no
distance, then the directional derivative of f at x in the direction ∆x is zero (i.e., ∇∆x f (x) = 0 for all x and ∆x
where k∆xkG(x) = 0). We also use a similar but stronger
assumption, which implies that ∇∆x f (z) = 0 for all x, z,
and ∆x where k∆xkG(x) = 0:
Assumption 5. There exists at least one solution, ∆x, to
the equality G(x)∆x = ∇f (z), for all x, z ∈ Rn .
•

(2)
In Theorem 2 we generalize the natural gradient to only

GeNGA

require x to reside on a semi-Riemannian manifold.2 Hereafter, to alleviate formatting problems, we write G for G(x)
and Gi for G(xi ).3 Also, let

h(x, v) := G+ ∇f (x) + I − G+ G v,

So,
∇f (x)|

(3)

where v ∈ Rn .
Theorem 2. If each x lies on a semi-Riemannian manifold
and Assumptions 1 and 4 hold, then for every v,
h(x, v)
kh(x, v)kG

(4)

h(x, v)
∇f (x)| h(x, v)
=p
kh(x, v)kG
∇f (x)| h(x, v)
p
= ∇f (x)| h(x, v)

= ∇f (x)| G+ ∇f (x) + ∇f (x)| v
 21
− ∇f (x)| G+ Gv .

Since
∇f (x)| G+ Gv = GG+ ∇f (x)

|

v = ∇f (x)| v,

we have that
is a direction of steepest ascent of f at x. Also, every direction of steepest ascent is given by (4), for some v.

∇f (x)|

p
h(x, v)
= ∇f (x)| G+ ∇f (x).
kh(x, v)kG

(6)

Since the right side of (6) does not depend on v, all v cause
∇f (x)| h(x, v)/kh(x, v)kG to take the same value.
Proof. The directions of steepest ascent of f at x are the
∆x that, for infinitesimal , maximize f (x + ∆x), subject to k∆xkG = 1 (Amari, 1998). By Assumption 1,
the directions of steepest ascent are those that maximize
∇f (x)| ∆x, subject to ∆x| G∆x − 1 = 0. Using the
method of Lagrange multipliers gives necessary conditions
for ∆x:
2λG∆x = ∇f (x),

(5)

for some positive scalar λ. The system of linear equations
specified by (5) must have one solution (Assumption 4),
and it may have many solutions since G is only positive
semidefinite. Every solution is given by (4) for some v.
In the remainder of this proof we will show that
∇f (x)| h(x, v)/kh(x, v)kG takes the same value for every v,
and thus that all h(x, v)/kh(x, v)kG are directions of steepest ascent. This means that, in this instance, the method
of Lagrange multipliers produces necessary and sufficient
conditions.
By Assumption 4 we have that GG+ ∇f (x) = ∇f (x). Using the definition of h(x, v),


Gh(x, v) =GG+ ∇f (x) + G − GG+ G v
=∇f (x).

Given some x1 ∈ Rn , generalized natural gradient ascent
(GeNGA) produces a sequence, (xi )∞
i=1 , by
e (xi ),
xi+1 = xi + αi ∇f
where (αi )∞
i=1 is a sequence of non-negative step sizes, and
e (xi ), points in
where the generalized natural gradient, ∇f
a direction of steepest ascent (but is not normalized):
e (x) := h(x, v),
∇f

(7)

for some v. When G is positive definite for all x, this
degenerates to NGA. Also, notice that, from the semiRiemannian point of view (measuring distances using k·kG
rather than k·k2 ), the length of the generalized natural gradients are all equal:
e (x)kG = kG+ ∇f (x)kG .
k∇f
Lastly, selecting v = 0 gives the direction of steepest ascent
with minimum Euclidean norm: G+ ∇f (x). We use an
assumption to specify when we require GeNGA to use this
direction of steepest ascent:
e (x) = G+ ∇f (x) always.
Assumption 6. ∇f
•

4. Convergence

2

We do not place smoothness restrictions on G for GeNGA or
one of our convergence proofs, so this is actually more general
than semi-Riemannian manifolds. However, to avoid convoluting the text, we still refer to x as residing on a semi-Riemannian
manifold.
3
This shorthand does not apply to any variables other than x.
That is, G always denotes G(x) and never G(z). Sometimes we
still write out G(x) for emphasis.

In this section we establish sufficient conditions to ensure
that GeNGA achieves different types of convergence. We
provide examples that illustrate the benefits and drawbacks
of each convergence guarantee in Sections 6.1, 6.2, and 6.3.
One approach to showing that GeNGA converges is to
match the requirements of an existing guarantee:

GeNGA

Assumption 7. There exist positive scalars c1 and c2 such
that
2
e (xi ),
(8)
c1 k∇f (xi )k2 ≤ ∇f (xi )| ∇f
and




e
∇f (xi ) ≤ c2 k∇f (xi )k2 ,

(9)

2

•

for all i.

Theorem 3. If Assumptions 1, 2, 3, 4, and 7
hold, then GeNGA causes either f (xi ) → ∞ or
limi→∞ k∇f (xi )k2 = 0.

1. Assumptions 1, 3, 5, and 8,
2. Assumptions 1, 3, 4, 6, and 8,
then the sequence, (xi )∞
i=1 produced by GeNGA causes either f (xi ) → ∞ or else f (xi ) converges to a finite value
e (xi )kG = 0.
and lim inf i→∞ k∇f
i
Proof. We adapt a proof that ordinary gradient descent
converges (Bertsekas & Tsitsiklis, 1997). For any x, z ∈
Rn , let g(ξ) := f (x − ξz), where ξ ∈ R. Then

Proof. This follows immediately from the work of Bertsekas & Tsitsiklis (2000).
A drawback of this guarantee is that different choices of v
when G is singular (i.e., selecting different generalized natural gradients when there are many) can cause the left side
of (9) to become arbitrarily large, which means that no c2
can exist. This means that Theorem 3 is not always applicable to GeNGA. However, when using NGA or GeNGA
with Assumption 6, Theorem 3 can be useful.
In the remainder of this section we present a guarantee that
is more applicable to GeNGA. It uses less restrictive assumptions but provides a correspondingly weaker guarantee. In order to provide this guarantee, we introduce a modified Lipschitz assumption that uses the Riemannian seminorm in place of the Euclidean norm and a generalized natural gradient in place of the gradient:
Assumption 8 (Riemann-Lipschitz Assumption). There
exists a finite constant, LG , such that, ∀x, z ∈ Rn ,
kG+ ∇f (x) − G+ ∇f (x − z)kG ≤ LG kzkG .

•

Intuitively, this says that, from a semi-Riemannian point of
view, the gradient of f is Lipschitz. Notice that, for any x
and A,
√
√
kA+ xkA = x| A+ AA+ x = x| A+ x = kxkA+ .
So, Assumption 8 implies that
k∇f (x) − ∇f (x − z)kG(x)+ ≤ LG kzkG(x) .

(10)

f (x − z) − f (x) =g(1) − g(0)
Z 1
=
∇g(ξ) dξ
0

Z

1

=−

z | ∇f (x − ξz) dξ.

0

Adding

R1
0

z | (∇f (x) − ∇f (x)) = 0, we get
Z

f (x − z) − f (x) = −

1

z | ∇f (x) dξ

0

Z
−

1

z | (∇f (x − ξz) − ∇f (x)) dξ

0
|

Z

= − z ∇f (x) −

1

z | (∇f (x − ξz) − ∇f (x)) dξ.

0

Let z := −αi h(x, v) and x := xi . Then
f (xi − z) − f (xi ) = αi h(x, v)| ∇f (xi )
Z 1
−
αi h(x, v)| (∇f (xi ) − ∇f (xi − ξz)) dξ
0

 |
+
=αi G+
∇f (xi )
i ∇f (xi ) + I − Gi Gi v
Z 1

 |
+
−
αi G+
i ∇f (xi ) + I − Gi Gi v
0

(∇f (xi ) − ∇f (xi − ξz)) dξ
=αi ∇f (xi )| G+
i ∇f (xi )


|
+ αi v I − Gi G+
(11)
i ∇f (xi )
Z 1
−
αi ∇f (xi )| G+
i (∇f (xi ) − ∇f (xi − ξz)) dξ
0

We show in Theorem 4 that with different combinations of
assumptions, GeNGA is guaranteed to converge to a desirable solution from a semi-Riemannian point of view, without the need for Assumption 7. That is, either f (xi ) → ∞
or the magnitude of the generalized natural gradient (measured using the seminorm of the semi-Riemannian manifold) goes to zero:
Theorem 4. If either of the following sets of assumptions
are satisfied:

Z
−

1



αi v | I − Gi G+
i (∇f (xi ) − ∇f (xi − ξz)) dξ.

0

(12)
If the first set of assumptions hold, then by Assumption 5,
GG+ ∇f (x) = ∇f (x) and GG+ ∇f (x − ξz) = ∇f (x −
ξz), so the terms on lines (11) and (12) are zero. If the second set of assumptions hold, then by Assumptions 4 and 6,
GG+ ∇f (x) = ∇f (x) and v = 0, so the terms on lines

GeNGA

From this relation, we see that for i ≥ ī, f (xi ) is monotonically nondecreasing, so either f (xi ) → ∞ or f (xi )
converges to a finite value. If the former case holds we are
done, so assume the latter case. By adding (14) over all
i ≥ ī, we obtain

(11) and (12) are zero. So, in both cases we get:
f (xi − z) − f (xi ) ≥ αi ∇f (xi )| G+
i ∇f (xi )
Z 1
−
αi ∇f (xi )| G+
i (∇f (xi ) − ∇f (xi − ξz)) dξ
0
2

≥αi k∇f (xi )kG+
i
Z 1
−
αi k∇f (xi )kG+ k∇f (xi ) − ∇f (xi − ξz)kG+ dξ,
i

0

f (xi − z) − f (xi ) ≥ αi k∇f (xi )k2G+
i
Z 1
−
αi k∇f (xi )kG+ LG kξzkGi dξ
=αi k∇f (xi )k2G+
i
1
0

2
αi kG+
i ∇f (xi )kGi ≤ lim f (xi ) − f (xī ) < ∞.
i→∞

i=ī

We see that there cannot exist an  > 0 such that
2
kG+
i ∇f (xi )kGi >  for all i greater than
P∞some î, since
this would contradict the assumption
i=0 αi = ∞.
Therefore, we must have lim inf i→∞ kG+
i ∇f (xi )kGi =
e
0. This implies our result since k∇f (xi )kG(xi ) =
kG+
i ∇f (xi )kGi .

i

0

−

∞
X

i

by the Cauchy-Schwarz inequality for semi-inner-product
spaces. By (10), which followed from Assumption 8,

Z

c

αi2 LG ξk∇f (xi )kG+
i

kh(x, v)kGi dξ. (13)

Notice that

Assumption 9. There exists a positive scalar c3 such that
for all x, z ∈ Rn , kzk2 ≤ c3 kzkG(x) .
•

kh(x, v)k2Gi =h(x, v)| Gi h(x, v)

 |
+
= G+
i ∇f (xi ) + I − Gi Gi v

 
+
Gi G+
i ∇f (xi ) + I − Gi Gi v


|
= ∇f (xi )| G+
I − Gi G+
i +v
i

Notice that Assumption 9 can only be satisfied if G is always positive definite.
Lemma 1. Assumptions 2 and 9 imply Assumptions 6
and 8.

Gi G+
i ∇f (xi )
+
=∇f (xi )| G+
i Gi Gi ∇f (xi )
=∇f (xi )| G+
i ∇f (xi )

Proof. Assumption 9 implies that G(x) is always positive
definite, so Assumption 6 is satisfied. Next we show that
Assumptions 2 and 9 imply Assumption 8.

=k∇f (xi )k2G+ .
i

k∇f (x) − ∇f (x − z)k2G+

So, continuing (13), we have

=(∇f (x) − ∇f (x − z))| G+ (∇f (x) − ∇f (x − z))



= G+ (∇f (x) − ∇f (x − z)), (∇f (x) − ∇f (x − z)) 2

2
αi k∇f (xi )kG+
i

f (xi − z) − f (xi ) ≥
Z 1
−
αi2 LG ξk∇f (xi )k2G+ dξ

≤kG+ (∇f (x) − ∇f (x − z))k2 k∇f (x) − ∇f (x − z)k2

i

0

In some cases it can be challenging to show that the
Riemann-Lipschitz assumption (Assumption 8) is satisfied.
We therefore introduce a new assumption that can be used
together with Assumption 2 to imply Assumption 8:

Z

2
=αi k∇f (xi )kG+
i

−

2
=αi k∇f (xi )kG+
i

α2 LG
k∇f (xi )k2G+ .
− i
i
2

αi2 LG k∇f (xi )k2G+
i

1

ξ dξ

(15)

≤c23 kG+ (∇f (x) − ∇f (x − z))kG kzkG

(16)

=c23 k∇f (x)

0

So,



αi LG
f (xi+1 ) ≥f (xi ) + αi 1 −
k∇f (xi )k2G+
i
2


αi LG
2
=f (xi ) + αi 1 −
kG+
i ∇f (xi )kGi .
2
Since αi → 0, we have for some positive constant c and all
i greater than some index ī,
2
f (xi+1 ) ≥ f (xi ) + αi ckG+
i ∇f (xi )kGi .

≤kG+ (∇f (x) − ∇f (x − z))k2 kzk2

(14)

− ∇f (x − z)kG+ kzkG .

where (15) comes from Assumption 2 and (16) comes from
Assumption 9. Dividing both sides of the inequality by
k∇f (x) − ∇f (x − z)kG+ , which is always positive, we
have
k∇f (x) − ∇f (x − z)kG+ ≤c23 kzkG ,
and hence Assumption 8 is satisfied with LG = c23 .
Notice that Assumptions 2 and 9 together are more restrictive than Assumption 8, so if they are not satisfied, it does
not mean that Assumption 8 is also not satisfied.

GeNGA

Not only can we use Lemma 1 to replace Assumptions 6
and 8 with Assumptions 2 and 9 in the requirements for
Theorem 4, but we can use Assumption 9 to provide a
stronger guarantee:
Theorem 5. If Assumptions 1, 2, 3, 4, and 9 hold, then
the sequence, (xi )∞
i=1 produced by GeNGA causes either
f (xi ) → ∞ or else f (xi ) converges to a finite value and
e (xi )k2 = lim inf i→∞ k∇f
e (xi )kG = 0.
lim inf i→∞ k∇f
i
Proof. We have from Theorem 4 and Lemma 1 that eie (xi )kG = 0. If the
ther f (xi ) → ∞ or lim inf i→∞ k∇f
i
former case holds we are done, so assume the latter case.
e (xi )kG = 0 implies
By Assumption 9, lim inf i→∞ k∇f
i
e
lim inf i→∞ k∇f (xi )k2 = 0.

5. Generalized Natural Policy Gradient
Methods
In the previous sections we introduced GeNGA and analyzed its convergence properties. In this section we consider a field, reinforcement learning, where NGA has been
applied even though the domain of the function being optimized is not guaranteed to be a Riemannian manifold, but
is guaranteed to be a semi-Riemannian manifold.
Reinforcement learning algorithms search for “good” policies for Markov decision processes (MDPs). Policies are
distributions that are typically parameterized by a vector
θ ∈ Rn . An objective function J : Rn → R is selected
to capture the desired properties of good policies. Our results apply to the standard average-reward and discountedreward objective functions (Sutton et al., 2000). Policy
search algorithms search for θ that maximize J. (Natural)
policy gradient algorithms estimate and ascend the (natural) gradient of J.
Natural policy gradient algorithms typically assume that
G(θ) is positive definite and thus invertible. This is not
the case for popular policy parameterizations like tabular
softmax action selection when G is the (average) Fisher information matrix (see Section 6.4 for an example). In this
section, we remove this assumption.
Some natural policy gradient algorithms estimate G(θ) and
e
∇J(θ) and then select ∇J(θ)
= G(θ)−1 ∇J(θ) (Bhatnae
gar et al., 2009). This can be easily corrected to ∇J(θ)
=
+
G(θ) ∇J(θ), which is a generalized natural gradient of
J at θ. However, other natural policy gradient algorithms
form estimates of the natural gradient directly, without estimating G(θ). We show that, without modification, they
perform generalized natural gradient ascent.
We will consider w ∈ Rn that satisfy Assumption 10,
which comes from the combination of a standard constraint
(Sutton et al., 2000, Equation 3) with the standard defini-

tion of G(θ) (Kakade, 2002, Equation 2):
Assumption 10. ∇J(θ) = G(θ)w.

•

The natural policy gradient theorem, states that if w is
chosen to satisfy Assumption 10, then G(θ)−1 ∇J(θ) =
w (Kakade, 2002, Theorem 1). This is useful because
accurate estimates of w that satisfy Assumption 10 can
be formed from small amounts of data using temporaldifference learning algorithms (Peters & Schaal, 2008).
However, this clearly requires that G(θ) is invertible.
Although NGA is not applicable when G is singular,
GeNGA is. We extend the natural policy gradient theorem
to allow for positive semidefinite G(θ). We find that every
w that satisfies Assumption 10 is still a generalized natural
gradient (unnormalized direction of steepest ascent) of J at
θ.
Theorem 6 (Generalized Natural Policy Gradient Theorem). If w is selected such that Assumption 10 holds, then
w is a generalized natural gradient of J at θ and every generalized natural gradient is given by a w that satisfies Assumption 10.
Proof. The w that satisfy Assumption 10 are all given by

w = G(θ)+ ∇J(θ) + I − G(θ)+ G(θ) v,
for some v. Every solution to G(θ)w = ∇J(θ) is given by
this equation for some v, and every v produces a solution.
Notice from (3) that this is merely h(θ, v). By (7), these w
are the generalized natural gradients.
This means that natural policy gradient algorithms that use
w that satisfy Assumption 10 as their steepest ascent directions are already implementing GeNGA, and will therefore
work properly when G(θ) is positive semidefinite. This is
important because most natural policy gradient algorithms
work this way (Morimura et al., 2005; Peters & Schaal,
2008; Bhatnagar et al., 2009; Degris et al., 2012). Although the algorithms are correct, convergence proofs that
assume that G(θ) is always positive definite (Bhatnagar
et al., 2009) do not apply when G(θ) is only guaranteed
to be positive semidefinite.

6. Examples
In this section we give examples to ground the preceding
theory.
6.1. Convergence by Theorem 3
We present an example where Theorem 3 can be applied.
Let f (x) := −x2 , G(x) = 2 + sin(x), αi = 1i , and x1
be any finite value. It is straightforward to show that Assumptions 1, 2, 3, 4, and 7 hold. So by Theorem 3, GeNGA

GeNGA

causes either f (xi ) → ∞ or limi→∞ k∇f (xi )k = 0. Since
f (x) is bounded above, only the latter can occur.
6.2. Convergence by Theorem 4




−1
.
1

1
−1

Notice that this G is positive semidefinite but not positive
definite. At each step, there will be an infinite number of directions of steepest ascent. In some cases you cannot guarantee that v = 0 is selected, for example, when using the
generalized natural policy gradient theorem (Theorem 6).
For this example, we select v = [t, t]| .
Since (9) in Assumption 7 is not satisfied and Assumption 9 is not satisfied, Theorems 3 and 5 are not applicable.
Similarly, Assumption 6 is not satisfied, so we cannot use
the second set of requirements for Theorem 4. However,
since Assumptions 1, 3, 5, and 8 are satisfied, by Theorem 4, the sequence, (xi )∞
i=1 produced by NGA causes either f (xi ) → ∞ or else f (xi ) converges to a finite value
e (xi )kG = 0. Since f (x) is bounded
and lim inf i→∞ k∇f
i
above, only the latter can occur.
6.3. Convergence by Theorem 5
Consider the application of NGA to f (x) = − 12 (x − 10)2 ,
where x ∈ R. Let x1 = 1, αi = 0.1
i , and
(
G(x) :=

1
2−x

1

-10
-20

Consider the application of NGA to f (x)
=
−(x| [1, −1]| )2 , where x ∈ R. Let x1 = [1, 1]| ,
αi = 1i , and
G(x) :=

0

if x ∈ [1, 2)
otherwise.

-30
-40
0

5

10

15

20

x

Figure 1. Sequence produced by NGA on the example from Section 6.3. The black curve is f and the points depict the sequence
(xi )∞
i=1 , where x1 is blue and xi is dark red for large i. Notice
that, from a Euclidean perspective, NGA converges prematurely.

nor lim inf i→∞ k∇f (xi )k2 = 0. That is, from a semiRiemannian point of view, the algorithm did the correct
thing—it moved an infinite distance towards the global
maximum. However, from the Euclidean point of view, this
only got it to 2.
6.4. Tabular Softmax Policies
Next, we present a simple reinforcement learning example
where Assumption 5 is satisfied. This example also shows
how the metric tensor can be positive semidefinite and not
positive definite when using a common policy parameterization.
Consider any MDP with one state and two actions. We use
a softmax policy parameterization. That is, the policy has
parameter vector θ ∈ R2 and the probability of action i is
given by
eθi
πθ (i) := θ1
.
e + eθ 2

This G is positive (definite) and Assumptions 1, 2, 3,
4, and 9 are satisfied. So, by Theorem 5, the sequence, (xi )∞
i=1 produced by NGA causes either f (xi ) →
∞ or else f (xi ) converges to a finite value and
e (xi )k2 = lim inf i→∞ k∇f
e (xi )kG = 0.
lim inf i→∞ k∇f
i
Notice that (8) in Assumption 7 is not satisfied, so the requirements of Theorem 3 are not satisfied. This is an interesting example because it showcases the difference between the convergence guarantees of Theorems 3 and 5.

Natural policy gradient methods typically use the average
Fisher information matrix (Bagnell & Schneider, 2003) as
their metric tensor. In this case:

The GeNGA update when x ∈ [1, 2) is

First, notice that G(θ) is positive semidefinite but not positive definite, and so GeNGA is required (in fact, a tabular
softmax policy for any finite number of states and actions
will result in G(θ) always being singular). Second, notice
that the columns of G(θ) span all vectors in R2 that sum to
zero.

xi+1 = xi +

x2 − 12x + 20
.
10i

It can be shown that this update causes xi → 2 without leaving the [1, 2) interval. This example is dee (xi )k2 =
picted in Figure 1. Notice that lim inf i→∞ k∇f
e (xi )kG = 0, but neither f (xi ) → ∞
lim inf i→∞ k∇f
i

2
X

∂ log πθ (i) ∂ log πθ (i)
∂θ
∂θ
i=1


1 −1
=πθ (1)πθ (2)
.
−1
1

G(θ) :=

|

πθ (i)

This raises the question, why is it appropriate for G(θ)
to not be positive definite? Allowing G(θ) to be positive

GeNGA

semidefinite means that there are vectors, ∆θ, such that
∆θ| G(θ)∆θ = k∆θk2G(θ) = 0. When computing the directions of steepest ascent, this means that there is a direction away from θ that does not incur any distance.

such that Pr(a1 |θ) = f (θ), where
(
1
1
if θ 6∈ [−2, 2]
2 +
f (θ) := 2θθ2 23
− 32 + 4 otherwise.

This is desirable when moving in the direction ∆θ from
θ does not change the distribution being optimized (in our
case, the policy). Notice that we are parameterizing a distribution over two possible events. This should require only
one parameter—the probability of one of the events, since
the probability of the other is one minus this probability.
However, our tabular softmax policy has two parameters.
So, there are directions of change to the tabular softmax
policy parameters that result in no change to the action
probabilities (specifically, adding the same amount to both
policy parameters). Moving along this direction should not
incur distance when computing the directions of steepest
ascent.

J(θ) = f (θ), so hereafter we discuss maximizing f . Also
notice that, although f is defined in a piecewise manner, it
is continuously differentiable and its derivative is Lipschitz.
Let αi = 4i and θ1 = 5. Ordinary gradient ascent on f
causes θi → 0 in this setting (Bertsekas & Tsitsiklis, 2000),
and θ = 0 is a global maximum of f .

In this case, the gradient of the standard objective functions, J, at any θ0 , can both be written as (Sutton et al.,
2000):

So, when θi 6∈ [−2, 2], the GeNGA update is

∇J(θ0 ) =

2
X
i=1

Qθ0 (i)

∂ log πθ0 (i)
,
∂θ0

where Qθ0 is a bounded real-valued function and θ0 are any
policy parameters. Since
 
∂ log πθ0 (1)
1
=πθ0 (2)
0
−1
∂θ
 
∂ log πθ0 (2)
−1
=πθ0 (1)
,
0
1
∂θ
we have that ∇J(θ0 ) must be a vector in R2 that sums to
zero, and hence it is in the column span of G(θ). So, Assumption 5 is satisfied.
6.5. Divergence of NGA for Policy Search
We showed in Theorem 1 that NGA can diverge. However, in that example we did not use the Fisher information matrix. This raises the question, can GeNGA, using
the Fisher information matrix, diverge when optimizing a
parameterized distribution, or does the Fisher information
matrix introduce some properties that ensure convergence?
We show that GeNGA can still diverge in this setting. In
this example the Fisher information matrix is not always
positive definite, so this example can not be used in place
of the one used to prove Theorem 1.
Consider a bandit problem (one-state MDP with rewarddiscount parameter γ = 0) with two actions, a1 and a2 .
Let the reward for taking action a1 be 1 and the reward for
taking action a2 be 0. In this setting, J(θ) = Pr(a1 ). We
parameterize the policy with a single parameter, θ ∈ R,

In this case, the Fisher information matrix can be written as
G(θ) :=
=

∇f (θ)2
f (θ) (1 − f (θ))
(
4
θ 6 −θ 2
−4θ 2
(θ 2 −24)(θ 2 +8)

if θ 6∈ [−2, 2]
otherwise.

θi+1 =θi + αi G(θi )−1 ∇f (θi )
=θi +

1
θ3
− i.
iθi
i

Since this sequence diverges without entering the [−2, 2]
interval, we have that GeNGA causes |θi | → ∞ while ordinary gradient ascent causes θi → 0 (the global maximum).

7. Conclusion and Future Work
We presented GeNGA, a generalization of NGA to allow
for positive semidefinite (possibly non-smooth) metric tensors. Next, we provided sufficient conditions to ensure
that the sequences generated by GeNGA achieve different
forms of convergence. We then showed how existing natural policy gradient algorithms could easily be updated to
use GeNGA or already use GeNGA. Lastly, we provided
examples to showcase the different types of convergence.
All of our convergence guarantees are for deterministic
e (x) is not
NGA and GeNGA. They do not apply when ∇f
known, but noisy, biased estimates of it can be generated.
It is straightforward to apply existing results in this setting if Assumption 7 holds (Bertsekas & Tsitsiklis, 2000).
One avenue of future work would be to extend Theorems
4 and 5 to provide convergence guarantees in this setting,
even when Assumption 7 does not hold.

References
Amari, S. Natural gradient works efficiently in learning.
Neural Computation, 10:251–276, 1998.
Bagnell, J. A. and Schneider, J. Covariant policy search.
In Proceedings of the International Joint Conference on
Artificial Intelligence, pp. 1019–1024, 2003.

GeNGA

Bertsekas, D. P. and Tsitsiklis, J. N. Gradient convergence
in gradient methods. Technical Report LIDS-P ; 2404,
Massachusetts Institute of Technology, Laboratory for
Information and Decision Systems, November 1997.
Bertsekas, D. P. and Tsitsiklis, J. N. Gradient convergence
in gradient methods with errors. SIAM J. Optim., 10:
627–642, 2000.
Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., and Lee,
M. Natural actor-critic algorithms. Automatica, 45(11):
2471–2482, 2009.
Degris, T., Pilarski, P. M., and Sutton, R. S. Model-free reinforcement learning with continuous action in practice.
In Proceedings of the 2012 American Control Conference, 2012.
Kakade, S. A natural policy gradient. In Advances in
Neural Information Processing Systems, volume 14, pp.
1531–1538, 2002.
Morimura, T., Uchibe, E., and Doya, K. Utilizing the natural gradient in temporal difference reinforcement learning with eligibility traces. In International Symposium
on Information Geometry and its Application, 2005.
Peters, J. and Schaal, S. Natural actor-critic. Neurocomputing, 71:1180–1190, 2008.
Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y.
Policy gradient methods for reinforcement learning with
function approximation. In Advances in Neural Information Processing Systems 12, pp. 1057–1063, 2000.

