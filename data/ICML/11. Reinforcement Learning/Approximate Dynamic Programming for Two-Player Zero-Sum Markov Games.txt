Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games

Julien Perolat(1)
Bruno Scherrer(2)
Bilal Piot(1)
Olivier Pietquin(1,3)
(1)
Univ. Lille, CRIStAL, SequeL team, France
(2)
Inria, Villers-leÃÄs-Nancy, F-54600, France
(3)
Institut Universitaire de France (IUF), France

Abstract
This paper provides an analysis of error propagation in Approximate Dynamic Programming applied to zero-sum two-player Stochastic Games.
We provide a novel and unified error propagation analysis in Lp -norm of three well-known algorithms adapted to Stochastic Games (namely
Approximate Value Iteration, Approximate Policy Iteration and Approximate Generalized Policy Iteratio,n). We show that we can achieve a
2Œ≥+0
stationary policy which is (1‚àíŒ≥)
2 -optimal, where
 is the value function approximation error and 0
is the approximate greedy operator error. In addition, we provide a practical algorithm (AGPI-Q)
to solve infinite horizon Œ≥-discounted two-player
zero-sum Stochastic Games in a batch setting. It
is an extension of the Fitted-Q algorithm (which
solves Markov Decisions Processes from data)
and can be non-parametric. Finally, we demonstrate experimentally the performance of AGPIQ on a simultaneous two-player game, namely
Alesia.

1. Introduction
A wide range of complex problems (e.g. computer networks, human-computer interfaces, games as chess or
checkers) can be addressed as multi-agent systems. This
is why Multi-Agent Reinforcement Learning (MARL) (Busoniu et al., 2008) has received a growing interest in the last
few years. While, in decision theory, Markov Decision Processes (MDPs) (Puterman, 1994) are widely used to control
a single agent in a complex environment, Markov Games
(MGs) (also named Stochastic Games (SGs)) (Shapley,
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

JULIEN . PEROLAT @ ED . UNIV- LILLE 1. FR
BRUNO . SCHERRER @ INRIA . FR
BILAL . PIOT @ UNIV- LILLE 3. FR
OLIVIER . PIETQUIN @ UNIV- LILLE 1. FR

1953) are an extension of this formal framework to describe
multi-agent systems. As for MDPs, SGs constitute a model
for MARL (Littman, 1994) and have been largely studied
in past years (Hu & Wellman, 2003; Greenwald et al., 2003;
Bowling & Veloso, 2001). In some sense, SGs are a generalization of both the game-theory and the MDP frameworks
where the agents may have different pay-offs (rewards in
the MDP vocabulary) they aim at maximizing.
This paper contributes to solving large scale games with
unknown dynamics. Especially, it addresses the problem of finding the Nash equilibrium for infinite horizon
Œ≥-discounted two-player zero-sum SGs in an approximate
fashion, possibly from batch data. SGs are modeled as
extensions of MDPs where Dynamic Programming (DP)
commonly serves as a basis to a wide range of practical
solutions (Puterman, 1994). However, when the size of
the game is too large or when its dynamics is not perfectly known, exact solutions cannot be computed. In
that case, Approximate Dynamic Programming (ADP) approaches are preferred as for MDPs (Bertsekas, 1995). Because DP relies on an iterative schema, interleaving Value
Function Approximation and Greedy Operator Approximations, two types of errors (corresponding to each source
of approximation) are introduced and can accumula,te over
iterationsprint(‚Äù‚Äî
More specifically, this paper provides a theoretical error
propagation analysis in Lp -norm of well-known ADP algorithms applied to games such as Approximate Value Iteration (AVI), Approximate Policy Iteration (API) and Approximate Generalized Policy Iteration (AGPI) (described
later). It also proposes a novel algorithm, named Approximate Generalized Policy Iteration-Q (AGPI-Q) extending
Fitted-Q iteration (Ernst et al., 2005) which is further analyzed in terms of error and complexity. AGPI-Q is evaluated on a simultaneous two-player game, namely Alesia.

Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games

1.1. Dynamic Programming techniques for MDP
ADP for MDPs has been the topic of many studies
these last two decades. Bounds in L‚àû can be found
in (Bertsekas, 1995) while Lp -norm ones were published
in (Munos & SzepesvaÃÅri, 2008) and (Farahmand et al.,
2010). Because approximations often come from the use of
supervised-learning algorithms, Lp -norm bounds are a significant improvement over L‚àû ones. Indeed, they can rely
on upper bounds provided in the supervised learning literature to estimate the overall error on the learnt policy. For
a unified analysis of PI, VI and MPI in Lp -norm see Scherrer et al. (2012). It allows evaluating several practical algorithms like Fitted-Q iteration (Ernst et al., 2005; Antos
et al., 2008), Classification-based MPI (Lagoudakis & Parr,
2003b; Lazaric et al., 2010; Gabillon et al., 2011) and LSPI
(Lagoudakis & Parr, 2003a).
1.2. Dynamic Programming techniques for Stochastic
Games
Zero-sum two-player Œ≥-discounted SGs are quite close to
Œ≥-discounted MDPs. In fact, the development of DP techniques for SGs has followed closely the development of
DP for MDPs (Shapley, 1953). Most of these algorithms
are detailed in Patek (1997) in the framework of Stochastic Shortest Path Games (SSPG). Zero-sum two-player discounted SG is a subclass of SSPG. Similar algorithms exist
to solve those games including PI (Patek, 1997), VI (Shapley, 1953) and MPI (called also Generalized Policy Iteration (GPI) (Patek, 1997) in the context of SSPG). Yet, there
has been very little attention paid to ADP for SGs after
Patek‚Äôs work except from Lagoudakis & Parr (2002). To
our knowledge, no analyses of the approximate version of
those algorithms exist in Lp -norm.

equilibrium (Von Neumann, 1947).
2.1. Two-Player Zero-Sum Discounted Stochastic
Games
A two-player Œ≥-discounted SG can be considered as a
tuple (S, (A1 (s))s‚ààS , (A2 (s))s‚ààS , p, r, Œ≥) where S is the
state space1 , A1 (s) is the finite set of actions player 1 can
play in state s, A2 (s) is the finite set of actions player 2
can play in state s, p(s0 |s, a1 , a2 ) with a1 ‚àà A1 (s) and
a2 ‚àà A2 (s) is the probability transition from state s to
state s0 , r(s, a1 , a2 ) with a1 ‚àà A1 (s) and a2 ‚àà A2 (s) is
the reward of both players bounded by Rmax and Œ≥ is the
discount factor. A strategy œÄ i , where i ‚àà {1, 2}, maps a
state s ‚àà S to a probability distribution œÄ i (.|s) over Ai (s).
Those strategies are named policies in the MDP literature.
We will adopt both vocabulary indifferently.
The stochastic kernel characterising the transition of the game is PœÄ1 ,œÄ2
(s0 |s)
=
0
1
2
Ea1 ‚àºœÄ1 (.|s),a2 ‚àºœÄ2 (.|s) [p(s |s, a , a )].
The stochastic
kernel is the probability to go from s to s0 when player
1 is playing strategy œÄ 1 and player 2 is playing strategy
œÄ 2 . The reward function of strategies (œÄ 1 , œÄ 2 ) will be
noted rœÄ1 ,œÄ2 = Ea1 ‚àºœÄ1 (.|s),a2 ‚àºœÄ2 (.|s) [r(s, a1 , a2 )]. This
quantity represents the reward each player can expect
while player 1 plays œÄ 1 and player 2 plays œÄ 2 .
One can see zero-sum two-player games as two players
maximizing opposite rewards. Another way to see it is
that the goal of player 1 is to maximize his cumulated Œ≥discounted reward while the goal of player 2 is to minimize
it. From now on, we will note ¬µ the policy of the maximizer
and ŒΩ the policy of the minimizer.
Let v¬µ,ŒΩ (s) = E[

+‚àû
P

Œ≥ t r¬µ,ŒΩ (st )|s0 = s, st+1 ‚àº

t=0

2. Framework
A SG is a generalization of an MDP to a n-player setting.
Each player has a control on the SG through a set of actions. At each step of the game, all players simultaneously
choose an action. The reward each player gets after one
step depends on the state and on the joint action of all players. Furthermore, the dynamics of the SG may depend on
the state and on the actions of all players.
Each player is interested in maximizing some criterion on
this sequence of rewards. Here, we will consider the problem of computing the minimax equilibrium in a two-player
zero-sum SG where each player tries to maximize his own
value, defined as the expected Œ≥-discounted cumulative reward. The value attained at the minimax equilibrium is the
optimal value. In the case of simultaneous games, the major difference between those games and MDPs is that each
player may have to randomize his strategies to reach this

P¬µ,ŒΩ (.|st )] be the cumulative reward if the players 1 and
2 respectively use the stationary strategies ¬µ and ŒΩ. The
value function v¬µ,ŒΩ maps the state s to its value v¬µ,ŒΩ (s).
2.2. Bellman Operators
Let us define the following five operators on value v:
TŒΩ,¬µ v = r¬µ,ŒΩ + Œ≥P¬µ,ŒΩ v
T¬µ v = min T¬µ,ŒΩ v

TÃÇŒΩ v = max T¬µ,ŒΩ v

T v = max T¬µ v

TÃÇ v = min TÃÇŒΩ v

ŒΩ

¬µ

¬µ

ŒΩ

Here ¬µ and ŒΩ are random policies. The min and max are
well defined because ¬µ and ŒΩ lie in compact sets since we
are considering finite sets of actions. All those operators
are contractions in L‚àû -norm with constant Œ≥. We have the
1
We will only consider a finite state space. The case where the
state space is continuous is beyond the scope of this paper.

Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games

following remarkable property (Patek, 1997):
‚àÄv, TÃÇ v = T v.

This can equivalently be written as follows:
(1)

Equation (1) is a consequence of von Neumann‚Äôs Minimax
theorem (Von Neumann, 1947; Patek, 1997).

¬µk+1 = G(vk ),
vk+1 = T¬µk+1 vk .
Policy Iteration (PI) iterates as follows

2.3. Minimax Equilibrium

¬µk+1 = G(vk ),

In the setting of zero-sum two-player SGs, notions of minimax equilibrium and of Nash equilibrium are equivalent.
In this case, the optimal value of the game is:

vk+1 = v¬µk+1 = (T¬µk+1 )+‚àû vk .

v ‚àó = min max v¬µ,ŒΩ = max min v¬µ,ŒΩ .
ŒΩ

¬µ

¬µ

ŒΩ

(2)

The existence of v ‚àó is proved in Patek (1997) for a more
general class of games. This value can be achieved using
mixed stationary strategies (Patek, 1997). Equation (2) is
a consequence of Equation (1) and of the contraction property. The value v ‚àó is the unique fixed point of the operator
T . We will note v¬µ = inf ŒΩ v¬µ,ŒΩ the fixed point of operator
T¬µ . An optimal counter strategy against ¬µ is any strategy
ŒΩ satisfying v¬µ,ŒΩ = v¬µ .
2.4. Policy Iteration, Value Iteration and Generalized
Policy Iteration
The three algorithms we intend to analyse are VI, PI and
GPI. They all share the same greedy step. A strategy ¬µ is
greedy with respect to some value v (noted ¬µ ‚àà G(v)) when
T v = T¬µ v = minŒΩ T¬µ,ŒΩ v.
Remark 1. In practice, trying to find ¬µ greedy with respect
to a value v means trying to maximize (T¬µ v)(s) for each
state s. Let us fix some state s; we try to find
(max min T¬µ,ŒΩ v)(s) =
¬µ

ŒΩ

max min E a‚àº¬µ(.|s),a0 ‚àºŒΩ(.|s), [r(s, a, a0 ) + Œ≥v(Œ£)].
¬µ(.|s) ŒΩ(.|s)

Œ£‚àºP¬µ,ŒΩ (.|s)

For a constant state s, this is equivalent to trying to find
the minimax equilibrium of a matrix game. The matrix of
the game with stochastic reward is defined by (r(s, a, a0 ) +
Œ≥v(Œ£a,a0 ))a‚ààA1 (s),a0 ‚ààA2 (s) where Œ£a,a0 ‚àº p(.|s, a, a0 ).
From the expectation of this matrix, the minimax equilibrium can be computed with linear programming.
In the case of turn-based games, finding a greedy policy is
much simpler. Indeed, since at each state only one player
controls the SG, finding a greedy strategy is reduced to finding a maximum over the actions of player 1.
The algorithms differ by the way they do the evaluation
step. We describe them by showing how they work from
iteration k to iteration k + 1. Value Iteration (VI) iterates
as follows:
vk+1 = T vk .

Finally, Generalized Policy Iteration (GPI) iterates in a
way that interpolates between VI and PI:
¬µk+1 = G(vk ),
vk+1 = (T¬µk+1 )m vk .
It is clear that GPI generalizes VI and PI. In particular,
the error propagation bounds for VI (when m = 1) and
PI (when m = ‚àû) will follow from the analysis we shall
provide for GPI.
2.5. Example and Special Cases
In this paper, we consider the game Alesia (Meyer et al.,
1997), that is a two-player zero-sum simultaneous game.
Both players control a token in the center of a five box
board. At the beginning each player has a finite budget
n. At each turn, every one has to bet at least 1 if his own
remaining budget is not 0. The goal of each player is to
put the token on his side of the board. At each turn the token moves toward the objective of the player who bets the
most (in case of equality the token doesn‚Äôt move). The one
wining is the one who can push his token outside of the
board.
This game can be modelled as follows. The state space S is
a triplet (token position ‚àà {1, ..., 5}, budget of player 1 ‚àà
{0, ..., n}, budget of player 2 ‚àà {0, ..., n}) and an absorbing state ‚Ñ¶ where the players fall at the end of the game.
The action space in state (s, u, v) is A1 ((s, u, v)) for
player 1. If u 6= 0, then A1 ((s, u, v)) = {1, ..., u}
else A1 ((s, u, v)) = {0}. For player 2, if v 6= 0, then
A2 ((s, u, v)) = {1, ..., v} else A1 ((s, u, v)) = {0}. If
player 1 bets a1 and player 2 bets a2 in state (s, u, v),
then the next state is (s0 , u ‚àí a1 , v ‚àí a2 ) where s0 =
s ‚àí 1a1 ‚â§a2 + 1a2 ‚â§a1 or ‚Ñ¶ if s0 ‚àà
/ {1, ..., 5}. The reward
is 1 if the token leaves the board from position 5 and ‚àí1 if
the token leaves the board from position 1.
Remark 2. In turn-based games, each state is controlled
by a single player. In the zero-sum two-player SGs
framework, they can be seen as a game where ‚àÄs ‚àà
S, card(A1 (s)) = 1 or card(A2 (s)) = 1. In this special
case, optimal strategies are deterministic (Hansen et al.,
2013). Furthermore, and as we already mentionned, the
greedy step (see definition in Sec. 2.4) reduces to finding

Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games

a maximum rather than a minimax equilibrium and is thus
significantly simpler. Furthermore, one can see an MDP as
a zero-sum two-player SG where one player has no influence on both the reward and the dynamics. Therefore, our
analysis should be consistent with previous MDP analyses.

3. Stochastic Games and Approximate
Generalized Policy Iteration (AGPI)

where v¬µk is the value when the maximizer plays ¬µk and
the minimizer plays the optimal counter-strategy against
¬µk . This is a natural measure of quality for the strategy
¬µk that would be output by the approximate algorithm.
By definition, we have:
‚àÄŒΩ, ‚àÄv, T¬µk v ‚â§ T¬µk ,ŒΩ v.

In this section, we analyse the algorithm in the case of approximations in the greedy and evaluation steps. This analysis was done in L‚àû -norm in Patek (1997) for PI. We generalize it for AGPI in the case of œÉ-weighted Lp -norm, that
is defined for a function
h and a distribution
œÉ on the state

 p1
P
space as khkp,œÉ =
|h(s)|p œÉ(s) .
s‚ààS

(5)

In addition, we shall consider a few notations. The minimizer policies ŒΩki , ŒΩÃÉk , ŒΩÃÇk and ŒΩ k are policies that respectively satisfy:
(T¬µk )i+1 vk‚àí1 = T¬µk ,ŒΩki ...T¬µk ,ŒΩk1 T¬µk vk‚àí1 ,
T¬µ‚àó vk = T¬µ‚àó ,ŒΩÃÉk vk ,

(6)
(7)

T¬µk vk = T¬µk ,ŒΩÃÇk vk ,
T¬µk v¬µk = T¬µk ,ŒΩ k v¬µk .

(8)

3.1. Approximate Generalized Policy Iteration
In Patek (1997), this algorithm is presented as GPI. It has
been first presented in (Van Der Wal, 1978). Similarly to
its exact counterpart, an iteration of this algorithm can be
divided in two steps: a greedy step and an evaluation step.
The main difference is that we account for possible errors
in both steps (respectively 0k and k ).
For the greedy step, we shall write ¬µk ‚Üê GÃÇ0k (vk‚àí1 ) for:

(3)

In other words, the strategy ¬µk is not necessarily the best
strategy, but it has to be at most 0k away from the best strategy.
For the evaluation step, we consider that we may have an
additive error k :
m

vk = (T¬µk ) vk‚àí1 + k .

dk = v‚àó ‚àí (T¬µk )m vk‚àí1 = v‚àó ‚àí (vk ‚àí k ),
sk = (T¬µk )m vk‚àí1 ‚àí v¬µk = (vk ‚àí k ) ‚àí v¬µk ,
bk = vk ‚àí T¬µk+1 vk ,
xk = (I ‚àí Œ≥P¬µk ,ŒΩÃÇk )k + 0k+1 ,

T vk‚àí1 ‚â§ T¬µk vk‚àí1 + 0k
or, ‚àÄ¬µ T¬µ vk‚àí1 ‚â§ T¬µk vk‚àí1 + 0k .

In order to bound lk , we will study the following quantities
similar to those introduced by Scherrer et al. (2012) (recall
that T¬µ and T¬µ,ŒΩ are defined in Sec. 2.2 and the stochastic
kernel is defined in Sec. 2.1):

(4)

yk = ‚àíŒ≥P¬µ‚àó ,ŒΩÃÉk k + 0k+1 .
Notice that lk = dk + sk . We shall prove the following relations, similar to the one proved in (Scherrer et al., 2012).
Lemma 1. The following linear relations hold:
bk ‚â§ Œ≥P¬µk ,ŒΩÃÇk Œ≥P¬µk ,ŒΩ m‚àí1 ...Œ≥P¬µk ,ŒΩk1 bk‚àí1 + xk ,
k

dk+1 ‚â§ Œ≥P¬µ‚àó ,ŒΩÃÉk dk + yk +

m‚àí1
X

Œ≥P¬µk ,ŒΩ j ...Œ≥P¬µk ,ŒΩk1 bk ,
k

j=1

Remark 3. Evaluation step: In the evaluation step the
policy ¬µk is fixed and we apply m times the operator
T¬µk ¬∑ = minŒΩ T¬µk ,ŒΩ ¬∑ . Since the policy of the maximizer is
fixed, the problem solved in the evaluation step consists in
finding an optimal m-horizon counter-policy for the minimizer.
3.2. Error Propagation
Since errors may accumulate from iterations to iterations,
we are interested in bounding the difference
lk = v‚àó ‚àí v¬µk ‚â• 0,
where v‚àó is the minimax value of the game (obtained when
both players play the Nash equilibrium ¬µ‚àó and ŒΩ‚àó ) and

‚àû
X
Œ≥P¬µk ,ŒΩki ...Œ≥P¬µk ,ŒΩk1 bk‚àí1 ).
sk ‚â§ (Œ≥P¬µk ,ŒΩ k )m (
i=1

Contrary to the analysis of Scherrer et al. (2012) for MDPs
in which the operator T¬µ is affine, it is in our case nonlinear. The proof that we now develop is thus slightly trickier.
Proof. Let us start with bk :
bk = vk ‚àí T¬µk+1 vk ,
= vk ‚àí T¬µk vk + T¬µk vk ‚àí T¬µk+1 vk .

Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games

In Equation (3) with ¬µ = ¬µk‚àí1 and k ‚Üê k + 1, we have
T¬µk vk ‚â§ T¬µk+1 vk + 0k+1 then:
bk ‚â§ vk ‚àí T¬µk vk + 0k+1 ,
= vk ‚àí k ‚àí T¬µk vk +Œ≥P¬µk ,ŒΩÃÇk k
| {z }

=

m‚àí1
X

k+1

‚â§

m‚àí1
X

[T¬µk+1 ,ŒΩ j

k+1

1
vk
...T¬µk+1 ,ŒΩk+1

j=1

‚àí T¬µk+1 ,ŒΩ j

k+1

+ k ‚àí Œ≥P¬µk ,ŒΩÃÇk k + 0k+1 ,
=

m‚àí1
X

Œ≥P¬µk+1 ,ŒΩ j

1
...Œ≥P¬µk+1 ,ŒΩk+1
(vk ‚àí T¬µk +1 vk ),

Œ≥P¬µk+1 ,ŒΩ j

1
...Œ≥P¬µk+1 ,ŒΩk+1
bk .

k+1

=

m‚àí1
X

k+1

From Equation (4), we have vk ‚àík = (T¬µk )m vk‚àí1 . Thus,
bk ‚â§ (T¬µk )m vk‚àí1 ‚àí T¬µk ,ŒΩÃÇk (T¬µk )m vk‚àí1 + xk (Eq. (6)) ,
= (T¬µk )m vk‚àí1
‚àí T¬µk ,ŒΩÃÇk T¬µk ,ŒΩ m‚àí1 ...T¬µk ,ŒΩk1 (T¬µk vk‚àí1 ) + xk (Eq. (6)),
k

j=1

Then dk+1 becomes:
dk+1 ‚â§ Œ≥P¬µ‚àó ,ŒΩÃÉk (v‚àó ‚àí vk ) + 0k+1
+

m‚àí1
X

Œ≥P¬µk+1 ,ŒΩ j

k+1

‚â§ T¬µk ,ŒΩÃÇk T¬µk ,ŒΩ m‚àí1 ...T¬µk ,ŒΩk1 vk‚àí1

‚â§ Œ≥P¬µ‚àó ,ŒΩÃÉk (v‚àó ‚àí vk ) + Œ≥P¬µ‚àó ,ŒΩÃÉk k ‚àí P¬µ‚àó ,ŒΩÃÉk k

k

= Œ≥P¬µk ,ŒΩÃÇk Œ≥P¬µk ,ŒΩ m‚àí1 ...Œ≥P¬µk ,ŒΩk1 (vk‚àí1 ‚àí T¬µk vk‚àí1 )
k

1
...Œ≥P¬µk+1 ,ŒΩk+1
bk ,

j=1

k

‚àí T¬µk ,ŒΩÃÇk T¬µk ,ŒΩ m‚àí1 ...T¬µk ,ŒΩk1 (T¬µk vk‚àí1 ) + xk (Eq. (5)),

1
...T¬µk+1 ,ŒΩk+1
T¬µk +1 vk ] see (5),

j=1

T¬µk ,ŒΩÃÇk is affine

+ (I ‚àí Œ≥P¬µk ,ŒΩÃÇk )k + 0k+1 .

1
...T¬µk+1 ,ŒΩk+1
T¬µk +1 vk ,

j=1

=T¬µk ,ŒΩÃÇk vk

= vk ‚àí k ‚àí T¬µk ,ŒΩÃÇk (vk ‚àí k )
{z
}
|

(T¬µk+1 )j vk ‚àí T¬µk+1 ,ŒΩ j

+ 0k+1 +

m‚àí1
X

+ xk ,

Œ≥P¬µk+1 ,ŒΩ j

k+1

1
...Œ≥P¬µk+1 ,ŒΩk+1
bk ,

j=1

‚â§ Œ≥P¬µk ,ŒΩÃÇk Œ≥P¬µk ,ŒΩ m‚àí1 ...Œ≥P¬µk ,ŒΩk1 bk‚àí1 + xk .
k

To bound dk+1 , we decompose it in the three following
terms:

‚â§ Œ≥P¬µ‚àó ,ŒΩÃÉk (v‚àó ‚àí ( vk ‚àí k )) ‚àíP¬µ‚àó ,ŒΩÃÉk k + 0k+1
| {z }
{z
}
|
(T¬µk )m vk‚àí1

+

m‚àí1
X

Œ≥P¬µk+1 ,ŒΩ j

k+1

dk+1 = v‚àó ‚àí (T¬µk+1 )m vk ,
= T¬µ‚àó v‚àó ‚àí T¬µ‚àó vk + T¬µ‚àó vk ‚àí T¬µk+1 vk
{z
} |
|
{z
}
1

2

+ T¬µk+1 vk ‚àí (T¬µk+1 )m vk .
{z
}
|
3

1 can be upper-bounded as follows:
In this equation, term 
T¬µ‚àó v‚àó ‚àí T¬µ‚àó vk
= T¬µ‚àó v‚àó ‚àí T¬µ‚àó ,ŒΩÃÉk vk with ŒΩÃÉk defined in Eq. (7),
‚â§ T¬µ‚àó ,ŒΩÃÉk v‚àó ‚àí T¬µ‚àó ,ŒΩÃÉk vk since ‚àÄŒΩ, T¬µ‚àó . ‚â§ T¬µ‚àó ,ŒΩ .
= Œ≥P¬µ‚àó ,ŒΩÃÉk (v‚àó ‚àí vk ).
2 is bounded by the greedy error:
By definition 

yk

1
...Œ≥P¬µk+1 ,ŒΩk+1
bk ,

j=1

‚â§ Œ≥P¬µ‚àó ,ŒΩÃÉk dk + yk
+

m‚àí1
X

Œ≥P¬µk+1 ,ŒΩ j

k+1

1
...Œ≥P¬µk+1 ,ŒΩk+1
bk .

j=1

Let us finally bound sk :
sk = (T¬µk )m vk‚àí1 ‚àí v¬µk ,
= (T¬µk )m vk‚àí1 ‚àí (T¬µk )‚àû vk‚àí1 ,
= (T¬µk )m vk‚àí1 ‚àí (T¬µk )m (T¬µk )‚àû vk‚àí1 ,
= (T¬µk )m vk‚àí1 ‚àí (T¬µk ,ŒΩ k )m (T¬µk )‚àû vk‚àí1 ,
with ŒΩ k defined in (8)
‚â§ (T¬µk ,ŒΩ k )m vk‚àí1 ‚àí (T¬µk ,ŒΩ k )m (T¬µk )‚àû vk‚àí1 since (5),
= (Œ≥P¬µk ,ŒΩ k )m (vk‚àí1 ‚àí (T¬µk )‚àû vk‚àí1 ),
‚àû
X
= (Œ≥P¬µk ,ŒΩ k )m ( (T¬µk )i vk‚àí1 ‚àí (T¬µk )i (T¬µk vk‚àí1 )),
i=0

T¬µ‚àó vk ‚àí T¬µk+1 vk ‚â§ 0k+1 (3) with ¬µ ‚Üê ¬µ‚àó , k ‚Üê k + 1

‚â§ (Œ≥P¬µk ,ŒΩ k )

‚àû
X
m

3 involves the bk quantity:
Finally, bounding term 
T¬µk+1 vk ‚àí (T¬µk+1 )m vk ,
=

m‚àí1
X

(T¬µk+1 )j vk ‚àí (T¬µk+1 )j+1 vk ,

j=1

‚â§ (Œ≥P¬µk ,ŒΩ k )m

Œ≥P¬µk ,ŒΩ i ...Œ≥P¬µk ,ŒΩ 1 (vk‚àí1 ‚àí T¬µk vk‚àí1 ),
k

i=0
‚àû
X

k

Œ≥P¬µk ,ŒΩ i ...Œ≥P¬µk ,ŒΩ 1 bk‚àí1 .
k

k

i=0

From linear recursive relations of the kind of Lemma 1,
Scherrer et al. (2012) show how to deduce a bound on the

Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games

Lp -norm of lk . This part of the proof being identical to that
of Scherrer et al. (2012), we do not develop it here. For
completeness however, we include it in Appendix A.1 of
the Supplementary Material.
Theorem 1. Let œÅ and œÉ be distributions over states. Let p,
q and q‚Äô be such that 1q + q10 = 1. Then, after k iterations,
we have:

4. Application
The analysis of error propagation presented in Sect. 3.2 is
general enough to develop several implementations. From
the moment one can control the error made at each iteration
step, the bound presented in Theorem 1 applies.
4.1. Algorithm

1

klk kp,œÅ

2(Œ≥ ‚àí Œ≥ k )(Cq1,k,0 ) p
‚â§
(1 ‚àí Œ≥)2

sup
1‚â§j‚â§k‚àí1

kj kpq0 ,œÉ ,

1

 0
(1 ‚àí Œ≥ k )(Cq0,k,0 ) p
j  0 ,
+
sup
pq ,œÉ
(1 ‚àí Œ≥)2
1‚â§j‚â§k
+

1
2Œ≥ k
(Cqk,k+1,0 ) p min(kd0 kpq0 ,œÉ , kb0 kpq0 ,œÉ ).
1‚àíŒ≥

where
Cql,k,d =

k‚àí1 ‚àû
(1 ‚àí Œ≥)2 X X j
Œ≥ cq (j + d),
Œ≥l ‚àí Œ≥k
j=i
i=l

with the following norm of a Radon-Nikodym derivative:


 d(œÅP¬µ1 ,ŒΩ1 ...P¬µj ,ŒΩj ) 

 .
cq (j) =
sup


dœÉ
¬µ1 ,ŒΩ1 ,...,¬µj ,ŒΩj
q,œÉ
Remark 4. The Radon-Nikodym derivative of measure œÅ
on S with respect to measure œÉ on S is in the discrete case
œÅ(s)
the function h defined by: h(s) = œÉ(s)
when œÉ(s) 6= 0, ‚àû
otherwise.
Remark 5. If player 2 has no influence on the game, then
the Œ≥-discounted two-player zero-sum SG is simply a Œ≥discounted MDP. In that case the bound is the same as in
Scherrer et al. (2012).
Remark 6. In the case of a SG with no discount factor
but with an absorbing state the expression given in lemma
1 is still valid. Instead of having a Œ≥-discounted transition
kernel we would have a simple transition kernel. And if this
transition kernel has the following property


l
Y



‚àÉl,
sup
|  P¬µi ,ŒΩi  ‚â§ Œ≥ < 1,


¬µ0 ,ŒΩ0 ,...,¬µl ,ŒΩl
i=0

‚àû

we could still have upper bounds on the propagation of errors.
Remark 7. One should notice that when p tends to infinity,
the bound becomes:
lim inf klk k‚àû ‚â§
k‚Üí+‚àû

2Œ≥
1
+
0 ,
(1 ‚àí Œ≥)2
(1 ‚àí Œ≥)2

where  and 0 are respectively the sup of errors at the
evaluation step and the sup of errors at the greedy step in
‚àû-norm. We thus recover the bounds computed by Patek
(1997).

In this section, we present the Approximate Generalized
Policy Iteration-Q (AGPI-Q) algorithm which is an extension for SG of Fitted-Q. This algorithm is offline and
uses the so-called state-action value function Q. The stateaction value function extends the value function by adding
two degrees of freedom for the first action of each player.
More formally, the state-action value function Q¬µ,ŒΩ (s, a, b)
is defined as
X
Q¬µ,ŒΩ (s, a, b) = E[r(s, a, b)] +
p(s0 |s, a, b)v¬µ,ŒΩ (s0 ).
s0 ‚ààS

We
assume
we
are
given
some
samples
((xj , aj1 , aj2 ), rj , x0j )j=1,...,N and an initial Q-function
(here we chose the null function). As it is an instance of
AGPI, each iteration of this algorithm is made of a greedy
step and an estimation step. The algorithm is precisely
described in Algorithm 1.
Algorithm 1 AGPI - Q for Batch sample
Input: ((xj , aj1 , aj2 ), rj , x0j )j=1,...,N some samples,
q0 = 0 a Q-function,
F an hypothesis space
for k=1,2,...,K do
Greedy step:
for all j do
a¬Øj = arg maxaÃÑ minbÃÑ qk‚àí1 (x0j , aÃÑ, bÃÑ) (solving a
matrix game)
end for
Evaluation step:
qk,0 = qk‚àí1
for i=1,...,m do
for all j do
q j = r(xj , aj1 , aj2 ) + Œ≥ minb qk,i‚àí1 (x0j , a¬Øj , b)
end for
N
P
qk,i = arg minq‚ààF
l(q(xj , aj1 , aj2 ), q j )
j=1

Where l is a loss function.
qk = qk,m
end for
end for
output qK
For the greedy step, the minimax policy for the maximizer
on each matrix game defined by (qk (x0j , a, b))a,b . In general, this step involves solving N linear programs; recall

Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games

that in the case of a turn-based game this step reduces to
finding a maximum. The evaluation step involves solving the MDP with an horizon m for the minimizer. This
part is similar to fitted-Q iteration. At each step, we try
to find the best fit over our hypothesis space for the next
Q-function according to some loss function l(x, y) (often,
l(x, y) = |x ‚àí y|2 ).
4.2. Analysis
For this algorithm, we have 0k = 0 and k the error made
on qk at each iteration. Let us note k,i the error or fitting
the Q-function on the feature space. The Bellman operator in the case of actions value function Q(s, a1 , a2 ) for
policy ¬µ and ŒΩ is (T¬µ,ŒΩ Q)(s, a1 , a2 ) = r(s, a1 , a2 ) +
Œ≥P¬µ,ŒΩ (Q(., ¬µ(.), ŒΩ(.))). The other non-linear operators
are analogous to those defined in Sec. 2.2. In this section
the operator used is the one on Q-function.
We have qk,i+1 = T¬µk qk,i + i . Let us define ŒΩk,i such as
T¬µk m qk,0 = T¬µk ,ŒΩk,m‚àí1 ...T¬µk ,ŒΩk,0 qk,0 . Furthermore we
have qk,i+1 ‚â§ T¬µk ,ŒΩk ,i qk,i + k,i . On the one hand, we
have:
k = qk,m ‚àí T¬µk

m

qk,0 ,

‚â§ T¬µk ,ŒΩk,m‚àí1 ...T¬µk ,ŒΩk,0 qk,0
+

m‚àí1
X

with
0

0 0
Cql,k,l ,k ,d

k‚àí1
‚àí1 X
‚àû
X kX
(1 ‚àí Œ≥)3
Œ≥ j cq (j + d).
= l
(Œ≥ ‚àí Œ≥ k )(Œ≥ l0 ‚àí Œ≥ k0 )
0
0
0
i=l i =l j=i+i

4.3. Complexity analysis
At the greedy step, the algorithm solves N minimax equilibria for a zero-sum matrix game. This is usually done
by linear programming. For state s, the complexity of
such an operation is the complexity of solving a linear
program with cs = 1 + card(A1 (s)) + card(A2 (s))
constrains and with card(A1 (s)) variables. Let us note
L(cs , card(A1 (s))) this complexity. Then, the complexity of this step is bounded by N L(c, a) (with c =
sup
cs and a =
sup
card(A1 (s))). Uss‚àà{x01 ,...,x0N }

s‚àà{x01 ,...,x0N }

ing the simplex method, L(c, a) may grow exponentially
with c while with the interior point method, L(c, a) is
O(a3.5 ) (Karmarkar, 1984). The time to compute qj in
the evaluation step depends on finding a maximum over
A2 (x0j ). And the regression complexity to find qk,i depends on the regression technique. Let us note this complexity R(N ). Finally, the complexity of this step is
mR(N ).
The overall complexity is thus O(N L(c, a) + mR(N )); in
general, the complexity of solving the linear program will
be the limiting factor.

P¬µk ,ŒΩk,m‚àí1 ...P¬µk ,ŒΩk,i+1 k,i

i=0

‚àí T¬µk ,ŒΩk,m‚àí1 ...T¬µk ,ŒΩk,0 qk,0 ,
‚â§

m‚àí1
X

P¬µk ,ŒΩk,m‚àí1 ...P¬µk ,ŒΩk,i+1 k,i .

5. Experiments
(9)

i=0

On the other hand (with ŒΩÃÉk,i as qk,i+1 = T¬µk ,ŒΩÃÉk,i qk,i +
k,i ), we have:
k = qk,m ‚àí T¬µk

m

qk,0 ,

‚â• T¬µk ,ŒΩÃÉk,m‚àí1 ...T¬µk ,ŒΩÃÉk,0 qk,0
+

m‚àí1
X

P¬µk ,ŒΩÃÉk,m‚àí1 ...P¬µk ,ŒΩÃÉk,i+1 k,i

i=0

‚àí T¬µk ,ŒΩÃÉk,m‚àí1 ...T¬µk ,ŒΩÃÉk,0 qk,0 ,
‚â•

m‚àí1
X

P¬µk ,ŒΩÃÉk,m‚àí1 ...P¬µk ,ŒΩÃÉk,i+1 k,i .

(10)

i=0

From these inequalities, we can provide the following
bound (the proof is given in Appendix B):
klk kp,œÅ ‚â§

2(Œ≥ ‚àí Œ≥ k )(1 ‚àí Œ≥ m ) 1,k,0,m,0 p1
(Cq
) sup ki,l kpq0 ,œÉ
(1 ‚àí Œ≥)3
i,l

+

1
2Œ≥ k
(C k,k+1,0 ) p min(kd0 kpq0 ,œÉ , kb0 kpq0 ,œÉ ),
1‚àíŒ≥ q
(11)

In this section, AGPI-Q is tested on the Alesia game described in Sec. 2.5 where we assume that both players start
with a budget n = 20. As a baseline, we use the exact
solution of the problem provided by VI. We have run the
algorithm for K = 10 iterations and for m ‚àà {1, 2, 3, 4, 5}
evaluation steps. We have considered different sample set
sizes, N = 2500, 5000, 10000. Each experiment is repeated 20 times. First, we generate N uniform samples
(xj ) over the state space. Then, for each state, we draw uniformly the actions of each player in the set of their own action space in that state aj1 ‚àº U(A1 (xj )), aj2 ‚àº U(A2 (xj )),
rj = r(xj , aj1 , aj2 ) and compute the next state x0j . As hypothesis space, we use CART trees (Breiman et al., 1984)
which exemplifies the non-parametric property of the algorithm.
The performance of the algorithm is measured as the
mean-squared error between the value function vK (s) =
minb maxa qK (s, a, b) where qK is the output of the algorithm AGPI-Q and the actual value function computed via
VI. Figure 1 shows the evolution of performance along iterations for N = 10000 for the different values of the parameter m. Figure 2 shows the exact value function (Fig. 2(a))
and the approximated one vK (Fig. 2(b)). The complete list
of experiments results can be found in the supplementary

Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games

file, especially for different size of sample set N = 2500
and N = 5000.
For each size of sample set, the asymptotic convergence is
better for small values of m. This is conform to Eq. (11), in
k
)(1‚àíŒ≥ m )
increases with m. However,
which the term 2(Œ≥‚àíŒ≥
(1‚àíŒ≥)3
for small values of k, the mean-squared error is reducing
when m is increasing. This is coherent with experimental
results when using MPI for MDP: the bigger m, the higher
the convergence rate. The price to pay for this acceleration of convergence towards the optimal value is an heavier evaluation step. This is similar to results in the exact
case (Puterman, 1994). Overall, this suggests to use large
values of m at the beginning of the algorithm and to reduce
it as k grows to get a smaller asymptotic error.

6. Conclusion and Perspectives
This work provides a novel and unified error propagation
analysis in Lp -norm of well-known algorithms (API, AVI
and AGPI) for zero-sum two-player SGs. It extends the error propagation analyses of Scherrer et al. (2012) for MDPs
to zero-sum two-player SGs and of Patek (1997) which is
an L‚àû -norm analysis for only API. In addition, we provide a practical algorithm (AGPI-Q) which learns a good
approximation of the Nash Equilibrium from batch data
provided in the form of transitions sampled from actual
games (the dynamics is not known). This algorithm is an
extension of Fitted-Q for zero-sum two-player SGs and can
thus be non-parametric. No features need to be provided
or hand-crafted for each different application which is a
significant advantage. Finally, we empirically demonstrate
that AGPI-Q performs well on a simultaneous two-player
game, namely Alesia.
It appears that the provided bound is highly sensitive to Œ≥
(which is a common problem of ADP). This is critical and
further work should concentrate on reducing the impact of
Œ≥ in the final error bound. Since non-stationary policies
can reduce the impact of Œ≥ in MDP (Scherrer & Lesner,
2012), extensions of this work to zero-sum two-player SGs
is forecasted. Moreover, we intend to apply AGPI-Q to
large scale games and implement it on real data.

References
Antos, A., SzepesvaÃÅri, C., and Munos, R. Fitted-Q Iteration
in Continuous Action-Space MDPs. In Proc. of NIPS,
pp. 9‚Äì16, 2008.
Figure 1. Mean-squared error (y-axis) between the estimated
value function and the true value function at step k (x-axis). For
n = 20 and N = 10000

Bertsekas, D. P. Dynamic Programming and Optimal Control, volume 1. Athena Scientific Belmont, MA, 1995.
Bowling, M. and Veloso, M. Rational and Convergent
Learning in Stochastic Games. In Proc. of IJCAI, volume 17, pp. 1021‚Äì1026, 2001.
Breiman, L., Friedman, J., Stone, C. J., and Olshen, R. A.
Classification and Regression Trees. CRC press, 1984.
Busoniu, L., Babuska, R., and De Schutter, B. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, 38(2):156‚Äì172,
March 2008.

(a) Exact value for the Alesia (b) Value computed by AGPIgame for player‚Äôs budget 0...20 Q for the game of Alesia for
and for token position 3
player‚Äôs budget 0...20 and for
token position 3
Figure 2. Value functions at token position 3

Ernst, D., Geurts, P., and Wehenkel, L. Tree-Based Batch
Mode Reinforcement Learning. In Journal of Machine
Learning Research, pp. 503‚Äì556, 2005.
Farahmand, A.-M., SzepesvaÃÅri, C., and Munos, R. Error
Propagation for Approximate Policy and Value Iteration.
In Proc. of NIPS, pp. 568‚Äì576, 2010.

Approximate Dynamic Programming for Two-Player Zero-Sum Markov Games

Gabillon, V., Lazaric, A., Ghavamzadeh, M., and Scherrer,
B. Classification-Based Policy Iteration with a Critic. In
Proc. of ICML, pp. 1049‚Äì1056, 2011.

Scherrer, B., Ghavamzadeh, M., Gabillon, V., and Geist,
M. Approximate Modified Policy Iteration. In Proc. of
ICML, 2012.

Greenwald, A., Hall, K., and Serrano, R. Correlated Qlearning. In Proc. of ICML, volume 3, pp. 242‚Äì249,
2003.

Shapley, L. S. Stochastic Games. Proceedings of the National Academy of Sciences of the United States of America, 39(10):1095, 1953.

Hansen, T. D., Miltersen, P. B., and Zwick, U. Strategy Iteration is Strongly Polynomial for 2-Player TurnBased Stochastic Games with a Constant Discount Factor. JACM, 60(1):1, 2013.

Van Der Wal, J. Discounted Markov Games: Generalized
Policy Iteration Method. Journal of Optimization Theory
and Applications, 25(1):125‚Äì138, 1978.

Hu, J. and Wellman, M. P. Nash Q-Learning for GeneralSum Stochastic Games. JMLR, 4:1039‚Äì1069, 2003.
Karmarkar, N. A New Polynomial-time Algorithm for Linear Programming. In Proc. of ACM Symposium on Theory of Computing, pp. 302‚Äì311, 1984.
Lagoudakis, M. G. and Parr, R. Least-squares policy iteration. Journal of Machine Learning Research, pp. 1107‚Äì
1149, 2003a.
Lagoudakis, M. G. and Parr, R. Reinforcement Learning as
Classification: Leveraging Modern Classifiers. In Proc.
of ICML, volume 3, pp. 424‚Äì431, 2003b.
Lagoudakis, Michail G and Parr, Ronald. Value function
approximation in zero-sum markov games. In Proc. of
UAI, pp. 283‚Äì292, 2002.
Lazaric, A., Ghavamzadeh, M., Munos, R., et al. Analysis
of a Classification-Based Policy Iteration Algorithm. In
Proc. of ICML, pp. 607‚Äì614, 2010.
Littman, M. L. Markov Games as a Framework for MultiAgent Reinforcement Learning. In Proc. of ICML, volume 94, pp. 157‚Äì163, 1994.
Meyer, Christophe, Ganascia, Jean-Gabriel, and Zucker,
Jean-Daniel. Learning strategies in games by anticipation. In IJCAI 97, August 23-29, 1997, 2 Volumes, pp.
698‚Äì707, 1997.
Munos, R. and SzepesvaÃÅri, C. Finite-time bounds for fitted
value iteration. JMLR, 9:815‚Äì857, 2008.
Patek, S. D. Stochastic Shortest Path Games: Theory and
Algorithms. PhD thesis, Massachusetts Institute of Technology, Laboratory for Information and Decision Systems, 1997.
Puterman, M. L. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. John Wiley & Sons,
1994.
Scherrer, B. and Lesner, B. On the Use of Non-Stationary
Policies for Stationary Infinite-Horizon Markov Decision Processes. In Proc. of NIPS, pp. 1826‚Äì1834, 2012.

Von Neumann, J. Morgenstern, 0.(1944) theory of games
and economic behavior. Princeton: Princeton UP, 1947.

