Large-Scale Markov Decision Problems with KL Control Cost
and its Application to Crowdsourcing

Yasin Abbasi-Yadkori
Queensland University of Technology

YASIN . ABBASIYADKORI @ QUT. EDU . AU

Peter L. Bartlett
University of California, Berkeley and Queensland University of Technology

BARTLETT @ CS . BERKELEY. EDU

Xi Chen
Stern School of Business, New York University

XICHEN @ NYU . EDU

Alan Malek
University of California, Berkeley

Abstract
We study average and total cost Markov decision
problems with large state spaces. Since the computational and statistical cost of finding the optimal policy scales with the size of the state space,
we focus on searching for near-optimality in a
low-dimensional family of policies. In particular, we show that for problems with a KullbackLeibler divergence cost function, we can recast
policy optimization as a convex optimization and
solve it approximately using a stochastic subgradient algorithm. This method scales in complexity with the family of policies but not the state
space. We show that the performance of the
resulting policy is close to the best in the lowdimensional family. We demonstrate the efficacy
of our approach by optimizing a policy for budget
allocation in crowd labeling, an important crowdsourcing application.

1. Introduction
This paper studies Markov decision problems (MDPs) with
expected average cost and total cost criteria. Given a transition matrix and loss function, the objective is to find a
near-optimal policy in a reasonable amount of time. Let
X be the state space with finite cardinality X ∈ N, A be
the action space with finite cardinality A ∈ N, and `(x, a)
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

MALEK @ EECS . BERKELEY. EDU

be the loss of taking an action a ∈ A at the state x ∈ X .
The average cost MDP policy design problem is to find the
optimal state-to-action mapping (a.k.a.
i
hPpolicy) π that minit
1
π
π
mizes the average cost limt→∞ t E
s=1 ` (xs , π(xs )) ,
where xπs is the state at time s under policy π. We will
write the transition matrix as P ∈ R(X×A)×X and use the
shorthand P(x,a),: for the (x, a) row of P , i.e. the distribution of the state subsequent to x under action a. The total
cost case is similar and the explicit formulation is given in
the next section.
Finding the optimal policy is equivalent to solving the following linear program (Manne, 1960),
max

λ∈R,h∈RX

λ,

s.t.

∀x ∈ X , (Lh)(x) ≤ λ + h(x) ,

where L is the Bellman operator defined by

(Lh)(x) = min `(x, a) + P(x,a),: h .
a∈A

(1)

The scalar λ is the average loss and the vector h is called
the differential value function. An optimal action achieves
the minimum on the right-hand side of (1).
In general, exactly solving an MDP is computationally expensive (it is known to be P-complete (Papadimitriou &
Tsitsiklis, 1987)). The widely used policy iteration and
value iteration approaches require at least O(X 2 A) periteration complexity which precludes their use for problems with large state spaces. For large problems, it is reasonable to constrain the search to a low-dimensional family of policies. In particular, we follow the popular approach (Schweitzer & Seidmann, 1985; de Farias & Van
Roy, 2003; Sutton & Barto, 1998) of using a linear approximation for the value function: for some fixed feature ma-

Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing

trix Ψ ∈ RX×d with d much smaller than X, we will approximate h(x) by (Ψw)(x) where w is a d-dimensional
parameter vector.
This linear restriction has been famously applied to the LP
formulation (Schweitzer & Seidmann, 1985) to obtain
max

λ∈R,w∈Rd

λ , s.t. ∀x ∈ X , (LΨw)(x) ≤ λ + (Ψw)(x) .

Unfortunately, the problem is still difficult to solve because
of the large number of constraints.
We can encapsulate this optimization problem by defining
the Bellman error as the gap in the Bellman optimality condition, i.e. kLh − h − λk1 . Then, for some distribution u,
solving the following optimization problem is a good proxy
for the LP:
min u> (LΨw − Ψw) ,
(2)
w

Appendix A explicitly relates optimal values of (2) to values of the LP; basically, solutions to (2) are nearly optimal
in the low dimensional class of policies



π : ∃w, ∀x, π(x) = argmin `(x, a) + P(x,a),: Ψw .
a∈A

Unfortunately, (2) is not convex (the Bellman operator is
a minimum over linear functions). Our contribution is
to identify an important class of MDPs which allow us
to transform (2) into a convex optimization and therefore
solve it efficiently. Specifically, we consider the class
of linearly solvable MDPs introduced by Todorov (2006;
2009a). Given some finite state space X , the action space
P is the space of state transition matrices, i.e. all X × X
matrices with rows that are probability distributions over
the state space. Given some function q : X → [0, Q] and
some fixed transition matrix P0 ∈ P, the KL loss function
is defined as
`(x, P ) = q(x) +

X
x0 ∈X

P (x, x0 ) log

P (x, x0 )
.
P0 (x, x0 )

(3)

Note that if there is an x0 ∈ X with P (x, x0 ) > 0 and
P0 (x, x0 ) = 0, then `(x, P ) = ∞. Thus, the decision
maker is forced to only play distributions that are absolutely continuous with P0 . The P0 distribution can be
thought of as representing some passive dynamics or base
policy. For many problems (e.g., the crowdsourcing application in Section 4), the transition matrix P can be transformed into a distribution µ over the action space such that
taking an action according to µ in state x results in a transition to a state distributed according to P (x, ·). The KL
cost tends to keep explored policies in the vicinity of P0 .
This is useful in the setting when we already have a good
policy or heuristic that we would like to optimize further or

when there is uncertainty in the model specification and we
do not want to deviate too much.
The beauty of using KL cost, as shown in Todorov (2009a),
is that the Bellman operator becomes a linear operator after
an exponential transformation h 7→ exp(−h). Todorov exploited this fact by solving for the optimal policy directly.
We exploit this by linearly parameterizing exp(−h) ≈ Φw,
which allows us to recast (2) as a convex optimization in w.
We reduce the number of constraints by sampling and using
a stochastic subgradient descent method. Our method can
find a near optimal solution in the low-dimensional class
while maintaining a computational cost that is independent
of the size of the state space.
Note that exploiting the linear solvability of the KL control
cost will not scale to large state spaces. Even in the best
case, the computational cost of solving such linear problems is Ω(X). Why is even Ω(X) too high? For many realistic problems, the state space size grows exponentially;
for queueing networks, the state space is exponential in the
number of queues, and for budget allocation, in the time
horizon. As a specific example, the crowdsourcing application presented in Section 4 has around 20500 ≈ 3 × 10650
states.
For these reasons, it is essential to develop algorithms
whose computation is independent of X. Our contribution
is to present such a method that provably finds the best policy in a restricted class.
1.1. Related Work
Popular approaches to solve large-scale MDPs include Approximate Dynamic Programming (ADP) and Approximate Linear Programming (ALP). One can find theoretical
results on ADP in Bertsekas & Tsitsiklis (1996); Sutton &
Barto (1998); Bertsekas (2007) and more recent papers on
Temporal-Difference (TD) methods (Sutton et al., 2009b;a;
Maei et al., 2009; 2010). ALP methods, first introduced
in Schweitzer & Seidmann (1985), were improved and analyzed more thoroughly in a long string of papers including (de Farias & Van Roy, 2003; 2004; 2006; Hauskrecht
& Kveton, 2003; Guestrin et al., 2004; Petrik & Zilberstein,
2009; Desai et al., 2012; Veatch, 2013).
As noted by Desai et al. (2012), the prior work on ALP
and ADP either requires access to samples from specialized
distributions (such as a distribution that depends on the optimal policy) or assumes the ability to solve an LP with as
many constraints as states. (See for example, Appendix A
in Abbasi-Yadkori et al. (2014) for a more detailed discussion.) Asymptotic behavior of TD methods (Sutton, 1988)
in large state and action spaces was studied both in onpolicy (Tsitsiklis & Van Roy, 1997) and off-policy (Sutton
et al., 2009b;a; Maei et al., 2009) settings. All these results

Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing

concern the policy estimation problem, i.e., estimating the
value of a fixed policy. The available results for the control
problem, i.e., estimating the value of the optimal policy,
are more limited (Maei et al., 2010) and prove only convergence to a local optimum of some objective functions.

In the KL cost setting, the Bellman operator is defined as
!
X
0
0
(LJ)(x) := min `(x, P ) +
P (x, x )J(x )
(5)

Todorov (2009b) and Zhong & Todorov (2011a;b) propose
computationally efficient algorithms for linearly solvable
MDPs with large state spaces. These methods however lack
theoretical guarantees. In this paper, we provide strong theoretical guarantees for the log-linear value function approximation.

and the optimal value function satisfies LJ = J. Given
some function J, the minimizer of (5) is called the greedy
policy with respect to J, denoted PJ . The KL cost has the
property that the greedy action is trivial to compute:
(
)
X
0
0
PJ (x, :) = argmin `(x, P ) +
P (x, x )J(x )

Abbasi-Yadkori et al. (2014) studied average cost MDPs
in the dual space with stationary distributions as variables
and showed a convex optimization reduction. They propose computationally efficient algorithms and show that the
quality of the solution is close to the best in the comparison class. In this work, we study the problem in the primal space with value functions as variables. In contrast to
solving MDPs in the dual space, we encounter additional
difficulty arising from the non-convexity of the resulting
optimization problem.
1.2. Notation
We use M (i, :) and M (:, j) to denote the ith row and jth
column of matrix M , respectively. For somePpositive vector C, we define scaled norms kvk1,c =
i ci |vi | and
kvk∞,c = maxi ci |vi |. The all ones vector in denoted 1.
Finally, for vectors v and w, the following operations are
always element-wise: v ≤ w, max(v, w), min(v, w) and
v  w, where the latter denotes multiplication.

The first part of this section defines the KL control cost
MDP and demonstrates why it is linearly solvable. The
second part describes our contribution: we reduce a highdimensional KL control cost MDP into a tractable problem
on a low dimensional subspace and present an algorithm
which does provably well with respect to the best in the
subclass of policies. The next section extends this analysis
to the average cost case.
Starting from some initial state x1 , the total cost is

J∗ (x1 ) = min E
P ∈P

∞
X

#
`(xt , P )

x0 ∈X

P ∈P

x0 ∈X

P (x, x0 )
= argmin
P (x, x ) log
P0 (x, x0 )e−J(x0 )
P ∈P
x0 ∈X
X



0



.

(4)

t=1

To ensure this is well defined, we assume that the MDP hits
a terminating (goal) state z with q(z) = 0 and P0 (z, z) =
1, i.e. z is an absorbing state. This implies that once the
learner reaches z, the only reasonable policy is P (z, z) = 1
and thus J∗ (z) = 0.

,

where we use the definition of `(x, P ) in (3). Using the fact
that the KL divergence of P and P is 0, the minimum is
0

PJ (x, x0 ) =

P0 (x, x0 )e−J(x )
,
Z(x)

(6)

P
0 −J(x0 )
where Z(x) =
is the normalizing
x0 P0 (x, x )e
factor. This allows us to write the Bellman operator as
X
(LJ)(x) = `(x, PJ ) +
PJ (x, x0 )J(x0 )
x0 ∈X

= q(x) +

X
x0 ∈X

= q(x) +

X


PJ (x, x0 )
0
+ J(x )
PJ (x, x ) log
P0 (x, x0 )
0



PJ (x, x0 ) log

x0 ∈X

1
Z(x)

= q(x) − log Z(x).

2. MDPs with Total Cost Criterion

"

P ∈P

This formula was exploited in Todorov (2006) by noting
that exponentiating Bellman’s optimality equation yields
the equations
e−J(x) = e−(LJ)(x) = e−q(x) Z(x) = e−q(x) P0 (x, :)e−J .
In vector form, we have e−J = e−q P0 e−J , which is linear
and will yield the optimal value function, J ∗ . The optimal
policy is PJ ∗ .
While this result is striking, it does not directly yield efficiently computable solutions since we would need to solve
an eigenvalue problem in X dimensions. Our contribution
is to formulate a computationally efficient algorithm guaranteed to return the best policy from a restricted class of
policies.
Motivated by the exponential transformation above, consider the class of value functions
J = {x 7→ Jw (x) := − log(Ψ(x, :)w) : w ∈ W} , (7)

Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing

where Ψ ∈ RX×d is a feature matrix and W ⊂ Rd is some
bounded set. This parameterization is different from ADP
and ALP which use a linear combination of basis functions.
The logarithmic transformation ensures that after the exponentiation the transformed value function is linear. Additionally, to keep the value functions well defined, we make
the following positivity assumption on features Ψ and the
set W: we assume that there exists a constant g > 0 such
that for any w ∈ W and any state x, Ψ(x, :)w ≥ g.
In the following section, we solve the problem: given an
MDP specified by q and P0 and a class of value functions
J , find parameters w
b such that1
JPJwb (x1 ) ≈ min JPJw (x1 )
Jw ∈J

(8)

in time polynomial in d and independent of X. We assume
that access to arbitrary entries of Ψ, q, P0 , or e−q P0 Ψ takes
unit time.
2.1. Recasting as a Convex Optimization
Recall from the introduction that the Bellman error was defined as the gap in the Bellman optimality equation. For
the case of total cost, this is |LJw (x) − Jw (x)|. Also, the
introduction and Appendix A argued that keeping the Bellman error small ensured that Jw was a good approximation
to the value function. Since the Bellman operator evaluates
to LJ = q − log Z, choosing e−Jw (x) = Ψ(x, :)w as our
parameterization of Jw from (7) causes the Bellman operator to become e−(LJw )(x) = e−q(x) P0 (x, :)Ψw. Thus,
e−Jw (x) − e−(LJw )(x) = Ψ(x, :)w − e−q(x) P0 (x, :)Ψw,
which is linear in w. This implies that |e−Jw (x) −
e−(LJw )(x) | is convex in w and can be minimized efficiently. Finally, we argue that minimizing the exponentiated Bellman error will force the Bellman error
|LJw (x) − Jw (x)| to be small since


0
0

e− max{u,u } |u − u0 | ≤ e−u − e−u 
(9)
0

≤ e− min{u,u } |u − u0 | .
We have now motivated our KL-cost convex optimization
formulation. Let T be the space of trajectories starting at
state x1 and ending at state z and let v be some probability distribution over T . For H some positive constant, we
choose w as the minimizer of the cost function
c(w) := − log(Ψ(x1 , :)w)
(10)

X
X 

+H
v(T )
Ψ(x, :)w − e−q(x) P0 (x, :)Ψw ,
T ∈T
1

x∈T

JPJ is the value function under the policy PJ .

where the − log(Ψ(x1 , :)w) term is the approximate value
of the policy, Jw (x1 ), and the summation is an estimate of
the Bellman error of Jw . In other words, we are approximately solving the constrained optimization problem:
min Jw (x1 ) s.t.

w∈W

e−Jw (x) − e−(LJw )(x) = 0 ∀x ∈ X

by introducing the constraints as a regularization instead.
The intuition is that strict feasibility of w is not necessary
as it will still produce a valid policy. Instead, we only need
the regularization to ensure that Jw (x1 ) is a good estimate
for JPJw .
2.2. Approximate Minimization Algorithm
In the next section, we give upper bounds on the error of
any -optimal w
b (i.e. w ∈ W, c(w)
b ≤ c(w) + ). In
principle, the precise method for obtaining an approximate
minimizer is not central to the analysis: the novel ideas
are in relating the convex objective (10) to the underlying
MDP. One caveat is that, for large problems, we cannot sum
over T . Thus, we will describe one particular method for
handling this difficulty.
An unbiased estimate of the subgradient of (10) is easy to
calculate as it decomposes for each T ∈ T . In fact, it is
straightforward to see that if we sample a trajectory T ∼ v,
then the following is an unbiased estimate of the subgradient:


1
r(w) = −
Ψ(x1 , :)
(11)
(Ψ(x1 , :)w)
"


X
+H
sign Ψ(x, :)w − e−q(x) P0 (x, :)Ψw
x∈T

#


Ψ(x, :) − e−q(x) P0 (x, :)Ψ .
Since we have a tractable subgradient estimation, a natural class of algorithms to minimize c(w) is stochastic subgradient descent methods (see e.g. Kushner & Yin (2003)
and references therein) which can obtain an -optimal solution in O( 12 ) iterations with high probability under certain
mild conditions. The details of the stochastic subgradient
method are provided in Figure 1. We note that the recently
developed accelerated or distributed stochastic subgradient
algorithms (e.g., Sra et al. (2011); Zinkevich et al. (2010);
Duchi et al. (2012) and references therein) or different averaging schemes of {wt } to obtain w
bT (Shamir & Zhang,
2013) can all be applied to minimize c.
As our algorithm is based on a stochastic subgradient
method, the per-step computation cost is linear in the number of features and the algorithm can be adapted to the online and offline settings.

Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing

Input: Starting state x1 , number of rounds N , constant H, a decreasing sequence of step sizes (ηt ), a
distribution v over trajectories.
Let ΠW be the Euclidean projection onto W.
Initialize w1 = 0.
for t := 1, 2, . . . , N do
Sample trajectory (x1 , a1 , . . . , z) ∼ v.
Compute the stochastic subgradient rt defined by
(11).
Update wt+1 = ΠW (wt − ηt rt ).
end for P
T
w
bT = T1 t=1 wt .
Return policy PJwb T defined by (6), (7).
Figure 1. The Stochastic Subgradient Method for Total Cost
Markov Decision Processes

2.3. Analysis
Assume that some approximate optimization algorithm
(such as a stochastic gradient method) produces a w
b that
is -optimal, i.e., for any w ∈ W, c(w)
b ≤ c(w) + . The
first step in the analysis is to relate the suboptimality of w
b
to the suboptimality of the policy. Specifically, we bound
the gap between the value function under the policies PJwb
and PJw for any w ∈ W in terms of the Bellman errors.
Recall that g is a lower bound on Ψ(x, :)w, and Q is an
upper bound on q.
Theorem 1. Assume that w
b is -optimal and choose any
H ≥ eQ−log g . Then, for any w ∈ W with lw =
min(Jw , LJw ), we have,
JPJwb (x1 ) − JPJw (x1 ) ≤ 
+ kPJwb − vk1 max
T ∈T

+

X

PJw (T )

T ∈T

+H

X
T ∈T

X

(12)
X

|Jwb (x) − LJwb (x)|

x∈T

|Jw (x) − LJw (x)|

x∈T

v(T )

X

e−lw (x) |Jw (x) − LJw (x)| ,

x∈T

where, with an abuse of notation, PJ (T ) denotes the probability of trajectory T under transition dynamics PJ .
The full proof is in Appendix B, but below is a rough outline. We first bound the gap between value function Jw
and the value function of the greedy policy w.r.t. Jw (i.e.,
JPJw ) in terms of the Bellman error |Jw (x) − LJw (x)| at
w. This indicates that if the Bellman error at w is small,
then we have Jw ≈ JPJw . The gap between Jw and Jwb can
be bounded using the fact that w
b is an -optimal solution of
the convex optimization problem. We thus have control of
the gaps between: Jw and JPJw ; Jwb and JPJwb ; and Jw and
Jwb . Combining these yields the bound between JPJwb (x1 )
and JPJw (x1 ) in Theorem 1.

In addition to the optimality gap , there are three terms in
the right hand side of the inequality (12). The first term
is related to the Bellman error at w.
b This term becomes
small whenever the Bellman error at w
b is small and/or the
sampling distribution v is close to PJwb . Unfortunately, PJwb
itself depends on v. Note that such dependencies also exist
in the results of de Farias & Van Roy (2003; 2006); Desai et al. (2012). Controlling this distribution mismatch
term remains a challenging open problem. The second
term
P in the bound is the expectation of the Bellman error
x∈T |Jw (x) − LJw (x)| under the distribution over T determined by the transition dynamics PJw , while the third
term is the expectation of the Bellman error under the distribution v over T . Thus, both the second and third terms
are small if one can find a Jw ∈ J which leads to a small
Bellman error. The size of the Bellman error at w is determined by the value function class J in (7), which depends on the feature matrix Ψ. This point can be seen more
clearly from the following equivalent statement of Theorem
1,

	
JPJwb (x1 ) ≤ inf JPJw (x1 ) + V (Jw ) + 
Jw ∈J
X
+ kPJwb − vk1 max
|Jwb (x) − LJwb (x)| ,
T ∈T

V (Jw ) = H

X
T ∈T

+

X
T ∈T

v(T )

X

e

x∈T

−lw (x)

|Jw (x) − LJw (x)|

x∈T

PJw (T )

X

|Jw (x) − LJw (x)| .

x∈T

As we can see, V : J → R+ can be viewed as a “penalty
function" which represents how far Jw is from the optimal
value function, since V (Jw ) = 0 if LJw = Jw (i.e., the
Bellman error is zero). Theorem 1 shows that the value
function of the policy corresponding to w,
b i.e., JPJwb (x1 ), is
upper bounded by the best possible greedy policy w.r.t. Jw
measured by the sum of the value JPJw (x1 ) and the penalty
V (Jw ). In practice, it is important to choose a good feature
matrix Ψ which exploits some underlying structure of the
problem so that for some choice of Jw , JPJw (x1 ) + V (Jw )
is small.

3. MDPs with Average Cost Criterion
Here, we extend the previous analysis to KL control cost
MDPs with an average cost criterion. The average cost of
policy P starting at state x1 is
" t
#
X
1
`(xt , P )
λP = lim E
t→∞ t
s=1
t

1X s
P (x1 , :)`(:, P ) = P ∞ (x1 , :)`(:, P ) ,
t→∞ t
s=1

= lim

where we have assumed that the Markov chain induced by
policy P is irreducible and aperiodic (see, for example, Put-

Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing

erman (1994)), and P ∞ = lims→∞ P s , which is independent of the starting state x. Thus, λP 1 = P ∞ `(:, P ). Our
goal is to find P∗ = argminP ∈P λP , the optimal policy.
As before, exact methods do not scale to large state spaces
and we need to reduce our search to a parameterized class
of approximate differential value functions and restrict our
search to greedy policies. Fortunately, we can apply much
of the total-cost analysis to the average cost case.
The Bellman optimality equation for the average cost problems is Lh = λ1 + h, where λ is a scalar and h is
an X-dimensional vector. If a pair (λ∗ , h∗ ) satisfy this
equation, then λ∗ is the average loss of the optimal policy and vector h∗ is called the differential value function
of the optimal policy. The greedy policy with respect to
any function h : X → R, denoted
P by Ph , is defined by
Ph (x) = argminP ∈P {`(x, P ) + x0 P (x, x0 )h(x0 )}.
Similar to the previous section, we consider parameterized approximate differential value functions with
some feature matrix Ψ ∈ RX×d and a bounded
parameter set W ⊂ Rd .
Specifically, define the
class of approximate differential value functions H =
{x 7→ hw (x) := − log(Ψ(x, :)w) : w ∈ W}.
Our goal in this section is to prove that we can find a w
b such
that λPhwb ≈ minhw ∈H λPhw for KL control cost MDPs
with an algorithm that has complexity polynomial in d and
independent of X. We assume the same computational
model as before: access to arbitrary entries of Ψ, q, P0 ,
and e−q P0 Ψ takes unit time.
Let b be an estimate of the optimal average cost. Recall the bound on the exponential of a difference from
(9). Applying this bound to |Lh(x) − (b + h(x))|, the
average
error, allows
 −q(x) cost Bellman −b
 us to argue that if
e
P0 (x, :)Ψw − e Ψ(x, :)w is small, then the Bellman error in state x is also small. This motivates the convex
optimization problem


min c(w) := v > e−b Ψw − e−q P0 Ψw
(13)

X
X 

≡
v(T )
e−b Ψ(x, :)w − e−q(x) P0 (x, :)Ψw ,

w∈W

T ∈T

x∈T

where v is a positive vector and c(w) is convex in w. We
note that instead of requiring v to be a distribution, we
choose v to be an “unnormalized" distribution, i.e., v = Cu
where C > 0 is a positive constant and u is a distribution.
The performance bound of a greedy policy w.r.t. hwb for an
-optimal solution w
b is provided in the next theorem.
Theorem 2. Let Phw be the greedy policy with respect to
value function hw . Also, let w
b be an -optimal solution and
uwb = max(Lhwb , hwb + b). Then, for any w ∈ W with

lw = min(Lhw , hw + b), we have,


λPhwb − λPhw ≤ b − λPhw 
+ k(e

−uw
b

(14)

 v − µPhwb )k1 kLhwb − hwb − bk∞

+ k (Lhw − hw − b) k1,(e−lw v) +  ,
where  denotes component-wise multiplication of vectors
and µPhwb is the stationary distribution of the transition dynamics Phwb .
Similar to Theorem 1, the right hand side of (14) also involves the Bellman error at w (i.e., Lhw − hw − b) and a
distribution mismatch term (i.e., e−uwb  v − µPhwb ). How
small the term kLhw − hw − bk1,v can be depends on the
class of differential value functions H, which further depends on the choice
of the
 feature matrix Ψ. We also note

that the term λPhw − b in (14) characterizes the quality
of the estimate b. In practice, one can formulate the problem of estimation of b as a one-dimensional optimization
problem, which can be solved by zero-order optimization
approaches. The proof of Theorem 2 is provided in Appendix C. To obtain an -optimal solution w,
b one can apply the stochastic subgraident descent algorithm. We note
that instead of sampling a trajectory T as in Figure 1, one
only needs to sample a state x from the distribution defined
by v(x)/kvk1 . The detailed algorithm is presented in Appendix C due to space constraints.

4. Applications to Crowdsourcing
So far, we have motivated, presented, and proven error
bounds for the total cost and average cost versions of KL
control cost MDPs. The rest of the paper is devoted to applying our algorithm to a fundamental problem in crowdsourcing: budget allocation. We are given a set of items and
want to learn some binary label for each item. For a unit
cost, we can have a member of the crowd return a noisy
label. Some labels are harder to identify than others and
receive noisier raw labels. The goal of budgeted learning
is to optimally query items to maximize the number of correctly inferred labels. Intuitively, some instances are easy
and require few raw labels for a consensus while more ambiguous instances will require more raw labels to obtain a
good estimate.
This budget allocation problem in crowdsourcing has been
studied recently in Chen et al. (2013). We adopt the same
model here: the learner can submit labeling tasks to a
worker pool with anonymous workers but cannot assign
items to particular workers. There are A instances with
the latent true labels Zi ∈ {0, 1} for 1 ≤ i ≤ A. The
labeling difficulty for the i-th instance is characterized by
its soft label θi ∈ [0, 1], which is defined as the probability
that the i-th instance will be labeled as positive by a random worker. When θi is close to 0 or 1, the i-th instance is

Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing

an easy one; while when θi is close to 0.5, the instance is
highly ambiguous. It is also assumed that the soft label is
consistent with the true label, i.e., θi ≥ 0.5 iff Zi = 1.
Assume that the budget is B. For each stage 0 ≤ t ≤ B−1,
an instance it from the action set A = {1, . . . , A} is chosen and the item is queried. The decision maker receives
the label before choosing the next item. In a pull marketplace with anonymous workers, it is reasonable to assume
that workers provide the label yit according to a Bernoulli
distribution with the parameter θit , i.e., Pr(yit = 1) = θit .
When the budget is exhausted, the decision maker infers
the true labels by aggregating all the collected labels. More
specifically, let H ∗ = {i : Zi = 1} = {i : θi ≥ 0.5}
be the set of true positive labels. At the final stage B,
the decision maker estimates the positive set H B and the
overall cost is defined using the binary classification error
|H B ∩ (H ∗ )c | + |(H B )c ∩ H ∗ |. Our goal is to find an allocation policy that minimizes the binary classification error.
In Chen et al. (2013), the budget allocation problem was
formulated as a Bayesian MDP with a total cost criterion.
Each θi is assumed to be drawn from Beta(a0i , b0i ). At each
stage t, the decision maker chooses an instance it from the
action set A and observes its label yit ∼ Bernoulli(θit ). By
the conjugacy between Beta and Bernoulli, the posterior of
θit will be updated by
(
Beta(atit + 1, btit )
if yit = 1;
t+1 t+1
Beta(ait , bit ) =
t
t
Beta(ait , bit + 1)
if yit = −1.
Therefore, the state at the stage t, xt , can be characterized
by an A × 2 state matrix where each row contains two elements, {ati , bti }. The state space at stage t is
(
Xt =

t
0
xt = {ati , bti }A
i=1 : ai − ai ∈ N0 ,

bti

−

a0i

)
A
X
t
0
t
0
∈ N0 ,
(ai − ai ) + (bi − bi ) = t ,
i=1

where N0 denotes the nonnegative integers and we use
{ati , bti }A
Therei=1 to represent the A × 2 state matrix.
SB
fore, the state space for the entire budget is X = t=0 X t ,
which grows exponentially with B. Under this model, the
state transition probability can be easily calculated as the
posterior probability of the next state given the current state
xt and the action it ; Pr(yit = 1|xt , it ) = E(θit |xt ) =
atit
ati +bti
t

.

t

The objective is to find a budget allocation policy which minimizes the posterior classification error
given the final state xB .
By defining I(a, b) =
Pr (θ ≥ 0.5|θ ∼ Beta(a, b)) and h(x) = min(x, 1 − x),
Proposition 2.1 in Chen et al. (2013) shows that the poste-

rior classification error takes the following form,
E

A
X



({i ∈ H}{i 6∈ H } + {i 6∈ H}{i ∈ H }) xB
∗

!

∗

i=1

=

A
X


B
h I(aB
i , bi ) ,

(15)

i=1

where {· ∈ A} is the indicator function of A. Therefore,
the loss function as a function of the state can be expressed
as
(
SB−1
0
x ∈ t=0 X t ,

(16)
q(x) = PA
B B
x ∈ X B.
i=1 h I(ai , bi )
So far, we have formulated the budget allocation problem
in crowdsourcing as an MDP with a total cost criterion
with a loss that only depends on the state. Once we define some passive dynamics P0 , we can form a KL control cost MDP. However, since the state space is exponentially large in B, even the nice properties of KL control cost
MDPs would not make an exact solution tractable. Therefore, we will apply the stochastic subgradient method from
Section 2. Once we obtain PJwb based on the learned w,
b
it can be easily transformed into a distribution over the
action space (i.e., randomized policy mapping from state
space to action space). In particular, given the current
A × 2 state matrix x = {ai , bi }A
i=1 , the next state matrix
is x0 ∈ {x + (ei , 0) : i = 1, . . . , A} ∪ {x + (0, ei ) :
i = 1, . . . , A}, that is, there are 2 · A possible next states.
Here, ei denotes the A × 1 column vector with 1 at the ith entry and 0 at all other entries. Therefore, given PJwb ,
we can define the probability of taking an action i ∈ A as
p(i|x) = PJwb (x, x + (ei , 0)) + PJwb (x, x + (0, ei )).
We also note that the stochastic subgradient method in Figure 1 has a per-iteration complexity independent of X. Although X is exponential in B, the state transition matrix is
extremely sparse since for each state x there are at most 2·A
possible next states. The dominant term in the subgradient computation, P0 (x, ·)Ψ, has complexity of only O(Ad)
and thus the per-iteration complexity is O(ABd) which is
independent of X. Also, the computation of the normalization term Z(x) in (6) has complexity of only O(A).
One last point of discussion is how to choose P0 . As
mentioned in the introduction, any reasonable policy is
also a natural choice for the “passive" dynamics P0 as the
stochastic subgradient method will improve on it. Therefore, we construct P0 based on the optimistic knowledge
gradient policy (Opt-KG) proposed in Chen et al. (2013),
which was shown to work quite well in practice. OptKG is an index policy that selects the next instance based
on the optimistic outcome of the cost. More precisely,
for any given state (a, b) of a single instance, the differ-

Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing
6

0.065

Subgradient
random Opt-KG
uniform

posterior classification error

5

0.06

normalized budget for data instances

5.5

4.5
4
3.5
3
2.5
2
1.5
40

60

80

100

120

140

160

180

200

B

0.055

0.05

0.045

0.04

B=2.5A
B=7.2A
B=9.1A

0.035

0.03
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

soft label

Figure 2. The left plot compares the average posterior classification error of the opt-KG policy, the uniform policy, and our optimized
policy for a variety of budgets with A = 20 labels to learn. The optimized policy makes improvements for the entire budget range. The
right plot indicates the portion of the budget used for each label as a function of the soft label. Soft labels closer to .5 are the hardest,
and we see that they receive more budget, especially as B grows. For large B, the policy spends most of the budget on the hard labels,
but for small B, the budget is spread more uniformly.

ence/reduction of cost in getting an extra positive or negative label takes the following form: C1 (a, b) = h(I(a +
1, b))−h(I(a, b)), C−1 (a, b) = h(I(a, b+1))−h(I(a, b)).
Given the current state x = {ai , bi }A
i=1 , Opt-KG associates the i-th instance with a score defined as C(ai , bi ) =
min(C1 (ai , bi ), C−1 (ai , bi )), which is based on the minimum cost of obtaining the positive and negative labels. It
can be proven that C(ai , bi ) < 0 for all possible ai and bi .
The randomized Opt-KG policy chooses the instance i ∈ A
i ,bi )|
with the probability p (i|x) = PA |C(a
. Then, the
|C(ai0 ,bi0 )|
i0 =1
“passive" dynamics P0 can be defined based on the randomized Opt-KG policy as for each i ∈ A,
ai
P0 (x, x + (ei , 0)) = p(i|x) ·
,
a i + bi
bi
P0 (x, x + (0, ei )) = p(i|x) ·
.
a i + bi
In addition, the distribution v over the trajectory space T
can also be constructed based on Opt-KG and one can easily sample the trajectory from v.
4.1. Numerical Testing
We test the proposed stochastic subgradient method on a
simulated dataset with (known) soft labels θi sampled uniformly from [0, 1]; this corresponds to a0i = b0i . To optimize the policy, we need knowledge of θi in order to
evaluate q; however, the policy, of course, will not need
to know θi . We use the randomized Opt-KG policy as
both the “passive" dynamics P0 and the trajectory sampling distribution v. The feature matrix is generated using the moments of Beta distributions as follows. For each
state x = {ai , bi }A
i=1 , let Xi ∼ Beta(ai , bi ) and define

the feature vector Ψ(x, .) ∈ R3A+1 as the concatenation
i
i
, 1 − E(Xi ) = aib+b
,
of the 3-tuples: E(Xi ) = aia+b
i
i
ai +1
ai
2
E(Xi ) = ai +bi ai +bi +1 for 1 ≤ i ≤ A and the single
number 1 (for modeling the bias).
We set A = 20 and vary the budget B from 30 to 200 (1.5A
to 10A). We run the stochastic subgradient
method (Fig√
ure 1) with a learning rate of ηt ∝ 1/ t, a uniform starting
weight w1 = d1 1, and regularization parameter H = 7.
To speed up convergence, we use the mini-batch strategy
(Dekel et al., 2012) where each stochastic subgradient is
estimated by averaging 200 independent stochastic subgradients, and ran the gradient descent for 2500 iterations. Additionally, we threshold PJ (·, ·) below by 0.
The left plot in Figure 2 compares the average posterior
classification error of policies corresponding to 1) choosing every data instance uniformly (a.k.a. pure exploration),
2) the Opt-KG policy, and 3) the policy obtained from our
stochastic subgradient method. For each policy, the posterior classification error was estimated by averaging the loss
for 10,000 independent runs. Our algorithm outperforms
both policies for every budget level, and generally achieves
the same performance as Opt-KP with 10% fewer samples.
The right half of Figure 2 plots the (normalized) number
of times each data instance is queried against the soft label of the instance θi (which is of course unknown to the
policy). The plots are generated by averaging 2500 independent runs. As the budget is increased, proportionally
more budget is spent on the harder instances, i.e. those
with soft labels close to 0.5. However, even with as few
as B = 2.5A queries, the harder labels tend to be queried
more often.

Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing

Acknowledgement
Xi Chen would like to thank Google Faculty Research
Awards to support this work. We gratefully acknowledge the support of the Australian Research Council
through the award of an Australian Laureate Fellowship
(FL110100281), the NSF through grant CCF-1115788, and
Adobe through a Digital Marketing Research Award.

References
Abbasi-Yadkori, Y., Bartlett, P., and Malek, A. Linear programming for large-scale Markov decision problems. In
Proceedings of the International Conference on Machine
Learning, 2014.
Bertsekas, D. P. Dynamic Programming and Optimal Control. Athena Scientific, 2007.
Bertsekas, D. P. and Tsitsiklis, J. Neuro-Dynamic Programming. Athena scientific optimization and computation
series. Athena Scientific, 1996.
Chen, X., Lin, Q., and Zhou, D. Optimistic knowledge gradient for optimal budget allocation in crowdsourcing. In
Proceedings of the International Conference on Machine
Learning, 2013.
de Farias, D. P. and Van Roy, B. The linear programming
approach to approximate dynamic programming. Operations Research, 51, 2003.
de Farias, D. P. and Van Roy, B. On constraint sampling
in the linear programming approach to approximate dynamic programming. Mathematics of Operations Research, 29, 2004.
de Farias, D. P. and Van Roy, B. A cost-shaping linear program for average-cost approximate dynamic programming with performance guarantees. Mathematics of Operations Research, 31, 2006.
Dekel, Ofer, Gilad-Bachrach, Ran, Shamir, Ohad, and
Xiao, Lin. Optimal distributed online prediction using
mini-batches. J. Mach. Learn. Res., 13(1), 2012.
Desai, V. V., Farias, V. F., and Moallemi, C. C. Approximate dynamic programming via a smoothed linear program. Operations Research, 60(3):655–674, 2012.
Duchi, John, Bartlett, Peter L, and Wainwright, Martin. Randomized smoothing for stochastic optimization.
SIAM Journal on Optimization, 22(2):674–701, 2012.
Guestrin, C., Hauskrecht, M., and Kveton, B. Solving factored mdps with continuous and discrete variables. In
UAI, 2004.

Hauskrecht, M. and Kveton, B. Linear program approximations to factored continuous-state markov decision processes. In NIPS, 2003.
Kushner, H. J. and Yin, G. Stochastic Approximation and
Recursive Algorithms and Applications. Springer, 2003.
Maei, H. R., Szepesvári, Cs., Bhatnagar, S., Precup, D.,
Silver, D., and Sutton, R. S. Convergent temporaldifference learning with arbitrary smooth function approximation. In NIPS, 2009.
Maei, H. R., Szepesvári, Cs., Bhatnagar, S., and Sutton,
R. S. Toward off-policy learning control with function
approximation. In ICML, 2010.
Manne, A. S. Linear programming and sequential decisions. Management Science, 6(3):259–267, 1960.
Papadimitriou, C. and Tsitsiklis, J. N. The complexity of
markov decision processes. Mathematics of Operations
Research, 12(3):441–450, 1987.
Petrik, M. and Zilberstein, S. Constraint relaxation in approximate linear programs. In ICML, 2009.
Puterman, M. L. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. John Wiley & Sons,
Inc., New York, NY, USA, 1st edition, 1994.
Schweitzer, P. and Seidmann, A. Generalized polynomial
approximations in Markovian decision processes. Journal of Mathematical Analysis and Applications, 110:
568–582, 1985.
Shamir, O. and Zhang, T. Stochastic gradient descent for
non-smooth optimization: Convergence results and optimal averaging schemes. In ICML, 2013.
Sra, S., Nowozin, S., and Wright, S. J. Optimization for
Machine Learning. MIT Press, 2011.
Sutton, R. S. and Barto, A. G. Reinforcement Learning: An
Introduction. Bradford Book. MIT Press, 1998.
Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesvári, Cs., and Wiewiora, E. Fast gradientdescent methods for temporal-difference learning with
linear function approximation. In ICML, 2009a.
Sutton, R. S., Szepesvári, Cs., and Maei, H. R. A convergent O(n) algorithm for off-policy temporal-difference
learning with linear function approximation. In NIPS,
2009b.
Sutton, Richard S. Learning to predict by the methods of
temporal differences. Machine Learning, 3:9–44, 1988.
Todorov, E. Linearly-solvable markov decision problems.
In NIPS, 2006.

Large-Scale Markov Decision Problems with KL Control Cost and its Application to Crowdsourcing

Todorov, E. Efficient computation of optimal actions.
PNAS, 106:1478–11483, 2009a.
Todorov, E. Eigenfunction approximation methods for
linearly-solvable optimal control problems. In ADPRL,
2009b.
Tsitsiklis, John N. and Van Roy, Benjamin. An analysis of
temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):
674–690, 1997.
Veatch, M. H. Approximate linear programming for average cost MDPs. Mathematics of Operations Research,
38(3), 2013.
Zhong, M. and Todorov, E. Moving least-squares approximations for linearly-solvable optimal control problems.
In ADPRL, 2011a.
Zhong, M. and Todorov, E. Aggregation methods for
linearly-solvable MDPs. In World Congress of the International Federation of Automatic Control, 2011b.
Zinkevich, M., Weimer, M., Smola, A. J., and Li, L. Parallelized stochastic gradient descent. In NIPS, 2010.

