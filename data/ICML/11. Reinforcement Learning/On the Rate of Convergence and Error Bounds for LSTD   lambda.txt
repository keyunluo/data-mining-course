On the Rate of Convergence and Error Bounds for LSTD(Î»)

Manel Tagorti
Bruno Scherrer
Inria, Villers-leÌ€s-Nancy, F-54600, France
UniversiteÌ de Lorraine, LORIA, UMR 7503, VandÅ“uvre-leÌ€s-Nancy, F-54506, France

Abstract
We consider LSTD(Î»), the least-squares
temporal-difference algorithm with eligibility
traces algorithm proposed by Boyan (2002). It
computes a linear approximation of the value
function of a fixed policy in a large Markov
Decision Process. Under a Î²-mixing assumption, we derive, for any value of Î» âˆˆ (0, 1), a
high-probability bound on the rate of convergence of this algorithm to its limit. We deduce
a high-probability bound on the error of this
algorithm, that extends (and slightly improves)
that derived by Lazaric et al. (2012) in the
specific case where Î» = 0. In the context
of temporal-difference algorithms with value
function approximation, this analysis is to our
knowledge the first to provide insight on the
choice of the eligibility-trace parameter Î» with
respect to the approximation quality of the space
and the number of samples.

1. Introduction
In a large Markov Decision Process context, we consider
LSTD(Î»), the least-squares temporal-difference algorithm
with eligibility traces proposed by Boyan (2002). It is a
popular algorithm for performing a projection onto a linear space of the value function of a fixed policy. Such a
value estimation procedure can for instance be useful in a
policy iteration context to eventually estimate an approximately optimal controller (Bertsekas & Tsitsiklis, 1996;
SzepesvaÌri, 2010).
The asymptotic almost sure convergence of LSTD(Î») was
proved by Nedic & Bertsekas (2002). Under a Î²-mixing assumption, and given a finite number of samples n, Lazaric
et al. (2012) derived a high-probability error bound with a
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

MANEL . TAGORTI @ INRIA . FR
BRUNO . SCHERRER @ INRIA . FR

OÌƒ( âˆš1n ) rate1 in the restricted situation where Î» = 0. Pires
& SzepesvaÌri (2012) also sketch an analysis of variations
of LSTD(0) with several sorts of regularizations. To our
knowledge, however, similar finite-sample error bounds are
not known in the literature for Î» > 0. The main goal of
this paper is to fill this gap. This is all the more important that it is known that the parameter Î» allows to control the quality of the asymptotic solution of the value: by
moving Î» from 0 to 1, one can continuously move from
an oblique projection of the value (Scherrer, 2010) to its
orthogonal projection and consequently improve the corresponding guarantee (Tsitsiklis & Roy, 1997) (restated in
Theorem 2, Section 3).
The paper is organized as follows. Section 2 starts by describing the necessary background. Section 3 then contains our main results. Theorem 1 shows that unpenalized
LSTD(Î») converges to its limit at the rate OÌƒ( âˆš1n ). We then
deduce a global error (Corollary 1) that sheds some light on
the role of the parameter Î», and discuss some of its practical consequences. Theorem 3 then extends this result to
the case of penalized LSTD(Î»). Section 4 will go on by
providing a detailed proof of our claims. Finally, Section 5
concludes by describing related and potential future work.

2. LSTD(Î») and Related Background
We consider a Markov chain taking its values on a finite or
countable state space X , with transition kernel P , and that
is ergodic2 ; consequently, it admits a unique stationary distribution Âµ. For any K âˆˆ R+ , we denote B(X , K) the set
of functions defined on X and bounded by K. We consider
a reward function r âˆˆ B(X , Rmax ) for some Rmax âˆˆ R, that
provides the quality of being in some state. The value function v related to the Markov chain is defined, for any state
i, as the average discounted sum of rewards along infinitely
1

Throughout the paper, we shall write f (n) = OÌƒ(g(n)) as a
shorthand for f (n) = O(g(n) logk g(n)) for some k â‰¥ 0.
2
We focus on finite/countable state spaces essentially because
it eases the presentation. We believe that extensions to more general state spaces is straight-forward.

On the Rate of Convergence and Error Bounds for LSTD(Î»)

long trajectories starting from i:
ï£®
ï£¹
âˆ
X

âˆ€i âˆˆ X , v(i) = E ï£°
Î³ j r(Xj )X0 = iï£» ,

seen that Î¸ is a solution of the equation AÎ¸ = b (Tsitsiklis
& Roy, 1997; Nedic & Bertsekas, 2002) where for any i,

j=0

where Î³ âˆˆ (0, 1) is a discount factor. It is well-known
that the value function v is the unique fixed point of
the linear Bellman operator T : âˆ€i âˆˆ X , T v(i) =
r(i) + Î³E [v(X1 )|X0 = i] . It can easily be seen that v âˆˆ
max
B(X , Vmax ) with Vmax = R
1âˆ’Î³ .
When the size |X | of the state space is very large, one may
consider approximating v by using a linear architecture.
Given some d (typically d  |X |), we consider a feature
matrix Î¦ = (Ï†(x))xâˆˆX = (Ï†1 . . . Ï†d ) of dimension |X | Ã—
d. For any x âˆˆ X , Ï†(x) = (Ï†1 (x), ..., Ï†d (x))T is the
feature vector in state x. For any j âˆˆ {1, ..., d}, we assume
that the feature function Ï†j : X 7â†’ R belongs to B(X , L)
for some finite L. Throughout the paper, we will make the
following assumption.
Assumption 1. The feature vectors (Ï†j )jâˆˆ{1,...,d} are linearly independent.
Let S be the subspace generated by the vectors (Ï†j )1â‰¤jâ‰¤d .
We consider the orthogonal projection Î  onto S with
respect
to the Âµ-weighed quadratic norm kf kÂµ =
p
P
2
xâˆˆX f (x) Âµ(x). It is well known that this projection
has the following closed form
Î  = Î¦(Î¦T DÂµ Î¦)âˆ’1 Î¦T DÂµ ,

A = Î¦T DÂµ (I âˆ’ Î³P )(I âˆ’ Î»Î³P )âˆ’1 Î¦
(3)
" i
#
X
=E
(Î³Î»)iâˆ’k Ï†(Xk )(Ï†(Xi ) âˆ’ Î³Ï†(Xi+1 ))T
k=âˆ’âˆ

(4)
and b = Î¦T DÂµ (I âˆ’ Î³Î»P )âˆ’1 r
" i
#
X
iâˆ’k
=E
(Î³Î») Ï†(Xk )r(Xi ) ,

where the sum starts from âˆ’âˆ to ensure that the process
(Xk ) is in stationary regime. Since for all x, Ï†(x) is of
dimension d, we see that A is a d Ã— d matrix and b is a
vector of size d. Under Assumption 1, it can be shown
(Nedic & Bertsekas, 2002) that the matrix A is invertible,
and thus vLST D(Î») = Î¦Aâˆ’1 b is well defined.
The LSTD(Î») algorithm that is the focus of this article is
now precisely described. Given one trajectory X1 , ...., Xn
generated by the Markov chain, the expectation-based expressions of A and b in Equations (4)-(5) suggest to compute the following estimates:
AÌ‚ =

(1)

and bÌ‚ =

where DÂµ is the diagonal matrix with elements of Âµ on the
diagonal, and for all u, uT denotes the transpose of u.

where zi =

The goal of LSTD(Î») is to estimate a solution of the equation v = Î T Î» v, where the operator T Î» is defined as a
geometric average of the applications of the powers T i of
the Bellman operator T for all i > 1:
âˆ€Î» âˆˆ (0, 1), âˆ€v, T Î» v = (1 âˆ’ Î»)

âˆ
X

Î»i T i+1 v.

(2)

i=0

Note in particular that when Î» = 0, one has T Î» = T .
By using the facts that T i is affine and kP kÂµ = 1 (Tsitsiklis & Roy, 1997), it has been shown that the operator T Î» is a contraction mapping of modulus (1âˆ’Î»)Î³
1âˆ’Î»Î³ â‰¤ Î³
(Nedic & Bertsekas, 2002). Since the orthogonal projector Î  is non-expansive with respect to Âµ (Tsitsiklis & Roy,
1997), the operator Î T Î» is contracting and thus the equation v = Î T Î» v has one and only one solution, which
we shall denote vLST D(Î») since it is what the LSTD(Î»)
algorithm converges to (Nedic & Bertsekas, 2002). As
vLST D(Î») belongs to the subspace S, there exists a Î¸ âˆˆ Rd
such that vLST D(Î») = Î¦Î¸ = Î T Î» Î¦Î¸. If we replace Î  and
T Î» with their expressions (Equations 1 and 2), it can be

(5)

k=âˆ’âˆ

nâˆ’1
1 X
zi (Ï†(Xi ) âˆ’ Î³Ï†(Xi+1 ))T
n âˆ’ 1 i=1
nâˆ’1
1 X
zi r(Xi )
n âˆ’ 1 i=1
i
X

(Î»Î³)iâˆ’k Ï†(Xk )

(6)

k=1

is the so-called eligibility trace. The algorithm then returns
vÌ‚LST D(Î») = Î¦Î¸Ì‚ with3 Î¸Ì‚ = AÌ‚âˆ’1 bÌ‚, which is a (finite sample)
approximation of vLST D(Î») . Using a variation of the law
of large numbers, Nedic & Bertsekas (2002) showed that
both AÌ‚ and bÌ‚ converge almost surely respectively to A and
b, which implies that vÌ‚LST D(Î») tends to vLST D(Î») . The
main goal of this paper is to deepen this analysis: we shall
estimate a bound on the rate of convergence of vÌ‚LST D(Î»)
to vLST D(Î») , and bound the error kvÌ‚LST D(Î») âˆ’ vkÂµ of the
overall algorithm.

3. Main result
This section contains our main results. Our key assumption for the analysis is that the Markov chain process that
generates the states has some mixing property4 .
3
We will see in Theorem 1 that AÌ‚ is invertible with high probability for a sufficiently big n.
4
A Markov chain that is ergodic and stationary is always Î²mixing (Bradley, 2005).

On the Rate of Convergence and Error Bounds for LSTD(Î»)

Assumption 2. The process (Xn )nâ‰¥1 is Î²th
mixing, in
h the sense that its i coefficienti Î²i =
t
suptâ‰¥1 E supBâˆˆÏƒ(Xt+i
âˆ ) |P (B|Ïƒ(X1 )) âˆ’ P (B)| tends to
0 when i tends to infinity, where Xlj = {Xl , ..., Xj } for
j â‰¥ l and Ïƒ(Xlj ) is the sigma algebra generated by Xlj .
Furthermore, (Xn )nâ‰¥1 mixes at an exponential decay rate
with parameters Î² > 0, b > 0, and Îº > 0 in the sense that
Îº
Î²i â‰¤ Î²eâˆ’bi .
Intuitively the Î²i coefficients measure the degree of dependence of samples separated by i time steps (the smaller the
coefficient the more independence). We are now ready to
state the main results of the paper, which provides a rate of
convergence of LSTD(Î»).
Theorem 1. Let Assumptions 1 and 2 hold and let
X1 âˆ¼ Âµ, where Âµ is the stationary distribution of
the chain. For any n â‰¥ 1 and Î´ âˆˆ (0, 1), define
o Îº1
n
,
1
, where Î›(n, Î´) =
I(n, Î´) = 32Î›(n, Î´) max Î›(n,Î´)
b
 2
log 8nÎ´ + log(max{4e2 , nÎ²}). Also define the positive
l
m
integer mÎ»n = log(nâˆ’1)
. Let n0 (Î´) be the smallest inte1
log
Î»Î³

ger such that for all n â‰¥ n0 (Î´),
2dL2
(1 âˆ’ Î³)Î½



2 p Î»
âˆš
(mn + 1)I(n âˆ’ 1, Î´)+
nâˆ’1

2
1
+
mÎ»n < 1
(n âˆ’ 1)(1 âˆ’ Î»Î³)
(n âˆ’ 1)

(7)

where Î½ is the smallest eigenvalue of the Gram matrix Î¦T DÂµ Î¦. Then, for all Î´, with probability at least
1 âˆ’ Î´, for all n â‰¥ n0 (Î´), AÌ‚ is invertible and the distance
kvLST D(Î») âˆ’ vÌ‚LST D(Î») kÂµ is upper bounded by
âˆš

4Vmax dL2
n âˆ’ 1(1 âˆ’ Î³)Î½

q

(mÎ»n + 1) I(n âˆ’ 1, Î´) + h(n, Î´)

with h(n, Î´) = OÌƒ( n1 log 1Î´ ).
The constant Î½ is positive under Assumption 1. For all Î´, it
is clear that the finite constant n0 (Î´) exists since the l.h.s.
of Equation (7) tends to 0 when n tends to infinity. As
mÎ»n and I(n âˆ’ 1, Î´) are of order OÌƒ(1), we
 see that
 can
LSTD(Î») estimates vLST D(Î») at a rate OÌƒ

âˆš1
n

. Finally,

mÎ»n

we can observe that since Î» 7â†’
is increasing, the rate of
convergence deteriorates when Î» increases. This negative
effect can be balanced by the fact that, as shown by the
following result from the literature, the quality of vLST D(Î»)
improves when Î» increases.
Theorem 2 (Tsitsiklis & Roy (1997)). The approximation
error satisfies
kv âˆ’ vLST D(Î») kÂµ â‰¤

1 âˆ’ Î»Î³
kv âˆ’ Î vkÂµ .
1âˆ’Î³

Since the constant equals 1 when Î» = 1, one recovers
the well-known fact that LSTD(1) computes the orthogonal projection Î v of v. By using the triangle inequality,
one deduces from Theorems 1 and 2 the following global
error bound.
Corollary 1. Let the assumptions and notations of Theorem 1 hold. For all Î´, with probability at least 1 âˆ’ Î´, for all
n â‰¥ n0 (Î´), the global error of LSTD(Î») satisfies:
kv âˆ’ vÌ‚LST D(Î») kÂµ â‰¤
+âˆš

4Vmax dL2
n âˆ’ 1(1 âˆ’ Î³)Î½

1 âˆ’ Î»Î³
kv âˆ’ Î vkÂµ
1âˆ’Î³
q
(mÎ»n + 1) I(n âˆ’ 1, Î´) + h(n, Î´).

The bound requires a sufficiently large number of samples n (n â‰¥ n0 (Î´)). For a fixed Î´, this number increases
when Î» increases. The existence of such a condition is not
surprising since we focus on an unregularized version of
LSTD(Î»), and thus the estimated matrix AÌ‚ may not be invertible when n is too small.
As we have already mentioned, Î» = 1 minimizes the bound
on the approximation error kv âˆ’ vLST D(Î») kÂµ (the first term
in the r.h.s. in Corollary 1) while Î» = 0 minimizes the
bound on the estimation error kvLST D(Î») âˆ’ vÌ‚LST D(Î») kÂµ
(the second term). For any Î´ and n â‰¥ n0 (Î´), there exists a value Î»âˆ— that minimizes the global error bound by
making an optimal compromise between the approximation
and estimation errors upper-bounds. When the number of
samples n tend to infinity, the optimal value Î»âˆ— tends to
1. Previous studies on the role of the parameter Î» were to
our knowledge empirical (Sutton & Barto, 1998; Downey
& Sanner, 2010) or dedicated to an exact representation of
the value function (Kearns & Singh, 2000). This is the first
time a bound on a temporal-difference learning algorithm
with value function approximation shows this trade-off explicitely.
The form of the result stated in Corollary 1 is slightly
stronger than the one of Lazaric et al. (2012). It has the
advantage to make clear the connection with the previous
analysis of Nedic & Bertsekas (2002) since our formulation implies the almost sure convergence of vÌ‚LST D(Î») to
vLST D(Î») : for some property P (n), our result is of the
form â€œâˆ€Î´, âˆƒn0 (Î´), such that with probability at least 1 âˆ’
Î´, âˆ€n â‰¥ n0 (Î´), P (n) holdsâ€ while the result
stated by (Lazaric et al., 2012) is of the form
â€œâˆ€n, âˆƒÎ´(n), such that with probability at least 1 âˆ’
Î´(n), P (n) holds.â€ In other words, we can fix a real Î´ such
that the property is true for all n â‰¥ n0 (Î´) with probability
at least 1 âˆ’ Î´, while in (Lazaric et al., 2012), Î´ depends on
the number of samples.
Pires & SzepesvaÌri (2012) studied penalized versions of
linear systems estimated with noise, and explained how

On the Rate of Convergence and Error Bounds for LSTD(Î»)

to apply their approach to LSTD(0). Such a penalization
allows to control the magnitude of Î¸Ì‚ in situations where
the matrix AÌ‚ is (close to) singular. This has the advantage of removing the need for a condition on the number
of samples to ensure the invertibility of AÌ‚, and as a side
effect this allows to derive bounds that are valid for any
value of the probability threshold Î´ and number of samples n (while in the above mentionned result without penalization, the minimum number of samples n0 (Î´) grows
to infinity when Î´ approaches 0). The most natural penalization that one would like to consider for LSTD(Î») is the
one where we add a term ÏI to the estimate AÌ‚ (Nedic &
Bertsekas, 2002). This amounts tonsolve the following peo
nalized problem: Î¸Ì‚Ï = arg minÎ¸ kAÌ‚Î¸ âˆ’ bÌ‚k22 + ÏkÎ¸k22 .
Unfortunately, this very form of regularizationâ€”squared
error with squared penaltyâ€”is not considered by Pires &
SzepesvaÌri (2012). It turns out that it is rather straightforward to bound the residual kAÎ¸Ì‚Ï âˆ’bk2 in this case by following an approach very similar to that described in Pires &
SzepesvaÌri (2012). Combined with the analyisis performed
for Theorem 1, we can derive the following result.
Theorem 3. Under Assumptions 1 and 2, for any Î´ âˆˆ (0, 1)
Ïn,Î´
and n consider the estimate vÌ‚LST
D(Î») = Î¦Î¸Ì‚Ï obtained
with penalization parameter Ïn,Î´ = 2Î2 (n, Î´) s.t.
4dL2
âˆš
Î(n, Î´) =
(1 âˆ’ Î»Î³) n âˆ’ 1

s

2



(mn
Î» + 1)I
2

n âˆ’ 1,


2n2 Î´
+
3

mÎ»n

2dL
4dL
.
+
(n âˆ’ 1)(1 âˆ’ Î»Î³)2
(n âˆ’ 1)(1 âˆ’ Î»Î³)
Ïn,Î´
Then, with probability at least 1âˆ’Î´, for all n, kvÌ‚LST
D(Î») âˆ’

vLST D(Î») kÂµ is bounded by
âˆš
âˆš
q
4Vmax dL(3 + dL)
âˆš
(mÎ»n + 1) I(n âˆ’ 1, Î´) + g(n, Î´),
âˆš
n âˆ’ 1(1 âˆ’ Î³) Î½
where g(n, Î´) and I(n, Î´) and mÎ»n are defined as in Theorem 1.
We defer the proof to Appendix B of the supplementary
material.

4. Proof of Theorem 1
This section provides a detailed proof of Theorem 1. The
proof is organized in four steps. In the first step, we study
the sensitivity of the solution vLST D(Î») to a potential deterministic deviation of the estimates AÌ‚ and bÌ‚ from their
limits A and b. In the second step, we shall derive a general concentration analysis to control with high probability
the deviations of processes defined through infinitely-long
eligibility traces. Then, in the third step, we will apply this
concentration analysis to AÌ‚ and bÌ‚. Finally, we will gather
all elements to deduce the high-probability bound on the
distance between vÌ‚LST D(Î») and vLST D(Î») .

4.1. Deterministic sensivity of LSTD(Î»)
We begin by showing the following lemma on the sensitivity of LSTD(Î»).
Lemma 1. Write A = AÌ‚âˆ’A, b = bÌ‚âˆ’b and Î½ the smallest
eigenvalue of the matrix Î¦T DÂµ Î¦. For all Î» âˆˆ (0, 1), the
error kvLST D(Î») âˆ’ vÌ‚LST D(Î») kÂµ is upper bounded by5 :
1 âˆ’ Î»Î³
âˆš k(I + A Aâˆ’1 )âˆ’1 k2 kA Î¸ âˆ’ b k2 ,
(1 âˆ’ Î³) Î½
where Î¸ = Aâˆ’1 b. Furthermore, if for some  and C,
1
kA k2 â‰¤  < C â‰¤ kAâˆ’1
k2 , then AÌ‚ is invertible and
k(I + A Aâˆ’1 )âˆ’1 k2 â‰¤

1
.
1 âˆ’ C

Proof. The definitions of vLST D(Î») and vÌ‚LST D(Î») lead to
vÌ‚LST D(Î») âˆ’ vLST D(Î») = Î¦Aâˆ’1 (AÎ¸Ì‚ âˆ’ b).

(8)

On the one hand, with the expression of A in Equation (3),
writing M = (1 âˆ’ Î»)Î³P (I âˆ’ Î»Î³P )âˆ’1 and MÂµ = Î¦T DÂµ Î¦,
we can see that

âˆ’1
Î¦Aâˆ’1 = Î¦ Î¦T DÂµ (I âˆ’ Î³P )(I âˆ’ Î»Î³P )âˆ’1 Î¦

âˆ’1
= Î¦ Î¦T DÂµ (I âˆ’ Î»Î³P âˆ’ (1 âˆ’ Î»)Î³P )(I âˆ’ Î»Î³P )âˆ’1 Î¦
= Î¦(MÂµ âˆ’ Î¦T DÂµ M Î¦)âˆ’1 .
Since the matrices A and MÂµ are invertible, the matrix (I âˆ’
MÂµâˆ’1 Î¦T DÂµ M Î¦) is also invertible and
Î¦Aâˆ’1 = Î¦(I âˆ’ MÂµâˆ’1 Î¦T DÂµ M Î¦)âˆ’1 MÂµâˆ’1 .
By definition, the projection matrix Î  defined in Equation (1) satisfies kÎ kÂµ = 1 and we know from Tsitsiklis &
Roy (1997) that the stochastic matrix P of the process also
satisfies kP kÂµ = 1. Hence, we have kÎ M kÂµ = (1âˆ’Î»)Î³
1âˆ’Î»Î³ <
1 and the matrix (I âˆ’ Î M ) is invertible. We can use the
identity X(I âˆ’ Y X)âˆ’1 = (I âˆ’ XY )âˆ’1 X with X = Î¦ and
Y = MÂµâˆ’1 Î¦T DÂµ M , and obtain
Î¦Aâˆ’1 = (I âˆ’ Î M )âˆ’1 Î¦MÂµâˆ’1 .

(9)

On the other hand, using the facts that AÎ¸ = b and AÌ‚Î¸Ì‚ = bÌ‚,
we can see that
AÎ¸Ì‚ âˆ’ b = AÎ¸Ì‚ âˆ’ b âˆ’ (AÌ‚Î¸Ì‚ âˆ’ bÌ‚)
= bÌ‚ âˆ’ b âˆ’ (AÌ‚ âˆ’ A)(Î¸Ì‚ âˆ’ Î¸) âˆ’ (AÌ‚ âˆ’ A)Î¸
= bÌ‚ âˆ’ AÌ‚Î¸ âˆ’ (b âˆ’ AÎ¸) + A Aâˆ’1 (AÎ¸ âˆ’ AÎ¸Ì‚)
= bÌ‚ âˆ’ AÌ‚Î¸ âˆ’ A Aâˆ’1 (AÎ¸Ì‚ âˆ’ b)
= (I + A Aâˆ’1 )âˆ’1 (bÌ‚ âˆ’ AÌ‚Î¸)
= (I + A Aâˆ’1 )âˆ’1 (b âˆ’ A Î¸).

(10)

5
When AÌ‚ is not invertible, we have vÌ‚LST D(Î») = âˆ and the
inequality is always satisfied since, as we will see shortly, the
invertiblity of AÌ‚ is equivalent to that of (I + A Aâˆ’1 ).

On the Rate of Convergence and Error Bounds for LSTD(Î»)

Using Equations (9) and (10), Equation (8) can be rewritten
as follows:
vÌ‚LST D(Î») âˆ’ vLST D(Î»)
= (I âˆ’ Î M )âˆ’1 Î¦MÂµâˆ’1 (I + A Aâˆ’1 )âˆ’1 (b âˆ’ A Î¸). (11)
We shall now bound kÎ¦MÂµâˆ’1 (I + A Aâˆ’1 )âˆ’1 (b âˆ’ A Î¸)kÂµ .
Notice that for all x,
q
kÎ¦MÂµâˆ’1 xkÂµ = xT MÂµâˆ’1 Î¦T DÂµ Î¦MÂµâˆ’1 x
q
1
(12)
= xT MÂµâˆ’1 x â‰¤ âˆš kxk2
Î½
where Î½ is the smallest (real) eigenvalue of the Gram matrix
MÂµ . By taking the norm in Equation (11) and using the
above relation, we get

4.2. Concentration inequality for infinitely-long
trace-based estimates
As both terms AÌ‚ and bÌ‚ have the same structure, we will
consider here a matrix that has the following general form:
GÌ‚ =

nâˆ’1
1 X
Gi with Gi = zi (Ï„ (Xi , Xi+1 ))T
n âˆ’ 1 i=1

where zi is the trace defined in Equation (6) and Ï„ : X 2 â†’
Rk . Let k.kF denote the Frobenius norm satisfying: for
Pd Pk
2
M âˆˆ RdÃ—k , kM k2F =
l=1
j=1 (Ml,j ) . The second important element of our analysis is the following concentration inequality for the infinitely-long-trace Î²-mixing
process GÌ‚.
Lemma 2. Let Assumptions 1 and 2 hold and let X1 âˆ¼ Âµ.
Define the d Ã— k matrix Gi such that

kvÌ‚LST D(Î») âˆ’ vLST D(Î») kÂµ
â‰¤ k(I âˆ’ Î M )

âˆ’1

kÂµ kÎ¦MÂµâˆ’1 (I

+ A A

âˆ’1 âˆ’1

)

(b âˆ’ A Î¸)kÂµ

1
â‰¤ k(I âˆ’ Î M )âˆ’1 kÂµ âˆš k(I + A Aâˆ’1 )âˆ’1 k2 kA Î¸ âˆ’ b k2 .
Î½
The first part of the lemma is obtained by using the fact that
kÎ M kÂµ = (1âˆ’Î»)Î³
1âˆ’Î»Î³ < 1, which implies that
âˆ’1

k(I âˆ’ Î M )


âˆ
âˆ

X
X

i
kÎ M kiÂµ
(Î M )  â‰¤
kÂµ = 


i=0

â‰¤

1âˆ’

i=0

Âµ

1
(1âˆ’Î»)Î³
1âˆ’Î»Î³

1 âˆ’ Î»Î³
=
.
1âˆ’Î³

(13)

We are going now to prove the second part of the lemma.
Since A is invertible, the matrix AÌ‚ is invertible if and only
if the matrix AÌ‚Aâˆ’1 = (A + A )Aâˆ’1 = I + A Aâˆ’1 is invertible. Let us denote Ï(A Aâˆ’1 ) the spectral radius of
the matrix A Aâˆ’1 . A sufficient condition for AÌ‚Aâˆ’1 to
be invertible is that Ï(A Aâˆ’1 ) < 1. From the inequality
Ï(M ) â‰¤ kM k2 for any square matrix M , we can see that
1
for any C and  that satisfy kA k2 â‰¤  < C < kAâˆ’1
k2 ,
Ï(A Aâˆ’1 ) â‰¤ kA Aâˆ’1 k2 â‰¤ kA k2 kAâˆ’1 k2 â‰¤


< 1.
C

It follows that the matrix AÌ‚ is invertible and


âˆ
âˆ  i
X

X


âˆ’1 âˆ’1
âˆ’1 i 
k(I + A A ) k2 =  (A A )  â‰¤


C
i=0

2

Gi =

i
X

(Î»Î³)iâˆ’l Ï†(Xl )(Ï„ (Xi , Xi+1 ))T .

(14)

l=1

Recall that Ï† = (Ï†1 , . . . , Ï†d ) is such that for all j, Ï†j âˆˆ
B(X , L). Assume that for all 1 â‰¤ j â‰¤ d, Ï„j âˆˆ B(X 2 , L0 ).
Let mÎ»n and I(n, Î´) be defined as in Theorem 1. Let
J(n, Î´) = I(n, 4n2 Î´). Then, for all Î´ in (0, 1), with probability at least 1 âˆ’ Î´,


nâˆ’1

 1 nâˆ’1
X
1 X


Gi âˆ’
E[Gi ]


n âˆ’ 1
n
âˆ’
1
i=1
i=1
2
âˆš
q
2 d Ã— kLL0
âˆš
â‰¤
(mÎ»n + 1) J(n âˆ’ 1, Î´) + (n),
(1 âˆ’ Î»Î³) n âˆ’ 1
âˆš

0

dÃ—kLL
where (n) = 2mÎ»n (nâˆ’1)(1âˆ’Î»Î³)
.

Proof. The proof of this result is tedious, so we only give
a sketch and defer the details to Appendix A in the Supplementary material. There are two main difficulties regarding the estimates Gi used to compute GÌ‚: 1) Gi is a
Ïƒ(X i+1 ) measurable function of the non-stationary vector
(X1 , . . . , Xi+1 ), and is consequently not stationary; 2) For
all i, Gi are computed from one single trajectory of the
Markov chain and are consequently mutually dependent.
To deal with the first issue (non-stationarity), we shall consider the m-truncated trace,
zim =

i=0

i
X

(Î»Î³)iâˆ’k Ï†(Xk ),

k=max(iâˆ’m+1,1)

This concludes the proof of Lemma 1.
and approximate GÌ‚ with the process GÌ‚m defined as:
Lemma 1 suggests that we control both terms kA k2 =
kAÌ‚ âˆ’ Ak2 and kb k2 = kbÌ‚ âˆ’ bk2 . The next subsection
shows how to do so with high probability.

GÌ‚m =

nâˆ’1
1 X m
m
T
G , with Gm
i = zi (Ï„ (Xi , Xi+1 )) .
n âˆ’ 1 i=1 i

On the Rate of Convergence and Error Bounds for LSTD(Î»)
m+1
) measurable function of the
Indeed, Gm
i is now a Ïƒ(X
stationary vector Zi = (Xiâˆ’m+1 , Xiâˆ’m+2 , . . . , Xi+1 ),
the vector Zi being stationary since we assumed X1 âˆ¼ Âµ.

To deal with the second issue (dependence of samples), for
any possible value of the truncation depth m, we shall use
the Î²-mixing assumption (Assumption 2) to transform the
dependent samples Gm
i into blocks of independent samples, by using the â€œblocking techniqueâ€ of Yu (1994) in a
way somewhat similar toâ€”but technically slightly more involved thanâ€”what Lazaric et al. (2012) did for LSTD(0).
This being done, we will be able to use a concentration inequality for i.i.d. processes from the literature (Lemma 7
in Appendix A in the Supplementary material). In addition to the use of a truncation depth m, a specific ingredient of the analysis of LSTD(Î») with respect to that
of LSTD(0) is that we need to prove that the stationary
process (Zi )iâ‰¥1 = (Xiâˆ’m+1 , Xiâˆ’m+2 , . . . , Xi+1 )iâ‰¥1 on
which the m-truncated process Gm
i is defined, inherits the
Î²-mixing property of the original process (Xi )iâ‰¥1 . This is
the purpose of the following technical lemma.
Lemma 3. Let (Xn )nâ‰¥1 be a Î²-mixing process, then
(Zn )nâ‰¥m = (Xnâˆ’m+1 , Xnâˆ’m+2 , . . . , Xn+1 )nâ‰¥m is a Î²mixing process such that its ith Î² mixing coefficient Î²iZ
X
satisfies Î²iZ â‰¤ Î²iâˆ’m
.

4.3. Bounding the deviations of AÌ‚ and bÌ‚
We shall now apply the concentration inequality of
Lemma 2 on the quantities of interest of Lemma 1, i.e. on
kA k2 and kA Î¸ âˆ’ b k2 .
Bounding kA k2 . By the triangle inequality, we have
kA k2 â‰¤ kE[A ]k2 + kA âˆ’ E[A ]k2 .

(15)

Write AÌ‚n,k = Ï†(Xk )(Ï†(Xn ) âˆ’ Î³Ï†(Xn+1 ))T . For all n
and k, we have: kAÌ‚n,k k2 â‰¤ 2dL2 . We can bound the first
term of the r.h.s. of Equation (15) by replacing A with its
expression in Equation (4):

"
#
nâˆ’1 i


1 XX


iâˆ’k
(Î»Î³) AÌ‚i,k 
kE[A ]k2 = A âˆ’ E


n âˆ’ 1 i=1
k=1
2
 "
!#
nâˆ’1
i
i


X
X
X
1


iâˆ’k
iâˆ’k
= E
(Î»Î³) AÌ‚i,k âˆ’
(Î»Î³) AÌ‚i,k 


n âˆ’ 1 i=1
k=âˆ’âˆ
k=1
2
 "
#
nâˆ’1
0


X
X
1


(Î»Î³)âˆ’k AÌ‚i,k 
(Î»Î³)i
= E


nâˆ’1
i=1

â‰¤

1
nâˆ’1

nâˆ’1
X

(Î»Î³)i

i=1

k=âˆ’âˆ

2

2

2

2dL
1
2dL
def
â‰¤
= 0 (n).
1 âˆ’ Î»Î³
n âˆ’ 1 (1 âˆ’ Î»Î³)2
(16)

Finally, setting m to mÎ»n will ensure that the distance
between GÌ‚ and GÌ‚m is bounded by (n) (as defined in
Lemma 2), and is therefore neglibible with respect to the
result of the deviation analysis obtained by the â€œblocking
techinqueâ€ of (Yu, 1994).

Let (Î´n )nâ‰¥1 be a sequence in (0, 1) that we will set later.
4dL2
With (n) = (nâˆ’1)(1âˆ’Î»Î³)
mÎ»n (defined in Lemma 2) and
0 (n) defined in Equation (16), define:

Using a very similar proof, we may derive a (simpler and)
general-purpose concentration inequality for Î²-mixing processes:
Lemma 4. Let Y = (Y1 , . . . , Yn ) be random variables
taking their values in the space Rd , generated from a stationary exponentially Î²-mixing process with parameters Î²,
b and Îº, and such that for all i, kYi âˆ’ E[Yi ]k2 â‰¤ B2 almost
surely. Then for all Î´ > 0, with probability at least 1 âˆ’ Î´,


n
n
1 X

1X
B2 p


Yi âˆ’
E[Yi ] â‰¤ âˆš
J(n, Î´)

n

n i=1
n
i=1

(17)

2

where J(n, Î´) is defined as in Lemma 2.
If the variables Yi were independent, we would have Î²i = 0
for all i, that is we could choose Î² = 0 and b = âˆ, so that
2
J(n, Î´) reduces to 32 log 8eÎ´ = O(1) and we recover standard concentration results for i.i.d. processes (such as the
one we describe in Lemma 7 in Appendix A in the Supplementary material). The price to pay for making a Î²-mixing
assumption (instead of simple independence) lies in the extra coefficient J(n, Î´) which is OÌƒ(1); in other words, it is
rather mild.

1 (n, Î´n ) =

4dL2
âˆš
(1 âˆ’ Î»Î³) n âˆ’ 1
+ (n) + 0 (n).

q

(mÎ»n + 1) J(n âˆ’ 1, Î´n )

By using Equation (15), the bound of Equation (16) and
Lemma 2 applied to A , we get
P {kA k2 â‰¥ 1 (n, Î´n )}
â‰¤ P{kA âˆ’ E[A ]k2 â‰¥ 1 (n, Î´n ) âˆ’ 0 (n)}
â‰¤ Î´n .

(18)

Bounding kA Î¸âˆ’b k2 . By using the fact that AÎ¸ = b, the
definitions of AÌ‚ and bÌ‚, and the fact that Ï†(x)T Î¸ = [Ï†Î¸](x),
we have
A Î¸ âˆ’ b = AÌ‚Î¸ âˆ’ bÌ‚
=

nâˆ’1
nâˆ’1
1 X
1 X
zi (Ï†(Xi ) âˆ’ Î³Ï†(Xi+1 )T )Î¸ âˆ’
zi r(Xi )
n âˆ’ 1 i=1
n âˆ’ 1 i=1

=

nâˆ’1
1 X
zi ([Ï†Î¸](Xi ) âˆ’ Î³[Ï†Î¸](Xi+1 ) âˆ’ r(Xi ))
n âˆ’ 1 i=1

=

nâˆ’1
1 X
zi âˆ† i
n âˆ’ 1 i=1

On the Rate of Convergence and Error Bounds for LSTD(Î»)

where, since vLST D(Î») = Î¦Î¸, âˆ†i is the following number:
âˆ†i = vLST D(Î») (Xi ) âˆ’ Î³vLST D(Î») (Xi+1 ) âˆ’ r(Xi ).
Let L0 be a bound on max1â‰¤iâ‰¤nâˆ’1 |âˆ†i | (we shall compute
L0 below). We can control kA Î¸ âˆ’ b k2 by following the
same proof steps as above. In fact we can see that
kA Î¸ âˆ’ b k2 â‰¤ kA Î¸ âˆ’ b âˆ’ E[A Î¸ âˆ’ b ]k2
+ kE[A Î¸ âˆ’ b ]k2 ,

(19)

Since Î¦T DÂµ Î¦ is a symmetric matrix, we have Î½ â‰¤
kÎ¦T DÂµ Î¦k2 .
We can see that kÎ¦T DÂµ Î¦k2
â‰¤
1

and therefore we can take L0 = 2

with kE[A Î¸ âˆ’ b ]k2 â‰¤ kE[A ]k2 kÎ¸k2 + kE[b ]k2 .
From what has been developed before we can see that
1
2dL2
kE[A ]k2 â‰¤ 0 (n) = nâˆ’1
(1âˆ’Î»Î³)2 . Similarly we can show
that kE[b ]k2 â‰¤

âˆš
dLRmax
1
nâˆ’1 (1âˆ’Î»Î³)2 .

We can hence conclude that

kE[A Î¸ âˆ’ b ]k2

âˆš
dLRmax def 0
2dL2
1
1
kÎ¸k2 +
= 0 (n).
â‰¤
2
n âˆ’ 1 (1 âˆ’ Î»Î³)
n âˆ’ 1 (1 âˆ’ Î»Î³)2
(20)

By using Equation (19), Equation (21) and Lemma 2 applied to a Î¸ âˆ’ b, we get
P(kA Î¸ âˆ’ b k2 â‰¥ 2 (n, Î´n ))
â‰¤ P(kA Î¸ âˆ’ b âˆ’ E[A Î¸ âˆ’ b ]k2 â‰¥ 2 (n, Î´n ) âˆ’ 00 (n))
â‰¤ Î´n .

(22)

To finish this third part of the proof, it remains to compute
the bound L0 on max1â‰¤iâ‰¤nâˆ’1 |âˆ†i |. To do so, it suffices to
bound vLST D(Î») (x) for all x. For all x âˆˆ X , we have
âˆš
|vLST D(Î») (x)| = |Ï†T (x)Î¸| â‰¤ kÏ†T (x)k2 kÎ¸k2 â‰¤ dLkÎ¸k2 ,
where the first inequality is obtained from the CauchySchwarz inequality. It remains to bound kÎ¸k2p
. On the one
hand,
we
have:
kv
k
=
kÎ¦Î¸k
=
Î¸ T MÂµ Î¸ â‰¥
Âµ
Âµ
LST
D(Î»)
âˆš
Î½kÎ¸k2 , and on the other hand, we have: kvLST D(Î») kÂµ =
max
k(I âˆ’ Î M )âˆ’1 Î (I âˆ’ Î»Î³P )âˆ’1 rkÂµ â‰¤ R
1âˆ’Î³ = Vmax . Therefore kÎ¸k2 â‰¤ Vâˆšmax
, and we can deduce that: âˆ€x âˆˆ
Î½
âˆš

X , |vLST D(Î») (x)| â‰¤

dLV
âˆš max .
Î½

Then, for all i we have

|âˆ†i | = |vLST D(Î») (Xi ) âˆ’ Î³vLST D(Î») (Xi+1 ) âˆ’ r(Xi )|
âˆš
âˆš
dLVmax
dLVmax
âˆš
â‰¤
+Î³ âˆš
+ (1 âˆ’ Î³)Vmax .
Î½
Î½

âˆš
âˆšdL Vmax .
Î½

4.4. Conclusion of the proof
Now that we know how to control both terms kA k2 and
kA Î¸ âˆ’ b k2 , we are ready to conclude the proof. Consider
the event

E = âˆƒn â‰¥ 1, {kA k2 â‰¥ 1 (n, Î´n )}
	
âˆª {kA Î¸ âˆ’ b )k2 â‰¥ 2 (n, Î´n )} .
Using the analysis of Section 4.3 and in particular Equations (18) and 22, we deduce that

2

4dL
mÎ»n (defined in Lemma 2) and
With (n) = (nâˆ’1)(1âˆ’Î»Î³)
0
0 (n) defined in Equation (20), define:
âˆš
q
2 dLL0
âˆš
2 (n, Î´n ) =
(mÎ»n + 1) J(n âˆ’ 1, Î´n )
(1 âˆ’ Î»Î³) n âˆ’ 1
+ (n) + 00 (n).
(21)

1

d maxj,k |Ï†Tk DÂµ Ï†j | = d maxj,k |Ï†Tk DÂµ2 DÂµ2 Ï†j | â‰¤
d maxj,k kÏ†Tk kÂµ kÏ†j kÂµ â‰¤ dL2 , so that Î½ â‰¤ dL2 . It follows that, for all i
âˆš
âˆš
âˆš
dLVmax
dLVmax
dL
âˆš
+Î³ âˆš
+ âˆš (1 âˆ’ Î³)Vmax ,
|âˆ†i | â‰¤
Î½
Î½
Î½

P(E) â‰¤

âˆ
X

P {kA k2 â‰¥ 1 (n, Î´n )}

n=1

+ P {kA Î¸ âˆ’ b )k2 â‰¥ 2 (n, Î´n )}
âˆ
1 Ï€2
1X 1
Î´=
Î´<Î´
â‰¤2
Î´n =
2
2 n=1 n
2 6
n=1
âˆ
X

if on the last line we set Î´n = 4n1 2 Î´. By the second part
of Lemma 1, for all Î´, with probability at least 1 âˆ’ Î´, for
all n such that 1 (n, Î´n ) < C, where C is chosen such that
1
C â‰¤ kAâˆ’1
k2 , then AÌ‚ is invertible and
2 (n, Î´n )
1 âˆ’ Î»Î³
âˆš
(1 âˆ’ Î³) Î½ 1 âˆ’ 1 (n,Î´n )
C


1 âˆ’ Î»Î³
1 (n, Î´n ) 2 (n, Î´n )
âˆš 2 (n, Î´n ) +
=
.
C âˆ’ 1 (n, Î´n )
(1 âˆ’ Î³) Î½

kvLST D(Î») âˆ’ vÌ‚LST D(Î») kÂµ â‰¤

The bound of the Theorem 1 is obtained by replacing
1 (n, Î´n ) and 2 (n, Î´n ) with their definitions in Equations (17) and (21), in particularly noticing that (n), 0 (n)
and 00 (n) are OÌƒ( n1 ).
To fully complete the proof of Theorem 1, we finally need
1
to show how to pick C â‰¤ kAâˆ’1
k2 . We have âˆ€v âˆˆ
p
âˆš
d
âˆ’1
R , kÎ¦A vkÂµ = (Aâˆ’1 v)T MÂµ Aâˆ’1 v â‰¥ Î½kAâˆ’1 vk2 .
âˆ’1
âˆ’1
We know that kÎ¦A vkÂµ = k(I âˆ’ Î M ) Î¦MÂµâˆ’1 vkÂµ â‰¤
1âˆ’Î»Î³
âˆš kvk2 where the inequalities are respectively ob(1âˆ’Î³) Î½
tained from Equations (12) and (13). Therefore kAâˆ’1 k2 â‰¤
(1âˆ’Î³)Î½
1âˆ’Î»Î³
(1âˆ’Î³)Î½ , and consequently we can take C = 1âˆ’Î»Î³ . Note
that the condition 1 (n, Î´n ) < C for this choice of C is
equivalent to the one that characterizes the index n0 (Î´) in
the theorem. This concludes the proof of Theorem 1.

On the Rate of Convergence and Error Bounds for LSTD(Î»)

5. Summary, Related and Future Work
This paper provides high-probability bound on the convergence rate for the standard LSTD(Î») and a penalized variation, in terms of the number of samples n and the parameter Î». Theorems 1 and 3 show that this convergence is
at the rate of OÌƒ( âˆš1n ), in the case where the samples are
generated from a stationary Î²-mixing process. Our result
is based on two original technical contributions: a) a deterministic sensitivity analysis of LSTD(Î») (Lemma 1) and
b) an original vector concentration inequality (Lemma 2)
for estimates that are based on eligibility traces. A simplified version of the latter (Lemma 4) is a general-purpose
concentration inequality that may apply to general stationary beta-mixing processes, which may be useful in many
other contexts where we want to relax the i.i.d. hypothesis
on the samples. Corollary 1, which is an immediate consequence of Theorem 1, is to our knowledge the very first
analytical result that provides insight on the choice of the
eligibility-trace parameter Î» of temporal-difference learning algorithm with respect to the approximation quality of
the space and the number of samples. Validating empirically the lessons that we can take from this result constitutes immediate interesting future work.
Under the same assumptions, the global error bound obtained by Lazaric et al. (2012) in the restricted case where
Î» = 0 has the following form:
!
r
âˆš
d log d
4 2
kv âˆ’ Î vkÂµ + O
,
kvÌƒLST D(0) âˆ’ vkÂµ â‰¤
1âˆ’Î³
Î½n
where vÌƒLST D(0) is the truncation with thresholds
{âˆ’Vmax , Vmax } of the estimate vÌ‚LST D(0) . In our analysis,
we get for Î» = 0:


d
1
âˆš
kv âˆ’ Î vkÂµ + OÌƒ
kvÌ‚LST D(0) âˆ’ vkÂµ â‰¤
.
1âˆ’Î³
Î½ n
On the one hand, the term âˆš
corresponding to the approximation error is a factor 4 2 better with our analysis;
our bound is thus asymptotically better. Note that, contrary to our approach, the analysis of Lazaric et al. (2012)
does not imply a rate of convergence for LSTD(0) (a
bound on kvLST D(0) âˆ’ vÌ‚LST D(0) kÂµ ); their arguments,
based on a model of regression with Markov design, consists in directly bounding the global error. On the other
hand, our bound on the estimation error depends linearly
on the features space dimension d and on Î½1 while the
one obtained by Lazaric
et al. (2012) takes the form of

p
O
d log d/(nÎ½) . Thus our bound seems suboptimal
on d and Î½. A technical element for explaining such a difference is the fact, mentionned above, that Lazaric et al.
(2012) consider the truncated version of vLST D(0) . Inp
deed, a close examination shows that the extra term d/Î½
in our bound results from a bound (uniform on x) on
vLST D(Î») (x).

A critical condition in the analysis of LSTD(0) previously
done by Lazaric et al. (2012) is that the noise term in
the Markov Regression model is a Martingale difference
sequence with respect to the filtration generated by the
Markov chain. As soon as Î» > 0, this property stops to
hold and it has not been clear how one may fix this issue.
We believe that the techniques we used for the proof of our
concentration inequality (Lemma 2)â€”the truncation of the
trace at some depth m and the focus on the â€œblockâ€ chain
(Zn ) = (Xiâˆ’m+1 , Xiâˆ’m , . . . , Xi+1 )â€”constitutes a potential track for addressing these issues. If successful, note
however that an extension to Î» > 0 of the work ofâˆšLazaric
et al. (2012) would still contain a suboptimal 4 2 extra
factor in the final bound.
Regarding the dependence with respect to the parameters
d and Î½, it is worth mentionning that the bound obtained
by Pires & SzepesvaÌri (2012) for a regularized version
of LSTD(0) depends also linearly âˆš
on d and kÎ¸k2 (which
in turn can be bounded by Vmax / Î½). In (Antos et al.,
2006) the bound does
 not
depend on Î½ but the convergence
1
4
rate is of order OÌƒ 1/n
which is a slower rate than the
one we get. In the deterministic design and pure regression settingâ€”pure regression corresponds to value function
learning with Î³ = 0â€”, the corresponding bound does not
also involve the parameter Î½ (GyoÌˆrfi et al., 2002). We do
not know whether one could have theâˆšbest of all worlds: the
best asymptotic bound without the 4 2 coefficient, and the
best rate with respect to n, d and Î½. This constitutes interesting future work.
More generally, in the future, we plan to instantiate our new
bound in a Policy Iteration context like Lazaric et al. (2012)
did for LSTD(0). An interesting follow-up work would
also be to extend our analysis of LSTD(Î») to the situation
where one considers non-stationary policies, as Scherrer &
Lesner (2012) showed that it allows to improve the overall performance of the Policy Iteration Scheme. Finally, a
challenging problem would be to consider convergence rate
LSTD(Î») in the off-policy case, for which the convergence
has recently been proved by Yu (2010).
Acknowledgments. We thank the anonymous reviewers,
whose comments helped to improve the paper. This work
was supported by the French National Research Agency
(ANR) through the project BARQ.

References
Antos, AndraÌs, SzepesvaÌri, Csaba, and Munos, ReÌmi.
Learning near-optimal policies with bellman-residual
minimization based fitted policy iteration and a single
sample path. In In COLT-19, pp. 574â€“588. SpringerVerlag, 2006.
Bertsekas, Dimitri P. and Tsitsiklis, John N.

Neuro-

On the Rate of Convergence and Error Bounds for LSTD(Î»)

Dynamic Programming. Athena Scientific, 1996.
Boyan, Justin A. Technical update: Least-squares temporal difference learning. Machine Learning, 49(2â€“3):233â€“
246, 2002. ISSN 0885-6125.
Bradley, Richard. Basic properties of strong mixing conditions. a survey and some open questions. Probability
Survey, 2:107â€“144, 2005.
Downey, Carlton and Sanner, Scott. Temporal difference
bayesian model averaging: A bayesian perspective on
adapting lambda. In FuÌˆrnkranz, Johannes and Joachims,
Thorsten (eds.), ICML, pp. 311â€“318. Omnipress, 2010.
GyoÌˆrfi, LaÌszloÌ, Kholer, Michael, Krzyzak, Adam, and
Walk, Harro. A Distribution-Free Theory of Nonparametric Regression. Springer-Verlag, New York, 2002.
Hayes, Thomas P. A large-deviation inequality for vectorvalued martingales, 2005. Technical report.
Kearns, M.J. and Singh, S.P. Bias-variance error bounds
for temporal difference updates. In Cesa-Bianchi, NicoloÌ€
and Goldman, Sally A. (eds.), COLT, pp. 142â€“147. Morgan Kaufmann, 2000.
Lazaric, Alessandro, Ghavamzadeh, Mohammad, and
Munos, ReÌmi. Finite-sample analysis of least-squares
policy iteration. Journal of Machine Learning Research,
13:3041â€“3074, October 2012.
Nedic, Angelia and Bertsekas, Dimitri P. Least squares policy evaluation algorithms with linear function approximation. Theory and Applications, 13:79â€“110, 2002.
Pires, Bernardo A. and SzepesvaÌri, Csaba. Statistical linear
estimation with penalized estimators: an application to
reinforcement learning. In ICML, pp. 1535â€“1542, 2012.
Scherrer, Bruno. Should one compute the temporal difference fix point or minimize the Bellman residual? the
unified oblique projection view. In ICML, pp. 959â€“966,
2010.
Scherrer, Bruno and Lesner, Boris. On the use of nonstationary policies for stationary infinite-horizon Markov
decision processes. In NIPS 2012 Adv.in Neural Information Processing, December 2012.
Sutton, Richard S. and Barto, Andrew G. Reinforcement
learning i: Introduction, 1998.
SzepesvaÌri, Csaba. Algorithms for Reinforcement Learning. Morgan and Claypool, 2010.
Tsitsiklis, John N. and Roy, Benjamin Van. An analysis
of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42:
674â€“690, 1997.

Yu, Bin. Rates of convergence for empirical processes stationnary mixing sequences. The Annals of Probability,
22(1):94â€“116, January 1994.
Yu, Huizhen. Convergence of least-squares temporal difference methods under general conditions. In ICML, pp.
1207â€“1214, 2010.

