Robust Distance Metric Learning via Simultaneous ℓ1 -Norm
Minimization and Maximization
Hua Wang
HUAWANGCS @ GMAIL . COM
Colorado School of Mines, Department of Electrical Engineering and Computer Science, Golden, Colorado 80401
Feiping Nie
FEIPINGNIE @ GMAIL . COM
Heng Huang
HENG @ UTA . EDU
Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, 76019

Abstract
Traditional distance metric learning with side information usually formulates the objectives using
the covariance matrices of the data point pairs in
the two constraint sets of must-links and cannotlinks. Because the covariance matrix computes
the sum of the squared ℓ2 -norm distances, it is
prone to both outlier samples and outlier features. To develop a robust distance metric learning method, we propose a new objective for
distance metric learning using the ℓ1 -norm distances. The resulted objective is challenging to
solve, because it simultaneously minimizes and
maximizes (minmax) a number of non-smooth
ℓ1 -norm terms. As an important theoretical contribution of this paper, we systematically derive
an efficient iterative algorithm to solve the general ℓ1 -norm minmax problem. We performed
extensive empirical evaluations, where our new
distance metric learning method outperforms related state-of-the-art methods in a variety of experimental settings.

1. Introduction
Distance metric plays a critical role in a number of machine learning algorithms. For example, K-means clustering and k-Nearest Neighbor (k-NN) classification need to
be supplied with a suitable distance metric, through which
neighboring data points can be identified. The commonly
used Euclidean distance metric assumes that every feature
of the input data is equally important and independent from
others. This assumption, however, may not be always satisfied in real world applications, especially when we deal
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

with high dimensional data where some features may not
be tightly related to the topic of interest. In contrast, a distance metric with good quality should identify important
features and discriminate relevant and irrelevant features.
Thus, supplying such a distance metric is highly problemspecific and determines the success or failure of a learning
algorithm (Xing et al., 2002; Hoi et al., 2006; Xiang et al.,
2008; Weinberger et al., 2006; Davis et al., 2007).
In this paper, we address the issue of the robustness of a
distance metric in the presence of both outlier samples and
outlier features. The former is defined as the data points
that deviates significantly from the majority of the data
points, and the latter is defined as features that do not have a
regular distribution over the data points. Given the side information of the must-links and cannon-links (Xing et al.,
2002; Hoi et al., 2006; Xiang et al., 2008), traditional distance metric learning approaches often formulate the objectives using the covariance matrices of the data point pairs in
the two constraint sets. However, because these estimates
are defined as the sum of the squared ℓ2 -norm distances,
they could be highly influenced by outlying observations
and features. That is, these measurements become inappropriate on contaminated data sets, because large errors
squared dominate the sum.
Many previous works have been done to improve the robustness of machine learning models through using the ℓ1 norm formulations (Ding et al., 2006; Cayton & Dasgupta,
2006; Gao, 2008; Ke & Kanade, 2004; Kwak, 2008;
Wright et al., 2009; Nie et al., 2011; Wang et al., 2013b;a).
However, most, if not all, these methods tried to improve
the robustness of the models like Principal Component
Analysis (PCA). Thus far, to our best knowledge, there
exists no work to utilize ℓ1 -norm based objective for distance metric learning. Although it is easy and straightforward to derive a robust formulation for metric learning using the ℓ1 -norm distances, it is non-trivial and difficult to
solve the resulted optimization problems involving the ℓ1 -

Robust Distance Metric Learning via Simultaneous ℓ1 -Norm Minimization and Maximization

norm distances. Because most metric learning objectives
(either ratio or substraction) have to simultaneously minimize the distances of the point pairs in the must-links and
maximize those in the cannot-links, all current optimization
methods in sparse learning, such as gradient projection, homotopy, iterative shrinkage-thresholding, proximal gradient, and augmented lagrange multiplier methods, cannot
efficiently solve the ℓ1 -norm based distance metric learning
objectives. Zha et al. (2009) made a successful attempt to
learn a robust distance metric, which, however, achieved its
goal by utilizing additional knowledge from auxiliary data
yet did not replace the traditional distance metric learning
objective by any new robust formulation.

tain application context:
(
S = {(xi , xj ) | xi and xj are in the same class} ,
D = {(xi , xj ) | xi and xj are not in the same class} ,
(1)
where we call S as must-links and D as cannot-links
(Xing et al., 2002). Note that it is not necessary for all data
points in X to be involved in S or D.

In this paper, we propose a new robust distance metric
learning objective that utilizes the ℓ1 -norm distances and
directly provides the robustness against outlier samples and
outlier features. However, because of using the ℓ1 -norm
distances, the resulted objective ends up to be a simultaneous ℓ1 -norm minimization and maximization (minmax)
problem. Although there exist a large number of optimization algorithms to solve the objectives involving ℓ1 -norm
or ℓ2,1 -norm minimizations, how to efficiently solve the ℓ1 norm minmax problem is scarcely studied in literature. As
an important theoretical contribution of this paper, we systematically derive an efficient iterative algorithm to solve
the general ℓ1 -norm minmax problem, whose convergence
is guaranteed by the rigorous theoretical analysis. We have
performed extensive experiments to apply our new distance
metric learning method on five benchmark data sets to evaluate its robustness against both outlier samples and outlier
features, in which we achieved very promising results that
are consistent with our theoretical analysis.

where M ∈ Rd×d is the Mahalanobis distance metric, a
symmetric matrix of size d × d. In general, M is a valid
metric if and only if M is a positive semi-definite matrix
by satisfying the non-negativity and the triangle inequality
conditions, i.e., M  0. When setting M to be the identity matrix Id×d , the distance computed by Eq. (2) becomes
the Euclidean distance. Our goal in robust distance metric
learning is to learn an optimal square matrix M from a collection of data points X in the presence of outliers, such
that the distances between the data point pairs in S are as
small as possible, whilst those in D are as large as possible.

2. Learning A Robust Distance Metric
In this section, we will first develop a robust objective
for distance metric learning using the ℓ1 -norm distances,
which, though, is highly non-smooth and difficult to solve.
In the next section, we will present an efficient iterative solution algorithm with a rigorous proof on its convergence.
Notations. Throughout this paper, we write matrices as
bold uppercase characters and vectors as bold lowercase
characters. Given a matrix M = [mij ], we denote its i-th
row and its j-th column as mi and mP
j , respectively. The
ℓ1 -Norm of M is defined as kMk1 = i,j |mij |.

Problem formalization.
Assume

	n that we have a set of
n data points X = xi ∈ Rd i=1 . For convenience, we
write X = [x1 , . . . , xn ] ∈ Rd×n . Besides, we are also
supplied with two sets of pairwise constraints among the
data points, which are manually labeled by users under cer-

Given any two data points xi and xj , a Mahalanobis distance between them can be computed as following:
q
T
kxi − xj kM = (xi − xj ) M (xi − xj ) , (2)

Learning a robust distance metric using the ℓ1 -norm
minmax formulation. Xing et al. (Xing et al., 2002) first
studied the problem of learning a distance metric from
must-links and cannot-links, in which the sum of the Mahalanobis distances between the point pairs in the mustlinks is minimized under the constraints developed from the
point pairs in the cannot-links. Despite its effectiveness,
it is computationally inefficient when dealing with highdimensional data (Xiang et al., 2008). Relevance Component Analysis (RCA) (Bar-Hillel et al., 2003) was then
proposed to solve the inverse matrix of the covariance
matrix of the data point pairs in the chunklets (mustlinks), which, though, may not exist in high dimensional
data. Disciminative Component Analysis (DCA) and Kernel DCA (Hoi et al., 2006) improved RCA by exploiting
negative constraints and aimed to capture nonlinear relationships using contextual information. Both RCA and
DCA, however, faced the singular problem when computing the covariance matrix for the data point pairs in the
must-links in the case of high dimensionality (Xiang et al.,
2008). To tackle this, Xiang et al. (2008) proposed to formulate the distance metric learning problem as a trace ratio
minimization problem as following.
Because M is a positive semi-definite matrix, we can
reasonably write M = WWT by eigen-decomposition,
where W ∈ Rd×r with r ≤ d. Thus the Mahalanobis distance underqthe metric M can be computed
as kxi − xj kM =

T

(xi − xj ) WWT (xi − xj ) =

Robust Distance Metric Learning via Simultaneous ℓ1 -Norm Minimization and Maximization

 T

W (xi − xj ) , which indeed defines a transformation
2
of y = WT x under the projection matrix W. Then denote
the covariance matrix
of the data point pairs in the mustP
T
links as Sw =
(xi ,xj )∈S (xi − xj ) (xi − xj ) and the
covariance P
matrix of the data point pairs in the cannot-links
T
as Sb =
(xi ,xj )∈D (xi − xj ) (xi − xj ) , Xiang et al.
(2008) proposed to learn the transformation matrix W by
solving the following objective:
min

WT W=I


2


T
T
−
x
)
W
(x

i
j
tr W Sb W
(xi ,xj )∈S
=

22 .
T
P


tr (W Sw W)
T
−
x
)
W
(x

i
j
(xi ,xj )∈D


P

2

(3)

The objective in Eq. (3) measures the ratio of the sums of
the two sets of the squared ℓ2 -norm distances, one set for
the data point pairs in the must-links and the other for those
in the cannot-links. As a result, same as other least square
minimization based models in machine learning and statistics, Eq. (3) is sensitive to the presence of outliers. Recent progress (Gao, 2008; Ke & Kanade, 2004; Ding et al.,
2006; Kwak, 2008; Wright et al., 2009) has shown that
the ℓ1 -norm distance can introduce robustness against both
outlier samples as well as outlier features, which have
been widely applied to replace the squared ℓ2 -norm distance in many machine learning methods, such as PCA
(Wright et al., 2009). Following the same motivations as
these prior studies, we propose to replace the squared ℓ2 norm distances in the distance metric learning objective in
Eq. (3) by the ℓ1 -norm distances to promote the robustness,
which leads to the following optimization problem:


P


T
−
x
)
W
(x

i
j
(xi ,xj )∈S
kAWk1

1 =
min P
,


T
kBWk
WT W=I
1
(xi − xj ) W
(xi ,xj )∈D

1

(4)
T
where each row of A is one (xi − xj ) that satisfies
(xi , xj ) ∈ S, and similarly each row of B is one
T
(xi − xj ) that satisfies (xi , xj ) ∈ D.
Note that Eq. (4) can be rewritten as following:
Pr
kAWk1
kAwi k1
min
,
= Pi=1
r
WT W=I kBWk1
i=1 kBwi k1

WT W=I

r
X
kAwi k
i=1

1

kBwi k1

,

Upon solution, the distance metric can be obtained by computing M = WWT .

3. An Efficient Algorithm to Solve ℓ1 -Norm
Minmax Problem
Despite the straightforward replacement from the squared
ℓ2 -norm distances in Eq. (3) to the ℓ1 -norm distances in
Eq. (6) to promote robustness against outliers, solving
Eq. (6) is very challenging, because it simultaneously minimizes and maximizes a number of non-smooth ℓ1 -norm
terms. Although there exist in literature a plethora of algorithms (Argyriou et al., 2008; Nie et al., 2010; Wang et al.,
2011; 2012) that can minimize the objectives involving ℓ1 norm or ℓ2,1 -norm terms, how to efficiently solve the objectives that simultaneously minimize and maximize ℓ1 -norm
terms are rarely studied. As an important theoretical contribution of this paper, we will derive an efficient algorithm
to solve the general ℓ1 -norm minmax problem in Eq. (6) in
the rest of this section.
Before deriving our new solution algorithm, we first introduce the following useful proposition.
Proposition 1 For a linear learning model, suppose the
output projection vector u can be linearly represented by
the input data X, and suppose an orthonormal projection matrix V has been learned by the same model, i.e.,
VT V = I. If we learn another projection vector u from
e = X − VVT X
X

(7)

by the same learning model, then u is orthogonal to V.
(5)

which minimizes the ratio between the overall ℓ1 -norm distances of the data point pairs in the must-links along all the
projecting directions and those in the cannot-links. A potential problem is that the ratio between the ℓ1 -norm distances of the data point pairs in the must-links and those
in the cannot-links may not be minimized along each individual projecting direction. Thus, we turn to minimize the
following better objective which emphasizes the projection
performance along every projecting direction:
min

Because Eq. (6) does not square the distances from either
data point or feature perspective, the outlying samples and
features will have less influence. As a result, same as other
ℓ1 -norm distance based learning models, the objective in
Eq. (6) is more robust against outliers than its counterpart
defined in Eq. (3). Thus, we call Eq. (6) as the proposed
robust distance metric learning model.

(6)

Proof. Since u can be linearly represented by the input
e we have u = Xα
e for certain α. Thus VT u =
data X,


T
T
V X − VV X α = VT X − VT VVT X α =
VT X − VT X α = 0, which completes the proof of
Proposition 1.

The main strength of Proposition 1 lies in that it converts a
difficult orthogonally constrained optimization problem to
a series of unconstrained optimization problems by using
the reconstruction residues from the learned projections in
Eq. (7), which, fortunately, is usually much easier to solve.
Equipped with Proposition 1, we derive the solution algorithm to the problem in Eq. (6) in Algorithm 1.

Robust Distance Metric Learning via Simultaneous ℓ1 -Norm Minimization and Maximization

Algorithm 1 An efficient iterative algorithm to solve the
general ℓ1 -norm minmax problem with orthogonal constraint in Eq. (6).
Input: Input data X , and the must-links S and cannot-links D.
1. Let X1 = X. Construct A1 and B1 from X1 , S and D, and compute w1
kAw k

by solving the problem minw1 kBw1 k1 .
1 1
2. j = 2.
while j ≤ r do
T
3. Compute Xj = Xj−1 − wj−1 wj−1
Xj−1 .
4. Construct Aj and Bj from Xj , S and D.
kAj wj k
5. Compute wj by solving the problem minwj B w 1 .
k j j k1
6. j = j + 1.
end while
Output: The learned projection matrix W = [w1 , . . . , wr ] ∈ Rd×r .

Obviously, Step 1 and Step 5 are the key steps of Algorithm 1, which solve the following problem:
w

kAwk1
.
kBwk1

(8)

In the following, we will derive the solution to Eq. (8). The
detailed procedures are summarized in Algorithm 3.
3.1. Useful Theorems

v∈C

f (v)
,
g(v)

where g(v) > 0 (∀ v ∈ C) ,

(9)

is computed by the root of the following function:
h(λ) = min f (v) − λg(v) ,
v∈C

(10)

given that f (v) − λg(v) is lower bounded.
∗

Proof. Suppose v is the global solution of the problem in
Eq. (9), and λ∗ is the corresponding global minimal objec(v∗ )
∗
tive value, the following holds: fg(v
∗ ) = λ . Thus ∀ v ∈ C,
(v)
we have fg(v)
≥ λ∗ . Because we know that g(v) > 0, we
can derive f (v) − λ∗ g(v) ≥ 0, which means:

min f (v) − λ∗ g(v) = 0

v ∈C

Proof. In Algorithm 2, from step 2 we know that f (vt ) −
λt g(vt ) = 0. According to step 3, we know that f (vt+1 )−
λt g(vt+1 ) ≤ f (vt ) − λt g(vt ). Combining the above two
inequalities, we have f (vt+1 ) − λt g(vt+1 ) ≤ 0, which
(vt+1 )
(vt )
indicates fg(v
≤ λt = fg(v
. That is, Algorithm 2
t+1 )
t)
decreases the objective value of Eq. (9) in each iteration,
which completes the proof of Theorem 2.

Theorem 3 Algorithm 2 is a Newton’s method that finds
the root of the function h(λ) in Eq. (10).
Proof. From step 3 in Algorithm 2 we know that
h(λt ) = f (vt+1 ) − λt g(vt+1 ) .

(12)

Thus h′ (λt ) = −g(vt+1 ).
In Newton’s method, the updated solution should be

Theorem 1 The global solution of the following general
optimization problem:
min

1. t = 1. Initialize vt ∈ C.
while not converge do
f (v )
2. Calculate λt = g(v t ) .
t
3. Calculate vt+1 = arg minv∈C f (v) − λt g(v).
4. t = t + 1.
end while

Theorem 2 Algorithm 2 decreases the objective value of
the problem in Eq. (9) in each iteration till converges.

In Algorithm 1, we learn one projection vector at a time.
Because we learn every projection vector on the reconstruction residue from the previously learned projections,
according to Proposition 1, the learned projection vectors
wj (1 ≤ j ≤ r) are orthogonal to each other.

min

Algorithm 2 The algorithm to solve Eq. (9).

⇐⇒

h(λ∗ ) = 0 .

(11)

That is, the global minimal objective value λ∗ of the problem in Eq. (9) is the root of the function h(λ), which completes the proof of Theorem 1.

To solve the problem in Eq. (9), we propose an iterative algorithm as summarized in Algorithm 2, whose convergence
is guaranteed by Theorem 2 and whose computational efficiency is guaranteed by Theorem 3.

h(λt )
h′ (λt )
(13)
f (vt+1 )
f (vt+1 ) − λt g(vt+1 )
=
,
= λt −
−g(vt+1 )
g(vt+1 )

λt+1 = λt −

which is exactly the step 2 in Algorithm 2. That is, Algorithm 2 is a Newton’s method to find the root of the function
h(λ).

Theorems (1–3) present a complete framework to solve the
general optimization problem in Eq. (9), where an efficient
iterative algorithm is provided in Algorithm 2 with rigorously proved convergence and satisfactory computational
efficiency. It is worth to noting that, besides applying it
to solve the ℓ1 -norm minmax problem as in our objective
in Eq. (6), we can also employ this framework to efficiently
solve many other optimization problems that widely appear
in machine learning and statistics, e.g., the trace-ratio minimization problem in Eq. (3) (Jia et al., 2009; Wang et al.,
2014). Therefore, Theorems (1–3) are considered as one of
the most important theoretical contribution of this paper.
3.2. Derivation of Algorithm 3
Now we derive the solution algorithm to Eq. (8) using the
optimization framework described in Theorems (1–3).

Robust Distance Metric Learning via Simultaneous ℓ1 -Norm Minimization and Maximization

Because the problem in Eq. (8) is a special case of the general optimization problem in Eq. (9), we can derive its solution algorithm using Algorithm 2, in which the key step is
to solve the problem in Step 2. Given the λ computed from
Step 1 of Algorithm 2, according to Step 2 of Algorithm 2,
instead of solving the original problem in Eq. (8), we turn
to solve the following optimization problem:

Algorithm 3 An efficient iterative algorithm to solve the
general ℓ1 -norm minmax problem in Eq. (8).
Input: Matrices A and B.
1. Initialize w by a random guess.
repeat
kAwk
2. Compute λ = kBwk1 .
1
3. Compute the diagonal matrix D with its i-th diagonal entry as dii =
1
.
i
2|a w|

4. Compute the vector s with its i-th entry as si =

min kAwk1 − λ kBwk1 .

(14)

w

bi w
.
|bi w|

T

5. Compute w by solving the linear equations 2A DAw = λBT s.
until Converges
Output: The learned projection vector w.

Solving the problem in Eq. (14), again, is challenging, because it is non-smooth due to involving the non-smooth ℓ1 norm terms. We derive its solution as following.

3.3. Analysis of Algorithm 3

By taking the derivative of Eq. (14) with respect w and
setting it as 0, we obtain1:

Convergence analysis of the algorithm. The following
theorem guarantees the convergence of Algorithm 3.



X bi T bi w
X 2 ai T ai w
−λ
=0 .
2|ai w|
|bi w|
i
i
Let dii =

1
2|ai w|

(15)

Theorem 4 Algorithm 3 decreases the objective value of
the problem in Eq. (8) in each iteration till converges.

and construct the diagonal matrix D with

Proof. First, it is obvious that Step 5 of Algorithm 3 computes the optimal solution of the following problem:

i

b w
its diagonal entries as dii , and let si = |b
i w| and construct the vector s with its i-th element as si , we can rewrite
Eq. (15) as following:
T

T

2A DAw − λB s = 0 .

(16)

min wT AT DAw − λwT BT s .

e the updated w by AlgoFor each iteration, we denote by w
rithm 3. According to Step 5 of Algorithm 3 and Eq. (17),
we have the following inequality:

Thus, we can compute w by solving a system of linear
equations in Eq. (16)2 .
Note that, in Eq. (16) both D and s are dependent on w.
Therefore they are unknown variables and can be seen as
the latent variables of the objective in Eq. (14), which can
be solved under the same iterative framework by alternatively optimizing them (Nie et al., 2010). Specifically, we
compute D and s upon the current w obtained in the last
iteration. Finally, we summarize the whole computation
procedures in Algorithm 3, which, to our best knowledge,
solves the ℓ1 -norm minmax problem for the first time3 .
1

Because the ℓ1 -norm is non-smooth, following
(Argyriou et al., 2008), we address thisp by introducing a
P
small perturbation to replace kvk1 by i vi2 + ζ, where v is
a general vector and ζ is a small positive constant. Apparently,
P p 2
vi + ζ reduces to kvk1 when ζ → 0. In the rest of
i
this paper, this replacement is always implicitly applied in the
definition of k·k1 , unless otherwise stated.
pSimilarly, given a scalar variable v, we always replace |v| by
v 2 + ζ to address the non-smoothness of |·|.
2
Note that, in real problems AT DA could be rank deficient,
because the number of constraints in the must-links could be
smaller than the dimensionality of the feature space. Therefore, in
practice we compute w by solving 2AT DA + ζI w = λBT s,
where I is the identity matrix and ζ is a small pertubation.
3
The objective of the ℓ1 -norm minmax problem in Eq. (14)
is non-convex, thus the algorithm is guaranteed to converge to a

(17)

w

e T AT DAw
e − λw
e T BT s ≤
w

(18)

wT AT DAw − λwT BT s .

Applying the definitions of A and s in Steps 3–4, we can
equivalently rewrite Eq. (18) by decoupling the computation for each row of A and B as following:
2
X (ai w)
e
i

2|ai w|

X (ai w)2
i

2|ai w|

−λ

T
Xw
e T (bi ) bi w
i

−λ

X wT (bi )T bi w
i

≤

|bi w|

(19)
,

|bi w|

e is a scalar and ai w
where we note that ai w
ie 2
ie 2
|a w| = (a w) .

T


ai w =

Because it can verified that for function z (x) = x −
given any x 6= α ∈ Rn , z (x) ≤ z (α) holds, we have:
2

e −
|ai w|

x2
2α ,

2

e
(ai w)
(ai w)
i
≤
|a
w|
−
.
2|ai w|
2|ai w|

(20)

local optima of the objective. A proper initialization could effectively avoid being trapped by local optima. In practice, we replace
the ℓ1 -norm in Eq. (14) by the squared ℓ2 -norm and solve it as initialization, which empirically works very well in our experiments.

Robust Distance Metric Learning via Simultaneous ℓ1 -Norm Minimization and Maximization

which implies that
T

e T (bi ) bi w
w
e ≤ 0 = |bi w| − |bi w| =
− |bi w|
|bi w|
i T

wT (b ) bi w
− |bi w| .
|bi w|

(22)

Then by adding the three inequalities in Eq. (19), Eq. (20)
and Eq. (22) in the both sides, we obtain:
X
X
X
X
e ≤
e −λ
|bi w| = 0
|ai w| − λ
|bi w|
|ai w|
i

i

i

i

P i
P i
e 1
e
kAwk
kAwk1
|a w|
|a w|
i
=⇒
=
≤ λ = Pi i
=P i
.
e
e 1
|b
|b
w|
w|
kBwk
kBwk1
i
i
(23)

Note that, the equalities in Eqs. (19–23) hold if and only
if the objective value converges. Therefore, the objective
value of the problem in Eq. (8) is decreased in each iteration
till converges, which completes the proof of Theorem 4. 

Computational analysis of the algorithm. In Algorithm 3, Steps 2–4 are computationally trivial. Step 5
solves a system of linear equations, which is very well
studied and efficient solution algorithms exist. Moreover,
according to Theorem 3, Algorithm 3 implements a Newton’s method to find the root of the function defined over
the objective value of the problem in Eq. (8). Thus, Algorithm 3 converges very fast with the quadratic convergence
rate, i.e., the difference between the current objective value
and the optimal objective value is smaller than c1ct at the tth iteration, where c > 1 is a certain constant. In summary,
Algorithm 3 scales very well to large-scale data sets, which
adds to the practical value of the proposed method.

4. Experiments
In this section, we evaluate the proposed method in the
tasks of data clustering, where our goal is to examine the
robustness of our new method under the conditions when
data outliers or feature outliers are present.
4.1. Data Preparation
We experiment with four benchmark data sets downloaded
from the UCI machine learning data repository, including
the Breast, Diabetes, Iris and Protein data sets, and one
image data set downloaded from the ORL database, whose
details are summarized in Table 1.
Following previous research, we generate the must-links
and cannot-links for each data set as follows. For each constraint, we randomly pick up one pair of data points from

Table 1. Descriptions of the experimental data sets.
Data set
Breast
Diabetes
Iris
Protein
ORL
n
d
c
r

683
10
5
10

768
8
4
8

n: number of samples.
c: number of clusters.

Clustering Accuracy

In addition, according to the Cauchy-Schwarz inequality
we have:
 i  i 
T
b w
e   b w  ≥ bi w
e bi w ,
(21)

150
4
2
4

116
20
8
16

400
10304
40
80

d: input dimensionality.
r: reduced dimensionality.

0.9

0.8

0.7

0.6

0.5
1

2

5

10

20

50

100

r

200

500 1000 2000 5000 10304

Figure 1. Clustering performance on the ORL data with respect to
r (the dimensionality of the projected (sub)space by W).

the input data sets (the labels of which are available for
evaluation purpose but unavailable for clustering). If the
labels of this pair of data points are the same, we generate a must link. If the labels are different, a cannot link is
generated. We pick up 100 constraints for each data set.
4.2. Parameter Selection of Our Method
The proposed method has only one parameter, i.e., r of the
reduced dimensionality of the projected (sub)space by the
transformation of W. In this subsection, we study its impact to the learned distance metrics by performing clustering on the ORL data, where we vary the value of r from
its minimum possible value of 1 to its maximum possible
value of 10304. For each experimental trial, first we learn
a distance metric from the input data with a given value of
the parameter r, then we perform K-means clustering using the learned distance metric. For each different value of
r, we repeat the experiment for 100 times to eliminate the
difference caused by the constraint pickup and the initialization of K-means clustering. The clustering performance
measured by clustering accuracy averaged over 100 trials
are reported in Figure 1.
A first glance at the results in Figure 1 shows that the clustering performance is very stable with respect to the parameter r in a considerably large range of [50, 10304], which
makes tuning parameter of our new method not a difficult
task. In addition, we also notice that, when r is small, e.g.,
when r < 50, the clustering performance is not satisfactory, which can be seen as follows. As discussed earlier,
metric learning can be equivalently performed as subspace

Robust Distance Metric Learning via Simultaneous ℓ1 -Norm Minimization and Maximization

Table 2. Clustering performances of the compared methods measured by clustering accuracy (mean ± std).
Original data
Noisy data
Perf. diff.
Original data
Noisy data
Perf. diff.
Breast data
Eu
Mah
Xing’s
RCA
DCA
Xiang’s
LMNN
ITML
Our method

94.23 ± 1.23
95.01 ± 0.21
94.24 ± 1.12
93.36 ± 0.54
92.07 ± 0.96
94.48 ± 0.41
95.12 ± 0.18
93.72 ± 1.01
95.94 ± 0.44

90.53 ± 1.03
91.42 ± 0.33
90.43 ± 0.96
89.45 ± 0.62
89.12 ± 0.88
90.34 ± 0.44
91.56 ± 0.13
90.24 ± 0.99
94.54 ± 0.51

Eu
Mah
Xing’s
RCA
DCA
Xiang’s
LMNN
ITML
Our method

85.52 ± 2.45
94.42 ± 1.15
92.36 ± 1.66
95.91 ± 1.72
96.54 ± 0.34
96.60 ± 0.31
96.41 ± 0.39
93.27 ± 0.74
97.03 ± 0.15

80.41 ± 2.51
89.44 ± 1.07
88.41 ± 1.95
89.40 ± 1.85
90.15 ± 0.74
91.24 ± 0.96
90.60 ± 0.86
88.95 ± 1.21
95.17 ± 0.31

Diabetes data
3.92%
3.87%
4.04%
4.19%
3.59%
4.38%
3.74%
3.71%
1.46%

55.83 ± 2.11
58.12 ± 1.74
56.61 ± 1.88
58.33 ± 1.21
57.53 ± 1.63
60.91 ± 0.64
60.44 ± 1.07
60.21 ± 0.81
62.14 ± 0.37

50.12 ± 2.23
52.41 ± 1.81
51.11 ± 1.64
53.01 ± 1.32
50.23 ± 1.44
54.15 ± 0.77
53.22 ± 1.11
53.67 ± 1.03
60.67 ± 0.42

5.97%
5.27%
4.27%
6.79%
6.62%
5.55%
6.03%
4.63%
1.91%

66.22 ± 2.11
68.02 ± 1.35
68.13 ± 1.62
68.09 ± 1.11
62.43 ± 2.11
73.47 ± 0.41
72.15 ± 0.56
73.94 ± 0.11
74.14 ± 0.19

61.54 ± 2.65
62.37 ± 1.41
63.41 ± 1.50
62.17 ± 1.23
58.41 ± 1.95
65.42 ± 0.77
66.10 ± 0.86
66.48 ± 0.69
72.15 ± 0.37

Iris data

10.40%
9.82%
9.72%
9.12%
12.69%
11.10%
11.94%
10.86%
2.37%

Protein data
7.11%
8.31%
6.93%
8.69%
6.44%
10.96%
8.39%
10.09%
2.68%

learning, because W is positive semi-definite. Therefore,
when r is too small, the input data are projected into a very
low-dimensional subspace, which might not have sufficient
representative capability to properly express the clustering
structures of the input data and lead to inferior clustering
performances.

implement these compared methods following their original papers, and fine tune their parameters to achieve the
best clustering accuracy in independent preliminary experiments. Again, once the distance metric is learned by a
method on a data set, K-means clustering is performed on
the same data set using the learned distance metric.

Based upon the above observations, we tentatively draw the
following conclusion. As long as r is not too small, we
can generally achieve decent clustering performances using
the learned distance metrics. Empirically, we select r =
min (d, 2c) in all our subsequent experiments, where d is
the dimensionality of the original data space and c is the
cluster number of the input data. The values of r for the five
experimental data sets are listed in the last row of Table 1.

We conduct experiments in following two conditions: (1)
original data and (2) noisy data with outlier samples. To
emulate the outlier data samples, given the input data set
X = [x1 , . . . , xn ] ∈ Rd×n , we corrupt it by a noise matrix X̃ ∈ Rd×n whose element are i.i.d. standard Gaussian
variables. Then we carry out the same learning and clustering procedures on X + σ X̃ as those on the original data,
kXk
where σ = nf X̃ F and nf is a given noise factor. In all
k kF
our experiments, we set nf = 0.1.

4.3. Clustering on Data with Outlier Samples
We evaluate the proposed method on noisy data with outlier samples using the four UCI data sets. We compare
our method against its two closest counterparts, including
(1) the Euclidean distance (EU) that sets M = I and (2)
the standard Mahalanobis distance (Mah) that sets the distance metric as the inverse of the sample covariance ma−1
trix, i.e. M = (Cov (X)) . We also compare our method
against several related and most recent metric learning
methods, including (3) Xing’s method (Xing et al., 2002),
(4) RCA method (Bar-Hillel et al., 2003), (5) DCA method
(Hoi et al., 2006), (6) Xiang’s method (Xiang et al., 2008),
(7) Large Margin Nearest Neighbor (LMNN) method
(Weinberger et al., 2006) and (8) Information-Theoretic
Metric Learning (ITML) method (Davis et al., 2007). We

For every experimental case, the clustering performance
measured by clustering accuracy are averaged over 100 trials to eliminate the difference caused picking up the constraints and initializing the K-means clustering procedures,
which are reported in Table 2.
From Table 2 we have the following interesting observations. First, our method is consistently better than all
other compared methods on all four experimental data sets,
which demonstrate that our method is able to learn an
effective distance metric that can improve the clustering
performance. Second, although the improvements by our
method over the competing methods on the original data
are mediocre, the improvements by our method on the contaminated data with outlier data samples are considerably

Robust Distance Metric Learning via Simultaneous ℓ1 -Norm Minimization and Maximization
100.0
80.0
60.0
40.0
20.0
0.0

Eu
Clustering accuracy on original
images
Clustering accuracy on occluded
images
Degradation (in percentage)

Mah

Xing's

RCA

DCA

Xiang's LNMM

ITML

Our
method

84.1

87.2

85.0

61.5

85.0

94.7

86.1

88.4

95.3

74.1

76.2

76.1

54.3

78.4

82.3

79.5

81.6

93.7

11.9

12.6

10.5

11.7

7.8

13.1

7.7

7.7

1.7

Figure 2. Clustering performance of the compared methods on
ORL image data set.

large. For example, on the noisy Diabetes data set, our new
distance metric learning method improves the clustering
performance over the simplest Euclidean distance method
by 21.05% = (60.67 − 50.12)/50.12 and outperforms Xiang’s method (with the best performance on the data set)
by 12.04% = (60.67 − 54.15)/54.15. Finally, by a more
careful examination on the experimental results, we also
notice that the clustering performances for all the methods
are degraded due to introducing outlier data samples, however, as shown in the columns of “Perf. diff.” in Table 2,
the degradations of the proposed method are much less than
those of the other compared methods. The degradations of
our method on all the four data sets are less than 3%. This
important observation clearly demonstrates the robustness
of our method against outlier data samples and empirically
justifies our motivation to use the ℓ1 -norm distance to improve the distance metric learning.
4.4. Clustering on Data with Outlier Features
Because we replace the traditional squared ℓ2 -norm distance by the ℓ1 -norm distance in our learning objective, the
learned distance metric is robust against not only outlier
samples but also outliers features. Thus in this subsection,
we evaluate the robustness against feature outliers of the
proposed method on the ORL image data. The ORL data
set includes 40 distinct individuals and each individual has
10 gray images with different expressions and facial details. The size of each image in this data set is 112 × 92.
Besides performing clustering on the original ORL data
following the same procedures as in the previous subsection, we also emulate corrupted features by occluding the
images. For each image, we first randomly pick up a location and place a black square of size 25×25 onto the image.
Then we perform clustering on the corrupted images. We
still compare our method against the 8 competing methods
using the same experimental settings as described in the
previous subsection. We repeat each test case for 100 times
and report the average clustering accuracy in Figure 2.
Figure 2 shows that our method is superior to all other competing methods on both the original images and the images
with occlusions. Moreover, on the occluded images where

300.0
250.0
200.0
150.0
100.0
50.0
0.0

Runningtime(second)

Xing's

RCA

DCA

Xiang's

LNMM

ITML

264.1

45.1

55.2

51.5

121.4

133.7

Our
method
14.5

Figure 3. Running time of the compared methods to learn the distance metric matrix on the ORL image data set.

the features are contaminated, compared to other methods,
the performance degradations of our new method is very
small, which provide one more concrete evidence to support the usefulness of the ℓ1 -norm distance in metric learning and confirm the correctness of the proposed method.
4.5. Computational Efficiency of Our Method
Finally, we evaluate the computational efficiency of the
proposed method, because, as analyzed earlier, it is one of
the most important advantage of the proposed method. We
report in Figure 3 the running time of the compared methods on the ORL image data set, where we experiment on
a Dell PowerEdge 2900 server, which has two quad-core
Intel Xeon 5300 CPUs at 3.0 GHz and 48G bytes memory.
Because the Euclidean distance (EU) method and the Mahalanobis distance (Mah) do not learn the distance metric
matrices, they are not involved in this experiment. From
Figure 3 we can see that our new metric learning method
requires significantly less time to learn the distance metric
matrix, which firmly demonstrates the computational advantage of the proposed method.

5. Conclusions
We proposed a robust distance metric learning method using the ℓ1 -norm distances, which formulated a simultaneous ℓ1 -norm minimization and maximization (minmax)
problem. The new objective uses the ℓ1 -norm between
both data points and features, thus our method is more robust to outliers. However, the new objective is much more
challenging to optimize, to solve which we derived an efficient algorithm and rigorously proved its convergence.
We have performed extensive experiments on both noiseless and noisy data, which have shown that the proposed
methods are more effective and more robust against outlier
samples and outlier features than traditional methods.

Acknowledgments
Corresponding Author: Heng Huang (heng@uta.edu).
This work was partially supported by US NSF IIS1117965, IIS-1302675, IIS-1344152.

Robust Distance Metric Learning via Simultaneous ℓ1 -Norm Minimization and Maximization

References
Argyriou, A., Evgeniou, T., and Pontil, M. Convex multitask feature learning. Machine Learning, 73(3):243–
272, 2008.
Bar-Hillel, A., Hertz, T., Shental, N., and Weinshall, D.
Learning distance functions using equivalence relations.
In ICML, 2003.
Cayton, L. and Dasgupta, S. Robust euclidean embedding.
In ICML, pp. 169–176, 2006.
Davis, J.V., Kulis, B., Jain, P., Sra, S., and Dhillon, I.S.
Information-theoretic metric learning. In Proceedings of
the 24th international conference on Machine learning,
pp. 209–216. ACM, 2007.
Ding, C., Zhou, D., He, X., and Zha, H. R1-pca: rotational
invariant l 1-norm principal component analysis for robust subspace factorization. In ICML, 2006.

Wang, Hua, Nie, Feiping, Huang, Heng, Risacher, Shannon, Saykin, Andrew J, and Shen, Li. Identifying adsensitive and cognition-relevant imaging biomarkers via
joint classification and regression. In Medical Image
Computing and Computer-Assisted Intervention (MICCAI 2011), pp. 115–123. Springer, 2011.
Wang, Hua, Nie, Feiping, Huang, Heng, Yan, Jingwen,
Kim, Sungeun, Risacher, Shannon L, Saykin, Andrew J,
and Shen, Li. High-order multi-task feature learning to
identify longitudinal phenotypic markers for alzheimer’s
disease progression prediction. In NIPS, pp. 1286–1294,
2012.
Wang, Hua, Nie, Feiping, and Huang, Heng.
SemiSupervised Robust Dictionary Learning via Efficient
ℓ2,0+ -Norms Minimization . In Proceedings of the
14th IEEE International Conference on Computer Vision
(ICCV 2013), pp. 1145–1152, 2013a.

Gao, J. Robust l1 principal component analysis and its
bayesian variational inference. Neural Computation, 20:
555–572, 2008.

Wang, Hua, Nie, Feiping, and Huang, Heng. Robust and
discriminative self-taught learning. In Proceedings of
The 30th International Conference on Machine Learning
(ICML 2013), pp. 298–306, 2013b.

Hoi, S.C.H., Liu, W., Lyu, M.R., and Ma, W.Y. Learning
distance metrics with contextual constraints for image
retrieval. In CVPR, 2006.

Weinberger, K.Q., Blitzer, J., and Saul, L.K. Distance metric learning for large margin nearest neighbor classification. In NIPS. Citeseer, 2006.

Jia, Y., Nie, F., and Zhang, C. Trace ratio problem revisited.
IEEE Transactions on Neural Networks (TNN), 20(4):
729–735, 2009.

Wright, John, Ganesh, Arvind, Rao, Shankar, Peng, Yigang, and Ma, Yi. Robust principal component analysis:
Exact recovery of corrupted. Advances in Neural Information Processing Systems, pp. 116, 2009.

Ke, Q. and Kanade, T. Robust l1 norm factorization in
the presence of outliers and missing data by alternative
convex programming. IEEE Conf. Computer Vision and
Pattern Recognition, pp. 592–599, 2004.

Xiang, S., Nie, F., and Zhang, C. Learning a mahalanobis distance metric for data clustering and classification. Pattern Recognition, 41(12):3600–3612, 2008.

Kwak, N. Principal component analysis based on l1-norm
maximization. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 30:1672–1680, 2008.
Nie, F., Huang, H., Cai, X., and Ding, C. Efficient and Robust Feature Selection via Joint l2,1-Norms Minimization. In NIPS, 2010.
Nie, Feiping, Huang, Heng, Ding, Chris, Luo, Dijun, and
Wang, Hua. Robust principal component analysis with
non-greedy ℓ1 -norm maximization. In Proceedings of
the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Two, pp. 1433–1438.
AAAI Press, 2011.
Wang, H., Nie, F., and Huang, H. Globally and Locally
Consistent Unsupervised Projection. In Proceedings
of the 28th AAAI Conference on Artificial Intelligence
(AAAI 2014), 2014.

Xing, E.P., Ng, A.Y., Jordan, M.I., and Russell, S. Distance
metric learning, with application to clustering with sideinformation. In NIPS, 2002.
Zha, Z.J., Mei, T., Wang, M., Wang, Z., and Hua, X.S. Robust distance metric learning with auxiliary knowledge.
In IJCAI, pp. 1327–1332, 2009.

