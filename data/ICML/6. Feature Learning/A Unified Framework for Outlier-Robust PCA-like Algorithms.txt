A Unified Framework for Outlier-Robust PCA-like Algorithms

Wenzhuo Yang
A 0096049@ NUS . EDU . SG
Department of Mechanical Engineering, National University of Singapore, Singapore 117576
Huan Xu
Department of Mechanical Engineering, National University of Singapore, Singapore 117576

Abstract
We propose a unified framework for making a
wide range of PCA-like algorithms – including
the standard PCA, sparse PCA and non-negative
sparse PCA, etc. – robust when facing a constant
fraction of arbitrarily corrupted outliers. Our
analysis establishes solid performance guarantees of the proposed framework: its estimation
error is upper bounded by a term depending on
the intrinsic parameters of the data model, the
selected PCA-like algorithm and the fraction of
outliers. Our experiments on synthetic and realworld datasets demonstrate that the outlier-robust
PCA-like algorithms derived from our framework have outstanding performance.

1. Introduction
Principal component analysis (PCA) (Pearson, 1901),
arguably the most widely applied dimension reduction
method, plays a significant role in data analysis in a broad
range of areas including machine learning, statistics, finance, biostatistics and many others. The standard PCA
performs the spectral decomposition of the sample covariance matrix, selects the eigenvectors corresponding to the
largest eigenvalues, and then constructs a low dimensional
subspace based on the selected eigenvectors. It is well
known that standard PCA, depending on different applications, may suffer from three weaknesses (Montanari &
Richard, 2014; Xu et al., 2013; Johnstone & Lu, 2009): 1)
PCA is notoriously fragile to outliers – indeed, its performance can significantly degrade in the presence of even few
corrupted samples, due to the quadratic error criterion used;
2) PCA cannot utilize additional information of the principal components: e.g., in certain applications, it is known
that the principal components should lie in the positive orProceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

MPEXUH @ NUS . EDU . SG

thant; 3) its output may lack interpretability since it does
not encourage sparse solutions.
Many efforts have been made to mitigate these weaknesses
of PCA. In recent years, numerous robust PCA algorithms
have been proposed to address the first issue (Devlin et al.,
1981; Xu & Yuille, 1995; Yang & Wang, 1999; la Torre &
Black, 2003; Dasgupta, 2003; Xu et al., 2013; Feng et al.,
2012). Among them, Xu et al. (2013) successfully tackles
the case where a constant fraction of samples are corrupted
in the high dimensional regime. Their proposed method,
termed HR-PCA (which stands for High-dimensional Robust PCA), is tractable, easily kernelizable, and is able to
robustly estimate the principal components even in the face
of a constant fraction of outliers and very low signal-tonoise ratio. To overcome the computational issue of HRPCA, Feng et al. (Feng et al., 2012) proposed a deterministic approach (DHR-PCA) that dramatically reduces the
computational work. However, neither HR-PCA nor DHRPCA deals with the last two weaknesses mentioned above.
To address the second weakness, Montanari & Richard
(2014) recently proposed a new algorithm called nonnegative PCA which handles the case that the principal
components are known to lie in the positive orthant, and
showed that near-optimal non-negative principal components can be extracted in nearly linear time. But similar
to the standard PCA, this algorithm is sensitive to outliers.
Indeed, the estimated principal components can be far from
the true ones in the face of even few outliers.
To address the third weakness, previous works focus on a
class of methods called sparse PCA that adapt the standard
PCA so that only a few of attributes of the resulting principle components are non-zero (e.g., Vu et al., 2013; Zou
et al., 2006; Shen & Huang, 2008; Journee & Y. Nesterov,
2008; Birnbaum et al., 2013; Vu & Lei, 2013; d’Aspremont
et al., 2007; Thiao et al., 2010). Some of these methods
are based on non-convex optimization formulations (Jolliffe et al., 2003; Moghaddam et al., 2005) while others use
ℓ1 -norm regularization (Zou et al., 2006). Recently, Vu et
al. (Vu et al., 2013) proposed FPS – a convex relaxation for-

A Unified Framework for Outlier-Robust PCA-like Algorithms

mulation of sparse principal subspace estimation based on a
semi-definite program with a Fantope constraint and established theoretical guarantees in the outlier-free regime. Yet,
one severe drawback of most sparse PCA algorithms is that
the output can be sensitive to the existence of even few outliers. This is clearly undesirable, as in real-world applications, the existence of outliers is ubiquitous. Recently, several robust sparse PCA have been proposed (Croux et al.,
2013; Wang & Cheng, 2012; Hubert et al., 2014) to handle
outliers, but all of them are only evaluated by experiments
and have no theoretical performance guarantees.
This paper is the first attempt to theoretically address these
issues of PCA simultaneously. In specific, we propose a
general framework for a wide range of PCA-like algorithms
to make them provably robust to a constant fraction of arbitrary outliers. Our framework is inspired by HR-PCA
(Xu et al., 2013; Feng et al., 2012), but overcomes the
drawbacks of HR-PCA and has the capability of converting a non-robust PCA-like algorithm such as non-negative
PCA (Montanari & Richard, 2014), sparse PCA (Vu et al.,
2013; Papailiopoulos et al., 2013) or non-negative sparse
PCA (Asteris et al., 2014), into its outlier-robust variant.
The analysis of our proposed framework is novel and different from that of HR-PCA. We analyze its performance
using two performance metrics: the subspace distance and
the expressed variance. We show that the subspace distance between its estimated principal components and the
ground-truth under the spiked model can be upper bounded
by a term depending on the parameters of the spike model,
the selected PCA-like algorithm and the fraction of outliers. The analysis of subspace distance in the presence
of outliers is new to the best of our knowledge. Moreover,
while the analysis of expressed variance for HR-PCA exists
in literature, our analysis of the expressed variance of this
framework is more general, in that it shows that maximal
robustness can be achieved for a wide range of PCA-like algorithms besides HR-PCA. Our numerical experiments results show that when outliers exist, the outlier-robust PCAlike algorithms developed from our framework outperform
their non-robust counterparts considerably.
Notation. We use lower-case boldface letters to denote column vectors and upper-case boldface letters to denote matrices. In this paper, ∥M∥2 is the spectral norm, ∥M∥∗
is the nuclear norm, ∥M∥1 is the element-wise ℓ1 norm,
∥M∥∞ is the element-wise infinity norm, and ∥M∥F is
the Frobenius norm. We use ∥M∥0 to denote the number of non-zero entries in M, and use subscript (·) to represent order statistics of a random variable. For example, let v1 , · · · , vn ∈ R, then v(1) , · · · , v(n) is a permutation of v1 , · · · , vn in a non-decreasing order. For matrix X, the first k singular values of X are denoted by
λ1 (X), · · · , λk (X).

2. Algorithms
In this section, we present our framework for outlier-robust
PCA-like algorithms. We first describe the problem setup
and necessary assumptions, and then show the details of the
algorithm along with the key intuition underlying it.
2.1. Problem Setup
Suppose there are n samples Y = {y1 , · · · , yn ∈ Rp }
which consist of t authentic samples z1 , · · · , zt ∈ Rp and
n − t outliers o1 , · · · , on−t ∈ Rp . The outliers are arbitrary. We denote the fraction of outliers by ρ = (n − t)/n
and assume that ρ < 0.5. The authentic samples zi are generated according to zi = Axi + ni where xi ∈ Rd are i.i.d.
samples of a random vector x with mean 0 and variance Id
and ni are independent realizations of standard Gaussian
N (0, Ip ). The matrix A ∈ Rp×d and the distribution of
x (denoted by ν) are unknown. The covariance of z is denoted by Σ. Since z = Ax+n, Σ = E[zz⊤ ] = AA⊤ +Ip .
We denote the one-dimensional marginal of ν along direction v ∈ Sd by ν̄v , and assume that ν̄v ({0}) < 0.5 for
all v ∈ Sd and it is sub-Gaussian, i.e., there exists θ > 0
such that ν̄v ((−∞, x] ∪ [x, +∞)) ≤ exp(1 − x2 /θ) for
all x > 0. Clearly, both assumptions are satisfied if ν is
Gaussian.
We make the following two assumptions: 1) A is full row
rank and n > d. This essentially means the intrinsic dimension of the authentic samples (ignoring the noise) is indeed
d. 2) The projection Π(k) = U(k)U(k)⊤ onto the subspace spanned by the eigenvectors U(k) of Σ corresponding to its k largest eigenvalues satisfies ∥Π(k)∥0 ≤ β 2 ,
where ∥Π(k)∥0 is the number of nonzero entries of Π(k).
Our goal is to approximately recover Π(k) even though
the samples contain a non-negligible fraction of arbitrary
outliers. For convenience, we let Π ≜ Π(k) in the following sections. Throughout the paper, “with high probability”
means with probability at least 1 − c max{p−10 , n−10 } for
some constant c.
2.2. General Formulation of PCA-like Algorithms
Many kinds of PCA-like algorithms have been proposed
in recent decades, e.g., sparse PCA (Zou et al., 2006; Papailiopoulos et al., 2013), non-negative PCA (Montanari &
Richard, 2014), etc., which play a significant role in machine learning, computer vision, statistics and data analysis. In this section, we consider a general formulation as
shown below for a wide range of these algorithms:
max
X∈C

⟨Σ̂, X⟩ − µ∥X∥1 ,

(1)

where Σ̂ is the empirical sample covariance matrix, C includes the constraints imposed on X, and µ is the weight of
the regularization term. Typically, µ is less than a certain

A Unified Framework for Outlier-Robust PCA-like Algorithms

universal constant. To see that this formulation can model
most PCA-like algorithms proposed in literature, let k be
the number of the principal components one wants to extract and F(k) be the set {X : 0 ⪯ X ⪯ Ip , tr(X) = k}
which includes the matrices that lie in the convex hull of
all feasible projection matrices. Thus, the following algorithms are all equivalent to Formulation (1) for appropriate
k, C and µ:
1. Standard PCA (Pearson, 1901): k = d, C = F(k) and
µ = 0;
2. Non-negative PCA (Montanari & Richard, 2014): k =
1, C = {uu⊤ : ∥u∥2 ≤ 1, u ≥ 0} and µ = 0;
3. Sparse PCA (Papailiopoulos et al., 2013): k = 1, C =
{uu⊤ : ∥u∥0 ≤ γ, ∥u∥2 ≤ 1} and µ = 0;
4. Fantope projection and selection√
(FPS) (Vu et al.,
2013): k = d, C = F(k) and µ ≍ logn p ;
5. Non-negative sparse PCA (Asteris et al., 2014): k =
1, C = {uu⊤ : ∥u∥0 ≤ γ, ∥u∥2 ≤ 1, u ≥ 0} and
µ = 0;
6. Large-scale sparse PCA (Zhang & El Ghaoui, 2011):
k = 1, C = {X : X ⪰ 0, tr(X) = 1}, µ > 0.
Since the feasible set C in (1) may be non-convex, the
global optimum of (1) may not be achievable. Therefore,
there are two important issues: 1) whether a PCA-like algorithm can probably find an optimal or near-optimal solution of (1), and 2) whether its solution converges to the
ground truth. We call the PCA-like algorithms that can find
optimal or near-optimal solutions of (1) “workable” algorithms, formally defined as:
Definition 1. A PCA-like algorithm is (η, γ)-workable if
there exist positive numbers η ≤ 1 and γ ≤ p such that
with high probability its output X̂ satisfies ∥X̂∥0 ≤ γ 2 and
[
]
⟨Σ̂, X̂⟩ − µ∥X̂∥1 ≥ (1 − η) ⟨Σ̂, Π⟩ − µ∥Π∥1 .
Note that η indicates the accuracy of the solution X̂, e.g.,
η = 0 means X̂ is optimal, while η = 0.5 means the cost
value corresponding to X̂ is half of the optimum. Parameter γ bounds the sparsity of X̂. For the first five algorithms
mentioned above, previous works have proved that all of
these algorithms are workable. In particular, η = 0, γ = p
for standard PCA and FPS, 0 < η < 1, γ = p for nonnegative PCA, and 0 < η < 1, γ ≪ p for sparse PCA
and non-negative sparse PCA. For large-scale sparse PCA,
no performance guarantees are known, but our experiments
show that this algorithm can still be put into our framework
to achieve robustness.

2.3. Outlier-Robust PCA-like Algorithm
Our framework is inspired by HR-PCA (Xu et al., 2013).
Therefore, before presenting its details, we briefly explain
the intuition behind HR-PCA. HR-PCA iteratively performs PCA to compute principal components (PCs) and
then randomly removes one point with a probability proportional to its magnitude after projected on the found PCs.
HR-PCA works for the following intuitive reasons. In
each iteration, a PC is computed either due to true samples which implies it is a “good” direction; or due to large
outliers in which case the random removal scheme will remove an outlier with high probability. Thus, for at least one
iteration, the algorithm will find a good direction, say wt .
Among all the directions found in the algorithm, the final
output of HR-PCA is the one with the largest Robust Variance Estimator (RVE). RVE measures the projection variance of the (n− t̂)-smallest points: A large RVE means that
that many of the points have a large variance in this direction, while a small RVE indicates otherwise. This makes
sure that the final output is close to wt , and hence a good
direction. A variant of HR-PCA is called deterministic HRPCA or DHR-PCA (Feng et al., 2012). Instead of removing
one point, DHR-PCA decreases the weights of all samples
according to their magnitudes after projected on the found
PCs in each iteration to reduce computational cost.
HR-PCA and DHR-PCA only focus on making standard
PCA robust to outliers but say nothing about whether it is
possible to improve the robustness of non-negative PCA
or sparse PCA. In this paper, we propose a more general
framework as shown in Algorithm 1 for developing outlierrobust PCA-like algorithms. In Algorithm 1, the weighted
covariance matrix acts as a robust covariance estimator
(Rousseeuw, 1985; Rousseeuw & Driessen, 1998; Croux &
Haesbroeck, 2000), and V t̂ (X) is the Robust Variance Es∑t̂
timator which is defined as V t̂ (X) ≜ 1t̂ i=1 ⟨yy⊤ , X⟩(i) ,
where y ∈ Y = {y1 , · · · , yn }. Intuitively, the term
⟨yy⊤ , X⟩ imitates the magnitude of y after it is projected
on the column subspace of X, so this RVE measures the
projection variance similar to the one in HR-PCA. As we
show below, a PCA-like algorithm becomes outlier-robust
if it is integrated into this general robustness framework.
For example, DHR-PCA can be easily deduced from this
framework by solving the standard PCA in Step 3.

3. Theoretical Guarantees
We now present the performance guarantees of Algorithm
1 with a (η, γ)-workable PCA-like algorithm. Typically,
there are two ways to measure the performance of PCA-like
algorithms (Xu et al., 2013; Vu et al., 2013). The first one,
termed the subspace distance (S.D.), measures the distance
between the subspace spanned by the estimated PCs and
the subspace spanned by the true PCs. The second one,

A Unified Framework for Outlier-Robust PCA-like Algorithms

Algorithm 1 Outlier-Robust PCA-like Algorithm
Input: Contaminated sample-set Y = {y1 , · · · , yn }, k,
T , t̂, µ.
Procedure:
1) Initialize: s = 0, Opt = 0; ŷi = yi and αi = 1 for
i = 1, · · · , n;
while s ≤ T do
2) Compute the weighted empirical covariance matrix
∑n
Σ̂ = n1 i=1 αi ŷi ŷi⊤ ;
3) Solve the PCA-like problem 1 and denote the output by X̂;
4) If V t̂ (X̂) > Opt, let Opt = V t̂ (X̂) and X∗ = X̂,
∑t̂
where V t̂ (X̂) ≜ 1t̂ i=1 ⟨yy⊤ , X̂⟩(i) ;
5) Update αi = (1 −

⟨yi yi⊤ ,X̂⟩
)αi ;
max{i|αi ̸=0} ⟨yi yi⊤ ,X̂⟩

6) s = s + 1;
end while
7) Perform SVD on X∗ and denote the top k eigenvectors by w1∗ , · · · , wk∗ ;
8) return w1∗ , · · · , wk∗ and X∗ .
termed the expressed variance (E.V.), measures the portion
of the signal Ax being expressed by the estimated principle
components. Formally, we have:
Definition 2. Let M1 , M2 be two symmetric matrices
and M1 , M1 be their respective k-dimensional principal subspaces, then the subspace distance is S.D. ≜
sin Θ(M1 , M1 ).
Definition 3. The
expressed variance of w1 , · · · , wk is de∑k
w⊤ AA⊤ w
i=1
fined as E.V. ≜ ∑k λi (AA⊤ ) i .
i=1

i

Notice that a smaller S.D. or a larger E.V. indicates a more
desirable solution. Also, S.D. ≥ 0 and E.V. ≤ 1 with equality achieved when the vectors w1 , · · · , wk span the same
space as the true PCs. Thus, to provide performance guarantees of the proposed algorithms, we lower bound the expressed variance as well as upper bound the subspace distance for the output. This is different from (Xu et al., 2013)
and (Feng et al., 2012) which only analyzed the expressed
variance (of HR-PCA and DHR-PCA respectively).
To analyze the performance of Algorithm 1, the following
“tail weight” function the first appeared in (Xu et al., 2013)
is required.
Definition 4. ((Xu et al., 2013)) For any γ ∈ [0, 1] and
v ∈ Sd , let δγ ≜ min{δ ≥ 0|ν̄v ([−δ, δ]) ≥ γ} and γν− =
ν̄v ((−δ, δ)). Then the “tail weight” functions Vv is defined
as follows:
∫ δγ −ϵ
Vv (γ) ≜ lim
x2 ν̄v (dx) + (γ − γν− )δr2 .
ϵ↓0

−δγ +ϵ

We define V + (γ) ≜ supv∈Sd Vv (γ) and V − (γ) ≜

inf v∈Sd Vv (γ). In the following subsections, we assume
that the feasible set C in (1) is a subset of F(k) – the convex hull of all the feasible projection matrices. This is not
a restrictive condition. Indeed all the algorithms listed in
Section 2.2 except large-scale sparse PCA meet this condition.
3.1. Upper Bound of Subspace Distance
We first bound the subspace distance for Algorithm 1. The
following lemma relates the subspace distance with the
Frobenius norm of X∗ − Π so that we only need to bound
∥X∗ − Π∥F .
Lemma 1. (Vu et al., 2013) If M is the principal ddimensional subspace of Σ and M∗ is the principal kdimensional subspace of X∗ , then
√
sin Θ(M, M∗ ) ≤ 2∥X∗ − Π∥F .
In the following parts, we let δk (AA⊤ ) ≜ λk (A)2 −
λk+1 (A)2 and let
}
{
f (B) = min 2B∥A∥22 + c1 τ, γB∥A∥22 + c2 γ(d∥A∥2 + 1) ,
where τ = max{ np , 1} and c is a universal constant. Notice
that f (B) is upper bounded by 2B∥A∥22 + c1 when p =
O(n) and by γB∥A∥22 + c2 γ(d∥A∥2 + 1) when p = Ω(n)
and γ ≪ p for some constants c1 and c2 . Therefore, in
the high dimensional case where p ≫ n, when sparse PCA
algorithms are applied, i.e., γ ≪ p, f (B) can still be small,
compared with np .
We now provide our first main theorem which states that
the output X̂ in Step 3 will be close to the true projection
matrix after a certain number of iterations.
Theorem 1. Suppose that ρ < 0.5 and log p ≤ n, then
there exists a finite number s ≤ n such that the output
Xs of the PCA-like algorithm in the sth stage satisfies the
following inequality with high probability,
√
∥Xs − Π∥F ≤ R(µ) + k min

√

1≥κ>2ρ

f (B1 ) + ηβB0
,
δk (AA⊤ )
(2)

where
R(µ) ≜





8(γ[ϵ0 (∥A∥22 +1)−µ]+ +µβ)
,
{ δk (AA⊤ )


 min
√

log p
n ,

√

8ϵ0 γ(∥A∥22 +1)
,2
δk (AA⊤ )

√p

ϵ1 k(∥A∥22 +1)
δk (AA⊤ )

µ=
̸ 0

}
,

µ = 0,

B0 = c2 (∥A∥22 + 1),
)1
(
3
ρ
n 4
B1 = κ + 1 − V − (1 − κ(1−ρ)
) + ϵ0 + c3 d log
,
n
and c0 , c1 , c2 , c3 are universal constants.
ϵ0 = c0

ϵ1 = c1

n,

A Unified Framework for Outlier-Robust PCA-like Algorithms

Remark. The upper bound of ∥Xs − Π∥F involves three
terms: 1) R(µ): R(µ) is related to the weight of the regularization term in (1). A positive µ can encourage sparse
solutions. From the formulation of R(µ), we know that setting µ to ϵ0 (∥A∥22 +1) when µ is non-zero leads to a tighter
bound. 2) f (B1 ): B1 involves ρ – the fraction of outliers,
and decreases when ρ decreases. Clearly, B1 → 0 when
3
n
and logn p converge to zero. 3) ηβB0 : This term
ρ, d log
n
contains η, i.e., the accuracy of the selected PCA-like algorithm. When the optimal solution of (1) can be achieved,
this term becomes zero.
Theorem 1 tells us that a good solution Xs can be generated for some iteration s. However, such s is not specified.
Thus, one can not take Xs as the output; instead, one can
choose a solution that is close to Xs as the output. In Algorithm 1, the solution with the maximal RVE is selected
as the final output X∗ . Other methods can also be applied
in practical applications based on specific information. The
following theorem provides the estimation error of X∗ .
Theorem 2. Suppose that ρ < 0.5 and log p ≤ n, the
following holds with high probability,
√

2 [(dB2 + kB4 )λ1 (A)2 + kf (B3 )]
,
δk (AA⊤ )
(3)
where B2 is the right hand side of (2),
∥X∗ − Π∥F ≤

t̂ − ρn
t̂
) + c0
B3 = 2 − V ( ) − V − (
t
t
−

B4 = min{c1
sal constants.

√

√p

n , c2 γ

log p
n },

(

d log3 n
n

) 14

γ = p, and

γ 2 log p
n

and c0 , c1 , c2 are univer-

log p
n → 0. To achieve consistency,
p
n → 0 for the standard PCA where

→ 0 for sparse PCA where γ ≪ p.

The following corollaries provide more interpretable
bounds of the subspace distance for the standard PCA, FPS
and sparse PCA discussed in Section 2.2.
Corollary 1. Suppose that ρ < 0.5 and log p ≤ n, then
when the PCA-like algorithm is the standard PCA (Pearson, 1901), the following holds with high probability,
√
∥X∗ − Π∥F ≤

√

B2 = 2

ϵd(λ1 (A)2 + 1)
+ min
1≥κ>2ρ
λd (A)2

√

2dB1 λ1 (A)2 + cdτ
,
λd (A)2

B1 is defined
in Theorem 1, B3 is defined in Theorem 2,
√
ϵ = c1 np , τ = max{ np , 1} and c, c0 , c1 are universal
constants.
The standard PCA imposes no constraint on the sparsity of
its solution, so when the ambient dimension p grows faster
than the sample number n, the bound in Corollary 1 will
go to infinity. One way to encourage sparsity is to impose
a “soft” constraint which upper bounds the l1 -norm of the
solution, e.g., FPS.
Corollary 2. Suppose that ρ < 0.5 and log p ≤ n, then
when the PCA-like algorithm is FPS (Vu et al., 2013), the
following holds with high probability,
√
4d(B2 + B3 + ϵ1 )λ1 (A)2 + cdτ
∗
∥X − Π∥F ≤
,
λd (A)2
(5)
where
√
ϵ0 (λ1 (A)2 + 1)β
2dB1 λ1 (A)2 + cdτ
B2 =
+
min
,
1≥κ>2ρ
λd (A)2
λd (A)2
B1 is defined
in Theorem 1, B3 is defined in Theorem 2,
√
√p
p
log p
ϵ0 = c0
n , ϵ1 = c1
n , τ = max{ n , 1} and c, c0 , c1
are universal constants.
2

,

Remark. This upper bound contains three terms: 1) B2
is the upper bound of ∥Xs − Π∥F as shown in Theorem
1. 2) B3 involves ρ and parameter t̂, which becomes small
when ρ decreases and
√t̂ approaches t. 3) B4 converges to
zero as np → 0 or γ
one should ensure that

where

4d(B2 + B3 + ϵ)λ1 (A)2 + cdτ
, (4)
λd (A)2

Notice that p cannot grow faster than nλdd(A) due to the
existence of outliers, but the first term in B2 in Corollary
2 involves logn p instead of np , which is much smaller than
that in Corollary 1. Thus, the soft constraint is helpful if the
true solution is indeed sparse. When the selected PCA-like
algorithm has a “hard” constraint on the sparsity, e.g., the
ones proposed by (Papailiopoulos et al., 2013) and (Asteris
et al., 2014), p can grow much faster than n.
Corollary 3. Suppose that ρ < 0.5 and log p ≤ n,
then when γ ≥ β and the PCA-like algorithm is sparse
PCA (Papailiopoulos et al., 2013) or non-negative sparse
PCA (Asteris et al., 2014), the following holds with high
probability,
∥X∗ − Π∥F ≤
√
2 [(dB2 + βB3 + βϵ0 )λ1 (A)2 + cβ(dλ1 (A) + 1)]
,
δ1 (AA⊤ )
(6)
where
ϵ0 (λ1 (A)2 + 1)β
+
δ1 (AA⊤ )
√
B1 λ1 (A)2 + c(dλ1 (A) + 1) + ηB0
√
.
γ min
1≥κ>2ρ
δ1 (AA⊤ )

B2 =

A Unified Framework for Outlier-Robust PCA-like Algorithms

B0 , B1 are√defined in Theorem 1, B3 is defined in Theorem
log p
n ,

2, ϵ0 = c0

and c, c0 are universal constants.

Recall that Σ = AA⊤ +Ip . The bound shown in Corollary
3 can be finite regardless
√ of the magnitude of the existing
outliers, e.g., when d, β
λ1 (Σ)
λ1 (Σ)−λ2 (Σ)

log p
n ,

(B1 + B3 + η)γ,

γ
λ1 (A)

Xu et al. (2013) and Feng et al. (2012) provided lower
bounds of E.V. when the standard PCA is selected in Algorithm 1. We now show that E.V. can be bounded from below when other PCA-like algorithm of form (1) (and workable) are used in Algorithm 1. Let H ∗ ≜ ⟨AA⊤ , X∗ ⟩ and
H ≜ ⟨AA⊤ , Π⟩, then we have the following theorem.
Theorem 3. Suppose that ρ < 0.5. For any κ, there exists
a constant c such that the following inequalities hold w.h.p,

E.V ≥

(1 − η)V −

t̂
t

−

ρ
1−ρ

(

V− 1 −
( )

(1 + κ)V +

i↑+∞

log pi
min{pi /ni , γi }
≤ +∞, lim ∑k
↓ 0,
2
i↑+∞
ni
j=1 λj (Ai )

ni
di
↑ +∞, ∑k
↓ 0, µi βi ↓ 0,
3
2
di log ni
j=1 λj (Ai )

3.2. Lower Bound of Expressed Variance

)

ni ↑ +∞, lim

and

are bounded from above.

(

Theorem 4. (Asymptotic Bound): Consider a sequence
of {Yi , di , ni , pi , µi , βi , γi }, where the asymptotic scaling
satisfies

ρ
κ(1−ρ)

)

t̂
t



1
(
) 12 (
3 )4
c
k
min{τ,
γς}
d
log
n


− +
+
V (0.5)
n
H
√
2(1 − η)µβ k
−
− max{1 − λk (X∗ ), λk+1 (X∗ )},
V + ( tt̂ )H
(7)
where τ = max{ np , 1} and ς = max{ logn p , 1}.
As discussed in Section 2.2, X∗ has the form X∗ = uu⊤
for the standard PCA, non-negative PCA (Montanari &
Richard, 2014), sparse PCA (Papailiopoulos et al., 2013)
and non-negative sparse PCA (Asteris et al., 2014), which
implies that the last term in (7) vanishes for these four algorithms when k = 1. But for FPS (Vu et al., 2013), this
term may not be zero. The following lemma shows that it
can converge to zero under certain circumstances.
Lemma 2. Suppose that S is a sequence of matrices such
that for any Sn ∈ S, Sn ∈ Sp×p
and λd (Sn )−λd+1 (Sn ) ≥
+
δ > 0. Let
Xn ≜ arg max ⟨Sn , X⟩ − µn ∥X∥1 ,
X∈F (d)

then if µn → 0 as n → +∞ and pd3/2 = o( µ1n ), we have
λd (Xn ) → 1 and λd+1 (Xn ) → 0 as n ↑ +∞.
The following result shows the asymptotic bound of the expressed variance in which we assume that the last term in
(7) converges to zero as n goes to infinity. This condition
holds for all the algorithms mentioned above.

Let ρ∗ = lim sup ρi ≤ 0.5 and suppose t̂ > 0.5n, then
if λk (X∗ ) → 1 and λk+1 (X∗ ) → 0 as ni ↑ +∞, the
following holds in probability when i ↑ +∞,
lim inf E.V ≥ (1−η) max
i

(
V− 1 −

κ

ρ∗
(1−ρ∗ )κ

)

(
V − t̂t −
( )

(1 + κ)V +

t̂
t

ρ∗
1−ρ∗

)
.

Furthermore, if µ̄v ({0}) = 0 for all v ∈ Sd , then the
breakdown point is ρ∗ = 0.5.
Corollary 4. Under the settings of the above theorem, the
following holds in probability for some constant C when
i ↑ +∞,
[
]
√
V − ( t̂t ) − C θρ∗ log(1/2ρ∗ )
.
lim inf E.V ≥ (1 − η)
i
V + ( t̂t )
3.3. Complexity
Recall that Algorithm 1 is an iterative algorithm that solves
a PCA-like algorithm in each iteration. Theoretically, the
number of iterations required to generate a good solution is
bounded by n. But in practice, one can stop the algorithm
at any time as long as the output of the robust variance estimator is good enough. We will show in the experiments
that 5-10 iterations are sufficient to achieve a good solution. Since the time and space complexity of Algorithm 1
mainly depends on performing the selected PCA-like algorithm, this means the computational cost of Algorithm 1 is
about 5-10 times higher than the non-robust PCA-like algorithm – robustness is not a free lunch, but you don’t pay
much.

4. Experimental Results
In this section, we show that our framework indeed makes
PCA-like algorithms more robust to outliers. We refer to
the selected PCA-like algorithm in Step 3 in Algorithm 1
as A and consider four algorithms induced from our framework: 1) OR-PCA: A is the standard PCA. OR-PCA has
been extensively studied in (Xu et al., 2013). 2) OR-SPCA:
A is FPS (Vu et al., 2013) to encourage sparse solutions.
3) Nonnegative OR-SPCA: A is non-negative sparse PCA
(Asteris et al., 2014). 4) Large-scale OR-SPCA: A is the
algorithm proposed by (Zhang & El Ghaoui, 2011) which

A Unified Framework for Outlier-Robust PCA-like Algorithms
1

is able to handle high dimensional data. Although this algorithm has no performance guarantees, it does work well
in the experiments.

1

Firstly, we illustrate the performance of OR-PCA and ORSPCA via numerical results on synthetic and real data. For
synthetic data, we generate matrix A via the following
three steps: 1) randomly generate sparse orthogonal matrices U ∈ Rp×d and V ∈ Rd×d such that ∥U∥2,0 = β
where ∥U∥2,0 is the number of non-zero rows in U; 2) generate a diagonal matrix S whose diagonal entries are drawn
from (a) the uniform distribution over [1, 2] or (b) the chi−0.5 −0.5x
e
square density x √2Γ(0.5)
where x is chosen from 0.05 to

0.6
0.4

FPS
ROB−SPCA
OR−PCA
OR−SPCA

0.6
0.4
0.2

0.2
0

0

0

0.1

0.2

0.3

0.4

0.5

ρ

0

0.1

0.2

0.3

0.4

0.5

0.3

0.4

0.5

ρ

(a)
0.9
0.8
0.7

FPS
ROB−SPCA
OR−PCA
OR−SPCA

0.6
0.4

FPS
ROB−SPCA
OR−PCA
OR−SPCA

0.6
Sparsity

Expressed Variance

1
0.8

0.5
0.4
0.3

0.2

0.2

0

0.1

0

0.1

0.2

0.3

0.4

0

0.5

0

ρ

0.1

0.2
ρ

(b)

Figure 1. The performance of OR-PCA, OR-SPCA, ROB-SPCA
and FPS under (a) p = 500, n = 300, c = 5 and (b) p =
1000, n = 300, c = 5. The singular values of A are uniformly
drawn from [1, 2].

We make a comparison between OR-PCA, OR-SPCA, FPS
and ROB-SPCA. ROB-SPCA is developed based on (Hubert et al., 2014), which uses ROBPCA (Hubert et al.,
2005) to estimate the robust sample covariance and then
applies FPS to compute the principal components. The
performance is evaluated by the “expressed variance” and
“sparsity”. The sparsity is defined by

1

Expressed Variance

1
0.8

FPS
ROB−SPCA
OR−PCA
OR−SPCA

0.8
0.6

Sparsity

0.05d using step-size 0.05; 3) finally, let A = USV⊤ .
The t authentic samples zi are generated by the function
zi = Axi + ni where xi ∼ N (0, Id ), ni ∼ N (0, σ 2 Ip ).
A ρ fraction outliers oi are generated with a uniform distribution over [−c, c]p where c is a constant.

FPS
ROB−SPCA
OR−PCA
OR−SPCA

0.8

Sparsity

Expressed Variance

0.8

0.4
0.2

FPS
ROB−SPCA
OR−PCA
OR−SPCA

0.6
0.4
0.2

0
0

0.1

0.2

0.3

0.4

0

0.5

ρ

0

0.1

0.2

0.3

0.4

0.5

ρ

(a)
0.8

1

where X is the projection matrix generated by each algorithm.

0.8

0.6

0.6
FPS
ROB−SPCA
OR−PCA
OR−SPCA

0.4
0.2

In the second experiment, we investigate the number of the
iterations required in Algorithm 1 to achieve good performance. We take OR-SPCA as an example. Figure 3 shows

FPS
ROB−SPCA
OR−PCA
OR−SPCA

0.2

0.1

0.2

0.3

0.4

0

0.5

ρ

0

0.1

0.2

0.3

0.4

0.5

ρ

(b)

Figure 2. The performance of OR-PCA, OR-SPCA, ROB-SPCA
and FPS under (a) p = 500, n = 300, c = 5 and (b) p =
1000, n = 300, c = 5. The singular values of A are uniformly
drawn from the chi-square density.

the effect of the number of iterations on the expressed
variance and sparsity for OR-SPCA under three cases that
p = 600, p = 800 and p = 1000, from which we observe
that only 5 iterations are required for OR-SPCA to generate acceptable results in all three cases. Empirically, we
1

1

0.8

0.8

0.6

0.6

Sparsity

Expressed Variance

tively. Parameter µ for FPS and OR-SPCA is 0.2 logn p .
For each parameter setup, we report the average results of
10 tests. Figures 1 and 2 show the performance of these
four algorithms. Clearly, FPS easily breaks down, even
when there exists only a small fraction of outliers. ROBSPCA breaks down when ρ is larger than 0.25. Actually,
most of robust PCA algorithms based on ROBPCA do not
work well when the fraction of outliers exceeds 0.25 (Xu
et al., 2013). One can also observe that OR-PCA and ORSPCA are much more robust than the other two algorithms,
and OR-SPCA can generate more sparse solutions than
OR-PCA without significant decrease in the expressed variance, which implies that our framework has the capability
of converting a non-robust SPCA algorithm, e.g., FPS, into
a robust one.

0.4

0
0

In the first experiment, we compare the performance of
each algorithm when ρ varies while the other parameters
are fixed. The parameters for generating test data are set as
follows: d = 10, σ = 0.05, β = 0.3p. Parameter T and
t̂ for OR-PCA and OR-SPCA are set to 10 and ρn, √
respec-

Sparsity

Expressed Variance

Sparsity ≜ |(i, j) : |Xij | > 0.001|/p2 ,

0.4

p = 600
p = 800
p = 1000

0.2
0
0

5

10
Iterations

15

0.4

p = 600
p = 800
p = 1000

0.2

20

0
0

5

10
Iterations

15

20

Figure 3. The effect of the number of iterations on the expressed
variance and sparsity. n, ρ and c are fixed: n = 300, ρ = 0.1,
c = 5.

A Unified Framework for Outlier-Robust PCA-like Algorithms

observe that 5-10 iterations are enough for Algorithm 1 to
compute good results in practical applications. Hence in
the following experiments on real data, parameter T is set
to 10.
In the third experiment, we show the performance of ORSPCA, OR-PCA and FPS on a real dataset of 600 samples
in which 75% of samples are drawn from MNIST (LeCun
et al., 1995) and 25% of samples are drawn from the CBCL
face image dataset (Sung, 1996). We take the digit images
as the authentic samples and the face images as the outliers.
Each image in this dataset is converted into a vector with
dimension 784. Figure 4 shows the leading ten principal
components extracted by FPS, OR-PCA and OR-SPCA. It
can be observed that OR-SPCA can generate more interpretable results than OR-PCA, i.e., each PC corresponds
to some strokes. Notice that the principal components extracted by OR-SPCA are more reliable than FPS. For example, the third principal component extracted by FPS clearly
mixes digits with faces, which is obviously unreliable.
(a)

(a)

(b)

(c)

(d)

Figure 5. We plot (a) five samples in the dataset, (b) the five
leading PCs extracted by non-negative SPCA on the clean data
(2429 face images), and the five leading PCs extracted by (c) nonnegative SPCA and (d) non-negative OR-SPCA on the dirty data
(2429 face images plus 125 outliers).

Table 1. The words associated with the leading two sparse principal components extracted by large-scale SPCA and large-scale
OR-SPCA. The ground truth is obtained by performing largescale sparse PCA on the clean data.
Ground-truth
1st PC
2st PC
million
point
percent
play
business
team
company season
market
game

LS-SPCA
1st PC
2st PC
site
fire
summer scientist
contract
oil
system
prices
person
district

LS-OR-SPCA
1st PC
2st PC
percent
team
company player
million
season
market
game
money
play

(b)
(c)
(d)

Figure 4. We plot the leading ten PCs extracted by OR-PCA, FPS
and OR-SPCA. (a) shows a couple of sample images. (b), (c)
and (d) show the results of OR-PCA, FPS and OR-SPCA, respectively.

Secondly, we evaluate the performance of the non-negative
OR-SPCA on the real world dataset constructing by mixing 2429 images in the CBCL face image dataset with 125
digit images randomly drawn from the MNIST dataset. We
take the face images as the authentic samples and the digit
images as the outliers. Each image in this dataset is converted into a vector with dimension 361. We compare
non-negative OR-SPCA with non-negative SPCA. Figure
5 shows the sample images and the five leading PCs computed by non-negative SPCA and non-negative OR-SPCA.
Clearly, non-negative SPCA fails in the face of these
“digit” outliers, while non-negative OR-SPCA can still extract good principal components that are close to the ones
generated by applying non-negative SPCA on the clean
data, i.e., 2429 face images only.
Finally, we use the NYTimes news article dataset from
the UCI Machine Learning Repository (Frank & Asuncion,
2010), which contains 300000 articles and a dictionary of
102660 unique words, to illustrate the performance of Algorithm 1 on large-scale data. 3000 random vectors whose

entries are randomly drawn from the uniform distribution
with support [0, 100] are added into the NYTimes dataset,
which are taken as outliers. We choose large-scale SPCA
(LS-SPCA) proposed by (Zhang & El Ghaoui, 2011) as A
and compare the corresponding large-scale OR-SPCA (LSOR-SPCA) with it. Table 1 provides the leading two sparse
PCs in which the first two columns shows the two leading
PCs extracted by LS-SPCA on the dataset without outliers,
and the other four columns presents the leading PCs extracted by LS-SPCA and LS-OR-SPCA on the dataset with
outliers. Clearly, the results of LS-SPCA are meaningless
when outliers exist, whereas LS-OR-SPCA can generate
quite similar results to the ground-truth where the first PC
is about business and the second PC is about sports.

5. Conclusion
In this paper, we proposed a unified framework for making
PCA-like algorithms robust to outliers. We provided theoretical performance analysis of the proposed framework
using both the subspace distance and the expressed variance metrics. To the best of our knowledge, this is the first
attempt to make a wide range of PCA-like algorithms provably robust to any constant fraction of arbitrarily corrupted
samples. As an immediate result, our framework leads to
robust sparse PCA and robust non-negative sparse PCA
with theoretic guarantees – the first of its kind to the best
of our knowledge. The experiments show that the outlierrobust PCA-like algorithms derived from our framework
outperforms their non-robust version and other alternatives
including HR-PCA and ROB-SPCA.

A Unified Framework for Outlier-Robust PCA-like Algorithms

Acknowledgments
This work is partially supported by the Ministry of Education of Singapore AcRF Tier Two grants R265000443112
and R265000519112, and A*STAR Public Sector Funding
R265000540305.

Jolliffe, I. T., Trendafilov, N. T., , and Uddin, M. A modified principal component technique based on the Lasso.
In JCGS, pp. 531–547, 2003.

References

Journee, M. and Y. Nesterov, Peter Richtarik, R. Sepulchre.
Generalized power method for sparse principal component analysis. Journal of Machine Learning Research,
pp. 517–553, 2008.

Asteris, M., Papailiopoulos, D. S., and Dimakis, A. G.
Nonnegative sparse PCA with provable guarantees. In
ICML, 2014.

la Torre, F. De and Black, M. J. A framework for robust
subspace learning. International Journal of Computer
Vision, 54(1/2/3):117–142, 2003.

Birnbaum, A., Johnstone, I. M., Nadler, B., and Paul,
D. Minimax bounds for sparse PCA with noisy highdimensional data. The Annals of Statistics, 41(3):1055–
1084, 2013.

LeCun, Y., Jackel, L., Bottou, L., Brunot, A., Cortes,
C., Denker, J., Drucker, H., Guyon, I., Müller, U.,
Säckinger, E., Simard, P., and Vapnik, V. Comparison of
learning algorithms for handwritten digit recognition. In
International Conference on Artificial Neural Networks,
pp. 53–60, 1995.

Croux, C. and Haesbroeck, G. Principal component analysis based on robust estimators of the covariance or
correlation matrix: Influence functions and efficiencies.
BIOMETRIKA, 87:603–618, 2000.
Croux, C., Filzmoser, P., and Fritz, H. Robust sparse principal component analysis. Technometrics, 55(2):202–214,
2013.
Dasgupta, S. Subspace detection: A robust statistics formulation. In Proceedings of the Sixteenth Annual Conference on Learning Theory, 2003.
d’Aspremont, A., El Ghaoui, L., Jordan, M. I., and Lanckriet, G. R. A direct formulation for sparse PCA using
semidefinite programming. SIAM review, 49(3):434–
448, 2007.
Devlin, S. J., Gnanadesikan, R., and Kettenring, J. R.
Robust estimation of dispersion matrices and principal
components. Journal of the American Statistical Association, 76(374):354–362, 1981.
Feng, J., Xu, H., and Yan, S. Robust PCA in highdimension: A deterministic approach. In ICML, 2012.
Frank, A. and Asuncion, A. UCI machine learning repository. 2010.
Hubert, M., Rousseeuw, P. J., and Branden, K. ROBPCA:
A new approach to robust principal component analysis.
Technometrics, 47(1):64–79, 2005.
Hubert, M., Reynkens, T., and Schmitt, E. Sparse PCA for
high-dimensional data with outliers. Technical Report,
2014.
Johnstone, I. M. and Lu, A. Y. On consistency and sparsity for principal components analysis in high dimensions. Journal of the American Statistical Association,
104(486):682–693, 2009.

Moghaddam, B., Weiss, Y., and Avidan, S. Spectral bounds
for sparse PCA: Exact and greedy algorithms. In NIPS,
2005.
Montanari, A. and Richard, E. Non-negative principal
component analysis: Message passing algorithms and
sharp asymptotics. In arXiv:1406.4775, 2014.
Papailiopoulos, D. S., Dimakis, A. G., and Korokythakis,
S. Sparse PCA through low-rank approximations. In
ICML, 2013.
Pearson, K. On lines and planes of closest fit to systems
of points in space. Philosophical Magazine, 2(6):559C–
572, 1901.
Rousseeuw, P. J. Multivariate estimation with high breakdown point. Mathematical statistics and applications, 8,
1985.
Rousseeuw, P. J. and Driessen, K. V. A fast algorithm for
the minimum covariance determinant estimator. Technometrics, 41:212–223, 1998.
Shen, H. and Huang, J. Z. Sparse principal component
analysis via regularized low rank matrix approximation.
Journal of Multivariate Analysis, 99:1015–1034, 2008.
Sung, K. Learning and example selection for object and
pattern recognition. PhD thesis, MIT, 1996.
Thiao, M., Dinh, T. P., and Thi, H. A. A DC programming
approach for sparse eigenvalue problem. In ICML, 2010.
Vu, V. Q. and Lei, Jing. Minimax bounds for sparse PCA
with noisy high-dimensional data. The Annals of Statistics, 41(6):2703–3110, 2013.

A Unified Framework for Outlier-Robust PCA-like Algorithms

Vu, V. Q., Cho, J., Lei, J., and Robe, K. Fantope projection and selection: A near-optimal convex relaxation of
sparse PCA. In NIPS, 2013.
Wang, L. and Cheng, H. Robust sparse PCA via weighted
elastic net. In Pattern Recognition, pp. 88–95. Springer,
2012.
Xu, H., Caramanis, C., and Mannor, S. Outlier-robust PCA:
the high-dimensional case. IEEE Transactions on Information Theory, 59(1):546–572, 2013.
Xu, L. and Yuille, A. L. Robust principal component analysis by self-organizing rules based on statistical physics
approach. IEEE Transactions on Neural Networks, 6(1):
131–143, 1995.
Yang, T. N. and Wang, S. D. Robust algorithms for principal component analysis. Pattern Recognition Letters, 20
(9):927–933, 1999.
Zhang, Y. and El Ghaoui, L. Large-scale sparse principal component analysis with application to text data. In
NIPS, 2011.
Zou, H., Hastie, T., and Tibshirani, R. Sparse principal
component analysis. In JCGS, pp. 265–286, 2006.

