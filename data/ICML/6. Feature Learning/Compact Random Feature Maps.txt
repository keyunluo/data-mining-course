Compact Random Feature Maps

Raffay Hamid
eBay Research Laboratory

RAFFAY @ CC . GATECH . EDU

Ying Xiao
Georgia Institute of Technology

YING . XIAO @ GATECH . EDU

Alex Gittens
eBay Research Laboratory

AGITTENS @ EBAY. COM

Dennis DeCoste
eBay Research Laboratory

DDECOSTE @ EBAY. COM

Abstract
Kernel approximation using random feature
maps has recently gained a lot of interest. This is
mainly due to their applications in reducing training and testing times of kernel based learning algorithms. In this work, we identify that previous
approaches for polynomial kernel approximation
create maps that can be rank deficient, and therefore may not utilize the capacity of the projected
feature space effectively. To address this challenge, we propose compact random feature maps
(CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove
the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efficiently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear
multi-class classifiers. We present experiments
on multiple standard data-sets with performance
competitive with state-of-the-art results.

1. Introduction
Kernel methods allow implicitly learning non-linear functions using explicit linear feature spaces (Schlkopf et al.,
1999). These spaces are typically high dimensional and
often pose what is called the curse of dimensionality. A solution to this problem is the well-known kernel trick (Aizerman et al., 1964), where instead of directly learning a
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

classifier in Rd , a non-linear mapping Φ : Rd → H is considered, where ∀ x, y ∈ Rd , hΦ(x), Φ(y)iH = K(x, y) for
a kernel K(x, y). A classifier H : x 7→ wT Φ(x) is then
learned for a w ∈ H.
It has been observed however that with increase in training data size, the support of the vector w can undergo unbounded growth, which can result in increased training as
well as testing time (Steinwart, 2003) (Bengio et al., 2006).
Previous approaches to address this curse of support have
mostly focused on embedding the non-linear feature space
H into a low dimensional Euclidean space while incurring an arbitrarily small distortion in the inner product values (Rahimi & Recht, 2007) (Kar & Karnick, 2012) (Le
et al., 2013) (Pham & Pagh, 2013). One way to do this
is to construct a randomized feature map Z : Rd → RD
such that for all x, y ∈ Rd , hZ(x), Z(y)i ≈ K(x, y). Each
component of Z(x) can be computed by first projecting
x onto a set of randomly generated d dimensional vectors
sampled from a zero-mean distribution, followed by computing the dot-products of the projections. While randomized feature maps can approximate the more general class
of dot-product kernels, in this work we focus on polynomial kernels, where K(x, y) is of the form (hx, yi+q)r ,
with q ∈ R+ and r ∈ N0 .
In previous works, it has been shown that |hZ(x), Z(y)i −
K(x, y)| reduces exponentially as a function of D (Kar &
Karnick, 2012) (Pham & Pagh, 2013). However in practice,
to approximate K(x, y) with sufficient accuracy, D can still
need to be increased to values that might not be amenable
to efficiently learn classifiers in RD . This is especially true
for higher values of r.
We also show that spaces constructed by random feature
maps can be rank deficient. This rank deficiency can result in the under-utilization of the projected feature space,
where the model parameters learned in RD can have signif-

Compact Random Feature Maps

icant number of components close to zero.

with more than a few hundreds of thousand data-points.

This presents us with the dilemma between better approximation of exact kernel values and efficient classifier learning. To resolve this dilemma, we propose compact random
feature maps (CRAFTMaps) as a more concise representation of random feature maps that can approximate polynomial kernels more accurately. We show that the information content of Z : Rd → RD can be captured more compactly by generating an alternate random feature map G :
RD → RE , such that E < D, and hG(Z(x)), G(Z(y))i approximates hZ(x), Z(y)i. CRAFTMaps are therefore constructed by first up projecting the original data non-linearly
to RD in order to minimize |hZ(x), Z(y)i − K(x, y)|. This
is followed by linearly down projecting the up-projected
vectors to RE with E < D in order to capture the underlying structure in RD more compactly. We present both
analytical as well as empirical evidence of the fact that the
“up/down” projections employed by CRAFTMaps approximate K(x, y) better than a direct random polynomial feature map Z : Rd → RE .

To solve this challenge, several schemes have been proposed to explicitly approximate the kernel matrix, including low-rank approximations (Blum, 2006) (Bach & Jordan, 2005), sampling individual entries (Achlioptas et al.,
2002), or discarding entire rows (Drineas & Mahoney,
2005). Similarly, fast nearest neighbor look-up methods
have been used to approximate multiplication operations
with the kernel matrix (Shen et al., 2005). Moreover,
concepts from computational geometry have also been explored to obtain efficient approximate solutions for SVM
learning (Tsang et al., 2006).

The additional cost of down projecting from RD to RE incurred by CRAFTMaps is well-justified by the efficiency
gains they offer in terms of training in RE . To further improve the efficiency of CRAFTMaps, we show how they
can be generated using structured random matrices, in particular Hadamard transform, that reduces the cost of multiplying two n × n matrices from O(n3 ) to O(n2 log(n)).
This gain is exploited for both up as well as down projection steps of CRAFTMaps.
The compactness of CRAFTMaps makes them particularly
suitable for using Hessian based methods to learn classifiers in a single pass over the data. Moreover, we show
how CRAFTMaps can be used to learn multi-class classifiers in a streaming manner, using the previously proposed
framework of error correcting output codes (ECOCs) (Dietterich & Bakiri, 1994), to minimize the least square error
between the predicted and the true class labels. This combination of CRAFTMaps and ECOCs is particularly powerful
as it can be formalized as a matrix-matrix multiplication,
and can therefore maximally exploit the multi-core processing power of modern hardware using BLAS3 (Golub
& Van Loan, 2012). Finally, by requiring minimal communication among mappers, this framework is well-suited for
map-reduce based settings.

2. Related Work
Extending the kernel machines framework to large scale
learning has been explored in a variety of ways (Bottou
et al., 2007) (Sonnenburg et al., 2006) (Joachims, 1999).
The most popular of these approaches are decomposition methods for solving Support Vector Machines (Platt,
1999) (Chang & Lin, 2011). While in general extremely
useful, these methods do not always scale well to problems

An altogether different approximation approach that has recently gained much interest is to approximate the kernel
function directly as opposed to explicitly operating on the
kernel matrix. This can be done by embedding the nonlinear kernel space into a low dimensional Euclidean space
while incurring an arbitrarily small additive distortion in
the inner product values (Rahimi & Recht, 2007). By relying only on the embedded space dimensionality, this approach presents a potential solution to the aforementioned
curse of support, and is similar in spirit to previous efforts
to avoid the curse of dimensionality in nearest neighbor
problems (Indyk & Motwani, 1998).
While the work done in (Rahimi & Recht, 2007) focuses
on translation invariant kernels, there have been several
subsequent approaches proposed to approximate other kernels as well, some of which include group invariant (Li
et al., 2010), intersection (Maji & Berg, 2009), and RBF
kernels (Le et al., 2013). There has also been an interest in approximating polynomial kernels using random feature maps (Kar & Karnick, 2012) and random tensor products (Pham & Pagh, 2013). Our work builds on these approaches and provides a more compact representation of
accurately approximating polynomial kernels.

3. Compact Random Feature Maps
We begin by demonstrating the rank deficiency of the previous polynomial kernel approximations (Kar & Karnick,
2012) (Pham & Pagh, 2013), followed by a detailed presentation of the CRAFTMaps framework.
3.1. Preliminaries
Following (Kar & Karnick, 2012), consider a positive definite kernel K : (x, y) 7→ f (hx, yi),Pwhere f admits a
∞
n
Maclaurin expansion, i.e., f (x) =
n=0 an x , where
(n)
an = f (0)/n!. An example of such a kernel is the
polynomial kernel K(x, y) = (hx, yi+q)r , with q ∈ R+
and r ∈ N0 . By defining estimators for each individual
term of the kernel expansion, one can approximate the exact kernel dot-products. To this end, let w ∈ {−1, 1}d
be a Rademacher vector, i.e., each of its components are

Compact Random Feature Maps

Algorithm 1 – R ANDOM F EATURE M APS (RFM)
Input: Kernel parameters q and r, output dimensionality
D, sampling parameter p > 1
Output: Random feature map Z : Rd → RD such that
hZ(x), Z(y)i ≈ K(x, y)
∞
(n)
P
1: Find f (x) =
an xn , where an = f n!(0)
n=0

2: for each i = 1 to D do
3:
Choose variable N using Pr[N = n] =

1: Up Project: Using Algorithm 1, construct random fea1
pn+1

Choose wj ∈ {−1, 1}d using N fair coin tosses
N
p
Q
wjT x
Define Zi : x 7→ aN pN+1

4:
5:

j=1

6: Construct Z : x 7→

√1 (Z1 , ·
D

· ·, ZD )

chosen independently using a fair coin toss from the set
{−1, 1}. It can be shown that for Pr[N = n] = 1/(pn+1 )
for some constant p > 1, and w1 , · · ·, wN as N independent Rademacher
vectors, the feature map Zi : Rd → R,
p
QN
T
N+1
Zi : x 7→ aN p
j=1 wj x gives an unbiased estimate
of the polynomial kernel. Generating D such feature maps
independently and concatenating them together constructs
a multi-dimensional
feature map Z : Rd → RD , Z : x 7→
√
1/ D(Z1 (x), · · ·, ZD (x)), such that E (hZ(x), Z(y)i) =
K(x, y). The procedure for generating random feature
maps for polynomial kernels is listed in Algorithm 1 and
illustrated in Figure 1.
3.2. Limitations of Random Feature Maps

q = 0, r = 2, D = 4

Input Vector

a n = {1, 0, 0}

r D

d

1
3
10
9
7

-1
-1
-1
1
1
-1
-1
-1

d
-1
-1
-1
1
1
1
-1
-1

1
1
-1
1
-1
1
-1
1

1
-1
-1
-1
-1
1
1
1



Random Projections

1
3
10
9
7





2
-10
-28
16
-4
24
-14
2

ture map Z : Rd → RD , such that hZ(x), Z(y)i ≈
K(x, y)
2: Down Project: Using Johnson-Lindenstrauss random projection, linearly down-project Z to construct
G : RD → RE such that hG(Z(x)), G(Z(y)i ≈
hZ(x), Z(y)i.

rank deficiency is also true for the space generated by random tensor products (Pham & Pagh, 2013) whose log-scree
plot is shown in green in Figure 2(a).
This rank deficiency can result in the under-utilization of
the projected feature space. Figure 2(b) shows the histogram of the linear weight vector learned in a 212 dimensional random feature map (Kar & Karnick, 2012) for a 7th
order polynomial kernel (q = 1). The plot was obtained
for 1000 randomly selected points from MNIST data for
two class-sets. The spike at zero shows that a majority of
the learned weight components do not play any role in classification.
3.3. CRAFTMaps using Up/Down Projections

The benefit of random feature maps to approximate the underlying eigen structure of the exact kernel space can come
at the cost of their rank deficiency. Consider e.g. Figure 2(a) where the black graph shows the log-scree plot
of the exact 7th order polynomial kernel (q = 1) obtained
using 1000 randomly selected set of points from MNIST
data. The red graph shows the log-scree plot for the random feature map (Kar & Karnick, 2012) in a 212 dimensional space. Note that the red plot is substantially lower
than the black one for majority of the spectrum range. This

-1
1
1
1
-1
1
1
-1

Algorithm 2 – CRAFTM APS USING RFM
Input: Kernel parameters q and r, up and down projection
dimensionalities D and E such that E < D, sampling parameter p > 1
Output: CRAFTMap G : Rd → RE , such that
hG(x), G(y)i ≈ K(x, y)



D


Projection Products

-10
-224
-48
-14
Random
Feature
Map

Figure 1: Projection of a 5 dimensional vector to a random feature map for a 2nd order homogenous polynomial kernel in 4 dimensions. As an = {1, 0, 0} here, we need r × D = 2 × 4 = 8
Rademacher vectors such that we could multiply each r = 2 projections to construct Z in D = 4 dimensions.

To address the limitations of random feature maps, we propose CRAFTMaps as a more accurate approximation of
polynomial kernels. The intuition behind CRAFTMaps is
to first capture the eigen structure of the exact kernel space
comprehensively, followed by representing it in a more
concise form. CRAFTMaps are therefore generated in the
following two steps:
Up Projection: Since the difference between hZ(x), Z(y)i
and K(x, y) reduces exponentially as a function of the dimensionality of Z (Kar & Karnick, 2012) (Pham & Pagh,
2013), we first up project the original data non-linearly
from Rd to a substantially higher dimensional space RD
to maximally capture the underlying eigen structure of the
exact kernel space.
Down Projection: Since the randomized feature map Z :
Rd → RD generated as a result of the up-projection step
is fundamentally rank-deficient (as shown previously in
§ 3.2), we linearly down project Z to a lower-dimensional
map G : RD → RE , such that E < D, and hG(Z(x)),
G(Z(y))i ≈ hZ(x), Z(y)i. The procedure to generate
CRAFTMaps is listed in Algorithm 2. Note that while Algorithm 2 uses random feature maps (Kar & Karnick, 2012)
for up-projection, one could also use other feature maps

Compact Random Feature Maps
(a) Rank Improvement using CRAFTMaps

Log (Top Eigen Values of Kernel Matrix)

−0.6

Exact Kernel
Tensor Sketch (Pham and Pagh)
Random Map (Kar and Karnick)

9
8

−0.2
0

CRAFTMaps over Tensor Sketch
CRAFTMaps over Random Map

7
6

0.2
0.4
0.6

5

50

−0.6

4

100

−0.4
−0.2

3

0

2

0.2
0.4

1
0

100 200 300

−0.4

0.6

0

100

200

300

400

500

600

700

800

(b)Weights Random Map (c)Weights CRAFTMaps

10

900 1000

Figure 2: (a) Rank deficiency of tensor sketch (Pham & Pagh,
2013) and random feature maps (Kar & Karnick, 2012), along
with rank improvements due to CRAFTMaps. (b-c) Histograms
of weight vectors learned in a 212 dimensional random feature
map (Kar & Karnick, 2012) and CRAFTMaps (here D was set
equal to 214 ).

e.g. tensor products (Pham & Pagh, 2013) to generate Z.
The rank improvement brought about by using
CRAFTMaps for random feature maps and tensor
sketch is shown in Figure 2-a by the dotted red and green
plots respectively. The improved utilization of the projected space of random feature maps due to CRAFTMaps
is demonstrated in Figure 2(c).

bounds of CRAFTMaps. Note that these bounds are independent of the dimensionality of the input space, which is a
significant improvement over both (Kar & Karnick, 2012)
and (Pham & Pagh, 2013).
Lemma 3.1. Fix an integer r ≥ 2, and define SD as:
SD =

D Y
r
X

hx, ωi,j ihx0 , ωi,j i

i=1 j=1

where x, x0 are vectors of unit Euclidean length, and ωi,j ∼
N (0, Id ) are independent Gaussian vectors. Then whenever D ≥ 3 · 4r+2 ε−2 ,




 1 !
1

1 Dε2 2r+2
0 r
r

Pr  SD − hx, x i  ≥ ε ≤ c exp −
D
2 11
where 0 < c < 0.766 is a universal constant.
Qr
Proof: Let Yi = j=1 hx, ωi,j ihx0 , ωi,j i, then the deviation of SD from its mean is estimated by the rate at which
the tails of Yi decay, which is in turn determined by the
rates at which the moments of Yi grow. We first verify that
the expectation of the summands indeed equals hx, x0 ir :
E (Yi ) =

r
Y


T
E xT ωi,j ωi,j
x0 = hx, x0 ir

j=1

Similarly, the k th moment of Yi can be determined as:
3.4. Error Bounds for CRAFTMaps
Recall that the following result obtained using an application of the Hoeffding inequality (Hoeffding, 1963) is central to the analysis of (Kar & Karnick, 2012):

r
 Y
 
T
E |Yi |k =
E |tr xT ωi,j ωi,j
x 0 |k

≤

Pr (|hZ(x), Z(y)i − K(x, y)| > ε) ≤ 2 exp −


2

Dε
8C2Ω

(1)

=
where D is the dimensionality of Z, and C2Ω is a constant
(defined below). We first examine this inequality more
closely for homogenous polynomial kernels K(x, y) =
r
hx, yi for all points on the unit sphere. In that case we
have,

2
1
2
2 2
CΩ = (pf (pR )) =
d2r
(2)
2r+1
√
where R = max kxk`1 = d and a suitable choice for p is
1/2. We only get a non-trivial bound when D & ε−2 d2r .
Note however that if we used explicit kernel expansion, we

would need substantially fewer features (at most d+r−1
).
r
The same holds for (Pham & Pagh, 2013) since they apply
the same Hoeffding inequality, and the analysis produces
the same asymptotics.
We therefore first present an improved error analysis
of (Kar & Karnick, 2012), focusing on homogeneous polynomial kernels. We then use this analysis to prove error

j=1
r h
Y
j=1
r
Y
j=1

 0 T k 
k i
T
x x  E tr xT ωi,j ωi,j
x
2

r

 Y
T
E |ωi,j
E |γj |2k
x|2k =
j=1

" 
#r
 rk
k
√
1
(2k)!
2k
=
≤ ( 2)r
= cr k rk
2
k!
e
√
Here γj ∼ N (0, 1), c = 2(2/e)k , and the last three expressions above follow from the formula for the moments
of a standard Gaussian random variables (Patel & Read,
1996). We now estimate moments of feature map approximation error.
!
D
X
k
1


Q = kE 
(Yi − E (Yi ))
D
i=1
Assuming k ≥ 2, and using Marcinkiewicz–Zygmund inequality (Burkholder, 1988) we have:
k


k
Q≤ √
E |Yi − E (Yi ) |k
D

Compact Random Feature Maps

A standard estimate of the right-hand quantity using Jenson’s inequality allows us to conclude that

Q≤

2k
√
D

k


E |Yi |k ≤ cr



2k
√
D

k

k rk

Finally, we apply Markov’s inequality to bound the tails of
the approximation error:


!

k
D
1 X

Q
2k

0 r
√
k rk
Pr 
Yi − hx, x i  ≥ ε ≤ k ≤ cr
D

ε
ε
D
i=1
 
√ 
= cr exp k log(2k r+1 ) − log(ε D)

Z : Rd → RD composed with a Johnson-Lindenstrauss
map Q : RD → RE , where D ≥ E, to obtain Z0 , then the
following holds:
r+1


log(n)r+1
log(n)1/2
hx, x0 ir − hZ0 (x), Z0 (y)i . 2
+
1/2
D
E1/2

with high probability ∀ x, x0 ∈ X simultaneously.
Proof: A Johnson-Lindenstrauss projection from RD to RE
preserves with high probability all pairwise inner products
of the n points {Z(x) : x ∈ X} in RD to within an additive factor of ε0 . log(n)1/2 /E1/2 . Applying the triangle
inequality:
r

Fixing α > 0 and assuming that D > e2α 4r+2 ε−2 and
k = b(ε2 De−2α /4)1/(2r+2) c ensures that
√
log(2k r+1 ) − log(ε D) ≤ −α

r

|hx, yi − hZ0 (x), Z0 (y)i| ≤ |hx, yi − hZ(x), Z(y)i| +
|hZ(x), Z(y)i − hZ0 (x), Z0 (y)i| := ε + ε0
Referring to Equation 3 to bound ε, we obtain the final error
bound:

log(n)1/2
2r+1 log(n)r+1
and k ≥ 2, so our earlier assumption when applying
+
ε + ε0 .
Marcinkiewicz–Zygmund inequality is valid. Thus
D1/2
E1/2


!

 2 ρ 
D

1 X

Dε

0 r
r
Pr 
Yi − hx, x i  ≥ ε ≤ c exp −α
In particular, the error is lower than random feature
D

4e2α
i=1
maps (Kar & Karnick, 2012) whenever:
√
2
where ρ = 1/(2r + 2) and c ≤ 2(2/e) < 0.766. Take
2r+1 log(n)r+1
log(n)1/2
2r+1 log(n)r+1
+
.
α = 1/2 to reach the bound in the theorem.

D1/2
E1/2
E1/2
Applying Lemma 3.1, the following corollary follows:
Fixing D = g(r)E for some constant g(r) ≥ 1,
Corollary 3.2. Let X ⊂ Rd be a set of n unit vectors. Let
CRAFTMaps provide a better error bound when:
ωi,j ∼ N (0, Id ) be a set of r · D independent Gaussian
2

random vectors. If D & 4r+1 log(n)2r+2 ε−2 then we have
log(n)r+1/2
≈1
g(r)
&
with high probability:
log(n)(r+1/2) − 2−(r+1)




D Y
r
1 X

3.5. Efficient CRAFTMaps Generation
0
0 r

hx,
ω
i
hx
,
ω
i
−
hx,
x
i
i,j
i,j
D
≤ε
 i=1 j=1

Recall that for Hessian based optimization of linear regression problems, the dominant cost of O(nD2 ) is spent
calculating the Hessian. By compactly representing ranwhich holds simultaneously ∀ x, x0 ∈ X.
dom feature maps in RE as opposed to RD for E < D,
Proof: We apply the Lemma 3.1 along with the trivCRAFTMaps provide a factor of D2 /E2 gain in the com2
ial union bound over O(n ) points. Thus, we require
plexity of Hessian computation. A straightforward version
exp(log(n2 ) − (Dε2 )1/(2r+2) ) to be small. In this case,
of CRAFTMaps would incur an additional cost of O(nDE)
picking D ≥ log(n2 )(2r+2) ε−2 suffices.

for the down-projection step. However, since for problems
at scale n >> D, the gains CRAFTMaps provide for clasAn alternate way to view this is to fix D, in which case the
sifier learning over random feature maps is well worth the
final approximation error will be bounded by:
relatively small additional cost they incur.
√
2 r+1
(3)
ε . log(n ) / D
These gains can be further improved by using structured random matrices for the up/down projections of
We can combine this with a usual JohnsonCRAFTMaps. One way to do this is to use the Hadamard
Lindenstrauss (Johnson & Lindenstrauss, 1984) random
matrix as a set of orthonormal bases, as opposed to usprojection as follows:
ing a random bases-set sampled from a zero mean distriTheorem 3.3. Let X ⊂ Rd be a set of n unit vectors. Supbution. The structured nature of Hadamard matrices enpose we map these vectors using a random feature map
ables efficient recursive matrix-matrix multiplication that

Compact Random Feature Maps

1
1
1
1

1 1
-1 1
1 -1
-1 -1
H2

x1

D1
1 -1 0 0 0
-1 0 1 0 0
-1 0 0 1 0
1 0 0 0 -1

4
8
7
6

1
-1
-1
1

4
8
7
6
x2

1
0
0
0

0 0
-1 0
0 1
0 0
D2

0
0
0
-1

Randomized
Hadamard Projections

Cx

5
1
3
-25
-3
25
-5
-1

5
1
3
-25
-3
25
-5
-1

Concatination
and Shuffling





25
-3
1
3
-5
-25
5
-1

decoding



D


Projection
Products

-38
2
63
-3

Output

o1

o2

o3

o4

o5

o6

1
1

0
1

0
1

0
1

1
0

1
0

C1

1

0

0

1

1

00

C3

0

0

1

0

1

0

C4

b1

b2

b3

b4

b5

b6

x

only requires O(n2 log(n)) operations compared to the
O(n3 ) operations needed for the product of two n × n
non-structured matrices. Constructing CRAFTMaps using Hadamard transform can therefore reduce the complexity of up projection from O(nDd) to O(nDlog(d)), and
that of down projection from O(nD2 ) to O(nDlog(D))
respectively. To employ Hadmard matrices for efficient
CRAFTMaps generation, we use the sub-sampled randomized Hadamard transform (SRHT) (Tropp, 2011).
While SRHT can be used directly for the down-projection
step, we need to incorporate a few novel modifications to it
before it can be used for up-projection. In particular, given
a kernel function K : (x, y) 7→ f (hx, yi) and a d dimenPD
sional1 vector x, we first construct T = d( i=1 Ni )/de
copies of x, where N is defined in Algorithm 1. Each copy
xt is multiplied by a diagonal matrix Mt whose entries are
set to +1 or −1 with equal probability. Each matrix Mt xt
is implicitly multiplied by the d × d Hadamard matrix H.
All rows of HMt xt for all t = {1, · · ·, T} are first concatenated, and then randomly permuted, to be finally used
according to Algorithm 1 to non-linearly up-project x from
Rd to RD (see Figure 3).

4. Classification Using ECOCs
To solve multi-class classification problems, we use error correcting output codes (ECOCs) (Dietterich & Bakiri,
1994) which employ a unique binary “codeword” of length
c for each of the k classes, and learn c binary functions, one
for each bit position in the codewords. For training, using
an example from class i, the required outputs of the c binary
functions are specified by the codeword for class i. Given a
test instance x, each of the c binary functions are evaluated
to compute a c-bit string s. This string is compared to the
k codewords, assigning x to the class whose codeword is
closest to s according to some distance (see Figure 4).
Overall, given d dimensional data from k classes, we use
up/down projections to construct its CRAFTMap representation in RE . We then use ECOCs to learn c binary linear
1

As Hadamards exist in powers of 2, usually x needs to be
zero-padded to the closest higher power of 2.

Figure 4: To learn bit 3 classifier, all points from C2 & C4 are
considered positive, and those
from C1 & C3 negative. If e.g.
the detected labels for a test instance are 110110, it is assigned
to C3 as it is at minimum distance from C3 codeword.

codewords

Figure 3: Randomized Hadamard basis to up-project an input
vector in 4 dimensional space to a random map for a 2nd order
homogenous kernel in a 4 dimensional space.

C2

Input

Input
Vector

1
1
-1
-1

regressors in RE . To test a point, we up/down project it to
RE and then pass it through the ECOCs to be classified to
one of the k classes.

5. Experiments and Results
We now present CRAFTMaps results on multiple data-sets.
Unless otherwise mentioned, we use the H-0/1 heuristic
of (Kar & Karnick, 2012) for random feature maps (RFM)
and CRAFTMaps on RFM.
5.1. Reconstruction Error
Figure 5 shows the normalized root mean square errors
(NRMSE) for MNIST data obtained for an r = 7 and
q = 1 kernel using random feature maps (RFM – top
row) (Kar & Karnick, 2012) versus CRAFTMaps (bottom
table) over RFM. These results were obtained using the
same set of 1000 randomly selected data points. As shown,
CRAFTMaps provide a significant improvement consistent
over a range of D and E. Similar trends can be found in Figure 6 where NRMSE for 6 different data-sets over a range
of E are shown.
Figure 7 (a) shows reconstruction results as a function
of polynomial degree obtained using 10 sets of 1000
randomly picked points from MNIST data. As shown,
CRAFTMaps consistently improve the reconstruction error
over a wide range of polynomial degrees.

1.134

0.575

0.442

0.297

0.242

0.175

2^15

0.485

0.356

0.256

0.206

0.186

0.175

2^16

0.429

0.343

0.236

0.182

0.154

0.138

2^17

0.416

0.332

0.218

0.158

0.123

0.103

2^18

0.414

0.326

0.205

0.144

0.108

0.088

2^19

0.397

0.329

0.208

0.139

0.099

0.075

2^20

0.381

0.324

0.204

0.136

0.098

0.074

2^10

2^11

2^12

2^13

2^14

2^15

E

D

q=0
r=2
D=4

1
-1
1
-1

Classes

4
8
7
6

T = (r x D)/d = 2

d

H1
1
1
1
1

Figure 5: NRMSE for polynomial kernel reconstruction using
r=7 and q=1 on MNIST. Top row is for RFM and bottom table for
CRAFTMaps on RFM.

Compact Random Feature Maps

0.5
0.4
0.3
0.2
0.1

128

256

512

2048

1
0.8
0.6
0.4
0.2
0

2048

4096

8192

1.2
1
0.8
0.6
0.4

256

512

16384

Down projection Dimensionality (E)

2048

Random Feature Map (RFM)
Tensor Sketch (TS)
CRAFTMaps RFM
CRAFTMaps TS

1.4
1.2
1
0.8
0.6
0.4
0.2
1024

2048

4096

8192

Random Feature Map (RFM)
Tensor Sketch (TS)
CRAFTMaps RFM
CRAFTMaps TS

1.4
1.2
1
0.8
0.6
0.4
0.2
0

4096

PENDIGITS

1.6

0

32768

1024

Down projection Dimensionality (E)

1.8

Random Feature Map (RFM)
Tensor Sketch (TS)
CRAFTMaps RFM
CRAFTMaps TS

1.2

1.4

0.2

PASCAL

1.4

Normalized Root Mean Square Error

1024

Down projection Dimensionality (E)

1.6

CFAR−100

1.6
Random Feature Map (RFM)
Tensor Sketch (TS)
CRAFTMaps RFM
CRAFTMaps TS

Normalized Root Mean Square Error

0.6

Normalized Root Mean Square Error

0.7

Normalized Root Mean Square Error

Normalized Root Mean Square Error

0.8

ADULT

1.8
Random Feature Map (RFM)
Tensor Sketch (TS)
CRAFTMaps RFM
CRAFTMaps TS

2048

4096

8192

16384

Down projection Dimensionality (E)

32768

USPS

0.7

Normalized Root Mean Square Error

IJCNN

0.9

Random Feature Map (RFM)
Tensor Sketch (TS)
CRAFTMaps RFM
CRAFTMaps TS

0.6
0.5
0.4
0.3
0.2
0.1
0

16384

1024

2048

Down projection Dimensionality (E)

4096

8192

16384

Down projection Dimensionality (E)

Figure 6: NRMSE obtained while reconstructing the polynomial kernel with r = 7 and q = 1. Here D = 2 × max(E).
2.0

0.6
0.5
0.4
0.3
0.2
0.1

5

6

7

Polynomial Degree

8

9

Tensor Sketch (TS)
Random Feature Maps (RFM)
CRAFTMaps TS
CRAFTMaps RFM

1.9
1.85
1.8

Percent Test Classification Error

0.7

1

1.95

Random Feature Map (RFM)
Tensor Sketch (TS)
CRAFTMaps RFM
CRAFTMaps TS

Percent Test Classification Error

Normalized Root Mean Square Error

0.8

10

1

1.65

1

1

3
0.4

10

1.7

2

2
4

2

0.3

1.55

3

60K

No. of Training Examples

8.1M

3
4

0.2

10

1.5

5

3

10

1.6

1.45

2

0.5

10

1.75

Random Maps(RFM) H01=1
Random Maps(RFM) H01=0
CRAFTMaps RFM H01 = 1
Tensor Sketch

0.6

1

10

4
4

2

10

5

3

Log Time (s)

10

5
5

10

4

Figure 7: (a) Reconstruction error as a function of polynomial degree, averaged over 10 randomly sampled 1000 points of MNIST
data. Here RD = 215 while RE = 213 . (b) Test classification for MNIST8M. Here D = 217 , E = 214 , q = 1, r = 7 and ECOCs =
200. (c) Log-log scatter plot of compute times (projection+Hessian) for MNIST data. For CRAFTMaps, projection took 18.1s, 36.3s,
75.9s, 186.3s, and 419.2s, while finding Hessian took 9.5s, 26.2s, 81.1s, 334.8s, and 1100.3s respectively. Note that CRAFTMaps
show significant per unit-time classification improvements for larger feature spaces.

5.2. Classification Error
Table 1 shows the test classification error using
CRAFTMaps on random feature maps (Kar & Karnick, 2012) and tensor sketch (Pham & Pagh, 2013)
compared to 5 alternate approaches over 4 different
data-sets. As can be observed, CRAFTMaps consistently
delivered improved classification performance, mostly
outperforming the considered alternatives.
We now explain results for CRAFTMaps on MNIST
data for small and substantially large projected feature

spaces. We also explain CRAFTMaps results on very large
amounts of training data using MNIST8M.
Small Feature Spaces: Table 1-a shows MNIST results
on feature space sizes 300 to 700 dimensions. Note that
for E < d (which for MNIST is 784 ), the random feature
maps cannot use the H-0/1 heuristic of (Kar & Karnick,
2012). CRAFTMaps however do not have this limitation
as even for E < d, D can still be >> d. This allows
CRAFTMaps to use the H-0/1 heuristic in RD , which in
turn reflects in RE . This results in substantial classification gains achieved by CRAFTMaps for small-sized fea-

Compact Random Feature Maps

ture spaces, and highlights their usefulness in applications
with low memory footprint such as mobile phone apps.
Large Feature Spaces: Table 1-b shows the MNIST results on feature sizes 212 to 216 , where CRAFTMaps managed to achieve 1.12% test error using the original 60K
training data (unit-length normalized, non-jittered and nondeskewed). While for small-sized feature spaces the exact
kernel on MNIST data perform quite well, CRAFTMaps
outperform all alternatives as the size of the feature space
grows to larger values.
Results on MNIST8M Data Figure 7 (b) shows the comparative performance of CRAFTMaps for a given sized E
(214 ) as training size varies from 60K to 8.1M. This experiment uses the same set of 10 thousand test points as
used for the experiments with MNIST data. It can be seen
that CRAFTMaps on random feature maps converge the
fastest, and consistently gives better classification performance compared to the other representations. These results were obtained using a polynomial kernel with r = 7,
q = 1, D = 217 , E = 214 , and ECOCs equal to 200. As
we increase E to 216 and D to 219 using CRAFTMaps on
RFM for 7th order polynomial kernel (q = 1), we achieved
test classification error of 0.91% on MNIST8M data-set.
5.3. Run-Time Analysis
Figure 7(c) shows the log-log scatter plot of the compute
times (projection + Hessian) for random feature maps (Kar
& Karnick, 2012), tensor sketching (Pham & Pagh, 2013),
and CRAFTMaps using random feature maps (with H-01
heuristic). These times were recorded for MNIST data using a 40-core machine. Notice that CRAFTMaps show significant per unit-time classification improvements towards
the right end of the x-axis. This is because as the size of
the projected space increases, the Hessian computation cost
becomes dominant. This naturally gives CRAFTMaps an
edge given their ability to encode information more compactly. The gains of CRAFTMaps are expected to grow
even more as the training size further increases.

6. Conclusions and Future Work
In this work, we proposed CRAFTMaps to approximate
polynomial kernels more concisely and accurately compared to previous approaches. An important context where
CRAFTMaps are particularly useful is the map-reduce setting. By computing a single Hessian matrix (with different gradients for each ECOC) in a concise feature space,
CRAFTMaps provide an effective way to learn multi-class
classifiers in a single-pass over large amounts of data.
Moreover, their ability to compactly capture the eigen
structure of the kernel space makes CRAFTMaps suitable
for smaller scale applications such as mobile phone apps.
Going forward, we would like to further explore how to

a-MNIST 1
FastFood
RKS
RFM
TS
CM RFM
CM TS
Exact
b-MNIST 2
FastFood
RKS
RFM
TS
CM RFM
CM TS
Exact
c-USPS
FastFood
RKS
RFM
TS
CM RFM
CM TS
Exact

300
32.8
8.9
14.0
13.1
9.5
12.6
6.0
212
2.78
2.94
3.17
3.25
3.09
2.90
2.49
210
5.87
5.89
5.97
5.92
5.68
5.77
5.73

400
26.6
6.7
12.3
11.2
7.7
10.8
5.4
213
2.20
2.51
2.30
2.41
2.18
2.20
2.21
211
5.18
5.78
5.33
5.03
5.03
5.03
5.08

500
15.3
5.9
11.4
10.0
7.2
8.9
5.0
214
2.02
2.13
1.91
2.01
1.79
1.75
1.80
212
4.83
5.53
4.68
4.63
4.48
4.28
4.83

600
11.5
5.3
10.3
8.6
6.6
7.9
4.5
215
1.87
1.91
1.62
1.65
1.52
1.44
1.49
213
4.78
4.98
4.48
4.48
4.28
4.23
−−

700
8.4
5.0
9.5
8.0
5.9
7.3
4.1
216
1.50
1.52
1.49
1.41
1.27
1.12
1.20
214
4.65
4.78
4.13
4.33
4.03
3.93
−−

d-COIL100
FastFood
RKS
RFM
TS
CM RFM
CM TS
Exact
e-PENDIGITS
FastFood
RKS
RFM
TS
CM RFM
CM TS
Exact

211
8.25
8.14
11.11
10.08
8.94
8.16
9.55
26
8.74
5.83
7.94
11.20
7.43
8.03
9.29

212
7.83
7.36
7.55
7.19
6.86
5.97
−−
27
5.03
4.25
3.94
4.57
3.57
3.80
3.74

213
6.80
6.50
6.33
5.69
5.47
4.75
−−
28
3.08
2.63
2.85
2.37
2.28
2.37
2.74

214
6.32
5.97
5.05
4.75
4.52
4.02
−−
29
2.85
2.17
2.28
1.80
1.97
2.05
2.31

215
5.21
4.81
4.83
4.27
4.08
3.96
−−
210
2.71
2.08
1.91
1.77
1.57
1.74
2.87

Table 1: Test classification errors for multiple data-sets is shown.
Here r = 7, 7, 5, 5 and 9 respectively while q = 1. RFM, TS,
RKS, Fastfood, and CM stand for (Kar & Karnick, 2012), (Pham
& Pagh, 2013), (Rahimi & Recht, 2007), (Le et al., 2013), and
CraftMaps respectively. First row of each table shows E, while
D = 8×E. Some entries for exact kernel are left empty as training
examples for these cases were less than projection dimensionality.

better allocate the set of random bases available to us to
approximate the different Maclaurin coefficients of a kernel more accurately. Furthermore, we currently commit
to a particular kernel function at the start of the training
process. However that kernel may not be optimal for the
specific learning problem at hand. We are interested in exploring if one can simultaneously solve what is know as
the “kernel alignment” problem (Cristianini et al., 2001),
as well as learning a low-dimensional kernel embedding
using random projections.

Compact Random Feature Maps

References
Achlioptas, D., McSherry, F., and Schlkopf, B. Sampling
Techniques for Kernel Methods. In Advances in Neural
Information Processing Systems 14, 2002.
Aizerman, A., Braverman, E. M., and Rozoner, L. I. Theoretical foundations of the potential function method in
pattern recognition learning. Automation and Remote
Control, 25:821–837, 1964.
Bach, F. R. and Jordan, M. I. Predictive low-rank decomposition for kernel methods. In Proceedings of the 22nd
International Conference on Machine Learning, 2005.
Bengio, Y., Delalleau, O., and Le Roux, N. The Curse of
Highly Variable Functions for Local Kernel Machines.
In Advances in Neural Information Processing Systems
18, 2006.

Johnson, W. B. and Lindenstrauss, J. Extensions of Lipschitz mappings into a Hilbert space. In Conference on
Modern Analysis and Probability, 1984.
Kar, P. and Karnick, H. Random Feature Maps for Dot
Product Kernels. Journal of Machine Learning Research, 22:583–591, 2012.
Le, Quoc, Sarlos, Tamas, and Smola, Alexander. Fastfoodcomputing hilbert space expansions in loglinear time. In
ICML, pp. 244–252, 2013.
Li, F., Ionescu, C., and Sminchisescu, C. Random Fourier
Approximations for Skewed Multiplicative Histogram
Kernels. In Pattern Recognition. Springer, 2010.
Maji, S. and Berg, A. C. Max-Margin Additive Classiers
for Detection. In IEEE 12th International Conference on
Computer Vision and Pattern Recognition, 2009.

Blum, A. Random Projection, Margins, Kernels, and
Feature-Selection. In Subspace, Latent Structure and
Feature Selection. Springer, 2006.

Patel, J. K. and Read, C. B. Handbook of the Normal Distribution. CRC Press, 1996.

Bottou, L., Chapelle, O., DeCoste, D., and Weston, J.
(eds.). Large-Scale Kernel Machines. MIT Press, 2007.

Pham, N. and Pagh, R. Fast and Scalable Polynomial Kernels via Explicit Feature Maps. In Proceedings of the
19th International Conference on Knowledge Discovery
and Data Mining, 2013.

Burkholder, D. L. Sharp inequalities for martingales and
stochastic integrals. Asterisque, 157-158:75–94, 1988.
Chang, C. and Lin, C. LIBSVM: A Library for Support
Vector Machines. ACM Transactions on Intelligent Systems and Technology, 2:27, 2011.
Cristianini, Nello, Shawe-Taylor, John, Elisseeff, Andre,
and Kandola, Jaz. On kernel target alignment. In NIPS,
volume 2, pp. 4, 2001.
Dietterich, T. G. and Bakiri, G. Solving Multiclass Learning Problems via Error-Correcting Output Codes. Journal of Artificial Intelligence Research, 2:263–286, 1994.
Drineas, P. and Mahoney, M. W. On the Nyström method
for approximating a Gram matrix for improved kernelbased learning. Journal of Machine Learning Research,
6:2153–2175, 2005.

Platt, J. C. Using Analytic QP and Sparseness to Speed
Training of Support Vector Machines. In Advances in
Neural Information Processing Systems 11, 1999.
Rahimi, A. and Recht, B. Random Features for Large-Scale
Kernel Machines. In Advances in Neural Information
Processing Systems 20, 2007.
Schlkopf, B., Burges, C. J. C., and Smola, A. J. (eds.).
Advances in Kernel Methods: Support Vector Learning.
MIT Press, 1999.
Shen, Y., Ng, A., and Seeger, M. Fast Gaussian Process Regression using KD-Trees. In Advances in Neural Information Processing Systems 18, pp. 1225–1232. Citeseer,
2005.

Golub, G. H. and Van Loan, C. F. Matrix Computations.
Johns Hopkins University Press, 2012.

Sonnenburg, Sören, Rätsch, Gunnar, Schäfer, Christin, and
Schölkopf, Bernhard. Large scale multiple kernel learning. The Journal of Machine Learning Research, 7:
1531–1565, 2006.

Hoeffding, W.
Probability Inequalities for Sums of
Bounded Random Variables. Journal of the American
Satistical Association, 58:13–30, 1963.

Steinwart, I. Sparseness of Support Vector Machines. Journal of Machine Learning Research, 4:1071–1105, 2003.

Indyk, P. and Motwani, R. Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality. In
Proceedings of the 30th annual ACM Symposium on the
Theory of Computing, 1998.
Joachims, Thorsten. Making large scale svm learning practical. 1999.

Tropp, J. A. Improved analysis of the subsampled randomized Hadamard transform. Advances in Adaptive Data
Analysis, 3:115–126, 2011.
Tsang, I. W., Kwok, J. T., and Cheung, P. Core vector
machines: Fast SVM training on very large data sets.
Journal of Machine Learning Research, 6(1):363, 2006.

