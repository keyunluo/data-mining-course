Statistical and Algorithmic Perspectives on Randomized
Sketching for Ordinary Least-Squares

Garvesh Raskutti
RASKUTTI @ STAT. WISC . EDU
University of Wisconsin, Madison, Department of Statistics, Department of Computer Science, Wisconsin Institute for
Discovery, Madison, WI 53706 USA
Michael W. Mahoney
MMAHONEY @ STAT. BERKELEY. EDU
University of California, Berkeley, ICSI, Department of Statistics, Berkeley, CA 94720 USA

Abstract
We consider statistical and algorithmic aspects of
solving large-scale least-squares (LS) problems
using randomized sketching algorithms. Prior results show that, from an algorithmic perspective,
when using sketching matrices constructed from
random projections and leverage-score sampling,
if the number of samples r much smaller than
the original sample size n, then the worst-case
(WC) error is the same as solving the original
problem, up to a very small relative error. From
a statistical perspective, one typically considers
the mean-squared error performance of randomized sketching algorithms, when data are generated according to a statistical linear model. In
this paper, we provide a rigorous comparison of
both perspectives leading to insights on how they
differ. To do this, we first develop a framework
for assessing, in a unified manner, algorithmic
and statistical aspects of randomized sketching
methods. We then consider the statistical prediction efficiency (PE) and the statistical residual efficiency (RE) of the sketched LS estimator; and
we use our framework to provide upper bounds
for several types of random projection and random sampling algorithms. Among other results,
we show that the RE can be upper bounded when
r is much smaller than n, while the PE typically
requires the number of samples r to be substantially larger. Lower bounds developed in subsequent work show that our upper bounds on PE
can not be improved.

Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

1. Introduction
Recent work in large-scale data analysis and Randomized
Linear Algebra (RLA) has focused on developing so-called
sketching algorithms: given a data set and an objective
function of interest, construct a small “sketch” of the full
data set, e.g., by using random sampling or random projection methods, and use that sketch as a surrogate to perform
computations of interest for the full data set (see (Mahoney,
2011) for a review). Most effort in this area has adopted an
algorithmic perspective, whereby one shows that, when the
sketches are constructed appropriately, one can obtain answers that are approximately as good as the exact answer
for the input data at hand, in less time than would be required to compute an exact answer for the data at hand.
From a statistical perspective, however, one is often more
interested in how well a procedure performs relative to an
hypothesized model than how well it performs on the particular data set at hand. Thus an important question to consider is whether the insights from the algorithmic perspective of sketching carry over to the statistical setting. To
address this, in this paper, we develop a unified approach
that considers both the statistical perspective and algorithmic perspective on recently-developed randomized sketching algorithms in RLA, and we provide bounds on two statistical objectives for several types of random projection
and random sampling sketching algorithms.
1.1. Overview of the problem
The problem we consider in this paper is the ordinary leastsquares (LS or OLS) problem: given as input a matrix X ∈
Rn×p of observed features or covariates and a vector Y ∈
Rn of observed responses, return as output a vector βOLS
that solves the following optimization problem:

βOLS = arg minp kY − Xβk22 .
β∈R

(1)

Randomized Leverage-score Sketching

We will assume that n and p are both very large, with n 
p, and for simplicity we will assume rank(X) = p, e.g., to
ensure a unique full-dimensional solution. The LS solution,
βOLS = (X T X)−1 X T Y , has a number of well-known
desirable statistical properties (Chatterjee & Hadi, 1988);
and it is also well-known that the running time or computational complexity for this problem is O(np2 ) (Golub &
Loan, 1996).1 For many modern applications, however, n
may be on the order of 106 − 109 and p may be on the order
of 103 −104 , and thus computing the exact LS solution with
traditional O(np2 ) methods can be computationally challenging. This, coupled with the observation that approximate answers often suffice for downstream applications,
has led to a large body of work on developing fast approximation algorithms to the LS problem (Mahoney, 2011).
One very popular approach to reducing computation is to
perform LS on a carefully-constructed “sketch” of the full
data set. That is, rather than computing a LS estimator from
Problem (1) from the full data (X, Y ), generate “sketched
data” (SX, SY ) where S ∈ Rr×n , with r  n, is a
“sketching matrix,” and then compute a LS estimator from
the following sketched problem:
βS = arg minp kSY − SXβk22 .
β∈R

(2)

Once the sketching operation has been performed, the additional computational complexity of βS is O(rp2 ), i.e., simply call a traditional LS solver on the sketched problem.
Thus, when using a sketching algorithm, two criteria are
important: first, ensure the accuracy of the sketched LS estimator is comparable to, e.g., not much worse than, the
performance of the original LS estimator; and second, ensure that computing and applying the sketching matrix S
is not too computationally intensive, e.g., that it is much
faster than solving the original problem exactly.
1.2. Prior results
Random sampling and random projections provide two
approaches to construct sketching matrices S that satisfy
both of these criteria and that have received attention recently in the computer science community. In terms of
running time guarantees, the running time bottleneck for
random projection algorithms for the LS problem is the
application of the projection to the input data, i.e., actually performing the matrix-matrix multiplication to implement the projection and compute the sketch. By using fast Hadamard-based random projections, however,
That is, O(np2 ) time suffices to compute the LS solution
from Problem (1) for arbitrary or worst-case input, with, e.g., the
Cholesky Decomposition on the normal equations, with a QR decomposition, or with the Singular Value Decomposition (Golub
& Loan, 1996).
1

Drineas et al. (Drineas et al., 2011) developed a random
projection algorithm that runs on arbitrary or worst-case
input in o(np2 ) time. (See (Drineas et al., 2011) for a
precise statement of the running time.) As for random
sampling, Drineas et al. (Drineas et al., 2006; 2012) have
shown that if the random sampling is performed with respect to nonuniform importance sampling probabilities that
depend on the empirical statistical leverage scores of the
input matrix X, i.e., the diagonal entries of the hat matrix
H = X(X T X)−1 X T , then one obtains a random sampling algorithm that achieves much better results for arbitrary or worst-case input.
Leverage scores have a long history in robust statistics and
experimental design. In the robust statistics community,
samples with high leverage scores are typically flagged
as potential outliers (see, e.g., (Chatterjee & Hadi, 2006;
1988; Hample et al., 1986; Hoaglin & Welsch, 1978; Huber & Ronchetti, 1981)). In the experimental design community, samples with high leverage have been shown to
improve overall efficiency, provided that the underlying
statistical model is accurate (see, e.g., (Royall, 1970; Zavlavsky et al., 2008)). This should be contrasted with their
use in theoretical computer science. From the algorithmic perspective of worst-case analysis, that was adopted
by Drineas et al. (Drineas et al., 2011) and Drineas et
al. (Drineas et al., 2012), samples with high leverage tend
to contain the most important information for subsampling/sketching. Thus it is beneficial for worst-case analysis to bias the random sample to include samples with large
leverage scores or to rotate with a random projection to a
random basis where the leverage scores are approximately
uniformized.
The running-time bottleneck for this leverage-based random sampling algorithm is the computation of the leverage
scores of the input data; and the obvious well-known algorithm for this involves O(np2 ) time to perform a QR decomposition to compute an orthogonal basis for X (Golub
& Loan, 1996). By using fast Hadamard-based random
projections, however, Drineas et al. (Drineas et al., 2012)
showed that one can compute approximate QR decompositions and thus approximate leverage scores in o(np2 ) time;
and (based on previous work (Drineas et al., 2006)) this
immediately implies a leverage-based random sampling algorithm that runs on arbitrary or worst-case input in o(np2 )
time (Drineas et al., 2012). Readers interested in the practical performance of these randomized algorithms should
consult B ENDENPIK (Avron et al., 2010) or LSRN (Meng
et al., 2014).
In terms of accuracy guarantees, both Drineas et
al. (Drineas et al., 2011) and Drineas et al. (Drineas et al.,
2012) prove that their respective random projection and
leverage-based random sampling LS sketching algorithms

Randomized Leverage-score Sketching

each achieve the following worst-case (WC) error guarantee: for any arbitrary (X, Y ),
kY − XβS k22 ≤ (1 + κ)kY − XβOLS k22 ,

(3)

with high probability for some pre-specified error parameter κ ∈ (0, 1). This 1 + κ relative-error guarantee2 is
extremely strong, and it is applicable to arbitrary or worstcase input. That is, whereas in statistics one typically assumes a model, e.g., a standard linear model on Y ,
Y = Xβ + ,

(4)

where β ∈ Rp is the true parameter and  ∈ Rn is a standardized noise vector, with E[] = 0 and E[T ] = In×n ,
in Drineas et al. (Drineas et al., 2011) and Drineas et
al. (Drineas et al., 2012) no statistical model is assumed
on X and Y , and thus the running time and quality-ofapproximation bounds apply to any arbitrary (X, Y ) input
data.
1.3. Our approach and main results
In this paper, we address the following fundamental questions. First, under a standard linear model, e.g., as given in
Eqn. (4), what properties of a sketching matrix S are sufficient to ensure low statistical error, e.g., mean-squared error? Second, how do existing random projection algorithms
and leverage-based random sampling algorithms perform
by this statistical measure? Third, how does this relate to
the properties of a sketching matrix S that are sufficient to
ensure low worst-case error, e.g., of the form of Eqn. (3), as
has been established previously (Drineas et al., 2011; 2012;
Mahoney, 2011)? We address these related questions in a
number of steps.
In Section 2, we will present a framework for evaluating the
algorithmic and statistical properties of randomized sketching methods in a unified manner; and we will show that
providing WC error bounds of the form of Eqn. (3) and
providing bounds on two related statistical objectives boil
down to controlling different structural properties of how
the sketching matrix S interacts with the left singular subspace of the design matrix. In particular, we will consider
†
†
the oblique projection matrix, ΠU
S = U (SU ) S, where (·)
denotes the Moore-Penrose pseudo-inverse of a matrix and
U is the left singular matrix of X. This framework will
allow us to draw a comparison between the WC error and
two related statistical efficiency criteria, the statistical prediction efficiency (PE) (which is based on the prediction
error E[kX(βb − β)k22 ] and which is given in Eqn. (7) below) and the statistical residual efficiency (RE) (which is
b 2 ] and which is given
based on residual error E[kY − X βk
2
2

The nonstandard parameter κ is used here for the error parameter since  is used below to refer to the noise or error process.

in Eqn. (8) below); and it will allow us to provide sufficient
conditions that any sketching matrix S must satisfy in order
to achieve performance guarantees for these two statistical
objectives.
In Section 3, we will present our main theoretical results,
which consist of bounds for these two statistical quantities for variants of random sampling and random projection sketching algorithms. In particular, we provide
upper bounds on the PE and RE (as well as the worstcase WC) for four sketching schemes: (1) an approximate
leverage-based random sampling algorithm, as is analyzed
by Drineas et al. (Drineas et al., 2012); (2) a variant of
leverage-based random sampling, where the random samples are not re-scaled prior to their inclusion in the sketch,
as is considered by Ma et al. (Ma et al., 2015); (3) a
vanilla random projection algorithm, where S is a random
matrix containing i.i.d. Gaussian or Rademacher random
variables, as is popular in statistics and scientific computing; and (4) a random projection algorithm, where S is a
random Hadamard-based random projection, as analyzed
in (Boutsidis & Gittens, 2013). For sketching schemes (1),
(3), and (4), our upper bounds for each of the two measures of statistical efficiency are identical up to constants;
and they show that the RE scales as 1 + pr , while the PE
scales as nr . In particular, this means that it is possible
to obtain good bounds for the RE when p . r  n (in
a manner similar to the sampling complexity of the WC
bounds); but in order to obtain even near-constant bounds
for PE, r must be at least of constant order compared to n.
For the sketching scheme (2), we show, on the other hand,
that under the (strong) assumption that there are k “large”
leverage scores and the remaining n − k are “small,” then
pk
the WC scales as 1 + pr , the RE scales as 1 + rn
, and the
k
PE scales as r . That is, sharper bounds are possible for
leverage-score sampling without re-scaling in the statistical setting, but much stronger assumptions are needed on
the input data. We also present a lower bound developed
in subsequent work by Pilanci and Waniwright (Pilanci &
Wainwright, 2014) which shows that under general conditions on S, the upper bound of nr for PE can not be improved. Hence our upper bounds in Section 3 on PE can
not be improved.
In Section 4, we will provide a brief discussion and conclusion. For space reasons, we do not include in this conference version the proofs of our main results or our empirical
results that support our theoretical findings; but they are included in the technical report version of this paper (Raskutti
& Mahoney, 2014).
1.4. Additional related work
Very recently, Ma et al. (Ma et al., 2015) considered statistical aspects of leverage-based sampling algorithms (called

Randomized Leverage-score Sketching

algorithmic leveraging in (Ma et al., 2015)). Assuming
a standard linear model on Y of the form of Eqn. (4),
the authors developed first-order Taylor approximations to
the statistical RE of different estimators computed with
leverage-based sampling algorithms, and they verified the
quality of those approximations with computations on real
and synthetic data. Taken as a whole, their results suggest that, if one is interested in the statistical performance
of these randomized sketching algorithms, then there are
nontrivial trade-offs that are not taken into account by standard WC analysis. Their approach, however, does not immediately apply to random projections or other more general sketching matrices. Further, the realm of applicability of the first-order Taylor approximation was not precisely quantified, and they left open the question of structural characterizations of random sketching matrices that
were sufficient to ensure good statistical properties on the
sketched data. We address these issues in this paper.
Subsequent work by Pilanci and Wainwright (Pilanci &
Wainwright, 2014) also considers a statistical perspective of sketching. Amongst other results, they develop a
lower bound which confirms that using a single randomized sketching matrix S can not achieve a better PE than nr .
This lower bound complements the upper bounds developed in this paper. Their main focus is to use this insight to
develop an iterative sketching scheme which yields bounds
on the SPE when an r × n sketch is applied repeatedly.

2. General framework and structural results
In this section, we develop a framework that allows us
to view the algorithmic and statistical perspectives on LS
problems from a common perspective. We then use this
framework to show that existing worst-case bounds as well
as our novel statistical bounds for the mean-squared errors can be expressed in terms of different structural conditions on how the sketching matrix S interacts with the data
(X, Y ).

βS = (SX)† SY.

(6)

(We emphasize that this does not in general equal
((SX)T SX)−1 (SX)T SY , since the inverse will not exist
if the sketching process does not preserve rank.) Our goal
here is to compare the performance of βS to βOLS . We
will do so by considering three related performance criteria, two of a statistical flavor, and one of a more algorithmic
or worst-case flavor.
From a statistical perspective, it is common to assume a
standard linear model on Y ,
Y = Xβ + ,
where we remind the reader that β ∈ Rp is the true parameter and  ∈ Rn is a standardized noise vector, with E[] = 0
and E[T ] = In×n . From this statistical perspective, we
will consider the following two criteria.
• The first statistical criterion we consider is the prediction efficiency (PE), defined as follows:
CP E (S) =

E[kX(β − βS )k22 ]
,
E[kX(β − βOLS )k22 ]

(7)

where the expectation E[·] is taken over the random
noise .
• The second statistical criterion we consider is the
residual efficiency (RE), defined as follows:
CRE (S) =

E[kY − XβS k22 ]
,
E[kY − XβOLS k22 ]

(8)

where, again, the expectation E[·] is taken over the
random noise .

2.1. A statistical-algorithmic framework
Recall that we are given as input a data set, (X, Y ) ∈
Rn×p × Rn , and the objective function of interest is the
standard LS objective, as given in Eqn. (1). Since we are
assuming, without loss of generality, that rank(X) = p, we
have that
βOLS = X † Y = (X T X)−1 X T Y,

dom sampling or random projection operations, for now
we let S be any r × n matrix. Then, we are interested in
analyzing the performance of objectives characterizing the
quality of a “sketched” LS objective, as given in Eqn (2),
where again we are interested in solutions of the form

(5)

where (·)† denotes the Moore-Penrose pseudo-inverse of a
matrix.
To present our framework and objectives, let S ∈ Rr×n
denote an arbitrary sketching matrix. That is, although we
will be most interested in sketches constructed from ran-

Recall that the standard relative statistical efficiency for two
(β1 )
estimators β1 and β2 is defined as eff(β1 , β2 ) = Var
Var(β ) ,
2

where Var(·) denotes the variance of the estimator (see,
e.g., (Lehmann, 1998)). For the PE, we have replaced the
variance of each estimator by the mean-squared prediction
error. For the RE, we use the term residual since for any
b Y − X βb are the residuals for estimating Y .
estimator β,
From an algorithmic perspective, there is no noise process
. Instead, X and Y are arbitrary, and β is simply computed
from Eqn (5). To draw a parallel with the usual statistical
generative process, however, and to understand better the

Randomized Leverage-score Sketching

relationship between various objectives, consider “defining” Y in terms of X by the following “linear model”:
Y = Xβ + ,
where β ∈ Rp and  ∈ Rn . Importantly, β and  here represent different quantities than in the usual statistical setting.
Rather than  representing a noise process and β representing a “true parameter” that is observed through a noisy Y ,
here in the algorithmic setting, we will take advantage of
the rank-nullity theorem in linear algebra to relate X and
Y .3 To define a “worst case model” Y = Xβ +  for
the algorithmic setting, one can view the “noise” process
 to consist of any vector that lies in the null-space of X T .
Then, since the choice of β ∈ Rp is arbitrary, one can construct any arbitrary or worst-case input data Y . From this
algorithmic case, we will consider the following criterion.
• The algorithmic criterion we consider is the worstcase (WC) error, defined as follows:
CW C (S) = sup
Y

kY − XβS k22
.
kY − XβOLS k22

(9)

This criterion is worst-case since we take a supremum
Y , and it is the performance criterion that is analyzed
in Drineas et al. (Drineas et al., 2011) and Drineas et
al. (Drineas et al., 2012), as bounded in Eqn. (3).
Writing Y as Xβ +, where X T  = 0, the worst-case error
can be re-expressed as:
CW C (S) =

kY − XβS k22
2.
Y =Xβ+, X T =0 kY − XβOLS k2
sup

Hence, in the worst-case algorithmic setup, we take a
supremum over , where X T  = 0, whereas in the statistical setup, we take an expectation over  where E[] = 0.
Before proceeding, several other comments about this
algorithmic-statistical framework and our objectives are
worth mentioning.
• From the perspective of our two linear models, we
have that βOLS = β + (X T X)−1 X T . In the statistical setting, since E[T ] = In×n , it follows that
βOLS is a random variable with E[βOLS ] = β and
E[(β − βOLS )(β − βOLS )T ] = (X T X)−1 . In the algorithmic setting, on the other hand, since X T  = 0,
it follows that βOLS = β.
3

The rank-nullity theorem asserts that given any matrix X ∈
R
and vector Y ∈ Rn , there exists a unique decomposition
Y = Xβ +, where β is the projection of Y on to the range space
of X T and  = Y − Xβ lies in the null-space of X T (Meyer,
2000).
n×p

• CRE (S) is a statistical analogue of the worst-case algorithmic objective CW C (S), since both consider the
kY −XβS k22
ratio of the metrics kY −XβOLS
. The difference is
k22
that a sup over Y in the algorithmic setting is replaced
by an expectation over noise  in the statistical setting. A natural question is whether there is an algorithmic analogue of CP E (S). Such a performance metric
would be:
sup
Y

kX(β − βS )k22
,
kX(β − βOLS )k22

(10)

where β is the projection of Y on to the range space
of X T . However, since βOLS = β + (X T X)−1 X T 
and since X T  = 0, βOLS = β in the algorithmic
setting, the denominator of Eqn. (10) equals zero, and
thus the objective in Eqn. (10) is not well-defined. The
“difficulty” of computing or approximating this objective parallels our results below that show that approximating CP E (S) is much more challenging (in terms
of the number of samples needed) than approximating
CRE (S).
• In the algorithmic setting, the sketching matrix S and
the objective CW C (S) can depend on X and Y in any
arbitrary way, but in the following we consider only
sketching matrices that are either independent of both
X and Y or depend only on X (e.g., via the statistical leverage scores of X). In the statistical setting, S
is allowed to depend on X, but not on Y , as any dependence of S on Y might introduce correlation between the sketching matrix and the noise variable .
Removing this restriction is of interest, especially in
light of the recent results that show that one can obtain WC bounds of the form Eqn. (3) by constructing
S by randomly sampling according to an importance
sampling distribution that depends on the influence
scores—essentially the leverage scores of the matrix
X augmented with −Y as an additional column—of
the (X, Y ) pair.
• Both CP E (S) and CRE (S) are qualitatively related
to quantities analyzed by Ma et al. (Ma et al.,
2015). In addition, CW C (S) is qualitatively similar
b ) in Ma et al., since in the algorithmic setto Cov(β|Y
ting Y is treated as fixed; and CRE (S) is qualitatively
b in Ma et al., since in the statistical
similar to Cov(β)
setting Y is treated as random and coming from a linear model. That being said, the metrics and results
we present in this paper are not directly comparable
to those of Ma et al. since, e.g., they had a slightly
different setup than we have here, and since they used
a first-order Taylor approximation while we do not.

Randomized Leverage-score Sketching

2.2. Structural results on sketching matrices
We are now ready to develop structural conditions characterizing how the sketching matrix S interacts with the
data X; this will allow us to provide upper bounds for the
quantities CW C (S), CP E (S), and CRE (S). To do this, recall that given the data matrix X, we can express the singular value decomposition of X as X = U ΣV T , where
U ∈ Rn×p is an orthogonal matrix, i.e., U T U = Ip×p . In
addition, we can define the oblique projection matrix
†
ΠU
S := U (SU ) S.

(11)

Note that if rank(SX) = p, then ΠU
S can be expressed
T T
−1 T T
as ΠU
=
U
(U
S
SU
)
U
S
S,
since
U T S T SU is inS
vertible. Importantly however, depending on the properties
of X and how S is constructed, it can easily happen that
rank(SX) < p, even if rank(X) = p.
Given this setup, we can now state the following lemma,
which characterizes how CW C (S), CP E (S), and CRE (S)
depend on different structural properties of ΠU
S and SU .
Lemma 1. For the algorithmic setting,
CW C (S)

= 1+


2
kΠU
k(Ip×p − (SU )† (SU ))δk22
S k2
+
.
sup
kk22
kk22
δ∈Rp ,U T =0

For the statistical setting,
CP E (S) =

2
k(Ip×p − (SU )† SU )ΣV T βk22
kΠU
S kF
+
,
p
p

and
CRE (S) = 1 +

CSP E (S) − 1
.
n/p − 1

Several points are worth making about Lemma 1.
• For all 3 criteria, the term which involves (SU )† SU
is a “bias” term that is non-zero in the case that
rank(SU ) < p. For CP E (S) and CRE (S), the
term corresponds exactly to the statistical bias; and
if rank(SU ) = p, meaning that S is a rank-preserving
sketching matrix, then the bias term equals 0, since
(SU )† SU = Ip×p . In practice, if r is chosen smaller
than p or larger than but very close to p, it may happen
that rank(SU ) < p, in which case this bias is incurred.
E (S)−1
• The final equality CRE (S) = 1 + CPn/p−1
shows
that in general it is much more difficult (in terms
of the number of samples needed) to obtain bounds
on CP E (S) than CRE (S)—since CRE (S) re-scales
CP E (S) by p/n, which is much less than 1. This will
be reflected in the main results below, where the scaling of CRE (S) will be a factor of p/n smaller than

CP E (S). In general, it is significantly more difficult to bound CP E (S), since kX(β − βOLS )k22 is p,
whereas kY −XβOLS k22 is n−p, and so there is much
less margin for error in approximating CP E (S).
• In the algorithmic or worst-case setting,
2
kΠU
S k2
is the relevant quansup∈Rn /{0},ΠU =0 kk
2
2

2
tity, whereas in the statistical setting kΠU
S kF is the
relevant quantity. The Frobenius norm enters in the
statistical setting because we are taking an average
over homoscedastic noise, and so the `2 norm of the
eigenvalues of ΠU
S need to be controlled. On the other
hand, in the algorithmic or worst-case setting, the
worst direction in the null-space of U T needs to be
controlled, and thus the spectral norm enters.

3. Main theoretical results
In this section, we provide upper bounds for CW C (S),
CP E (S), and CRE (S), where S correspond to random
sampling and random projection matrices. In particular, we
provide upper bounds for 4 sketching matrices: (1) a vanilla
leverage-based random sampling algorithm from Drineas
et al. (Drineas et al., 2012); (2) a variant of leverage-based
random sampling, where the random samples are not rescaled prior to their inclusion in the sketch; (3) a vanilla
random projection algorithm, where S is a random matrix containing i.i.d. sub-Gaussian random variables; and
(4) a random projection algorithm, where S is a random
Hadamard-based random projection, as analyzed in (Boutsidis & Gittens, 2013).
3.1. Random sampling methods
Here, we consider random sampling algorithms. To do so,
first define a random samplingP
matrix S̃ ∈ Rn as follows:
n
S̃ij ∈ {0, 1} for all (i, j) and j=1 S̃ij = 1, where each
row has an independent multinomial distribution with probabilities (pi )ni=1 . The matrix of cross-leverage scores is defined as L = U U T ∈ Rn×n , and `i = Lii denotes the
leverage score corresponding
the ith sample. Note that
Pto
n
the leverage scores satisfy i=1 `i = trace(L) = p and
0 ≤ `i ≤ 1.
The sampling probability distribution we consider (pi )ni=1
is of the form pi = (1 − θ) `pi + θqi , where {qi }ni=1 satisfies
Pn
0 ≤ qi ≤ 1 and i=1 qi = 1 is an arbitrary probability
distribution, and 0 ≤ θ < 1. In other words, pi is a convex
combination of a leverage-based distribution and another
arbitrary distribution. Note that for θ = 0, the probabilities
are proportional to the leverage scores, whereas for θ = 1,
the probabilities follow the distribution defined by {qi }ni=1 .
We consider two sampling matrices, one where the random
sampling matrix is re-scaled, as in Drineas et al. (Drineas

Randomized Leverage-score Sketching

et al., 2011), and one in which no re-scaling takes place.
In particular, let SN R = S̃ denote the random sampling
matrix (where the subscript N R denotes the fact that no
re-scaling takes place). The re-scaled sampling matrix is
SR ∈ Rr×n = S̃W , where W ∈qRn×n is a diagonal re1
scaling matrix, where [W ]jj =
rpj and Wji = 0 for
j 6= i. The quantity

1
pj

is the re-scaling factor.

In this case, we have the following result.

Cp
C0p
Theorem 1. For S = SR , with r ≥ (1−θ)
log (1−θ)
,
then with probability at least 0.7, it holds that
rank(SR U ) = p and that:
CW C (SR ) ≤

1 + 12

CP E (SR ) ≤

44

n
r

p
r

p
1 + 44 .
r
Several things are worth noting about this result. First, note
that both CW C (SR ) − 1 and CRE (SR ) − 1 scale as pr ; thus,
it is possible to obtain high-quality performance guarantees
for ordinary least squares, as long as pr → 0, e.g., if r is
only slightly larger than p. On the other hand, CP E (SR )
scales as nr , meaning r needs to be close to n to provide
similar performance guarantees. Next, note that all of the
upper bounds apply to any data matrix X, without assuming any additional structure on X.
CRE (SR ) ≤

Also note that the distribution {qi }ni=1 does not enter the
results which means our bounds hold for any choice of
{qi }ni=1 and don’t depend on θ. This allows to consider
different distributions. A standard choice is uniform, i.e.,
qi = n1 (see e.g. Ma et al. (Ma et al., 2015)). The other
important example is that of approximate leverage-score
sampling developed in (Drineas et al., 2012) that reduces
computation. Let (`˜i )ni=1 denote the approximate leverage
scores developed by the procedure in (Drineas et al., 2012).
Based on Theorem 2 in (Drineas et al., 2012), |`i − `˜i | ≤ θ
where 0 < θ < 1 for r sufficiently large. Now, using
˜
pi = `pi , pi can be re-expressed as pi = (1 − θ) `pi + θqi
where (qi )ni=1 is a distribution (unknown since we only
have a bound on the approximate leverage scores). Hence,
the performance bounds achieved by approximate leveraging are equivalent to those achieved by adding θ multiplied
by a uniform or other arbitrary distribution.
Next, we consider the leverage-score estimator without rescaling SN R . In order to develop nontrivial bounds on
CW C (SN R ), CP E (SN R ), and CRE (SN R ), we need to
make a (strong) assumption on the leverage-score distribution on X. To do so, we define the following.
Definition 1 (k-heavy hitter leverage distribution). A sequence of leverage scores (`i )ni=1 is a k-heavy hitter leverage score distribution if there exist constants c, C > 0 such

that for 1 ≤ i ≤ k, cp
≤ `i ≤ Cp
kP
k and for the remaining
p
n − k leverage scores, i=k+1 `i ≤ 34 .
The interpretation of a k-heavy hitter leverage distribution
is one in which only k samples in X contain the majority of the leverage score mass. The parameter k acts as a
measure of non-uniformity, in that the smaller the k, the
more non-uniform are the leverage scores. The k-heavy
hitter leverage distribution allows us to model highly nonuniform leverage scores which allows us to state the following result.
Theorem 2. For S = SN R , with θ = 0 and assuming a k-heavy
hitter leverage distribution and r ≥

c1 p log c2 p , then with probability at least 0.6, it holds
that rank(SN R ) = p and that:
44C 2 p
c2 r
4
44C k
CP E (SN R ) ≤
c2 r
44C 4 pk
CRE (SN R ) ≤ 1 + 2
.
c nr
Notice that when k  n, bounds in Theorem 2 on
CP E (SN R ) and CRE (SN R ) are significantly sharper than
bounds in Theorem 1 on CP E (SR ) and CRE (SR ). Hence
not re-scaling has the potential to provide sharper bound in
the statistical setting. However a much stronger assumption
on X is needed for this result.
CW C (SN R ) ≤ 1 +

3.2. Random projection methods
Here, we consider two random projection algorithms, one
based on a sub-Gaussian projection matrix and the other
based on a Hadamard projection matrix. To do so, define [SSGP ]ij = √1r Xij , where (Xij )1≤i≤r,1≤j≤n are i.i.d.
sub-Gaussian random variables with E[Xij ] = 0, variance
2
E[Xij
] = σ 2 and sub-Gaussian parameter 1. In this case,
we have the following result.
Theorem 3. For any matrix X, there exists a constant c
such that if r ≥ c0 log n, then with probability greater than
0.7, it holds that rank(SSGP ) = p and that:
p
CW C (SSGP ) ≤ 1 + 11
r
n
CP E (SSGP ) ≤ 44(1 + )
r
p
CRE (SSGP ) ≤ 1 + 44 .
r
Notice that the bounds in Theorem 3 for SSGP are equivalent to the bounds in Theorem 1 for SR , except that r
is required only to be larger than O(log n) rather than
O(p log p). Hence for smaller values of p, random subGaussian projections are more stable than leverage-score
sampling based approaches. This reflects the fact that to
a first-order approximation, leverage-score sampling performs as well as performing a smooth projection.

Randomized Leverage-score Sketching

Next, we consider the randomized Hadamard projection
matrix. In particular, SHad = SU nif HD, where H ∈
Rn×n is the standard Hadamard matrix (see e.g. (Hedayat
& Wallis, 1978)), SU nif ∈ Rr×n is an r × n uniform sampling matrix, and D ∈ Rn×n is a diagonal matrix with
random equiprobable ±1 entries.
Theorem 4. For any matrix X, there exists a constant c
such that if r ≥ cp log n(log p + log log n), then with probability greater than 0.8, it holds that rank(SHad ) = p and
that:
p
r
n
CRE (SHad ) ≤ 40 log(np)(1 + )
r

CW C (SHad ) ≤ 1 + 40 log(np)

p
CP E (SHad ) ≤ 1 + 40 log(np)(1 + ).
r
Notice that the bounds in Theorem 4 for SHad are equivalent to the bounds in Theorem 1 for SR , up to a constant
and log(np) factor. As discussed in Drineas et al. (Drineas
et al., 2011), the Hadamard transformation makes the leverage scores of X approximately uniform (up to log(np) factor), which is why the performance is similar to the subGaussian projection (which also tends to make the leverage
scores of X approximately uniform). We suspect that the
additional log(np) factor is an artifact of the analysis since
we use an entry-wise concentration bound; using more sophisticated techniques, we believe that the log(np) can be
improved.
3.3. Lower Bounds
In concurrent work, Pilanci and Wainwright (Pilanci &
Wainwright, 2014) amongst other results develop lower
bounds on the numerator in CP E (S) which prove that our
upper bounds on CP E (S) can not be improved. We re-state
Theorem 1 (Example 1) in Pilanci and Wainwright (Pilanci
& Wainwright, 2014) in a way that makes it most comparable to our results.
Theorem 5 (Theorem 1 in
(Pilanci & Wainwright, 2014)). For any sketching matrix satisfying
kE[S T (SS T )−1 S]kop ≤ η nr , any estimator based on
(SX, SY ) satisfies the lower bound with probability
greater than 1/2:
CP E (S) ≥

n
.
128ηr

Gaussian and Hadamard projections as well as re-weighted
approximate leverage-score sampling all satisfy the condition kE[S T (SS T )−1 S]kop ≤ η nr . On the other hand unweighted leverage-score sampling does not satisfy this condition and hence does not satisfy the lower bound which is
why we are able to prove a tighter upper bound when the
matrix X has highly non-uniform leverage scores. This

proves that CP E (S) is a quantity that is more challenging to control than CRE (S) and CW C (S) when only a single sketch is used. Using this insight, Pilanci and Wainwright (Pilanci & Wainwright, 2014) show that by using
a particular iterative Hessian sketch, CP E (S) can be controlled up to constant. In addition to providing a lower
bound on the PE using a sketching matrix just once, Pilanci and Wainwright also develop a new iterative sketcthing scheme where sketching matrices are used repeatedly
can reduce the PE significantly.

4. Discussion and conclusion
In this paper, we developed a framework for analyzing algorithmic and statistical criteria for general sketching matrices S ∈ Rr×n applied to the LS objective. Our framework reveals that the algorithmic and statistical criteria depend on different properties of the oblique projection ma†
trix ΠU
S = U (SU ) U , where U is the left singular matrix for X. In particular, the algorithmic WC criteria dekΠU
S k2
, since in that case
pends on the quantity supU T =0 kk
2
the data may be arbitrary and worst-case, whereas the two
statistical criteria (RE and PE) depends on kΠU
S kF , since in
that case the data follow a linear model with homogenous
noise variance.
Using our framework we develop upper bounds for three
performance criterion applied to 4 sketching schemes. Our
upper bounds reveal that in the regime where p < r  n,
our sketching schemes achieve optimal performance up to
constant in terms of WC and RE. On the other hand, the
PE scales as nr meaning r needs to be close to n for good
performance; and subsequent lower bounds in Pilanci and
Wainwright (Pilanci & Wainwright, 2014) show that this
upper bound can not be improved.

References
Avron, H., Maymounkov, P., and Toledo, S. Blendenpik:
Supercharging LAPACK’s least-squares solver. SIAM
Journal on Scientific Computing, 32:1217–1236, 2010.
Boutsidis, C. and Gittens, A. Improved matrix algorithms
via the subsampled randomized Hadamard transform.
SIAM Journal of Matrix Analysis and Applications, 34:
1301–1340, 2013.
Chatterjee, S. and Hadi, A. S. Influential observations, high
leverage point, and outliers in linear regression. Statistical Science, 1(3):379–416, 2006.
Chatterjee, S. and Hadi, A.S. Sensitivity Analysis in Linear
Regression. John Wiley & Sons, New York, 1988.
Drineas, P., Mahoney, M.W., and Muthukrishnan, S. Sampling algorithms for `2 regression and applications. In

Randomized Leverage-score Sketching

Proceedings of the 17th Annual ACM-SIAM Symposium
on Discrete Algorithms, pp. 1127–1136, 2006.
Drineas, P., Mahoney, M. W., Muthukrishnan, S., and Sarlos, T. Faster least squares approximation. Numerical
Mathematics, 117:219–249, 2011.
Drineas, P., Magdon-Ismail, M., Mahoney, M. W., and
Woodruff, D. P. Fast approximation of matrix coherence
and statistical leverage. Journal of Machine Learning
Research, 13:3475–3506, 2012.
Golub, G.H. and Loan, C.F. Van. Matrix Computations.
Johns Hopkins University Press, Baltimore, 1996.
Hample, F. R., Ronchetti, E. M., Roisseeuw, P. J., and Stahel, W. A. Robust Statisitcs: The Approach Based on
Influence Functions. John Wiley & Sons, New York,
1986.
Hedayat, A. and Wallis, W. D. Hadamard matrices and
their applications. Annals of Statistics, 6(6):1184–1238,
1978.
Hoaglin, D.C. and Welsch, R.E. The hat matrix in regression and ANOVA. The American Statistician, 32(1):17–
22, 1978.
Huber, P. J. and Ronchetti, E. M. Robust Statisitcs. John
Wiley & Sons, New York, 1981.
Lehmann, E. L. Elements of Large-Sample Theory.
Springer Verlag, New York, 1998.
Ma, P., Mahoney, M. W., and Yu, B. A statistical perspective on algorithmic leveraging. JMLR, 2015. To appear.
Mahoney, M. W. Randomized algorithms for matrices
and data. Foundations and Trends in Machine Learning. NOW Publishers, Boston, 2011. Also available at:
arXiv:1104.5557.
Meng, X., Saunders, M. A., and Mahoney, M. W. LSRN:
A parallel iterative solver for strongly over- or underdetermined systems. SIAM Journal on Scientific Computing, 36(2):C95–C118, 2014.
Meyer, Carl D. Matrix Analysis and Applied Linear Algebra. SIAM, 2000.
Pilanci, M. and Wainwright, M. J. Iterative Hessian
sketch: Fast and accurate solution approximation for
constrained least-squares.
Technical report, 2014.
Preprint: arXiv:1411.0347.
Raskutti, G. and Mahoney, M. W. A statistical perspective on randomized sketching for ordinary least-squares.
Technical report, 2014. Preprint: arXiv:1406.5986.

Royall, R. M. On finite population sampling theory under
certain linear regression models. Biometrika, 57:377–
387, 1970.
Zavlavsky, A. M., Zheng, H., and Adams, J. Optimal sample allocation for design-consistent regression in a cancer services survey when design variables are known for
aggregates. Survey Mehodology, 34(1):65–78, 2008.

