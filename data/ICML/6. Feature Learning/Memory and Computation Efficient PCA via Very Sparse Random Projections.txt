Memory and Computation Efficient PCA via Very Sparse Random Projections
Farhad Pourkamali-Anaraki
FARHAD . POURKAMALI @ COLORADO . EDU
Shannon M. Hughes
SHANNON . HUGHES @ COLORADO . EDU
Department of Electrical, Computer, and Energy Engineering, University of Colorado at Boulder, CO, 80309, USA

Abstract
Algorithms that can efficiently recover principal
components in very high-dimensional, streaming, and/or distributed data settings have become
an important topic in the literature. In this paper, we propose an approach to principal component estimation that utilizes projections onto very
sparse random vectors with Bernoulli-generated
nonzero entries. Indeed, our approach is simultaneously efficient in memory/storage space, efficient in computation, and produces accurate PC
estimates, while also allowing for rigorous theoretical performance analysis. Moreover, one can
tune the sparsity of the random vectors deliberately to achieve a desired point on the tradeoffs between memory, computation, and accuracy. We rigorously characterize these tradeoffs and provide statistical performance guarantees. In addition to these very sparse random
vectors, our analysis also applies to more general random projections. We present experimental results demonstrating that this approach allows for simultaneously achieving a substantial
reduction of the computational complexity and
memory/storage space, with little loss in accuracy, particularly for very high-dimensional data.

1. Introduction
Principal component analysis (PCA) is a fundamental tool
in unsupervised learning and data analysis that finds the
low-dimensional linear subspace that minimizes the meansquared error between the original data and the data projected onto the subspace. The principal components (PCs)
can be obtained by a singular value decomposition (SVD)
of the data matrix or eigendecomposition of the data’s covariance matrix. PCA is frequently used for dimensionality
reduction, feature extraction, and as a pre-processing step
for learning and recognition tasks such as classification.
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

There is a wealth of existing literature that develops computationally efficient approaches to computing these PCs.
However, the overwhelming majority of this literature assumes ready access to the stored full data samples.
However, this full data access is not always possible in
modern data settings. Modern data acquisition capabilities have increased massively in recent years, which can
lead to a wealth of rapidly changing high-dimensional data.
Hence, in very large database environments, it may not
be feasible or practical to access all the data in storage
(Muthukrishnan, 2005).
Moreover, in applications such as sensor networks, distributed databases, and surveillance, data is typically distributed over many sensors. Accessing all the data at once
requires tremendous communication costs between the sensors and a central processing unit. Algorithms that don’t
require access to all the data can help reduce this communication cost (Balcan et al., 2013). A third case is streaming
data, where one must acquire and store the data in real time
to have full access, which may not be feasible.
One promising strategy to address these issues in a computationally efficient way, which also allows for rigorous
theoretical analysis, is to use very sparse random projections. Random projections provide informative lowerdimensional representations of high-dimensional data,
thereby saving memory and computation. They are widely
used in many applications, including databases and data
stream processing (Li et al., 2006; Indyk, 2006) and compressive sensing (Donoho, 2006).
Initial attempts have been made to perform PCA using only
the information embedded in random projections. Unfortunately, however, theoretical guarantees have generally only
been given for random vectors with i.i.d. entries drawn
from the Gaussian distribution. This common choice is
convenient in terms of theoretical analysis, but undesirable
in practice. Such dense random vectors require relatively
high storage space, and high computation because of the
large amount of floating point arithmetic needed to compute each projection.
In this paper, we instead aim to recover PCs from very

Efficient PCA via Very Sparse Random Projections

sparse random projections with Bernoulli entries. These
sparse random projections can be implemented using simple database operations. For example, this type of random
projection can be obtained by simply adding two small subsets of the entries of a data sample and then subtracting the
results. They thus require little computation or data access. For distributed data, this type of sparse Bernoulli projection could be obtained via localized aggregation in the
network requiring minimal communication (assuming all
sensors can communicate with one another). (If a network
topology must be respected, the sparse random projections
could presumably be adjusted accordingly, but we have not
yet analyzed this case.) In short, very sparse random projections are or could potentially be extremely practical for
a variety of situations.
Our theoretical analysis begins by assuming a probabilistic
generative model for the data, related to the spiked covariance model. Under this model, we show that PCs computed
from very sparse random projections are close estimators
of the true underlying PCs. Moreover, one can adjust the
sparsity of the random projections as desired to greatly reduce memory and computation (at the cost of some accuracy). We give rigorous theoretical analysis of the resulting tradeoffs between memory, computation, and accuracy as we vary sparsity, showing that efficiency in memory and computation may be gained with little sacrifice in
accuracy. In fact, our analysis will also apply more generally to any random projections with i.i.d. zero mean entries
and bounded second-, fourth-, sixth- and eighth-order moments, although we focus on the sparse-Bernoulli case.
In Section 2, we present a brief review of related work.
The model assumptions and notation are in Section 3. We
present an overview of the main contributions in Section
4. In Section 5, the main results are stated with some discussion of their consequences. Proofs are reserved to the
supplementary material. Finally, we present experimental
results demonstrating the performance and efficiency of our
approach compared with prior work in Section 6.

2. Related Work
Algorithms that can efficiently recover PCs from a collection of full data samples have been an important topic in the
literature for decades. A comprehensive survey of these algorithms can be found in (Halko et al., 2011b; Gilbert et al.,
2012) and the references therein. This includes several
lines of work. The first involves techniques that are based
on dimensionality reduction, sketching, and sub-sampling
for low-rank matrix approximation such as (Halko et al.,
2011a). In these methods, the computational complexity is
typically reduced by performing SVD on the smaller matrix obtained by sketching or subsampling. However, these
methods require accessible storage of all the data samples.
This may not be practical for modern data processing ap-

plications where data samples are too vast or generated too
quickly to be stored accessibly.
The second line of work involves online algorithms specifically tailored to have extremely low-memory complexity
such as (Arora et al., 2012) and the references therein. Typically, these algorithms assume that the data is streaming
by, that real-time PC estimates are needed, and they obtain these by solving a stochastic optimization problem,
in which each arriving data sample is used to update the
PCs in an iterative procedure. As a couple recent examples of this line of work, (Mitliagkas et al., 2013) show
that a blockwise stochastic variant of the power method can
recover PCs in this low-memory setting from O(p log p)
samples, although the computational cost is not examined.
Meanwhile, (Arora et al., 2013) bound the generalization
error of PCs learned with their algorithm to new data samples and also analyze its computational cost.
Our problem lies somewhere between the above two lines
of work. We don’t assume that memory/data access is not a
concern, but at the same time, we also don’t assume the extremely restrictive setting where one-sample-at-a-time realtime PC updates are required. Instead, we aim to reduce
both memory and computation simultaneously for PCA
across a broad class of big data settings, e.g. for enormous
databases where loading into local memory may be difficult or costly, for streaming data when PC estimates do
not have to be real-time, or for distributed data. We also
aim to provide tunable tradeoffs for the amount of accuracy
that will be sacrificed for each given reduction in memory/computation, in order to aid in choosing a desired balance point between these.
To do this, we recover PCs from random projections. There
have been some related prior attempts to extract PCs from
random projections of data (Fowler, 2009; Qi and Hughes,
2012). In both, the problem of recovering PCs from random projections has been considered only for dense Gaussian random projections. However, dense vectors are undesirable for practical applications since they require relatively high storage space and computation (including lots
of floating point arithmetic) as noted in the introduction.
Our work will make use of sparse random vectors with
Bernoulli entries which will be more efficiently implementable in a large database environment.
Chen et al. (2013) have estimated the covariance matrix of data from general sub-Gaussian random projections
to reduce memory use. However, convergence guarantees are given only for the case of infinite data samples,
making it hard to realistically use these results in memory/computation vs. accuracy tradeoffs, and computational
cost is not examined. We will address both these issues.
As a final note, we observe that our work also can be

Efficient PCA via Very Sparse Random Projections

viewed as an example of emerging ideas in computational
statistics (see (Chandrasekaran and Jordan, 2013)) in which
tradeoffs between computational complexity, dataset size,
and estimation accuracy are explicitly characterized, so that
a user may choose to reduce computation in very highdimensional data settings with knowledge of the risk to the
accuracy of the result.

3. Problem Formulation and Notation
In this paper, we focus on a statistical model for the data
that is applicable to various scenarios. Assume that our
original data in Rp are centered at x2Rp and {vi }di=1 2
Rp are the d orthonormal PCs. We consider the following probabilistic generative model for the data samPd
ples, xi =x + j=1 wij j vj + zi , i=1, . . . , n, where
{wi }ni=1 and {zi }ni=1 are drawn i.i.d. from N (0, Id⇥d )
2
and N (0, ✏p Ip⇥p ), respectively. Also, { i }di=1 are scalar
constants reflecting the energy of the data in each principal direction such that 1 > 2 >. . .> d >0. The additive noise term zi allows for some error in our assumptions. Note that the underlying covariance matrix of the
Pd
data is Ctrue , j=1 j2 vj vjT , and the signal-to-noise raPd
tio is SNR= ✏h2 , where h, j=1 j2 . In fact, this model is
related to the spiked covariance model (Johnstone, 2001)
in which the data’s covariance matrix is assumed to be a
low-rank perturbation of the identity matrix.
We then introduce a very general class of random projections. Assume that matrices {Ri }ni=1 2Rp⇥m , m<p, are
formed by drawing each of their i.i.d. entries from a distribution whose mean µ1 is assumed to be zero and whose k th
order moments, µk , are assumed finite for k = 2, 4, 6, 8. In
particular, we will be interested in a popular class of sparse
random projections, but our analysis will apply to any distribution satisfying these assumptions.
Each random projection yi 2Rm is then obtained by taking
inner products of the data sample xi 2Rp with the random
vectors comprising the columns of Ri , i.e. yi =RTi xi . The
main goal of this paper is to provide theoretical guarantees
for estimating the center and PCs of {xi }ni=1 from these
random projections.

4. Our Contributions
In this paper, we introduce two estimators for the center
and underlying covariance matrix of data {xi }ni=1 from
sparse random projections {yi =RTi xi }ni=1 . In typical
PCA, theP
center is estimated using the empirical center
n
xemp = n1 i=1 xi . PCs are then obtained by eigendecomposition
of the empirical covariance matrix Cemp =
Pn
1
xemp )(xi xemp )T , that typically comes
i=1 (xi
n
close to the true covariance matrix (Vershynin, 2012).
Similar to typical PCA, we show that the empirical center
and empirical covariance matrix of the new data samples
{Ri yi }ni=1 (scaled by a known factor) result in accurate

estimates of the original center x, and the true underlying
covariance matrix Ctrue . (Note that Ri yi approximately
represents a projection in Rp of xi onto the column space
of Ri , but we have eliminated a computationally expensive
matrix inverse here.) We will provide rigorous theoretical
analysis for the performance of these estimators in terms of
parameters such as the measurement ratio m/p, number of
samples n, SNR, and moments µk .
Our approach is quite general and we believe it can eventually be applicable to various data processing applications
in which the data is very high-dimensional, streaming, or
distributed. Particularly for the case of distributed data, we
may need to adjust the set-up to ensure the random projections respect network topology, but we believe it could be
done following the strategies in (Wang et al., 2012a;b).
We will be particularly interested in applying our general
distribution results to the case of very sparse measurement
matrices. Achlioptas (2001) first showed that, in the classic
Johnson–Lindenstrauss result on pairwise distance preservation, the dense Gaussian projection matrices can be replaced with sparse projection matrices, where each entry
is distributed on { 1, 0, 1} with probabilities { 16 , 23 , 16 },
achieving a three-fold speedup in processing time. Li et al.
(2006) then drew each entry from { 1, 0, 1} with prob1
1 1
abilities { 2s
,1
s , 2s }, achieving a more significant sfold speedup in processing time. In this paper, we refer to
this second distribution as a sparse-Bernoulli distribution
with sparsity parameter s. Sparse random projections have
been applied in many other applications to substantially reduce computational complexity and memory requirements
(Omidiran and Wainwright, 2010; Zhang et al., 2012).
Motivated by the success of these methods, we propose to
recover PCs from sparse random projections of the data,
in which each entry of {Ri }ni=1 is drawn i.i.d. from the
sparse-Bernoulli distribution. In this case, each column of
{Ri }ni=1 has ps nonzero entries, on average. This choice
has the following properties simultaneously:
• The computation cost for obtaining each projection
is O( mp
s ) and thus the cost to acquire/access/hold in
memory the data needed for the algorithm is O( mpn
s ).
Specifically, we are interested in choosing m and s
so that the compression factor , m
s <1. In this case,
our framework requires significantly less computation
cost and storage space. First, the computation cost
to acquire/access each data sample is O( p), <1, in
contrast to the cost for acquiring each original data
sample O(p). This results in a substantial cost reduction for the sensing process, e.g. for streaming
data. Second, once acquired, observe that the projected data samples {Ri yi }ni=1 2Rp will be sparse,
having at most O( p) nonzero entries each. This results in a significant reduction, O( pn) as opposed to

Efficient PCA via Very Sparse Random Projections

O(pn), in memory/storage requirements and/or communication cost, e.g. transferring distributed data to a
central processing unit.
• Given the sparse data matrix formed by {Ri yi }ni=1 ,
one can make use of efficient algorithms for performing (partial) SVD on very large sparse matrices, such
as the Lanczos algorithm (Golub and Van Loan, 2012)
and svds in MATLAB. In general, for a p ⇥ n matrix,
the computational cost of SVD is O(p2 n). However,
for large sparse matrices such as ours, the cost can be
reduced to O( p2 n) (Lin and Gunopulos, 2003).
In the remainder of this paper, we will characterize the accuracy of the estimated center and PCs in terms of m, p,
n, SNR, moments of the distribution (which for sparseBernoulli will scale with s), etc. As we will see, under certain conditions on the PCs, we may choose as
low as
/ p1 for constant accuracy. Hence, assuming
n = O(p) samples, the memory/storage requirements for
our approach can scale with p in contrast to p2 for standard algorithms that store the full data, and a similar factor
of p savings in computation can be achieved compared with
regular SVD. Less aggressive savings will also be available
for other PC types.

5. Main Results
We present the main results of our work in this section,
with all proofs delayed to the supplemental material. Interestingly, we will see that the shape of the distribution for
each entry of {Ri }ni=1 plays an important role in our results. The kurtosis, defined as , µµ42 3, is a measure of
2
peakedness and heaviness of tail for a distribution. It can
also be thought of as a measure of non-Gaussianity, since
the kurtosis of the Gaussian distribution is zero. It turns out
that the distribution’s kurtosis is a key factor in determining
PC estimation accuracy. For sparse-Bernoulli, the kurtosis
increases with increasing sparsity parameter s.
5.1. Mean and Variance of Center Estimator
Theorem 1. Assume that {Ri }ni=1 , {xi }ni=1 , {yi }ni=1 , m,
n, and µ2 are as defined in SectionP3, and define the nbn = 1 1 n Ri yi . Then, the
sample center estimator x
i=1
mµ2 n
bn is the true center of the original
mean of the estimator x
bn ]=x, for all n, including the base case
data x, i.e. E[x
bn converges
n=1. Furthermore, as n ! 1, the estimator x
b
to the true center: limn!1 xn =x.
We see that the empirical center of {Ri yi }ni=1 is a (scaled)
unbiased estimator for the true center x. Note that this theorem does not depend on the number of projections m or
sparsity parameter s, and thus does not depend on , as
a sufficiently high number of samples will compensate for
unfavorable values of these parameters. We further note
that, when n ! 1, there is no difference between the
Gaussian, very sparse, or other choices of random projections. This is consistent with the observation that random

projection matrices consisting of i.i.d. entries must only be
zero mean to preserve pairwise distances in the JohnsonLindenstrauss theorem (Li et al., 2006).
Theorem 2. Assume that {Ri }ni=1 , {xi }ni=1 , {yi }ni=1 , m,
n, p, µ2 , h, and SNR are as defined in Section 3, and kurtosis  is as defined above. Then,
Pthe variance of the unbiased
bn = 1 1 n Ri yi is
center estimator x
✓ mµ
✓ 2 n i=1◆ ✓
◆
⇣ ⌘
1
m +1
1
bn =
Var x
h
1
+
1
+
+
nm
SNR
p
p
p
✓
◆
◆
+1
2
+ 1+
kxk2 .
(5.1)
p

We see that as the number of samples n and measurement
ratio m/p increase, the variance of this estimator decreases
at rate n1 and close to m1/p . Interestingly, the power of the
Pd
signal, i.e. h= j=1 j2 , works against the accuracy of the
estimator. The intuition for this is that, for the center estimation problem, it is desirable to have all the data samples close to the center, which happens for small h. For
sparse random projections, we observe that the kurtosis is
s
=s 3 and thus +1
p t p . Hence, variance scales with
increasing sparsity, although sufficient data samples n are
enough to combat this effect. Indeed, when s>p, the variance increases heavily since many of the random vectors
are zero, and thus the corresponding projections cannot
capture any information about the original data. Overall,
this result shows an explicit tradeoff between reducing n or
increasing s to reduce memory/computation and the variance of the resulting estimator. Finally, given this mean and
variance, probabilistic error bounds can be immediately obtained via Chebyshev, Bernstein, etc. inequalities.
5.2. Mean and Variance of Covariance Estimator
Theorem 3. Assume that {Ri }ni=1 , {xi }ni=1 , {yi }ni=1 , m,
n, p, µ2 , h, ✏, and Ctrue are as defined in Section 3, and 
is the kurtosis. Moreover, assume that {xi }ni=1 are centered
b n=
at x=0. Define
covariance estimator C
Pn the n-sample
1
1
T T
i=1 Ri yi yi Ri . Then, for all n, the mean
(m2 +m)µ22 n
b n ]=C
b true + E, where C
b true ,
of this estimator is: E[C

h

2
Ctrue + ↵Ip⇥p , ↵, m+1
+ ( p(m+1)
+ (m+p+1)
p(m+1) )✏ , and
P
d

2
T
E , m+1
j=1 j diag(vj vj ), where diag(A) denotes
the matrix formed by zeroing all but the diagonal entries of
b true + E. Then, as n ! 1,
A. Furthermore, let C1 ,C
b
b n =C1 .
the estimator Cn converges to C1 : limn!1 C
b
We observe that the limit of the estimator Cn has two comb true , has the same eigenvectors with
ponents. The first, C
slightly perturbed eigenvalues (↵ tends to be very small in
high dimensions) and the other, E, is an error perturbation
term. Both ↵ and E scale with the kurtosis, reflecting the
necessary tradeoff between increasing sparsity (decreasing
memory/computation) and maintaining accuracy.

We first consider a simple example to gain some intuition
1000
for this theorem. A set of data samples {xi }3000
i=1 2 R

Efficient PCA via Very Sparse Random Projections

(m +m)µ2

sented by blue dots and red circles respectively. We see
that the projected data samples are scattered somewhat into
other directions for all four cases. However, the amount of
scattered energy for the Gaussian and sparse-Bernoulli for
s=3 is quite small. This can be easily verified from the
fact that the amount of perturbation depends on the kurtosis, and for both cases the kurtosis is =0. As we increase the parameter s, the kurtosis =s 3 gets larger,
and this is consistent with the observation that the projected data samples get more scattered into other directions.
We also note the similarity of our findings to (Li et al.,
2006)’s result that the variance of the pairwise distances
in Johnson–Lindenstrauss depends on the kurtosis of the
distribution being used for random projections. Despite the
perturbation, in all cases, the PC can be recovered accurately.
p Note also that scaling the projected data points by
1/ (m2 + m) µ22 preserves the energy in the direction of
the PC (i.e. the eigenvalue).

b true have the same
In Theorem 3, we see that Ctrue and C
set of eigenvectors with the eigenvalues of Ctrue increased
1

1
1
by ↵=h{ m+1
+ ( p(m+1)
+ p1 + m+1
) SNR
}. Thus, ↵ is
a decreasing function of p, m/p and SNR, and in particular
goes to 0 as p ! 1 for constant projection ratio m/p. This
is illustrated in Fig. 5.2. Thus, surprisingly, in the highdimensional regime, the amount of perturbation of eigenvalues becomes increasingly negligible even for small measurement ratios.
Now, let’s examine the error matrix E. We observe that
E can be viewed as representing a bias of the estimated
PCs towards the nearest canonical basis vectors; it stems
from anisotropy in the distribution for Ri when this is nonGaussian (note  = 0, and thus E = 0, for the Gaussian
case). In later sections, we will use the 2-norm of E, kEk2 ,
to bound the angle between the estimated and true PCs. InkEk
deed, we find, for constant , h 2 , the same angular PC
estimation error is achieved. We now study kEk2 , leading
to useful observations, for several types of PCs. (An expanded discussion with full derivations is included in the
supplementary materials.)
(1) Smooth PCs: It has frequently been observed that
sparse-Bernoulli random projections are most effective on
vectors that are “smooth” (Ailon and Chazelle, 2009),
meaning that their maximum entry is of size O( p1p ). Large
images, videos, and other natural signals with distributed
energy are obvious examples of this type. (Other sig-

0.1

0.1
p=200
p=500
p=1000
p=5000

0.08
0.06

0.06

0.04

0.04

0.02

0.02

0
0.1

0.2

0.3

Measurement Ratio m/p

0.4

p=200
p=500
p=1000
p=5000

0.08

α/h

α/h

are generated from one PC. We also generate the mea1000⇥200 m
surement matrices {Ri }3000
( /p = 0.2) with
i=1 2R
i.i.d. entries both for the Gaussian distribution and the
sparse-Bernoulli distribution for various values of the sparsity parameter s. In Fig. 5.1, we view two dimensions
(the original PC’s and one other) of the data {xi }3000
i=1 and
the scaled projected data p 21
{Ri yi }3000
i=1 , repre2

0
0.1

0.2

0.3

0.4

Measurement Ratio m/p

(a)
(b)
Figure 5.2. Variation of the parameter ↵
for (a)  = 0 and (b)  =
h
200, varying p and measurement ratio m/p, and fixed SNR = 5.

nals are often preconditioned to be smooth via multiplication with a Hadamard conditioning matrix.) We may eas

ily observe then that kEk2  m+1
µ2max h, or  m+1
µ2max ,
where µmax is the mutual coherence (Elad, 2007) between

the PCs and the canonical basis, and we note m+1
 1.
As we will see in Section 5.3, we will want to keep small
enough to guarantee a certain fixed angular error ✓0 . In fact,
this can be satisfied by requiring
C(✓0 )µ2max , where
C(✓0 ) is a constant depending on the error ✓0 . Hence, for
smooth PCs, we need only have / p1 , reducing memory
and computation by a rather remarkable factor of p.
(2) All Sparse PCs:
In the case of all sparse

PCs, we may write E as E= m+1
Ctrue + E0
p

4
where
kE0 k2  m+1 1 µmin h
and
µmin ,
min1id max1jp |hvi , ej i| represents the closeness of the PCs to the canonical basis {ej }pj=1 . Thus,
unlike for other sparse-Bernoulli applications, we find that
sparse PCs can still be recovered very well here, although
the eigenvalues may be heavily scaled by the known factor

1 + m+1
. Doing this, and taking E0 as the resulting error
p
term, we can let / 1 µ4min to maintain constant .
(3) Neither Sparse nor Smooth PCs: In this case, we can
still apply the analysis for case (1), just with a larger µ2max
and less aggressive memory/computation savings.
(4) Mixture of PC Types: In this case, we may split E into
two error matrices, associated with each of the sparse and
non-sparse PCs. Recovery of the d-dimensional PC subspace still performs well here. However, if the eigenvalues
{ j2 }dj=1 do not decay sufficiently fast, scaling of the eigenvalues for the sparse PCs may reorder the individual components. Please see the supplementary material for further
discussions and simulations.
Theorem 4. Assume that {Ri }ni=1 , {xi }ni=1 , {yi }ni=1 ,
m, n, p, µk , h, and SNR are as defined in Secb n=
tion
the covariance matrix estimator C
Pn3. Consider
1
1
T T
i=1 m(m+1)µ22 Ri yi yi Ri . Then, the deviation of our
n
n-sample estimator from its mean value is upper bounded:

2
1
b n C1
E C
 (⌧1 ⌧2 ) h2
(5.2)
n
F
n
⇣
⌘o
e
1 2
1
1 2
where ⌧1 ,⇠ 1 + SNR
+ 2 hh2 + p2 SNR
+ p1 SNR
,
✓
⇣
⌘2 ◆ ⇣
⌘
e


⌧2 , hh2 p p 1 + p1 1 + m+1
+ 2 + p + 2 m+1
0,

4

4

2

2

2

2

0

−2

0

−2

−4
−4

−2

0

2

−4
−4

4

Dimension 2

4

Dimension 2

4

Dimension 2

Dimension 2

Efficient PCA via Very Sparse Random Projections

0

−2

−2

Dimension 1

0

2

4

0

−2

−4
−4

−2

Dimension 1

0

2

−4
−4

4

−2

Dimension 1

0

2

4

Dimension 1

(a) Gaussian
(b) s = 3
(c) s = 20
(d) s = 50
Figure 5.1. Accurate recovery of the PC under random projections using both Gaussian and sparse random projection matrices for
p⇥m
various values of s. In each figure, there are n=3000 data samples uniformly distributed on a line in R1000 . {Ri }n
,
i=1 2R
m/p=0.2, are generated with i.i.d. entries drawn from (a) N (0, 1) and (b,c,d) the sparse-Bernoulli distribution for s=3, 20, 50. In
n
each
p figure, we view two dimensions (the original PC’s and one other) of the data {xi }i=1 (blue dots) and the scaled projected data
1/ (m2 + m) µ22 {Ri RTi xi }n
(red
circles).
We
observe
that,
in
all
cases,
the
projected
data samples are symmetrically distributed
i=1
around the PC, and the inner product magnitude between the PC estimated from the projected data and the true PC is at least 0.998.

Pd
e
h, j=1

4
j,

and ⇠ = max(⇠1 , ⇠2 ), where
✓
◆
◆
2 ✓
µ8/µ4
µ6/µ3
2
(µ4/µ22 )
1
2
2
⇠1 
+
4+ m
+
3+ m
m3
m2
/p
m2
/p
! ✓
◆2
2
µ4/µ
6
1
1
3
2
+
6+ m +
+ 1+ m
+
2
m
m
/p (m/p)2
/p
p ( /p)

and
⇠2 

µ6/µ3
2

m2

↵
h

6
pm/p

◆

+

(µ4/µ22 )
m2

2

✓

1+

◆

5
pm/p
! ✓

26
2
13
1
+
+
+ 1+ m
m
pm/p m/p p (m/p)2
/p
10
7
13
2
+ m +
+
2 +
3.
m
m
p /p p2 (m/p)2
p ( /p)
p ( /p)
+

µ4/µ2

✓

=

2

2+

Note that ⇠ has various terms that scale with p1 ,
the higher order moments µ8/µ42 , µ6/µ32 , and µ4/µ22 .

1
m/p

◆2

, and

We see that as the number of data samples n increases,
the variance decreases at rate n1 , converging quickly to the
limit. Moreover, the variance of our estimator is a decreasing function of the measurement ratio m/p and SNR. We
further note that the parameter ⇠ gives us important information about the effect of the tails of the distribution on the
convergence rate of the covariance estimator. More preµ8/µ4
cisely, for sparse random projections, we see that m3 2 =
µ6/µ3

(µ4/µ2 )2

µ4/µ2

s 3
) = 13 , m2 2 = m22 = 12 , and m 2 = 1 . Hence, for
(m
a fixed number of data samples, decreasing the compression factor leads to an increase of the variance and a loss
in accuracy, as we will see in Section 6. This is as we
would expect since there is an inherent tradeoff between
saving computation and memory and the accuracy. However, characterizing this tradeoff allows to be chosen in
an informed way for large datasets.
5.3. Memory, Computation and PC Accuracy Tradeoffs
We now use the covariance matrix estimator results to
bound the error of its eigenvalues and eigenvectors, using
related results from matrix perturbation theory.

First, note that using the variance of our estimator (Eq. 5.2)
b n C1 ", with
in the Chebyshev inequality yields C
probability at least 1
bn
C

bn
 C

b true
C

C1

2

1
n"2 (⌧1

bn
 C

F

C1

⌧2 )h2 . Hence,
2

+ C1

+ kEk2  kEk2 + "

F

b true
C

2

(5.3)

1
with probability at least 1
⌧2 ) h2 . In fact,
n"2 (⌧1
Eq. 5.3 can be used to characterize tradeoffs between memory, computation, and PC estimation accuracy (as an angle
between estimated subspaces) in terms of our parameters
n, m/p, etc. For simplicity in what follows and to help keep
the intuition clear, we focus on the case where the number
of samples n ! 1 and " ! 0 in Eq. 5.3 above. However,
it is trivial to adjust these results to the case of finite n by
including a nonzero " in the derivations that follow.

For illustrative purposes, we start by analyzing the case of a
single PC and use the following Lemma. In the following,
(A) and i (A) denote the set of all eigenvalues and the
ith eigenvalue of A, respectively.
Lemma 5. (Hogben, 2006; Davis and Kahan, 1970) Supe
pose A is a real symmetric matrix and A=A
+ E is the
e
e ) is an exact eigenpair
perturbed matrix. Assume that ( , v
e where ke
of A
vk2 = 1. Then
(a) e

 kEk2 for some eigenvalue

of A.

(b) Let
be the closest eigenvalue of A to e and v
be its associated eigenvector with kvk2 =1, and let ⌘=
min 2 (A), 6= e
0 . If ⌘ > 0, then
0

0

sin \ (e
v, v) 

kEk2
⌘

(5.4)

where \ (e
v, v) denotes the canonical angle between the
two eigenvectors.
We will use this Lemma to bound the angle between the
b n and the true PC in the single PC
PC estimate from C

Efficient PCA via Very Sparse Random Projections

2

kEk2 =(1

)

2

.

bn
C

1

.

0.05
2000

4000

(5.5)

(5.6)

This equation allows us to characterize the statistical tradeoff between the sparsity parameter s and the accuracy of
kEk
the estimated PC. Observe that this is the same = h 2
that we discussed in Section 5.2. To ensure fixed maximum
angular error for PC estimation, i.e. sin \(e
v, v)sin ✓0 ,
sin ✓0
we should choose such that  1+sin
.
For smooth
✓0
PCs, we may satisfy this by choosing
C(✓0 )µ2max for
✓0
C(✓0 ), 1+sin
O( p1 ). Hence, the memsin ✓0 , which gives
ory/storage requirements of our method can scale with p
in contrast to standard algorithms that scale with p2 , while
the computational complexity of SVD can scale with p2 as
opposed to p3 . Although the smooth case is of special interest, less aggressive, but still substantial, savings are also
available for other PC types.
For the general case of d PCs, we consider the eigendecomb n and C
b true :
position of the perturbed matrix C

 T
⇥
⇤ S1 0
V1
b true = V1 V2
C
0 S2
V2T
"
#"
#
h
i S
e1 0
eT
V
1
bn = V
e1 V
e2
C
e2
eT .
0 S
V
2
The distance between each perturbed eigenvalue
and the corresponding original eigenvalue depends
on the amount of perturbation. We now have that
b
b
j ( Cn )
j (Ctrue ) kEk = h for all j=1, . . . , d.
2

Moreover, it is possible to quantify the rotation of eigenvectors using the notion of canonical angle matrix defined
e 1 2Rp⇥d are
in (Davis and Kahan, 1970). Note that V1 , V
the first (true and estimated) PCs. The canonical angles between them are defined as ✓i =arccos ⇢i , where {⇢i }di=1 are
eTV
e 1 ) 1/2 V
e T V1 (VT V1 ) 1/2 , in
the singular values of (V
1
1
1
T
e
our case, just V1 V1 . The canonical angle matrix is then
e 1 , V1 )=diag(✓1 , . . . , ✓d ). Based on the redefined as ⇥(V
sults given in (Davis and Kahan, 1970; Gilbert et al., 2012):
e 1 , V1 )  kEk2
sin ⇥(V
⌘
2

6000

8000

0.95
0.9
0.85
0.8
0.75

γ=1/50
γ=1/100
γ=1/200

0.7
0.65

10000

2000

4000

Dimension (p)

↵

We then get the following tradeoff between the accuracy of
the estimated eigenvector and the parameters of our model:
sin \ (e
v, v) 

0.1

Inner Product Magnitude

⌘

n=2*p,γ =1/50
n=2*p, γ=1/100
n=3*p,γ=1/50
n=3*p, γ=1/100

0.15

6000

(a)

10000

(b)
2

0.4

10
γ=1/50
γ=1/100
γ=1/200

0.3

γ=1/50
γ=1/100
γ=1/200
SVD

1

0.2
0.1
0
2000

8000

Dimension (p)

Time in sec.

i=2,...,p

1

⇣

Normalized Singular Value Error

. We find the parameter ⌘:
⇣ ⌘
⇣
⌘
b
b
⌘ = min
1 Cn
i Ctrue =
2

kEk2 =

0.2

Normalized Center Error

case. Since Ctrue has only one eigenpair ( 2 , v) with
b true has an eigenpair ( 2+ ↵, v) and
nonzero eigenvalue, C
b
i (Ctrue )=↵, i=2, . . . , p. From Lemma 5, we see that
b n satisfies 1 (C
b n ) ( 2+ ↵) 
the largest eigenvalue of C

10

0

10

−1

10

−2

4000

6000

8000

10
2000

10000

Dimension (p)

4000

6000

8000

10000

Dimension (p)

(c)
(d)
Figure 6.1. Results for synthetic data: (a) normalized estimation
error for the center for varying n and , (b) magnitude of the inner
product between the estimated and true PC for varying , (c) normalized estimation error for for varying , and (d) computation
time to perform the SVD for the original vs. randomly projected
data for varying .

where ⌘,min1id,1jp

(S1 )ii

d

ing the same logic as in 5.5, we find ⌘

⇣

f2
S
2
d

⌘

jj

> 0. Ush. Hence,

2
d

choosing s, m, etc. such that satisfies < h , the maxie 1 and V1 satisfies
mum canonical angle between V
sin ✓i 

2
d

,

i = 1, . . . , d.

(5.7)

h

This is the same form we saw in Eq. 5.6. Hence, for smooth
PCs, we may again choose / p1 .

6. Experimental Results

In this section, we examine the tradeoffs between memory,
computation, and accuracy for the sparse random projections approach on both synthetic and real-world datasets.
First, we synthetically generate samples {xi }ni=1 2Rp distributed along one PC with =20. Each entry of the center
and PC is drawn from the uniform distribution on [0, 20)
and [0, 1), respectively. The PC is then normalized to have
unit `2 -norm. We consider a relatively noisy situation with
SNR=1. We then estimate the center of the original data
from the sparse random projections, where m/p=0.2, for
varying n and compression factors . Our results are averaged over 10 independent trials. Fig. 6.1(a) shows the
accuracy for the estimated center, where the error is the
distance between the estimated and the true center normalized by the true center’s norm. As expected, when n or dimension p increase, the compression factor can be tuned
to achieve a substantial reduction of storage space while
obtaining accurate estimates. This is desirable for highdimensional data stream processing.
We then fix n=2p, and plot the inner product magnitude
between the estimated and true PC in Fig. 6.1(b) and the

Efficient PCA via Very Sparse Random Projections

compared to the original SVD and BSOI, respectively. In
terms of memory requirements, 340 MB is needed to store
the original data. However, the required memory for our
1
1
framework is 44 MB for = 20
and 24 MB for = 40
. The
projected data thus can easily reside in the main memory.

0.5
0.4
SVD on Original Data
Our Method,γ=1/20
Our Method,γ=1/40
BSOI

0.3
0.2
2

4

6

8

Number of PCs

10

Time in sec. (log scale)

Explained Variance

3

10

2

10

1

10

0

10

2

4

6

8

10

Number of PCs

(a)
(b)
Figure 6.2. Results for the MNIST dataset. Our proposed approach is compared with two methods: (1) performing MATLAB’s svds on the full original data, (2) BSOI (Mitliagkas et al.,
2013). Plot of (a) performance accuracy based on the explained
variance and (b) computation time for performing SVD. We see
that our approach performs as well as SVD on the original data
and outperforms BSOI with significantly less computation time.

computation time in Fig. 6.1(d) for varying . We observe
that, despite saving nearly two orders of magnitude in com1
1
1
putation time and also in memory (note = 50
, 100
, 200
)
compared to PCA on the full data, the PC is well-estimated.
Moreover, the approach remains increasingly effective for
higher dimensions, which is of crucial importance for modern data processing applications. We further note that, as
the dimension increases, we can decrease the compression
factor while still achieving a desired performance. For
1
1
example, = 100
for p=4⇥103 and = 200
for p=104 have
almost the same accuracy. This is consistent with the observation / p1 from before.
We also plot the estimation error for the singular value in
Fig. 6.1(c). The error is the distance between the singular
value obtained by performing SVD on {Ri yi }ni=1 and on
the original data {xi }ni=1 , normalized by the latter value.

Finally, we consider the MNIST dataset to see a realworld application outside the spiked covariance model.
This dataset contains 70,000 samples of handwritten digits, which we have resized to 40⇥40 pixels. Hence, we have
70,000 samples in R1600 . To evaluate the performance of
our method, we use the explained variance described in
e
(Mitliagkas et al., 2013). Given estimates of d PCs V2
p⇥d
R
and the data matrix X, the fraction of explained variT
e T XXT V)/tr(XX
e
ance is defined as tr(V
). We compare
the performance of our approach with (1) performing SVD
(using MATLAB svds) on the original data that are fully
acquired and stored, and as a useful point of comparison,
with (2) the online algorithm Block-Stochastic Orthogonal
Iteration (BSOI) (Mitliagkas et al., 2013), where the data
samples are fully acquired but not stored. We show the results in Fig. 6.2 for the measurement ratio m/p=0.1.
In terms of accuracy, our approach performs about as well
as SVD on the original data, and has slightly better performance compared to BSOI. The sparse random projections
result in a significant reduction of computational complexity, with one order and two orders of magnitude speedup

Moreover, we have compared our method with the fast randomized SVD algorithm in (Halko et al., 2011a). The estimation accuracy of this method is very close to SVD on
the original data, and the computation time is about 1.2
seconds, which is slightly less than the computation time of
our method. This is as we would expect, since fast randomized SVD is designed specifically for low-computational
complexity. However, (Halko et al., 2011a) is a full data
method, meaning that it is assumed that the full data is
available for computation and does not require time or cost
to access. Our approach performs approximately as well in
similar computation time while also allowing a reduction in
memory (or data access or data communication costs) by a
1
1
factor of , in this case 20
and 40
. This can be a significant advantage in the case where data is stored in a large
database system or distributed network.
This example indicates that our approach results in a significant simultaneous reduction of memory and/or computational cost with little loss in accuracy.

7. Conclusions
We have presented a memory- and computation-efficient
approach for estimation of PCs via very sparse random projections. This approach simultaneously reduces substantially the required memory and computation for PC estimation, while still providing high accuracy. More importantly,
it allows us to rigorously analyze each of memory, computation, and accuracy in terms of the sparsity of the projection, for various PC models. Thus, we have been able to
give provable tradeoffs between memory, computation, and
accuracy. Furthermore, a user of this approach could even
use the sparsity of the projections to tune to any desired
point on this three-way tradeoff. We believe that this approach could be valuable for various important modern data
processing applications such as massive databases, distributed networks, and high-dimensional data stream processing, although we have not focused on the specific details of these in favor of more theoretical analysis. Indeed,
we observe that our approach performs well in initial practical simulations, e.g. for the MNIST dataset, with large
reduction of both memory and computation without sacrificing accuracy.
Acknowledgements: This material is based upon work
supported by the National Science Foundation under Grant
CCF-1117775.

Efficient PCA via Very Sparse Random Projections

References
D. Achlioptas. Database-friendly random projections. In
Proceedings of the twentieth ACM SIGMOD-SIGACTSIGART symposium on Principles of database systems,
pages 274–281, 2001. 4
N. Ailon and B. Chazelle. The fast Johnson-Lindenstrauss
transform and approximate nearest neighbors. SIAM
Journal on Computing, 39:302–322, 2009. 5.2
R. Arora, A. Cotter, K. Livescu, and N. Srebro. Stochastic
optimization for PCA and PLS. In 50th Annual Allerton
Conference on Communication, Control, and Computing
(Allerton), pages 861–868, 2012. 2
R. Arora, A. Cotter, and N. Srebro. Stochastic optimization
of PCA with capped MSG. In NIPS, pages 1815–1823,
2013. 2
M. Balcan, S. Ehrlich, and Y. Liang. Distributed k-means
and k-median clustering on general topologies. In NIPS,
pages 1995–2003, 2013. 1
V. Chandrasekaran and M. Jordan. Computational and statistical tradeoffs via convex relaxation. Proc. of the National Academy of Sciences, 110:E1181–E1190, 2013. 2
Y. Chen, Y. Chi, and A. Goldsmith. Exact and stable covariance estimation from quadratic sampling via convex
programming. arXiv preprint arXiv:1310.0807, 2013. 2
C. Davis and W. Kahan. The rotation of eigenvectors by
a perturbation. III. SIAM J. on Numerical Analysis, 7:
1–46, 1970. 5, 5.3
D. Donoho. Compressed sensing. IEEE Transactions on
Information Theory, 52:1289–1306, 2006. 1
M. Elad. Optimized projections for compressed sensing.
IEEE Trans. SP, 55:5695–5702, 2007. 5.2
J. Fowler. Compressive-projection principal component
analysis. IEEE Trans. on Image Process., pages 2230–
2242, 2009. 2
A. Gilbert, J. Park, and M. Wakin. Sketched SVD: Recovering spectral features from compressive measurements.
arXiv preprint arXiv:1211.0361, 2012. 2, 5.3
G. H. Golub and C. F. Van Loan. Matrix computations,
volume 3. JHU Press, 2012. 4
N. Halko, P. Martinsson, Y. Shkolnisky, and M. Tygert. An
algorithm for the principal component analysis of large
data sets. SIAM Journal on Scientific Computing, 33(5):
2580–2594, 2011a. 2, 6

N. Halko, P. Martinsson, and J. Tropp. Finding structure
with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review,
53(2):217–288, 2011b. 2
L. Hogben. Handbook of linear algebra. CRC Press, 2006.
5
P. Indyk. Stable distributions, pseudorandom generators,
embeddings, and data stream computation. Journal of
the ACM (JACM), 53(3):307–323, 2006. 1
I. Johnstone. On the distribution of the largest eigenvalue in
principal components analysis. The Annals of Statistics,
29(2):295–327, 2001. 3
P. Li, T. Hastie, and K. Church. Very sparse random projections. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 287–296, 2006. 1, 4, 5.1, 5.2
J. Lin and D. Gunopulos. Dimensionality reduction by random projection and latent semantic indexing. In proceedings of the Text Mining Workshop, at the 3rd SIAM
International Conference on Data Mining, 2003. 4
I. Mitliagkas, C. Caramanis, and P. Jain. Memory limited,
Streaming PCA. In NIPS, 2013. 2, 6.2, 6
S. Muthukrishnan. Data streams: Algorithms and applications. Now Publishers Inc, 2005. 1
D. Omidiran and M. Wainwright. High-dimensional variable selection with sparse random projections: measurement sparsity and statistical efficiency. The Journal of
Machine Learning Research, 99:2361–2386, 2010. 4
H. Qi and S. Hughes. Invariance of principal components
under low-dimensional random projection of the data. In
ICIP, pages 937–940, 2012. 2
R. Vershynin. How close is the sample covariance matrix
to the actual covariance matrix? Journal of Theoretical
Probability, 25(3):655–686, 2012. 4
M. Wang, W. Xu, E. Mallada, and A. Tang. Sparse recovery with graph constraints: Fundamental limits and
measurement construction. In IEEE Proceedings INFOCOM, pages 1871–1879, 2012a. 4
M. Wang, W. Xu, E. Mallada, and A. Tang. Sparse recovery
with graph constraints. CoRR, abs/1207.2829, 2012b. 4
K. Zhang, L. Zhang, and M. Yang. Real-time compressive
tracking. In Computer Vision–ECCV, pages 864–877.
Springer, 2012. 4

