Sparse meta-Gaussian information bottleneck

MeÃÅlanie Rey
University of Basel, Basel, Switzerland

MELANIE . REY @ UNIBAS . CH

Thomas J. Fuchs
Jet Propulsion Laboratory, California Institute of Technology, Pasadena, USA
Volker Roth
University of Basel, Basel, Switzerland

Abstract
We present a new sparse compression technique
based on the information bottleneck (IB) principle, which takes into account side information.
This is achieved by introducing a sparse variant
of IB which preserves the information in only
a few selected dimensions of the original data
through compression. By assuming a Gaussian
copula we can capture arbitrary non-Gaussian
margins, continuous or discrete. We apply our
model to select a sparse number of biomarkers
relevant to the evolution of malignant melanoma
and show that our sparse selection provides reliable predictors.

1. Introduction
Dimensionality reduction is an important domain of research for which a large variety of techniques have been
developed. Given observations of a multivariate random
variable X, the aim is to construct a new representation of
the data with reduced dimensionality. Amongst the most
prominent methods, Principal Component Analysis (PCA)
and Canonical Component Analysis (CCA) have been extensively studied and extended. Other classical techniques
include Compressed Sensing, Factor Analysis and methods for manifold modeling. Every dimensionality reduction technique first needs to define what is important in the
data and should therefore be preserved, e.g. PCA aims at
preserving the variance. In this respect, dimensionality reduction is related to the data compression problem where
the question of defining what is relevant is implicitly answered by the choice of a distortion function, i.e. a funcProceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

FUCHS @ CALTECH . EDU

VOLKER . ROTH @ UNIBAS . CH

tion evaluating the loss incurred by compression. An interesting alternative to distortion functions is given by the
Information bottleneck method (IB) (Tishby et al., 1999).
Instead of evaluating the distortion between the compression T and original data X, IB introduces a relevance variable Y and the aim becomes to compress the data while
retaining most information about Y. An important advantage of IB is that compression and information are solely
expressed in information-theoretic quantities. We seek T
such that I(X; T ) is small, meaning high compression,
and I(Y ; T ) is large, meaning high informativeness. While
some choices of distortion function or dependence measure
(e.g. correlation) are not suitable for certain types of data,
mutual information is a very general and theoretically wellfounded dependence measure (Joe, 1989). In this paper, we
present a new sparse compression technique based on the
information bottleneck principle, i.e. we perform feature
selection with relevance information. This is achieved by
introducing a sparse variant of IB in which T is built using
only a few selected dimensions of the original data. Efficient IB algorithms were limited to discrete data before the
introduction of Gaussian IB (GIB) (Chechik et al., 2005).
GIB considers the case of Gaussian variables for which it
provides an insightful and practical analytical solution. Recently, meta-Gaussian IB (MGIB) (Rey & Roth, 2012) has
generalised GIB to the continuous meta-Gaussian distributions i.e. distributions with a Gaussian copula and arbitrary
continuous margins. However, the compression achieved
by existing IB methods is usually not sparse and, therefore cannot be used for feature selection. Our model is
an extension of MGIB, we impose sparsity on the projection obtained through MGIB and thereby select a sparse
set of features. Our method shares some similarities with
sparse regression techniques like Lasso (Tibshirani, 1996)
but is based on different, less restrictive assumptions and
achieves sparsity without imposing any norm penalty. To
our knowledge, there exists no IB method to accommodate
mixed distributions, i.e. distributions with continuous and

Sparse meta-Gaussian information bottleneck

discrete margins, without resorting to discretisation of the
continuous dimensions. Besides introducing sparsity, another contribution of this paper is to extend MGIB to mixed
data by considering discrete margins as the result of a discretisation process of hidden continuous variables. This extension is motivated by the high prevalence of mixed data in
many domains where feature selection is sought, especially
in the medical and biological fields (de Leon & Chough,
2013).

2. Information Bottleneck (IB)
General IB. Consider two random vectors X =
(X1 , . . . , Xp ) and Y = (Y1 , . . . , Yq ) with joint distribution
pXY . To obtain a compression T of X that is most informative about Y , we solve the following variational problem
min L | L ‚â° I(X; T ) ‚àí Œ≤I(T ; Y ),

(1)

pT |X

where the Lagrange parameter Œ≤ > 0 determines the tradeoff between compression of X and preservation of information about Y . Since T is conditionally independent of
Y given X, it is fully characterised by its joint distribution
with X, denoted by pXT . No analytical solution is available for the general problem defined by (1). When X and
Y are discrete, pXT can be obtained iteratively using the
generalised Blahut-Arimoto algorithm, and T is a discrete
variable defining soft clusters of X.
Gaussian IB. GIB considers the special case of jointly
Gaussian variables (X, Y ):

(X, Y ) ‚àº N


0p+q , Œ£ =

Œ£x
Œ£yx

Œ£Tyx
Œ£y


,

(2)

where 0p+q is the zero vector of length p + q. As shown in
Chechik et al. (2005) an optimal compression T is obtained
with a noisy linear transformation of X: T = AX + Œæ,
where A ‚àà Rp√óp , Œæ ‚àº N (0p , Œ£Œæ ). The noise term Œæ is
independent of X, and as such will not carry any information about X. We can therefore assume that the noise
components are independent from each other and, without
prior information, that these components have equal variance. As shown in Chechik et al. (2005), Œ£Œæ can be set to
the identity matrix I w.l.o.g, and the minimisation problem
(1) is thus reduced to:
min L | L ‚â° I(X; AX + Œæ) ‚àí Œ≤I(AX + Œæ; Y ).
A

(3)

The optimisation problem (3) has the major advantage
that an analytical solution can be calculated: the optimal
compression is a Gaussian variable T ‚àº N (0p , Œ£t ) with
Œ£t = AŒ£x AT + I. The projection matrix A has a particular structure where the number of non-zero rows depends

on Œ≤:
 T

Ô£±
T
c
Ô£¥
Ô£≤  0T ; .T. . ; 0 T  if 0 c‚â§ Œ≤ ‚â§ Œ≤1 c
Œ±1 v1 ; 0 ; . . . , 0
if Œ≤1 ‚â§ Œ≤ ‚â§ Œ≤2 ,
A=
Ô£¥
..
Ô£≥
.

(4)

where 0T is a p-dimensional row vector and semicolons
separate rows of A. Here v1T , . . . , vpT are the left eigenvectors of Œ£x|y Œ£‚àí1
x sorted by their corresponding increasing eigenvalues Œª1 , . . . , Œªp . There are p critical Œ≤ values
Œ≤ic = q
(1 ‚àí Œªi )‚àí1 , and the Œ±i coefficients are defined by

Œ≤(1‚àíŒªi )‚àí1
with ri = viT Œ£x vi . In summary, the
Œ±i =
Œªi ri
projection matrix A is a combination of weighted eigenvectors of Œ£x|y Œ£‚àí1
x and the number of selected eigenvectors depends on the parameter Œ≤.

Meta-Gaussian IB. Consider random vectors X, Y with
a Gaussian copula and arbitrary margins, i.e. their cumulative distribution function (cdf) FXY (x, y) has the form
CP (FX1 (x1 ), . . . , FXp (xp ), FY1 (y1 ), . . . , FYq (yq )), (5)
where FXi , FYi are the marginal cdfs of X, Y and CP is a
Gaussian
copula

 parametrized by a correlation matrix P =
T
Px Pyx
. In Rey & Roth (2012) it was pointed out
Pyx Py
that since the mutual information between continuous variables depends only on their copula, the IB problem is actually independent of the marginal distributions. The main
idea in MGIB is that the IB problem for meta-Gaussian data
can be solved by applying GIB in the space of the normal
scores XÃÉ = (Œ¶‚àí1 ‚ó¶FX1 (X1 ), . . . , Œ¶‚àí1 ‚ó¶FXp (Xp )) and YÃÉ ,
where Œ¶ denotes the standard univariate Gaussian cdf. An
optimal projection T is obtained by first calculating (XÃÉ, YÃÉ )
and then computing T = AXÃÉ + Œæ as in GIB. A is entirely
determined by the covariance matrix of (XÃÉ, YÃÉ ) which also
equals the correlation matrix P parametrizing the Gaussian
copula CP (the normal scores have unit variance by definition). In summary, we can apply GIB to meta-Gaussian
data as long as we can make inference on two elements: P
and the underlying Gaussian variables, which in the continuous case are simply the normal scores (XÃÉ, YÃÉ ).

3. Sparse IB
We first assume that the underlying Gaussian variables of
our copula model and P are known. Inference will be discussed at the end of section 3. To achieve sparse compression we consider the MGIB model but restrict A to the class
of diagonal matrices. If we denote by a = (a1 , . . . , ap ) ‚àà
Rp the vector of the diagonal entries of A, the resulting projection is the vector AX = (a1 X1 , . . . , ap Xp ). By varying
the Lagrange parameter of the minimisation problem, the

Sparse meta-Gaussian information bottleneck

vector a becomes sparse and thereby selects only a few dimensions of X. As for the case of a full projection matrix
we can assume that the noise components are uncorrelated
with unit variance i.e. Œ£Œæ = I. Our optimisation problem
is simplified by two convenient properties:
1. For any symmetric positive definite matrix B and diagonal matrix D, log |DBD + I| = log |BD2 + I|
2
depends only on the squared entries Dii
. It is therefore sufficient to consider the space of positive diagonal matrices in Rp , denoted by D+
p . In the following
we use the notation A := D2 .
2. log |BA+I| is concave in A and strictly monotone increasing in every component Aii . Concavity directly
follows from the concavity of log |B| and the fact that
A 7‚Üí BA + I is an affine function.
The minimisation problem (3) can be rewritten for some
Œ∫0 ‚â• 0 as (see also (Chechik et al., 2005)):
min log |Px A + I| s.t.
{z
}

|
A:A‚ààD+
p

(6)

2I(X;T )

log |Px A + I| ‚àí log |QA + I| ‚â• Œ∫0 ,
{z
}
|
2I(T ;Y )

T
is the conditional covariance
where Q = Px ‚àí Pxy Py‚àí1 Pxy
matrix of X given Y . The corresponding Lagrangian is

L0 (A, Œ≤) = log |Px A + I| ‚àí

p
X

Œ∑j Ajj

(7)

j=1

‚àí Œ≤(log |Px A + I| ‚àí log |QA + I| ‚àí Œ∫0 ),
where Œ∑j is the Lagrange parameter for the j-th nonnegativity constraint. Nontrivial solutions for the original
IB with full matrix A exist only for Œ≤ > 1, see (Chechik
et al., 2005). We transform the IB problem into an equivalent form that exchanges objective function and constraint.
Introducing a new Lagrange parameter Œª = (Œ≤‚àí1)/Œ≤, 0 <
Œª < 1, and dividing (7) by Œ≤, we find the Lagrangian
L(A, Œª) = log |QA+I|‚àí

p
X

j Ajj +Œª(Œ∫‚àílog |Px A+I|),

j=1
Œ∑

with j = Œ≤j , Œ∫ ‚â• 0. The corresponding minimisation
problem can then be rewritten as
min log |QA + I| s.t. log |Px A + I| ‚â• Œ∫,
|
{z
}
|
{z
}

A:A‚ààD+
p

:=f (a)

(8)

:=g(a)

which amounts to minimising a concave function f (a) over
a convex set {b ‚àà Rp |g(b) ‚â• Œ∫}. Thus, the global minimum is attained at the boundary g(a) = Œ∫. Note that
for Œ∫ = 0 the constraint is always satisfied and there is
a unique minimum at a = 0. While we cannot characterise

all stationary points of the Lagrangian problem as formulated in (7), the minimisation problem (8) has a particular
form (concave function over a convex set) for which algorithms with guaranteed convergence to a globally optimal
solution exist (Benson & Horst, 1991). A Matlab code for
this method is available online1 . However, this algorithm
is not efficient in higher dimensions and we therefore propose a log barrier interior point method detailed later in
Algorithm 1.
The solution set S of optimisation problem (8) is defined as
the set of points a‚àó in the non-negative orthant of Rp which
are global minima for a certain value of Œ∫, i.e. S = {a‚àó ‚àà
Rp+ s.t. a‚àó is a solution of (8) for some Œ∫ ‚â• 0}. In the
following theorem, we show that for an interval [0, Œ∫c2 ], S is
a curve parametrized by Œ∫, meaning that to every Œ∫ in this
interval corresponds a unique point in S. In the following
we assume that Px and Py are random matrices of maximal
rank and write Œ¶ := Q‚àí1 , Œ® := Px‚àí1 . We denote points in
S by a‚àó = (a‚àó1 , . . . , a‚àóp ).
Theorem 3.1. With probability one, there exist Œ∫c2 > Œ∫c1 >
0 such that:
1. If Œ∫ ‚àà [0, Œ∫c1 ] then
 Œ∫
e ‚àí1
a‚àói =
0

if i = argminj (Qjj ) =: if ,
else.

2. If Œ∫ ‚àà [Œ∫c1 , Œ∫c2 ] then
Ô£±
Ô£≤ G(Œ∫; Œ®, Œ¶) if i = is ,
c1 a‚àóis + c0 if i = if ,
a‚àói =
Ô£≥
0
else
where c1 =

|Œ®|‚àíŒ¶22
|Œ®|‚àíŒ¶11 , c0

G(Œ∫; Œ®, Œ¶) = ‚àí

+

=

|Œ®|(Œ¶11 ‚àíŒ¶22 )
|Œ®|‚àíŒ¶11

and

|Œ®|(1 + c1 ) + c0
2c1

(|Œ®|(1 + c1 ) + c0 )2
|Œ®|(c0 + 1 ‚àí eŒ∫ )
‚àí
2
4c1
c1

0.5
,

The value of is can be determined by searching over
the p ‚àí 1 possible combinations (if , i) and choosing the dimension is which gives the minimal value
of f (a).
We use the term critical values to designate Œ∫c1 , Œ∫c2 . We
call if the most informative dimension of X and is the
second most informative dimension. Theorem 3.1 tells us
that, for small enough Œ∫ values, the solution set is a curve
parametrized by Œ∫ which starts at the point zero, runs along
the if -axis until c0 is reached, and then takes the form of
a straight line with slope c1 , see Figure 1. We therefore
call S the solution path. We prove this result in three steps
given by Lemma 3.1, Lemma 3.2 and Lemma 3.3.
1
http://www.mathworks.com/matlabcentral/fileexchange/36247function-for-global-minimisation-of-a-concave-function

Sparse meta-Gaussian information bottleneck

leads to a system of 2 equations in 3 variables (a1 , a2 and
Œ∫) which implicitly defines the set of all stationary points
with strictly positive components:

HÃÉ12 : fa2 ga1 ‚àí fa1 ga2 = 0
(12)
HÃÉ0 :
g(a) ‚àí Œ∫ = 0

Figure 1. Objective function f (filled contours, colour coded) and
constraint g (thick blue contour lines). The solution path is depicted by the red line. Above the critical constraint value (in this
example the Œ∫c1 ‚âà 0.8 contour line), all solutions lie on the line
a‚àó2 = c1 a‚àó1 + c0 with positive slope c1 , below the critical value,
the solution path is a vertical line at the origin, i.e. solutions are
sparse (a‚àó1 = 0).

Lemma 3.1 (Most informative dimension). The most informative dimension is the dimension if with the smallest corresponding entry in Q, i.e. if = argmini (Qii ). Moreover,
when only one component if of a is non-zero, a‚àóif = eŒ∫ ‚àí1.
Proof. For every possible choice of i = 1, . . . , p, the value
of ai is uniquely determined by the constraint:
Œ∫ = g(ai ) = log((Px )ii ai + 1) = log(ai + 1),

(9)

which implies that ai = eŒ∫ ‚àí 1. The optimal if is then
determined by the minimum value of f (ai ) = log(Qii ai +
1) = log(Qii (eŒ∫ ‚àí 1) + 1).
Lemma 3.2 bellow provides an analytical solution in the
case p = 2. In the following we denote the partial deriva‚àÇf
(a) = fai .
tives of a real function f of a by ‚àÇa
i
Lemma 3.2 (2-dimensional case). Assume p = 2 and,
w.l.o.g. that the most informative dimension is if = 2. Let
c1 , c0 , G(Œ∫; Œ®, Œ¶) be defined as in Theorem 3.1. The first
critical value is Œ∫c1 = log(c0 +1), and with probability one,
Œ∫c1 > 0. Moreover, the optimal a‚àó is given
1. for every Œ∫ ‚àà [Œ∫c1 , ‚àû) by
a‚àó2 = c1 a‚àó1 + c0 ,

(10)

a‚àó1

(11)

= G(Œ∫; Œ®, Œ¶),

2. for every Œ∫ ‚àà [0, Œ∫c1 ] by a‚àó2 = eŒ∫ ‚àí 1, a‚àó1 = 0.
Proof. We first determine the set of stationary points with
strictly positive components. When ai > 0, i = 1, 2, the
non-negativity constraints are inactive and i = 0, i =
1, 2. Stationary points are characterised by a vanishing
Lagrangian gradient ‚àáL = 0, which here means that
‚àáf (a) = Œª‚àág(a). When p = 2, this proportionality condition is equivalent to the orthogonality condition
‚àíga2 fa1 + ga1 fa2 = 0. Adding the constraint g(a) = Œ∫

To solve the above system we first need to to compute the partial derivatives fai , gai . Rewriting f (a) =
log(|Q||Q‚àí1 + A|) = log |Q| + log |Œ¶ + A| and g(a) =
log |Px | + log |Œ® + A|, we directly obtain
faj =

|Œ¶ + A|[‚àíj]
,
|Œ¶ + A|

gaj =

|Œ® + A|[‚àíj]
.
|Œ® + A|

(13)

From expressions 13 we can see that HÃÉ12 can advantageously be replaced by
H12 : |Œ¶ + A||Œ® + A|(fa2 ga1 ‚àí fa1 ga2 ) = 0.

(14)

Further, HÃÉ0 can be replaced by H0 : |Px ||Œ® + A| ‚àí eŒ∫ = 0.
We solve system (12) in two steps. First, using H12 we
obtain an expression for a2 as a function of a1 , leading to
equation (10). Second, we replace a2 in H0 by expression
(10), leading to equation (11). Finally, we can compute the
critical value Œ∫c1 by solving a‚àó1 = G(Œ∫; Œ®, Œ¶) = 0 for Œ∫.
Calculations are straightforward and can be found in the
supplementary material. Solutions for Œ∫ ‚â§ Œ∫c1 are given by
Lemma 3.1. The probability of c0 being equal to zero is
null, i.e. P{c0 = 0} = 0, since P{Q11 = Q22 } = 0 for
random correlation matrices Px and Py . This implies that
Œ∫c1 > 0 with probability one and there always exist values
of Œ∫ for which the 1-dimensional solution is optimal.
We come back to the general p-dimensional case with the
following result.
Lemma 3.3 (General p-dimensional case). In the pdimensional problem, with probability one, the following
statements hold:
1. There is an non-empty interval [Œ∫c1 , Œ∫c3 ] for which the
global optimum is attained with two active dimensions.
2. There is an non-empty interval [Œ∫c1 , Œ∫c2 ] for which the
global optimum is attained in a fixed 2-dimensional
subspace.
Proof. Assume w.l.o.g. that the most informative dimension is if = 1. As seen in the proof of Lemma 3.2, stationary points with strictly positive components are characterised by ‚àáf (a) = Œª‚àág(a). A new dimension j 6= 1
becomes active when faj = Œªgaj .
We prove the first statement by contradiction. Assume that
there exists no global optimum having exactly two nonzero components, this means that when varying Œ∫, solutions

Sparse meta-Gaussian information bottleneck

‚Äùjump‚Äù from one to (at least) three non-zero components2 .
In particular, at the jumping point a‚àó = (a‚àó1 , 0, . . . , 0) two
equations of the form

H1m : fam ga1 ‚àí fa1 gam = 0
(15)
H1s :
fas ga1 ‚àí fa1 gas = 0
for m, s 6= 1 with m 6= s must be fulfilled. When only
the component a‚àó1 is non-zero, both equations in (15) are
linear in a‚àó1 . We can therefore eliminate a‚àó1 from the above
system and are left with one equation in Px and Q only.
(Px )21m Q11 (Q11 ‚àí Qss ) + (Px )21s Q11 (‚àíQ11 + Qmm )+
[Q11 (Q21s ‚àí Q11 Qss ) + Q11 (‚àíQ21m ‚àí Qmm + Q11 Qmm )
+ (Q21m Qss ‚àí Q21s Qmm )] = 0

(16)

However, since Px , Py are random matrices, the probability
that equation (16) holds is null. We can therefore conclude
that with probability one there is only one other dimension
m 6= 1 for which fam = Œªgam . This implies that there
exist Œ∫c3 > Œ∫c1 ‚â• 0 such that exactly two dimensions are
active.
We now prove the second statement. We restrict ourselves
w.l.o.g to the interval [0, Œ∫c3 ] where the global optimum is
attained only with one or two dimensions. The most informative dimension if being fixed, there are p ‚àí 1 different
possible 2-dimensional subspaces. We can apply Lemma
3.2 to each subspace separately to obtain p ‚àí 1 different
values of the first critical Œ∫: 0 < Œ∫c1,1 < ¬∑ ¬∑ ¬∑ < Œ∫c1,p . Since
each Œ∫c1,i is a function of different entries in Œ® and Œ¶ (see
Lemma 3.2), and recalling that Px , Py are random matrices, we can see that all values Œ∫c1,1 , . . . , Œ∫c1,p are indeed
distinct. The solution path S leaves the if axis when the
first solution with two non-zero components becomes optimal, i.e. when Œ∫ = Œ∫c1,1 and we can finally set Œ∫c1 := Œ∫c1,1 .
In the interval [Œ∫c1,1 , Œ∫c1,2 [ only one 2-dimensional subspace
can contain global optima, namely the subspace having first
critical value Œ∫c1,1 . We can therefore set Œ∫c2 := Œ∫c1,2 .

We solve optimisation problem (8) using a log barrier interior point method detailed in Algorithm 1. Algorithm 1
starts by minimising f for a large value of the constraint Œ∫
such that all dimensions are selected, and then successively
decreases Œ∫ until finally a unique dimension remains active.
Even if we cannot prove that the solution path obtained
with Algorithm 1 connects only global minima, we can verify that it reaches the globally optimal 2-dimensional subspace. This was indeed in the case in all simulations conducted. We also propose in the supplementary material an
additional method to check that while running Algorithm 1
the solution path does not have any bifurcation.

Algorithm 1 Optimisation of sparse MGIB
1. Denote the entries of A by a ‚àà Rp and fix the set of Œ∫
values: Œ∫ ‚àà {Œ∫0 > Œ∫1 > ¬∑ ¬∑ ¬∑ > Œ∫m };
2. Initialisation step with Œ∫ = Œ∫0 :
compute amax ‚àà Rp : amax
= (eŒ∫ ‚àí 1)/(Px )jj ;
P maxj ‚àí1
m
set a := 1/ j (aj ) ;
compute Œª1 , . . . , Œªp the eigenvalues of Px ;
compute c = argmin[0,am ] f1 (v), v ‚àà R with
hP
i2
P
‚àí1
f1 (v) =
log(Œª
)
+
log(Œª
+
v)
‚àí
Œ∫
;
j
j
j
j
set a = (c + Œ¥, . . . , c + Œ¥) for a small Œ¥ > 0;
3. Optimisation for Œ∫ ‚â• Œ∫0 :
for Œ∫ ‚àà {Œ∫0 , . . . , Œ∫m } do
for  ‚Üí 0 do
Set a‚àó = argminf2 (w) where w ‚àà Rp , W is the
diagonal matrix with elements w and
f2 (w) = log |Px|y W + I|‚àí

P
 log log |Px W + I| ‚àí Œ∫ ‚àí  j log(wj );
end for
Exclude the dimensions corresponding to zero elements in a‚àó from the minimisation;
end for

Inference for mixed continuous-discrete data. To
make our model applicable to mixed data we use Gaussian hidden variables and, as in MGIB, apply GIB to
these underlying variables. This approach is based on
the fact that every ordinal categorical variable can be assumed to be a quantised version of an underlying continuous one (Joe, 1989). Our model can be applied to data
with any combination of continuous and ordinal discrete
margins. Copula models have mainly been used with continuous margins since for any continuous multivariate cdf
F = (F1 , . . . , Fn ) Sklar‚Äôs theorem (Sklar, 1959) ensures
the existence and the uniqueness of the copula C such
that F (v1 , . . . , vn ) = C(F1 (v1 ), . . . , Fn (vn )). However,
as explained in Genest & NesÃålehovaÃÅ (2007), the copula
construction C(F1 (.), . . . , Fn (.)) stills leads to a valid cdf
when all or some of the margins are discrete. The main
difficulty in copula modeling with discrete margins arises
from the fact that uniqueness of the copula is guaranteed
only on the range of the margins and traditional estimation techniques face an unidentifiability problem (Genest
& NesÃålehovaÃÅ, 2007). Despite this unidentifiability issue,
efficient methods for copula estimation have recently been
developed (Pitt et al., 2006), (Smith & Khaled, 2012). We
follow the Bayesian semiparametric approach for Gaussian
copula introduced in Hoff (2007) and consider the following model:
(XÃÑ1 , . . . , XÃÑp , YÃÑ1 , . . . , YÃÑq ) ‚àº N (0, P ),

2

The case of a jump from a = (0, . . . , 0) to a least three active
dimensions can similarly be excluded.

Xj =

‚àí1
FX
(Œ¶(XÃÑj )),
j

Yl =

FY‚àí1
(Œ¶(YÃÑl )),
l

(17)
(18)

Sparse meta-Gaussian information bottleneck
‚àí1
for j = 1, . . . , p, l = 1, . . . , q. where FX
, FY‚àí1
are
j
l
the generalised inverse of arbitrary, continuous or discrete,
cdfs. We assume a parametric form for the copula, namely
Gaussian, but not for the margins which are treated nonparametrically. Equation (18) imply that Xj ‚àº FXj , Yl ‚àº
FYl . Unlike in the continuous case, the underlying Gaussian variables XÃÑ = (XÃÑ1 , . . . , XÃÑp ), YÃÑ = (YÃÑ1 , . . . , YÃÑq ) are
not of the form (Œ¶‚àí1 ‚ó¶ FX1 (X1 ), . . . , Œ¶‚àí1 ‚ó¶ FXp (Xp )).
When some margins are discrete, estimates based on the
empirical marginal distributions (Liu et al., 2009) or on
the rank correlation cannot be used. To make inference on P and (XÃÑ, YÃÑ ) we use the marginal likelihood
method introduced in Hoff (2007). Denote the observed
data matrix by Z = (zij ), where the ith row zi‚àó =
(xi1 , . . . , xip , yi1 , . . . , yiq ) is the ith observation of (X, Y ),
and the corresponding unobserved realisations of (XÃÑ, YÃÑ )
by ZÃÑ. Even without assuming any knowledge about the
margins, the observed data Z provide some information
about ZÃÑ. Since the marginal cdfs are non-decreasing,
the following inequality holds: zmj < znj ‚áí zÃÑmj <
zÃÑnj , ‚àÄm, n, j, and ZÃÑ must lie in the set D :
n
ZÃÑ ‚àà G = (gij ) ‚àà Rn√ó(p+q) | max{gkj : zkj < zij }
o
< gij < min{gkj : zij < zkj } =: D.
(19)

4. Experiments

1. MGIB bound: MGIB is applied to observations of the
continuous hidden variables XÃÑ, YÃÑ . These hidden variables are not observable in practice and MGIB bound
provides an upper bound on achievable information
curves.
2. MGIB: We apply MGIB to observations {(xi , yi ), i =
1, . . . , n} of the mixed variables without adjustment
for mixed data i.e. using the model introduced in Rey
& Roth (2012).
3. Sparse MGIB: We apply sparse MGIB (no adjustment
for mixed data) to {(xi , yi )}.
4. MMGIB: Mixed Meta-Gaussian IB is MGIB for
mixed data applied to {(xi , yi )}.
5. Sparse MMGIB: Sparse version of MMGIB applied to
{(xi , yi )}.
We assess the efficiency of the different compression matrices A obtained by the above methods on a test set with
5000 observations. The compression T is calculated using
the projection matrix A obtained on the training data and
the mutual information I(XÃÑ; T ), I(YÃÑ ; T ) are calculated using the formula for meta-Gaussian variables given in Rey &
Roth (2012). Simulations are conducted with mixed margins for X and Y . For each dimension we use one of
the following distributions: Student t4 , Binomial(2, 0.5)
or Binomial(10, 0.5). By varying the parameter Œ∫ between 0.1 and 80 we can represent I(Y ; T ) as a function
of I(X; T ) and obtain the information curves. Each experiment is repeated to obtain the 50 curves for each method
(shown in the top panel of Figure 2). We can see that
some information was lost during the discretisation process I(X; Y ) < I(XÃÑ, YÃÑ ), and therefore the most effective compression (green curves) is obtained when applying
MGIB to XÃÑ, YÃÑ (not observable in practice). MMGIB (blue
curves) performs clearly better than MGIB (red curves) on
mixed data and achieves a compression rate closer to the
MGIB bound. The top panel of Figure 2 also illustrates the
difference between sparse and traditional IB. The information curves for sparse IB lie slightly below the traditional
IB curves since less information can be captured when A is
restricted to a diagonal matrix. However, sparse and traditional IB curves tend to the same value I(Y ; T ) as I(X; T )
increases.

Simulation: Comparison between different IB methods.
We generate training samples with n = 1000 observations
(xi , yi ), i = 1, . . . , n and dimensions fixed to p = 15,
q = 15. The samples are drawn from a meta-Gaussian distribution (i.e. in the form of (5)) in two steps. First, we generate the Gaussian hidden variables (XÃÑ, YÃÑ ), then using the
margin transformations (18) we obtain samples of (X, Y ).
P is obtained by scaling a covariance matrix drawn from
a Wishart distribution. We use a Wishart distribution centered at a correlation matrix P0 populated with some high
correlation values to ensure some dependency between X
and Y . In our first experiment we compare:

Simulation: Feature selection. To test the efficiency of
sparse MMGIB in selecting relevant dimensions of X, we
generate data such that only some dimensions of X were
informative about Y . This is achieved by using a correlation matrix P0 with only a few non-zero entries and a
Wishart distribution with high degrees of freedom. The
margins of X and Y were again mixed, following either a Beta(0.5, 0.5) or a Binomial(10, 0.5) distribution.
The bottom panel of Figure 2 shows results obtained with
the following choice for P0 : three dimensions of X are
strongly informative with corresponding values of 0.8 in

The probability of the observed data can then be written as:
p(Z|P, FX , FY ) = p(Z, ZÃÑ ‚àà D|P, FX , FY )
= Pr(ZÃÑ ‚àà D|P ) p(Z|ZÃÑ ‚àà D, P, FX , FY ),
where FX = {FX1 , . . . , FXp }, FY = {FY1 , . . . , FYq }.
The marginal likelihood approach estimates P using
Pr(ZÃÑ ‚àà D|P ) only, treating FX and FY as nuisance parameters. Bayesian inference conducted using Gibbs sampling gives samples of the posterior distribution, p(P |ZÃÑ ‚àà
D) ‚àù p(P ) p(ZÃÑ ‚àà D|P ), can handle missing values and
rank-deficient correlation matrices when n < p + q. Our
algorithm (available in the supplementary material along
with our R implementation) follows Hoff (2007) but is reparametrized in terms of precision matrix to gain efficiency.

Sparse meta-Gaussian information bottleneck

P0 , three other dimensions have entries 0.6, three more dimensions 0.4 and the six remaining dimensions are noise
with zero correlation in P0 . Figure 2 shows the solution
paths for the 15 entries of A, each line corresponding to one
dimension of X. The information curve obtained is shown
in red with the corresponding Œ∫ values. The most informative dimensions (green curves) are selected first and can be
clearly identified. Then, as Œ∫ increases, the remaining informative dimensions (blue and lilac curves) are selected as
well. The noise dimensions are always selected last, when
the value of Œ∫ becomes higher, to allow I(X; T ) to reach
the required level of 0.5Œ∫.

(Fuchs & Buhmann, 2011). Identification of a small number of key variables provides additional insight into the disease‚Äôs mechanisms, and is crucial for cost-effective prognosis and therapy optimisation. We consider this selection
problem in the context of cutaneous malignant melanoma
(MM), the most common cause for fatalities in skin cancer.
A first promising approach to identify biomarkers important for survival prediction was reported in Meyer et al.
(2012). Data was available in the form of immunohistochemical (IHC) expressions of 70 candidate biomarkers
measured for 364 patients. Additionally, 9 different clinical
observations were available which reflected experts‚Äô opinion about the stage of the tumor or directly characterised
the severeness of the disease in terms of survival times. Focusing exclusively on survival information (and ignoring
all other clinical attributes), a 7 marker signature which is
used to separate the patients into a low-risk and high-risk
group has been proposed in Meyer et al. (2012). In particular, the signature is defined via a risk-score of the form
P7

(Œ≤i xi )Œ±i
1 if xi is measured
, Œ±i =
score(x) = i=1
P7
0 if xi is missing
Œ±i
i=1

where Œ≤i are the regression coefficients of a univariate Cox
model and xi are are the IHC expression measurements of
the 7 markers. A convincing statistical interpretation of
the selected markers, however, remains unclear: the selection proceeded in several stages where only univariate tests
have been used. Moreover, the relation of the biomarkers
to established prognosis-related clinical observations like
the pathological Tumor-Node-Metastasis (pTNM) staging
or the Clark level was ignored in the model.

Figure 2. Top: The figure shows 50 (overlapping) information
curves for each method. MGIB bound (green curves) provides a
benchmark. MMGIB (blue curves) achieves a better compression
than MGIB (red curves). Sparsity in T is achieved at the price
of a small decrease in efficiency (dashed curves). Bottom: Solution paths for the 15 entries of A and corresponding information
curve with Œ∫ values. Green curves correspond to very informative
dimensions of X (correlation 0.8 in P0 ), blue and lilac curves to
informative dimensions (corr. of 0.6 and 0.4 in P0 , respectively),
and gray dashed curves represent noise (corr. 0 in P0 ).

4.1. Real data
We consider a real world problem from computational biology. A recurring and important task in medical prognosis
is to identity biomarkers relevant to the disease evolution

Our sparse MMGIB model is best adapted to this problem,
since the data falls into two groups which nicely fit into our
framework: the 70 markers constitute the candidate features X, whereas the 9 clinical observations can be used
for the target variable Y . Defining a signature of molecular markers might be seen as finding the best sparse compression of the biomarkers‚Äô expression on a molecular level
which is still informative with respect to the clinical data in
the second group. Further, the technical specifications of
this dataset also perfectly fit into our mixed-data Bayesian
framework: most of the expression levels are represented as
ordered factors in a semi-quantitative scoring system with
5 levels, but other variables like survival times are continuous in nature, and roughly 10% of all values are missing.
Repeated experiments conducted to a final selection of 6
markers as explained in the left panel of Figure 3.
Interestingly, our information-theoretic analysis method
which is not particularly tailored to survival prediction
could nicely reproduce the survival regression results in
Meyer et al. (2012): using our set of 6 markers (containing 4 markers which were already part of the original 7
markers signature) in the same ‚Äúsignature‚Äù formalism also

Sparse meta-Gaussian information bottleneck

Y subject to a constraint of preserving information about
the molecular markers X. Figure 4 shows the corresponding solutions paths. Interestingly, it turns out that the disease specific event status for overall survival contains by
far the most information about the molecular markers. This
dominance of the disease specific event status over classical prognosis-related quantities like the T score or the clark
level is interesting for the following reason: it basically
shows that the classical clinical indicators fail to capture
all relevant disease-specific information contained in the
molecular data, showing that quantitative analysis of the
biomarkers‚Äô expression patterns indeed adds valuable information about prognosis and survival in addition to the
expert‚Äôs macroscopic scoring/staging estimates.

Figure 3. Left: boxplots of the elements Aii in the diagonal projection matrix computed on the basis of 50 consecutive samples
from the posterior distribution of the correlation matrix in the
Gibbs sampler. We show here only the values for the 6 markers
finally selected (those with a median above zero). The markers
MTAP, CD20, Bcl-X and Beta-catenin were already identified in
Meyer et al. (2012). Right: Kaplan-Meier plots of the two patient groups from the test cohort resulting from thresholding the
risk score in eq. (4.1) computed from our 6 marker signature

led to clear separation of low-risk and high-risk patients
on an independent test cohort. The right panel of Figure
3 shows Kaplan-Meier estimates of two different patient
groups separated by using the risk-score defined above applied to our signature. The Œ≤i coefficients were calculated
on the training set described above but the 6 markers expressions were measured on an external test cohort of 221
patients. We see that the 6 markers selected using our
method indeed provide highly effective prognosis predictors. Given the relatively small number of patients, the differences in p-values (2 ¬∑ 10‚àí4 for our signature vs. 2 ¬∑ 10‚àí5
reported in panel A of Figure 6 in Meyer et al. (2012)) are
probably not too relevant. This observation is corroborated
by the fact that the effect of regular updates on the the patients‚Äô censoring status from new follow-up reports have
led to even bigger differences for the individual signatures
in the past. We conclude that our sparse IB model can indeed be used as a high-quality prognosis predictor, even
though it was not specifically designed for survival regression. The real advantage of the IB model, however, is its
capability to extract many more details about the interaction between markers and a rich set of clinical measurements. Much could be said about the interpretation of the
joint (X, Y )-correlation matrix obtained within the semiparametric copula framework. Due to space constraints,
however, we focus here only on one particularly interesting aspect, namely the reversal of the roles of X and Y ,
resulting in a sparse compression of the clinical variables

Figure 4. Solution paths for 9 diagonal entries of A when the roles
of X and Y are reversed.

5. Conclusion
Sparse Meta-Gaussian IB provides a very flexible method
for sparse compression with side information. By assuming
a Gaussian copula, it encompasses a wide class of distributions with arbitrary non-Gaussian margins (continuous or
discrete). Using relevance information, we do not need to
impose any norm penalty to obtain sparsity. Our Bayesian
framework for copula estimation can handle large-scale
high dimensional datasets with potentially missing values.
Our log barrier interior point algorithm is efficient even in
high dimensions. Moreover, we prove that the two globally best input features can be found in arbitrary dimensions in an efficient way, thereby also providing an additional validation of the results obtained with our algorithm.
Finally, we demonstrate in a clinical application that our
model can compete with state-of-the-art survival prediction
methods, while additionally allowing for an in-depth analysis of relevance- and interaction patterns between molecular markers and clinical measurements. We conclude that
the proposed model is a highly flexible analysis tool which
has the potential to significantly advance the field of exploratory data analysis within a well-defined informationtheoretic framework.

Sparse meta-Gaussian information bottleneck

References
Benson, H. P. and Horst, R. A branch and bound-outer
approximation algorithm for concave minimization over
a convex set. Journal of Computers and Mathematics
with Applications, 21(6/7):67‚Äì76, 1991.
Chechik, G., Globerson, A., Tishby, N., and Weiss, Y. Information bottleneck for Gaussian variables. Journal of
Machine Learning Research, 6:165‚Äì188, 2005.
de Leon, A.R. and Chough, K. CarrieÃÅre. Analysis of mixed
data: methods and applications. Chapman and Hall,
2013.
Fuchs, T.J. and Buhmann, J.M. Computational pathology: challenges and promises fortissue analysis. Journal of Computerized Medical Imaging and Graphics, 35(7):515‚Äì530, April 2011.
ISSN 08956111.
doi: DOI:10.1016/j.compmedimag.2011.02.
006. URL http://www.sciencedirect.com/
science/article/pii/S0895611111000383.
Genest, C. and NesÃålehovaÃÅ, J. A primer on copulas for count
data. Astin Bulletin, 37(2):475‚Äì515, 2007.
Hoff, Peter D. Extending the rank likelihood for semiparametric copula estimation. Annals of Applied Statistics, 1
(1):273, 2007.
Joe, H. Relative entropy measures of multivariate dependence. Journal of the American Statistical Association,
84(405):157‚Äì164, 1989.
Liu, H., Lafferty, J., and Wasserman, L. The nonparanormal: semiparametric estimation of high dimensional
undirected graphs. Journal of Machine Learning Research, 10:2295‚Äì2328, 2009.
Meyer, S., Fuchs, T.J., and Wild, P.J. A seven-marker signature and clinical outcome in malignant melanoma: a
large-scale tissue-microarray study with two independen
patient cohorts. PLoS ONE, 7(6), 2012.
Pitt, M., Chan, D., and Kohn, R. Efficient Bayesian inference for Gaussian copula regression models. Biometrika,
93(3):537‚Äì554, 2006.
Rey, M. and Roth, V. Meta-gaussian information bottleneck. Proceedings of the Neural Information Processing
Systems (NIPS), 2012.
Sklar, A. Fonctions de reÃÅpartition aÃÄ n dimensions et leurs
marges. Publications de l‚ÄôInstitut de Statistique de
l‚ÄôUniversiteÃÅ de Paris, 8:229‚Äì231, 1959.
Smith, M.S. and Khaled, M.A. Estimation of copula models with discrete margins via Bayesian data augmentation. Journal of the American Statistical Association,
107(497):290‚Äì303, 2012.

Tibshirani, R. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society, 58(1):
267‚Äì288, 1996.
Tishby, N., Pereira, F.C., and Bialek, W. The information
bottleneck method. The 37th annual Allerton Conference on Communication, Control, and Computing, (2930):368‚Äì377, 1999.

