Optimal Mean Robust Principal Component Analysis

Feiping Nie
FEIPINGNIE @ GMAIL . COM
Jianjun Yuan
WRIYJJ @ GMAIL . COM
Heng Huang
HENG @ UTA . EDU
Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, 76019

Abstract
Principal Component Analysis (PCA) is the most
widely used unsupervised dimensionality reduction approach. In recent research, several robust
PCA algorithms were presented to enhance the
robustness of PCA model. However, the existing
robust PCA methods incorrectly center the data
using the `2 -norm distance to calculate the mean,
which actually is not the optimal mean due to the
`1 -norm used in the objective functions. In this
paper, we propose novel robust PCA objective
functions with removing optimal mean automatically. Both theoretical analysis and empirical
studies demonstrate our new methods can more
effectively reduce data dimensionality than previous robust PCA methods.

1. Introduction
Machine learning techniques have been widely applied to
many scientific domains as diverse as engineering, astronomy, biology, remote sensing, and economics. The dimensionality of scientific data could be more than thousands,
such as digital images and videos, gene expressions and
DNA copy numbers, documents, and financial time series.
As a result, data analysis on such data sets suffers from the
curse of dimensionality. To solve this problem, the dimensionality reduction (subspace learning) algorithms have
been proposed to project the original high-dimensional feature space to a low-dimensional space, wherein the important statistical properties are well preserved.
Among these methods, the unsupervised dimensionality reduction methods are more useful in the practical applications since labeled data are usually expensive to obtain and
we often have no any prior knowledge for new scientific
problems. Thus, in this work, we focus on unsupervised
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

dimensionality reduction. Although PCA (Jolliffe, 2002)
is the most popularly used method, it is sensitive to the
data outliers because of the square `2 -norm based objective function. In real-world applications, the data outliers
often largely appear in the datasets, thus PCA may not get
the optimal performance.
To address this problem, multiple robust PCA methods
have been presented, such as the rotational invariant L1
PCA (Ding et al., 2006) (R1PCA) and convex robust PCA
(Wright et al., 2009; Xu et al., 2012). The R1PCA minimizes the `2,1 -norm reconstruction error by imposing the
`2 -norm on the feature dimension and the `1 -norm on the
data points dimension, such that the effect of data outliers will be reduced by the `1 -norm. Later this idea was
extended to robust tensor factorization (Huang & Ding,
2009). The convex robust PCA methods utilize the convex
relaxation objectives, such that the global solutions can be
achieved. However, all existing robust PCA methods neglect the mean calculation problem. Because the `1 -norm
or `2,1 -norm are used in different robust PCA objectives,
the square `2 -norm distance based mean is not the correct
mean anymore.
In this paper, we propose the novel robust PCA objective
functions with removing the optimal mean automatically.
We first show that the Euclidean distance based mean is
only valid for the traditional PCA. In robust PCA formulations, the `1 -norm or `2,1 -norm are used as loss functions,
such that the Euclidean distance based mean is not the correct one. Starting from two widely used robust PCA formulations, we propose our corresponding optimal mean robust
PCA objectives. We integrate the mean calculation into
the dimensionality reduction optimization objective, such
that the optimal mean can be obtained to enhance the dimensionality reduction. The optimization algorithms are
derived to solve the proposed non-smooth objectives with
convergence analysis. Both theoretical analysis and empirical studies demonstrate our new methods can more effectively reduce data dimensionality than previous robust PCA
methods.
Notations: For a vector v, we denote the `2 -norm of v by

Submission and Formatting Instructions for ICML 2014

√
kvk2 = v T v. For a matrix M , we denote the (i, j)-th
element by mij , the i-th column by mi and the i-th row
2
by mi . Denote kM kF = T r(M T M ), where trace(·) is
the trace operator for a square matrix, and denote kM k∗ =
1
T r((M T M ) 2 ) as the trace P
norm (nuclear norm). In this
paper, we denote kM k2,1 = Pi kmi k2 as the `2,1 -norm of
matrix M , denote kM k1 = i,j |mij | as the `1 -norm of
matrix M , and denote kM k2 = σmax (M ) as the `2 -norm
of matrix M , where σmax (M ) is the largest singular value
of M .

2. Principal Component Analysis Revisited
d×n

Given a data matrix X = [x1 , x2 , ..., xn ] ∈ R
, where
d is the dimensionality of data points and n is the number
of data points. Essentially, Principal Component Analysis
(PCA) is to find a low-rank matrix to best approximate the
given data matrix in the sense of Euclidean distance. Suppose the data are centered, i.e. the mean of the data is zero,
PCA is to solve the following problem:
min
rank(Z)=k

2

kX − ZkF .

(1)

According to the full rank decomposition, any matrix Z ∈
Rd×n with rank k can be decomposed as Z = U V T , where
U ∈ Rd×k , V ∈ Rn×k . After denoting an identity matrix
by I, the problem (1) can be rewritten as


X − U V T 2 .
min
(2)
F
U ∈Rd×k ,V ∈Rn×k ,U T U =I

Taking the derivative w.r.t. V and setting it to zero, we have
V = X T U . As a result, the problem (2) becomes:
max

U ∈Rd×k ,U T U =I

T

T

T r(U XX U ) .

(3)

The columns of the optimal solution U to problem (3) are
the k eigenvectors of XX T corresponding to the k largest
eigenvalues.
In the above derivation, we suppose the mean of the data
is zero. In the general case that the mean of the data is
not zero, PCA is to best approximate the given data matrix
with an optimal mean removed. Denote 1 as a column vector with all the elements being one, the problem of PCA
becomes:


X − b1T − Z 2 .
min
(4)
F

Taking the derivative w.r.t. b and setting it to zero, we have
(I −U U T )(b1T −X)1 = 0. Suppose U ⊥ is the orthogonal
complement of U , i.e. [U, U ⊥ ] is orthonormal matrix. Then
for any vector (b1T − X)1, it can be represented as:
(b1T − X)1 = U α + U ⊥ β .

(7)

So we have (I − U U T )(U α + U ⊥ β) = 0 ⇔ U ⊥ β = 0 ⇔
β = 0. Then Eq. (7) becomes:
b=

1
(X1 + U α),
n

(8)

where α could be any k-dimensional column vector. Denote a centering matrix H = I − n1 11T , by substituting
Eq. (8) into the problem (6), we have:
max

U ∈Rd×k ,U T U =I

T r(U T XHX T U ) .

(9)

We can see that no matter the data X is centered or not,
the problem (9) is unchanged. We can simply set α = 0
in Eq. (8), so the optimal mean in the problem (4) is b =
1
n X1. That is to say, in traditional PCA, we can simply
center the data such that X1 = 0, and then solve Eq. (3)
instead of solving Eq. (9).

3. Robust Principal Component Analysis with
Optimal Mean
The problemP(4) in the traditional PCA can be written as
n
2
min
i=1 kxi − b − zi k2 . It is known that squared
b,rank(Z)=k

loss function is very sensitive to outliers. Therefore, the
squared approximation errors will make the traditional
PCA not robust to outliers in the data.
Previous robust PCA methods use a non-squared loss function to improve the robustness, but still center data via the
`2 -norm distance based mean, which is incorrect. Instead
of using the `2 -norm distance based mean, we propose a
new robust PCA with an optimal mean automatically removed in the given data. Our new robust PCA is to solve
the following problem:
min
b,rank(Z)=k

n
X

kxi − b − zi k2 ,

(10)

i=1

b,rank(Z)=k

Note the b ∈ Rd×1 in problem (4) is also a variable to be
optimized. Similarly, the problem (4) can be rewritten as:


X − b1T − U V T 2 . (5)
min
F
b,U ∈Rd×k ,V ∈Rn×k ,U T U =I

Taking the derivative w.r.t. V and setting it to zero, we have
V = (X − b1T )T U . Then the problem (5) becomes:


X − b1T − U U T (X − b1T )2 . (6)
min
F
b,U ∈Rd×k ,U T U =I

where we optimize the mean in the robust PCA objective.
We can write the problem (10) in matrix form as follows:


X − b1T − Z  .
(11)
min
2,1
b,rank(Z)=k

Similarly, the problem (11) can be rewritten as:
min

b,U ∈Rd×k ,V ∈Rn×k ,U T U =I

n 

X
T

xi − b − U (v i )  . (12)
i=1

2

Submission and Formatting Instructions for ICML 2014

For each i, by setting the derivative w.r.t v i to zero, we have
v i = (xi − b)T U . Substituting it into Eq. (12), we arrive at
the following problem1 :
min

b,U ∈Rd×k ,U T U =I

n
X


(I − U U T )(xi − b) .
2

Algorithm 1 Algorithm to solve the problem (13).
Initialize D as an identity matrix
while not converge do
1. Update the columns of U by the k right singular

i=1

In this paper, we use an iterative re-weighted method to
solve the problem (13). The detailed algorithm is outlined
in Algorithm 1, and the theoretical analysis of the algorithm
is given in the last of this section. In each iteration, we
solve the following problem:
min

n
X

b,U ∈Rd×k ,U T U =I

2

dii (I − U U T )(xi − b)2 ,

(14)

i=1

end while

according to the definition of dii in the Step 3 of Algorithm
1, we have:
n
X
k(I − Ũ Ũ T )(xi − b̃)k22
2k(I − U U T )(xi − b)k2
i=1

Taking the derivative w.r.t. b and then setting it to zero, we
have (I − U U T )(b1T − X)D1 = 0. Similarly, we can let
(b1T − X)D1 = U α + U ⊥ β and get β = 0, so we have
(b1T − X)D1 = U α and thus we have
XD1
Uα
+
1T D1 1T D1

n
X
k(I − U U T )(xi − b)k22
.
≤
2k(I − U U T )(xi − b)k2
i=1

Substituting Eq. (15) into the problem (14), the problem
becomes
max

T r(U T XHd X T U ),

(18)

According to the Lemma 1 in (Nie et al., 2010), we know
(15)



2 


n


(I − Ũ Ũ T )(xi − b̃)
X


T
2 
(I − Ũ Ũ )(xi − b̃) −
T )(x − b)k 
2k(I
−
U
U
2
i
2
i=1

where α could be any k-dimensional column vector.

U ∈Rd×k ,U T U =I

1

1
2k(I−U U T )(xi −b)k2

where dii is the weights as calculated in Algorithm 1.

b=

T

1

D2
) corresponding to the k
vectors of X(D 2 − D11
1T D1
largest singular values.
2. Update b by b = 1XD1
T D1
3. Update the diagonal matrix D, where the ith diagonal element of D is updated by dii =

(13)

(16)

T

D
where Hd = D − D11
is the weighted centering ma1T D1
trix. The columns of the optimal solution U to problem (16)
are the k eigenvectors of XHd X T corresponding to the k
largest eigenvalues. When the dimensionality is larger than
the number of data, i.e. d > n, the problem (16) can be
efficiently solved by the SVD of the matrix:
!
1
1
D11T D 2
X D2 −
.
(17)
1T D1

≤


 !
n


(I − U U T )(xi − b)2
X


T
2
(19)
−
−
U
U
)(x
−
b)
(I

i
2k(I − U U T )(xi − b)k2
2
i=1

Summing Eq. (18) and Eq. (19) on both sides, we have
n 
n

X
X




(I − U U T )(xi − b)
(I − Ũ Ũ T )(xi − b̃) ≤
2
2

i=1

i=1

(20)
Since the problem (13) has an obvious lower bound 0, the
Algorithm 1 will converge. The equality in Eq. (20) holds
only when the Algorithm 1 converges. Thus, in each iteration, Algorithm 1 monotonically decreases the objective of
the problem (13) until the algorithm converges.


3.1. Theoretical Analysis of Algorithm 1

Theorem 2 The Algorithm 1 will converge to a local minimal solution to the problem (13).

Theorem 1 The Algorithm 1 will monotonically decrease
the objective of the problem (13) in each iteration until the
algorithm converges.

Proof: The Lagrangian function of problem (13) is:

Proof: In the Steps 1 and 2 of Algorithm 1, denote the updated U and b by Ũ and b̃, respectively. Since the updated
U and b are the optimal solutions of the problem (14) and
1
In practice, similar to (Nie et al., 2010), the kzk2 is replaced
√
by z T z + ε, where ε → 0.

L1 (U, b, Λ)

=

n
X


(I − U U T )(xi − b)
2
i=1

−T r((U T U − I)Λ) .

(21)

Taking the derivative w.r.t. U and b respectively and setting
them to zero, we get the KKT condition of the problem (13)

Submission and Formatting Instructions for ICML 2014

Algorithm 2 Algorithm to solve the problem (27).
Initialize x ∈ C
while not converge do
1. For each i, calculate Di = h0i (gi (x))
2. Update x byP
the optimal solution to the problem
min f (x) + T r(DiT gi (x))

as follows:
n 

P
(I − U U T )(xi − b)
∂
2
i=1

∂U
∂

− UΛ = 0

n 

P
(I − U U T )(xi − b)
2

i=1

=0
(22)
∂b
Using the matrix calculus, we can write the Eq. (22) as follows:
n
T
X
(I − U U T )(xi − b)(b − xi ) U
− UΛ = 0
k(I − U U T )(xi − b)k2
i=1
(I − U U T )(b − xi )
=0
k(I − U U T )(xi − b)k2

X
i

(23)

In each iteration of Algorithm 1, we find the optimal solution to the problem (14). Thus the converged solution of
Algorithm 1 satisfies the KKT condition of problem (14).
The Lagrangian function of problem (14) is:
L2 (U, b, Λ) =

n
X

2

dii (I − U U T )(xi − b)2

i=1

−T r((U T U − I)Λ) .

(24)

Taking the derivative w.r.t. U and b respectively and setting
them to zero, we get the KKT condition of the problem (14)
as follows:
n

2
P
∂
dii (I − U U T )(xi − b)
2

i=1

∂U
n
P

∂

i=1

− UΛ = 0


2
dii (I − U U T )(xi − b)2

=0
(25)
∂b
Similarly, we can write the Eq. (25) as follows using the
matrix calculus:
n
X
T
2dii (I − U U T )(xi − b)(b − xi ) U = U Λ
i=1

X

2dii (I − U U T )(b − xi ) = 0

(26)

x∈C

i

end while
weighted algorithm applied in Algorithm 1 can be used to
solve the following more general problem:
X
min f (x) +
hi (gi (x)),
(27)
x∈C

i

where hi (x) is an arbitrary concave function in the domain
of gi (x). The details to solve problem (27) is described
in Algorithm 2, where h0i (gi (x)) denotes any supergradient
of the concave function hi at point gi (x). According to the
definition of supergradient, it can be easily proved that the
Algorithm 2 converges. It can be seen that the converged
solution satisfies KKT condition of problem (27), thus the
Algorithm 2 will converge to a local optimal solution to
the problem (27). Empirical evidences show Algorithm 2
converges very fast and usually converges in 20 iterations.
Algorithm 2 is very easy to implement and powerful. For
example, it can be used to minimize the `p -norm, the `2,p norm, the Schatten `p -norm (Nie et al., 2012), and many robust loss functions such as the capped (truncated) `p -norm,
where 0 < p ≤ 2.
More interestingly, if we need to maximize the objective in
Eq.(27), we only require that hi (x) is an arbitrary convex
function in the domain of gi (x). In this case, in Algorithm
2, the h0i (gi (x)) in the first step is changed to be any subgradient of hi at point gi (x), and the ’min’ in the second
step is changed to ’max’. Therefore, it can be verified that
the Algorithm 2 can also be used to maximize the `p -norm
(Nie et al., 2011), the `2,p -norm, and the Schatten `p -norm,
where p ≥ 1.

i

According to the definition of dii in Algorithm 1, we can
see that Eq. (26) is the same as Eq. (23) when the Algorithm 1 is converged. Therefore, the converged solution of
Algorithm 1 satisfies Eq. (23), the KKT condition of problem (14). Thus the converged solution of Algorithm 1 is a
local minimal solution to the problem (13).


4. Optimal Mean Robust PCA with Convex
Relaxation
Recently, a convex relaxed robust PCA was proposed to
solve the following problem (Wright et al., 2009):
min kX − Zk1 + γkZk∗ .
Z

3.2. Extension to a General Algorithm
It is worth noting that the content in this section is a parallel
work with (Nie et al., 2010)2 . Later, we found that the re2

The motivation to derive this kind of algorithm is similar to
the motivation in Eq.(21) of (Nie et al., 2009) for solving the trace

(28)

To better pursuit the outliers in the data points, a recent
work is proposed to solve the following problem (Xu et al.,
2012):
min kX − Zk2,1 + γkZk∗ .
(29)
Z

ratio problem. It is a very useful trick for algorithm derivation.

Submission and Formatting Instructions for ICML 2014

Algorithm 3 Algorithm to solve the problem (30).
Let 1 < ρ < 2. Initialize µ = 0.1, E = 0, Λ = 0
while not converge do
1. Update b and Z by solving
2
1


min X̃ − b1T − Z  + γ̃kZk∗
b,Z 2
F
where X̃ = X − E + µ1 Λ and γ̃ =
2. Update E by solving

We will see that the problem (32) can also be solved with
the closed form solution. First, we have the following lemmas:

(32)

γ
µ.

2
1


min E − X̃  + γ̃kEk2,1
E 2
F

Lemma 1 Suppose A = U SV T , where the `2 -norms of
all the columns of U and V are 1, and S is a nonnegative
diagonal matrix. Then kAk∗ ≤ T rS.
Proof: Note that kAk∗ =

(33)

so we have kAk∗ =
max
T r(X U SV T Y ) =
X T X=I,Y T Y =I
P
P T
P
T
max
j sj
i xi uj vj yi ≤
j sj = T rS.
X T X=I,Y T Y =I

Lemma 2 kZk∗ = min

Z=AB T

However, all these work don’t take the optimal mean into
account. Although we can center the data such that X1 = 0
before solving problem (29), the mean b = n1 X1 is not
necessarily the optimal mean in the problem (29) since the
`2,1 -norm loss function instead of the Frobenious norm loss
function is used in the objective.
In this section, we consider the optimal mean for the `2,1 norm loss function, and propose to solve the following
problem:


min X − b1T − Z 2,1 + γkZk∗ .
(30)
b,Z

We can see problem (30) is a convex relaxation of problem
(11). We use Augmented Lagrangian Multiplier (ALM)
method to solve the proposed problem (30).

kEk2,1 + γkZk∗ .

(31)

b,Z,E

1
µ
kX − b1T − Z − E + Λk2F
2
µ

i
2
1
2 (kAkF

i

i

2

+ kBkF ), where the second step holds according
to Lemma 1 and the third step holds according to CauchySchwarz inequality. On the other hand, suppose the SVD
1
1
of Z is Z = U ΣV T , let A = U Σ2 and B = V Σ 2 , then
T
T
we have Z = AB and kZk∗ = U ΣV ∗ = T r(Σ) =
1
1
2
2
1
1
T 2
2 2
2
2 (kU Σ kF + kΣ V kF ) = 2 (kAkF + kBkF ). ThereT
fore, under the constraint Z = AB , the minimization of
2
2
1

2 (kAkF + kBkF ) is kZk∗ .
Lemma 3 If ZH 6= Z, then kZHk∗ < kZk∗ .
Proof: Suppose {Â, B̂} = arg min

The problem (33) can be easily solved and has the closed
form solution as follows: ei = (kx̃i k2 − γ̃)+ kx̃x̃iik , where
2
(s)+ is defined as (s)+ = max(0, s).

2
1
2 (kAkF

2

+ kBkF ),

then according to Lemma 2 we have kZk∗ = 12 (kÂk2F +
kB̂k2F ) and Z = ÂB̂ T . Thus ZH = Â(H B̂)T , according
to ZH 6= Z and Lemma 2, we have
kZHk∗ =
≤
=

. Solving this problem with joint variables b, Z, E is difficult, we use an alternating method to solve it. The detailed
algorithm is outlined in Algorithm 3. When fix E, we solve
problem (32) to update b, Z, and when fix b, Z, we solve
problem (33) to update E.

2

+ kBkF ).



Proof: Suppose Z = AB T , then kZk∗ = AB T ∗ ≤
rP
P
2P
2
kai k2 kbi k2 ≤
kai k2 kbi k2 = kAkF kBkF ≤

With the ALM method, we need to solve the following augmented problem in each iteration:
min kEk2,1 + γkZk∗ +

2
1
2 (kAkF

Z=AB T

First, problem (30) can be rewritten as:
min

T r(X T AY ),

The
in the last but one step holds since
P Tinequality
T
T
T
T
i xi uj vj yi = vj Y X uj ≤ σmax (Y X ) = 1, where
T
σmax (Y X ) denotes the largest singular value of matrix
Y X T which is 1 since X T X = I, Y T Y = I.


where X̃ = X − b1T − Z + µ1 Λ and γ̃ = µ1 .
3. Update Λ by Λ = Λ + µ(X − b1T − Z − E)
3. Let µ = min(ρµ, 108 )
end while

b,Z,E,X−b1T −Z=E

max

X T X=I,Y T Y =I
T

<

min

ZH=AB T

1
2
2
(kAkF + kBkF )
2

1
(kÂk2F + kH B̂k2F )
2
1
1
(kÂk2F + kB̂k2F − 1T B̂ B̂ T 1)
2
n
1
(kÂk2F + kB̂k2F ) = kZk∗ ,
2

(34)

where the last inequality holds since ZH 6= Z and Z =
ÂB̂ T indicates B̂ T 1 6= 0. Therefore, we have ZH 6= Z ⇒
kZHk∗ < kZk∗ .

The problem (32) can be solved according to the following
theorem:

Submission and Formatting Instructions for ICML 2014

Theorem 3 The unique optimal solution b̂, Ẑ to the problem (32) is b̂ = n1 X̃1 and Ẑ = U (Σ − γ̃I)+ V T , where
X̃H = U ΣV T is the compact Singular Value Decomposition (SVD) of X̃H and the (i, j)-th element of (M )+ is
defined as max(0, mij ).
Proof: Taking the derivative of Eq. (32) w.r.t. b and setting
it to zero, we have:
1
1
X̃1 − Ẑ1 .
n
n
So the problem (32) becomes:
b̂ =

1
min kX̃H − ZHk2F + γ̃kZk∗ .
b,Z 2

(35)

(36)

5. Experimental Results
The main goal of PCA is to reduce the dimensionality such
that the reduced features represent and reconstruct the original data as good as possible. In the experiments, we show
how well the reconstruction of the proposed new optimal
mean robust PCA methods compared to the previous PCA
and robust PCA methods. The compared PCA methods include original PCA (denoted as PCA), robust PCA with L1
maximization (denoted as L1PCA) (Kwak, 2008), R1PCA
(Ding et al., 2006) and robust PCA with convex relaxation
(solving Eq. (29), denoted as CRPCA) (Xu et al., 2012).
The proposed optimal mean robust PCA methods solving Eq. (11) and Eq. (30) are denoted as RPCA-OM and
CRPCA-OM, respectively.

First, we verify that Ẑ = U (Σ − γ̃I)+ V T satisfies:
0 ∈ ẐH − X̃H + γ̃∂kẐk∗ .

(37)

Denote the compact SVD of X̃H as X̃H = U ΣV T =
U1 Σ1 V1T + U2 Σ2 V2T , where the singular values in Σ1 are
all greater than γ̃ and the singular values in Σ2 are all
smaller than or equal to γ̃. Then Ẑ can be written as
U1 (Σ1 − γ̃I)V1T . On the other hand, we have X̃H1 =
0 ⇒ U ΣV T 1 = 0 ⇒ V T 1 = 0 ⇒ Ẑ1 = 0 ⇒ ẐH = Ẑ.
So we have the following equation:
X̃H − ẐH
= U1 Σ1 V1T + U2 Σ2 V2T − U1 (Σ1 − γ̃I)V1T
= γ̃U1 V1T + U2 Σ2 V2T .

(38)

It is known (Watson, 1992) that the set of the subgradients
of kẐk∗ is:

5.1. Experimental Setup
In the experiments, we use 12 benchmark face image
datasets, including AT&T (Samaria & Harter, 1994),
UMIST (Graham & Allinson, 1998), Yale (face data),
YaleB (Georghiades et al., 2001), Palm (Hou et al., 2009),
CMU-PIE (Sim & Baker, 2003), FERET (Philips et al.,
1998), MSRA, Coil (Nene et al., 1996), JAFFE, MNIST,
and AR. We downloaded the image data from different
websites. Some of them were resized by previous work,
but this won’t effect our evaluation.
In each dataset, we randomly select 20% images and randomly place a 1/4 size of occlusion in the selected
images.
P
The reconstruction error is calculated as i kxri − xoi k2 ,
where xoi is the original image without occlusion and xri is
the reconstructed image.

∂kẐk∗ = {U1 V1T +M : U1T M = 0, M V1 = 0, kM k2 ≤ 1}
So we have U1 V1T + γ̃1 U2 Σ2 V2T ∈ ∂kẐk∗ , and according
to Eq. (38) we have:
X̃H − ẐH ∈ γ̃∂kẐk∗ .

(39)

Therefore, Ẑ = U (Σ − γ̃I)+ V T satisfies Eq. (37). Since
the problem (32) is convex, Ẑ = U (Σ − γ̃I)+ V T is one of
the optimal solution to the problem (32).
Unlike the problem (4) which has many optimal solutions,
we further show that the solution b̂, Ẑ is the unique optimal
solution to the problem (32).
According to Lemma 3, we know the optimal solution Ẑ to
the problem (36) must satisfy ẐH = Ẑ, thus the optimal b̂
is b̂ = n1 X̃1 according to Eq. (35), and the problem (36) is
equivalent to the following problem:
1
min kX̃H − Zk2F + γ̃kZk∗ .
b,Z 2

(40)

Since the problem (40) is strictly convex, the optimal solution is unique. Therefore, b̂ = n1 X̃1, Ẑ = U (Σ − γ̃I)+ V T
is the unique optimal solution to the problem (32).


5.2. Reconstruction Error Comparison for Robust
PCA methods
We first compare the reconstruction error of PCA, L1PCA,
R1PCA and our proposed RPCA-OM methods on 12
benchmark datasets in Table 1. Because these methods
can share the common reduced dimensionality, their reconstruction errors can be compared under the same dimension. In Table 1, we compare the reconstruction errors under nine different dimensions from 10 to 50. We cannot list
more results due to limited space.
From Table 1, we can observe that:
1) The robust PCA methods R1PCA and RPCA-OM are
consistently better than the other two PCA methods, when
there are occlusions in the data which indicates these robust
PCA methods are effective and robust to outliers and noise
in the data, except for projected dimension 15 in YALEB.
2) L1PCA is better than PCA in some cases but is worse in
other cases. The reason is that L1PCA is to maximize the
`1 -norm, but not to minimize the reconstruction error.

Submission and Formatting Instructions for ICML 2014

MNIST (×104 )

JAFFE (×104 )

YALEB (×105 )

Coil20 (×105 )

MSRA (×105 )

FERET (×105 )

CMU-PIE (×103 )

AT&T (×104 )

UMIST (×104 )

AR (×104 )

YALE (×104 )

PALM (×105 )

Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)
Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)
Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)
Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)
Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)
Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)
Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)
Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)
Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)
Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)
Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)
Dimension
PCA
L1PCA
R1PCA
RPCA-OM (our)

10
20.964
18.255
17.911
17.885
10
34.429
17.162
16.824
16.640
10
10.451
9.9809
10.020
9.9742
10
19.632
19.694
19.569
19.397
10
28.314
16.181
16.237
16.112
10
21.619
21.570
21.481
21.430
10
7.8659
7.8727
7.8431
7.8010
10
29.333
28.948
28.877
28.774
10
28.917
28.568
28.486
28.383
10
23.293
23.298
23.032
22.912
10
22.119
17.959
17.742
17.692
10
14.703
14.734
14.665
14.651

15
17.354
16.384
16.005
15.974
15
29.642
15.852
15.567
15.420
15
9.6717
9.3133
9.3594
9.3290
15
18.912
18.091
17.988
17.905
15
27.593
15.372
15.440
15.302
15
20.394
20.308
20.302
20.262
15
6.6812
6.6765
6.6478
6.6134
15
27.426
27.141
27.109
27.036
15
27.248
27.022
26.971
26.900
15
21.004
21.006
20.837
20.767
15
19.445
16.591
16.253
15.150
15
13.355
13.373
13.319
13.300

20
15.849
15.198
14.865
14.859
20
25.499
14.958
14.847
14.716
20
9.0074
8.7559
8.7214
8.6944
20
17.766
17.095
16.972
16.903
20
26.326
14.765
14.786
14.683
20
19.543
19.470
19.435
19.395
20
5.8340
5.8485
5.8068
5.7799
20
26.152
26.026
25.784
25.713
20
25.966
25.765
25.709
25.600
20
19.601
19.612
19.417
19.321
20
17.995
15.514
15.207
14.461
20
12.376
12.377
12.300
12.287

25
14.866
14.282
13.895
13.899
25
24.791
14.472
14.414
14.265
25
8.5250
8.2912
8.2502
8.2169
25
17.092
16.446
16.326
16.270
25
24.612
14.265
14.180
14.068
25
18.864
18.805
18.791
18.763
25
5.2123
5.2148
5.1777
5.1547
25
25.119
24.980
24.857
24.756
25
24.910
24.726
24.603
24.498
25
18.428
18.499
18.251
18.193
25
16.940
14.872
14.526
14.009
25
11.596
11.628
11.527
11.499

30
13.890
13.584
13.044
13.037
30
24.020
14.052
13.973
13.818
30
8.0723
7.8940
7.8399
7.8154
30
16.554
15.913
15.828
15.766
30
23.127
13.827
13.796
13.708
30
18.294
18.232
18.220
18.154
30
4.7648
4.7691
4.7334
4.7089
30
24.270
24.128
23.903
23.814
30
24.053
23.888
23.740
23.655
30
17.544
17.637
17.356
17.279
30
16.152
14.310
14.015
13.507
30
11.011
11.033
10.954
10.935

35
13.193
12.895
12.350
12.303
35
23.188
13.635
13.662
13.526
35
7.7539
7.5742
7.5261
7.4969
35
16.029
15.497
15.411
15.376
35
21.938
13.476
13.462
13.345
35
17.753
17.726
17.673
17.638
35
4.3930
4.3852
4.3555
4.3315
35
23.613
23.432
23.266
23.153
35
23.330
23.243
23.050
22.944
35
16.816
16.905
16.644
16.530
35
15.658
13.745
13.530
12.890
35
10.561
10.580
10.461
10.437

40
12.559
12.256
11.719
11.707
40
22.321
13.328
13.275
13.155
40
7.4673
7.2877
7.2370
7.2078
40
15.528
15.124
15.032
14.995
40
21.511
13.180
13.162
13.074
40
17.307
17.293
17.232
17.200
40
4.0672
4.0694
4.0319
4.0096
40
23.002
22.873
22.742
22.631
40
22.780
22.640
22.532
22.424
40
16.164
16.225
15.998
15.879
40
15.139
13.308
12.997
12.889
40
10.119
10.156
10.053
10.035

45
11.924
11.666
11.148
11.064
45
21.066
13.021
12.993
12.867
45
7.2194
7.0594
6.9991
6.9719
45
15.204
14.799
14.706
14.667
45
20.592
12.945
12.927
12.837
45
16.880
16.871
16.811
16.784
45
3.8033
3.8020
3.7716
3.7547
45
22.425
22.319
22.176
22.064
45
22.180
22.054
21.964
21.850
45
15.617
15.723
15.428
15.306
45
14.728
12.874
12.631
12.502
45
9.7871
9.7885
9.7027
9.6745

50
10.776
11.359
10.774
10.627
50
12.855
12.594
12.641
12.500
50
6.8686
6.8297
6.7686
6.7420
50
14.907
14.509
14.419
14.380
50
20.039
12.737
12.728
12.631
50
16.518
16.536
16.442
16.413
50
3.5799
3.5889
3.5473
3.5345
50
21.931
21.798
21.676
21.549
50
21.698
21.628
21.448
21.312
50
15.185
15.232
14.999
14.874
50
14.218
12.434
12.331
12.200
50
9.4969
9.4881
9.4058
9.3826

Table 1. Reconstruction error comparisons of four PCA methods on 12 benchmark datasets using different dimensions. The best reconstruction result under each dimension is bolded.

Submission and Formatting Instructions for ICML 2014
5

4.5

5

ATT

x 10

4
CRPCA
CRPCA−OM(our)

4

5

UMIST

x 10

3

3.5

5

YALE

x 10

CRPCA
CRPCA−OM(our)

10
CRPCA
CRPCA−OM(our)

2.5

1.5

2.5

8

2
1.5

2

Reconstruction Error

2

Reconstruction Error

2.5

1.5

1

60
r

30

(a) AT&T
4

60
r

0

90

2.5

2.5

0.5

1.5

1

60
r

30

(e) CMU-PIE
2
CRPCA
CRPCA−OM(our)

1.8

1

2.5

Reconstruction Error

2

1.5

1

0.5

2

1.5

1

0.5

30

60
r

0

90

30

(g) FERET
6

MSRA

x 10

JAFFE
CRPCA
CRPCA−OM(our)

1.5

(f) Coil
6

MNIST

x 10

3

x 10

2.5

0

90

2
CRPCA
CRPCA−OM(our)

1.8

1.6

1.6

1.4

1.4

Reconstruction Error

5

60
r

1.2
1
0.8

14

12

10

0.8

0.4

0.4

0.2

0.2

0

0

YALEB

x 10

CRPCA
CRPCA−OM(our)

CRPCA
CRPCA−OM(our)

1

0.6

90

(h) JAFFE

Palm

x 10

60
r

5

1.2

0.6

90

(d) AR

0.5

0

90

60
r

5

FERET

x 10

2

0.5

30

30

CRPCA
CRPCA−OM(our)

Reconstruction Error

Reconstruction Error

1

0

90

(c) YALE

2

1.5

60
r

6

Coil

x 10

CRPCA
CRPCA−OM(our)

2

0

30

(b) UMIST
6

CMUPIE

x 10

CRPCA
CRPCA−OM(our)

Reconstruction Error

4

1

0

90

Reconstruction Error

30

Reconstruction Error

0

Reconstruction Error

5

2

0.5

0.5

0

6

3

0.5

3

7

1

1

2.5

CRPCA
CRPCA−OM(our)

3

3

Reconstruction Error

Reconstruction Error

3.5

AR

x 10

9

8

6

4

2

30

60
r

(i) MNIST

90

30

60
r

90

(j) MSRA

30

60
r

90

(k) Palm

0

30

60
r

90

(l) YALEB

Figure 1. Reconstruction errors under different γ obtained by CRPCA and our CRPCA-OM methods.

3) Since the optimal mean is considered in the reconstruction error minimization, our RPCA-OM method consistently outperforms other three methods in most cases.
5.3. Reconstruction Error Comparison for Convex
Robust PCA methods
In CRPCA and our CRPCA-OM methods, the projection
dimension cannot be selected. We can only get the reconstruction data via adjusting the parameter γ. Thus, we compare these two methods together. We choose the range of
γ based on the suggestion from (Wright et al., 2009), in
1
which the γ is suggested with the scale of m 2 (m is the dimension of matrix Z). Considering the size of images used
in our experiments, we select the range of γ from 30 to 90.
The reconstruction error comparison of these two methods
are shown in Fig.1. From Fig.1, we can conclude that:
1) As the value of the regularization parameter γ increases,
the reconstruction error for both methods increases as well,
which is due to the weight we put in the reconstruction error
decreases. As a result, the algorithm pays less attention to
minimizing the reconstruction error.

2) Our CRPCA-OM method is consistently better than CRPCA approach, because CRPCA-OM method takes into account the optimal mean in the Eq. (29), which as we have
said previously is not the Frobenious norm loss function’s
mean, but the `2,1 -norm loss function’s mean.

6. Conclusions
In this paper, we proposed the novel optimal mean robust
PCA models with automatically removing the correct data
mean. To solve the proposed non-smooth objectives, we
derive the new optimization algorithms with proved convergence. Both theoretical analysis and empirical results
show our new robust PCA with optimal mean models consistently outperform the existing robust PCA methods.

Acknowledgments
Corresponding Author: Heng Huang (heng@uta.edu)
This work was partially supported by US NSF IIS1117965, IIS-1302675, IIS-1344152.

Submission and Formatting Instructions for ICML 2014

References
Ding, Chris, Zhou, Ding, He, Xiaofeng, and Zha,
Hongyuan. R1-pca: Rotational invariant l1-norm principal component analysis for robust subspace factorization. Int’l Conf. Machine Learning, 2006.
face data, YALE. http://cvc.yale.edu/projects/
yalefaces/yalefaces.html.
Georghiades, A., Belhumeur, P., and Kriegman, D. From
few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE Transactions
on PAMI, 23(6):643–660, 2001.
Graham, Daniel B and Allinson, Nigel M. Face recognition: From theory to applications. NATO ASI Series F,
Computer and Systems Sciences, 163:446–456, 1998.
Hou, Chenping, Nie, Feiping, Zhang, Changshui, and Wu,
Yi. Learning an orthogonal and smooth subspace for
image classification. IEEE Signal Process. Lett., 16(4):
303–306, 2009.
Huang, Heng and Ding, Chris. Robust tensor factorization
using r1 norm. CVPR, 2009.
Jolliffe, I. T. Principal Component Analysis. Series:
Springer series in statistics, New York: Springer-Verlag,
2nd edition, 2002.
Kwak, N. Principal component analysis based on L1norm maximization. IEEE Transactions on PAMI, 30
(9):1672–1680, 2008. ISSN 0162-8828.
Nene, Sameer, Nayar, Shree K., and Murase, Hiroshi.
Columbia object image library (coil-100). Technical report, 1996.
Nie, Feiping, Xiang, Shiming, Jia, Yangqing, and Zhang,
Changshui. Semi-supervised orthogonal discriminant
analysis via label propagation. Pattern Recognition, 42
(11):2615–2627, 2009.
Nie, Feiping, Huang, Heng, Cai, Xiao, and Ding, Chris.
Efficient and robust feature selection via joint `2,1 -norms
minimization. In NIPS, 2010.
Nie, Feiping, Huang, Heng, Ding, Chris H. Q., Luo, Dijun,
and Wang, Hua. Robust principal component analysis
with non-greedy L1-norm maximization. In IJCAI, pp.
1433–1438, 2011.
Nie, Feiping, Huang, Heng, and Ding, Chris H. Q. Lowrank matrix recovery via efficient schatten p-norm minimization. In AAAI, 2012.

Philips, I., Wechsler, H., Huang, J., and Rauss, P. The feret
database and evaluation procedure for face recognition
algorithms. Image and Vision Computing, 16:295–306,
1998.
Samaria, Ferdinando and Harter, Andy. Parameterisation of
a stochastic model for human face identification, 1994.
http://www.cl.cam.ac.uk/research/dtg/attarchive/
facedatabase.html.
Sim, T. and Baker, S. The cmu pose, illumination, and
expression database. IEEE Transactions on PAMI, 25
(12):1615–1617, 2003.
Watson, GA. Characterization of the subdifferential of
some matrix norms. Linear Algebra and its Applications,
170:33–45, 1992.
Wright, J., Ganesh, A., Rao, S., and Ma, Y. Robust principal component analysis: Exact recovery of corrupted
low-rank matrices via convex optimization. NIPS, 2009.
Xu, Huan, Caramanis, Constantine, and Sanghavi, Sujay.
Robust pca via outlier pursuit. Information Theory, IEEE
Transactions on, 58(5):3047–3064, 2012.

