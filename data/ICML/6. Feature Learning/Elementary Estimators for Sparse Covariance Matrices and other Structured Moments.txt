Elementary Estimators for Sparse Covariance Matrices
and other Structured Moments
Eunho Yang
Department of Computer Science, The University of Texas, Austin, TX 78712, USA

EUNHO @ CS . UTEXAS . EDU

Aurélie C. Lozano
IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA

ACLOZANO @ US . IBM . COM

Pradeep Ravikumar
Department of Computer Science, The University of Texas, Austin, TX 78712, USA

Abstract
We consider the problem of estimating expectations of vector-valued feature functions; a special case of which includes estimating the covariance matrix of a random vector. We are interested in recovery under high-dimensional settings, where the number of features p is potentially larger than the number of samples n, and
where we need to impose structural constraints.
In a natural distributional setting for this problem, the feature functions comprise the sufficient
statistics of an exponential family, so that the
problem would entail estimating structured moments of exponential family distributions. For
instance, in the special case of covariance estimation, the natural distributional setting would
correspond to the multivariate Gaussian distribution. Unlike the inverse covariance estimation
case, we show that the regularized MLEs for covariance estimation, as well as natural Dantzig
variants, are non-convex, even when the regularization functions themselves are convex; with the
same holding for the general structured moment
case. We propose a class of elementary convex estimators, that in many cases are available
in closed-form, for estimating general structured
moments. We then provide a unified statistical
analysis of our class of estimators. Finally, we
demonstrate the applicability of our class of estimators via simulation and on real-world climatology and biology datasets.

Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

PRADEEPR @ CS . UTEXAS . EDU

1. Introduction
Covariance matrix estimation is an increasingly important
problem with applications in varied multivariate settings. A
motivating application for this paper is climate data analysis, specifically climate change detection (Ribes et al.,
2009), where covariance estimation is used for the computation of so-called Empirical Orthogonal Functions (Wikle
& Cressie, 1999), which in turn are used to determine climate variability indices such as the Arctic Oscillation. In
classical statistical settings where the number of observations n is much larger than the number of dimensions p,
a strong statistical estimator of the population covariance
matrix is the sample covariance matrix itself. Its strong statistical guarantees however fail to hold in high-dimensional
settings when p > n (Johnstone, 2001; Johnstone & Lu,
2004). A natural distributional setting for the covariance
estimation problem is when the random vector is multivariate Gaussian: in that case the sample covariance matrix serves as the maximum likelihood estimator (MLE). In
high-dimensional regimes however, such MLEs are typically not consistent, and it is necessary to use structurally
constrained estimation involving regularized MLEs. We
show however that even with the use of convex regularization functions, regularized MLE estimators for the covariance matrix solve non-convex programs. We also show that
natural Dantzig variants (the Dantzig estimator technically
is defined for sparse linear regression) are also non-convex.
Practical high-dimensional covariance matrix estimators
have thus typically focused not on likelihood-based regularized programs, but on thresholding and shrinkage.
Ledoit & Wolf (2003) for instance proposed to shrink the
sample covariance matrix to the identity matrix. Bickel &
Levina (2008a); Rothman et al. (2009) proposed thresholding estimators for covariance matrices under the structural
assumption that each row of the covariance matrix satis-

Elementary Estimators for Sparse Covariance Matrices and other Structured Moments

fies a weak sparsity assumption; El Karoui (2008) considered an alternative notion of sparsity based on the number of closed paths of any length in the associated graph.
There has also been a line of work on banded covariance
matrices, where the entries of the covariance matrix are assumed to decay based on their distance from the diagonal.
For such banded covariance matrices, Furrer & Bengtsson
(2007) proposed to shrink the sample covariance entries
based on this distance from the diagonal; and Bickel &
Levina (2008b); Cai et al. (2010) have analyzed the consistency of such banded estimators. In recent years, there
have been considerable advances in estimation of highdimensional parameters under varied structural constraints
such as sparsity, group-sparsity, low-rank structure, etc.
However, these are largely restricted to regularized MLE
estimators, which for the covariance estimation case, as
noted above, lead to non-convex programs. This leads to
the following question:
“Can we provide tractable estimators with strong statistical
guarantees for high-dimensional covariance matrices under
general structural constraints?”
Note that under such general structural constraints (e.g.
group sparsity), the specific thresholding and shrinkage
based estimators discussed above, which are designed for
their specific structure, would not be applicable. Recall that
the population covariance matrix is the expectation of the
outer-product of a centered random vector with itself. In
this paper, we thus actually consider a generalization of the
above question:
“Can we provide tractable estimators with strong statistical
guarantees for expectations of general vector-valued feature functions (i.e. moments) under general structural constraints?”
Note that a natural distributional assumption for this general problem entails the random vector being drawn from
an exponential family with sufficient statistics set to the
feature functions, so that the task reduces to recovering
the moment parameters of this exponential family. Even
in this general structured moment setting, we show that the
regularized MLE estimator, even with convex regularization functions, as well as a natural Dantzig variant, lead to
non-convex programs. We note that this problem of recovering structured moments has been the subject of much less
investigation when compared to the estimation of structured canonical parameters of such exponential family distributions (e.g. estimation of structured inverse covariance
matrices). We conjecture this is in part because regularized MLEs for the estimation of such structured canonical parameters lead to convex programs, unlike the case
with structured moments. The estimation of structured moments is nonetheless an important problem: it not only
includes the important covariance matrix problem, which

corresponds to the multivariate Gaussian exponential family case, but also the graphical model inference problem of
estimating moment parameters for general positive graphical distributions such as Ising models.
Our approach to addressing the questions above involves an
estimator that solves for a parameter with minimum structural complexity subject to certain very simple structural
constraints. Our estimator is reminiscent of the form of the
Dantzig estimator (Candès & Tao, 2007) for sparse linear
regression, but it is actually available in closed form for the
sparse covariance case, and corresponds to very simple operations in other structural constraint settings. Our class of
algorithms are thus not only computationally practical, but
also highly scalable. Interestingly, even though the class of
estimators is elementary, in our unified statistical analysis
of our class of algorithms for general structural constraints,
we show that they come with strong statistical guarantees
with near-optimal convergence rates. We illustrate the applicability of our framework via simulation and by applying
it to two real-world problems, one on climate analysis, and
the other on 3-D organization of chromosomes.

2. Setup
Let X ∈ Rp be a random vector with distribution P, and
let {Xi }ni=1 denote n i.i.d. observations drawn from P. In
this paper, we consider the task of estimating some moment parameter µ∗ := E[φ(X)] of this distribution, where
φ : Rp 7→ Rm is some vector-valued feature function
of interest. In the analysis to follow, it will be useful to
considerP
the empirical expectation of the feature function,
n
µ
bn = n1 i=1 φ(Xi ).
2.1. Example: Estimating Covariance Matrices
A key example of the above problem is the estimation of the
covariance matrix Σ∗ = E[(X − E(X))(X − E(X))> ].
bn =
The empirical covariance matrix is then given by Σ

>
Pn
Pn
1
1
, where X = n i=1 Xi . A
i=1 Xi − X Xi − X
n
natural distributional setting for such covariance estimation
is when the random vector X is multivariate Gaussian, with
mean µ∗ , and covariance matrix Σ∗ :


P(X; µ∗ , Σ∗ ) ∝ exp − 1/2(X − µ∗ )Σ∗ −1 (X − µ∗ )T .
Under this distributional setting, a natural estimator of the
covariance matrix is the regularized Gaussian maximum
likelihood estimator:
n
o
b n ii + logdet(Σ) + λn R(Σ) , (1)
minimize hhΣ−1 , Σ
Σ0

where R(Σ) is an arbitrary penalty function encouraging
specific structure in the covariance matrix, and λn is the
corresponding regularization parameter. For instance, a

Elementary Estimators for Sparse Covariance Matrices and other Structured Moments

structural assumption of sparsity of the underlying covariance matrix would suggest the use of the element-wise `1
norm regularization function.
Another natural estimator is based on the Dantzig estimator (Candès & Tao, 2007). The Dantzig estimator was developed for sparse linear regression, and estimates the parameter with the minimum `1 norm that at the same time
satisfies a constraint entailed by the stationary condition of
the `1 -regularized least squares estimator. Following this
resume, we first derive the stationary condition of (1) as
b n Σ−1 + Σ−1 + λn z = 0,
−Σ−1 Σ

Remark. Note that the earlier multivariate Gaussian covariance estimation problem can be re-written in this
setting. Specifically, the multivariate Gaussian distribution can be written in canonical parameterization (or
Gaussian
Markov random fields) as:
n
o P(X; θ, Θ) =

exp hθ, Xi + hΘ, XX T i − A(θ, Θ) , where A(θ, Θ) =

where z here is the subgradient of R(Σ). Assume the
following dual function R∗ (·) is well-defined: R∗ (A) =
supΣ:R(Σ)6=0 hA,Σi
R(Σ) . When R(·) is a vector or matrix norm
for instance, R∗ (·) is the corresponding dual norm. The
subgradient z then has the following property that R∗ (z) ≤
bn −
1 (Watson,
1992), which in turn entails R∗ Σ−1 (Σ

−1
Σ)Σ
≤ λn . Thus, the counterpart of the Dantzig estimator for estimating the structured covariance matrix of
multivariate Gaussian can be written as
minimize R(Σ)
Σ


b n − Σ)Σ−1 ≤ λn .
s.t. R∗ Σ−1 (Σ

(see Wainwright & Jordan (2008) for an expanded discussion of exponential families and moments). When the distribution (3) belongs to such a minimal exponential family,
it can then be re-written
using moment-basedoparameters
n
as: P(X; µ) = exp hθ(µ), φ(X)i − A(θ(µ)) .

(2)

Unfortunately, it can been seen both the estimators in (1)
and (2) are non-convex, as stated in the following proposition.
Proposition 1. The estimation problems in (1) and (2)
are both non-convex, even when the regularization function
R(·) itself is a convex function.
It thus remains to derive convex tractable estimators for
structured covariance matrices, even under natural distributional settings.
2.2. Estimating General Moments
The development in the previous section for the specific
example of structured covariance matrices extends to the
general problem of estimating expected feature functions
µ∗ := E[φ(X)]. As in the covariance estimation case,
a natural distributional setting is when the random vector
X is distributed as an exponential family, with sufficient
statistics set φ(X):
n
o
P(X; θ) = exp hθ, φ(X)i − A(θ) .
(3)
Suppose that this is a minimal exponential family, so that
the parameter θ(µ) that gives rise to expected sufficient
statistics (hereafter, moments) µ is obtained as θ(µ) =
∇A∗ (µ) where A∗ (·) is the conjugate dual function to A(·)

1/2 log(n log(2π)) − log det(−2Θ) − 1/4 θT Θ−1 θ. The
moments of this distribution are given as µ = E[X] =
−1/2Θ−1 θ, and E[XX T ] = µµT − 1/2Θ−1 ; so that the
centered second moment is given as Σ = E[(X − µ)(X −
µ)T ] = −1/2Θ−1 . Note that the multivariate Gaussian can
be equivalently parameterized (as in the previous section)
in terms of these moments as:
n
o
P(X; µ, Σ) ∝ exp − 1/2(X − µ)Σ−1 (X − µ)T .
Pn
Given the empirical moments, µ
bn = n1 i=1 φ(X (i) ), the
negative log-likelihood can then be written as:

L(µ) := −hθ(µ), µ
bn i + A θ(µ) ,
so that a regularized MLE with a regularization function
R(·) is given as:
n
o

minimize − hθ(µ), µ
bn i + A θ(µ) + R(µ) , (4)
µ

which can be seen to be non-convex in general. Let us consider the Dantzig variant in this general setting. The gradient of the negative log-likelihood is given by

∇L(µ) = −∇2 A∗ (µ) µ
bn + ∇2 A∗ (µ) ∇A θ(µ)

= ∇2 A∗ (µ) − µ
bn + µ .
Thus, the “Dantzig” variant of the structured moment estimator then takes the form:
minimize R(µ)
µ


s. t. R∗ ∇2 A∗ (µ)(µ − µ
bn ) ≤ λn ,

(5)

which too can be seen to non-convex in general, as we already observed in (1) and (2) as a special case.
We thus get a counterpart of the proposition in the structured covariance case:
Proposition 2. The estimation problems in (4) and (5) are
both non-convex programs for general exponential families, even when the regularization function R(·) itself is a
convex function.

Elementary Estimators for Sparse Covariance Matrices and other Structured Moments

2.3. Other Examples
Other important examples of multivariate exponential families where computing moments is of interest include the
Ising model (see Ravikumar et al. (2010) and references
therein) and the multivariate Bernoulli distribution (Dai
et al., 2013). Indeed, probabilistic graphical model distributions in general, when positive, can be expressed as
exponential family distributions, and computing moments
of the resulting exponential families constitutes the important problem of graphical model inference; see Wainwright
& Jordan (2008) for additional discussion and examples.
In the next section, we propose an elementary estimator,
that is not only convex, but also has a closed form in many
cases. In the section following that, we show that the estimator while elementary, nonetheless comes with strong
statistical guarantees.

minimize R(µ)
µ

(6)

where R∗ (·) is the dual function.
It can be seen that the estimator in (6) solves a convex program, unlike the regularized MLE and Dantzig variants discussed in the previous section. Moreover, while it is reminiscent of the Dantzig estimator (Candès & Tao, 2007), for
many typical settings of the regularization function R(·),
the elementary estimator is available in closed-form.
Suppose for instance the regularization function R(·) is
given by an “atomic” gauge function, as defined in Chandrasekaran et al. (2010). Specifically, suppose we are given
a set A := {aj }j∈I of very “simple” objects or “atoms”,
and that the regularization function R(µ) can be written as

c

nX
j∈I

cj : µ =

X

Remark. As a special case of the above, consider the
use of `1 regularization for off-diagonals (for recovering
a sparse covariance matrix for instance). The regularization
function R(·) can then be written as kΣk1,off :=
P
|Σ
ij |. The corresponding dual-norm can then be
i6=j
shown to be equal to: kΣk∞,off := maxi6=j |Σij |. Our elementary estimator (6) with this setting of the regularization
function then takes the following form:
Σ

The previous section showed that even under natural distributional settings, natural estimators such as the regularized
MLE, as well as a Dantzig variant, yield non-convex optimization problems for recovering structured moments. In
this paper, we thus consider the following elementary estimator, which we call the “Elem-Moment” estimator, that is
specified by a regularization function R(·):

R(µ) = inf

i∈I

minimize kΣk1,off

3. The Elem-Moment Estimator

s. t. R∗ (b
µn − µ) ≤ λn .

Proposition 3. Suppose R(·) is an “atomic” gauge function as specified by (7). Suppose also that the optimal coefficients solving (7) with µ set to the sample expected sufficient
statistics µ
bn , are given
ci }i∈I , so that µ
bn =
P
P as {b
ci ai , and R(b
µn ) = i∈I b
ci . Then, the optimal soi∈I b
lution µ
b of (6) is given by
X
µ
b=
max{b
ci − λn , 0}ai .

o
cj aj , aj ∈ A, cj ≥ 0 . (7)

j∈I

There, they showed that this class includes many popular regularization functions including the `1 norm, `1 /`q
norms, the nuclear norm, and others.
The following proposition then specifies the solution of (6)
with R(·) set to such an atomic norm.

b n − Σk∞,off ≤ λn .
s. t. kΣ

(8)

It can be seen the solution is given by the element-wise
b n (only for off-diagonal entries), so
soft-thresholding of Σ
b = Sλ (Σ
b n ), where [Sλ (u)]i = sign(ui ) max(|ui |−
that Σ
n
λ, 0), is the soft-thresholding function.

4. Error Bounds
In this section, we show that the ease of computing our
class of estimators does not come at the cost of strong statistical guarantees. We provide a general analytical framework for deriving error bounds for our class of estimators (6) in a high-dimensional setting, where the expected
feature function µ∗ = E[φ(X)] has some “structure”.
For a formalization of the notion of structure, we follow the unified statistical framework of Negahban et al.
(2012). There, they use subspace pairs (M, M⊥ ), where
M ⊆ M, to capture any structured parameter. M is the
model subspace that captures the constraints imposed on
the model parameter, which is typically low-dimensional.
On the other hand, M⊥ is the perturbation subspace of parameters that represents perturbations away from the model
subspace. Following their terminology, we assume that the
regularization function in (6) is decomposable with respect
to a subspace pair (M, M⊥ ):
(C1) R(u + v) = R(u) + R(v), ∀u ∈ M, ∀v ∈ M⊥ .
Such decomposability captures the suitability of a regularization function R(·) to particular structure. As Negahban et al. (2012) showed, for standard structural constraints
such as sparsity, low-rank, etc., we can define corresponding low-dimensional model subspaces, as well as regularization functions that are decomposable with respect to the
corresponding subspace pairs.

Elementary Estimators for Sparse Covariance Matrices and other Structured Moments

Example 1. Given any subset S ⊆ {1, . . . , p} of the coordinates, let M(S) be the subspace of vectors in Rp that
have support contained in S. It can be seen that any parameter θ ∈ M(S) would be atmost |S|-sparse. For this case,
we use M(S) = M(S), so that M⊥ (S) = M⊥ (S). Negahban et al. (2012) show that the `1 norm R(θ) = kθk1 ,
commonly used as a sparsity-encouraging regularization
function, is decomposable with respect to subspace pairs
(M(S), M⊥ (S)).
We also use their definition of subspace compatibility
constant that captures the relative value between the
regularization function R(·) and the error norm k · k,
over vectors in the subspace M: Ψ(M, k · k) :=
supu∈M\{0} R(u)
kuk . We also define the projection operator
ΠM̄ (u) := argminv∈M̄ ku − vk2 .
For technical simplicity, we consider the case where µ∗ is
exactly structured with respect to some subspace pair:
(C2) There exists a structured subspace-pair (M, M⊥ )
such that the model parameter satisfies ΠM⊥ (µ∗ ) = 0.
Theorem 1. Suppose we solve the estimation problem (6),
such that true structured moment satisfies Condition (C2),
the regularization function satisfies Condition (C1), and the
constraint term λn is set as λn ≥ R∗ (b
µn − µ∗ ). Then, the
optimal solution µ
b of (6) satisfies:
R∗ (b
µ − µ∗ ) ≤ 2λn ,

(9)

and T > 0, such that for any |t| ≤ T ,
 2
E etXi ≤ c0 , i = 1, . . . , p .
This condition is satisfied for instance if X follows a multivariate normal distribution or if the entries of X are
bounded. For simplicity, we also assume that E(X) = 0,
so that E(XX > ) = Σ∗ .
4.1.1. C OVARIANCE M ATRICES WITH E LEMENT- WISE
SPARSITY OF OFF - DIAGONALS
As a concrete example, we first consider the case where the
true covariance Σ∗ has sparse off-diagonals: it has at most
k non-zero off-diagonal elements. A natural variant of our
elementary estimator in (6) under this assumption would be
the one with R(·) := k · k1,off as in (8).
q
Corollary 1. Suppose that λn = M logn p for a sufficiently large constant M . Then, with probability at least
1 − C1 exp(−C2 nλ2n ), we have
r
log p
∗
b
kΣ − Σ k∞,off ≤ 2M
,
n
r
b − Σ∗ kF ≤ 4M k log p ,
kΣ
n
r
b − Σ∗ k1,off ≤ 8M k log p ,
kΣ
n

kb
µ − µ k2 ≤ 4λn Ψ(M) ,

(10)

for some constants C1 , C2 > 0.

R(b
µ − µ∗ ) ≤ 8λn Ψ(M)2 .

(11)

It is instructive to compare the results of this corollary to
those in Bickel & Levina (2008a); Rothman et al. (2009)
where authors consider estimating covariance matrices by
thresholding, however they focus on the matrices that are
invariant under permutations. We note that an application of our Theorem 1 is able to provide tighter bounds
by decoupling the probabilistic component from the nonprobabilistic bound; for instance, compare our consistent
`2 (or Frobenius norm) error bound in (10) against that in
Theorem 2 of Bickel & Levina (2008a)
q whereauthors prokp log p
∗
b
. Moreover,
kF = O
vide the bound of kΣ−Σ

∗

We note that Theorem 1 is a non-probabilistic result, and
holds deterministically for any selection of λn or any distributional setting of the covariates X. While Theorem 1
builds on concepts and notations such as decomposable
regularization functions from Negahban et al. (2012), it is
worthwhile to note that their analysis does not apply to our
class of estimators: there they consider regularized convex
programs, whereas here, we consider an elementary class
of constrained programs. Moreover, unlike their case, the
form of our estimators also allows us to provide bounds in
R∗ (·) norm, which guarantee the estimates are structured,
under similar conditions to those imposed on convex regularized programs in Negahban et al. (2012).
While the result in Theorem 1 seems a bit abstract, in the
sequel, we provide corollaries that obtain concrete instantiations of Theorem 1 for specific settings of the feature
functions, structures and regularization functions R(·), and
distributional assumptions on X.
4.1. Bounds for Covariance Estimation
In what follows, we shall assume that the components of X
are sub-Gaussian, that is, there exist some constants c0 ≥ 0

n

our results can be applied to any covariance matrix beyond
subset of matrices discussed in Bickel & Levina (2008a);
Rothman et al. (2009).
4.1.2. C OVARIANCE M ATRICES WITH E LEMENT- WISE
G ROUP S PARSITY
Suppose the indices {1, . . . , p} × {1, . . . , p} of the covariance matrix entries are partitioned into L disjoints groups
G = {G1 , . . . , GL }. Let d denote the maximum group
∗
cardinality maxL
j=1 |Gj |. Suppose that Σ is group-sparse
with respect to this set of groups, so that |{j ∈ {1, . . . , L} :
Σ∗Gj 6= 0| ≤ k, where Σ∗Gj is the vector comprising entries of Σ∗ corresponding to the indices in Gj . Thus, the

Elementary Estimators for Sparse Covariance Matrices and other Structured Moments

element-wise support of Σ∗ can be expressed as the union
of at most k groups in G. A natural regularization function
R(·) for this setting is the group-structured norm
kΣkG,ν :=

L
X

kΣGl kν , where ν ≥ 2,

(12)

l=1

where k · kν is the element-wise `ν vector norm.
Corollary 2. Suppose that we solve the variant of
the elementary estimator in (6), with the regularization function R(·) set to the group-structured norm in
(12), and
with the constraint penalty λn set as λn =
?p
(log d + log L)/n for a sufficiently large conM d1/ν
stant M > 0. Then, with probability at least 1 −
C1 exp(−C2 nλ2n ), we have
p
b − Σ∗ k∗G,ν ≤ 2M d1/ν ? (log d + log L)/n ,
kΣ
p
b − Σ∗ kF ≤ 4M d1/ν ? k(log d + log L)/n ,
kΣ
p
b − Σ∗ kG,ν ≤ 8M kd1/ν ? (log d + log L)/n
kΣ
where kΣk∗G,ν := maxg kΣGg kν ∗ for a constant ν ∗ satisfying ν1 + ν1∗ = 1.

The emerging line of work indicated above that address
such superposition-structure is again based on regularized
MLEs, which would have the same non-convexity caveats
for our general structured moment problem, as detailed in
the previous sections for even clean structural constraints.
In this section, we thus extend our “Elem-Moment” estimators in (6) to cover such “superposition-structure” as well.
To set up our notation,
we assume that the true moment is
P
given as µ∗ = α∈I µ∗α , where µ∗α is a “clean” structured
parameter with respect to a subspace pair (Mα , M⊥
α ), for
Mα ⊂ Mα . For instance, the individual components µ∗α
could individually be sparse, low-rank, column-sparse, etc.,
while the overall moment parameter µ∗ is none of these
structures per se, just a superposition of these.
Our “Elem-Moment” class of estimators in (6), naturally
extends to these superposition-structured problems as follows, in what we call “Elem-Super-Moment” estimators:
X
minimize
λα Rα (µα )
µ1 ,µ2 ,...,µ|I|

α∈I


X 
s. t. R∗α µ
bn −
µα ≤ λα

for ∀α ∈ I. (13)

α∈I

5. Extension to Superposition Structures
While there have been considerable advances in structurally constrained high-dimensional estimation in recent
years, there has been an increasing realization, especially
in the context of matrix decomposition problems, that typical structural constraints such as sparsity, group-sparsity,
etc. are too stringent, and may not be very realistic. Over
the last few years, there has been an emerging line of work
that addresses this issue by “mixing and matching” different structures (Chandrasekaran et al., 2011; Hsu et al.,
2011; McCoy & Tropp, 2011; Xu et al., 2012; Jalali et al.,
2010; Agarwal et al., 2012; Yang & Ravikumar, 2013). As
an illuminating example, consider the principal component
analysis (PCA) problem, where we are given i.i.d. random
vectors Xi ∈ Rp where Xi = Ui + vi . Ui ∼ N (0, Θ∗ ),
with a low-rank covariance matrix Θ∗ = LLT , for some
loading matrix L ∈ Rp×r , corresponds to the set of lowdimensional observations without noise; and vi ∈ Rp is
a noise/error vector that is typically assumed to be spherically Gaussian distributed, vi ∼ N (0, σ 2 Ip×p ) , or in
ideal settings vi = 0. The goal in PCA is to then recover
the covariance matrix Θ∗ from samples {Xi }ni=1 , with the
“clean” structural constraint that Θ∗ is low-rank. However,
in realistic settings, with outliers, the noise vector may be
distributed as vi ∼ N (0, Γ∗ ), where Γ∗ is elementwise
sparse. In this case, the covariance matrix of Xi has the
form Σ∗ = Θ∗ + Γ∗ , where Θ∗ is low-rank, and Γ∗ is
sparse. Thus, Σ∗ is neither low-rank nor sparse, but a superposition of two matrices, one of which is low-rank, and
the other which is sparse.

This class of problems can be solved via simple closedform operations by employing the parallel proximal algorithm of Combettes & Pesquet (2008). The details of the
algorithm are presented in the Supplementary Materials.
5.1. Error bounds
As a natural extension of (C2), we assume that each component exactly µ∗α lies in its structured subspace:
(C3) ΠM⊥
(µ∗α ) = 0,
α

∀α ∈ I .

In recent work, Yang & Ravikumar (2013) extend the analysis of regularized convex programs of Negahban et al.
(2012) from the vanilla structural constraint case to the superposition structural constraint case. Their analysis however is restricted to specialized regularized convex programs, and is not applicable to our class of elementary constrained “Elem-Super-Moment” estimators in (13). In the
sequel, we thus derive an extension of our Theorem 1 to
this superposition-structured setting.
We borrow the following condition from Yang & Ravikumar (2013), which is a structural incoherence condition ensuring that the non-interference of different structures:
(C4) (Structural
Incoherence)
Let
Ω
:=
n
o
3λ Ψ (M̄ )
maxγ1 ,γ2 2 + λγγ1Ψγγ1(M̄γγ1) . For any α, β ∈ I,
2
2
n
 2

max σmax PM̄α PM̄β , σmax PM̄α PM̄⊥
,
β
o
1
σmax PM̄⊥
PM̄⊥
≤ 16Ω
where PM̄ denote the
2
α
β

Elementary Estimators for Sparse Covariance Matrices and other Structured Moments
Table 1. Average performance measures and standard errors for sparse plus low-rank covariance estimation.
Spectral
Frobenius
Nuclear
Matrix 1-norm
Method
7.10 (0.15)
8.56(0.18)
35.87 (0.43)
11.65 (0.12)
Elem-Super-Moment
Thresholding
n=100,p=200
8.30 (0.17)
10.43 (0.11)
45.84 (0.39)
19.85 (0.21)
Well-conditioned
12.22 (0.12) 13.19 (0.17)
48.11 (0.45)
23.89(0.18)
50.77 (0.72)
Elem-Super-Moment 25.63 (0.54) 26.67 (0.49) 198.76 (1.31)
Thresholding
n=100,p=400
33.55 (0.49) 41.91(0.60) 331.41 (2.05)
67.64 (0.73)
35.71 (0.50) 34.83 (0.46) 207.97(2.27)
93.60 (0.91)
Well-conditioned

matrix corresponding to the projection operator for the
subspace M.
Under these two mild conditions, we can provide the following statistical guarantees for our estimators:
Theorem 2. Suppose that the true structured moment µ∗
satisfies conditions (C3) and (C4). Furthermore, suppose
we solve elementary estimators in (13) setting the constraint parameters λα such that λα ≥ R∗α (b
µn − µ∗ ). Then,
the optimal solution {b
µα }α∈I of (13) satisfies the following error bounds:
R∗α (b
µ − µ∗ ) ≤ 2λα ,

(14)

16|I| 

2
max λα Ψ(Mα ) ,

Rα (b
µα − µ∗α ) ≤
α∈I
λ
p α
∗
kb
µ − µ kF ≤ 4 2|I| max λα Ψ(Mα ) .
α∈I

where µ
b=

P

α∈I

µ
bα , and µ∗ =

P

α∈I

(15)
(16)

µ∗α .

5.2. Covariance Matrices with Low-rank plus
Element-wise sparse Structures
As an illustration of superposition structured moments, we
consider the case where Σ∗ = Σ∗1 + Σ∗2 , and where Σ∗1 is a
low-rank matrix, and Σ∗2 is an element-wise sparse matrix.
The natural selection from the estimation class (13) would
be the following:
minimize λ1 |||Σ1 |||∗ + λ2 kΣ2 k1,off
Σ1 ,Σ2

b n − (Σ1 + Σ2 )|||2 ≤ λ1
s. t. |||Σ
b n − (Σ1 + Σ2 )k∞,off ≤ λ2 ,
kΣ

(17)

where ||| · |||∗ and ||| · |||2 represent the nuclear norm and spectral norm of a matrix, respectively. Then, the consistency of
this estimator can be easily derived as the following corollary of Theorem 2:
Corollary 3. Suppose that the true structured covariance
matrix is given as Σ∗ = Σ∗1 + Σ∗2 , where k1 and k2 denote
the rank of Σ∗1 and the number non-zero elements of Σ∗2 , respectively. Also suppose we solve the elementary estimator
q
p
variant in (17) setting λ1 = M1 np and λ2 = M2 logn p
for sufficiently large constants M1 , M2 > 0. Then, with
b satisprobability at least 1 − 2 exp(−Cp), the solution Σ
fies the following error bounds:

r
p b
log p
, kΣ − Σ∗ k∞,off ≤ 2M2
,
n
n
r
r


b − Σ∗ kF ≤ 8 max M1 k1 p , M2 k2 log p ,
kΣ
n
n
q
n q
o2
q 
b − Σ∗ |||∗ ≤ 32 n max M1 k1 p , M2 k2 log p
|||Σ
,
M1
p
n
n

b − Σ∗ |||2 ≤ 2M1
|||Σ

b − Σ∗ k1,off ≤
kΣ

32
M2

r

q

n
log p



q
n q
o2
p
max M1 kn1 p , M2 k2 log
.
n

6. Experiments
Simulation We first confirm the usefulness of our framework in the presence of superposition structures. Specifically, we focus on covariance estimation where the true
covariance has a sparse plus low-rank structure. We consider Σ∗ = Σ∗1 + Σ∗2 , where Σ∗1 = 0.5(1p 1Tp ). and
Σ∗2 = Ip/5 ⊗ (0.2(15 1T5 ) + 0.2I5 ), where ⊗ denotes the
Kronecker product. We perform 100 simulation runs. For
each simulation run, we generate n = 100 observations
from N (0, Σ∗ ). We compare our “Elem-Super-Moment”
estimator with the thresholding method of Bickel & Levina (2008a) and the well-conditioned estimator of Ledoit
& Wolf (2003). For each method, the tuning parameters
are set using 5-fold cross validation with Frobenius norm
as described in Bickel & Levina (2008a). We consider
p = 200, 400. As performance measures, we used the spectral, Frobenius, nuclear and matrix 1-norm of the difference
between estimated and true covariance. The results presented in Table 1 show that “Elem-Super-Moment” clearly
outperforms the other methods. In addition, our method
is able to recover the sparsity pattern of the sparse component with True Positive and True Negative rates greater
than 99.50% and 95.24% respectively.
Climate dataset We demonstrate the applicability of our
class of estimators on a climatology dataset. We used 4times daily surface temperature data from NCEP/NCAR
Reanalysis 1. The data is for the year 2011 and uses a 2.5
degree latitude x 2.5 degree longitude global grid covering
90N - 90S, 0E - 357.5E, so that we have 144 × 73 locations. We considered each location as a feature, so that
p = 144 × 73 = 10512, and used observations across time

-50

0

50

100

150

-150

-100

-50

0

50

100

1.0
0.8
0.4

150

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

50

Figure 2. Our Elem-Super-Moment covariance estimates (with `1
and nuclear norms) for Hi-C data analysis on chromosome 14;
(Left) HindIII dataset, and (Right) Ncol dataset

-50

-50

0

50

(a) PCA on the sample covariance matrix

0

0.0

-100

0.0

-150

0.2

0.2

-50

-50

0.4

0.6

0.8
0.6

50
0

0

50

1.0

Elementary Estimators for Sparse Covariance Matrices and other Structured Moments

action frequencies, which would correspond to the population covariance matrix. We consider two Hi-C datasets
(b) PCA on our Elem-Moment covariance estimate with `1 norm. taken from Lieberman-Aiden et al. (2009) which are biological replicates assembled using different restriction enzymes (HindIII and NcoI). One approach to validate the
Figure 1. Contour plots of the first two principal components uscontact map (or population covariance) estimators would
ing PCA on (top) the sample covariance matrix, and (bottom) our
be to estimate the contact map based on each dataset sepaElem-Moment covariance estimate with `1 norm.
rately, and measure the “reproducibility” between both estimates, measured using Spearman correlation. We applied
as samples, so that n = 4 × 365 = 1460, and computed
our “Elem-Super-Moment” estimator with two regularizathe p × p spatial sample covariance matrix. To evaluate the
tion functions set to `1 norm and the nuclear norm, to the
covariance matrix estimates, we used empirical orthogonal
normalized data provided by Hu et al. (2008). The raw data
functions (EOFs), which actually correspond to the princiexhibits an average correlation across the 23 chromosomes
pal components of the covariance matrix, which are comof 0.7241, that of the normalized data 0.8041 (see Hu et al.
monly employed in spatio-temporal statistics to investigate
(2008)). Our estimator improves the correlation further to
spatial patterns in data. By visualizing the spacial contour
0.8355.
plots of a given EOF, one can get an idea of which geoVisualizing the fine details of the contact map is challenggraphical regions contribute greatly to that principle coming due to the high resolution of the map (2761 × 2761
ponent. We depict these contour plots for the first two prinmatrix in this experiment). In Figure 2, we thus depict a
cipal components using (a) PCA on the sample covariance
portion of our estimated contact map for chromosomes 14
matrix and (b) PCA on our Elem-Moment estimate with
only, so the reader can get an idea of the data structure. As
the regularization set to the `1 norm. As can be seen from
can be seen from Figure 2, the contact maps look quite simthe figures, our method clearly separates the Northern and
ilar for both biological replicates, which corroborates our
Southern hemispheres, which is as expected by climate scihigh correlation value of 0.8355 noted earlier. (For comentists. In contrast, PCA on the sample covariance itself is
parison with the original data, a picture of the raw data
unable to make that distinction.
is provided in Figure 1 B&D of Lieberman-Aiden et al.
(2009).) The bright diagonals correspond to nearby interHi-C dataset We also illustrate the usefulness of our
actions within the same chromosome. We can also distinclass of estimators on data from Hi-C, a very recent
guish some interaction blocks around the diagonals as well
methodology to study the 3-D architecture of genomes.
as more distal interactions. Given this encouraging prelimBriefly, the data consists of the observed frequencies of
inary analysis, we plan to perform an in-depth biological
chromatin interaction between any two genomic loci (out
analysis using our Elem-Moment estimators in future work.
of a total of as many as 40, 000 loci). A comprehensive
Acknowledgments
review of Hi-C is provided in Dekker et al. (2013). Note
E.Y and P.R. acknowledge the support of ARO via
that these empirical interaction frequencies can be collated
W911NF-12-1-0390 and NSF via IIS-1149803, IISas a sample covariance matrix. The goal then is to esti1320894, DMS-1264033.
mate the true contact map comprising the expected inter-150

-100

-50

0

50

100

150

-150

-100

-50

0

50

100

150

Elementary Estimators for Sparse Covariance Matrices and other Structured Moments

References
Agarwal, A., Negahban, S., and Wainwright, M. J. Noisy matrix
decomposition via convex relaxation: Optimal rates in high dimensions. Annals of Statistics, 40(2):1171–1197, 2012.
Bickel, P. J. and Levina, E. Covariance regularization by thresholding. Annals of Statistics, 36(6):2577–2604, 2008a.
Bickel, P. J. and Levina, E. Regularized estimation of large covariance matrices. Annals of Statistics, 36(1):199–227, 2008b.
Cai, T. T., Zhang, C.-H., and Zhou, H. H. Optimal rates of convergence for covariance matrix estimation. Annals of Statistics,
2010. To appear.
Candès, E. and Tao, T. The Dantzig selector: Statistical estimation
when p is much larger than n. Annals of Statistics, 35(6):2313–
2351, 2007.
Chandrasekaran, V., Recht, B., Parrilo, P. A., and Willsky, A. S.
The convex geometry of linear inverse problems. In 48th
Annual Allerton Conference on Communication, Control and
Computing, 2010.
Chandrasekaran, V., Sanghavi, S., Parrilo, P. A., and Willsky,
A. S. Rank-sparsity incoherence for matrix decomposition.
SIAM Journal on Optimization, 21(2), 2011.
Combettes, P. L. and Pesquet, J. C. A proximal decomposition
method for solving convex variational inverse problems. Inverse Problems, 24(27), 2008.
Dai, B., Ding, S., and Wahba, G. Multivariate bernoulli distribution. Bernoulli, 19(4):1465–1483, 2013.
Dekker, J., Marti-Renom, M. A., and Mirny, L. A. Exploring the
three-dimensional organization of genomes: interpreting chromatin interaction data. Nature reviews. Genetics, 14(6):390–
403, 2013.
El Karoui, N. Operator norm consistent estimation of large dimensional sparse covariance matrices. Annals of Statistics, 36
(6):2717–2756, 2008.
Furrer, R. and Bengtsson, T. Estimation of high-dimensional
prior and posterior covariance matrices in kalman filter variants. Journal of Multivariate Analysis, 98(2):227–255, 2007.
Hsu, D., Kakade, S. M., and Zhang, T. Robust matrix decomposition with sparse corruptions. IEEE Trans. Inform. Theory, 57:
7221–7234, 2011.
Hu, M., Deng, K., Selvaraj, S., Qin, Z., Ren, B., and Liu, J. S.
Hicnorm: removing biases in hi-c data via poisson regression.
Bioinformatics, 28(23):3131–3133, 2008.
Jalali, A., Ravikumar, P., Sanghavi, S., and Ruan, C. A dirty
model for multi-task learning. In Neur. Info. Proc. Sys. (NIPS),
23, 2010.
Johnstone, I. M. On the distribution of the largest eigenvalue in
principal components analysis. Annals of Statistics, 29(2):295–
327, 2001.
Johnstone, I. M. and Lu, A. Y. Sparse principal components analysis. Unpublished Manuscript, 2004.

Ledoit, O. and Wolf, M. A well-conditioned estimator for largedimensional covariance matrices. Journal of Multivariate
Analysis, 88:365411, 2003.
Lieberman-Aiden, E., van Berkum, N. L., Williams, L., Imakaev,
M., and Ragoczy, T. [and others]. Comprehensive mapping of
long-range interactions reveals folding principles of the human
genome. Science, 326(5950):289–293, October 2009.
McCoy, M. and Tropp, J. A. Two proposals for robust pca using
semidefinite programming. Electron. J. Statist., 5:1123–1160,
2011.
Negahban, S., Ravikumar, P., Wainwright, M. J., and Yu, B.
A unified framework for high-dimensional analysis of Mestimators with decomposable regularizers. Statistical Science,
27(4):538–557, 2012.
Ravikumar, P., Wainwright, M. J., and Lafferty, J. Highdimensional ising model selection using `1 -regularized logistic
regression. Annals of Statistics, 38(3):1287–1319, 2010.
Ravikumar, P., Wainwright, M. J., Raskutti, G., and Yu, B. Highdimensional covariance estimation by minimizing `1 -penalized
log-determinant divergence. Electronic Journal of Statistics, 5:
935–980, 2011.
Ribes, A., Azais, J. M., and Planton, S. Adaptation of the optimal
fingerprint method for climate change detection using a wellconditioned covariance matrix estimate. Journal of Climate
Dynamics, 33:707–722, 2009.
Rothman, A. J., Levina, E., and Zhu, J. Generalized thresholding
of large covariance matrices. Journal of the American Statistical Association (Theory and Methods), 104:177–186, 2009.
Wainwright, M. J. and Jordan, M. I. Graphical models, exponential families and variational inference. Foundations and Trends
in Machine Learning, 1(1–2):1—305, December 2008.
Watson, G. A. Characterization of the subdifferential of some
matrix norms. Linear Algebra and its Applications, 170:33–
45, 1992.
Wikle, C. K. and Cressie, N. A dimension reduced approach to
space-time kalman filtering. Biometrika, 86:815–829, 1999.
Xu, H., Caramanis, C., and Sanghavi, S. Robust pca via outlier pursuit. IEEE Transactions on Information Theory, 58(5):
3047–3064, 2012.
Yang, E. and Ravikumar, P. Dirty statistical models. In Neur. Info.
Proc. Sys. (NIPS), 26, 2013.

