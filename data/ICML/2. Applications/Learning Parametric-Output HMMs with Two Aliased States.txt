Learning Parametric-Output HMMs with Two Aliased States

Roi Weiss
Department of Computer Science, Ben-Gurion University, Beer Sheva, 84105, Israel.

ROIWEI @ CS . BGU . AC . IL

Boaz Nadler
BOAZ . NADLER @ WEIZMANN . AC . IL
Department of Computer Science and Applied Mathematics, The Weizmann Institute of Science, Rehovot, 76100, Israel.

Abstract
In various applications involving hidden Markov
models (HMMs), some of the hidden states are
aliased, having identical output distributions.
The minimality, identifiability and learnability
of such aliased HMMs have been long standing
problems, with only partial solutions provided
thus far. In this paper we focus on parametricoutput HMMs, whose output distributions come
from a parametric family, and that have exactly
two aliased states. For this class, we present
a complete characterization of their minimality
and identifiability. Furthermore, for a large family of parametric output distributions, we derive
computationally efficient and statistically consistent algorithms to detect the presence of aliasing and learn the aliased HMM transition and
emission parameters. We illustrate our theoretical analysis by several simulations.

1. Introduction
HMMs are a fundamental tool in the analysis of time series. A discrete time HMM with n hidden states is characterized by a n √ó n transition matrix and by the emissions
probabilities from these n states. In several applications,
the HMMs, or more general processes such as partially observable Markov decision processes, are aliased, with some
states having identical output distributions. In modeling of
ion channel gating, for example, a common assumption is
that at any given time an ion channel can be in only one of a
finite number of hidden states, some of which are open and
conducting current while others are closed, see e.g. Fredkin
& Rice (1992). Fitting an aliased HMM to electric current
measurements, allows biologists to gain important insights
regarding the gating process. Other examples appear in the
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

fields of reinforcement learning (Chrisman, 1992; McCallum, 1995; Brafman & Shani, 2004; Shani et al., 2005) and
robot navigation (Jefferies & Yeap, 2008; Zatuchna & Bagnall, 2009). In the latter case, aliasing occurs whenever different spatial locations appear (statistically) identical to the
robot, given its limited sensing devices. As a last example,
HMMs with several silent states that do not emit any output (Leggetter & Woodland, 1994; Stanke & Waack, 2003;
Brejova et al., 2007), can also be viewed as aliased.
Key notions related to the study of HMMs, be them aliased
or not, are their minimality, identifiability and learnability:
Minimality. Is there an HMM with fewer states that induces the same distribution over all output sequences?
Identifiability. Does the distribution over all output sequences uniquely determines the HMM‚Äôs parameters, up
to a permutation of its hidden states?
Learning. Given a long output sequence from a minimal
and identifiable HMM, efficiently learn its parameters.
For non-aliased HMMs, these notions have been intensively studied and by now are relatively well understood,
see for example Petrie (1969); Finesso (1990); Leroux
(1992); Allman et al. (2009) and CappeÃÅ et al. (2005). The
most common approach to learn the parameters of an HMM
is via the Baum-Welch iterative algorithm (Baum et al.,
1970). Recently, tensor decompositions and other computationally efficient spectral methods have been developed
to learn non-aliased HMMs (Hsu et al., 2009; Siddiqi et al.,
2010; Anandkumar et al., 2012; Kontorovich et al., 2013).
In contrast, the minimality, identifiability and learnability of aliased HMMs have been long standing problems,
with only partial solutions provided thus far. For example, Blackwell & Koopmans (1957) characterized the identifiability of a specific aliased HMM with 4 states. The
identifiability of deterministic output HMMs, where each
hidden state outputs a deterministic symbol, was partially
resolved by Ito et al. (1992). To the best of our knowledge,
precise characterizations of the minimality, identifiability
and learnability of probabilistic output HMMs with aliased

Learning Parametric-Output HMMs with Two Aliased States

states are still open problems. In particular, the recently
developed tensor and spectral methods mentioned above,
explicitly require the HMM to be non-aliasing, and are not
directly applicable to learning aliased HMMs.

{fŒ∏ : Y ‚Üí R | Œ∏ ‚àà Œò} be a family of parametric probability density functions where Œò is a suitable parameter
space. A parametric-output HMM is defined by a tuple
H = (A, Œ∏, œÄ 0 ) where A is the n √ó n transition matrix
between the hidden states

Main results. In this paper we study the minimality,
identifiability and learnability of parametric-output HMMs
that have exactly two aliased states. This is the simplest
possible class of aliased HMMs, and as shown below, even
its analysis is far from trivial. Our main contributions are
as follows: First, we provide a complete characterization of
their minimality and identifiability, deriving necessary and
sufficient conditions for each of these notions to hold. Our
identifiability conditions are easy to check for any given 2aliased HMM, and extend those derived by Ito et al. (1992)
for deterministic outputs. Second, we address the problem
of learning a possibly aliased HMM from a long sequence
of its outputs. To this end, we first derive an algorithm to
detect whether an observed output sequence corresponds
to a non-aliased HMM or to an aliased one. In the former
case, the HMM can be learned by various methods, such as
Anandkumar et al. (2012); Kontorovich et al. (2013). In the
latter case we show how the aliased states can be identified
and present a method to recover the HMM parameters. Our
approach is applicable to any family of output distributions
whose mixtures are efficiently learnable. Examples include
high dimensional Gaussians and products distributions, see
Feldman et al. (2008); Belkin & Sinha (2010); Anandkumar et al. (2012) and references therein. After learning the
output mixture parameters, our moment-based algorithm
requires only a single pass over the data. As far as we
know, it is the first statistically consistent and computationally efficient scheme to handle 2-aliased HMMs. While our
approach may be extended to more complicated aliasing,
such cases are beyond the scope of this paper. We conclude
with some simulations illustrating the performance of our
proposed algorithms.

Aij = Pr(Xt+1 = i | Xt = j) = P (i | j),

2. Definitions & Problem Setup

œÄ 0 ‚àà Rn is the distribution of the initial state, and the vector of parameters Œ∏ = (Œ∏1 , Œ∏2 , . . . , Œ∏n ) ‚àà Œòn determines
the n probability density functions (fŒ∏1 , fŒ∏2 , . . . , fŒ∏n ).
To produce the HMM‚Äôs output sequence, first a Markov se‚àí1
quence of hidden states x = (xt )Tt=0
is generated according to the distribution
P (x) = œÄx00

TY
‚àí1

P (xt | xt‚àí1 ).

t=1
‚àí1
Next, the output sequence y = (yt )Tt=0
, where the output
yt at time t depends only on xt , is generated according to

P (y | x) =

TY
‚àí1

P (yt | xt ) =

t=0

TY
‚àí1

fŒ∏xt (yt ).

t=0

We denote by PH,k : Y k ‚Üí R the joint distribution of
the first k consecutive outputs of the HMM H. For y =
(y0 , . . . , yk‚àí1 ) ‚àà Y k this distribution is given by
X
PH,k (y) =
P (y | x)P (x).
x‚àà[n]k

Further we denote by PH = {PH,k | k ‚â• 1} the set of all
these distributions.
2-Aliased HMMs. For an HMM H with output parameters Œ∏ = (Œ∏1 , Œ∏2 , . . . , Œ∏n ) ‚àà Œòn we say that states i and j
are aliased if Œ∏i = Œ∏j . In this paper we consider the special
case where H has exactly two aliased states, denoted as 2AHMM. Without loss of generality, we assume the aliased
states are the two last ones, n‚àí1 and n. Thus, Œ∏i 6= Œ∏j for
all 1 ‚â§ i < j ‚â§ n ‚àí 1, whereas Œ∏n‚àí1 = Œ∏n .

Notation. We denote by In the n √ó n identity matrix and
1n = (1, . . . , 1)T ‚àà Rn . For v ‚àà Rn , diag(v) is the
n √ó n diagonal matrix with entries vi on its diagonal. The
i-th row and column of a matrix A ‚àà Rn√ón are denoted
by A[i,¬∑] and A[¬∑,i] , respectively. We also denote [n] =
{1, 2, . . . , n}. For a discrete random variable X we abbreviate P (x) for Pr(X = x). For a second random variable
Z, the quantity P (z | x) denotes either Pr(Z = z | X = x),
or the conditional density p(Z = z|X = x), depending on
whether Z is discrete or continuous.

We denote the vector of the n‚àí1 unique output parameters
of H by Œ∏ÃÑ = (Œ∏1 , Œ∏2 , . . . , Œ∏n‚àí2 , Œ∏n‚àí1 ) ‚àà Œòn‚àí1 . For future
use, we define the aliased kernel KÃÑ ‚àà R(n‚àí1)√ó(n‚àí1) as the
matrix of inner products between the n‚àí1 different fŒ∏i ‚Äôs,
Z
KÃÑij ‚â° hfŒ∏i , fŒ∏j i =
fŒ∏i (y)fŒ∏j (y)dy, i, j ‚àà [n‚àí1]. (1)

Hidden Markov Models. Consider a discrete-time
HMM with n hidden states {1, . . . , n}, whose output alphabet Y is either discrete or continuous. Let FŒ∏ =

(A1) The parametric family FŒ∏ of the output distributions
is
independent of order n: for any distinct {Œ∏i }ni=1 ,
Plinearly
n
i=1 ai fŒ∏i ‚â° 0 iff ai = 0 for all i ‚àà [n].

Y

Assumptions. As in previous works (Leroux, 1992; Kontorovich et al., 2013), we make the following standard assumptions:

Learning Parametric-Output HMMs with Two Aliased States

(A2) The transition matrix A is ergodic and its unique stationary distribution œÄ = (œÄ1 , œÄ2 , . . . , œÄn ) is positive.
Note that assumption (A1) implies that the parametric family FŒ∏ is identifiable, namely fŒ∏ = fŒ∏0 iff Œ∏ = Œ∏0 . It also
implies that the kernel matrix KÃÑ of (1) is full rank n‚àí1.

3. Decomposing the transition matrix A
The main tool in our analysis is a novel decomposition of
the 2A-HMM‚Äôs transition matrix into its non-aliased and
aliased parts. As shown in Lemma 1 below, the aliased part
consists of three rank-one matrices, that correspond to the
exit from, entrance to, and dynamics within the two aliased
states. This decomposition is used to derive the conditions
for minimality and identifiability (Section 4), and plays a
key role in learning the HMM (Section 5).
To this end, we introduce a pseudo-state nÃÑ, combining the
two aliased states n‚àí1 and n. We define
œÄnÃÑ = œÄn‚àí1 + œÄn

and

Œ≤ = œÄn‚àí1 /œÄnÃÑ .

(2)

We shall make extensive use of the following two matrices:
Ô£∂
Ô£´
0 0
.
.
.. .. Ô£∑
Ô£¨
In‚àí2
Ô£∑ ‚àà R(n‚àí1)√ón ,
B = Ô£¨
Ô£≠
0 0 Ô£∏
0 ...
0 1 1
Ô£´
Ô£∂
0
.
.. Ô£∑
Ô£¨
In‚àí2
Ô£¨
Ô£∑
n√ó(n‚àí1)
Ô£¨
.
CŒ≤ = Ô£¨
0 Ô£∑
Ô£∑‚ààR
Ô£≠ 0 ...
Ô£∏
0
Œ≤
0 ...
0 1‚àíŒ≤
As explained below, these matrices can be viewed as projection and lifting operators, mapping between non-aliased
and aliased quantities.
Non-aliased part. The non-aliased part of A is a stochastic matrix AÃÑ ‚àà R(n‚àí1)√ó(n‚àí1) , obtained by merging the two
aliased states n‚àí1 and n into the pseudo-state nÃÑ. Its entries
are given by
Ô£´
Ô£∂
P (1 | nÃÑ)
Ô£∑
Ô£¨
..
Ô£∑
Ô£¨
A[1:n‚àí2]√ó[1:n‚àí2]
.
Ô£∑ , (3)
AÃÑ = Ô£¨
Ô£¨
P (n‚àí2 | nÃÑ) Ô£∑
Ô£≠
Ô£∏
P (nÃÑ | 1) . . . P (nÃÑ | n‚àí2)
P (nÃÑ | nÃÑ)
where the transition probabilities into the pseudo-state are
P (nÃÑ | j)

= P (n‚àí1 | j) + P (n | j),

‚àÄj ‚àà [n],

the transition probabilities out of the pseudo-state are defined with respect to the stationary distribution by
P (i | nÃÑ)

=

Œ≤P (i | n‚àí1) + (1‚àíŒ≤)P (i | n),

‚àÄi ‚àà [n]

and lastly, the probability to stay in the pseudo-state is
P (nÃÑ | nÃÑ)

=

Œ≤P (nÃÑ | n‚àí1) + (1‚àíŒ≤)P (nÃÑ | n).

It is easy to check that the unique stationary distribution of
AÃÑ is œÄÃÑ = (œÄ1 , œÄ2 , . . . , œÄn‚àí2 , œÄnÃÑ ) ‚àà Rn‚àí1 . Finally, note that
AÃÑ = BACŒ≤ , œÄÃÑ = BœÄ and œÄ = CŒ≤ œÄÃÑ, justifying the lifting
and projection interpretation of the matrices B, CŒ≤ .
Aliased part. Next we introduce some key quantities that
distinguish between the two aliased states. Let suppin =
{j ‚àà [n] | P (nÃÑ | j) > 0} be the set of states that can move
into at least one of the aliased states. We define
(
P (n‚àí1 | j)
j ‚àà suppin
P (nÃÑ | j)
(4)
Œ±j =
0
otherwise,
as the relative probability of moving from state j to state
n‚àí1, conditional on moving to either n‚àí1 or n. We define
the two vectors Œ¥ out , Œ¥ in ‚àà Rn‚àí1 as follows: ‚àÄi, j ‚àà [n‚àí1],
(
P (i | n‚àí1) ‚àí P (i | n)
i<n‚àí1
out
Œ¥i
=
(5)
P (nÃÑ | n‚àí1) ‚àí P (nÃÑ | n) i = n ‚àí 1
Ô£±
Ô£¥
(Œ± ‚àíŒ≤)P (nÃÑ | j)
j < n‚àí1
Ô£¥
Ô£≤ j
in
(6)
Œ¥j =
Œ≤(Œ±n‚àí1 ‚àíŒ≤)P (nÃÑ | n‚àí1)
Ô£¥
Ô£¥
Ô£≥ + (1‚àíŒ≤)(Œ± ‚àíŒ≤)P (nÃÑ | n) j = n‚àí1.
n

In other words, Œ¥ out captures differences in the transition probabilities out of the aliased states. In particular, if
Œ¥ out = 0 then starting from either one of the two aliased
states, the Markov chain evolution is identical. As proven
in Theorem 1 below, such an HMM is not minimal as its
two aliased states can be lumped together,
Similarly, Œ¥ in compares the relative probabilities into the
aliased states Œ±j , to the stationary one Œ≤ = œÄn‚àí1 /œÄnÃÑ . This
quantity also plays a role in the minimality of the HMM.
Lastly, for our decomposition, we define the scalar
Œ∫ = (Œ±n‚àí1 ‚àí Œ≤)P (nÃÑ | n‚àí1) ‚àí (Œ±n ‚àí Œ≤)P (nÃÑ | n).

(7)

Decomposing A. The following lemma provides a decomposition of the transition matrix in terms of AÃÑ, Œ¥ out ,
Œ¥ in , Œ∫ and Œ≤ (all proofs are given in the Appendix).
Lemma 1. The transition matrix A of a 2A-HMM can be
decomposed as
A = CŒ≤ AÃÑB + CŒ≤ Œ¥ out cTŒ≤ + b(Œ¥ in )T B + Œ∫ bcTŒ≤ ,

(8)

where cŒ≤ T = (0, . . . , 0, 1‚àíŒ≤, ‚àíŒ≤) ‚àà Rn and b =
(0, . . . , 0, 1, ‚àí1)T ‚àà Rn .
In (8), the first term is the merged transition matrix AÃÑ ‚àà
R(n‚àí1)√ó(n‚àí1) lifted back into Rn√ón . This term captures all

Learning Parametric-Output HMMs with Two Aliased States

of the non-aliased transitions. The second matrix is zero
except in the last two columns, accounting for the exit transition probabilities from the two aliased states. Similarly,
the third matrix is zero except in the last two rows, differentiating the entry probabilities. The fourth term is non-zero
only on the lower right 2 √ó 2 block involving the aliased
states n ‚àí 1, n. This term corresponds to the internal dynamics between them. Note that each of the last three terms
is at most a rank-1 matrix, which together can be seen as a
perturbation due to the presence of aliasing.
In Section 4 we will show the importance of Eq. (8) for
the minimality and identifiability of two-aliased HMMs. In
section 5 we shall see that given a long output sequence
from the HMM, the presence of aliasing can be detected
and the quantities AÃÑ, Œ¥ out , Œ¥ in , Œ∫ and Œ≤ can all be estimated
from it. An estimate for A is then obtained via Eq. (8).

4. Minimality and Identifiability
Two HMMs H and H 0 are said to be equivalent if their observed output sequences are statistically indistinguishable,
namely PH 0 = PH . Similarly, an HMM H is minimal if
there is no equivalent HMM with fewer number of states.
Note that if H is non-aliased then Assumptions (A1-A2)
readily imply that it is also minimal (Leroux, 1992). In this
section we present necessary and sufficient conditions for a
2A-HMM to be minimal, and for two minimal 2A-HMMs
to be equivalent. Finally, we derive necessary and sufficient
conditions for a minimal 2A-HMM to be identifiable.
4.1. Minimality
The minimality of an HMM is closely related to the notion
of lumpability: can hidden states be merged without changing the distribution PH (Fredkin & Rice, 1986; White et al.,
2000; Huang et al., 2014). Obviously, an HMM is minimal
iff no subset of hidden states can be merged. The following theorem gives precise conditions for the minimality of
a 2A-HMM.
Theorem 1. Let H be a 2A-HMM satisfying Assumptions
(A1-A2) whose initial state X0 is distributed according to
œÄ 0 = (œÄ10 , œÄ20 , . . . , Œ≤ 0 œÄnÃÑ0 , (1‚àíŒ≤ 0 )œÄnÃÑ0 ). Then,
(i) If œÄnÃÑ0 6= 0 and Œ≤ 0 6= Œ≤ then H is minimal iff Œ¥ out 6= 0.
(ii) If œÄnÃÑ0 = 0 or Œ≤ 0 = Œ≤ then H is minimal iff both
Œ¥ out 6= 0 and Œ¥ in 6= 0.
By Theorem 1, a necessary condition for minimality of a
2A-HMM is that the two aliased states have different exit
probabilities, Œ¥ out 6= 0. Namely, there exists a non-aliased
state i ‚àà [n‚àí2] such that P (i | n‚àí1) 6= P (i | n). Otherwise
the two aliased states can be merged. If the 2A-HMM is
started from its stationary distribution, then an additional

necessary condition is Œ¥ in 6= 0. This last condition implies
that there is a non-aliased state j ‚àà suppin \{n‚àí1, n} with
relative entrance probability Œ±j 6= Œ≤.
4.2. Identifiability
Recall that an HMM H is (strictly) identifiable if PH
uniquely determines the transition matrix A and the output
parameters Œ∏, up to a permutation of the hidden states. We
establish the conditions for identifiability of a 2A-HMM in
two steps. First we derive a novel geometric characterization of the set of all minimal HMMs that are equivalent to
H, up to a permutation of the hidden states (Theorem 2).
Then we give necessary and sufficient conditions for H to
be identifiable, namely for this set to be the singleton set,
consisting of only H itself (Appendix C). In the process,
we provide a simple procedure (Algorithm 1) to determine
whether a given minimal 2A-HMM is identifiable or not.
Equivalence between minimal 2A-HMMs. Necessary
and sufficient conditions for the equivalence of two minimal HMMs were studied in several works (Finesso, 1990;
Ito et al., 1992; Vanluyten et al., 2008). We now provide
analogous conditions for parametric output 2A-HMMs.
Toward this end, we define the following 2-dimensional
family of matrices S(œÑn‚àí1 , œÑn ) ‚àà Rn√ón given by
Ô£∂
Ô£´
0
0
..
..
Ô£∑
Ô£¨
In‚àí2
.
.
Ô£∑
Ô£¨
S(œÑn‚àí1 , œÑn ) = Ô£¨
0
0 Ô£∑
Ô£∑.
Ô£¨
Ô£≠ 0 ...
0
œÑn‚àí1
œÑn Ô£∏
0 ...
0 1‚àíœÑn‚àí1 1‚àíœÑn
Clearly, for œÑn‚àí1 6= œÑn , S is invertible. As in (Ito et al.,
1992), consider then the following similarity transformation of the transition matrix A,
AH (œÑn‚àí1 , œÑn ) = S(œÑn‚àí1 , œÑn )‚àí1 AS(œÑn‚àí1 , œÑn ). (9)
It is easy to verify that 1Tn AH = 1Tn . However, AH is
not necessarily stochastic, as depending on œÑn‚àí1 , œÑn it may
have negative entries. The following lemma resolves the
equivalence of 2A-HMMs, in terms of this transformation.
Lemma 2. Let H = (A, Œ∏, œÄ) be a minimal 2A-HMM
satisfying Assumptions (A1-A2). Then a minimal HMM
H 0 = (A0 , Œ∏ 0 , œÄ 0 ) with n0 states is equivalent to H iff
n0 = n and there exists a permutation matrix Œ† ‚àà Rn√ón
and œÑn‚àí1 > œÑn such that Œ∏ 0 = Œ† Œ∏ and
œÄ 0 = Œ† S(œÑn‚àí1 , œÑn )‚àí1 œÄ, A0 = Œ† AH (œÑn‚àí1 , œÑn ) Œ†‚àí1 ‚â• 0.
The feasible region. By Lemma 2, any matrix
AH (œÑn‚àí1 , œÑn ) whose entries are all non-negative yields an
HMM equivalent to the original one. We thus define the
feasible region of H by
ŒìH = {(œÑn‚àí1 , œÑn ) ‚àà R2 | AH (œÑn‚àí1 , œÑn ) ‚â• 0, œÑn‚àí1 > œÑn }. (10)

Learning Parametric-Output HMMs with Two Aliased States

By definition, ŒìH is non-empty, since (œÑn‚àí1 , œÑn ) = (1, 0)
recovers the original matrix A. As we show below, ŒìH is
determined by three simpler regions Œì1 , Œì2 , Œì3 ‚äÇ R2 . The
region Œì1 ensures that all entries of AH are non-negative
except possibly in the lower right 2√ó2 block corresponding
to the two aliased states. The regions Œì2 and Œì3 ensure nonnegativity of the latter, depending on whether the aliased
relative probabilities of (4) satisfy Œ±n‚àí1 ‚â• Œ±n or Œ±n‚àí1 <
Œ±n , respectively. For ease of exposition we assume as a
convention that P (nÃÑ | n‚àí1) ‚â• P (nÃÑ | n).
Theorem 2. Let H be a minimal 2A-HMM satisfymin min
ing Assumptions (A1-A2).
There exist (œÑn‚àí1
, œÑn ),
max max
(œÑn‚àí1
, œÑn ), (œÑ ‚àí , œÑ + ) ‚àà R2 , and convex monotonic decreasing functions f, g : R ‚Üí R such that
(
Œì1 ‚à© Œì2 Œ±n‚àí1 ‚â• Œ±n
ŒìH =
Œì1 ‚à© Œì3 Œ±n‚àí1 < Œ±n ,
where the regions Œì1 , Œì2 , Œì3 ‚äÇ R2 are given by
Œì1

=

min max
[œÑn‚àí1
, œÑn‚àí1 ]

Œì2

=

[œÑ + , ‚àû) √ó [œÑ ‚àí , œÑ + ]

Œì3

=

{(œÑn‚àí1 , œÑn ) ‚àà Œì1 | f (œÑn‚àí1 ) ‚â§ œÑn ‚â§ g(œÑn‚àí1 ) }.

√ó

[œÑnmax , œÑnmin ]

In addition, the set ŒìH is connected.
The feasible regions in the two possible cases (Œ±n‚àí1 ‚â• Œ±n
or Œ±n‚àí1 < Œ±n ) are illustrated in Appendix C, Fig.4.
Strict Identifiability. By Lemma 2, for strict identifiability of H, ŒìH should be the singleton set ŒìH = {(1, 0)}.
Due to lack of space, sufficient and necessary conditions
for this to hold, as well as a corresponding simple procedure to determine whether a 2A-HMM is identifiable, are
given in Appendix C.2.
Remark. While beyond the scope of this paper, we note
that instead of strict identifiability of a given HMM, several works studied a different concept of generic identifiability (Allman et al., 2009), proving that under mild conditions the class of HMMs is generically identifiable. In
contrast, if we restrict ourselves to the class of 2A-HMMs,
then our Theorem 2 implies that this class is generically
non-identifiable. The reason is that by Theorem 2, for any
2A-HMM whose matrix A has all its entries positive, there
are an infinite number of equivalent 2A-HMMs, implying
its non-identifiability.

5. Learning a 2A-HMM
‚àí1
Let (Yt )Tt=0
be an output sequence generated by a
parametric-output HMM that satisfies Assumptions (A1A2) and initialized with its stationary distribution, X0 ‚àº œÄ.
We assume the HMM is either non-aliasing, with n ‚àí 1

‚àí1
(Yt )Tt=0

determine model order n‚àí1
and fit Œ∏ÃÑ = (Œ∏1 , . . . , Œ∏n‚àí1 )

(i)

Is the HMM 2-aliasing?

(ii)

non-aliasing
(n ‚àí 1 states)
(iii)

2-aliasing
(n states)

estimate AÃÑ
(iv)

identify aliasing
component Œ∏n‚àí1
and estimate A

Figure 1. Learning a 2A-HMM.

states, or 2-aliasing with n states. We further assume that
the HMM is minimal and identifiable, as otherwise its parameters cannot be uniquely determined.
In this section we study the problems of detecting whether
the HMM is aliasing and recovering its output parameters
‚àí1
Œ∏ and transition matrix A, all in terms of (Yt )Tt=0
. As outlined in Fig.1, the proposed learning procedure consists of
the following steps:
(i) Determine the number of output components n‚àí1 and
estimate the n‚àí1 unique output distribution parameters Œ∏ÃÑ and the projected stationary distribution œÄÃÑ.
(ii) Detect if the HMM is 2-aliasing.
(iii) In case of a non-aliased HMM, estimate the (n‚àí1) √ó
(n ‚àí 1) transition matrix AÃÑ, as for example in Kontorovich et al. (2013) or Anandkumar et al. (2012).
(iv) In case of a 2-aliased HMM, identify the component
Œ∏n‚àí1 corresponding to the two aliased states, and estimate the n √ó n transition matrix A.
We now describe in detail each of these steps. As far as
we know, our learning procedure is the first to consistently
learn a 2A-HMM in a computationally efficient way. In
particular, the solutions for problems (ii) and (iv) are new.
Estimating the output distribution parameters. As the
HMM is stationary, each observable Yt is a random realization from the following parametric mixture model,
Y ‚àº

n‚àí1
X

œÄÃÑi fŒ∏ÃÑi (y).

(11)

i=1

Hence, the number of unique output components n ‚àí 1, the
corresponding output parameters Œ∏ÃÑ and the projected stationary distribution œÄÃÑ can be estimated by fitting a mixture
‚àí1
model (11) to the observed output sequence (Yt )Tt=0
.
Consistent methods to determine the number of components in a mixture are well known in the literature (Titter-

Learning Parametric-Output HMMs with Two Aliased States

ington et al., 1985). Estimating Œ∏ÃÑ and œÄÃÑ can be done by either the EM algorithm, or any recently developed spectral
method (Dasgupta, 1999; Achlioptas & McSherry, 2005;
Anandkumar et al., 2012). As our focus is on the aliasing
aspects of the HMM, in what follows we assume that the
number of unique output components n‚àí1, the output parameters Œ∏ÃÑ and the projected stationary distribution œÄÃÑ are
exactly known. As in Kontorovich et al. (2013), it is possible to show that our method is robust to small errors in
these quantities (not presented).

In the following, these relations will be used to detect aliasing, identify the aliased states and recover the aliased transition matrix A.
Empirical moments. In practice, the unknown moments
T ‚àí1
(12,13) are estimated from the output sequence (Yt )t=0
by
(t)

MÃÇij

(c)

To solve problems (ii), (iii) and (iv) above, we first introduce the moment-based quantities we shall make use of.
Given Œ∏ÃÑ and œÄÃÑ or estimates of them, for any i, j ‚àà [n‚àí1],
we define the second order moments with time lag t by
(t)

t ‚àà {1, 2, 3}.

(12)

The consecutive in time third order moments are defined by
(c)
Gij

‚àÄc ‚àà [n‚àí1].

= E[fŒ∏i (Y0 )fŒ∏c (Y1 )fŒ∏j (Y2 )],

(13)

We also define the lifted kernel, K = B T KÃÑB ‚àà Rn√ón . One
can easily verify that for a 2A-HMM,
M(t)
G

=

= KÃÑBAt CŒ≤ diag(œÄÃÑ)KÃÑ

(c)

(14)

= KÃÑBA diag(K[¬∑,c] )ACŒ≤ diag(œÄÃÑ)KÃÑ. (15)

Next we define the kernel free moments M (t) , G(c) ‚àà
R(n‚àí1)√ó(n‚àí1) as follows:
M (t) = KÃÑ ‚àí1 M(t) KÃÑ ‚àí1 diag(œÄÃÑ)‚àí1
(c)

G

= KÃÑ

‚àí1

G

(c)

KÃÑ

‚àí1

‚àí1

diag(œÄÃÑ)

.

(17)

Let R(2) , R(3) , F (c) ‚àà R(n‚àí1)√ó(n‚àí1) be given by
(18)

R(3) = M (3) ‚àíM (2) M (1) ‚àíM (1) M (2) + (M (1) )3 (19)
F (c) = G(c) ‚àí M (1) diag(KÃÑ[¬∑,c] )M (1) .

(20)

The following key lemma relates the moments (18, 19, 20)
to the decomposition (8) of the transition matrix A.
Lemma 3. Let H be a minimal 2A-HMM with aliased
states n ‚àí 1 and n. Let AÃÑ, Œ¥ out , Œ¥ in and Œ∫ be defined in
(3,5,6,7) respectively. Then the following relations hold:
M (1)
R

(2)

R

(3)

F

(c)

=
=
=
=

AÃÑ
Œ¥

out

Œ∫R

in T

(Œ¥ )

(2)

KÃÑn‚àí1,c R

(2)

,

‚àÄc ‚àà [n‚àí1].

fŒ∏i (Yl )fŒ∏c (Yl+1 )fŒ∏j (Yl+2 ).

l=0

MÃÇ (t)

= KÃÑ ‚àí1 MÃÇ(t) KÃÑ ‚àí1 diag(œÄÃÑ)‚àí1

(25)

(c)

= KÃÑ ‚àí1 GÃÇ (c) KÃÑ ‚àí1 diag(œÄÃÑ)‚àí1 .

(26)

The empirical estimates for (18,19,20) similarly follow.
To analyze the error between the empirical and population
quantities, we make the following additional assumption:
(A3) The output distributions are bounded. Namely there
exists L > 0 such that ‚àÄi ‚àà [n] and ‚àÄy ‚àà Y, fŒ∏i (y) ‚â§ L.
‚àí1
Lemma 4. Let (Yt )Tt=0
be an output sequence generated
by an HMM satisfying Assumptions (A1-A3). Then, as T ‚Üí
‚àû, for any t ‚àà {1, 2, 3} and c ‚àà [n ‚àí 1], all error terms
1
MÃÇ (t) ‚àí M (t) , RÃÇ(t) ‚àí R(t) and FÃÇ (c) ‚àí F (c) are OP (T ‚àí 2 ).
In fact, due to strong mixing, all of the above quantities are
asymptotically normally distributed (Bradley, 2005).

(16)

Note that by Assumption (A1), the kernel KÃÑ is full rank and
thus KÃÑ ‚àí1 exists. Similarly, by (A2) œÄÃÑ > 0, so diag(œÄÃÑ)‚àí1
also exists. Thus, (16,17) are well defined.

R(2) = M (2) ‚àí (M (1) )2

1
T ‚àí2

T
‚àí3
X

With known (or estimated) KÃÑ, œÄÃÑ the corresponding empirical kernel free moments are given by
GÃÇ

Mij = E[fŒ∏i (Y0 )fŒ∏j (Yt )],

TX
‚àít‚àí1
1
fŒ∏i (Yl )fŒ∏j (Yl+t ),
T ‚àít
l=0

GÃÇij

5.1. Moments

=

5.2. Detection of aliasing
We now proceed to detect if the HMM is aliased (step (ii)
in Fig.1). We pose this as a hypothesis testing problem:
H0 : H is non-aliased with n‚àí1 states
vs.
H1 : H is 2-aliased with n states.
We begin with the following simple observation:
Lemma 5. Let H be a minimal non-aliased HMM with n‚àí1
states, satisfying Assumptions (A1-A3). Then R(2) = 0.
In contrast, if H is 2-aliasing then according to (22) we
have R(2) = Œ¥ out (Œ¥ in )T . In addition, since the HMM is assumed to be minimal and started from the stationary distribution, Theorem 1 implies that both Œ¥ out 6= 0 and Œ¥ in 6= 0.
Thus R(2) is exactly a rank-1 matrix, which we write as

(21)

R(2) = œÉuv T

(22)
(23)

where œÉ is the unique non-zero singular value of R(2) .
Hence, our hypothesis testing problem takes the form:

(24)

H0 : R(2) = 0 vs. H1 : R(2) = œÉuv T with œÉ > 0.

with

kuk2 = kvk2 = 1,

œÉ > 0, (27)

Learning Parametric-Output HMMs with Two Aliased States

In practice, we only have the empirical estimate RÃÇ(2) . Even
if œÉ = 0, this matrix is typically full rank with n‚àí1 nonzero singular values. Our problem is thus detecting the rank
of a matrix from a noisy version of it. There are multiple
methods to do so. In this paper, motivated by Kritchman
& Nadler (2009), we adopt the largest singular value œÉÃÇ1 of
RÃÇ(2) as our test statistic. The resulting test is
if œÉÃÇ1 ‚â• hT return H1 , otherwise return H0 ,

(28)

where hT is a predefined threshold. By Lemma 4, as
T ‚Üí ‚àû the singular values of RÃÇ(2) converge to those of
R(2) . Thus, as the following lemma shows, with a suitable
threshold this test is asymptotically consistent.
Lemma 6. Let H be a minimal HMM satisfying Assumptions (A1-A3) which is either non-aliased or 2-aliased.
1
Then for any 0 <  < 21 , the test (28) with hT = ‚Ñ¶(T ‚àí 2 + )
is consistent. Namely, as T ‚Üí ‚àû
P (reject H1 | H1 holds) + P (reject H0 | H0 holds) ‚Üí 0
and asymptotically the test correctly detects whether the
HMM is non-aliased or 2-aliased.
If the HMM was detected as non-aliasing, then its (n‚àí1) √ó
(n‚àí1) transition matrix can be estimated, for example, by
the spectral methods of Kontorovich et al. (2013) or Anandkumar et al. (2012). We now turn our attention to the case
where the HMM was detected as an aliased one.
Estimating the non-aliased transition matrix AÃÑ. It is
easy to show that in the 2-aliased case, the (n‚àí1) √ó (n‚àí1)
transition matrix most consistent with the first two moments is nothing but the non-aliased transition matrix AÃÑ.
Hence, applying for example the spectral method of Kontorovich et al. (2013) yields an estimate AÃÑÀÜ, which is not
only strongly consistent, but also satisfies that as T ‚Üí ‚àû,
1
AÃÑÀÜ = AÃÑ + OP (T ‚àí 2 ).

(29)

5.3. Identifying the aliased component Œ∏n‚àí1
Assuming the HMM was detected as 2-aliasing, our next
task, step (iv), is to identify the aliased component. Recall
that if the aliased component is Œ∏n‚àí1 , then by (24)
F (c)

= KÃÑn‚àí1,c R(2) ,

‚àÄc ‚àà [n‚àí1].

We thus estimate the index i ‚àà [n‚àí1] of the aliased component by solving the following least squares problem:
2
X 
 (c)

iÃÇ = argmin
(30)
FÃÇ ‚àí KÃÑi,c RÃÇ(2)  .
i‚àà[n‚àí1]

c‚àà[n‚àí1]

F

The following result shows this method is consistent.
Lemma 7. For a minimal 2A-HMM satisfying Assumptions
(A1-A3) with aliased states n‚àí1 and n,
lim Pr( iÃÇ 6= n‚àí1) = 0.

T ‚Üí‚àû

5.4. Learning the aliased transition matrix A
Given the aliased component, we estimate the n √ó n transition matrix A using the decomposition (8). First, recall that
by (22), R(2) = Œ¥ out (Œ¥ in )T = œÉuv T . As singular vectors
are determined only up to scaling, we have that
œÉ
Œ¥ out = Œ≥u
and
Œ¥ in = v,
Œ≥
where Œ≥ ‚àà R is a yet undetermined constant. Thus, the
decomposition (8) of A takes the form:
œÉ
(31)
A = CŒ≤ AÃÑB + Œ≥CŒ≤ ucTŒ≤ + bv T B + Œ∫ bcTŒ≤ .
Œ≥
Since AÃÑ, œÉ, u and v were estimated in previous steps, we
are left to determine the scalars Œ≥, Œ≤ and Œ∫ of Eq. (7).
As for Œ∫, according to (23) we have R(3) = Œ∫R(2) . Thus,
plugging the empirical versions, Œ∫ÃÇ is estimated by
2



(32)
Œ∫ÃÇ = argmin RÃÇ(3) ‚àí rRÃÇ(2)  .
F

r‚ààR

To determine Œ≥ and Œ≤ we turn to the similarity transformation AH (œÑn‚àí1 , œÑn ), given in (9). As shown in Section
3, this transformation characterizes all transition matrices
equivalent to A. To relate AH to the form of the decomposition (31), we reparametrize œÑn‚àí1 and œÑn as follows:
Œ≥ 0 = Œ≥(œÑn‚àí1 ‚àí œÑn ),

Œ≤0 =

Œ≤ ‚àí œÑn
.
œÑn‚àí1 ‚àí œÑn

Replacing œÑn‚àí1 , œÑn with Œ≥ 0 , Œ≤ 0 we find that AH is given by
œÉ
AH = CŒ≤ 0 AÃÑB + Œ≥ 0 CŒ≤ 0 ucTŒ≤ 0 + 0 bv T B + Œ∫ bcTŒ≤ 0 . (33)
Œ≥
Note that putting Œ≥ 0 = Œ≥ and Œ≤ 0 = Œ≤ recovers the decomposition (31) for the original transition matrix A.
Now, since H is assumed identifiable, the constraint
AH (œÑn‚àí1 , œÑn ) ‚â• 0 has the unique solution (œÑn‚àí1 , œÑn ) =
(1, 0), or equivalently (Œ≥ 0 , Œ≤ 0 ) = (Œ≥, Œ≤). Thus, with exact
knowledge of the various moments, only a single pair of
values (Œ≥ 0 , Œ≤ 0 ) will yield a non-negative matrix (33). This
perfectly recovers Œ≥, Œ≤ and the original transition matrix A.
In practice we plug into (33) the empirical versions AÃÑÀÜ, Œ∫ÃÇ,
œÉÃÇ1 , uÃÇ1 and vÃÇ 1 , where uÃÇ1 , vÃÇ 1 are the left and right singular vectors of RÃÇ(2) , corresponding to the singular value œÉÃÇ1 .
As described in Appendix D.5, the values (Œ≥ÃÇ, Œ≤ÃÇ) are found
by maximizing a simple two dimensional smooth function.
The resulting estimate for the aliased transition matrix is
AÃÇ

œÉÃÇ1
= CŒ≤ÃÇ AÃÑÀÜB + Œ≥ÃÇCŒ≤ÃÇ uÃÇ1 cTŒ≤ÃÇ + bvÃÇ T1 B + Œ∫ÃÇ bcTŒ≤ÃÇ .
Œ≥ÃÇ

The following theorem proves our method is consistent.
Theorem 3. Let H be a 2A-HMM satisfying assumption
(A1-A3) with aliased states n‚àí1 and n. Then as T ‚Üí ‚àû,
AÃÇ = A + oP (1).

Learning Parametric-Output HMMs with Two Aliased States

.1

1
.5

3

0.3

.25
.1

.5

.453

T
T
T
T

2
.087

3

.7

0.2

0.1

Figure 2. The aliased HMM (left) and its corresponding nonaliased version with states 3 and 4 merged (right).

0
0

0.2

0.4

Motivated by ion channel gating (Crouzy & Sigworth,
1990; Rosales et al., 2001; Witkoskie & Cao, 2004), we
consider the following HMM H with n = 4 hidden states
(Fig.2, left). The output distributions are univariate Gaussians N (¬µi , œÉi2 ) , the matrix A and (fŒ∏i )4i=1 are given by
Ô£∂
Ô£´
fŒ∏1 = N (3, 1)
0.3 0.25 0.0 0.8
Ô£¨ 0.6 0.25 0.2 0.0 Ô£∑
fŒ∏2 = N (6, 1)
Ô£∑
A =Ô£¨
Ô£≠ 0.0 0.5 0.1 0.1 Ô£∏ ,
fŒ∏3 = N (0, 1)
fŒ∏4 = N (0, 1).
0.1 0.0 0.7 0.1
States 3 and 4 are aliased and by Procedure 1 in Appendix
C.3 this 2A-HMM is identifiable. The rank-1 matrix R(2)
has a singular value œÉ = 0.33. Fig.2 (right) shows its nonaliased version HÃÑ with states 3 and 4 merged.
To test our aliasing detection algorithm, we generated T
outputs from the original aliased HMM and from its nonaliased version HÃÑ. Fig.3 (top left) shows the empirical densities (averaged over 1000 independent runs) of the largest
singular value of RÃÇ(2) , for both H and HÃÑ. Fig.3 (top
right) shows similar results for a 2A-HMM with œÉ = 0.22.
When œÉ = 0.33, already T = 1000 outputs suffice for
essentially perfect detection of aliasing. For the smaller
œÉ = 0.22, more samples are required. Fig.3 (middle left)
shows the false alarm and misdetection probabilities vs.
sample size T of the aliasing detection test (28) with thresh1
old hT = 2T ‚àí 3 . The consistency of our method is evident.
Fig.3 (middle right) shows the probability of misidentifying the aliased component Œ∏3ÃÑ . We considered the same 2AHMM H but with different means for the Gaussian output
distribution of the aliased states, ¬µ3ÃÑ = {0, 1, 2}. As expected, when fŒ∏3ÃÑ is closer to the output distribution of the
non-aliased state fŒ∏1 (with mean ¬µ1 = 3), identifying the
aliased component is more difficult.
Finally, we considered the following methods to estimate
A: The Baum-Welch algorithm with random initial guess

0.8

detection error

= 100 - non-aliased
= 100 - 2-aliased
= 1000 - non-aliased
= 1000 - 2-aliased

0.2

0.1

0
0

1

0.2

0.4

0.6

0.8

1

œÉÃÇ1 (RÃÇ(2) )
0.8

non-aliased œÉ = 0
2-aliased œÉ = .22
2-aliased œÉ = .33

0.6

2-aliased - ¬µ3ÃÑ = 0
2-aliased - ¬µ3ÃÑ = 1
2-aliased - ¬µ3ÃÑ = 2

Pr(iÃÇ 6= 3ÃÑ)

0.6

0.4
0.2

0.4
0.2

0
0

200

400

600

sample size T

800

1000

0
0

200

700

0

10

E||AÃÇ ‚àí A||2F

The following simulation results illustrate the consistency
of our methods to detect aliasing, identify the aliased component and learn the transition matrix A. As our focus is
on the aliasing, we assume for simplicity that the output parameters Œ∏ÃÑ and the projected stationary distributions œÄÃÑ are
exactly known.

0.6

T
T
T
T

œÉÃÇ1 (RÃÇ(2) )
0.8

6. Numerical simulations

0.3

= 100 - non-aliased
= 100 - 2-aliased
= 1000 - non-aliased
= 1000 - 2-aliased

density

.2

.1

4

.6

2

density

.8

.25

running time (secs)

.6

1

‚àí1

10

BW
MoM+exact
BW+MoM+exact
BW+exact

‚àí2

10

3

10

4

10

5

10

sample size T

600
500

400

600

sample size T

800

1000

BW
MoM+exact
BW+MoM+exact
BW+exact

400
300
200
100
0

1

2

3

sample size T

4

5
5

x 10

Figure 3. Top: Empirical density of the largest singular value of
RÃÇ(2) with œÉ = 0.33 (left) and œÉ = 0.22 (right). Middle: Misdetection probability of aliasing/non-aliasing (left) and probability
of misidentifying the correct aliased component (right). Bottom:
Average error E||AÃÇ ‚àí A||2F and runtime comparison of different
algorithms vs. sample size T .

of the HMM parameters (BW); our method of moments
with exactly known Œ∏ÃÑ (MoM+Exact); BW initialized with
the output of our method (BW+MoM+Exact); and BW
with exactly known output distributions but random initial
guess of the transition matrix (BW+Exact). Fig.3 (bottom
left) shows on a logarithmic scale the mean square error
E||AÃÇ ‚àí A||2F vs. sample size T , averaged over 100 independent realizations. Fig.3 (bottom right) shows the running time as a function of T . In both figures, the number of
iterations of the BW was set to 20.
These results show that with a random initial guess of the
HMM parameters, BW requires far more than 20 iterations
to converge. Even with exact knowledge of the output distributions but a random initial guess of the matrix A, BW
still fails to converge after 20 iterations. In contrast, our
method yields a relatively accurate estimator in only a fraction of run-time. Furthermore, using this estimator as an
initial guess for BW yields even better accuracy.

Learning Parametric-Output HMMs with Two Aliased States

Acknowledgments
We thank Aryeh Kontorovich for the helpful discussions.
This research was supported by the Frankel Center for
Computer Science.

References
Achlioptas, Dimitris and McSherry, Frank. On spectral
learning of mixtures of distributions. In Learning Theory, pp. 458‚Äì469. Springer, 2005.
Allman, Elizabeth S, Matias, Catherine, and Rhodes,
John A. Identifiability of parameters in latent structure
models with many observed variables. The Annals of
Statistics, pp. 3099‚Äì3132, 2009.

Crouzy, Serge C and Sigworth, Frederick J. Yet another
approach to the dwell-time omission problem of singlechannel analysis. Biophysical journal, 58(3):731, 1990.
Dasgupta, Sanjoy. Learning mixtures of gaussians. In
Foundations of Computer Science, 1999. 40th Annual
Symposium on, pp. 634‚Äì644, 1999.
Feldman, Jon, O‚ÄôDonnell, Ryan, and Servedio, Rocco A.
Learning mixtures of product distributions over discrete
domains. SIAM Journal on Computing, 37(5):1536‚Äì
1564, 2008.
Finesso, Lorenzo. Consistent estimation of the order for
Markov and hidden Markov chains. Technical report,
DTIC Document, 1990.

Anandkumar, Animashree, Hsu, Daniel, and Kakade,
Sham M. A method of moments for mixture models and
hidden Markov models. In COLT, 2012.

Fredkin, Donald R and Rice, John A. On aggregated
markov processes. Journal of Applied Probability, pp.
208‚Äì214, 1986.

Baum, L.E., Petrie, T., Soules, G., and Weiss, N. A maximization technique occurring in the statistical analysis
of probabilistic functions of Markov chains. The Annals
of Mathematical Statistics, 41(1):pp. 164‚Äì171, 1970.

Fredkin, Donald R and Rice, John A. Maximum likelihood estimation and identification directly from singlechannel recordings. Proceedings of the Royal Society of
London. Series B: Biological Sciences, 249(1325):125‚Äì
132, 1992.

Belkin, M. and Sinha, K. Polynomial learning of distribution families. In Foundations of Computer Science
(FOCS), 2010 51st Annual IEEE Symposium on, pp.
103‚Äì112, 2010.

Hsu, Daniel, Kakade, Sham M., and Zhang, Tong. A spectral algorithm for learning hidden Markov models. In
COLT, 2009.

Blackwell, David and Koopmans, Lambert. On the identifiability problem for functions of finite Markov chains.
The Annals of Mathematical Statistics, pp. 1011‚Äì1015,
1957.

Huang, Qingqing, Ge, Rong, Kakade, Sham, and Dahleh,
Munther. Minimal realization problems for hidden
markov models. arXiv preprint arXiv:1411.3698, 2014.

Bradley, Richard C. Basic properties of strong mixing conditions. a survey and some open questions. Probab. Surveys, 2:107‚Äì144, 2005.

Ito, Hisashi, Amari, S-I, and Kobayashi, Kingo. Identifiability of hidden Markov information sources and
their minimum degrees of freedom. Information Theory,
IEEE Transactions on, 38(2):324‚Äì333, 1992.

Brafman, R. I. and Shani, G. Resolving perceptual aliasing
in the presence of noisy sensors. In NIPS, pp. 1249‚Äì
1256, 2004.

Jaeger, Herbert. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6):1371‚Äì
1398, 2000.

Brejova, B., Brown, D. G., and Vinar, T. The most probable annotation problem in HMMs and its application to
bioinformatics. Journal of Computer and System Sciences, 73(7):1060‚Äì1077, 2007.

Jefferies, M. E. and Yeap, W. Robotics and cognitive approaches to spatial mapping, volume 38. Springer, 2008.

CappeÃÅ, Olivier, Moulines, Eric, and RydeÃÅn, Tobias. Inference in hidden Markov models. Springer Series in Statistics. Springer, New York, 2005.
Chrisman, L. Reinforcement learning with perceptual
aliasing: The perceptual distinctions approach. In AAAI,
pp. 183‚Äì188. Citeseer, 1992.

Kontorovich, A. and Weiss, R.
Uniform Chernoff
and Dvoretzky-Kiefer-Wolfowitz-type inequalities for
Markov chains and related processes. Journal of Applied
Probability, 51:1‚Äì14, 2014.
Kontorovich, Aryeh, Nadler, Boaz, and Weiss, Roi. On
learning parametric-output HMMs. In Proceedings of
The 30th International Conference on Machine Learning, pp. 702‚Äì710, 2013.

Learning Parametric-Output HMMs with Two Aliased States

Kritchman, Shira and Nadler, Boaz. Non-parametric detection of the number of signals: Hypothesis testing and
random matrix theory. IEEE Transactions on Signal Processing, 57(10):3930‚Äì3941, 2009.
Leggetter, CJ and Woodland, P. C. Speaker adaptation of
continuous density HMMs using multivariate linear regression. In ICSLP, volume 94, pp. 451‚Äì454, 1994.
Leroux, Brian G. Maximum-likelihood estimation for hidden Markov models. Stochastic processes and their applications, 40(1):127‚Äì143, 1992.
McCallum, R Andrew. Instance-based utile distinctions for
reinforcement learning with hidden state. In ICML, pp.
387‚Äì395, 1995.
Newey, Whitney K. Uniform convergence in probability
and stochastic equicontinuity. Econometrica, 59:1161‚Äì
1167, 1991.
Petrie, T. Probabilistic functions of finite state Markov
chains. The Annals of Mathematical Statistics, pp. 97‚Äì
115, 1969.
Rosales, Rafael, Stark, J Alex, Fitzgerald, William J, and
Hladky, Stephen B. Bayesian restoration of ion channel records using hidden Markov models. Biophysical
journal, 80(3):1088‚Äì1103, 2001.
Shani, G., Brafman, R. I., and Shimony, S. E. Model-based
online learning of POMDPs. In ECML, pp. 353‚Äì364.
Springer, 2005.
Siddiqi, Sajid M., Boots, Byron, and Gordon, Geoffrey J.
Reduced-rank Hidden Markov Models. In AISTAT,
2010.
Stanke, M. and Waack, S. Gene prediction with a hidden
Markov model and a new intron submodel. Bioinformatics, 19(suppl 2):ii215‚Äìii225, 2003.
Stewart, G.W. and Sun, Ji-guang. Matrix Perturbation Theory. Academic Press, 1990.
Titterington, D Michael, Smith, Adrian FM, Makov, Udi E,
et al. Statistical analysis of finite mixture distributions,
volume 7. Wiley New York, 1985.
Vanluyten, Bart, Willems, Jan C, and De Moor, Bart.
Equivalence of state representations for hidden Markov
models. Systems & Control Letters, 57(5):410‚Äì419,
2008.
White, Langford B, Mahony, Robert, and Brushe, Gary D.
Lumpable hidden Markov models-model reduction and
reduced complexity filtering. Automatic Control, IEEE
Transactions on, 45(12):2297‚Äì2306, 2000.

Witkoskie, James B and Cao, Jianshu. Single molecule
kinetics. i. theoretical analysis of indicators. The Journal
of chemical physics, 121(13):6361‚Äì6372, 2004.
Zatuchna, Z. V. and Bagnall, A. Learning mazes with aliasing states: An LCS algorithm with associative perception. Adaptive Behavior, 17(1):28‚Äì57, 2009.

