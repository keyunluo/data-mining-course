Robust Estimation of Transition Matrices in High Dimensional Heavy-tailed
Vector Autoregressive Processes

Huitong Qiu
Johns Hopkins University, 615 N. Wolfe St., Baltimore, MD 21210 USA

HQIU 7@ JHU . EDU

Sheng Xu
Johns Hopkins University, 615 N. Wolfe St., Baltimore, MD 21210 USA

SHXU @ JHU . EDU

Fang Han
Johns Hopkins University, 615 N. Wolfe St., Baltimore, MD 21210 USA

FHAN @ JHU . EDU

Han Liu
Princeton University, 98 Charlton Street, Princeton, NJ 08544 USA

HANLIU @ PRINCETON . EDU

Brian Caffo
Johns Hopkins University, 615 N. Wolfe St., Baltimore, MD 21210 USA

Abstract
Gaussian vector autoregressive (VAR) processes
have been extensively studied in the literature.
However, Gaussian assumptions are stringent for
heavy-tailed time series that frequently arises in
finance and economics. In this paper, we develop a unified framework for modeling and estimating heavy-tailed VAR processes. In particular, we generalize the Gaussian VAR model by
an elliptical VAR model that naturally accommodates heavy-tailed time series. Under this model,
we develop a quantile-based robust estimator for
the transition matrix of the VAR process. We
show that the proposed estimator achieves parametric rates of convergence in high dimensions.
This is the first work in analyzing heavy-tailed
high dimensional VAR processes. As an application of the proposed framework, we investigate
Granger causality in the elliptical VAR process,
and show that the robust transition matrix estimator induces sign-consistent estimators of Granger
causality. The empirical performance of the proposed methodology is demonstrated by both synthetic and real data. We show that the proposed
estimator is robust to heavy tails, and exhibit superior performance in stock price prediction.
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

BCAFFO @ JHU . EDU

1. Introduction
Vector autoregressive models are widely used in analyzing
multivariate time series. Examples include financial time
series (Tsay, 2005), macroeconomic time series (Sims,
1980), gene expression series (Fujita et al., 2007; OpgenRhein & Strimmer, 2007), and functional magnetic resonance images (Qiu et al., 2015).
Let X1 , . . . , XT P Rd be a stationary multivariate time
series. We consider VAR models1 such that
Xt “ AXt´1 ` Et for t “ 2, . . . , T,
where A is the transition matrix, and E2 , . . . , ET are latent innovations. The transition matrix characterizes the
dependence structure of the VAR process, and plays a fundamental role in forecasting. Moreover, the sparsity pattern
of the transition matrix is often closely related to Granger
causality. In this paper, we focus on estimating the transition matrix in high dimensional VAR processes.
VAR models have been extensively studied under the Gaussian assumption. The Gaussian VAR model assumes that
the latent innovations are i.i.d. Gaussian random vectors,
and are independent from past observations (Lütkepohl,
2007). Under this model, there is vast literature on estimating the transition matrix under high dimensional settings.
These estimators can be categorized into regularized estimators and Dantzig-selector-type estimators. The former
1

For simplicity, we only consider order one VAR models in
this paper. Extensions to higher orders can be obtained using the
same technique as in Chapter 2.1 of Lütkepohl (2007).

Robust Estimation of VAR Processes

can be formulated by
b reg

A

:“ argmin lpY ´ MXq ` Pρ pMq,

(1.1)

MPRdˆd

where Y :“ pX1 , . . . , XT ´1 q P RdˆpT ´1q , X :“
pX2 , . . . , XT q P RdˆpT ´1q , lp¨q is a loss function, and
Pρ p¨q is a penalty function with penalty parameter ρ. Common choices of the loss function include least squares loss
and negative log-likelihood (Hamilton, 1994). For the
penalty function, various `1 penalties (Wang et al., 2007;
Hsu et al., 2008; Shojaie & Michailidis, 2010) and ridge
penalty (Hamilton, 1994) are widely used. Theoretical
properties of `1 penalized estimators are studied in Narki
& Rinaldo (2011), Song & Bickel (2011), and Basu &
Michailidis (2013).
In parallel to the penalized minimum loss estimators, Han
& Liu (2013) proposed a Dantzig-selector-type estimator,
which is formulated as the solution to a linear programming problem. In contrast to the `1 regularized estimators,
consistency of the Dantzig-selector-type estimator do not
rely on restricted eigenvalue conditions. These conditions
do not explicitly account for the effect of serial dependence.
Moreover, the Dantzig-selector-type estimator weakens the
sparsity assumptions required by the `1 regularized estimators.
Although extensively studied in the literature, Gaussian
VAR models are restrictive in their implications of light
tails. Heavy-tailed time series frequently arise in finance,
macroeconomics, signal detection, and statistical physics,
to name just a few (Feldman & Taqqu, 1998). For analyzing these data, more flexible models and robust estimators
are desired.
In this paper, we develop a unified framework for modeling and estimating heavy-tailed VAR processes. In particular, we propose an elliptical VAR model that allows for
heavy-tailed processes. The elliptical VAR model covers
the Gaussian VAR model as a special case. Under this
model, we show that the transition matrix is closely related
to quantile-based scatter matrices. The relation serves as
a quantile-based counterpart of the Yule-Walker equation2
(Lütkepohl, 2007). Motivated by this relation, we propose
a quantile-based robust estimator of the transition matrix.
The estimator falls into the category of Dantzig-selectortype estimators, and enjoys similar favorable properties as
the estimator in Han & Liu (2013). We investigate the
asymptotic behavior of the estimator in high dimensions,
and show that although set in a more general model, it
achieves the same rates of convergence as the Gaussianbased estimators. The effect of serial dependence is also
explicitly characterized in the rates of convergence.
2

The Yule-Walker equation connects the transition matrix with
the covariance matrix and the lag-one autocovariance matrix of
the process.

As an application of the framework developed in this paper, we investigate Granger causality estimation under the
elliptical VAR process. We show that just as in Gaussian
VAR models, Granger causality relations are also captured
by the sparsity patterns of the transition matrix. The robust
transition matrix estimator developed in this paper induces
sign-consistent estimators of these relations.

2. Background
In this section, we introduce the notation employed in this
paper, and provide a review on elliptical distributions and
robust scales. Elliptical distributions provide a basis for our
model, while robust scales motivate our methodology.
2.1. Notation
Let v “ pv1 , . . . , vd qT be a d-dimensional real vector, and
M “ rMjk s P Rd1 ˆd2 be a d1 ˆ d2 matrix with Mjk as
the pj, kq entry. Denote by vI the subvector of v whose
entries are index by a set I Ă t1, . . . , du. Similarly, denote
by MU,V the submatrix of M whose entries are indexed
by U Ă t1, . . . , d1 u and V Ă t1, . . . , d2 u. Let MU,˚ “
MU,t1,...,d2 u . For 0 ă q ă 8, we define the vector `q
řd
norm of v as }v}q :“ p j“1 |vj |q1{q , and the vector `8
norm of v as }v}8 :“ maxdj“1 |vj |. Let the matrix `max
norm of M be }M}max :“ maxjk |Mjk |, the matrix `8
řd
norm be }M}8 :“ b
maxj k“1 |Mjk |, and the Frobenius
ř
T
2
norm be }M}F :“
jk Mjk . Let X “ pX1 , . . . , Xd q
and Y “ pY1 , . . . , Yd qT be two random vectors. We write
d
X “ Y if X and Y are identically distributed. We use
0, 1, . . . to denote vectors with 0, 1, . . . at every entry.
2.2. Elliptical Distribution
Definition 2.1 (Fang et al. (1990)). A random vector
X P Rd follows an elliptical distribution with location
µ P Rd and scatter S P Rdˆd if and only if there exists
a nonnegative random variable ξ P R, a rank k matrix
R P Rdˆk with S “ RRT , a random vector U P Rk independent of ξ and uniformly distributed in the k dimensional
sphere, Sk´1 , such that
d

X “ µ ` ξRU .

(2.1)

In this case, we denote X „ ECd pµ, S, ξq. S is called the
scatter matrix, and ξ is called the generating variate.
Remark 2.2. (2.1) is often referred to as the stochastic representation of the elliptical random vector X. Of note, by
Theorem 2.3 in Fang et al. (1990) and the proof of Theorem
1 in Cambanis et al. (1981), Definition 2.1 is equivalent if
d
we replace ““” with simply ““”.
Proposition 2.3 (Theorems 2.15 and 2.16 in Fang et al.
(1990)). Suppose X „ ECd pµ, S, ξq and rankpSq “ k.

Robust Estimation of VAR Processes

Let B P Rpˆd be a matrix and ν P Rp be a vector. Denote
l “ rankpBSBT q. Then, we have
?
ν ` BX „ ECp pν ` Bµ, BSBT , ξ Bq,
where B „ Betapl{2, pk ´lq{2q follows a Beta distribution
if k ą l, and B “ 1 if k “ l.
2.3. Robust Scales

Remark 3.2. The elliptical VAR process in Definition
3.1 can be generated by an iterative algorithm following
Rémillard et al. (2012). In detail, by the property of elT
liptical distributions, the density function
of pXtT , Et`1
qT
a
T ´1
can be written by hpx, eq “ 1{ |Σ||Ψ|gpx Σ x `
eT Ψ´1 eq for some function g, and the density function of
Xt and the conditional density function of Et`1 given Xt
can be written by
1
g1 pxT Σ´1 xq
h1 pxq “ a
|Σ|
1
and h2 pe | xq “ a
g2 peT Ψ´1 eq,
|Ψ|

Let X P R be a random variable with a sequence of observations X1 , . . . , XT . Denote F as the distribution function
of X. For a constant q P r0, 1s, we define the q-quantiles
of X and tXt uTt“1 to be
QpX; qq “ QpF ; qq :“ inftx : PpX ď xq ě qu,
)
! t
T
pkq
b
ě
q
.
QptX
u
;
qq
:“
X
where
k
“
min
t
:
t t“1
T
p1q

pT q

Here X
ď ¨¨¨ ď X
are the order statistics of the
sample tXt uTt“1 . We say QpX; qq is unique if there exists a
T
b
unique x such that PpX ď xq “ q. We say QptX
t ut“1 ; qq
T
is unique if there exists a unique X P tXt ut“1 such that
X “ X pkq . Following Rousseeuw & Croux (1993), we
define the population and sample quantile-based scales as

b
σ
bQ ptXt uTt“1 q :“ Qpt|X
s ´ Xt |u1ďsătďT ; 1{4q,

(2.2)

e is an independent copy of X. σ
where X
bQ ptXt uTt“1 q
can be computed using OpT log T q time and OpT q storage
(Rousseeuw & Croux, 1993).

3. Model
In this paper, we model the time series of interest by an
elliptical VAR process.
Definition 3.1. A sequence of observations X1 , . . . , XT P
Rd is an elliptical VAR process if and only if the following
conditions are satisfied:
1. X1 , . . . , XT follow a lag-one VAR process
Xt “ AXt´1 ` Et , for t “ 2, . . . , T,

ż
g1 prq “
Rd

gp}z}22 `rqdz and g2 prq “

gpr`xT Σ´1 xq
.
g1 pxT Σ´1 xq

The elliptical VAR process X1 , . . . , XT can be generated
by the following algorithm:
1. Generate X1 from h1 pxq.
2. For t “ 2, . . . , T ,

Q

e 1{4q,
σ pXq :“ Qp|X ´ X|;

where g1 and g2 are defined by

(3.1)

where A P Rdˆd is the transition matrix, and
E2 , . . . , ET P Rd are latent innovations.
´1
T
qT uTt“1
are stationary and absolutely
2. tpXtT , Et`1
continuous elliptical random vectors:
ˆ
˙
˙ ¯
´ ˆ
Xt
Σ 0
„ EC2d 0,
,ξ ,
(3.2)
Et`1
0 Ψ
where Σ and Ψ are positive definite matrices, and ξ ą
0 with probability 1.

(a) generate Et from h2 pe | Xt´1 q;
(b) set Xt “ AXt´1 ` Et .
Remark 3.3. By definition, it follows that an elliptical
VAR process is a stationary process. A special case of the
elliptical VAR process is the Gaussian VAR process. An
elliptical VAR process is Gaussian VAR if (3.2) is replaced
by
ˆ

Xt
Et`1

˙

´
„ N2d

ˆ

Σ
0,
0

˙¯
0
.
Ψ

The elliptical VAR process generalizes the Gaussian VAR
process in two aspects. First, the elliptical model generalizes the Gaussian model by allowing heavy tails. This
makes robust methodologies necessary for estimating the
process. Secondly, the elliptical VAR model does not require that the observations are independent from future latent innovations.
Next, we show that there exists an elliptical random vector
L “ pX1T , . . . , XTT , E2T , . . . , ETT qT such that the two conditions in Definition 3.1 are satisfied. To this end, let L0 :“
pX1T , E2T , . . . , ETT qT „ ECT d p0, diagpΣ, Ψ, . . . , Ψq, ζq
and define
L “ pX1T , . . . , XTT , E2T , . . . , ETT qT :“ BL0 ,

(3.3)

Robust Estimation of VAR Processes

where
¨

I
0
0
˚ A
I
0
˚ 2
˚ A
A
I
˚
˚
¨¨¨
˚ T ´1
AT ´2 AT ´3
B :“ ˚
˚A
˚ 0
I
0
˚
˚ 0
0
I
˚
˝
¨¨¨
0
0
0

˛

¨¨¨ 0
¨¨¨ 0 ‹
‹
¨¨¨ 0 ‹
‹
‹
‹
p2T ´1qdˆT d
¨¨¨ I ‹
.
‹PR
‹
¨¨¨ 0 ‹
¨¨¨ 0 ‹
‹
‚
¨¨¨ I

By Proposition 2.3, L is an elliptical random vector. The
next Lemma gives sufficient and necessary conditions for
L to satisfy the two conditions in Definition 3.1.
Lemma 3.4. 1. L „ ECp2T ´1qd p0, Ω, ζq satisfies Condition 1. Partition the scatter Ω according to the dimensions of tXt uTt“1 and tEt uTt“2 :
ˆ
˙
ΩX ΩXE
Ω :“
.
(3.4)
ΩT
ΩE
XE

Denote Σ1 :“ Σt,t`1 . We call Σ a scatter matrix of the
elliptical VAR process, and Σ1 a lag-one scatter matrix.
For any c ą 0,
? since L „ ECd p0, Ω, ζq implies L „
ECd p0, cΩ, ζ{ cq, cΣ and cΣ1 are also scatter matrix and
lag-one scatter matrix of the elliptical VAR process.
Next, we show that the scatter matrix and lag-one scatter
matrix are closely related to the robust scales defined in
Section 2.3. In particular, we show that the robust scale σ Q
motivates an alternative definition of the scatter matrix and
lag-one scatter matrix.
Let X1 , . . . , XT be an elliptical VAR process with Xt “
pXt1 , . . . , Xtd qT . We define
Q
Q
1 1
RQ “ rRQ
jk s and R1 “ rpR1 qj k s,

(3.10)

where the entries are given by

We have
¨
Ψ
˚
ΩE “ ˝
0

Proposition 3.5. Let X1 , . . . , XT be an elliptical VAR
process with latent innovations E2 , . . . , ET . Then L “
pX1T , . . . , XTT , E2T , . . . , ETT qT is an absolutely continuous
elliptical random vector.

0
..

‹
‚ P RpT ´1qdˆpT ´1qd .

.

2. L satisfies Condition
equations hold:
¨
Σ
˚ ΣT
12
ΩX “ ˚
˝
ΣT
1T

˛
(3.5)

Ψ
2 if and only if the following
Σ12
Σ
¨¨¨
ΣT
2T

¨¨¨
¨¨¨
¨¨¨

˛
Σ1T
Σ2T ‹
‹ P RT dˆT d ,
‚
Σ
(3.6)

Σ “ AΣAT ` Ψ,

(3.7)

T u

(3.8)

Σt,t`u “ ΣpA q ,
#
0,
if j ď k;
and pΩXE qIj Ik “
Aj´k´1 Ψ, if j ą k,

(3.9)

for t “ 1, . . . , T ´ 1, u “ 1, . . . , T ´ t, j “ 1, . . . , T ,
and k “ 2, . . . , T ´ 1. Here ΩXE “ rpΩXE qIj Ik s is
a partition of ΩXE into d ˆ d matrices, where Il :“
tpl ´ 1qd ` 1, . . . , ldu for l “ 1, . . . , T .
Lemma 3.4 is a consequence of Proposition 2.3. Detailed
proof is collected in the supplementary material. Lemma
3.4 shows that there exists an elliptical random vector
L “ pX1T , . . . , XTT , E2T , . . . , ETT qT that satisfies the two
conditions in Definition 3.1. On the other hand, the algorithm in Remark 3.2 generate a unique sequence of random
vectors X1 , . . . , XT , E2 , . . . , ET . Therefore, we immediately have the following proposition.

Q
2
RQ
jj :“ σ pX1j q , for j “ 1, . . . , d,
”
ı
1 Q
2
Q
2
σ
pX
`X
q
´σ
pX
´X
q
, for j ‰ k,
RQ
:“
1j
1k
1j
1k
jk
4
”
ı
1 Q
2
Q
2
1
1
1
1
1
1
σ
pX
`
X
q
´
σ
pX
´
X
q
,
pRQ
q
:“
1j
2k
1j
2k
1 j k
4
for j 1 , k 1 “ 1, . . . , d.

The next theorem shows that RQ and RQ
1 are scatter matrix
and lag-one scatter matrix of the elliptical VAR process.
Theorem 3.6. For the elliptical VAR process in Definition
3.1, we have
Q
RQ “ mQ Σ and RQ
1 “ m Σ1 ,

(3.11)

where mQ is a constant.
The proof of Theorem 3.6 exploits the summation stability
of elliptical distributions and Proposition 2.3. Due to space
limit, the detailed proof is collected in the supplementary
material. Combining Lemma 3.4 and Theorem 3.6, we obtain the following theorem.
Theorem 3.7. For the elliptical VAR process in Definition
3.1, let RQ and RQ
1 be defined as in (3.10). Then, we have
Q T
RQ
1 “R A .

(3.12)

(3.12) serves as a quantile-based counterpart as the Yule
Walker equation VarpX1 q “ CovpX1 , X2 qAT . Theorem
3.7 motivates the robust estimator of A introduced in the
next section.

Robust Estimation of VAR Processes

4. Method
In this section, we propose a robust estimator for the transition matrix A. We first introduce robust estimators of RQ
and RQ
1 . Based on these estimators, the transition matrix
A can be estimated by solving an optimization problem.
Let X1 , . . . , XT be an elliptical VAR process. We define
b Q :“ rR
b Q s and R
b Q :“ rpR
b Q qjk s,
R
1
1
jk

The consistency of the estimator depends on the degree of
dependence over the process X1 , . . . , XT . We first introduce the φ-mixing coefficient for quantifying the degree of
dependence.
Definition 5.1. Let tXt utPZ be a stationary process. De0
fine F´8
:“ σpXt : t ď 0q and Fn8 :“ σpXt : t ě nq
to be the σ-fileds generated by tXt utď0 and tXt utěn , respectively. The φ-mixing coefficient is defined by
φpnq :“

where the entries are given by

sup
0 ,APF 8 ,PpBqą0
BPF´8
n

b Q :“ σ
R
bQ ptXtj uTt“1 q2 , for j “ 1, . . . , d,
jj
”
ı
b Q :“ 1 σ
bQ ptXtj `Xtk uTt“1 q2 ´b
σ Q ptXtj ´Xtk uTt“1 q2 ,
R
jk
4
for j ‰ k P t1, . . . , du,
”
´1 2
b Q qjk :“ 1 σ
bQ ptXtj ` Xt`1,k uTt“1
q ´
pR
1
4
ı
´1 2
σ
bQ ptXtj ´ Xt`1,k uTt“1
q , for j, k “ 1, . . . , d.

|PpA | Bq ´ PpAq|.

Let tXt utPZ be an infinite elliptical VAR process in the
sense that any contiguous subsequence of tXt utPZ is an elliptical VAR process. For brevity, we also call tXt utPZ
´
an elliptical VAR process. Let φj pnq, φ`
jk pnq, φjk pnq,
`
´
ψj 1 k1 pnq, and ψj 1 k1 pnq be the φ-mixing coefficients of
tXtj utPZ , tXtj ` Xtk utPZ , tXtj ´ Xtk utPZ , tXtj 1 `
Xt`1,k1 utPZ , and tXtj 1 ´ Xt`1,k1 utPZ , respectively. Here
j, k, j 1 , k 1 P t1, . . . , du but j ‰ k. Define

Motivated by Theorem 3.7, we proposed to estimate A by
b “ argminMPRdˆd ř |Mjk |
A
jk
b Q MT ´ R
b Q }max ď λ.
s.t. }R
1

´
`
´
Φpnq “ sup tφj pnq, φ`
jk pnq, φjk pnq, ψj 1 k1 pnq, ψj 1 k1 pnqu,
j,k,j 1 ,k1

(4.1)

The optimization problem (4.1) can be further decomposed
into d subproblems (Han & Liu, 2013). Specifically, the
b can be estimated by
j-th row of A
b j˚ “ argmin d }v}1
A
vPR
b Q v ´ pR
b Q q˚j }8 ď λ.
s.t. }R
1

(4.2)

Thus, the d rows of A can be estimated in parallel. (4.2)
is essentially a linear programming problem, and can be
solved efficiently using the simplex algorithm.
Remark 4.1. Since σ
bQ can be computed using OpT log T q
time (Rousseeuw & Croux, 1993), the computational comb Q and R
b Q are Opd2 T log T q. Since T ! d in
plexity of R
1
Q
b
b
practice, R and RQ
1 can be computed almost as efficiently
as their moment-based counterparts
T
Tÿ
´1
ÿ
T
b“ 1
b1 “ 1
S
Xt XtT and S
Xt Xt`1
,
T t“1
T ´ 1 t“1

(4.3)

řT
and ΘpT q :“ n“1 Φpnq. Φ and Θ characterize the degree
of dependence over the multivariate process tXt utPZ .
Next, we introduce an identifiability condition on the distribution function of X1 .
f1 “ pX
f2 “
e11 , . . . , X
e1d qT and X
Condition 1. Let X
T
e
e
pX21 , . . . , X2d q be independent copies of X1 and X2 .
`
´
´
Let Fj , Fjk
, Fjk
, G`
j 1 k1 , and Gj 1 k1 be the distribution
e1j |, |X1j ` X1k ´ X
e1j ´ X
e1k |,
functions of |X1j ´ X
e
e
e
e
|X1j ´ X1k ´ X1j ` X1k |, |X1j 1 ` X2k1 ´ X1j 1 ´ X2k1 |,
e1j 1 ` X
e2k1 |. We assume that there
and |X1j 1 ´ X2k1 ´ X
exist constants κ ą 0 and η ą 0 such that
d
F pyq ě η
|y´QpF ;1{4q|ďκ dy
inf

`
´
´
for any F P tFj , Fjk
, Fjk
, G`
j 1 k1 , Gj 1 k1 : j ‰ k and j, k,
1 1
j , k “ 1, . . . , d.u.

bQ
Then next lemma presents the rates of convergence for R
Q
b .
and R
1

which have Opd2 T q complexity and are used in Han & Liu
(2013).

5. Theoretical Properties
In this section, we present theoretical analysis of the proposed transition matrix estimator. Due to space limit, the
proofs of the results in this section are collected in the supplementary material.

Lemma 5.2. Let tXt utPZ be an elliptical VAR process satisfying Condition 1. Let X1 , . . . , XT be a sequence of observations from tXt utPZ . Suppose that log d{T Ñ 0 as
T Ñ 8. Then, for T large enough, with probability no
smaller than 1 ´ 8{d2 , we have
b Q ´ RQ }max ď rpT q,
}R
b Q ´ RQ }max ď r1 pT q,
}R
1
1

(5.1)
(5.2)

Robust Estimation of VAR Processes

where the rates of convergence are defined by
c
! 2 ” 8p1 ` 2ΘpT qq log d 4ΘpT q ı2
rpT q “ max 2
`
,
η
T
T
c
Q ”
8p1 ` 2ΘpT qq log d 4ΘpT q ı)
4σmax
`
, (5.3)
η
T
T
c
! 1 ” 16p1 ` 2ΘpT qq log d 8ΘpT q ı2
r1 pT q “ max 2
`
,
η
T
T
c
Q ”
16p1 ` 2ΘpT qq log d 8ΘpT q ı)
2τmax
`
. (5.4)
η
T
T

C1 , C2 ą 0, the rates of convergence in Theorem 5.3 reduces to
c
˙
ˆ
ΘpT q log d
b
}A ´ A}max “ OP MT
,
T
c
˙1´α 
„ ˆ
b ´ A}8 “ OP s MT ΘpT q log d
.
}A
T

Q
Here σmax
:“ maxtσ Q pX1j q, σ Q pX1j ` X1k q, σ Q pX1j ´
Q
X1k q : j ‰ k P t1, . . . , duu, τmax
:“ maxtσ Q pX1j `
Q
X2k q, σ pX1j ´ X2k q : j, k P t1, . . . , duu.

Φpnq ď 1{n1` for some  ą 0,

Based on Lemma 5.2, we can further deliver the rates
b under the matrix `max norm and
of convergence for A
`1 norm. We start with some additional notation. For
α P r0, 1q, s ą 0, and MT ą 0 that may scale with T ,
we define the matrix class
Mpα, s, MT q :“
d
!
)
ÿ
|Mjk |α ď s, }M}1 ď MT .
M P Rdˆd : max
1ďjďd

k“1

Mp0, s, MT q is the set of sparse matrices with at most
s non-zero entries in each row and bounded `1 norm.
Mpα, s, MT q is also investigated in Cai et al. (2011) and
Han & Liu (2013).
Theorem 5.3. Let tXt utPZ be an elliptical VAR process
satisfying Condition 1, and X1 , . . . , XT be a sequence
of observations. Suppose that log d{T Ñ 0 as T Ñ 8,
the transition matrix A P Mpα, s, MT q, and RQ is nonsingular. Define
c
! 2 ” 16p1`2ΘpT qq log d 8ΘpT q ı2
rmax pT q “ max 2
,
`
η
T
T
c
Q
Q
4 maxpσmax
, τmax
q ” 16p1 ` 2ΘpT qq log d 8ΘpT q ı)
`
.
η
T
T
If we choose the tuning parameter
λ “ p1 ` MT qrmax pT q
in (4.1), then, for T large enough, with probability no
smaller than 1 ´ 8{d2 , we have
b ´ A}max ď 2}pRQ q´1 }1 p1 ` MT qrmax pT q, (5.5)
}A
”
ı1´α
Q ´1
b
}A´A}
ď
4s
2}pR
q
}
p1`M
qr
pT
q
. (5.6)
8
1
T max
Remark 5.4. If we assume that η ě C1 and
Q
Q
σmax
, τmax
, }pRQ q´1 }1 ď C2 for some absolute constants

řT
Here ΘpT q “ n“1 Φpnq characterizes the degree of serial dependence in the process tXt utPZ . If we further assume polynomial decaying φ-mixing coefficients
(5.7)

ř8
1`
we have ΘpT q ď
ă 8 and the
n“1 1{n
b ´
rate of convergence aare further reduced to }A
b ´ A}8 “
A}max “a OP pMT log d{T q and }A
OP rspMT log d{T q1´α s, which are the parametric rates
obtained in Han & Liu (2013) and Basu & Michailidis
(2013). Condition (5.7) has been commonly assumed in
the time series literature (Pan & Yao, 2008)

6. Granger Causality
In this section, we demonstrate an application of framework developed in this paper. In particular, we discuss the
characterization and estimation of Granger causality under
the elliptical VAR model. We start with the definition of
Granger causality.
Definition 6.1 (Granger (1980)). Let tXt utPZ be a stationary process, where Xt “ pXt1 , . . . , Xtd qT . For j ‰ k P
t1, . . . , du, tXtk utPZ Granger causes tXtj utPZ if and only
if there exists a measurable set A such that
PpXt`1,j P A | tXs usďt q ‰ PpXt`1,j P A | tXs,zk usďt q,
for all t P Z, where Xs,zk is the subvector obtained by
removing Xsk from Xs .
For a Gaussian VAR process tXt utPZ , we have that
tXtk utPZ Granger causes tXtj utPZ if and only if the
pj, kq entry of the transition matrix is non-zero (Lütkepohl,
2007). In the next theorem, we show that a similar property
holds for the elliptical VAR process.
Theorem 6.2. Let tXt utPZ be an elliptical VAR process
with transition matrix A. Suppose Xt has finite second
order moment, and VarpXtk | Xs,zk usďt ‰ 0 for any k P
t1, . . . , du. Then, for j ‰ k P t1, . . . , du, we have
1. If Ajk ‰ 0, then tXtk utPZ Granger causes tXtj utPZ .
2. If we further assume that Et`1 is independent of
tXs usďt for any t P Z, we have that tXtk utPZ
Granger causes tXtj utPZ if and only if Ajk ‰ 0.

Robust Estimation of VAR Processes

The proof of Theorem 6.2 exploits the autoregressive structure of the process X1 , . . . , XT , and the properties on conditional distributions of elliptical random vectors. We refer
to the supplementary material for the detailed proof.
Remark 6.3. The assumption that VarpXtk | Xs,zk usďt ‰
0 requires that Xtk cannot be perfectly predictable from the
past or from the other observed random variables at time
t. Otherwise, we can simply remove tXtk utPZ from the
process tXt utPZ , since predicting tXtk utPZ is trivial.
Assuming that Et`1 is independent of tXs usďt for any
t P Z, the Granger causality relations among the processes
ttXjt utPZ : j “ 1, . . . , du is characterized by the non-zero
entries of A. To estimate the Granger causality relations,
e “ rA
e jk s, where
we define A
e jk :“ A
b jk Ip|A
b jk | ě γq,
A
for some threshold parameter γ. To evaluate the consise and A regarding sparsity pattern, we detency between A
fine function signpxq :“ Ipx ą 0q ´ Ipx ă 0q. For a
matrix M, define signpMq :“ rsignpMjk qs.
e recovers
The next theorem gives the rate of γ such that A
the sparsity pattern of A with high probability.
Theorem 6.4. Assume that the conditions in Theorem 5.3
holds, and A P Mp0, s, MT q. If we set
γ “ 2}pRQ q´1 }1 p1 ` MT qrmax pT q,
2

then, with probability no smaller than 1 ´ 8{d , we have
e “ signpAq, provided that
signpAq
min
tpj,kq:Ajk ą0u

|Ajk | ě 2γ.

(6.1)

Theorem 6.4 is a direct consequence of Theorem 5.3. We
refer to the supplementary material for a detailed proof.

7. Experiments
In this section, we demonstrate the empirical performance
of the proposed transition matrix estimator using both synthetic and real data. In addition to the proposed robust
Dantzig-selector-type estimator (R-Dantzig), we consider
the following two competitors for comparison:
1. Lasso: an `1 regularized estimator defined in (1.1)
2
with
ř lpY ´ MXq “ }Y ´ MX}F and Pρ pMq “
ρ jk Mjk .
2. Dantzig: the estimator proposed in Han & Liu (Han
b Q and R
bQ
& Liu, 2013), which solves (4.1) with R
1
b and S
b 1 defined in (4.3).
replaced by S
Lasso is solved using R package glmnet. Dantzig and RDantzig are solved by the simplex algorithm.

7.1. Synthetic Data
In this section, we demonstrate the effectiveness of RDantzig under synthetic data. To generate the time series,
we start with an initial observation X1 and innovations
E2 , . . . , ET . Specifically, we consider three distributions
for pX1T , E2T , . . . , ETT qT :
Setting 1: a multivariate Gaussian distribution: N p0, Φq;
Setting 2: a multivariate t distribution with degree of freedom 3, and covariance matrix Φ;
Setting 3: an elliptical distribution with log-normal generating variate, log N p0, 2q, and covariance matrix Φ.
Here the covariance matrix Φ is block diagonal: Φ “
diagpΣ, Ψ, . . . , Ψq P RT dˆT d . We set d “ 50 and
T “ 25. Using pX1T , E2T , . . . , ETT qT , we can generate
pX1T , . . . , XTT qT by
pX1T , . . . , XTT qT “ GpX1T , E2T , . . . , ETT qT ,
where G is given by
¨
I
0
0
˚ A
I
0
˚ 2
A
I
G :“ ˚
˚ A
˝
¨¨¨
AT ´1 AT ´2 AT ´3

˛
¨¨¨ 0
¨¨¨ 0 ‹
‹
T dˆT d
¨¨¨ 0 ‹
.
‹PR
‚
¨¨¨ I

By Proposition 2.3, pX1T , . . . , XTT qT follows a multivariate
Gaussian distribution in Setting 1, a multivariate t distribution in Setting 2, and an elliptical distribution in Setting
3 with the same log-normal generating variate.
We generate the parameters A and Σ following Han &
Liu (2013). Specifically, we generate the transition matrix A using the huge R package, with patterns band, cluster, hub, and random. We refer to Han & Liu (2013) for
a graphical illustration of the patterns. Then we rescale A
so that }A}2 “ 0.8. Given A, we generate Σ such that
}Σ}2 “ 2}A}2 . Using (3.7), we set Ψ “ Σ ´ AΣAT .
Table 1 presents the errors in estimating the transition matrix and their standard deviations. The tuning parameters
λ and ρ are chosen by cross validation. The results are
based on 1,000 replicated simulations. We note two observations: (i) Under the Gaussian model (Setting 1), RDantzig has comparable performance as Dantzig, and outperforms Lasso. (ii) In Settings 2-3, R-Dantzig produces significantly smaller estimation errors than Lasso
and Dantzig. Thus, we conclude that R-Dantzig is robust
to heavy tails.
Figure 1 plots the prediction errors s against sparsity s
for the three transition matrix estimators. We observe that
R-Dantzig achieves smaller prediction errors compared to
Lasso and Dantzig.

Robust Estimation of VAR Processes
Table 1. Averaged errors and standard deviations in estimating the transition matrix under the matrix Frobenius norm (`F ), `max norm,
and `8 norm. The results are based on 1,000 replications.
Lasso

Dantzig

R-Danzig

`F

`max

`8

`F

`max

`8

`F

`max

`8

band
cluster
Setting 1
hub
random

4.26(0.74)
3.04(0.65)
2.77(0.61)
2.71(0.01)

0.60(0.25)
0.52(0.19)
0.66(0.05)
0.47(0.01)

2.15(0.38)
1.82(0.35)
2.53(0.22)
1.08(0.02)

3.24(1.07)
2.25(0.39)
1.87(0.01)
2.58(0.36)

0.49(0.22)
0.41(0.11)
0.64(0.02)
0.48(0.19)

1.10(0.07)
1.00(0.58)
1.87(0.02)
1.21(0.83)

3.65(0.01)
2.47(0.01)
1.90(0.01)
2.74(0.01)

0.50(0.03)
0.44(0.01)
0.65(0.01)
0.47(0.01)

1.06(0.05)
1.13(0.04)
1.90(0.06)
1.19(0.08)

band
cluster
Setting 2
hub
random

9.53(0.58)
8.52(0.38)
8.20(0.28)
8.65(0.19)

1.11(0.22)
1.00(0.13)
0.97(0.09)
0.98(0.10)

10.45(1.29)
9.24(1.16)
8.53(1.02)
9.55(1.42)

3.72(0.19)
2.58(0.22)
3.87(0.01)
2.79(0.07)

0.52(0.10)
0.46(0.03)
0.78(0.03)
0.56(0.01)

1.18(0.66)
1.24(0.27)
3.30(0.02)
1.35(0.15)

3.62(0.01)
2.57(0.27)
1.88(0.01)
2.72(0.02)

0.47(0.02)
0.44(0.01)
0.64(0.01)
0.48(0.01)

0.84(0.08)
1.09(0.50)
1.90(0.06)
1.12(0.10)

band
cluster
Setting 3
hub
random

9.43(0.25)
8.59(0.34)
8.16(0.35)
8.81(0.43)

1.07(0.16)
0.94(0.10)
0.95(0.10)
1.04(0.12)

10.83(1.16)
9.70(0.98)
8.79(0.88)
9.31(1.25)

3.79(0.18)
2.66(0.10)
2.51(0.11)
2.71(0.13)

0.52(0.02)
0.44(0.02)
0.66(0.03)
0.47(0.01)

1.16(0.01)
1.51(0.22)
2.34(0.15)
1.28(0.16)

3.69(0.11)
2.55(0.11)
2.01(0.23)
2.55(0.10)

0.49(0.04)
0.43(0.01)
0.64(0.01)
0.46(0.01)

1.14(0.45)
1.32(0.26)
2.07(0.30)
1.04(0.29)

7.2. Real Data

T
1 ÿ
b s Xt´1 }2 .
s :“
}Xt ´ A
T ´ 1 t“2

15.2

14.8

Prediction Error

In this section, we exploit the VAR model in stock price
prediction. We collect adjusted daily closing prices3 of 435
stocks in the S&P 500 index from January 1, 2003 to December 31, 2007. This gives us T “ 1, 258 closing prices
of the 435 stocks. Let Xt be a vector of the 435 closing
prices on day t, for t “ 1, . . . , T . We model tXt uTt“1
by a VAR process, and estimate the transition matrix using
b s be an estimate of
Lasso, Dantzig, and R-Dantzig. Let A
the transition matrix with sparsity s4 . We define the predicb s to be
tion error associated with A

Dantzig
Lasso
14.4
R−Dantzig

14.0

0.25

0.50

0.75

1.00

Sparsity Level

Figure 1. Prediction errors in stock prices plotted against the sparsity of the estimated transition matrix.

8. Conclusion
In this paper, we developed a unified framework for modeling and estimating heavy-tailed VAR processes in high
dimensions. Our contributions are three-fold. (i) In model
level, we generalized the Gaussian VAR model by an elliptical VAR model to accommodate heavy-tailed time series.
The model naturally couples with quantile-based scatter
matrices and Granger causality. (ii) Methodologically, we
proposed a quantile-based estimator of the transition matrix, which induces an estimator of Granger causality. Experimental results demonstrate that the proposed estimator
3

The adjusted closing prices account for all corporate actions
such as stock splits, dividends, and rights offerings.
4
s P r0, 1s is defined to be the fraction of non-zero entries in
b s , and can be controlled by the tuning parameters λ and ρ.
A

is robust to heavy tails. (iii) Theoretically, we showed that
the proposed estimator achieves parametric rates of convergence in matrix `max norm and `8 norm. The theory
explicitly captures the effect of serial dependence, and implies sign-consistency of the induced Granger causality estimator. To our knowledge, this is the first work on modeling and estimating heavy-tailed VAR processes in high
dimensions. The methodology and theory proposed in this
paper have broad impact in analyzing non-Gaussian time
series. The techniques developed in the proofs have independent interest in understanding robust estimators under
high dimensional dependent data.

Robust Estimation of VAR Processes

References
Basu, Sumanta and Michailidis, George. Estimation in
high-dimensional vector autoregressive models. arXiv
preprint arXiv:1311.4175, 2013.
Cai, Tony, Liu, Weidong, and Luo, Xi. A constrained `1
minimization approach to sparse precision matrix estimation. Journal of the American Statistical Association,
106(494):594–607, 2011.
Cambanis, Stamatis, Huang, Steel, and Simons, Gordon.
On the theory of elliptically contoured distributions.
Journal of Multivariate Analysis, 11(3):368–385, 1981.
Fang, Kai-Tai, Kotz, Samuel, and Ng, Kai Wang. Symmetric Multivariate and Related Distributions. Chapman
and Hall, 1990.
Feldman, Raya and Taqqu, Murad. A Practical Guide to
Heavy Tails: Statistical Techniques and Applications.
Springer, 1998.
Fujita, André, Sato, João R, Garay-Malpartida, Humberto M, Yamaguchi, Rui, Miyano, Satoru, Sogayar,
Mari C, and Ferreira, Carlos E. Modeling gene expression regulatory networks with the sparse vector autoregressive model. BMC Systems Biology, 1(1):1–39, 2007.
Granger, Clive WJ. Testing for causality: a personal viewpoint. Journal of Economic Dynamics and Control, 2:
329–352, 1980.
Hamilton, James Douglas. Time series analysis, volume 2.
Princeton University Press, 1994.
Han, Fang and Liu, Han. Transition matrix estimation
in high dimensional time series. In Proceedings of the
30th International Conference on Machine Learning, pp.
172–180, 2013.
Hsu, Nan-Jung, Hung, Hung-Lin, and Chang, Ya-Mei.
Subset selection for vector autoregressive processes using lasso. Computational Statistics & Data Analysis, 52
(7):3645–3657, 2008.
Lütkepohl, Helmut. New Introduction to Multiple Time Series Analysis. Springer, 2007.
Nardi, Yuval and Rinaldo, Alessandro. Autoregressive process modeling via the lasso procedure. Journal of Multivariate Analysis, 102(3):528–549, 2011.
Opgen-Rhein, Rainer and Strimmer, Korbinian. Learning
causal networks from systems biology time course data:
an effective model selection procedure for the vector autoregressive process. BMC bioinformatics, 8(Suppl 2):
S3, 2007.

Pan, Jiazhu and Yao, Qiwei. Modeling multiple time series
via common factors. Biometrika, 95(2):365–379, 2008.
Qiu, Huitong, Han, Fang, Liu, Han, and Caffo, Brian. Joint
estimation of multiple graphical models from high dimensional time series. Journal of the Royal Statistical
Society: Series B (Statistical Methodology) (in press),
2015.
Rémillard, Bruno, Papageorgiou, Nicolas, and Soustra,
Frédéric. Copula-based semiparametric models for multivariate time series. Journal of Multivariate Analysis,
110:30–42, 2012.
Rousseeuw, Peter J and Croux, Christophe. Alternatives to
the median absolute deviation. Journal of the American
Statistical Association, 88(424):1273–1283, 1993.
Shojaie, Ali and Michailidis, George. Discovering graphical granger causality using the truncating lasso penalty.
Bioinformatics, 26(18):i517–i523, 2010.
Sims, Christopher A. Macroeconomics and reality. Econometrica, pp. 1–48, 1980.
Song, Song and Bickel, Peter J. Large vector autoregressions. arXiv preprint arXiv:1106.3915, 2011.
Tsay, Ruey S. Analysis of financial time series, volume
543. John Wiley & Sons, 2005.
Wang, Hansheng, Li, Guodong, and Tsai, Chih-Ling. Regression coefficient and autoregressive order shrinkage
and selection via the lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(1):
63–78, 2007.

