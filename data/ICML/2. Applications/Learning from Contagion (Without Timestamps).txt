Learning from Contagion (Without Timestamps)

Kareem Amin
Hoda Heidari
Michael Kearns
Computer and Information Science, University of Pennsylvania

Abstract
We introduce and study new models for learning from contagion processes in a network. A
learning algorithm is allowed to either choose
or passively observe an initial set of seed infections. This seed set then induces a final
set of infections resulting from the underlying
stochastic contagion dynamics. Our models differ from prior work in that detailed vertex-byvertex timestamps for the spread of the contagion
are not observed. The goal of learning is to infer
the unknown network structure. Our main theoretical results are efficient and provably correct
algorithms for exactly learning trees. We provide
empirical evidence that our algorithm performs
well more generally on realistic sparse graphs.

1. Introduction
We present new techniques for learning from contagion in
networks. Our motivation is settings in which we are able
to select or observe an initial “seed” set of infected vertices,
and only at some later time, observe the set of subsequent
infections resulting from some underlying stochastic contagion process on the network. Much classical work on
network diffusion processes focuses on characterizing how
contagions spread across a known network. Our aim, on
the other hand, is to develop a more comprehensive theory for inferring unknown network structure from observed
contagion behavior.
As an example, consider epidemiological studies, where an
initial set of infections are observed in a population, and
after a while, new infections develop. One might hope to
learn the underlying social interactions within the population so as to better prevent future contagion. In direct marketing, a marketer might advertise to a group of potential
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

AKAREEM @ SEAS . UPENN . EDU
HODA @ SEAS . UPENN . EDU
MKEARNS @ CIS . UPENN . EDU

customers. After some time, certain members of the population adopt a product.1 Inferring edges in the network can
help optimize future campaigns.
Only recently has attention been given to the problem of
inferring network structure from contagion/diffusion processes (see Section 1.1). These recent works have assumed
that, rather than observing only the initial and final infection sets, the learner is given temporal information describing the exact order in which infections take place on the
network. This is a reasonable assumption when considering, for example, contagion in online social networks,
where timestamped content is readily available. However,
for many other natural contagion processes, accurate transcripts of how the contagion evolved are often unavailable;
instead one only observes snapshots of infection states.
For the underlying contagion process, we examine the wellstudied independent cascade model (Kempe et al., 2003;
Goldenberg et al., 2001a;b). We consider both the case
where the learner has active seed queries, or the weaker
model of only passive seed observations. Our main results
provide efficient algorithms for learning tree-like structures
for both active and passive seed selection, along with impossibility results showing that some of our sample-size
dependencies are necessary. We then investigate the important and natural extension of these results to cyclical
graphs. We show that a natural extension of the algorithms
we propose can be utilized to learn the structure (exactly or
with low error) so long as the underlying network is sufficiently sparse. When the network is too dense, the presence of many short cycles in a network presents challenges
in our models that bear a high-level similarity to those of
problems such as inference in Bayesian networks — the
short cycles create many paths via which vertices can influence each other, conflating the sources for large-scale contagion and making learning difficult. However, we hope
that some of the analytical and algorithmic tools we develop may be useful for even richer classes of networks.
1
In this example the marketer actively selects the seed set, instead of passively observing it.

Learning from Contagion (Without Timestamps)

1.1. Related Work
Our work mainly concerns a well-studied model for contagion, the independent cascade model (Kempe et al.,
2003; Goldenberg et al., 2001a;b) Only recently has there
been work on learning network structure from contagion
(Rodriguez & lkopf, 2012; Myers & Leskovec, 2010;
Rodriguez et al., 2011; Abrahao et al., 2012; GomezRodriguez et al., 2010; Du et al., 2012; Netrapalli & Sanghavi, 2012), primarily (with slight variations) in the independent cascade model.
As previously mentioned, a key assumption in these works
is that the learner observes a sequence of cascade processes
with timestamps, or the exact order in which vertices became infected. Gathering such information about the infection times is not always possible. For example, when
monitoring the effectiveness of a viral marketing campaign,
a firm might survey a population some time into the campaign in order to assess its effectiveness, but it is unlikely
that this would reveal information on the precise order respondents adopted the product. In contrast, we assume that
we have access to data (Si , Ai ), where Si is some initial
seed set of infections, and Ai are infections eventually resulting from the contagion process when the vertices in Si
were initially infected.
This weaker assumption vastly changes the types of techniques that must be employed in order to infer network
structure. In particular, the learner can no longer directly
attribute the infection of some vertex to some previously
infected vertex by watching the contagion unfold. To the
best of our knowledge (Gripon & Rabbat, 2013) is the only
work that addresses the problem of learning the network
structure without such transcripts. There the learner is allowed access to all triples {a, b, c} where {a, b, c} are pathconnected vertices.
Furthermore, for the learning objective, unlike (Rodriguez
& lkopf, 2012; Myers & Leskovec, 2010; Rodriguez et al.,
2011) we do not seek to maximize the likelihood of the observed data, rather, similar to (Abrahao et al., 2012), our
goal is to (exactly or approximately) reconstruct the underlying network.
As mentioned in the introduction, we study two ways of
seeding, or initiating, the infection: the active model, in
which the learner can choose the seeds, and the passive
model in which the seed sets are randomly sampled from
a distribution. Both the active (see (Abrahao et al., 2012;
Myers & Leskovec, 2010)) and the passive model (see (Netrapalli & Sanghavi, 2012; Gomez-Rodriguez et al., 2010))
have been studied in prior work.
Finally, we highlight that our results are largely combinatorial in nature, whereas the previous literature uses convex programming (Myers & Leskovec, 2010; Rodriguez

et al., 2011; Du et al., 2012; Netrapalli & Sanghavi, 2012),
or submodularity (Rodriguez & lkopf, 2012; GomezRodriguez et al., 2010) to approximate an optimal network.
There are a number of other papers that are remotely related to our work: (Vert & Yamanishi, 2005) examine the
problem of network inference in the context of supervised
learning, where part of the graph is revealed to the learner
and the goal is to learn edges that connect “similar” vertices to each other. (Lippert et al., 2009) propose an unsupervised kernel-based method instead.

2. Models
The broad setting we consider is the following: There is
an unknown network structure G(V, E) in which an initial
seed set S ⊆ V becomes infected; the contagion subsequently spreads according to the rules of some underlying
(possibly stochastic) contagion model with parameters θ,
and generates the final infected set A(S) ⊆ V observed at
some later time. The network structure, contagion model,
and seed infection set define a distribution D = D(S, G, θ)
on 2V , and a draw A ∼ D specifies the subset of V that
becomes infected after the contagion has run its course.
Our results examine the stochastic independent cascade
(Kempe et al., 2003; Goldenberg et al., 2001a;b) model of
contagion.2 In the independent cascade model each edge
(u, v) has an associated infection probability p(u,v) , and the
dynamics of the contagion are as follows: when a vertex u
first becomes infected, it infects each of its neighbors, say
v ∈ N (u), with probability p(u,v) . We say the edge (u, v)
is flipped once this probability p(u,v) trial is conducted. Regardless of whether v is infected as a result, the edge (u, v)
can never be flipped again. An alternate way of viewing the
model is that we first flip each edge in G with the appropriate probability, and delete edges for which the trial fails,
resulting in a subgraph of G. Then any vertex in the same
connected component of this subgraph as some seed vertex
becomes infected.
In the active seed selection model, a learner is permitted to choose an adaptive sequence of seed sets S1 , ..., SN
each of which generates a resulting set of infections Ai =
A(Si ) ∼ D(Si , G, θ). In the passive seed selection model,
the learner is not permitted to choose the seed sets, but
rather they are chosen randomly from a distribution P.
In the passive model, we will need to refer to the distribution which is induced by first drawing S ∼ P then
A ∼ D(S, G, θ). For this we use the shorthand D(P, G, θ).
Broadly speaking, the active seed model is more appropriate for settings such as viral marketing, where the learner
2
While similar results can be obtained for the linear threshold
model of contagion, due to space considerations we only focus on
the independent cascade model.

Learning from Contagion (Without Timestamps)

can repeatedly target different populations for infections
via promotions, advertising, or give-aways, while the more
challenging passive seed model is better suited for settings
in which Nature determines the initial infections, as in the
spread of actual disease such as annual flu cycles. Note that
in either case we assume the underlying contagion model
is known, but the actual parameters of the model (e.g. the
probabilities p(u,v) ) are not.
The main problem we consider is that of exactly detecting the underlying network structure.3 Doing so efficiently
will in general require that the true or target graph belong
to some restricted class G. By “efficient” we mean the standard polynomial dependence on |V |, and on other parameters that we shall discuss at the appropriate place. Within
this framework, we consider classes of graphs that are restricted to be trees or “near” trees in the sense of having
few or limited cycles. We also demonstrate empirically
that similar algorithms provide us with low error rates when
learning sparse network structures.

3. Exactly Learning Trees with Active Seeds
Our first result gives an efficient algorithm for exactly
learning the structure of an arbitrary undirected tree in the
active seed model. The sample size required4 (and running time) will depend inversely on 1/∆, where we define
∆ = minu,v min{1 − p(u,v) , p(u,v) }. This dependence is
clearly necessary for any algorithm — if the p(u,v) can be
arbitrarily small, infections will never be observed, and exact learning of the structure is impossible.
Theorem 1. In the active seed selection model and independent cascade contagion, any tree structure is exactly
learnable from samples {(Si , Ai (Si ))}M
i=1 , where M is
1
, |V | and log( 1δ ).
polynomial in ∆
We present an algorithm that will only select seed sets of
size one (singleton seeds). The algorithm generates M =
m|V | observations by selecting each vertex u as the seed
m times (m will be determined by the analysis).5
We then define Ru (v) = {i | v ∈ Ai ∧ Si = {u}}. Thus
Ru (v) denotes the rounds in which v was infected given
that u was the seed vertex. We shall show with high probability, the containment relationships between the Ru (v)
uniquely determines the tree structure. The proof of Theorem 1 follows as a consequence.
3

Our experimental results investigate the relaxation to approximating the structure. It would also be natural to consider approximating the distribution D(Si , G, θ), for which we have results
outside the scope of this paper.
4
Note that throughout this work, we have not attempted to optimize the required sample sizes.
5
Thus the approach will also suffice in the passive setting
when P is supported on singleton sets of V .

Let N (u) denote the set of vertices adjacent to vertex u.
Lemma 1. Suppose v 6∈ N (u) and m ≥ ∆12 log(1/δ).
Then with probability at least 1 − δ there exists a v 0 where
Ru (v) ( Ru (v 0 ).
Proof. If v is not a neighbor of u, then there exists some
neighbor of u, v 0 , along the path from u to v. Given that
the underlying network is a tree, every time v is infected, v 0
must also be infected, establishing that Ru (v) ⊆ Ru (v 0 ).
For the strict inclusion, let v 00 be the immediate neighbor
of v 0 that isn’t u (but is possibly v). Given that the underlying network is a tree, every time u is selected as the
seed vertex, the probability that the contagion reaches v 0
but fails to reach v 00 is puv0 (1 − pv0 v00 ) ≥ ∆2 . Thus, with
probability at least ∆2 , v 0 is infected but not v. The probability that the infection reaches both v 0 and v for all m
rounds in which u is the seed vertex is therefore at most
(1 − ∆2 )m ≤ exp(−∆2 m) ≤ δ for m = ∆12 log(1/δ),
establishing the lemma.
Lemma 2. Suppose v, v 0 ∈ N (u) and m ≥ ∆12 log(2/δ).
Then neither Ru (v) ⊆ Ru (v 0 ) nor Ru (v 0 ) ⊆ Ru (v) with
probability at least (1 − δ).
Proof. Given that the underlying network is a tree, if u is
the seed vertex, the probability that
 the contagion reaches
v but not v 0 is p(u,v) 1 − p(u,v0 ) ≥ ∆2 . Standard arguments yield that m ≥ ∆12 log(1/δ) suffices for this to occur on at least one round, thereby ensuring that Ru (v) 6⊆
Ru (v 0 ). The symmetric argument, swapping v and v 0 , and
a union bound yields the lemma.
We say that v is maximally infected by u if there does
0
not exist a v 0 such
 that Ru (v) ⊆ Ru (v ). If m =
1
O ∆2 log(|V |/δ) , then with probability at least (1 − δ),
for all u, v pairs, v is maximally infected by u if and only
if v ∈ N (u). This suggests a clear procedure for detecting
the neighborhood of each vertex u. Repeating this for all
vertices, we can exactly learn the tree with high probability.

4. Exactly Learning Trees with Passive Seeds
While the algorithm for learning in the active model required non-trivial book keeping, the learner in that setting
is greatly aided by the fact that all infections observed in a
given round must have originated from a particular seed. In
the passive model, this is no longer the case – the affects of
seeding a particular vertex may never be seen in isolation.
Thus attributing any infection in Ai to any particular vertex
in Si is more difficult.
As for the seed distribution P, we obviously cannot hope
to exactly learn for arbitrary distributions, since this would
include pathological cases such as when Si = {V } with

Learning from Contagion (Without Timestamps)

probability one. Therefore in what follows we assume that
P belongs to the family of product distributions where each
u ∈ Si becomes initially infected independently with probability qu for unknown 0 < qu < 16 . We will once again
let ∆ be the smallest value among the pu,v , qu ,1 − pu,v and
1 − qu .

We first derive an expression for P (v ∈ A | u ∈ S):
P (v ∈ A | u ∈ S) = P (D(u, v) | u ∈ S)P (v ∈ A | u ∈ S, D(u, v))
+ P (C(u, v) | u ∈ S)P (v ∈ A | u ∈ S, C(u, v))
= P (D(u, v))P (v ∈ A | u ∈ S, D(u, v))
+ P (C(u, v))P (v ∈ A | u ∈ S, C(u, v))
= P (D(u, v))P (v ∈ A | u ∈ S, D(u, v))
+ P (C(u, v))

4.1. Characterizing Lifts

= P (D(u, v))P (v ∈ A | D(u, v)) + P (C(u, v))

Our algorithm will work by observing the relation that a
particular vertex’s infection has on likelihood of infection
at other locations in the tree. To this end, we will define the
lift u has on v as:
L(v | u) = P (v ∈ Ai | u ∈ Si ) − P (v ∈ Ai )

(1)

In other words, this lift is the increase in the probability
that v is infected from conditioning on u’s presence in the
(passive) seed set. Before describing the algorithm, we will
record some useful facts about lifts.
First notice that the outcome of the independent cascade
model is determined by the outcomes of |V |+|E| Bernoulli
random variables. |V | coins are flipped to determine which
vertices belong to the seed set Si , after which the |E| edges
of G are flipped to determine the outcome Ai of the contagion process. We say that an edge (u, v) is “active” if the
random variable corresponding to it is equal to 1, and thus
contagion is allowed to pass freely through that edge. Thus,
if (u, v) is active, either both u and v are infected, or neither
is infected. We define the active component containing u
to be the subgraph of T which contains u and any vertex v
for which all edges in the path u−v are active. Once again,
observe that all vertices in the active component containing
u are either infected or uninfected.
Our first lemma gives a decomposition for L(v | u) =
φ(u, v)ψ(u, v) in terms of twoQquantities φ, ψ. The first of
these quantities is φ(u, v) = (w,w0 )∈u−v p(w,w0 ) , where
the product is over the edge set of the path u − v. In other
words, φ(u, v) is simply the probability that the u − v path
is active. We say that s − w is an infecting tributary for
u − v if s ∈ Si , w ∈ u − v and the path s − w is active
(we consider the trivial path s − s when both s ∈ u − v and
s ∈ Si to be an infecting tributary for u − v as well). Let
E(u, v) be the event that there is no infecting tributary for
u − v and ψ(u, v) = P (E(u, v)).
Lemma 3. For any u, v, L(v | u) = φ(u, v)ψ(u, v).
Proof. Fix u and v. Let C(u, v) (for “Connected”) be the
event that the u − v path is active. Let D(u, v) (for “Disjoint”) be the event the u−v path is not active (equivalently,
u and v belong to disjoint active components).
6

We leave as an open question whether we can learn under
more general assumptions on the family of seed set distributions.

This first two equalities above follow from the law of total
probability, and the fact that the event u ∈ S is independent
of C(u, v). The next equality follows by observing that v ∈
A with certainty if u ∈ S and the u − v path is active. The
final line follows from the conditional independence of the
events u ∈ S and v ∈ A given D(u, v). Writing
P (v ∈ A) = P (D(u, v))P (v ∈ A | D(u, v))
+ P (C(u, v))P (v ∈ A | C(u, v))

and applying the above, we conclude that:
L(v | u) = P (v ∈ A | u ∈ S) − P (v ∈ A)
= P (C(u, v)) − P (C(u, v))P (v ∈ A | C(u, v))
= P (C(u, v))(1 − P (v ∈ A | C(u, v)))
= φ(u, v)P (v 6∈ A | C(u, v))

Conditioned on u − v being active, the probability that
v 6∈ A is precisely the probability that there are no infecting
tributaries for u − v, completing the proof.
The previous lemma gives several insights into L(· | ·).
For example, we see that the function is symmetric in its
arguments. Also note that lifts are monotonic along paths:
for any u−v path and w along that path, u provides a better
lift to w than to v. A useful special case is recorded in the
next lemma.
Lemma 4. Let u − v be a path of length greater than 1,
and let (w, w0 ) be any edge along that path. L(v | u) ≤
(1 − ∆)L(w | w0 ).
Proof. Appealing to Lemma 3, we see that L(v | u) =
φ(u, v)ψ(u, v). Recall that for every edge e, pe ≤ 1 − ∆.
Since u − v contains at least one edge not equal to (w, w0 ),
by definition of φ, L(v | u) ≤ (1 − ∆)φ(w, w0 )ψ(u, v).
Recalling the definition of ψ, we see that E(u, v) implies E(w, w0 ). If there are no infecting tributaries for
u − v there cannot be any infecting tributaries for w −
w0 . Thus ψ(u, v) ≤ ψ(w, w0 ), and L(v | u) ≤ (1 −
∆)φ(w, w0 )ψ(w, w0 ). Applying Lemma 3 once more completes the proof.
4.2. Algorithm
Armed with our understanding of lifts, we can now give an
algorithm for learning an unknown tree T under passive infections. The central observation of the previous section is

Learning from Contagion (Without Timestamps)

that, when w is a vertex along a u − v path, then u must
provide a better lift to w than to v. We might have hoped
that a vertex u gives all its highest lifts to its neighbors.
Unfortunately, this is not true. u could, for example, be at
the head of a path u − v for which every edge e ∈ u − v
has pe very close to 1. Thus the presence of an infection at
u has a large effect on the infection rate of all the vertices
along u − v. In contrast, some of u’s immediate neighbors
w might already be prone to infection, or may have small
p(u,v) , and therefore conditioning on u being seeded does
not change the observed infection-rate of w by much. Ultimately, we will have to exploit the monotonicity of L(· | ·)
along paths, established in the previous section.
In what follows, we assume that the algorithm has observed
M iid samples (Si , Ai ), from which it has derived estimates L̂(v | u) for each pair of vertices u, v. The exact
nature of this sampling will be covered subsequently. For
the sake of intuition, suppose that these estimates are perfect. The algorithm works by growing a forest of connected
components of T . Given such a component C, a u ∈ C
and a v 6∈ C, if u and v are not neighbors then there is
an edge (x, y) 6= (u, v) where the u − v path crosses out
of the component C (i.e. x ∈ C, but y 6∈ C). Lemma
4 tells us that L(v | u) ≤ L(y | x). Thus, by taking
(u∗ , v ∗ ) = arg maxu∈C,v6∈C L(v | u), the algorithm is
guaranteed to find an edge of the network. We present the
pseudocode for the algorithm below.
Algorithm 1 Algorithm for exactly learning trees under
passive seeds and independent cascade.
1: Input: Estimates L̂(· | ·), vertex set V
2: % Begin with singleton components
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

Ci = {u} for each u ∈ V
% and the empty edge-set.
Set Components = {{v} | v ∈ V }.
Set Ê = ∅
% Iterate until we have a single
component.
while |C| 6= 1 do
for Ci ∈ C do
% Discover an edge.
(u∗ , v ∗ ) = arg maxu∈Ci ,v6∈Ci L̂(v | u)
Add (u∗ , v ∗ ) to E.
end for
Set Components equal to connected components
of (V, Ê)
end while
Return (V, Ê)

rem establishes the correctness of the algorithm given sufficiently good estimates. First, let us define notation for
the smallest lift between two neighboring vertices: Lmin =
min(u,v)∈E L(v | u).
Theorem 2. Suppose for every u, v, |L̂(v | u) − L(v |
u)| < ∆L2min . Then Algorithm 1 returns Ê = E in time
O |V |2 log |V | .
Proof. Fix an arbitrary subgraph Ci of the true network
(V, E). To prove correctness, it suffices to show that for
this choice of Ci line 10 returns (u∗ , v ∗ ) only if (u∗ , v ∗ ) ∈
E. Suppose for the sake of contradiction that this is not
the case. Then the true u∗ − v ∗ path contains an edge
(w, w0 ) where w ∈ Ci and w0 6∈ Ci . Lemma 4 tells us
that L(w | w0 ) − L(v | u) ≥ ∆L(w | w0 ) ≥ ∆Lmin . By
assumption, |L̂(v | u) − L(v | u)| < ∆L2min , and therefore
L̂(w | w0 )−L̂(v | u) > 0. Since w ∈ Ci , w0 6∈ Ci , (u∗ , v ∗ )
could not have been the argmax of line 10, establishing the
contradiction.
Finally, we provide an upper bound on the number of samples M needed in order to satisfy the hypothesis
PM of Theorem 2 with high probability. Let n̂A (v) = i=1 1(v ∈
PM
Ai ), n̂S,A (u, v) = i=1 1(u ∈ Si , v ∈ Ai ), and n̂S (u) =
PM
i=1 1(u ∈ Si ). Taking L̂(v | u) = n̂S,A (u, v)/n̂S (u) −
n̂A (v)/M lets us state the following theorem.


Theorem 3. M = O L2 1 ∆4 log(|V |/δ) suffices so that
min

for any u, v, |L(v | u) − L̂(v | u)| < ∆Lmin /2 with probability at least 1 − δ. Furthermore, as a consequence of this
fact and Theorem
 2, Algorithm 1 returns Ê = E in time
O |V |2 log |V | with probability at least 1 − δ.
Proof. Fix a u, v and  > 0. There exists a C > 0 such that
taking M = C2 log(16|V |2 /δ), guarantees with probability
1
at least 1 − 2|Vδ |2 : (1) (1 − )P (u ∈ S) < M
n̂S (u) < (1 +
1
)P (u ∈ S), (2) | M n̂S,A (u, v) − P (u ∈ A, v ∈ S)| ≤ ,
1
and (3) | M
n̂A (v) − P (v ∈ A)| ≤ . This follows by applying a multiplicative Chernoff bound for (1), Hoeffding’s
inequality for (2,3) and taking a union bound.
Assume that  < 1/2. With probability at least 1 −

δ
2|V |2 :

L̂(v | u) ≤ (P (v ∈ A, u ∈ S) + )/((1 − )P (u ∈ S)) − P (v ∈ A) + 
= (1 − )

−1

[P (v ∈ A, u ∈ S) + ]/P (u ∈ S) − P (v ∈ A) + 

)[P (v ∈ A, u ∈ S) + ]/P (u ∈ S) − P (v ∈ A) + 
1−
≤ (1 + 2)[P (v ∈ A, u ∈ S)/P (u ∈ S) + /∆] − P (v ∈ A) + 
= (1 +

≤ P (v ∈ A, u ∈ S)/P (u ∈ S) − P (v ∈ A) + 3 + (1 + 2)/∆
≤ L(v | u) + 6/∆

4.3. Analysis
It is not difficult
 to show that this algorithm requires
O |V |2 log |V | computation time. The following theo-

The first inequality is a consequence of the concentration
inequalities previously mentioned. The second inequality
holds from assuming  < 1/2 and the fact that P (u ∈

Learning from Contagion (Without Timestamps)

S) = qu > ∆. The third follows because P (v ∈ A, u ∈
S)/P (u ∈ S) = P (v ∈ A | u ∈ A) < 1. Taking  =
1/12∆2 Lmin gives L̂(v | u) ≤ L(v | u) + ∆Lmin /2 with
probability at least 1 − 2|Vδ |2 . The symmetric argument lets
us conclude the same for L̂(v | u) ≥ L(v | u) − ∆Lmin /2.
A union bound concludes the proof.
4.4. Necessary Dependence on Minimum Lift
The only dependence of our algorithm whose necessity is
1
in
not self-evident is that of the inverse minimum lift Lmin
the sampling complexity bound of Theorem 3. We now
show that this dependence is in fact unavoidable in the
worst case: there are trees in which Lmin can be arbitrarily
small, and any algorithm for exact learning must sample
1
observations.
approximately Lmin
The lower bound relies on the following construction. Let
V consist of 4 “special” vertices {a, b, z0 , z1 } and 2n − 1
(0)
(0)
(1)
(1)
additional vertices V = {z, x1 , ..., xn−1 , x1 , ..., xn−1 }.
We describe two trees. The tree Tan is given by first connecting z0 to a and z1 to b. Tbn , on the other hand, has
z0 connected to b and z1 connected to a. Everything else
about both Tan and Tbn will be the same. In particular, z0
(0)
(1)
will connect to each of xi , z1 to each of xi and both
z0 and z1 will be connected to z. Finally, let the infection
probability be p for any edge, and seed probability be q for
any vertex be ∆.
Now suppose that T n is drawn from {Tan , Tbn } with equal
probability. In order to exactly learn E, the learner must
determine whether the edges (a, z0 ), (b, z1 ) or the edges
(b, z0 ), (a, z1 ) exist.
The basic idea behind this construction is that the lift that
either z0 or z1 provide to either a or b can be made arbitrarily small by making n large, since then z0 and z1 are
infected with near certainty under a product distribution
on seed sets. However, detecting whether we are in Tna or
Tnb requires distinguishing lifts between these four vertices.
This leads to the following result, whose proof is technical
and deferred to the Appendix.
Theorem 4. For any , there exists an n such that
Lmin (T n ) < , and any algorithm must sample at least
(32∆Lmin )−1 observations in order to reconstruct the
edge set of T n with probability at least 7/8.

5. A Generalized Lift-Based Algorithm
The algorithm described in Section 4 specifically leverages
the fact that the underlying network is a tree. It computes
all pairwise lifts in the network, then greedily selects the
top |V | − 1 edges, as long as those edges do not create
a cycle. Therefore, this algorithm will certainly fail to

learn a network that is not a tree. However, even for nontrees, observe that the lifts L(v | u) themselves remain
well-defined. Moreover, we should continue to expect that
lifts reveal relevant information about the structure of a network. A large lift L(v | u) implies that that v is far more
likely to be infected when u is seeded, relative to its background probability of being infected.
In this section we will consider a modified version of the
lift algorithm more suitable to general networks. This algorithm (which we call K-lifts), expects as input estimates
of lifts L̂(· | ·), as before. It then simply greedily selects
the K largest lifts, and constructs the edges between the
vertices responsible for those lifts (see Algorithm 2).
Algorithm 2 K-lifts algorithm.
1:
2:
3:
4:
5:
6:

Input: Estimates L̂(· | ·), vertex set V , K > 0.
Set Ê = ∅
while |Ê| =
6 K do
Add arg max{u,v}6∈Ê L(v | u) to Ê.
end while
Return (V, Ê)

We will demonstrate that even when the network we are trying to learn is no longer a tree, this “lift-based” approach
may still be effective. In other words, there is robustness
to this approach even when the assumptions of Section 4
are violated. We first study the performance of the algorithm when learning Erdős-Reńyi networks. We discover
that lifts continue to predict network structure under certain conditions. We then provide evidence that the algorithm can also perform well on more realistic networks.
We conclude by proving some simple facts about the Klifts algorithm. On the one hand, we show that the algorithm learns simple cycles. However, we also find a network which is sparse and nearly a tree on which the algorithm will make many mistakes. Thus, there are indeed
networks which will provably foil the lift algorithm.
5.1. Experiments
Let G(n, ϕ) be an Erdős-Rényi random network7 of size
n, where each edge is present in the network independently with probability ϕ (which implies the edge density
of the network is close to ϕ with high probability). For the
underlying network in our experiments, we will consider
G(100, ϕ) for ϕ = 0.01, ..., 0.1. In all the experiments,
each vertex becomes initially infected independently and
with probability 0.05 (so roughly 5 vertices are seeded.)
Denote the number of edges in a network G by m(G).
For simplicity, we will set the algorithm’s parameter K =
7
Note that the traditional notation for this is G(n, p), but we
use p to indicate the transmission probability of an edge.

Learning from Contagion (Without Timestamps)

(a)

(b)

(c)

(d)

Figure 1. The edge density of the underlying network vs. the error rate of the K-lifts algorithm for different values of p. (The error
bars around the points represent the 95% confidence interval; for each edge density ϕ, two random networks with edge density ϕ were
generated, and the K-lift algorithm was run twice on each network.)

m(G), the correct number of edges, in each experiment.
In other words, the algorithm is told the correct number
of edges. Since we are setting K = m(G), it is meanÊ∆E|
ingful to consider err(E, Ê) = |m(G)
, where E is the
edge set of G and Ê is the edge set returned by the algorithm. In other words, err(E, Ê) is the fraction of those
edges which the algorithm either fails to find or add mistakenly. For estimating the lifts, we need sufficient number
of samples. Let M denote the number of samples available to the learning algorithm. We run the experiments
for M ∈ {1000, 10000, 100000, 1000000} and observe the
relation between the performance of the K-lifts algorithm
and the number of samples.
The results of the experiments are summarized in Figure
1. Each of these plots depicts the error of the K-lifts
algorithm err(E, Ê) against the density of the network
D(G) = 2m(G)/(n(n − 1)), and each plot corresponds to
a particular value of p. The experiments demonstrate that
indeed the K-lifts algorithm enjoys some robustness even
when the tree assumption is violated. This is most obvious
in Figure 1 (a). However, this is not unconditional. There
are two forces that make learning more difficult: increasing
the edge density, or increasing the transmission probability

p. Increasing either will cause the sample size required in
order to attain a low error rate to increase in turn.
Why might this be the case? We hypothesize that the lift algorithm’s performance begins to degrade when, with high
probability, large portions of the network are infected after
each contagion process. Alternatively, this is the point at
which seeds no longer cause infections local to the seed’s
neighborhood. The lift algorithm fundamentally measures
the difference in infection rate of a vertex u under two different circumstances. When any u tends to frequently be
infected, more and more samples are required to detect
this difference. As the network density ϕ and transmission probability p increase, at some point the probability of
large-scale non-local infections becomes certain.
It is interesting to note that small p can hinder learning as
well. After all, if p = 0, seed infections never spread, and
there is nothing to learn. We see this phenomenon in Figure
1 (a). When M = 1000 and p = 0.1 we simply do not see
enough infections to be able to accurately estimate lifts.
Notice also that since the algorithm is greedy, in the cases
where it exactly learns the structure, the assumption K =
m(G) is mild. If a good estimate of m(G) is provided instead, the algorithm will only make an additional number

Learning from Contagion (Without Timestamps)

of mistakes equal to missing/extra edges in the estimate.

L(v | u) − L(w | u) ≥ ∆ where v is immediately clockwise to u and w is of distance d ≥ 2 clockwise to u.

Other networks. We also ran the generalized lift algorithm on more realistic models and data sets, including a
real collaboration network, NetScience (Newman, 2003;
Boccaletti et al., 2006; Newman, 2006), and networks
generated by the Small Worlds model (Watts & Strogatz,
1998).

Let E + be the edges between v and w moving clockwise,
and ϕ be the probability that one of these edges is inactive.
Let E − be the edges between u and w moving counterclockwise, and V − be the edges between u and w moving
counterclockwise, inclusive. Define A to be the event that
v ∈ X(u), w 6∈ X(u) and X(u) is not infected. Define
B be the event that w ∈ X(u), v 6∈ X(u) and X(u) is
not infected. And Define F the event that w, v ∈ X(u)
and X(u) is not infected. Lemma 5 and the fact that these
events are disjoint tells us that L(v | u) − L(w | u) =
P (A) + P (F ) − P (B) − P (F ) = P (A) − P (B). However B can only occur if (u, v) is inactive, every edge
in E − is active, and some edge in E + is inactive. Furthermore B requires that all of V − be unseeded. Thus,
P (B) ≤ (1 − p)pn−d ϕ(1 − q)n−d+1 . Since d ≤ n2
and ϕ ≤ 1, P (B) ≤ (1 − p)pn/2 ϕ(1 − q)n/2+1 ≤
(1 − p)pn/2 (1 − q)n/2+1 . Also, A occurs if (u, v) is active,
the edge immediately clockwise to v and the edge immediately counterclockwise of u are inactive, and u and v are
not seeded. Thus, P (A) ≥ p(1 − p)2 (1 − q)2 . The fact that
n > 5 and p ≤ 12 , proves the theorem.

On the real collaboration network, NetScience, which consists of 1589 vertices and 2742 edges, we continue to observe low reconstruction error. We obtain an error rate near
0 when edges have an infection transmission probability of
10%, an error rate of 0.06 when the transmission probability is 20%, and an error rate of 0.16 when the transmission
probability is 30%.
On networks generated from the Small Worlds model, reconstruction error ranges from 0 to a bit above 0.1 at
transmission probability 20% or smaller. As perhaps expected, performance improves as we increase the amount
of rewiring in the Small Worlds model, moving from networks with very high clustering and many short cycles towards random connectivity. At transmission probability
30% the degradation of the algorithm is more rapid. Results are similar but slightly better for networks generated
from a less symmetric model that also balances high clustering with low diameter.
5.2. Theoretical Robustness and Limitations
We conclude with theoretical results that partially characterize the behavior of the K-lift algorithm on cyclical networks. To prove these results we will need a utility lemma
that helps us characterize the lift between two vertices. Let
X(u) be the active component of u (see Section 4.1).
Lemma 5. On an arbitrary network G, L(v | u) =
P (v ∈ X(u), X(u) is not infected) = P (u ∈
X(v), X(v) is not infected).
Proof. The proof follows along identically to the proof of
Lemma 3 by replacing the event C(u, v) with {v ∈ X(u)},
and omitting the final substitutions for φ, ψ.
We can now show that the K-lift algorithm exactly learns
simple cycles.
Theorem 5. Let C = (V, E) be a cycle on n > 5 vertices
and p(u,v) = p ≤ 21 , qu = q for all u, v. There exists a
∆ > 0 such that any (u, v) ∈ E and (w, z) 6∈ E, satisfy
L(v | u) − L(w | z) ≥ ∆.
Proof. By symmetry, L(v | u) is the same for any edge
(u, v) in E and L(w | z) is the same for any w and z distance d apart. So without loss of generality, we show that

While this may seem promising, there exist networks that
are almost trees – trees with a single edge added – on which
the K-lift algorithm would make many mistakes. Consider
the following network on 2n − 1 vertices: Let S be a star
consisting of n − 1 vertices centered at v0 . Let C be a cycle
consisting of the remaining n vertices as well as v0 . Define
G = S ∪C. We use the notation V (C) to refer to the vertex
set of C, with E(C), V (S), E(S) defined analogously.
Theorem 6. Suppose p(u,v) = qu = 1/2 for all u, v. When
run on G with K = m(G) = 2n − 1, the K-lifts algorithm
returns Ê satisfying err(E, Ê) > 1/2 − o(1).
Proof. Name the vertices in C, v0 , v1 , ..., vn according
to the order in which they appear in the cycle. Let
U consist of all pairs of vertices on C which are distance 2 apart without crossing v0 . More explicitly U =
(v1 , v3 ), (v2 , v4 ), ..., (vn−2 , vn ). It suffices to show that for
any (v, v0 ) ∈ E(S), and any (x, y) ∈ U , L(x | y) >
L(v | v0 ), which means, the K-lift algorithm will build
the chords in U before it considers the edges in the star S,
making n − 2 mistakes. By Lemma 5, L(v | v0 ) < P (v0 6∈
A) < (1 − qp)n−1 . On the other hand, for any (x, y) ∈ U
the probability that x and y belong to the same uninfected
component is at least p2 (1−p)2 (1−q)4 . Therefore Lemma
5 also implies that L(x | y) ≥ p2 (1 − p)2 (1 − q)4 . A sufficiently large n proves the theorem.
Acknowledgement. We give warm thanks to Moez Draief
for discussions on the results presented here.

Learning from Contagion (Without Timestamps)

References
Abrahao, Bruno, Chierichetti, Flavio, Kleinberg, Robert,
and Panconesi, Alessandro. Trace complexity of network inference. In KDD, 2012.
Boccaletti, S., Latora, V., Moreno, Y., Chavez, M., and
Hwang, D.-U. Complex networks: Structure and dynamics. Physics Reports, (424), 2006.
Du, Nan, Song, Le, Smola, Alexander J, and Yuan, Ming.
Learning networks of heterogeneous influence. In NIPS,
2012.
Goldenberg, Jacob, Libai, Barak, and Muller, Eitan. Talk of
the network: A complex systems look at the underlying
process of word-of-mouth. In Marketing Letters, 123:
211-223, 2001a.
Goldenberg, Jacob, Libai, Barak, and Muller, Eitan. Using
complex systems analysis to advance marketing theory
development. In Academy of Marketing Science Review,
2001b.
Gomez-Rodriguez, Manuel, Leskovec, Jure, and Krause,
Andreas. Inferring networks of diffusion and influence.
In KDD, 2010.
Gripon, Vincent and Rabbat, Michael. Reconstructing a
graph from path traces. In CoRR abs/13016916, 2013.
Kempe, David, Kleinberg, Jon, and Tardos, Eva. Maximizing the spread of influence through a social network. In
KDD, 2003.
Lippert, Christoph, Stegle, Oliver, Ghahramani, Zhoubin,
and Borgwardt, Karsten M. A kernel method for unsupervised structured network inference. In In Proceedings
of the International Conference on Artificial Intelligence
and Statistics AISTATS, 2009.
Myers, Seth A and Leskovec, Jure. On the convexity of
latent social network inference. In NIPS, 2010.
Netrapalli, Praneeth and Sanghavi, Sujay. Learning the
graph of epidemic cascades. In SIGMETRICS, 2012.
Newman, Mark. The structure and function of complex
networks. SIAM Review, (45), 2003.
Newman, Mark. Finding community structure in networks
using the eigenvectors of matrices. Preprint, 2006.
Rodriguez, Manuel and lkopf, Bernhard. Submodular inference of diffusion networks from multiple trees. In
ICML, 2012.
Rodriguez, Manuel Gomez, Balduzzi, David, and
Schlkopf, Bernhard. Uncovering the temporal dynamics of diffusion networks. In ICML, 2011.

Vert, Jean-Philippe and Yamanishi, Yoshihiro. Supervised
graph inference. In NIPS, 2005.
Watts, Duncan and Strogatz, Steven. Collective dynamics
of small-world networks. Nature, (393), June 1998.

