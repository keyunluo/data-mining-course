Stochastic Variational Inference for Bayesian Time Series Models

Matthew James Johnson
Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA USA

MATTJJ @ CSAIL . MIT. EDU

Alan S. Willsky
Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA USA

WILLSKY @ MIT. EDU

Abstract
Bayesian models provide powerful tools for analyzing complex time series data, but performing inference with large datasets is a challenge.
Stochastic variational inference (SVI) provides a
new framework for approximating model posteriors with only a small number of passes through
the data, enabling such models to be fit at scale.
However, its application to time series models
has not been studied.
In this paper we develop SVI algorithms for
several common Bayesian time series models,
namely the hidden Markov model (HMM), hidden semi-Markov model (HSMM), and the nonparametric HDP-HMM and HDP-HSMM. In addition, because HSMM inference can be expensive even in the minibatch setting of SVI, we develop fast approximate updates for HSMMs with
durations distributions that are negative binomials or mixtures of negative binomials.

1. Introduction
Bayesian time series models can be applied to complex
data in many domains, including data arising from behavior and motion (Fox et al., 2010; 2011), home energy consumption (Johnson & Willsky, 2013), physiological signals
(Lehman et al., 2012), single-molecule biophysics (LindeÃÅn
et al., 2013), brain-machine interfaces (Hudson, 2008), and
natural language and text (Griffiths et al., 2004; Liang et al.,
2007). However, scaling inference in these models to large
datasets is a challenge.
Many Bayesian inference algorithms require a complete
pass over the data in each iteration and thus do not scale
well. In contrast, some recent Bayesian inference methods
st

Proceedings of the 31 International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

require only a small number of passes and can even operate
in the single-pass or streaming settings (Broderick et al.,
2013). In particular, stochastic variational inference (SVI)
(Hoffman et al., 2013) provides a general framework for
scalable inference based on mean field and stochastic gradient optimization. However, while SVI has been studied
extensively for topic models (Hoffman et al., 2010; Wang
et al., 2011; Bryant & Sudderth, 2012; Wang & Blei, 2012;
Ranganath et al., 2013; Hoffman et al., 2013), it has not
been applied to time series.
In this paper, we develop SVI algorithms for the core
Bayesian time series models based on the hidden Markov
model (HMM), namely the Bayesian HMM and hidden
semi-Markov model (HSMM), as well as their nonparametric extensions based on the hierarchical Dirichlet process (HDP), the HDP-HMM and HDP-HSMM. Both the
HMM and HDP-HMM are ubiquitous in time series modeling, and so the SVI algorithms developed in Sections 3
and 4 are widely applicable.
The HSMM and HDP-HSMM extend their HMM counterparts by allowing explicit modeling of state durations
with arbitrary distributions. However, HSMM inference
subroutines have time complexity that scales quadratically
with the observation sequence length, which can be expensive even in the minibatch setting of SVI. To address this
shortcoming, in Section 5 we develop a new method for
Bayesian inference in (HDP-)HSMMs with negative binomial durations that allows approximate SVI updates with
time complexity that scales only linearly with sequence
length. The methods in this paper also provide the first
batch mean field algorithm for HDP-HSMMs.
Our code is available at github.com/mattjj/pyhsmm.

2. Background
Here we review the key ingredients of SVI, namely
stochastic gradient algorithms, the mean field variational
inference problem, and natural gradients of the mean field
objective for models with complete-data conjugacy.

SVI for Time Series Models

complete-data likelihood p(z (k) , y (k) |œÜ) are a conjugate
pair of exponential family distributions. That is, if

2.1. Stochastic gradient ascent
Consider the optimization problem
max f (œÜ, yÃÑ)
œÜ

where

f (œÜ, yÃÑ) =

K
X

ln p(œÜ)=hŒ∑œÜ , tœÜ (œÜ)i ‚àí AœÜ (Œ∑œÜ )
g(œÜ, yÃÑ

(k)

)

ln p(z

(k)

, y (k) |œÜ)=hŒ∑zy (œÜ), tzy (z (k) , y (k) )i‚àíAzy (Œ∑zy (œÜ))

k=1

and where yÃÑ = {yÃÑ (k) }K
k=1 is fixed. Then if kÃÇ is sampled
uniformly over {1, 2, . . . , K}, we have
h
i
‚àáœÜ f (œÜ) = K ¬∑ EkÃÇ ‚àáœÜ g(œÜ, yÃÑ (kÃÇ) ) .
Thus we can generate approximate gradients of the objective using only one yÃÑ (k) at a time. A stochastic gradient
algorithm for a sequence of stepsizes œÅ(t) and positive definite matrices G(t) is given in Algorithm 1. From standard
(Robbins &
1951; Bottou, 1998), if
P‚àû results
PMonro,
‚àû
(t)
= ‚àû and t=1 (œÅ(t) )2 < ‚àû and G(t) has
t=1 œÅ
uniformly bounded eigenvalues, then the algorithm converges to a stationary point, i.e. œÜ‚àó , lim œÜ(t) satisfies
t‚Üí‚àû

‚àáœÜ f (œÜ‚àó , yÃÑ) = 0.

Since each update in the stochastic gradient ascent algorithm only operates on one yÃÑ (k) , or minibatch, at a time, it
can scale to the case where yÃÑ is large.
Algorithm 1 Stochastic gradient ascent
Initialize œÜ(0)
for t = 1, 2, . . . do
kÃÇ (t) ‚Üê Uniform({1, 2, . . . , K})
(t)
œÜ(t) ‚Üê œÅ(t) KG(t) ‚àáœÜ g(œÜ(t‚àí1) , yÃÑ (kÃÇ ) )

then conjugacy (Bernardo & Smith, 2009, Proposition 5.4)
implies that tœÜ (œÜ) = (Œ∑zy (œÜ), ‚àíAzy (Œ∑zy (œÜ)), so that
p(œÜ|z (k) , yÃÑ (k) ) ‚àù exp{hŒ∑œÜ + (tzy (z (k) , yÃÑ (k) ), 1), tœÜ (œÜ)i}.
Conjugacy also implies the optimal q(œÜ) is in the same
family, i.e. q(œÜ) = exp {he
Œ∑œÜ , tœÜ (œÜ)i ‚àí AœÜ (e
Œ∑œÜ )} for some
parameter Œ∑eœÜ (Bishop & Nasrabadi, 2006, Section 10.4.1).
With this structure, there is a simple expression for the gradient of L with respect to Œ∑eœÜ . To simplify notation, we
PK
write t(z, yÃÑ) , k=1 (tzy (z (k) , yÃÑ (k) ), 1), Œ∑e , Œ∑eœÜ , Œ∑ , Œ∑œÜ ,
and A , AœÜ . Then dropping terms constant over Œ∑e we have
L = Eq(œÜ)q(z) [ln p(œÜ|z, yÃÑ) ‚àí ln q(œÜ)]
= hŒ∑ + Eq(z) [t(z, yÃÑ)], ‚àáA(e
Œ∑ )i ‚àí (he
Œ∑ , ‚àáA(e
Œ∑ )i‚àíA(e
Œ∑ ))
where we have used the exponential family identity
Eq(œÜ) [tœÜ (œÜ)] = ‚àáA(e
Œ∑ ). Differentiating over Œ∑e, we have


‚àáŒ∑eL = ‚àá2 A(e
Œ∑ ) Œ∑ + Eq(z) [t(z, yÃÑ)] ‚àí Œ∑e .
e is defined (Hoffman et al., 2013)
The natural gradient ‚àá
‚àí1 Œ∑e
2
e Œ∑e , ‚àá A(e
as ‚àá
Œ∑)
‚àáŒ∑e, and so expanding we have

2.2. Stochastic variational inference
e Œ∑eL = Œ∑ +
‚àá

Given a probabilistic model
p(œÜ, z, y) = p(œÜ)

K
Y

K
X

Eq(z(k) ) [(tzy (z (k) , yÃÑ (k) ), 1)] ‚àí Œ∑e.

k=1

p(z (k) |œÜ)p(y (k) |z (k) , œÜ)

k=1

that includes global latent variables œÜ, local latent vari(k) K
ables z = {z (k) }K
}k=1 ,
k=1 , and observations y = {y
the mean field problem is to approximate the posterior
p(œÜ, z|yÃÑ) for fixedQdata yÃÑ with a distribution of the form
q(œÜ)q(z) = q(œÜ) k q(z (k) ) by finding a local minimum
of the KL divergence from the approximating distribution
to the posterior or, equivalently, finding a local maximum
of the marginal likelihood lower bound


p(œÜ, z, yÃÑ)
L , Eq(œÜ)q(z) ln
‚â§ p(yÃÑ).
(1)
q(œÜ)q(z)
SVI optimizes the objective (1) using a stochastic natural
gradient ascent algorithm over the global factors q(œÜ).
Natural gradients of L with respect to the parameters of
q(œÜ) have a convenient form if the prior p(œÜ) and each

Therefore a stochastic natural gradient ascent algorithm on
the global variational parameter Œ∑eœÜ proceeds at iteration t
by sampling a minibatch yÃÑ (k) and taking a step of some size
œÅ(t) in an approximate natural gradient direction via
Œ∑eœÜ ‚Üê (1‚àíœÅ(t) )e
Œ∑œÜ +œÅ(t) (Œ∑œÜ +s¬∑Eq‚àó (z(k) ) [t(z (k) , yÃÑ (k) )])
where s , |yÃÑ|/|yÃÑ (k) | scales the minibatch statistics to represent the full dataset. In each step we find the optimal local factor q ‚àó (z (k) ) with standard mean field updates and the
current value of q(œÜ). There are automatic methods to tune
the sequence of stepsizes (Snoek et al., 2012; Ranganath
et al., 2013), though we do not explore them here.
2.3. Hidden Markov Models
A Bayesian Hidden Markov Model (HMM) on N states includes priors on the model parameters, namely the initial
state distribution and transition matrix rows œÄ = {œÄ (i) }N
i=0

SVI for Time Series Models

and the observation parameters Œ∏ = {Œ∏(i) }N
i=1 . The full
generative model over the parameters, a state sequence
x1:T of length T , and an observation sequence y1:T is
!
œÄ (1)
iid
..
Œ∏(i) ‚àº p(Œ∏), œÄ (i) ‚àº Dir(Œ±(i) ), A ,
.
x1 ‚àº œÄ

(0)

,

xt+1 ‚àº œÄ

(xt )

,

œÄ (N )
(xt )

yt ‚àº p(yt |Œ∏

)

where we abuse notation slightly here and use p(Œ∏) and
p(yt |Œ∏) to denote the prior distribution over Œ∏ and the conditional observation distribution, respectively. When convenient, we collect the transition rows {œÄ (i) }N
i=1 into the
QN
transition matrix A and write q(A) , i=1 q(œÄ (i) ).
Conditioned on the model parameters (œÄ, Œ∏) and a fixed observation sequence yÃÑ1:T , the distribution of x1:T is Markov
on a chain graph. Defining likelihood potentials L by
Lt,i , p(yÃÑt |Œ∏(i) ), the density p(x1:T |yÃÑ1:T , œÄ, Œ∏) is


TP
‚àí1
T
P
(0)
exp ln œÄx1 +
ln Axt ,xt+1 +
ln Lt,xt ‚àí Z .
t=1

t=1

(2)
where Z is the normalizing constant for the distribution.
We say p(x1:T |yÃÑ1:T , œÄ, Œ∏) = HMM(A, œÄ (0) , L).
In mean field inference for HMMs (Beal, 2003), we approximate the full posterior p(œÄ, Œ∏, x1:T |yÃÑ1:T ) with a mean
field variational family q(œÄ)q(Œ∏)q(x1:T ) and update each
variational factor in turn while fixing the others. When updating q(x1:T ), by taking expectations of the log of (2) with
respect to the variational distribution over parameters, we
e œÄ
e with
see the update sets q(x1:T ) = HMM(A,
e(0) , L)
n
o

	 (0)
(0)
ei,j , exp Eq(œÄ) ln Ai,j œÄ
A
ei , exp ln Eq(œÄ(0) ) œÄi

	
e t,i , exp Eq(Œ∏ ) ln p(yÃÑt |xt = i) .
L
i

(3)

We can compute the expectations with respect to q(x1:T )
necessary for the other factors‚Äô updates using the standard
HMM message passing recursions for forward messages F
and backward messages B using these HMM parameters:
Ft,i ,

N
P

Bt,i ,

(0)

eji L
e t,i
Ft‚àí1,j A

F1,i , œÄ
ei

(4)

N
P
eij L
e t+1,j Bt+1,j
A

BT,i , 1.

(5)

j=1

from a state-specific duration distribution each time a state
is entered. That is, if state i is entered at time t, a duration
d is sampled d ‚àº p(d|œë(i) ) for some parameter œë(i) and
the state stays fixed until xt+d‚àí1 , when a Markov transition step selects a new state for xt+d . For identifiability,
self-transitions are often ruled out; the transition matrix A
is constrained via Ai,i = 0 and the Dirichlet prior on each
row is placed on the off-diagonal entries. The parameters
œÄ (0) and Œ∏ are treated as in the HMM.
Analogous to the HMM case, for a fixed observation
sequence yÃÑ1:T , we define likelihood potentials L by
Lt,i , p(yÃÑt |Œ∏(i) ) and now define duration potentials D
via Dd,i , p(d|œë(i) ) and say p(x1:T |yÃÑ1:T , œÄ, Œ∏, œë) =
HSMM(A, œÄ (0) , L, D).
In mean field inference for
HSMMs, as developed in (Hudson, 2008), we approximate
the posterior p(Œ∏, œë, œÄ, x1:T |yÃÑ1:T ) with a variational family
q(Œ∏)q(œë)q(œÄ)q(x1:T ). When updating the factor q(x1:T )
e œÄ
e D)
e using the defiwe have q(x1:T ) = HSMM(A,
e(0) , L,
nitions in (3) and
n
o
e d,i , exp Eq(œë(i) ) ln p(d|œë(i) ) .
D
Expectations with respect to q(x1:T ) can be computed in
terms of the standard HSMM forward messages (F, F ‚àó )
and backward messages (B, B ‚àó ) via the recursions (Murphy, 2002):
Ft,i ,

t‚àí1
P

‚àó
e d,i L
e t‚àíd+1:t,i ,
Ft‚àíd,i
D

‚àó
Ft,i
,

j=1

d=1
‚àó
Bt,i
,

TP
‚àít

N
P
ej,i Ft,j (6)
A

e d,i L
e t+1:t+d,i
Bt+d,i D

Bt,i ,

N
P

‚àó e
Bt,j
Ai,j (7)

j=1

d=1
(0)

‚àó
with F1,i
, œÄi and BT,i , 1. These messages require
2
O(T N + T N 2 ) time to compute for general duration distributions.

3. SVI for HMMs and HSMMs
In this section we apply SVI to both HMMs and HSMMs
and express the SVI updates in terms of HMM and HSMM
messages. For notational simplicity, we consider a dataset
(k)
of K sequences each of length T , written yÃÑ = {yÃÑ1:T }K
k=1 ,
and take each minibatch to be a single sequence, which we
write without the superscript index as yÃÑ1:T .

j=1

3.1. SVI update for HMMs
The messages can be computed in O(T N 2 ) time.
2.4. Hidden semi-Markov Models
The Hidden semi-Markov Model (HSMM) (Murphy, 2002;
Hudson, 2008; Johnson & Willsky, 2013) augments the
generative process of the HMM by sampling a duration

In terms of the notation in Section 2.2, the global variables
are the HMM parameters and the local variables are the hidden states; that is, œÜ = (A, œÄ (0) , Œ∏) and z = x1:T . To make
the updates explicit, we assume the observation parameter
priors p(Œ∏(i) ) and likelihoods p(yt |Œ∏(i) ) are conjugate pairs
of exponential family distributions for each i so that the

SVI for Time Series Models

conditionals have the form
n
o
(i)
(i) (i)
p(Œ∏(i) |y) ‚àù exp hŒ∑Œ∏ + (t(i)
y (y), 1), tŒ∏ (Œ∏ )i .
At each iteration of the SVI algorithm we sample a sequence yÃÑ1:T from the dataset and perform a stochastic gradient step on q(A)q(œÄ (0) )q(Œ∏) of some size œÅ. To compute
the gradient, we need to collect expected sufficient statistics with respect to the optimal factor for q(x1:T ), which in
turn depends on the current value of q(A)q(œÄ (0) )q(Œ∏).
Writing the priors and mean field factors as
n
o
(i) (i)
p(œÄ (i) ) = Dir(Œ±),
p(Œ∏(i) ) ‚àù exp hŒ∑Œ∏ , tŒ∏ (Œ∏(i) )i ,

where Z is the normalizer Z ,

(0)

PN

i=1

‚àó
B1,i
œÄ
ei .

To be written in terms of the HSMM messages the expected
state indicators I[xt = i] must be expanded to
P
I[xt = i] =
I[xœÑ +1 = i 6= xœÑ ] ‚àí I[xœÑ = i 6= xœÑ +1 ]
œÑ <t

Intuitively, this expansion expresses that a state is occupied
after a transition into it occurs and until a transition out
occurs while it is occupied. Then we have
‚àó
‚àó
Eq(x1:T ) I[xt+1 = i, xt 6= xt+1 ] = Ft,i
Bt,i
/Z

Eq(x1:T ) I[xt = i, xt 6= xt+1 ] = Ft,i Bt,i /Z.
from which we can compute Eq(x1:T ) I[xt = i], which we
(i)

(i)

(i)

q(œÄ ) = Dir(e
Œ± ),

n
o
(i) (i)
q(Œ∏ ) ‚àù exp he
Œ∑Œ∏ , tŒ∏ (Œ∏(i) )i

and using the messages F and B as in (4) and (5), we define
PT
(i)
tÃÇ(i)
y , Eq(x1:T )
t=1 I[xt = i]ty (yÃÑt )
PT
= t=1 Ft,i Bt,i ¬∑ (t(i)
(8)
y (yÃÑt ), 1)/Z
P
(i)
T ‚àí1
(tÃÇtrans )j , Eq(x1:T ) t=1 I[xt = i, xt+1 = j]
PT ‚àí1
ei,j L
e t+1,j Bt+1,j /Z
=
Ft,i A
(9)
t=1

(tÃÇinit )i , Eq(x1:T ) I[x1 = i] = œÄ
e0 B1,i /Z
where I[¬∑] is 1 if its argument is true and 0 otherwise and Z
PN
is the normalizing constant Z , i=1 FT,i .
With these expected statistics, taking a natural gradient step
in the parameters of q(A), q(œÄ0 ), and q(Œ∏) of size œÅ is
(i)

Œ∑eŒ∏ (i) ‚Üê (1 ‚àí œÅ)Œ∑eŒ∏ (i) + œÅ(Œ∑Œ∏ + s ¬∑ tÃÇ(i)
y )
(i)

Œ±
e(i) ‚Üê (1 ‚àí œÅ)e
Œ±(i) + œÅ(Œ±(i) + s ¬∑ tÃÇtrans )
(i)

Œ±
e(0) ‚Üê (1 ‚àí œÅ)e
Œ±(0) + œÅ(Œ±(0) + s ¬∑ tÃÇinit )

(10)

(12)

3.2. SVI update for HSMMs
The SVI updates for the HSMM are very similar to those
for the HMM with the addition of a duration update, but
writing the expectations in terms of the HSMM messages
is substantially different. The form of these expected statistics follow from the standard HSMM E-step (Murphy,
2002; Hudson, 2008).
Using the HSMM messages (F, F ‚àó ) and (B, B ‚àó ) defined
in (6)-(7), we can write
PT ‚àí1
(i)
(tÃÇtrans )j , Eq(x1:T ) t=1 I[xt = i, xt+1 = j, xt 6= xt+1 ]
=

‚àó e
t=1 Ft,i Bt,j Ai,j /Z

Finally, we compute the expected duration statistics as indicators on every possible duration d = 1, 2, . . . , T via
P
(i)
(tÃÇdur )d , E I[xt 6= xt+1 , xt+1:t+d = i, xt+d+1 6= i]
t

=

T ‚àíd+1
X

e d,i F ‚àó Bt+d,i (Qt+d
e0
D
t,i
t0 =t Lt ,i )/Z.

(13)

t=1

Note that this step alone requires O(T 2 N ) time even after
the messages have been computed.
If the priors and mean field factors over duration param(i) (i)
eters are p(œë(i) ) ‚àù exp{hŒ∑œë , tœë (œë(i) )i} and q(œë(i) ) ‚àù
(i) (i) (i)
exp{he
Œ∑œë , tœë (œë )i}, and the duration likelihood is
(i)
(i)
p(d|œë ) = exp{htœë (œë(i) ), (td (d), 1)i} then the duration
factor update is
(i)

(i)

T
P

(i)

Œ∑eœë ‚Üê (1 ‚àí œÅ)e
Œ∑œë + œÅ(Œ∑œë + s(

(i)

(tÃÇdur )d ¬∑ (td (d), 1))).

d=1

(11)

where s , |yÃÑ|/|yÃÑ1:T | as in Section 2.2.

PT ‚àí1

use in the definition of tÃÇy given in (8).

(i)

4. SVI for HDP-HMMs and HDP-HSMMs
In this section we extend our methods to the Bayesian
nonparametric versions of these models, the HDP-HMM
and the HDP-HSMM. The generative model for the HDPHMM with scalar concentration parameters Œ±, Œ≥ > 0 is
Œ≤ ‚àº GEM(Œ≥),
x1 ‚àº œÄ (0) ,

œÄ (i) ‚àº DP(Œ±Œ≤),

iid

Œ∏(i) ‚àº p(Œ∏(i) )

xt+1 ‚àº œÄ (xt ) ,

yt ‚àº p(yt |Œ∏(xt ) )

where Œ≤ ‚àº GEM(Œ≥) denotes sampling from a stick breaking distribution defined by
Y
iid
vj ‚àº Beta(1, Œ≥),
Œ≤k =
(1 ‚àí vj )vk
j<k

and œÄ (i) ‚àº DP(Œ±Œ≤) denotes sampling a Dirichlet process
w ‚àº GEM(Œ±),

iid

zk ‚àº Œ≤,

œÄ (i) =

‚àû
X
k=1

wk Œ¥zk .

SVI for Time Series Models

To perform mean field inference in HDP models, we approximate the posterior with a truncated variational distribution. While a common truncation is to limit the two
stick-breaking distributions in the definition of the HDP
(Hoffman et al., 2013), a more convenient truncation for
our models is the ‚Äúdirect assignment‚Äù truncation, used in
(Liang et al., 2007) for batch mean field with the HDPHMM and in (Bryant & Sudderth, 2012) in an SVI algorithm for LDA. The direct assignment truncation limits the
support of q(x1:T ) to the finite set {1, 2, . . . , K}T for a
truncation parameter K, i.e. fixing q(x1:T ) = 0 when any
xt > K. Thus the other factors, namely q(œÄ), q(Œ≤), and
q(Œ∏), only differ from their priors in their distribution over
the first K components. As opposed to standard truncation,
this family of approximations is nested over K, enabling a
search procedure over the truncation parameter as developed in (Bryant & Sudderth, 2012). A similar search procedure can be used with the HDP-HMM and HDP-HSMM
algorithms in this paper, though we do not explore it here.
A disadvantage to the direct assignment truncation is that
the update to q(Œ≤) is not conjugate given the other factors
as in Hoffman et al. (2013). Following Liang et al. (2007),
to simplify the update we use a point estimate by writing
q(Œ≤) = Œ¥Œ≤ ‚àó (Œ≤). Since the main effect of Œ≤ is to enforce
shared sparsity among the œÄ (i) , it is reasonable to expect
that a point approximation for q(Œ≤) will suffice.
The updates to the factors q(Œ∏) and q(x1:T ) are identical to those derived in the previous sections. To derive the SVI update for q(œÄ), we write the relevant
part of the untruncated model and truncated variational
(i)
(i)
factors as p((œÄ1:K , œÄrest )) = Dir(Œ± ¬∑ (Œ≤1:K , Œ≤rest )) and
(i)
(i)
(i)
Œ±(i) ), respectively, where œÄrest ,
q((œÄ1:K , œÄrest )) = Dir(e
PK
P
(i)
K
1 ‚àí k=1 œÄk and Œ≤rest , 1 ‚àí k=1 Œ≤k for i = 1, . . . , K.
(i)
Therefore the updates to q(œÄ ) are identical to those
in (11) except the number of variational parameters is
K + 1 and the prior hyperparameters are replaced with
Œ± ¬∑ (Œ≤1:K , Œ≤rest ).
The gradient of the variational objective with respect to Œ≤ ‚àó
is given by



p(Œ≤, œÄ)
‚àáŒ≤ ‚àó L = ‚àáŒ≤ ‚àó Eq(œÄ) ln
q(Œ≤)q(œÄ)


K
P
‚àó
(i) ‚àó
‚àó
= ‚àáŒ≤ ln p(Œ≤ ) +
Eq(œÄ(i) ) ln p(œÄ |Œ≤ )

We use this gradient expression to take a truncated gradient
step on Œ≤ ‚àó during each SVI update, using a backtracking
line search to ensure the updated value satisfies Œ≤ ‚àó ‚â• 0.
The updates for q(œÄ) and q(Œ≤) in the HDP-HSMM differ
only in that the variational lower bound expression changes
slightly because the support of each q(œÄ (i) ) is restricted to
the off-diagonal (and renormalized). We can adapt q(œÄ (i) )
by simply dropping the ith component from the representation and writing
(i)

(i)

(i)

q((œÄ1:K\i , œÄrest )) = Dir(e
Œ±\i ),
and we change the second term in the gradient for Œ≤ ‚àó to
‚àÇ
(i) ‚àó
‚àÇŒ≤k‚àó Eq(œÄ) [ln p(œÄ |Œ≤ )]
(i)

(i)

= Œ≥œà(e
Œ±k ) ‚àí Œ≥œà(e
Œ±K+1 ) + Œ≥œà(Œ≥

P
j6=i

Œ≤j‚àó ) ‚àí Œ≥œà(Œ≤k‚àó )

when k 6= i, otherwise the partial derivative is 0.
Using these gradient expressions for Œ≤ ‚àó and a suitable
gradient-based batch optimization procedure we can also
perform batch mean field updates for the HDP-HSMM.

5. Fast updates for negative binomial HSMMs
General HSMM inference is much more expensive than
HMM inference, having runtime O(T 2 N + T N 2 ) compared to just O(T N 2 ) on N states and a sequence of length
T . The quadratic dependence on T can be severely limiting
even in the minibatch setting of SVI, since minibatches often must be sufficiently large for good performance (Hoffman et al., 2013; Broderick et al., 2013).
A common approach to mitigate HSMM computational
complexity (Hudson, 2008; Johnson & Willsky, 2013) is
to limit the support of the duration distributions, either in
the model or as an approximation in the message passing
computation, and thus limit the terms in the sums of (6)
and (7). This truncation approach can be readily applied
to the algorithms presented in this paper. However, truncation can be undesirable or ineffective if states have long
durations. In the this section, we develop approximate updates for a particular class of duration distributions with
unbounded support for which the computational complexity is only linear in T .

i=1
‚àÇ
‚àÇŒ≤k‚àó

ln p(Œ≤ ‚àó ) = 2

P
i‚â•k

1‚àí

1
P
j<i

Œ≤j‚àó

‚àí (Œ≥ ‚àí 1)

P
i‚â•k

1‚àí

1
P

j‚â§i

5.1. HMM embeddings of negative binomial HSMMs
Œ≤j‚àó

‚àÇ
(i) ‚àó
‚àÇŒ≤k‚àó Eq(œÄ) [ln p(œÄ |Œ≤ )]
(i)

(i)

= Œ≥œà(e
Œ±k ) ‚àí Œ≥œà(e
Œ±K+1 )+Œ≥œà(Œ≥

K+1
P
j=1

Œ≤j‚àó ) ‚àí Œ≥œà(Œ≤k‚àó ).

The negative binomial distribution we use has two parameters (r, p), where 0 < p < 1 and r is a positive integer. Its
probability mass function (PMF) for k = 1, 2, . . . is


k+r‚àí2
p(k|r, p) =
exp{(k ‚àí 1) ln p + r ln(1 ‚àí p)}.
k‚àí1
(14)

SVI for Time Series Models

Fixing r, the family of distributions parameterized by p is
an exponential family and admits a conjugate Beta prior.
However, as a family over (r, p) it is not exponential because the of the binomial coefficient base measure term
which depends on r. When r = 1 the distribution is geometric, and so the class of HSMMs with negative binomial
durations include HMMs. By varying r and p, the mean
and variance can be controlled separately, making the negative binomial a popular choice for duration distributions
(Bulla & Bulla, 2006; Fearnhead, 2006).
A negative binomial random variable can be represented as
a sum of r geometric
random variables: if x ‚àº NB(r, p)
Pr
and y = 1 + i=1 zi with p(zi = k) = pk (1 ‚àí p), then
x ‚àº y. Therefore given an HSMM in which the durations
of state i are distributed as NB(r(i) , p(i) ) we can construct
PN
an HMM on i=1 r(i) states that encodes the same process, where HSMM state i corresponds to ri states in the
HMM. We call this construction an HMM embedding of
the HSMM, and the resulting HMM transition matrix AÃÑ is
!
C AÃÑ ¬∑¬∑¬∑
1

p(i) 1‚àíp(i)

Ci ,

..
p(i)

12

AÃÑ21 C2

AÃÑ ,

..
.

!

Ô£´

Ô£∂

, AÃÑij , Ô£≠

.
1‚àíp(i)

Ô£∏
(ij)

Aij pÃÑ1

¬∑¬∑¬∑

Aij pÃÑ

(ij)
r (j)

Eq(œÄ)q(p)q(Œ∏) ln AÃÑ. Defining q(xÃÑ1:T ) as the corresponding
distribution over HMM states, we can write expected suf(i)
(i) (i)
ficient statistics tÃÇd , (tÃÇ1 , tÃÇ0 ) for the duration factors
in terms of the expected transition statistics in the HMM
embedding:
(i)
tÃÇd,1

, Eq(xÃÑ1:T )

(i)
tÃÇd,0

, Eq(xÃÑ1:T )

If every r(i) is fixed and the p(i) are the only duration parameters, we can use the HMM embedding to perform efficient conjugate SVI (or batch mean field) updates to the
duration factors q(p(i) ). We write the duration prior and
mean field factors as
p(p(i) ) = Beta(a(i) , b(i) )

q(p(i) ) = Beta(e
a(i) , eb(i) ).

The embedding allows us to write the variational lower
bound for the HSMM as an equivalent HMM variational lower bound with effective transition matrix

T
‚àí1 X
r (i)
X

I[xÃÑt = (i, k), xÃÑt+1 6= xÃÑt+1 ].

t=1 k=1

We can compute these expected transition statistics efficiently from the HMM messages using (9). The SVI update
to the duration factors is then of the form
(i)

e
a(i) ‚Üê (1 ‚àí œÅ)e
a(i) + œÅ(a(i) + s ¬∑ tÃÇd,1 )

(15)

eb(i) ‚Üê (1 ‚àí œÅ)eb(i) + œÅ(b(i) + s ¬∑ tÃÇ(i) )
d,0

(16)

for some stepsize œÅ and minibatch scaling s. We can
similarly write the transition, initial state, and observation
statistics for the other HSMM mean field factors in terms
of its embedding:
Pr(i)
(i)
t=1
k=1 I[xÃÑt = (i, k)]ty (yÃÑt )
PT ‚àí1
Eq(xÃÑ1:T ) t=1 I[xÃÑt = (i, r(i) ) 6= xÃÑt+1 ]

tÃÇy(i) , Eq(xÃÑ1:T )
(i)

This construction draws on ideas from HSMMs with ‚Äúparametric macro-states‚Äù (GueÃÅdon, 2005, Section 3) and on
expanded-state HMMs (ESHMMs) (Russell & Moore,
1985; Russell & Cook, 1987; Johnson, 2005). However,
this precise construction for negative binomial durations
does not appear in those works. Furthermore, we extend
these ideas by applying Bayesian inference as well as methods to fit (a posterior over) the r(i) parameter, as we discuss
in the next section.

I[xÃÑt = xÃÑt+1 = (i, k)]

t=1 k=1

(tÃÇtrans )j ,
where pÃÑ(ij) is defined in the supplementary materials. We
write the HMM embedding state sequence as xÃÑ1:T , where
each xÃÑt decomposes as xÃÑt = (xt , k) for k = 1, 2, . . . , r(xt )
according
PNto the block structure of AÃÑ. If we define
R , N1 i=1 r(i) , then passing messages in this structured
HMM embedding can be done in O(T N R + T N 2 ) time.

T
‚àí1 X
r (i)
X

PT

(tÃÇinit )i , Eq(xÃÑ1:T ) I[xÃÑ1 = (xt , 1)].
Using these statistics we perform SVI updates to the corresponding the mean field factors as in (10)-(12).
We have shown that by working with an efficient HMM
embedding representation we can compute updates to the
HSMM mean field factors in time O(T N R + T N 2 ) when
durations are negative binomially distributed with fixed r.
In the next subsection we extend these fast updates to include a variational representation to the posterior of r.
5.2. Approximate updates for fitting q(r, p)
By learning r as well as p, negative binomial HSMMs can
learn to be HMMs when appropriate and generally provide
a much more flexible class of duration distributions. In this
subsection, we derive an exact SVI update step for fitting
both r and p, explain its computational difficulties, and propose a fast approximate alternative based on sampling.
We define mean field factors q(r(i) )q(p(i) |r(i) ), where each
q(r(i) ) is a categorical distribution with finite support taken
(i)
to be {1, 2, . . . , rmax } and each q(p(i) |r(i) ) is Beta, i.e.
q(r(i) ) ‚àù exp{he
ŒΩ (i) , Ir(i) i}
q(p(i) |r(i) ) = Beta(e
a(i,r) , eb(i,r) ).

SVI for Time Series Models

where Ir(i) is an indicator vector with the r(i) th entry set
to 1 and the others to zero. The priors are defined similarly. To simplify notation, in this section we often drop
the superscript i from the notation.
Two challenges arise in computing updates to q(r, p). First,
the optimal variational factor on the HSMM states is
q(x1:T ) ‚àù exp Eq(œÄ)q(r,p)q(Œ∏) ln p(x1:T |yÃÑ1:T , œÄ, r, p, Œ∏).
Due to the expectation over q(r), this factor does not have
the form required to use the efficient HMM embedding of
Section 5.1, and so the corresponding general HSMM messages require O(T 2 N + T N 2 ) time to compute. Second,
as we show next, due to the base measure term in (14), to
compute an exact update to q(r, p) requires computing the
expected duration indicator statistics of (13), a computation
which itself requires O(T 2 N ) time even after computing
the HSMM messages.
First, we show that the update to q(p|r) is straightforward.
To derive an update for q(p|r), we write the relevant part of
the variational lower bound as


p(r, p, D)
(17)
L , Eq(r,p)q(x1:T ) ln
q(r, p)
p(r)
= Eq(r) ln
+ Eq(r)q(x1:T ) h(r, D)
q(r)


p(p)pÃÑ(D|r, p)
+ Eq(r) Eq(p|r) ln
q(p|r)
where
D is the set of relevant durations in x1:T , h(r, D) ,
P
r+k‚àí2
arises from the negative binomial base
k‚ààD ln
k‚àí1
P
measure term, and ln pÃÑ(D|r, p) , k‚ààD k ln p + r ln(1 ‚àí
p) collects the negative binomial PMF terms excluding the
base measure. The only terms in (17) that depend on q(p|r)
are in the final bracketed term. Furthermore, each of these
terms corresponds to the variational lower bound for the
fixed-r case described in Section 5.1, and so each q(p|r)
factor can be updated with Eqs. (15)-(16).
To compute an update for q(r), we note that since it is a
distribution with finite support we can write its completedata conditional in exponential family form trivially via

As discussed in Wang & Blei (2012), this sampling approximation does not optimize the variational lower bound
over q(x1:T ) and so it should yield an inferior objective
value. Indeed, while the optimal mean field update sets
q(x1:T ) ‚àù exp{Eq [ln p(œÄ, Œ∏, œë, x1:T , yÃÑ1:T )]}, this update
approximates q(x1:T ) ‚àù Eq [p(œÄ, Œ∏, œë, x1:T , yÃÑ1:T )]. However, Wang & Blei (2012) found this approximate update
yielded better predictive performance in some topic models, and provided an interpretation as an approximate expectation propagation (EP) update.
(k)

With the collected samples
S , {x1:T }Sk=1 , we set the
P
1
factor qÃÇ(x1:T ) = S xÃÇ1:T ‚ààS Œ¥xÃÇ1:T (x1:T ), where we use
the notation qÃÇ(x1:T ) to emphasize that it is a sample-based
representation. It is straightforward to compute the expectation over states in (18) by plugging in the sampled durations. The update to the parameters of q(r(i) , p(i) ) is
ŒΩe(i) ‚Üê (1 ‚àí œÅ)e
ŒΩ (i) + œÅ(ŒΩ (i) + s ¬∑ tÃÇ(i)
r )
e
a(i,r) ‚Üê (1 ‚àí œÅ)e
a(i,r) + œÅ(a(i) + s ¬∑ tÃÇ(i,r)
)
a
eb(i,r) ‚Üê (1 ‚àí œÅ)eb(i,r) + œÅ(b(i) + s ¬∑ tÃÇ(i,r) )
b
tÃÇ(i,r)
,
a

1 X
S

X

1 X
S

X

(d ‚àí 1)

xÃÇ‚ààS d‚ààD (i) (xÃÇ)

(i,r)

tÃÇb

,

r

xÃÇ‚ààS d‚ààD (i) (xÃÇ)

(tÃÇr(i) )r , (e
a(i,r) + tÃÇ(i,r)
‚àí 1)Eq(p|r) [ln(p(i,r) )]
a

p(r|p, D) ‚àù exp{hŒΩ + tr (p, D), Ir i}
P
tr (p, D)j , k‚ààD ln p(p|k, r = j) + ln h(j, k)
and so from the results in Section 2.2 the natural gradient
of (17) with respect to the parameters of q(r) is
e ŒΩeL = ŒΩ + Eq(p|r)q(x ) tr (p, D) ‚àí ŒΩe.
‚àá
1:T

Therefore computing an exact SVI update on the q(r, p)
factors is expensive both because the HSMM messages
cannot be computed using the methods of Section 5.1 and
because given the messages the required statistics are expensive to compute. To achieve an update running time that
is linear in T , we propose to use a sample approximation
to q(x1:T ) inspired by the method developed in (Wang &
Blei, 2012). That is, we sample negative binomial HSMM
models from the distribution q(œÄ)q(Œ∏)q(r, p) and use the
embedding to generate a sample of x1:T under each model.
Using the message passing methods of Section 5.1, the first
state sequence sample for each model can be collected in
time O(T N R + T N 2 ), and additional state sequence samples from the same model can be collected in time O(T N ).

(18)

Due to the base measure term h(j, k), to compute this update requires evaluating the expected statistics of Eq. (13),
which require O(T 2 N ) time.

(i,r)
+ (eb(i,r) + tÃÇb ‚àí 1)Eq(p|r) [ln(1 ‚àí p(i,r) )]


X
X
d+r‚àí2
+
ln
d‚àí1
(i)
xÃÇ‚ààS

d‚ààD

(xÃÇ)

and where D(i) (xÃÇ1:T ) denotes the set of durations of state
i in the sequence xÃÇ1:T . The updates to the other factors are
as before but with expectations taken over qÃÇ(x1:T ).
The methods presented in this section for HSMMs with
negative binomial durations can be extended in several

ways. In particular, one can use similar methods to perform
efficient updates when state durations are modeled as mixtures of negative binomial distributions. Since each negative binomial can separately parameterize mean and variance, in this way one can generate a flexible and convenient
family of duration distributions analogous to the Gaussian
mixture model pervasive in density modeling.

held-out predictive likelihood

SVI for Time Series Models

6. Experiments

‚âà Eq(œÄ)q(Œ∏) p(yÃÑtest |œÄ, Œ∏)
and approximating the expectation by sampling models
from the variational distribution. To reproduce these figures, see the code in the supplementary materials.
First, we compare the performance of SVI and batch mean
field algorithms for the HDP-HMM. We sampled a 10-state
HMM with 2-dimensional Gaussian emissions and generated a dataset of 100 observation sequences of length 3000
each. We chose a random subset of 95% of the sequences
as training sequences and held out 5% as test sequences.
We repeated the fitting procedures 5 times with identical
initializations drawn from the prior, and we report the median performance with standard deviation error bars. The
SVI procedure made only one pass through the training set.
Figure 1(a) shows that the SVI algorithm produces fits that
are comparable in performance in the time it takes the batch
algorithm to complete a single iteration.
Similarly, we compare the SVI and batch mean field algorithms for the HDP-HSMM with Poisson durations. Due to
the much greater computational complexity of HSMM inference, we generated a set of 30 sequences of length 2000
each and used 90% of the sequences in the training set.
Figure 1(b) again demonstrates that the SVI algorithm can
fit such models in the time it takes the batch algorithm to
complete a single iteration.
Finally, we compare the performance of the exact SVI update for the HSMM with that of the fast approximate update proposed in Section 5. We again generated data using Poisson duration distributions, but we train models using negative binomial durations where p ‚àº Beta(1, 1) and
r ‚àº Uniform({1, 2, . . . , 10}). We generated 55 observation sequences of length 3000 and used 90% of the sequences in the training set. We compare the sampling algorithm‚Äôs performance for several numbers of samples S.

svi
batch

‚àí56000
‚àí58000
‚àí60000
‚àí62000
‚àí64000
‚àí66000
‚àí68000 -2
10

10-1

100 101 102
time (seconds)

103

held-out predictive likelihood

(a) SVI vs Batch HDP-HMM
‚àí3800
‚àí4000
‚àí4200
‚àí4400
‚àí4600
‚àí4800
‚àí5000
‚àí5200
‚àí5400
‚àí5600 0
10

svi
batch

101
102
time (seconds)

103

(b) SVI vs Batch HDP-HSMM
held-out predictive likelihood

We conclude with a numerical study to validate these algorithms and in particular measure the effectiveness of the
approximate updates proposed in Section 5.2. As a performance metric, we evaluate an approximate posterior predictive density on held-out data, writing
Z Z
p(yÃÑtest |yÃÑtrain ) =
p(yÃÑtest |œÄŒ∏)p(œÄ, Œ∏|yÃÑtrain )dœÄdŒ∏

‚àí54000

‚àí12000
‚àí13000
‚àí14000
‚àí15000
‚àí16000
‚àí17000
‚àí18000
‚àí19000 -1
10

S=10
S=5
S=1
exact

100
101
102
time (seconds)

103

(c) Exact vs Approx. HSMM SVI
Figure 1. Synthetic numerical experiments.

Figure 1(c) shows that the approximate update from Section 5 results in higher predictive performance than that of
the model trained with the exact update even using a single
sample. This performance is likely dataset-dependent, but
the experiment demonstrates that the approximate update
may be very effective in some cases.

7. Conclusion
This paper develops scalable SVI-based inference for
HMMs, HSMMs, HDP-HMMs, and HDP-HSMMs, and
provides a technique to make Bayesian inference in negative binomial HSMMs much more practical. These models
are widely applicable to time series inference, so these algorithms and our code may be immediately useful to the
community.

Acknowledgements
This research was supported in part under AFOSR Grant
FA9550-12-1-0287.

SVI for Time Series Models

References
Beal, Matthew James. Variational algorithms for approximate
Bayesian inference. PhD thesis, University of London, 2003.
Bernardo, JoseÃÅ M and Smith, Adrian FM. Bayesian theory, volume 405. Wiley. com, 2009.
Bishop, Christopher M and Nasrabadi, Nasser M. Pattern recognition and machine learning, volume 1. springer New York,
2006.
Bottou, LeÃÅon. Online learning and stochastic approximations.
On-line learning in neural networks, 17:9, 1998.
Broderick, Tamara, Boyd, Nicholas, Wibisono, Andre, Wilson,
Ashia C, and Jordan, Michael. Streaming variational bayes.
In Advances in Neural Information Processing Systems, pp.
1727‚Äì1735, 2013.
Bryant, Michael and Sudderth, Erik B. Truly nonparametric online variational inference for hierarchical dirichlet processes.
In Advances in Neural Information Processing Systems, pp.
2708‚Äì2716, 2012.
Bulla, Jan and Bulla, Ingo. Stylized facts of financial time series
and hidden semi-markov models. Computational Statistics &
Data Analysis, 51(4):2192‚Äì2209, 2006.
Fearnhead, Paul. Exact and efficient bayesian inference for multiple changepoint problems. Statistics and computing, 16(2):
203‚Äì213, 2006.
Fox, E. B., Sudderth, E. B., Jordan, M. I., and Willsky, A. S. Sharing features among dynamical systems with beta processes. In
Neural Information Processing Systems 22. MIT Press, 2010.

Lehman, Li-wei H, Nemati, Shamim, Adams, Ryan P, and Mark,
Roger G. Discovering shared dynamics in physiological signals: Application to patient monitoring in icu. In Engineering
in Medicine and Biology Society (EMBC), 2012 Annual International Conference of the IEEE, pp. 5939‚Äì5942. IEEE, 2012.
Liang, Percy, Petrov, Slav, Jordan, Michael I, and Klein, Dan. The
infinite pcfg using hierarchical dirichlet processes. In EMNLPCoNLL, pp. 688‚Äì697, 2007.
LindeÃÅn, Martin, Johnson, Stephanie, van de Meent, Jan-Willem,
Phillips, Rob, and Wiggins, Chris H. Analysis of dna looping kinetics in tethered particle motion experiments using hidden markov models. Biophysical Journal, 104(2):418A‚Äì418A,
2013.
Murphy, K. Hidden semi-markov models (segment models).
Technical Report, November 2002. URL http://www.cs.
ubc.ca/Àúmurphyk/Papers/segment.pdf.
Ranganath, Rajesh, Wang, Chong, David, Blei, and Xing, Eric.
An adaptive learning rate for stochastic variational inference.
In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 298‚Äì306, 2013.
Robbins, Herbert and Monro, Sutton. A stochastic approximation
method. The Annals of Mathematical Statistics, pp. 400‚Äì407,
1951.
Russell, Martin and Moore, Roger. Explicit modelling of state occupancy in hidden markov models for automatic speech recognition. In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP‚Äô85., volume 10, pp. 5‚Äì8.
IEEE, 1985.

Fox, Emily, Sudderth, Erik B, Jordan, Michael I, and Willsky,
Alan S. Bayesian nonparametric inference of switching dynamic linear models. Signal Processing, IEEE Transactions
on, 59(4):1569‚Äì1585, 2011.

Russell, Martin J and Cook, A. Experimental evaluation of duration modelling techniques for automatic speech recognition. In
Acoustics, Speech, and Signal Processing, IEEE International
Conference on ICASSP‚Äô87., volume 12, pp. 2376‚Äì2379. IEEE,
1987.

Griffiths, Thomas L, Steyvers, Mark, Blei, David M, and Tenenbaum, Joshua B. Integrating topics and syntax. In Advances in
neural information processing systems, pp. 537‚Äì544, 2004.

Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. Practical
bayesian optimization of machine learning algorithms. arXiv
preprint arXiv:1206.2944, 2012.

GueÃÅdon, Yann. Hidden hybrid markov/semi-markov chains.
Computational statistics & Data analysis, 49(3):663‚Äì688,
2005.

Wang, Chong and Blei, David. Truncation-free online variational
inference for bayesian nonparametric models. In Advances in
Neural Information Processing Systems 25, pp. 422‚Äì430, 2012.

Hoffman, M, Blei, D, Wang, Chong, and Paisley, John. Stochastic
variational inference. Journal of Machine Learning Research,
14:1303‚Äì1347, 2013.

Wang, Chong, Paisley, John W, and Blei, David M. Online variational inference for the hierarchical dirichlet process. In International Conference on Artificial Intelligence and Statistics,
pp. 752‚Äì760, 2011.

Hoffman, Matthew, Bach, Francis R, and Blei, David M. Online
learning for latent dirichlet allocation. In advances in neural
information processing systems, pp. 856‚Äì864, 2010.
Hudson, Nicolas H. Inference in hybrid systems with applications
in neural prosthetics. PhD thesis, California Institute of Technology, 2008.
Johnson, Matthew J. and Willsky, Alan S. Bayesian nonparametric hidden semi-markov models. Journal of Machine Learning
Research, 14:673‚Äì701, February 2013.
Johnson, Michael T. Capacity and complexity of hmm duration
modeling techniques. Signal Processing Letters, IEEE, 12(5):
407‚Äì410, 2005.

