Accelerated Online Low-Rank Tensor Learning
for Multivariate Spatio-Temporal Streams
Rose Yu
Dehua Cheng
Yan Liu
Department of Computer Science, University of Southern California

Abstract
Low-rank tensor learning has many applications
in machine learning. A series of batch learning
algorithms have achieved great successes. However, in many emerging applications, such as climate data analysis, we are confronted with largescale tensor streams, which pose significant challenges to existing solutions. In this paper, we
propose an accelerated online low-rank tensor
learning algorithm (ALTO) to solve the problem.
At each iteration, we project the current tensor
to a low-dimensional tensor, using the information of the previous low-rank tensor, in order to
perform efficient tensor decomposition, and then
recover the low-rank approximation of the current tensor. By randomly selecting additional
subspaces, we successfully overcome the issue of
local optima at an extremely low computational
cost. We evaluate our method on two tasks in
online multivariate spatio-temporal analysis: online forecasting and multi-model ensemble. Experiment results show that our method achieves
comparable predictive accuracy with significant
speed-up.

1. Introduction
Low-rank tensor learning enjoys a broad range of applications in practical machine learning problems (Kolda &
Bader, 2009), ranging from signal processing, computer
vision, to neuroscience. One classical example is learning a low-rank tensor for multivariate regression, for which
a series of effective batch learning algorithms have been
developed (De Lathauwer et al., 2000; Guo et al., 2012;
Zhou et al., 2013; Bahadori et al., 2014). We notice that in
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

QIYU @ USC . EDU
DEHUA . CHENG @ USC . EDU
YANLIU . CS @ USC . EDU

many emerging applications, large-scale tensor data come
in streams, such as the spatio-temporal climate observations in climate data analysis. Batch learning algorithms
would suffer from computational bottleneck, especially
facing the challenge of short response time. Therefore, effective and fast online learning algorithms are a must for
enabling real-time large-scale tensor analysis.
Online learning of low-rank tensors aims to dynamically
update a tensor while preserving the low-rank structure.
While online low-rank matrix learning has been intensively
studied, e.g. (Brand, 2002; Meka et al., 2008; Shalit et al.,
2010), online tensor learning remains underexplored. The
problem is extremely challenging due to the inherent complexity of tensor analysis (Hillar & Lim, 2013). Local solutions (Sun et al., 2008) have achieved wide success in
real applications but lack rigorous theoretical understanding. For certain rank structures, we can use the nuclear
norm as a convex surrogate for the rank constraint and solve
the problem with off-the-shelf online low-rank matrix algorithms (Avron et al., 2012; Ouyang et al., 2013). Nevertheless, it is known that optimizing over convex surrogate
loss may lead to sub-optimal solutions (Zhang et al., 2013).
Moreover, solving an optimization problem with nuclear
norm regularization itself is computationally expensive.
In this paper, we develop a novel framework, namely the
Accelerated Low-rank Tensor Online Learning (ALTO) algorithm to address the problem. Our solution follows a
two-step procedure: we first solve an unconstrained tensor learning problem, and then adjust the solution tensor
to satisfy the low-rank constraint. ALTO significantly accelerates the online learning process by keeping track of
the low-rank components of the solution obtained at each
iteration: It performs a dimension reduction of the tensor using previous low-rank components and tensor matrix
multiplications, in order to avoid the expensive operations
of singular value decomposition (SVD) on unfolded matrices of the full tensor. In addition, it employs randomization techniques to select additional dimensions so as to
overcome the issue of local optima in existing incremental

Accelerated Online Low-Rank Tensor Learning

tensor learning algorithms (Sun et al., 2008). Theoretical
analysis shows that our randomization technique can significantly reduce the noise at a cost of very minor biases.
As a side outcome, we also observe an interesting property: despite being non-convex, the low-rank space usually
behaves like a convex set in its neighborhood.

position can be computed by SVD on all possible unfolded
matrices of a tensor. It is also known as high order singular value decomposition (HOSVD) (De Lathauwer et al.,
2000).

We demonstrate the effectiveness of our algorithm via two
machine learning tasks in spatio-temporal stream analysis:
one is the classical forecasting problem (i.e., performing nstep ahead prediction from historical observations), and the
other is the multi-model ensemble problem, a fundamental task in climatology to make predictions by combining
the forecasting results from multiple simulation models.
In these applications, the data often exhibit unique properties, such as spatial proximity, temporal periodicity and
variable correlations, which can be captured naturally via
the low-rank constraint. We conduct experiments on both
synthetic and real-world application datasets, including a
Foursquare check-in dataset, a daily weather dataset and a
climate ensemble dataset. Experimental results show that
our algorithm can achieve competitive prediction accuracy
with significant speed-up. In addition, the low-rank tensor parameters learned by our algorithm from the climate
dataset provide interesting insights into the underlying relationships between simulations models and physical processes.

Consider the following low-rank tensor learning problem
for regression with predictor tensor Z 2 RQ⇥T ⇥M , and
response tensor X 2 RP ⇥T ⇥M . Our goal is to learn a
model parameter tensor W 2 RP ⇥Q⇥M whose rank is upper bounded by R.
n
o
c = argminW P kW:,:,m Z:,t,m X:,t,m k2
W
F
t,m

2. Online Low-Rank Tensor Learning
2.1. Preliminaries
Across the paper, we use calligraphy font for tensors, such
as X , Y, bold uppercase letters for matrices, such as A, B,
and bold lowercase letters for vectors, such as x, y.
Rank-R Projection For any matrix M, let p (M, R) be
the projection of M to the top-R spectral spaces. It can
be calculated using top-R truncated SVD: M = U⌃V> ,
>
p (M, R) = UR ⌃R VR
. The rank R might be omitted
when the context is clear.
Tensor Unfolding Each dimension of a tensor is a mode.
An n-mode unfolding of a tensor A along mode i transforms a tensor into a matrix A(i) by treating i as the
first mode of the matrix and cyclically concatenating other
modes. The indexing follows the convention in (Kolda &
Bader, 2009). It is also known as tensor matricization.
N-Mode Product The n-mode product between tensor A
and matrix U on mode i is represented as A ⇥i U and is
defined as (A ⇥i U)(i) = UA(i) .

Tucker Decomposition Tucker decomposition factorizes a
tensor A into A = S ⇥1 U1 · · · ⇥n Un , where {Un } are
all unitary matrices and the core tensor satisfies that S(i) is
row-wise orthogonal for all i = 1, 2, . . . , n. Tucker decom-

2.2. General Framework

s.t.

rank(W)  R,

(1)

where a single : is used to index entire rows or columns.
Different from matrices, there are several meaningful ways
to define the rank of a tensor (Kolda & Bader, 2009). The
matrix rank of the mode-n unfolding rank(W(n) ) is called
the
PNn-rank of tensor W. And the summation of n-rank
n=1 rank(W(n) ), namely the sum-n-rank of a tensor W,
is commonly applied because it is easy to compute (Hillar
& Lim, 2013; De Lathauwer et al., 2000) than others (e.g.,
CP rank).
Bahadori et al. (2014) provides a greedy algorithm for computing the solution in the batch setting. It can also be solved
by replacing the rank constraint with the nuclear norm constraint. However, optimizing over this convex surrogate
loss may lead to sub-optimal solutions and is computationally expensive (Zhang et al., 2013). Therefore, we propose the Accelerated Low-rank Tensor Online Learning
(ALTO) algorithm, which solves the problem via a simple
two-step approach: (1) solving the unconstrained optimization problem given the new data, (2) updating the solution
with the low-rank constraint, i.e. projecting the solution
to the space of low-rank tensors. As we will show later
through theoretical analysis that while the first step yields
approximately low-rank tensor, this two-step approach is
nearly optimal. It is also computationally efficient since
the unconstrained optimization problem has a closed form
solution, which is preferable since in online settings, where
the data stream arrives in mini-batches, we need to dynamically update the model tensor while preserving its low-rank
structures.
2.3. Tensor Stream in Online Setting
In Step 1, thePquadratic loss function in Equation 1 is
M
equivalent to m=1 kW:,:,m Z:,:,m
X:,:,m k2F for m =
1, 2, · · · , M , which is an ordinary linear regression.

In online setting, the predictor tensor Z and the response
tensor X are updated over time. Especially for the appli-

Accelerated Online Low-Rank Tensor Learning

cation of our interest, i.e., the multivariate spatio-temporal
stream analysis, both Z and X grow along the temporal dimension as time T increases. We define Wm = W:,:,m and
similarly for others, the unconstrained optimization prob2
lem at time T can be written as minW kWZ1:T X1:T kF ,
where we omit the index m for simplicity. Suppose that at
time stamp T , we receive a new batch of data of size b, we
can update the parameter tensor in the k-th iteration W (k)
with two possible strategies: one is exact update, and the
other is increment update.
Exact update Notice that we can obtain a closed-form
solution of W (k) by using all the data from time stamp 1 to
T + b as follows:
W(k) = X1:T +b Z†1:T +b .
where † denotes matrix pseudo-inverse. Note that the
pseudo-inverse can be computed efficiently via the Woodbury matrix identity (Woodbury, 1950). At each iteration,
we can compute the inverse of the complete data covari1
ance (Z1:T +b Z>
by inverting a smaller matrix con1:T +b )
structed from the new data ZT +1:T +b at a computational
cost linear to the batch size b, with a small memory overhead to store the inverse of the previous covariance matrix
1
(Z1:T Z>
. We defer the details to Appendix B.1.
1:T )
Increment update We can also incrementally update the
value of W given the new data as follows:
W(k) = (1

↵)W(k

1)

+ ↵XT +1:T +b Z†T +1:T +b .

The difference of the two updating scheme lies in the variables we store in memory. For exact update, we store the
data statistics required to reconstruct the model. It gives
an exact solution for the linear regression problem given
all the historical observations. For incremental update, we
store the previous model, compute the solution for current
data only, and then take a convex combination of two models. Note that different statistical properties of these two
updating scheme may require different theoretical analysis
tools, but the low-rank projection of the solution is invariant to the updating strategy.
2.4. Online Low-Rank Tensor Approximation
In Step 2, we need to project the solution from Step 1 to
the low-rank tensor space. In ALTO, we measure the rank
with respect to the sum-n-rank of the tensor: We restrict
the maximum n-rank of tensor W over all modes to be no
larger than R. In order to obtain the n-rank projection,
we resort to Tucker decomposition (De Lathauwer et al.,
2000), which decomposes a tensor into a core tensor and
a set of projection matrices. The dimensions of the core
tensor are n-ranks of the tensor itself. The projection is

generally time consuming, as it usually involves SVD on
unfolded matrices at each mode of a full tensor. For the
online setting, this operation needs to be repeated for each
iteration, which is infeasible for large-scale applications. In
ALTO, we utilize the projection results from the last iteration to approximate the current projection. It eliminates the
need of SVD on unfolded matrices of a full tensor. Instead,
it performs dimension reduction and computes the SVD on
unfolded matrices of a low-dimensional tensor.
Without the loss of generality, we elaborate ALTO via a
third order tensor. Given the Tucker decomposition of W 2
RN ⇥N ⇥N from the previous iteration:
W (k

1)

= S (k

1)

(k 1)

⇥1 U1

(k 1)

⇥2 U2

(k 1)

⇥3 U3

.

(k 1)

we first augment each Ui
2 RN ⇥R with K random
column vectors for i = 1, 2, 3, which are drawn from a zero
mean Gaussian distribution. These random column vectors
are introduced as noise perturbation. Then we apply GramSchmidt process to create orthonormal augmented projec(k 1)
tion matrices Vi
2 RN ⇥(R+K) , which has K more
(t 1)
columns than Ui
, for i = 1, 2, 3 respectively.
(k 1)

With augmented projection matrices Vi
, we project
0
the tensor W (k) to an augmented core tensor S (k) with
dimension (R + K) ⇥ (R + K) ⇥ (R + K).
S

0

(k)

= W (k

1)

(k 1)>

⇥ 1 V1

(k 1)>

⇥ 2 V2

(k 1)>

⇥ 3 V3

.

Then we compute the rank-R approximation of the aug0
mented core by decomposing S (k) :
S

0

(k)

0

(k)

⇡ S (k) ⇥1 V1

0

(k)

⇥ 2 V2

0

(k)

⇥ 3 V3

(k)
where S
is the new core tensor with dimension R⇥R⇥R
0
(k)
and Vi is of size (R + K) ⇥ R. We update the new
(k)
(k 1) 0 (k)
projection matrices as Ui = Vi
Vi for i = 1, 2, 3.
And the final low-rank projection of the solution tensor of
current iteration is given by
(k)

(k)

(k)

W (k) = S (k) ⇥1 U1 ⇥2 U2 ⇥3 U3 .
We summarize the workflow of ALTO in Algorithm 1. The
0
rank-R approximation of the augmented core S (k) is computed by iterating over all the modes and sequentially mapping the unfolded tensor into the rank-R subspace. We
name this procedure as low-rank Tensor Sequential Mapping (TSM), which is described in Algorithm 2.
ALTO is computationally efficient since the augmented
0
core tensor S (k) has dimension (R + K) ⇥ (R + K) ⇥
(R + K), which is much smaller than W (k) . At each iteration, the low-rank mapping procedure TSM only involves
top-R SVD on matrices of size (R + K) ⇥ (R + K)2 , in
comparison to the expensive top-R SVD on N ⇥ N 2 matrices in most existing low-rank tensor learning approaches.

Accelerated Online Low-Rank Tensor Learning

Algorithm 1 Accelerated Low-rank Tensor Online Learning (ALTO)
[W new , Unew ] = ALTO(W, U, R, K):
Input: original tensor W and projection matrices
Ui , i = 1, 2, 3. rank R, augmentation factor K
Output: updated tensor W new and projection matrices
Unew
i , i = 1, 2, 3.
1 Augment, orthogonalize and normalize Ui , i = 1, 2, 3 to
Vi , i = 1, 2, 3 with R + K columns.
2 Project W ! S 0 = W ⇥1 V1> ⇥2 V2> ⇥3 V3> .
3 Find the rank-R approximation to S 0 with TSM:
TSM(S 0 , R) = S ⇥1 V10 ⇥2 V20 ⇥3 V30 .
4 Return Unew
= Vi Vi0 , i = 1, 2, 3 and
i
new
new
new
W
= S ⇥1 Unew
1 ⇥2 U2 ⇥3 U3 .
Algorithm 2 low-rank Tensor Sequential Mapping
W new = TSM(W, R):
Input:: tensor W and target rank R.
Output:: tensor W new .
1 Update W(1)
p W(1) , R , where p (M, R) maps M
to its top-R singular spaces.
2 Update W(2)
p W(2) , R .
3 Update W(3)
p W(3) , R .
4 Return W new = W.
The K random column vectors are introduced so that the
algorithm can jump out of the same low-rank subspace.
A heuristic algorithm called Streaming Tensor Analysis
(STA) is explored in (Sun et al., 2006), where the new
core tensor is simply computed by S (k) = W (k) ⇥1
(k 1) >
(k 1) >
(k 1) >
(U1
) ⇥2 (U2
) ⇥3 (U3
) . However, since
the projection restricts the tensor to a fixed subspace, STA
suffers from local optima because even when the projection
matrices are updated after one examines the core tensor, the
space is still largely invariant. Our algorithm resolves this
issue via the randomization technique.
Note that when the augmentation factor K is so large that
Vi becomes full rank, ALTO turns into an iterative singular value thresholding procedure, where the solution obtained from each iteration is directly projected to the space
of low-rank tensor via top-R truncated SVD. Similar idea
has been examined in (Jain et al., 2010) for the low-rank
matrix learning in batch settings.
2.5. Theoretical Analysis of ALTO
We provide theoretical analysis of ALTO in terms of lowrankness, approximation accuracy as well as the behavior
of the randomized projection technique. We summarize the
main results and defer the detailed proofs to Appendix A.
Let W be the tensor before TSM procedure. The follow-

ing proposition guarantees that the output W new has low
n-rank.
Proposition 1 (Low-Rank Guarantee). Given a tensor
W 2 RI⇥J⇥K and a target rank R, then for W new =
TSM(W, R), its i-rank is no greater than R for any i.
The result directly follows the conclusions from HOSVD
(De Lathauwer et al., 2000).
We denote W⇤ as the rank-R target matrix and consider a
matrix W in its neighborhood with kW W⇤ kF < ✏. The
following proposition guarantees the approximation accuracy of TSM.
Proposition 2. Let W⇤ be an N ⇥ N matrix with (1)
rank(W⇤ ) = R, (2)kW⇤ kF < Cw , (3) k (W⇤ ) > w ,
W be an N ⇥ N matrix such that kW W⇤ kF  ✏, E
be a random matrix with (3) zero mean, (4) N (E)
e,
(5)kEkF  ✏e , then we have that
2

2

W⇤ kF  kW + E

kp (W + E)

W⇤ kF ,

when
(N

2
8(✏+✏e )4 Cw

2R)

4
w

2
e

AND

w

4 (✏e + ✏) .

Proposition 2 shows that when the target matrix is low-rank
and it has reasonable condition number, then in its neighborhood, we can conduct low-rank mapping and expect the
error to be reduced.
Due to the non-convexity of the low-rank space, the lowrank mapping may push the output further away from the
target even if the target itself lies within the low-rank matrix
space. Luckily, we prove that this is a rare event and that the
space of rank-R matrix can be treated as “nearly-convex”
in its neighborhood.
In order to see this, denote the full SVD of W as W =
>
[U1 , U2 ] diag (⌃1 , ⌃2 ) [V1 , V2 ] , the blocks with subscript 1 correspond to the top-R space. Then we have
kW

2

W⇤ kF
= ⌃2

kp (W)

2

W⇤ kF

⇤
U>
2 W V2

2

F

⇤
U>
2 W V2

2
F

.

(2)

By Wedin sin ✓ theorem (Wedin, 1972), we know that
⇤
2
U>
2 W V2 F ⇠ O(✏ ), while k⌃2 kF is very likely to
be at the level of O(✏). That is, the reduced noise is
a first order quantity, while the newly introduced bias is
of the second order. It justifies that the approximation
error is reduced after the low-rank sequential mapping:
kp (W) W⇤ kF  kW W⇤ kF .
Note that error reduction in Proposition 2 holds under certain statistical assumptions. However, the error is still contained well within a factor of 2 without any statistical assumptions: kp (W) W⇤ kF  2kW W⇤ kF . For tensors, the error is upper bounded by a factor of 8 in the worst
case scenario, as described by the following proposition:

Accelerated Online Low-Rank Tensor Learning

Proposition 3 (Approximation Guarantee). Given a tensor
W ⇤ 2 RI⇥J⇥K where its i-rank is no greater than R for
all i. If tensor W 2 RI⇥J⇥K satisfies kW W ⇤ kF  ✏
and W new = TSM(W, R), then kW new W ⇤ kF  8✏.
The guarantees above rely on the low-rank assumption of
the solution before TSM. If the data is generated from a
low-rank model, the estimator can be proved to be approximately low-rank via the standard maximum likelihood estimation analysis. For many real applications, as we will
show later in Section 3, we do observe the low-rank structures from the data.
Next, we further discuss the randomized projection technique. We examine the random projection on tensor W 2
RP ⇥Q⇥M at its mode-n unfolding. For instance, the mode1 unfolding of tensor W = S ⇥1 U1 ⇥2 U2 ⇥3 U3 can be
represented as
>

W(1) = U1 S(1) (U3 ⌦ U2 ) ,
where ⌦ is the matrix Kronecker product. The matrix
U3 ⌦ U2 is also an orthonormal matrix. And when Ui is
augmented with K additional dimension to Vi , the corresponding V3 ⌦ V2 is augmented with 2K degrees of freedom. This connection essentially allows us to study the
tensor problem from the matrix perspective.
To understand the random projection, we start with cases
when K = max{P, Q, M } R and K 0 . We show that
setting K in the middle provides a trade-off between the
amount of induced bias and the reduced noise. The case
with K = max{P, Q, M } R projects the tensor to the
whole space, i.e., all the information are kept, so that the
analysis is exactly as it in Lemma 2. For K
0, the potential bias introduced by the information loss during the
projection, as analyzed in the “nearly-convexity” section,
is in the order of O(✏2 ). From rank R + K to rank R, the
projection step will introduce additional bias that is proportional to the order of O(✏2 ), but the noise reduction is
K
likely to be in the order of O( R+K
✏), which dominates the
extra bias if K is not too small comparing with R. This
also indicates that we should set K to a larger value when
R increases.
2.6. Applications for Multivariate Spatio-Temporal
Streams
Tensor provides a concise representation of multivariate
spatio-temporal data. We formulate two important tasks
of multivariate spatio-temporal stream as tensor learning
problems, which can be efficiently solved with ALTO.
O NLINE F ORECASTING
We are given access to M climate variables of P locations. At each time step t = 1, 2, · · · , T , we observe a

set of measurements Xp,t,m for p 2 {1, 2, · · · , P } and
m 2 {1, 2, · · · , M }. Suppose we also know the geographical coordinates of P locations. The task of online forecasting is to predict the value of (Xp,t+1,m , Xp,t+1,m , · · · , )
for all variables and locations given their historical measurements.
We use the classic Vector Auto-regressive (VAR) model
of lag L to describe the multivariate time series data,
where we assume the generative process as X:,t,m =
W:,:,m Xt,m + E:,t,m , for m = 1, . . . , M and t = L +
>
>
>
1, . . . , T . Here Xt,m = [X:,t
1,m , . . . , X:,t L,m ] denotes
the concatenation of L-lag historical data before time t.
We learn a model coefficient tensor W 2 RP ⇥P L⇥M to
forecast multiple variables simultaneously, where W:,:,m =
[W1m , W2m , · · · , WKm ] 2 RP ⇥LP .

In order to achieve good prediction performance, we note
two properties of spatio-temporal data: one is local smoothness, which assumes that the data in adjacent locations are
likely to be similar, and the other is shared latent structures,
i.e., the data lie in some shared latent structures across
space, time and variables. We achieve the local smoothness via a spatial Laplacian matrix, where the Laplacian
matrix is defined as L = D A. Here A is a kernel matrix constructed
by pairwise similarity and diagonal matrix
P
Di,i =
(A
i,j ) (one example of the similarity matrix
j
can be based on the geographical distances of the locations). We can achieve the global latent structures via the
low-rank constraint. Therefore the online forecasting problem can be formulated as follows.
⇢
M
P
>
c
W = argmin kXb X k2F + µ
tr(Xb:,:,m
LXb:,:,m )
W

s.t. Xb:,t,m = W:,:,m Xt,m ,

m=1

N
P

n=1

rank(W(n) )  R

>
>
>
where Xt,m = [X:,t
1,m , . . . , X:,t L,m ] denotes the concatenation of L-lag historical data before time t.

M ULTI - MODEL E NSEMBLE
The multi-model ensemble problem arises in climate modeling. In the past decades, numerous climate models have
been developed to generate large simulation data sets of future climate projections (Tebaldi & Knutti, 2007). Sophisticated physical models share similar representations of the
ocean-atmosphere and land-ice processes but have different
parameter uncertainty levels. Learning the correlation between model outputs and the actual observations can help
quantify uncertainty in climate models and prompt the design of more accurate models. The multi-model ensemble
task seeks a way to learn such correlation. It aims to combine climate model outputs into a more accurate description
of the observations. While classic methods such as model
coupling (Van den Berge et al., 2011) has been used in ex-

Accelerated Online Low-Rank Tensor Learning

isting work, we provide an alternative way to automatically
learn the ensemble model and make predictions.
Suppose we have gathered the model simulation outputs
from S models of M climate variables in P locations over
time period T . At the same time, we are given access to
the actual observations of the same variables, locations and
time. As in the forecasting problem setting, we can represent the observation measurements using a three-mode
tensor X 2 RP ⇥T ⇥M . Similarly, we encode the model
outputs with a four-mode tensor Y 2 RP ⇥T ⇥M ⇥S . Those
model outputs serve as “experts” for the climate prediction.
Incorporating those experts’ advice can reduce the uncertainty of the forecasts.
As opposed to the forecasting task, we only focus on the
current time stamp correlation between model outputs and
observations. We start with a simple linear model as X =
>
>
W:,:,m Yt,m , where Yt,m = [Y:,t,m,1
, . . . , Y:,t,m,S
]> denotes the concatenation of S model outputs at time t for
variable m, and W 2 RP ⇥P S⇥M characterizes the “importance” of various models in climate predictions. We
formulate the multi-model ensemble task as follows:
⇢
M
P
>
c
W = argmin kXb X k2F + µ
tr(Xb:,:,m
LXb:,:,m )
W

m=1

N
P
s.t. Xb:,t,m = W:,:,m Yt,m ,
rank(W(n) )  R
n=1

Where the Laplacian matrix L serves similar role as in the
forecasting task to account for the spatial proximity of observations. With change of variables, both the online forecasting and the multi-model ensemble problem can be reformulated into the low-rank tensor learning framework in
Equation 1. Details are deferred to Appendix B.2.
2.7. Discussion
A plethora of excellent work have been conducted for analysis of multivariate spatio-temporal data streams. For online forecasting task, time series models such autoregressive (AR), and autoregressive moving average (ARMA)
models fail to capture the complex shared structure of the
spatio-temporal data. Classic state-space models (Cressie
& Wikle, 2011) often require high-level domain knowledge and manual work to specify the parametric form of
the covariance functions. For multimodel ensemble task,
(Wiegerinck & Selten, 2011) learns a super model whose
dynamics are a convex combination of the individual model
components. Unfortunately, learning the parameters of
those statistical models is computationally expensive, making them infeasible for large-scale applications.
Our work has connection to the common practice of imposing low-rank constraint to capture the task relatedness
(Ando & Zhang, 2005; Argyriou et al., 2008). However, the

nature of multi-variate spatio-temporal requires us to capture the correlations not only between tasks (or features),
but also between space and time. A recent study in multilinear multitask learning (Romera-Paredes et al., 2013) describes the multi-linear commonality of the data with lowrank tensor. They consider Tucker and PARAFAC tensor
decomposition in the batch setting. They use alternating
minimization method for tensor learning, which converges
slow in practice and easily yields local optima. Another
line of work in online multitask learning (Abernethy et al.,
2007; Cavallanti et al., 2010; Saha et al., 2011) considers
a different setting where data points from different tasks
arrive one-at-a-time adversarially while in our setting, the
data from multiple tasks all arrive at the same time.

3. Experiments
We conduct experiments on synthetic data as well as real
world application data in climate and social networks. Empirical studies for tensor learning in batch settings have already been conducted in many existing work, such as (Bahadori et al., 2014). Therefore we compare our algorithm
with the following online learning baselines:
• INV: closed form solution of Exact Update for VAR
model without low-rank constraint.
• SADMM: stochastic alternating direction method of
multipliers (Ouyang et al., 2013) adapted for tensor
nuclear norm regularizer.
• ISVT: iterative singular value thresholding (Jain et al.,
2010) generalized to tensor mode-n rank constraint.
• GREEDY: greedy sequential rank-1 approximation
(Bahadori et al., 2014) for low-rank tensor learning
in batch setting.
3.1. Synthetic Datasets
We generate the synthetic data stream of 30000 time stamps
according to the VAR(2) model X:,t,m = W:,:,m Xt,m +
E:,t,m for m = 1, . . . , M and t = K + 1, . . . , T , where
parameter tensor W 2 R30⇥60⇥20 is randomly drawn from
standard normal distribution. We project W with tensor
sequential mapping of rank 2. The noise at each time is independently standard normal distributed. We set the initial
batch size to 200, the mini-batch size to 100, and repeat the
experiment for 10 times. Figure 1 compares the average parameter estimation RMSE and the run time for ALTO and
baselines over 10 random runs. We measure the run time
on a machine with a 6-core 12-thread Intel Xenon 2.67GHz
processor and 12GB memory.
As the true tensor is low-rank, low-rank tensor learning algorithms ISVT and ALTO outperform INV at each iteration in terms of parameter estimation accuracy. SADMM

Accelerated Online Low-Rank Tensor Learning

1.5
1
0.5
0
0

100
200
Iteration Number

(a) Estimation RMSE

300

600
400
200
0

SADMM
ISVT
INV
ALTO

0.2
0.18
0.16
0.14
0.12

ISVT

20

SADMM

(b) Run Time

Figure 1. (a) Average parameter estimation RMSE (b) Overall run
time comparison over 10 random runs for ALTO and baselines on
the synthetic dataset.

outperforms INV in first few iterations, but later converges
to a sub-optimal solution, since it utilizes a surrogate loss
function. We also adapt Streaming Tensor Analysis (STA)
(Sun et al., 2008) for our experiment. We observe that STA
stays at a local optimal point and the performance barely
improves after the initial iteration, which demonstrates the
benefit of adding random projection in ALTO.
3.2. Spatio-Temporal Application Datasets
We conduct experiments on real world applications of multivariate spatio-temporal streams, online forecasting and
multi-model ensemble respectively.

15

10

5

0.1
ALTO

Run Time (Sec)

RMSE

2

0.22
800

Forecasting RMSE

STA
SADMM
ISVT
INV
ALTO

Run Time (Sec)

2.5

40
60
80
Iteration Number

100

ALTO

(a) Forecasting RMSE

ISVT

SADMM

(b) Run Time

Figure 2. (a) Forecasting RMSE with respect to iteration number
(b) Per iteration run time comparison for ALTO and baselines on
Foursquare dataset.
Table 1. Forecasting RMSE (top) and overall run time (bottom)
comparison for ALTO and baselines on Foursquare and AWS
datasets with 90 % training data with respect to different lag of
the VAR model.

L AG

ALTO

1
2
3

0.1239
0.1244
0.1241

1
2
3

ISVT

SADMM

F OURSQUARE
0.1285
0.1240
0.1244
0.1234
0.1240
0.1242
AWS
0.9318 1.0055
0.9441
0.9285 0.9182
0.9447
0.9303 0.9297
0.9485

INV

G REEDY

0.1394
0.1357
0.1362

0.1246
0.1225
0.1223

1.4707
1.0853
0.9840

0.8951
0.9131
0.9166

O NLINE F ORECASTING
We use following two data sets for online forecasting:

DATA SET

ALTO

ISVT

SADMM

Foursquare The Foursquare dataset contains the users’
check-in records in the Pittsburgh area from Feb 24 to May
23, 2012, categorized by different venue types such as Art
& Entertainment, College & University, and Food. The
dataset records the number of check-ins by 121 users in
each of the 15 categories of venues over 1200 time intervals, as well as their friendship network.

F OURSQUARE
AWS

16 ( S )
20 ( S )

65 ( S )
64 ( S )

119 ( S )
126 ( S )

AWS The AWS dataset is provided by AWS Convergence
Technologies, Inc. of Germantown, MD. It consists of 76
daily maximum values of 4 variables: surface wind speed
(mph) and gust speed (mph), temperature and precipitation.
We choose 153 weather stations located on a grid laying in
the 35N-50N and 70W-90W block.
Figure 2 shows the forecasting RMSE per iteration and the
run time on the Foursquare dataset. The superior performance of SADMM, ISVT and ALTO in forecasting accuracy over INV justify the low-rank assumptions. Compared
with SADMM or ISVT, ALTO requires less computational
time while achieving more accurate solutions.
Table 1 shows the forecasting RMSE and the overall run
time with 90 % training data on both datasets for VAR

model with different lags. We present the results from
state-of-art batch algorithm GREEDY as a reference. In
general, the forecasting performance of online low-rank
tensor learning algorithms significantly outperforms INV,
and is comparable to that of batch algorithms. ALTO obtains accurate forecasting results with much faster speed.
We also vary the rank and evaluate the performance of the
ALTO algorithm. Figure 4(a) shows that both ISVT and
ALTO achieves slight increase in accuracy as the rank decreases, but the difference is marginal.
M ULTIMODEL E NSEMBLE
We evaluate our method on the multimodel ensemble task.
For observation series, we collect the monthly measurements from NCEP-DOE Reanalysis 2 (Jones, 1999). For
model outputs, 7 different model simulation data are taken
from the World Climate Research Programme’s (WCRP’s)
CMIP3 multi-model dataset and processed with CDO soft-

Accelerated Online Low-Rank Tensor Learning

ALTO
ISVT
SADMM
INV

1

0.5

1200
1000
Run Time (Sec)

Forecasting RMSE

1.5

800
600
400
200

0
0

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19

(a) RMSE

0

ALTO ISVTSADMM INV

(b) Runtime(Sec)

Figure 3. Per variable forecasting RMSE for 18 variables (a) and overall run time (b) comparison of multi-model ensemble for ALTO
and baselines using 90 % training data, with 7 different models over 20 years.

0.1255

1.2

0.1254

1.1

0.1253

Forecasting RMSE

Forecasting RMSE

ware. 1 We align the variables of observation series with
the model output series. 19 variables are selected with 252
time points from 1979 to 1999. (See Appendix B.3 for details of dataset processing ).

0.1252
0.1251
0.125
0.1249

ISVT
ALTO

0.1248
0.1247
0

5

Rank

10

(a) Foursquare

ISVT
ALTO

1
0.9
0.8
0.7
0.6

15

0.5
0

5

10
Rank

15

20

(b) Multi-model Ensemble

Figure 4. Forecasting RMSE using 90 % data with the rank value
for (a) Foursquare forecasting and (b) multi-model ensemble.

We use model outputs to predict the observation measurements. 90% of the time series are used for online training.
Figure 4(b) describes the forecasting RMSE for all variables with respect to the rank value. ALTO selects rank 13
as its optimal rank while ISVT chooses rank 7. We also
examine the forecasting error for each variable separately
using the learned model. Figure 3 shows the forecasting
RMSE for 18 of the 19 variables and overall run time in
second. ALTO is not only more accurate but also much
faster than baselines.
Multimodel ensemble accounts for the different uncertainties in climate models. This difference is partially due to
the geographical configuration of the research institutes. To
see this, we aggregate the parameters of the learned tensors
of all variables and color-code the models. Figure 5 shows
the area where a particular model is most influential (i.e.,
corresponding to the largest value of the aggregated param1
CDO 2015: Climate Data Operators.
Available at: http://www.mpimet.mpg.de/cdo

Figure 5. Climate models and their influential areas. Different
color denotes different models. The influence is computed by aggregating the model parameters.

eters). Japan Center for Climate System Research (Red)
has a dominating area in Asia. Norway Bjerknes Centre
for Climate Research (Yellow) is most influential in Europe. Other interesting findings reveal that Japan Meteorological Research Institute (Blue) is more accurate in the
south hemisphere. Russia Institute for Numerical Mathematics (Green) shows most expertise in oceans.

4. Conclusion
In this paper, we propose a simple and efficient algorithm,
namely ALTO, to accelerate the process of online low-rank
tensor learning. We introduce randomized projection technique in ALTO to overcome the local optimal issue and
provide theoretical justifications. We formulate two classical tasks in multivariate spatio-temporal data streams: online forecasting and multi-model ensemble, via the tensor
learning framework. We demonstrate that our algorithm
can produce accurate predictions and significantly reduce
the computational costs. For future work, we are interested
in examining broader applications and relaxing the assumptions of ALTO for better theoretical properties.

Accelerated Online Low-Rank Tensor Learning

5. Acknowledgement
The authors would like to thank Mohammad Taha Bahadori
for insightful discussions and the anonymous reviewers
for their constructive feedback. The authors are grateful to Tanachat Nilanon for his assistance in processing
multi-model data. The research was sponsored by NSF research grants IIS-1134990, IIS-1254206 and U.S. Defense
Advanced Research Projects Agency (DARPA) under Social Media in Strategic Communication (SMISC) program,
Agreement Number W911NF-12-1-0034. The views and
conclusions are those of the authors and should not be interpreted as representing the official policies of the funding
agency, or the U.S. Government.

References
Abernethy, Jacob, Bartlett, Peter, and Rakhlin, Alexander.
Multitask learning with expert advice. In Learning Theory, pp. 484–498. Springer, 2007.
Ando, Rie Kubota and Zhang, Tong. A framework for
learning predictive structures from multiple tasks and
unlabeled data. The Journal of Machine Learning Research, 6:1817–1853, 2005.

Guo, Weiwei, Kotsia, Irene, and Patras, Ioannis. Tensor
learning for regression. Image Processing, IEEE Transactions on, 21(2):816–827, 2012.
Hillar, Christopher J and Lim, Lek-Heng. Most tensor
problems are np-hard. Journal of the ACM (JACM), 60
(6):45, 2013.
Jain, Prateek, Meka, Raghu, and Dhillon, Inderjit S. Guaranteed rank minimization via singular value projection.
In Advances in Neural Information Processing Systems,
pp. 937–945, 2010.
Jones, Philip W. First-and second-order conservative
remapping schemes for grids in spherical coordinates.
Monthly Weather Review, 127(9):2204–2210, 1999.
Kolda, Tamara G and Bader, Brett W. Tensor decompositions and applications. SIAM review, 51(3):455–500,
2009.
Meka, Raghu, Jain, Prateek, Caramanis, Constantine, and
Dhillon, Inderjit S. Rank minimization via online learning. In Proceedings of the 25th International Conference
on Machine learning, pp. 656–663. ACM, 2008.

Argyriou, Andreas, Evgeniou, Theodoros, and Pontil, Massimiliano. Convex multi-task feature learning. Machine
Learning, 73(3):243–272, 2008.

Ouyang, Hua, He, Niao, Tran, Long, and Gray, Alexander. Stochastic alternating direction method of multipliers. In Proceedings of the 30th International Conference
on Machine Learning, pp. 80–88, 2013.

Avron, Haim, Kale, Satyen, Sindhwani, Vikas, and Kasiviswanathan, Shiva P. Efficient and practical stochastic subgradient descent for nuclear norm regularization.
In Proceedings of the 29th International Conference on
Machine Learning (ICML-12), pp. 1231–1238, 2012.

Romera-Paredes, Bernardino, Aung, Hane, BianchiBerthouze, Nadia, and Pontil, Massimiliano. Multilinear
multitask learning. In Proceedings of The 30th International Conference on Machine Learning, pp. 1444–1452,
2013.

Bahadori, Mohammad Taha, Yu, Qi Rose, and Liu, Yan.
Fast multivariate spatio-temporal analysis via low rank
tensor learning. In Advances in Neural Information Processing Systems, pp. 3491–3499, 2014.

Saha, Avishek, Rai, Piyush, Venkatasubramanian, Suresh,
and Daume, Hal. Online learning of multiple tasks and
their relationships. In International Conference on Artificial Intelligence and Statistics, pp. 643–651, 2011.

Brand, Matthew. Incremental singular value decomposition of uncertain data with missing values. In Computer
VisionECCV 2002, pp. 707–720. Springer, 2002.

Shalit, Uri, Weinshall, Daphna, and Chechik, Gal. Online learning in the manifold of low-rank matrices. In
Advances in neural information processing systems, pp.
2128–2136, 2010.

Cavallanti, Giovanni, Cesa-Bianchi, Nicolo, and Gentile,
Claudio. Linear algorithms for online multitask classification. The Journal of Machine Learning Research, 11:
2901–2934, 2010.
Cressie, Noel and Wikle, Christopher K. Statistics for
spatio-temporal data. John Wiley & Sons, 2011.
De Lathauwer, Lieven, De Moor, Bart, and Vandewalle,
Joos. A multilinear singular value decomposition. SIAM
journal on Matrix Analysis and Applications, 21(4):
1253–1278, 2000.

Sun, Jimeng, Tao, Dacheng, and Faloutsos, Christos. Beyond streams and graphs: dynamic tensor analysis. In
Proceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pp. 374–383. ACM, 2006.
Sun, Jimeng, Tao, Dacheng, Papadimitriou, Spiros, Yu,
Philip S, and Faloutsos, Christos. Incremental tensor
analysis: Theory and applications. ACM Transactions
on Knowledge Discovery from Data (TKDD), 2(3):11,
2008.

Accelerated Online Low-Rank Tensor Learning

Tebaldi, Claudia and Knutti, Reto. The use of the
multi-model ensemble in probabilistic climate projections. Philosophical Transactions of the Royal Society A:
Mathematical, Physical and Engineering Sciences, 365
(1857):2053–2075, 2007.
Van den Berge, LA, Selten, FM, Wiegerinck, WAJJ, and
Duane, GS. A multi-model ensemble method that combines imperfect models through learning. Earth System
Dynamics, 2:161–177, 2011.
Wedin, Per-Åke. Perturbation bounds in connection with
singular value decomposition. BIT Numerical Mathematics, 12(1):99–111, 1972.
Wiegerinck, W and Selten, F. Supermodeling: Combining
imperfect models through learning. 2011.
Woodbury, Max A. Inverting modified matrices. Memorandum report, 42:106, 1950.
Zhang, Hongyang, Lin, Zhouchen, and Zhang, Chao. A
counterexample for the validity of using nuclear norm
as a convex surrogate of rank. In Machine Learning
and Knowledge Discovery in Databases, pp. 226–241.
Springer, 2013.
Zhou, Hua, Li, Lexin, and Zhu, Hongtu. Tensor regression
with applications in neuroimaging data analysis. Journal
of the American Statistical Association, 108(502):540–
552, 2013.

