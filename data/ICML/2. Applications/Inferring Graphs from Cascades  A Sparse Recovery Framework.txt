Inferring Graphs from Cascades: A Sparse Recovery Framework

JEANPOUGETABADIE @ G . HARVARD . EDU

Jean Pouget-Abadie
Harvard University

THOREL @ SEAS . HARVARD . EDU

Thibaut Horel
Harvard University

Abstract
In the Network Inference problem, one seeks to
recover the edges of an unknown graph from the
observations of cascades propagating over this
graph. In this paper, we approach this problem from the sparse recovery perspective. We
introduce a general model of cascades, including the voter model and the independent cascade
model, for which we provide the first algorithm
which recovers the graph’s edges with high probability and O(s log m) measurements where s is
the maximum degree of the graph and m is the
number of nodes. Furthermore, we show that
our algorithm also recovers the edge weights (the
parameters of the diffusion process) and is robust in the context of approximate sparsity. Finally we prove an almost matching lower bound
of Ω(s log m
s ) and validate our approach empirically on synthetic graphs.

1. Introduction
Graphs have been extensively studied for their propagative abilities: connectivity, routing, gossip algorithms, etc.
A diffusion process taking place over a graph provides
valuable information about the presence and weights of its
edges. Influence cascades are a specific type of diffusion
processes in which a particular infectious behavior spreads
over the nodes of the graph. By only observing the “infection times” of the nodes in the graph, one might hope
to recover the underlying graph and the parameters of the
cascade model. This problem is known in the literature as
the Network Inference problem.
More precisely, solving the Network Inference problem
involves designing an algorithm taking as input a set of
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).

observed cascades (realisations of the diffusion process)
and recovers with high probability a large fraction of the
graph’s edges. The goal is then to understand the relationship between the number of observations, the probability
of success, and the accuracy of the reconstruction.
The Network Inference problem can be decomposed and
analyzed “node-by-node”. Thus, we will focus on a single node of degree s and discuss how to identify its parents among the m nodes of the graph. Prior work has
shown that the required number of observed cascades is
O(poly(s) log m) (Netrapalli & Sanghavi, 2012; Abrahao
et al., 2013).
A more recent line of research (Daneshmand et al., 2014)
has focused on applying advances in sparse recovery to the
network inference problem. Indeed, the graph can be interpreted as a “sparse signal” measured through influence
cascades and then recovered. The challenge is that influence cascade models typically lead to non-linear inverse
problems and the measurements (the state of the nodes at
different time steps) are usually correlated. The sparse recovery literature suggests that Ω(s log m
s ) cascade observations should be sufficient to recover the graph (Donoho,
2006; Candes & Tao, 2006). However, the best known upper bound to this day is O(s2 log m) (Netrapalli & Sanghavi, 2012; Daneshmand et al., 2014)
The contributions of this paper are the following:
• we formulate the Graph Inference problem in the context of discrete-time influence cascades as a sparse recovery problem for a specific type of Generalized Linear Model. This formulation notably encompasses the
well-studied Independent Cascade Model and Voter
Model.
• we give an algorithm which recovers the graph’s edges
using O(s log m) cascades. Furthermore, we show
that our algorithm is also able to efficiently recover the
edge weights (the parameters of the influence model)
up to an additive error term,
• we show that our algorithm is robust in cases where
the signal to recover is approximately s-sparse by

Inferring Graphs from Cascades: A Sparse Recovery Framework

proving guarantees in the stable recovery setting.
• we provide an almost tight lower bound of Ω(s log
observations required for sparse recovery.

m
s )

The organization of the paper is as follows: we conclude
the introduction by a survey of the related work. In Section 2 we present our model of Generalized Linear Cascades and the associated sparse recovery formulation. Its
theoretical guarantees are presented for various recovery
settings in Section 3. The lower bound is presented in Section 4. Finally, we conclude with experiments in Section 5.
Related Work The study of edge prediction in graphs
has been an active field of research for over a
decade (Liben-Nowell & Kleinberg, 2008; Leskovec et al.,
2007; Adar & Adamic, 2005). (Gomez Rodriguez et al.,
2010) introduced the N ETINF algorithm, which approximates the likelihood of cascades represented as a continuous process. The algorithm was improved in later
work (Gomez-Rodriguez et al., 2011), but is not known to
have any theoretical guarantees beside empirical validation
on synthetic networks. Netrapalli & Sanghavi (2012) studied the discrete-time version of the independent cascade
model and obtained the first O(s2 log m) recovery guarantee on general networks. The algorithm is based on a likelihood function similar to the one we propose, without the
ℓ1 -norm penalty. Their analysis depends on a correlation
decay assumption, which limits the number of new infections at every step. In this setting, they show a lower bound
of the number of cascades needed for support recovery with
constant probability of the order Ω(s log(m/s)). They also
suggest a G REEDY algorithm, which achieves a O(s log m)
guarantee in the case of tree graphs. The work of (Abrahao et al., 2013) studies the same continuous-model framework as (Gomez Rodriguez et al., 2010) and obtains an
O(s9 log2 s log m) support recovery algorithm, without the
correlation decay assumption. (Du et al., 2013) propose a
similar algorithm to ours for recovering the weights of the
graph under a continuous-time independent cascade model,
without proving theoretical guarantees.
Closest to this work is a recent paper by Daneshmand et al.
(2014), wherein the authors consider a ℓ1 -regularized objective function. They adapt standard results from sparse
recovery to obtain a recovery bound of O(s3 log m) under
an irrepresentability condition (Zhao & Yu, 2006). Under
stronger assumptions, they match the (Netrapalli & Sanghavi, 2012) bound of O(s2 log m), by exploiting similar
properties of the convex program’s KKT conditions. In
contrast, our work studies discrete-time diffusion processes
including the Independent Cascade model under weaker assumptions. Furthermore, we analyze both the recovery of
the graph’s edges and the estimation of the model’s parameters, and achieve close to optimal bounds.

The work of (Du et al., 2014) is slightly orthogonal to ours
since they suggest learning the influence function, rather
than the parameters of the network directly.

2. Model
We consider a graph G = (V, E, Θ), where Θ is a |V | ×
|V | matrix of parameters describing the edge weights of G.
Intuitively, Θi,j captures the “influence” of node i on node
j. Let m ≡ |V |. For each node j, let θj be the j th column
vector of Θ. A discrete-time Cascade model is a Markov
V
process over a finite state space {0, 1, . . . , K − 1} with
the following properties:
1. Conditioned on the previous time step, the transition
events between two states in {0, 1, . . . , K − 1} for
each i ∈ V are mutually independent across i ∈ V .
2. Of the K possible states, there exists a contagious
state such that all transition probabilities of the
Markov process can be expressed as a function of the
graph parameters Θ and the set of “contagious nodes”
at the previous time step.
V

3. The initial probability over {0, 1, . . . , K − 1} is
such that all nodes can eventually reach a contagious
state with non-zero probability. The “contagious”
nodes at t = 0 are called source nodes.
In other words, a cascade model describes a diffusion process where a set of contagious nodes “influence” other
nodes in the graph to become contagious. An influence cascade is a realisation of this random process, i.e. the successive states of the nodes in graph G. Note that both the “single source” assumption made in (Daneshmand et al., 2014)
and (Abrahao et al., 2013) as well as the “uniformly chosen
source set” assumption made in (Netrapalli & Sanghavi,
2012) verify condition 3. Also note that the multiple-source
node assumption does not reduce to the single-source assumption, even under the assumption that cascades do not
overlap. Imagining for example two cascades starting from
two different nodes; since we do not observe which node
propagated the contagion to which node, we cannot attribute an infected node to either cascade and treat the problem as two independent cascades.
In the context of Network Inference, (Netrapalli & Sanghavi, 2012) focus on the well-known discrete-time independent cascade model recalled below, which (Abrahao et al.,
2013) and (Daneshmand et al., 2014) generalize to continuous time. We extend the independent cascade model in
a different direction by considering a more general class
of transition probabilities while staying in the discrete-time
setting. We observe that despite their obvious differences,
both the independent cascade and the voter models make

Inferring Graphs from Cascades: A Sparse Recovery Framework

the network inference problem similar to the standard generalized linear model inference problem. In fact, we define
a class of diffusion processes for which this is true: the
Generalized Linear Cascade Models. The linear threshold
model is a special case and is discussed in Section 6.
2.1. Generalized Linear Cascade Models
Let susceptible denote any state which can become contagious at the next time step with a non-zero probability. We
draw inspiration from generalized linear models to introduce Generalized Linear Cascades:
Definition 1. Let X t be the indicator variable of “contagious nodes” at time step t. A generalized linear cascade
model is a cascade model such that for each susceptible
node j in state s at time step t, the probability of j becoming “contagious” at time step t + 1 conditioned on X t is a
Bernoulli variable of parameter f (θj · X t ):
P(Xjt+1 = 1|X t ) = f (θj · X t )

(1)

where f : R → [0, 1]
In other words, each generalized linear cascade provides, for each node j ∈ V a series of measurements
(X t , Xjt+1 )t∈T sampled from a generalized linear model.
j

Note also that E[Xit+1 | X t ] = f (θi · X t ). As such, f can
be interpreted as the inverse link function of our generalized linear cascade model.

Defining Θi,j ≡ log( 1−p1 i,j ), this can be rewritten as:
m
Y


t
e−Θi,j Xi
P Xjt+1 = 1 | X t = 1 −

(IC)

i=1

= 1 − e−Θj ·X

t

Therefore, the independent cascade model is a Generalized
Linear Cascade model with inverse link function f : z 7→
1 − e−z . Note that to write the Independent Cascade Model
as a Generalized Linear Cascade Model, we had to introduce the change of variable Θi,j = log( 1−p1 i,j ). The recovery results in Section 3 pertain to the Θj parameters.
Fortunately, the following lemma shows that the recovery
error on Θj is an upper bound on the error on the original
pj parameters.
Lemma 1. kθ̂ − θ∗ k2 ≥ kp̂ − p∗ k2 .
2.2.2. T HE L INEAR VOTER M ODEL
In the Linear Voter Model, nodes can be either red or blue.
Without loss of generality, we can suppose that the blue
nodes are contagious. The
P parameters of the graph are normalized such that ∀i,
j Θi,j = 1. Each round, every
node j independently chooses one of its neighbors with
probability Θi,j and adopts their color. The cascades stops
at a fixed horizon time T or if all nodes are of the same
color. If we denote by X t the indicator variable of the set
of blue nodes at time step t, then we have:
m

 X
Θi,j Xit = Θj · X t
P Xjt+1 = 1|X t =

(V)

i=1

2.2. Examples
2.2.1. I NDEPENDENT C ASCADE M ODEL
In the independent cascade model, nodes can be either susceptible, contagious or immune. At t = 0, all source nodes
are “contagious” and all remaining nodes are “susceptible”.
At each time step t, for each edge (i, j) where j is susceptible and i is contagious, i attempts to infect j with probability pi,j ∈ [0, 1]; the infection attempts are mutually independent. If i succeeds, j will become contagious at time
step t + 1. Regardless of i’s success, node i will be immune
at time t + 1, such that nodes stay contagious for only one
time step. The cascade process terminates when no contagious nodes remain.
If we denote by X t the indicator variable of the set of contagious nodes at time step t, then if j is susceptible at time
step t + 1, we have:
m
Y


Xt
(1 − pi,j ) i .
P Xjt+1 = 1 | X t = 1 −
i=1

Thus, the linear voter model is a Generalized Linear Cascade model with inverse link function f : z 7→ z.
2.2.3. D ISCRETIZATION OF C ONTINUOUS M ODEL
Another motivation for the Generalized Linear Cascade
model is that it captures the time-discretized formulation of the well-studied continuous-time independent cascade model with exponential transmission function (CICE)
of (Gomez Rodriguez et al., 2010; Abrahao et al., 2013;
Daneshmand et al., 2014). Assume that the temporal resolution of the discretization is ε, i.e. all nodes whose (continuous) infection time is within the interval [kε, (k + 1)ε)
are considered infected at (discrete) time step k. Let X k
be the indicator vector of the set of nodes ‘infected’ before
or during the k th time interval. Note that contrary to the
discrete-time independent cascade model, Xjk = 1 =⇒
Xjk+1 = 1, that is, there is no immune state and nodes
remain contagious forever.
Let Exp(p) be an exponentially-distributed random variable of parameter p and let Θi,j be the rate of transmis-

Inferring Graphs from Cascades: A Sparse Recovery Framework

where λ is the regularization factor which helps prevent
overfitting and controls the sparsity of the solution.

Figure 1: Illustration of the sparse-recovery approach. Our
objective is to recover the unknown weight vector θj for
each node j. We observe a Bernoulli realization whose parameters are given by applying f to the matrix-vector product, where the measurement matrix encodes which nodes
are “contagious” at each time step.
sion along directed edge (i, j) in the CICE model. By the
memoryless property of the exponential, if Xjk 6= 1:

The generalized linear cascade model is decomposable in
the following sense: given Definition 1, the log-likelihood
can be written as the sum of m terms, each term i ∈
{1, . . . , m} only depending on θi . Since this is equally
true for kΘk1 , each column θi of Θ can be estimated by
a separate optimization program:
θ̂i ∈ argmax Li (θi | x1 , . . . , xn ) − λkθi k1

where we denote by Ti the time steps at which node i is
susceptible and:
Li (θi | x1 , . . . , xn ) =

P(Xjk+1 = 1|X k ) = P( min Exp(Θi,j ) ≤ ǫ)
= P(Exp(

Θi,j Xit ) ≤ ǫ) = 1 − e−ǫΘj ·X

+ (1 − xt+1
) log 1 − f (θi · xt )
i
t

i=1

Therefore, the ǫ-discretized CICE-induced process is a
Generalized Linear Cascade model with inverse link function f : z 7→ 1 − e−ǫ·z .
2.2.4. L OGISTIC C ASCADES
“Logistic cascades” is the specific case where the inverse
link function is given by the logistic function f (z) =
1/(1 + e−z+t ). Intuitively, this captures the idea that there
is a threshold t such that when the sum of the parameters of
the infected parents of a node is larger than the threshold,
the probability of getting infected is close to one. This is
a smooth approximation of the hard threshold rule of the
Linear Threshold Model (Kempe et al., 2003). As we will
see later in the analysis, for logistic cascades, the graph inference problem becomes a linear inverse problem.
2.3. Maximum Likelihood Estimation
Inferring the model parameter Θ from observed influence
cascades is the central question of the present work. Recovering the edges in E from observed influence cascades is
a well-identified problem known as the Network Inference
problem. However, recovering the influence parameters is
no less important. In this work we focus on recovering Θ,
noting that the set of edges E can then be recovered through
the following equivalence: (i, j) ∈ E ⇔ Θi,j 6= 0
Given observations (x1 , . . . , xn ) of a cascade model,
we can recover Θ via Maximum Likelihood Estimation
(MLE). Denoting by L the log-likelihood function, we consider the following ℓ1 -regularized MLE problem:
Θ̂ ∈ argmax
Θ

1
L(Θ | x1 , . . . , xn ) − λkΘk1
n

1 X t+1
xi log f (θi · xt )
|Ti |
t∈Ti

i∈N (j)

m
X

(2)

θ



In the case of the voter model, the measurements include all
time steps until we reach the time horizon T or the graph
coalesces to a single state. For the independent cascade
model, the measurements include all time steps until node
i becomes contagious, after which its behavior is deterministic. Contrary to prior work, our results depend on the
number of measurements and not the number of cascades.
Regularity assumptions To solve program (2) efficiently, we would like it to be convex. A sufficient condition is to assume that Li is concave, which is the case if f
and (1 − f ) are both log-concave. Remember that a twicedifferentiable function f is log-concave iff. f ′′ f ≤ f ′2 .
It is easy to verify this property for f and (1 − f ) in the
Independent Cascade Model and Voter Model.
Furthermore, the data-dependent bounds in Section 3.1 will
require the following regularity assumption on the inverse
link function f : there exists α ∈ (0, 1) such that

	
1
max |(log f )′ (zx )|, |(log(1 − f ))′ (zx )| ≤
α

(LF)

for all zx ≡ θ∗ · x such that f (zx ) ∈
/ {0, 1}.
′

′

(z)
f (z)
1
In the voter model, ff (z)
= z1 and (1−f
)(z) = 1−z . Hence
(LF) will hold as soon as α ≤ Θi,j ≤ 1 − α for all (i, j) ∈
E which is always satisfied for some α for non-isolated
′
(z)
nodes. In the Independent Cascade Model, ff (z)
= ez1−1
′

f (z)
and (1−f
)(z) = 1. Hence (LF) holds as soon as pi,j ≥ α for
all (i, j) ∈ E which is always satisfied for some α ∈ (0, 1).

For the data-independent bound of Proposition 1, we will
require the following additional regularity assumption:

	
1
(LF2)
max |(log f )′′ (zx )|, |(log(1 − f ))′′ (zx )| ≤
α

Inferring Graphs from Cascades: A Sparse Recovery Framework

for some α ∈ (0, 1) and for all zx ≡ θ∗ ·x such that f (zx ) ∈
/
{0, 1}. It is again easy to see that this condition is verified
for the Independent Cascade Model and the Voter model
for the same α ∈ (0, 1).
Convex constraints The voter model is only defined
when Θi,j ∈ (0, 1) for all (i, j) ∈ E. Similarly the independent cascade model is only defined when Θi,j > 0.
Because the likelihood function Li is equal to −∞ when
the parameters are outside of the domain of definition of
the models, these contraints do not need to appear explicitly in the optimization program.
In
P the specific case of the voter model, the constraint
j Θi,j = 1 will not necessarily be verified by the estimator obtained in (2). In some applications, the experimenter might not need this constraint to be verified, in
which case the results in Section 3 still give a bound on
the recovery error. If this constraint needs to be satisfied,
then by Lagrangian
duality,
there exists a λ ∈ R such that

P
adding λ
j θj − 1 to the objective function of (2) enforces the constraint. Then, it suffices to apply the results
of Section 3 to the augmented objective to obtain the same
recovery guarantees. Note that the added term is linear and
will easily satisfy all the required regularity assumptions.

3. Results
In this section, we apply the sparse recovery framework to
analyze under which assumptions our program (2) recovers
the true parameter θi of the cascade model. Furthermore,
if we can estimate θi to a sufficiently good accuracy, it is
then possible to recover the support of θi by simple thresholding, which provides a solution to the standard Network
Inference problem.
We will first give results in the exactly sparse setting in
which θi has a support of size exactly s. We will then relax
this sparsity constraint and give results in the stable recovery setting where θi is approximately s-sparse.
As mentioned in Section 2.3, the maximum likelihood estimation program is decomposable. We will henceforth focus on a single node i ∈ V and omit the subscript i in the
notations when there is no ambiguity. The recovery problem is now the one of estimating a single vector θ∗ from a
set T of observations. We will write n ≡ |T |.
3.1. Main Theorem
In this section, we analyze the case where θ∗ is exactly
sparse. We write S ≡ supp(θ∗ ) and s = |S|. Recall,
that θi is the vector of weights for all edges directed at the
node we are solving for. In other words, S is the set of all
nodes susceptible to influence node i, also referred to as its
parents. Our main theorem will rely on the now standard

restricted eigenvalue condition introduced by (Bickel et al.,
2009a).
Definition 2. Let Σ ∈ Sm (R) be a real symmetric matrix
and S be a subset of {1, . . . , m}. Defining C(S) ≡ {X ∈
Rm : kXS c k1 ≤ 3kXS k1 }. We say that Σ satisfies the
(S, γ)-restricted eigenvalue condition iff:
∀X ∈ C(S), X T ΣX ≥ γkXk22

(RE)

A discussion of the (S, γ)-(RE) assumption in the context
of generalized linear cascade models can be found in Section 3.3. In our setting we require that the (RE)-condition
holds for the Hessian of the log-likelihood function L: it essentially captures the fact that the binary vectors of the set
of active nodes (i.e the measurements) are not too collinear.
Theorem 1. Assume the Hessian ∇2 L(θ∗ ) satisfies the
(S, γ)-(RE) for some γ > 0 and that (LF) holds for some
α > 0.qFor any δ ∈ (0, 1), let θ̂ be the solution of (2) with
λ≡2

log m
,
αn1−δ

then:

6
kθ̂ − θ k2 ≤
γ
∗

r

s log m
αn1−δ

w.p. 1 −

1
enδ log m

(3)

Note that we have expressed the convergence rate in the
number of measurements n, which is different from the
number of cascades. For example, in the case of the voter
model with horizon time T and for N cascades, we can
expect a number of measurements proportional to N × T .
Theorem 1 is a consequence of Theorem 1 in (Negahban
et al., 2012) which gives a bound on the convergence rate
of regularized estimators. We state their theorem in the
context of ℓ1 regularization in Lemma 2.
Lemma 2. Let C(S) ≡ {∆ ∈ Rm | k∆S k1 ≤ 3k∆S c k1 }.
Suppose that:
∀∆ ∈ C(S), L(θ∗ + ∆) − L(θ∗ )
− ∇L(θ∗ ) · ∆ ≥ κL k∆k22 − τL2 (θ∗ )

(4)

for some κL > 0 and function τL . Finally suppose that
λ ≥ 2k∇L(θ∗ )k∞ , then if θ̂λ is the solution of (2):
kθ̂λ − θ∗ k22 ≤ 9

λ
λ2 s
+ 2 2τL2 (θ∗ )
κL
κL

To prove Theorem 1, we apply Lemma 2 with τL = 0.
Since L is twice differentiable and convex, assumption (4)
with κL = γ2 is implied by the (RE)-condition. For a good
convergence rate, we must find the smallest possible value
of λ such that λ ≥ 2k∇Lθ∗ k∞ . The upper bound on the
ℓ∞ norm of ∇L(θ∗ ) is given by Lemma 3.

Inferring Graphs from Cascades: A Sparse Recovery Framework

Lemma 3. Assume (LF) holds for some α > 0. For any
δ ∈ (0, 1):
r
log m
1
∗
k∇L(θ )k∞ ≤ 2
w.p. 1 − nδ log m
1−δ
αn
e

q
log m
solving (2) for λ ≡ 2 αn
1−δ we have:
s
r
s log m
3 s log m
∗
∗
+ 4 4 4 1−δ kθ∗ − θ⌊s⌋
k1
kθ̂ − θ k2 ≤
γ αn1−δ
γ αn

The proof of Lemma 3 relies crucially on AzumaHoeffding’s inequality, which allows us to handle correlated observations. This departs from the usual assumptions made in sparse recovery settings, that the measurements are independent from one another. We now show
how to use Theorem 1 to recover the support of θ∗ , that is,
to solve the Network Inference problem.

As in Corollary 1, an edge recovery guarantee can be derived from Theorem 2 in the case of approximate sparsity.

Corollary 1. Under the same assumptions as Theorem 1,
let Ŝη ≡ {j ∈ {1, . . . , m} : θ̂j > η} for η > 0. For
∗
0 < ǫ < η, let Sη+ǫ
≡ {i ∈ {1, . . . , m} : θi∗ > η + ǫ} be
the set of all true ‘strong’ parents. Suppose the number of
m
measurements verifies: n > 9sαγlog
2 ǫ2 . Then with probability
1
∗
1− m
, Sη+ǫ
⊆ Ŝη ⊆ S ∗ . In other words we recover all
‘strong’ parents and no ‘false’ parents.

Assuming we know a lower bound α on Θi,j , Corollary 1
can be applied to the Network Inference problem in the fol∗
= S
lowing manner: pick ǫ = η2 and η = α3 , then Sη+ǫ


s log m
provided that n = Ω α3 γ 2 . That is, the support of θ∗
can be found by thresholding θ̂ to the level η.

3.3. Restricted Eigenvalue Condition
There exists a large class of sufficient conditions under
which sparse recovery is achievable in the context of regularized estimation (van de Geer & Bühlmann, 2009). The
restricted eigenvalue condition, introduced in (Bickel et al.,
2009b), is one of the weakest such assumption. It can be
interpreted as a restricted form of non-degeneracy. Since
we apply it to the Hessian of the log-likelihood function
∇2 L(θ), it essentially reduces to a form of restricted strong
convexity, that Lemma 2 ultimately relies on.
Observe that the Hessian of L can be seen as a re-weighted
Gram matrix of the observations:

1 X t t T t+1 f ′′ f − f ′2 ∗ t
∇2 L(θ∗ ) =
x (x ) xi
(θ · x )
|T |
f2
t∈T

f ′′ (1 − f ) + f ′2 ∗ t
(θ
·
x
)
− (1 − xt+1
)
i
(1 − f )2

In practice, exact sparsity is rarely verified. For social networks in particular, it is more realistic to assume that each
node has few “strong” parents’ and many “weak” parents.
In other words, even if θ∗ is not exactly s-sparse, it can be
well approximated by s-sparse vectors.

If f and (1 − f ) are c-strictly log-convex for c > 0, then
min ((log f )′′ , (log(1 − f ))′′ ) ≥ c. This implies that the
(S, γ)-(RE) condition in Theorem 1 and Theorem 2 reduces to a condition
on the Gram matrix of the observations
P
X T X = |T1 | t∈T xt (xt )T for γ ′ ≡ γ · c.

Rather than obtaining an impossibility result, we show that
the bounds obtained in Section 3.1 degrade gracefully in
∗
this setting. Formally, let θ⌊s⌋
∈ argminkθk0 ≤s kθ − θ∗ k1
be the best s-approximation to θ∗ . Then we pay a cost pro∗
portional to kθ∗ −θ⌊s⌋
k1 for recovering the weights of nonexactly sparse vectors. This cost is simply the “tail” of θ∗ :
the sum of the m − s smallest coordinates of θ∗ . We recover the results of Section 3.1 in the limit of exact sparsity. These results are formalized in the following theorem,
which is also a consequence of Theorem 1 in (Negahban
et al., 2012).

The (RE)-condition has the following concentration property: if it holds for the expected Hessian matrix
E[∇2 L(θ∗ )], then it holds for the finite sample Hessian matrix ∇2 L(θ∗ ) with high probability.

3.2. Approximate Sparsity

Theorem 2. Suppose the (RE) assumption holds for the
m
kθ∗ k1 on the folHessian ∇2 f (θ∗ ) and τL (θ∗ ) = κ2 log
n
lowing set:
∗
k1 }
C ′ ≡{X ∈ Rp : kXS c k1 ≤ 3kXS k1 + 4kθ∗ − θ⌊s⌋

∩ {kXk1 ≤ 1}
If the number of measurements n ≥

64κ2
γ s log m,

then by

(RE) with high probability The Generalized Linear
Cascade model yields a probability distribution over the observed sets of infected nodes (xt )t∈T . It is then natural to
ask whether the restricted eigenvalue condition is likely to
occur under this probabilistic model. Several recent papers
show that large classes of correlated designs obey the restricted eigenvalue property with high probability (Raskutti
et al., 2010; Rudelson & Zhou, 2013).

Therefore, under an assumption which only involves the
probabilistic model and not the actual observations, we can
obtain the same conclusion as in Theorem 1:
Proposition 1. Suppose E[∇2 L(θ∗ )] verifies the (S, γ)(RE) condition and assume (LF) and (LF2). For δ > 0,
1
if n1−δ ≥ 28γα
s2 log m, then ∇2 L(θ∗ ) verifies the (S, γ2 )(RE) condition, w.p ≥ 1 − e−n

δ

log m

.

Inferring Graphs from Cascades: A Sparse Recovery Framework

Observe that the number of measurements required in
Proposition 1 is now quadratic in s. If we only keep
the first measurement from each cascade, which are independent, we can apply Theorem 1.8 from (Rudelson &
Zhou, 2013), lowering the number of required cascades to
s log m log3 (s log m).
If f and (1 − f ) are strictly log-convex, then the previous
observations show that the quantity E[∇2 L(θ∗ )] in Proposition 1 can be replaced by the expected Gram matrix:
A ≡ E[X T X]. This matrix A has a natural interpretation:
the entry ai,j is the probability that node i and node j are
infected at the same time during a cascade. In particular,
the diagonal term ai,i is simply the probability that node i
is infected during a cascade.

4. A Lower Bound
In (Netrapalli & Sanghavi, 2012), the authors explicitate
a lower bound of Ω(s log m
s ) on the number of cascades
necessary to achieve good support recovery with constant
probability under a correlation decay assumption. In this
section, we will consider the stable sparse recovery setting of Section 3.2. Our goal is to obtain an informationtheoretic lower bound on the number of measurements necessary to approximately recover the parameter θ∗ of a cascade model from observed cascades. Similar lower bounds
were obtained for sparse linear inverse problems in (Price
& Woodruff, 2011; 2012; Ba et al., 2011).
Theorem 3. Let us consider a cascade model of the form
(1) and a recovery algorithm A which takes as input n random cascade measurements and outputs θ̂ such that with
probability δ > 21 (over the measurements):
kθ̂ − θ∗ k2 ≤ C min kθ − θ∗ k2
kθk0 ≤s

(5)

where θ∗ is the true parameter of the cascade model. Then
n = Ω(s log m
s / log C).
This theorem should be contrasted with Theorem 2: up to
an additive s log s factor, the number of measurements required by our algorithm is tight. The proof of Theorem 3
follows an approach similar to (Price & Woodruff, 2012).
We present a sketch of the proof in the Appendix and refer
the reader to their paper for more details.

5. Experiments
In this section, we validate empirically the results and assumptions of Section 3 for varying levels of sparsity and
different initializations of parameters (n, m, λ, pinit ), where
pinit is the initial probability of a node being a source node.
We compare our algorithm to two different state-of-the-art
algorithms: GREEDY and MLE from (Netrapalli & Sanghavi, 2012). As an extra benchmark, we also introduce

a new algorithm LASSO, which approximates our SPARSE
MLE algorithm.
Experimental setup We evaluate the performance of the
algorithms on synthetic graphs, chosen for their similarity
to real social networks. We therefore consider a WattsStrogatz graph (300 nodes, 4500 edges) (Watts & Strogatz, 1998), a Barabasi-Albert graph (300 nodes, 16200
edges) (Albert & Barabási, 2001), a Holme-Kim power law
graph (200 nodes, 9772 edges) (Holme & Kim, 2002), and
the recently introduced Kronecker graph (256 nodes, 10000
edges) (Leskovec et al., 2010). Undirected graphs are converted to directed graphs by doubling the edges.
For every reported data point, we sample edge weights
and generate n cascades from the (IC) model for n ∈
{100, 500, 1000, 2000, 5000}. We compare for each algorithm the estimated graph Ĝ with G. The initial probability
of a node being a source is fixed to 0.05, i.e. an average of
15 nodes source nodes per cascades for all experiments, except for Figure (f). All edge weights are chosen uniformly
in the interval [0.2, 0.7], except when testing for approximately sparse graphs (see paragraph on robustness). Adjusting for the variance of our experiments, all data points
are reported with at most a ±1 error margin.
The paramp
eter λ is chosen to be of the order O( log m/(αn)). We
report our results as a function of the number of cascades
and not the number of measurements: in practice, very few
cascades have depth greater than 3.
Benchmarks We compare our SPARSE MLE algorithm
to 3 benchmarks: GREEDY and MLE from (Netrapalli &
Sanghavi, 2012) and LASSO. The MLE algorithm is a
maximum-likelihood estimator without ℓ1 -norm penalization. GREEDY is an iterative algorithm. We introduced the
LASSO algorithm in our experiments to achieve faster computation time:
θ̂i ∈ arg min
θ

X

|f (θi · xt ) − xt+1
|2 + λkθi k1
i

t∈T

L ASSO has the merit of being both easier and faster to optimize numerically than the other convex-optimization based
algorithms. It approximates the SPARSE MLE algorithm by
making the assumption that the observations xt+1
are of the
i
t
form: xt+1
=
f
(θ
·
x
)
+
ǫ,
where
ǫ
is
random
white
noise.
i
i
This is not valid in theory since ǫ depends on f (θi · xt ),
however the approximation is validated in practice.
We did not benchmark against other known algorithms
(NETRATE (Gomez-Rodriguez et al., 2011) and FIRST
EDGE (Abrahao et al., 2013)) due to the discrete-time assumption. These algorithms also suppose a single-source
model, whereas SPARSE MLE, MLE, and GREEDY do not.
Learning the graph in the case of a multi-source cascade

Inferring Graphs from Cascades: A Sparse Recovery Framework

Acknowledgments
We would like to thank Yaron Singer, David Parkes, Jelani
Nelson, Edoardo Airoldi and Or Sheffet for helpful discussions. We are also grateful to the anonymous reviewers for
their insightful feedback and suggestions.

References
Abrahao, Bruno D., Chierichetti, Flavio, Kleinberg,
Robert, and Panconesi, Alessandro. Trace complexity
of network inference. In The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, KDD 2013, Chicago, IL, USA, August 11-14,
2013, pp. 491–499, 2013.
Adar, Eytan and Adamic, Lada A. Tracking information
epidemics in blogspace. In 2005 IEEE / WIC / ACM International Conference on Web Intelligence (WI 2005),
19-22 September 2005, Compiegne, France, pp. 207–
214, 2005.
Albert, Réka and Barabási, Albert-László.
Statistical mechanics of complex networks. CoRR, condmat/0106096, 2001.

Du, Nan, Song, Le, Woo, Hyenkyun, and Zha, Hongyuan.
Uncover topic-sensitive information diffusion networks.
In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, pp. 229–
237, 2013.
Du, Nan, Liang, Yingyu, Balcan, Maria, and Song, Le. Influence function learning in information diffusion networks. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 2016–2024,
2014.
Gomez Rodriguez, Manuel, Leskovec, Jure, and Krause,
Andreas. Inferring networks of diffusion and influence.
In Proceedings of the 16th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
KDD ’10, pp. 1019–1028, New York, NY, USA, 2010.
ACM. ISBN 978-1-4503-0055-1.
Gomez-Rodriguez, Manuel, Balduzzi, David, and
Schölkopf, Bernhard. Uncovering the temporal dynamics of diffusion networks. CoRR, abs/1105.0697,
2011.

Ba, Khanh Do, Indyk, Piotr, Price, Eric, and Woodruff,
David P. Lower bounds for sparse recovery. CoRR,
abs/1106.0365, 2011.

Gupta, Ankit, Nowak, Robert, and Recht, Benjamin. Sample complexity for 1-bit compressed sensing and sparse
classification. In IEEE International Symposium on Information Theory, ISIT 2010, June 13-18, 2010, Austin,
Texas, USA, Proceedings, pp. 1553–1557, 2010.

Bickel, Peter J, Ritov, Ya’acov, and Tsybakov, Alexandre B. Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, pp. 1705–1732, 2009a.

Holme, Petter and Kim, Beom Jun. Growing scale-free
networks with tunable clustering. Physical review E, 65:
026–107, 2002.

Bickel, Peter J., Ritov, Ya’acov, and Tsybakov, Alexandre B. Simultaneous analysis of lasso and dantzig selector. Ann. Statist., 37(4):1705–1732, 08 2009b.

Javanmard, Adel and Montanari, Andrea. Confidence intervals and hypothesis testing for high-dimensional regression. The Journal of Machine Learning Research,
15(1):2869–2909, 2014.

Boufounos, Petros and Baraniuk, Richard G. 1-bit compressive sensing. In 42nd Annual Conference on Information Sciences and Systems, CISS 2008, Princeton, NJ,
USA, 19-21 March 2008, pp. 16–21, 2008.
Candes, Emmanuel J and Tao, Terence. Near-optimal signal recovery from random projections: Universal encoding strategies? Information Theory, IEEE Transactions
on, 52(12):5406–5425, 2006.
Daneshmand, Hadi, Gomez-Rodriguez, Manuel, Song, Le,
and Schölkopf, Bernhard. Estimating diffusion network
structures: Recovery conditions, sample complexity &
soft-thresholding algorithm. In Proceedings of the 31th
International Conference on Machine Learning, ICML
2014, Beijing, China, 21-26 June 2014, pp. 793–801,
2014.
Donoho, David L. Compressed sensing. Information Theory, IEEE Transactions on, 52(4):1289–1306, 2006.

Kempe, David, Kleinberg, Jon M., and Tardos, Éva. Maximizing the spread of influence through a social network.
In Proceedings of the Ninth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
Washington, DC, USA, August 24 - 27, 2003, pp. 137–
146, 2003.
Leskovec, Jure, McGlohon, Mary, Faloutsos, Christos,
Glance, Natalie S., and Hurst, Matthew. Patterns of cascading behavior in large blog graphs. In Proceedings
of the Seventh SIAM International Conference on Data
Mining, April 26-28, 2007, Minneapolis, Minnesota,
USA, pp. 551–556, 2007.
Leskovec, Jure, Chakrabarti, Deepayan, Kleinberg, Jon M.,
Faloutsos, Christos, and Ghahramani, Zoubin. Kronecker graphs: An approach to modeling networks.
Journal of Machine Learning Research, 11:985–1042,
2010.

Inferring Graphs from Cascades: A Sparse Recovery Framework

Liben-Nowell, David and Kleinberg, Jon. Tracing information flow on a global scale using Internet chain-letter
data. Proceedings of the National Academy of Sciences,
105(12):4633–4638, 2008.
Negahban, Sahand N., Ravikumar, Pradeep, Wrainwright,
Martin J., and Yu, Bin. A unified framework for highdimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4):538–557,
December 2012.
Netrapalli, Praneeth and Sanghavi, Sujay. Learning the
graph of epidemic cascades. SIGMETRICS Perform.
Eval. Rev., 40(1), June 2012. ISSN 0163-5999.
Plan, Yaniv and Vershynin, Roman. Dimension reduction
by random hyperplane tessellations. Discrete & Computational Geometry, 51(2):438–461, 2014.
Price, Eric and Woodruff, David P. (1 + eps)-approximate
sparse recovery. In Ostrovsky, Rafail (ed.), IEEE 52nd
Annual Symposium on Foundations of Computer Science, FOCS 2011, Palm Springs, CA, USA, October 2225, 2011, pp. 295–304. IEEE Computer Society, 2011.
ISBN 978-1-4577-1843-4.
Price, Eric and Woodruff, David P. Applications of the
shannon-hartley theorem to data streams and sparse recovery. In Proceedings of the 2012 IEEE International
Symposium on Information Theory, ISIT 2012, Cambridge, MA, USA, July 1-6, 2012, pp. 2446–2450. IEEE,
2012. ISBN 978-1-4673-2580-6.
Raskutti, Garvesh, Wainwright, Martin J., and Yu, Bin. Restricted eigenvalue properties for correlated gaussian designs. Journal of Machine Learning Research, 11:2241–
2259, 2010.
Rudelson, Mark and Zhou, Shuheng. Reconstruction from
anisotropic random measurements. IEEE Transactions
on Information Theory, 59(6):3434–3447, 2013.
van de Geer, Sara, Bühlmann, Peter, and Zhou, Shuheng.
The adaptive and the thresholded lasso for potentially
misspecified models (and a lower bound for the lasso).
Electron. J. Statist., 5:688–749, 2011.
van de Geer, Sara A. and Bühlmann, Peter. On the conditions used to prove oracle results for the lasso. Electron.
J. Statist., 3:1360–1392, 2009.
Watts, Duncan J. and Strogatz, Steven H. Collective dynamics of ‘small-world’ networks. Nature, 393(6684):
440–442, 1998.
Zhang, Cun-Hui and Zhang, Stephanie S. Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical

Society: Series B (Statistical Methodology), 76(1):217–
242, 2014.
Zhao, Peng and Yu, Bin. On model selection consistency
of lasso. J. Mach. Learn. Res., 7:2541–2563, December
2006. ISSN 1532-4435.
Zou, Hui. The adaptive lasso and its oracle properties.
Journal of the American Statistical Association, 101
(476):1418–1429, 2006.

