Efficient Dimensionality Reduction for High-Dimensional Network Estimation

Safiye Celik
SAFIYE @ CS . WASHINGTON . EDU
Department of Computer Science and Engineering, University of Washington, Seattle, WA 98195
Benjamin A. Logsdon
Department of Genome Sciences, University of Washington, Seattle, WA 98195

BLOGSDON @ CS . WASHINGTON . EDU

Su-In Lee
SUINLEE @ CS . WASHINGTON . EDU
Departments of Computer Science and Engineering, Genome Sciences, University of Washington, Seattle, WA 98195

Abstract
We propose module graphical lasso (MGL),
an aggressive dimensionality reduction and
network estimation technique for a highdimensional Gaussian graphical model (GGM).
MGL achieves scalability, interpretability and robustness by exploiting the modularity property of
many real-world networks. Variables are organized into tightly coupled modules and a graph
structure is estimated to determine the conditional independencies among modules. MGL iteratively learns the module assignment of variables, the latent variables, each corresponding
to a module, and the parameters of the GGM
of the latent variables. In synthetic data experiments, MGL outperforms the standard graphical
lasso and three other methods that incorporate latent variables into GGMs. When applied to gene
expression data from ovarian cancer, MGL outperforms standard clustering algorithms in identifying functionally coherent gene sets and predicting survival time of patients. The learned
modules and their dependencies provide novel
insights into cancer biology as well as identifying possible novel drug targets.

1. INTRODUCTION
Probabilistic graphical models provide a powerful framework to represent rich statistical dependencies among random variables, hence their broad application to biology,
computer vision and robotics. An edge in a graphical
model represents a conditional dependence between the
Proceedings of the 31 st International Conference on Machine
Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).

(b)

(a)

1

2

8
9

3

4

7
5

6

1

2
3

8

L1

L2

4
5

9

L3

7
6

Figure 1. (a): GGM representation of X = {X1 , . . . , X9 }; (b)
MGL representation of X

two nodes the edge connects. In a Gaussian graphical
model (GGM), edges are parameterized by elements of the
inverse covariance matrix (precision matrix). Biologists are
increasingly interested in understanding how thousands of
genes interact, which has stimulated considerable research
into structure estimation of high-dimensional GGM.
A popular approach to estimating the graph structure of a
high-dimensional GGM is the graphical lasso (Friedman
et al., 2007) that independently penalizes each off-diagonal
element of the inverse covariance matrix with an L1 norm.
However, the independence assumption is unrealistic for
many real-world networks that are structured, where edges
are not mutually independent. For example, in gene regulatory networks, genes involved in similar functional modules are more likely to interact with each other. In addition, there are often high-level interactions between functional modules, which can be difficult to identify in a standard GGM representation (see Fig. 1(a)). Importantly, how
genes are organized into functional modules and how these
modules interact with each other are scientifically relevant.
In this paper, we propose a general framework to accommodate the modular nature of many real-world networks.

Efficient Dimensionality Reduction for High-Dimensional Network Estimation
(a)

L1

L2 L3 L4

L5

(b)

2. MODULE GRAPHICAL LASSO

L1

2.1. Preliminaries

L2
L3
L4
L5

(c)

(d)
0.05
0
-0.05
-0.1
-0.15

-0.2
-0.25
-0.3

Figure 2. (a): Heatmap of Î£âˆ’1
L . White elements are zero and
colored ones are nonzero; thus, colored elements correspond to
edges in the graph. (b): Heatmap of Î£âˆ’1
X . (c): MGL estimate of
âˆ’1
Î£âˆ’1
.
(d):
GGM
estimate
of
Î£
.
X
X

Our approach, called module graphical lasso (MGL), is
characterized by the incorporation of latent variables into
the GGM. Fig. 1(b) illustrates a toy example where three
latent variables L1 , L2 and L3 have mutual dependencies
in addition to connections to observed variables by directed
edges. Each of L1 , L2 and L3 represents aggregate activity
level of specific functional modules as defined by a core of
tightly coupled genes. The undirected edges among latent
variables determine the dependencies among these functional modules. As can be seen in Fig. 1, MGL provides
a more compact representation of the conditional independence relationships compared to the equivalent GGM.
By modeling the conditional independence relationships
among k latent variables instead of p (k  p), we show
that MGL scales better than standard graphical lasso when
p  n, enabling us to efficiently learn a GGM with thousands of variables. We considered a toy example with 5
latent variables L and 15 observed variables X with the
inverse covariance matrix of the latent variables (Î£âˆ’1
L ) illustrated in Fig. 2. Given the same data consisting of 100
observations on X, MGL almost perfectly estimates Î£âˆ’1
X ,
whereas graphical lasso fails to reveal the latent structure
among the 5 groups of variables (Fig. 2).
The rest of the paper is organized as follows. In Sections
2 and 3, we provide the formulation and the learning algorithm for MGL. In Sec. 4, we present the results of our
experiments on synthetic data and ovarian cancer gene expression data. We conclude with a discussion in Sec. 5.
Derivations of the learning algorithms and proofs are available at http://leelab.cs.washington.edu/projects/MGL.

Assume that we wish to learn the Gaussian graphical
model (GGM) with p variables based on n observations
i.i.d.
x[1], . . . , x[n] âˆ¼ N (0, Î£), where Î£ is a p Ã— p covariance matrix. It is well known that the sparsity pattern of
Î£âˆ’1 represents the conditional independence relationships
among the variables (Mardia et al., 1979; Lauritzen, 1996).
Specifically, (Î£âˆ’1 )jj 0 = 0 for some j 6= j 0 if and only if
Xj and Xj 0 are conditionally independent given Xk with
k = {1, . . . , p} \ {j, j 0 }. Hence, the nonzero pattern of
Î£âˆ’1 corresponds to the graph structure of a GGM. In order to obtain a sparse estimate for the GGM, a number of
authors (Yuan & Lin, 2007; Banerjee et al., 2008; Friedman
et al., 2007) have considered maximizing the penalized log
likelihood, a method called graphical lasso:
ï£±
ï£¼
ï£²
ï£½
X
max log det Î˜ âˆ’ tr(SÎ˜) âˆ’ Î»
|Î˜jj 0 | , (1)
Î˜0 ï£³
ï£¾
0
j6=j

where S is empirical covariance matrix, Î» is a positive tuning parameter, the constraint Î˜  0 restricts the solution
to the space of positive definite matrices of size p Ã— p, and
the last term is the element-wise L1 penalty. We denote
by Î˜ the estimate of inverse covariance matrix throughout
the paper. When Î» is large, the resulting estimate will be
sparse.
The probabilistic interpretation of the L1 penalty term defines the optimization parameters Î˜ as random variables
rather than fixed parameters. This interpretation requires
that we optimize the joint probability density:
log P (S, Î˜) = log P (S|Î˜) + log P (Î˜)

(2)

The use of the Laplacian prior P (Î˜j,j 0 ) = Î»/2 Â·
exp(âˆ’Î»|Î˜j,j 0 |) leads to the optimization problem described in Eq. 1. The hyperparameter Î» adjusts the sparsity
of the optimization variable Î˜.
2.2. Module Graphical Lasso Formulation
Let L = {L1 , . . . , Lk } be a set of latent variables: L âˆ¼
N (0, Î£L ), where Î£L is a k Ã— k covariance matrix. Let
X = {X1 , . . . , Xp } be a set of observed variables, each
2
2
having the distribution: Xi |LZi , ÏƒZ
âˆ¼ N (LZi , ÏƒZ
),
i
i
where Zi refers to the index of the latent variable which
Xi is associated with. Here, we refer to a set of observed
variables that correspond to the same latent variable as a
module. As an example, the jth module Mj can be defined as Mj = {Xi |Zi = j} for 1 â‰¤ j â‰¤ k. Thus,
Z = {Z1 , . . . , Zp } defines the module assignment of p
variables into k modules.

Efficient Dimensionality Reduction for High-Dimensional Network Estimation

Then, the joint probability distribution function
P (X, L, Z, Î£L ) of the MGL has the following form:
P (X, L, Z, Î£L )
p
Y
=
P (Xi |LZi )P (L|Î£L )P (Î£L âˆ’1 )P (Z)
i=1
p
Y

(3)

j6=j

MGL can be seen as a generalized k-means clustering that
takes into account the Mahalanobis distances between latent variables (2nd term in Eq. 3), in addition to the distances between each variable and the corresponding latent
variable (1st term in Eq. 3).
Given n observations x[1], . . . , x[n] âˆˆ Rp in X, MGL aims
to estimate values on the latent variables L, module assignment variables Z, and the inverse covariance matrix of the
latent variables Î£âˆ’1
L . In order to estimate the inverse covariance matrix over the observed variables, Î£âˆ’1
X , we can
âˆ’1
use the relationship between Î£âˆ’1
and
Î£
,
as
described
L
X
in Lemma 3.
2.3. Properties of Module Graphical Lasso
Lemma 1. The joint distribution of X = {X1 , . . . , Xp }
and L = {L1 , . . . , Lk } is Gaussian: (X, L) âˆ¼
N (0, Î£XL ), where Î£XL is a (p + k) Ã— (p + k) covariance matrix.
Lemma 2. The marginal probability distribution of the observed variables X = {X1 , . . . , Xp } is Gaussian: X âˆ¼
N (0, Î£X ), where Î£X is a p Ã— p covariance matrix.
Lemma 3. Let Î£L be a k Ã— k covariance matrix of L. The
relationship between Î£X and Î£L is as

	âˆ’1
Î£X = A âˆ’ C| Bâˆ’1 C
,

(4)

where A is a p Ã— p diagonal matrix whose element Aij =
2
1/ÏƒZ
if i = j and 0 otherwise; C is a k Ã— p matrix whose
i
element Cij = âˆ’(1/Ïƒi2 ) if Xj âˆˆ Mi and 0 otherwise; and
|M1 |/Ïƒ12
ï£¯
..
Î£L âˆ’1 + ï£°
.
0
ï£®

B =

...
..
.

0
..
.

...

|Mk |/Ïƒk2

MGL jointly clusters variables into modules and learns a
network among the modules through an iterative procedure. This key aspect differentiates MGL from previous
approaches that can be organized into four categories:
The first category includes latent factor models, such as
latent factor analysis or probabilistic PCA (Tipping &
Bishop, 1999), which do not learn the network among latent factors.


(Xi âˆ’ LZi )2
q
exp âˆ’
=
2
2ÏƒZ
2
2Ï€ÏƒZ
i
i=1
i


1
1 | âˆ’1
Â·p
exp âˆ’ L Î£L L
2
(2Ï€)k |Î£L |
Y Î»

	
Â·
exp âˆ’Î»|(Î£L âˆ’1 )jj 0 | P (Z).
2
0


1

2.4. Related Work

ï£¹
ï£º
ï£»

|Mk | meaning the number of X variables in the module k.

Second, Toh & Horimoto (2002) clusters variables first and
then learns the dependency structure among the cluster centroids, instead of jointly clustering and learning the network. This method can achieve improved scalability and
interpretability; however, we showed through our extensive
experiments that MGL outperforms this approach based on
all of the evaluation criteria we incorporated.
Third, He et al. (2012) models each latent variable as a
linear combination of variables and estimates the network
among k latent variables. Although this approach also
learns a network of k latent variables instead of p variables,
it does not explicitly cluster variables, which results in a
vastly different learning algorithm from MGL. Clustering
of variables is a key feature of MGL reducing the number
of parameters and increasing the modelâ€™s interpretability,
which enables interesting analyses shown in Sec. 4.2.2.
Finally, many authors attempted to incorporate latent variables into GGMs. However, they do not explicitly cluster
variables into modules, and require the learning of Î£âˆ’1 of p
variables instead of k latent variables (k  p), which drastically increase the number of parameters. Chandrasekaran
et al. (2012) assume that Î£âˆ’1 of observed variables decomposes into a sparse matrix and a low-rank matrix, and the
low-rank matrix represents the effect of unobserved latent
variables. They proposed a convex optimization algorithm
that utilizes both L1 and nuclear norm as penalty terms.
The SIMoNe (Ambroise et al., 2009) uses an ExpectationMaximization approach (Dempster et al., 1977) for variational estimation of the latent structure while inferring the
network among the entire variables. In contrast, MGL performs a more aggressive dimensionality reduction by learning a network of k latent variables instead of p observed
variables (k << p). Guo & Wang (2010) proposed an algorithm consisting of three steps: 1) apply the graphical
lasso to compute an adjacency matrix of the variables; 2)
partition variables into disjoint clusters; and 3) estimate a
sparse Î£âˆ’1 with a modified penalty term such that withincluster edges are less strongly penalized. Given the module assignment of variables, Duchi et al. (2008) proposed
to penalize the infinity-norm and Schmidt et al. (2009) proposed to penalize the two-norm of the inverse covariance
matrix block corresponding to each module in the network

Efficient Dimensionality Reduction for High-Dimensional Network Estimation

of the variables. Marlin et al. (2009) and Marlin & Murphy (2009) make use of these methods (Duchi et al., 2008;
Schmidt et al., 2009), after first identifying the groups of
the variables when the modular structure is unknown.

from MGL is defined and bounded; every coordinate group
reached by the iterates is a stationary point of the MGL
objective function. And we observed that the value of the
objective likelihood function monotonically increases.

3. LEARNING ALGORITHM

3.2. Iterative estimation of L, Z and Î˜L

3.1. Overview

3.2.1. E STIMATION OF L

Here, we present our learning algorithm that optimizes the
likelihood function based on the joint distribution described
in Eq. 3. Given X (âˆˆ RpÃ—n ) that contains n observations
x[1], . . . , x[n] âˆˆ Rp on X, MGL aims to learn the following:

To estimate L given Z and Î˜L , from Eq. 5, we solve the
following problem:
)
(
p
2
X
k Xi âˆ’ LZi k2
|
. (6)
max
âˆ’tr (LL Î˜L ) âˆ’
L1 ,...,Lk
ÏƒZ2 i
i=1

- L (âˆˆ RkÃ—n ) containing the values on L in the n observations l[1], . . . , l[n] âˆˆ Rk ;

Setting the derivative of the objective function in Eq. 6 to
zero with respect to Lm leads to:
X
P
2
(Î˜L )im Li
Xi âˆˆMm Xi âˆ’ Ïƒm

p

- Z (âˆˆ {1, . . . , k} ) specifying the module assignment of
X1 , . . . , Xp into k modules; and
- Î˜L (âˆˆ RkÃ—k ) denoting the estimate of the inverse covariance matrix Î£âˆ’1
L . Using the Lemma 3, we can obtain
Î˜X (âˆˆ RpÃ—p ), the precision matrix estimate of X.
We choose to address our learning problem by finding the
joint maximum a posteriori (MAP) assignment to all of the
optimization variables â€“ L, Z , and Î˜L . This means that we
optimize the following objective function with respect to L,
Z , and Î˜L ( 0):
log P (X , L, Z , Î˜L ; Î», Ïƒ)

(5)

= log P (Î˜L ) + log P (L|Î˜L )
+ log P (X |L, Z ) + log P (Z )
n
(log det Î˜L âˆ’ tr(SL Î˜L ))
=
2
p
2
X
X
k Xi âˆ’ LZi k2
,
âˆ’Î»
|(Î˜L )jj 0 | âˆ’
2ÏƒZ2 i
0
i=1
j6=j

1
|
n LL

where SL =
is the empirical estimate of the covariance matrix of L, Xi denotes the ith row of the matrix X ,
Li denotes the ith row of the matrix L, and Î» is a positive
tuning parameter that adjusts the sparsity of Î˜L .
Throughout this paper, we choose hard assignment of variables to modules to reduce the number of parameters and
to increase each moduleâ€™s biological interpretability, where
interpretability is a key MGL design feature. Soft assignment is a straightforward extension. We also assume a uniform prior distribution over Z.
We use a coordinate ascent procedure over the three sets of
optimization variables â€“ L, Z , and Î˜L . We iteratively estimate each of the optimization variables until convergence.
Since our objective is continuous on a compact level set,
based on Thm. 4.1 in Tseng (2001), the solution sequence

Lm =

i6=m
2
|Mm | + Ïƒm
(Î˜L )mm

,

(7)

where Mm means a set of Xi that belongs to the mth module: Mm = {Xi |Zi = m}, and |Mm | means the number
of variables that belong to Mm . We update Lm for each
m (1 â‰¤ m â‰¤ k), based on the current values of the other
latent variables.
If all elements in Î˜L are equal to zero, Lm would be set
to the centroid of the mth module. This leads to a nice interpretation of the MGL learning algorithm with respect to
the k-means clustering. The k-means clustering algorithm
is the special case of the MGL when no network structure
is assumed to exist among the latent variables (cluster centroids). More specifically, the MGL is a generalization of
the k-means with the distance metric determined by the
sparse estimate of the latent structure (Î˜L ).
3.2.2. E STIMATION OF Z
In order to estimate Z given L and Î˜L , we solve the following:
( p
)
X k Xi âˆ’ LZ k2
i
2
max
âˆ’
,
(8)
Z1 ,...,Zp
ÏƒZ2 i
i=1
which, when Ïƒ1 , . . . , Ïƒk = 1, finds the module for Xi that
minimizes the Euclidean distance between Xi and the latent variable.
3.2.3. E STIMATION OF Î˜L
To estimate Î˜L given L and Z , we solve the following optimization problem:
ï£¼
ï£±
ï£²
ï£½
X
max log det Î˜L âˆ’ tr (SL Î˜L ) âˆ’ Î»
|(Î˜L )jj 0 | ,
Î˜L 0 ï£³
ï£¾
0
j6=j

(9)

Efficient Dimensionality Reduction for High-Dimensional Network Estimation

where SL = n1 LL| is the empirical estimate of the covariance matrix of L. Since L is given, the optimization problem in Eq. 9 can be solved by the standard graphical lasso
algorithm applied to L.

4. EXPERIMENTAL RESULTS
We present our results on synthetic data (Sec. 4.1) and ovarian cancer gene expression data (Sec. 4.2).
4.1. Synthetic Data
We compared MGL algorithm with four other methods in
terms of the performance of learning networks with latent
variables: 1) the standard graphical lasso (Glasso) (Friedman et al., 2007), 2) the method proposed by Toh & Horimoto (2002) that clusters the variables and learns the network of cluster centroids (Toh), 3) the SIMoNe method
proposed by Ambroise et al. (2009), and 4) the regularized
maximum likelihood decomposition (RMLD) method proposed by Chandrasekaran et al. (2012).
For Glasso, we used CRAN R package QUIC (Hsieh et al.,
2011); for SIMoNe, we used CRAN R package simone;
and for RMLD, we used LogdetPPA (Wang et al., 2010),
a MATLAB software for log-determinant SDP. We implemented MGL in C, and we used the C source code
of CRAN R package huge (Zhao et al., 2012) to estimate the inverse covariance matrix of the latent variables
(Sec. 3.2.3).
Toh & Horimoto (2002) originally uses hierarchical clustering for grouping the variables. In our interpretation of
Toh, we used k-means algorithm for clustering the variables due to k-meansâ€™ better cluster quality and scalability
that we observed for high-dimensional data. Also, we used
Glasso to learn the network of cluster centroids for Toh.
So, throughout this paper, the method we refer by Toh is
k-means followed by Glasso. In terms of module assignments and latent variables, k-means and Toh are identical.
We used Ïƒ1 , . . . , Ïƒk = 1 for MGL throughout Sec. 4, such
that we evaluate MGL in the simplest and efficient setting.
When Ïƒ1 , . . . , Ïƒk = 1, Eq. 8 is equal to the Euclidean distance objective of the k-means clustering algorithm, which
we use as the first step for Toh.
4.1.1. DATA G ENERATION
We synthetically generated data based on the joint distribution described in Eq. 3. We first generate the inverse
covariance matrix Î£L âˆ’1 by creating A as Aii = 0.5 and
(
i.i.d.

Aij (i 6= j) âˆ¼

0
w. prb. 1 âˆ’ (b âˆ’ a)
,
Unif([a, b]) w. prb. b âˆ’ a
(10)

and setting Î£L âˆ’1 = A + A| . We arranged the parameters a and b such that the resulting matrix Î£L âˆ’1 is positive definite. If it is still not positive definite, which happened only rarely, we regenerated the matrix A. Then,
we used Lemma 3 to generate Î£X based on Î£L and Ïƒ.
i.i.d.
We generated the data for X according to x1 , . . . , xn âˆ¼
pÃ—n
N (0, Î£X ), which results in X âˆˆ R
.
In order to evaluate these algorithms in varying degrees of
high-dimensionality, we created three settings in terms of
(P , K, N ), where P is the number of variables, K is the
number of latent variables, and N is the sample size.
Setting I - (100, 10, 10)
Setting II - (150, 10, 10): The difference from Setting I is
the number of variables P , which increases the dimensionality of the data by 1.5 times.
Setting III - (150, 15, 10): We increased the number of
latent variables K such that the sample size N is smaller
than K.
By setting a = 0.2 and b = 0.6 in Eq. 10, we created two
different data matrices (training and test datasets) in each of
Settings I, II and III. The sparsity (i.e., ratio of the number
of nonzero edges to the number of all potential edges) of the
resulting data matrices was around 35%. We used one of
the two data matrices for training MGL and its competitors,
and the other one for testing.
4.1.2. S YNTHETIC TEST LOG - LIKELIHOODS
We measure the performance of MGL and four competing methods in terms of test log-likelihood using the training/test datasets described above. We chose cross-data test
log-likelihood as an evaluation metric because it allows
direct comparisons between methods that incorporate latent variables and methods that do not (given that each
method estimates a p-dimensional precision matrix). Test
log-likelihood allows us to evaluate how well the learned
models fit unseen data.
We performed 5-fold cross validation tests within the training dataset in order to select Î» that gives the best average
test log-likelihood for each method. In this cross-validation
for choosing Î», we used a wide range of the Î» values such
that the solutions for the inverse covariance matrices range
from a full matrix to an empty matrix.
Fig. 3 shows the difference of the test log-likelihood between each method and the SIMoNe method in Settings I,
II and III. For MGL and Toh, we present the results for 3
different k values representing the number of latent variables â€“ K/2, K and 2K, where K means the true number
of modules, assuming that the true number of modules (K)
is unknown by the methods.

Efficient Dimensionality Reduction for High-Dimensional Network Estimation

It can be seen that MGL outperforms all of its competitors
in all of the three simulation settings we considered. Although SIMoNe and RMLD are specific generalizations of
Glasso, Ambroise et al. (2009) showed that Glasso outperforms SIMoNe when p = n and p = 2n, and Giraud &
Tsybakov (2012) argued that RMLD results are valid and
meaningful only when p < n, consistently with our results.
We note that in Fig. 3a and Fig. 3c, the test log-likelihood
was maximized when we used more latent variables than
in the generating model. This is a result of the highdimensionality of the data. But when we increased k further, test log-likelihood of MGL and Toh decreased, and
for k = p, they both became equal to the one of Glasso as
expected.

(a)

Test  relative to SiMoNE

200

(b)

150

100

100

50

Si
M
R oNE
G ML
la D
s
T so
M oh5
G
T L
M oh15
G 0
T L1
M oh20
G 0
L2
0

0

Test  relative to SiMoNE

500

Given the data, MGL estimates Z , L and Î˜L (see Eq. 5),
which describe a gene module network characterized by
the assignments of genes to modules and the latent structure among the modules (Fig. 1(b)). We evaluated MGL
based on: 1) how well the learned model fits unseen data
(Sec. 4.2.1); 2) how significantly the inferred modules are
coherent in terms of gene functions (Sec. 4.2.2); and 3) how
well the inferred latent variables are predictive of survival
time (Sec. 4.2.3). We also present some of the biologically
interesting findings that we obtain from the MGL results
(Sec. 4.2.4).
Since this application requires learning a network with
>10K variables, the methods that attempt to learn the network of all individual variables do not scale. Therefore, we
compared MGL with only Toh that first clusters the variables and then learns the network of cluster centroids.

50

4.2.1. C ROSS - VALIDATION TEST LOG - LIKELIHOODS
0

Si
M
R oN
G ML E
la D
s
T so
M oh5
G
T L
M oh 5
G 10
T L1
M oh20
G 0
L2
0

Test  relative to SiMoNE

150

noma â€“ Tothill (269 samples) (Tothill et al., 2008), TCGA
(560 samples) (TCGA, 2012), and Denkert (80 samples)
(Denkert et al., 2009). We mainly used Tothill for training,
and TCGA and Denkert for testing.

(c)

400
300
200
100

Si

M
R oN
G ML E
la D
s
T so
M oh7
G
T L
M oh 7
G 15
T L1
M oh35
G 0
L3
0

0

Figure 3. For (P , K, N ) (a) (100, 10, 10), (b) (150, 10, 10),
and (c) (150, 15, 10), we considered SIMoNe as reference and
computed the difference in cross-data test log-likelihood of each
method compared to the one of SIMoNe (y-axis). Each bar
corresponds to (1) SIMoNe, (2) RMLD (3) Glasso, (4) Toh for
k = K/2, (5) MGL for k = K/2, (6) Toh for k = K, (7) MGL
for k = K, (8) Toh for k = 2K, and (9) MGL for k = 2K.

We applied k-means clustering and used the resulting clusters as a starting point for MGL and Toh. We compared between MGL and Toh in terms of the cross-validation (CV)
test log-likelihood of the estimated p-dimensional precision
matrices. We performed model selection using Bayesian
Information Criterion (BIC) for k-means. Cluster count
(k) was determined as 150 by BIC. Since the data is highdimensional, we performed 2-fold CV. We used a wide
range of the Î» values such that the solutions for the module precision matrices range from a full matrix to an empty
matrix. The results were averaged over 10 iterations due to
non-deterministic nature of the k-means. Fig. 4 shows the
test log-likelihoods of each method. MGL clearly outperforms Toh, meaning that the learned model by MGL fits
unseen data better than the one by Toh. Moreover, the
standard deviation of the test log-likelihoods of the folds
is smaller for MGL than Toh, indicating the robustness of
MGL. In the subsequent sets of experiments (Sections 4.2.2
and 4.2.3), we use k = 150 (as determined by BIC) and
Î» = .004 (as chosen by CV).

4.2. Cancer Gene Expression Data

4.2.2. F UNCTIONAL ENRICHMENT OF MODULES

Ovarian cancer is the 5th leading cause of cancer death
among US women and has a 5-year survival rate of 30%
(Bast et al., 2009). Learning the gene regulatory network
from expression data is an effective strategy to identify
novel disease mechanisms (Akavia et al., 2010; TCGA,
2012). Thus, we experimented MGL on three gene expression datasets containing 10404 gene expression levels in a total of 909 patients with ovarian serous carci-

Genes assigned to the same module are likely to share similar functions, and those in the connected modules are likely
to be involved in similar cellular processes as well. We
define a super-module (or a super-cluster) as the set of
genes in two connected modules (or clusters). We compared super-clusters from the learned network by Toh to
super-modules from the learned network by MGL in terms
of functional coherence. For each of the 4722 Curated

Efficient Dimensionality Reduction for High-Dimensional Network Estimation

âˆ’1.194

âˆ’1.196

âˆ’1.198
0

0.005

0.01
Î»

0.015

0.02

Figure 4. Comparison between MGL and Toh in terms of crossvalidation test log-likelihood for varying Î» values and for k =
150 (which was determined by BIC). Standard deviation between
the test log-likelihoods of the folds are shown by the error bars.

GeneSets from the Molecular Signatures Database (Liberzon et al., 2011), we computed the significance of the
overlap between the GeneSet and super-modules (or superclusters). We applied Bonferroni correction to the p-values
and only considered the GeneSets with p < 0.05 in either MGL or Toh. We repeated this process 50 times with
different random initial points for k-means. As can be
seen in Fig. 5(a), for each of 50 runs, there are a larger
number of GeneSets that are more significantly overlapped
with MGL super-modules than with Toh super-clusters.
Thus, MGL improves the initial network of Toh, resulting in far more shared processes between modules that are
connected in the estimated network. Additionally, we observed that in each independent run, MGL improves the
actual p-values. In Fig. 5(b), for each of 4722 functional
categories, the smallest p-value achieved by MGL supermodules is plotted (y-axis) against that achieved by Toh
super-clusters (x-axis). The results for all 50 runs were
aggregated in Fig. 5(b). Most of the dots in Fig. 5(b) lie
above the diagonal, meaning that for most of the functional categories, MGL super-modules achieve better enrichment than Toh super-clusters. Moreover, 6 GeneSets
were observed to be enriched by MGL super-modules with
p-values not only smaller than 10âˆ’90 , but also smaller than
the best p-values for Toh super-clusters (10âˆ’20 ). These
GeneSets were related to cell differentiation and increased
cell growth, which are core processes relevant to cancer
progression. This shows MGLâ€™s power to detect core cancer modules. We also performed the experiment explained
above for learned modules without considering the latent
network among them, and observed that the functional enrichment results for modules were consistent with the ones
for super-modules. As can be seen in Fig. 6(a), for each
of 50 runs, there are a larger number of GeneSets that
are more significantly overlapped with MGL modules than
with Toh clusters. An additional interesting observation is
that MGL learns much sparser module networks than Toh

(a)

500

MGL supermodules: Best âˆ’log10p

âˆ’1.192

for any attempted Î» value in a wide range. For Î» = .004
and k = 150, the average number of the edges was 6324.7
for the Toh network, and was 4626.6 for the MGL network.
MGL removes a handful of dependencies from the initial
Toh network and adds a number of new dependencies while
improving the module assignments meanwhile. Sparsity of
MGL networks is plausible in terms of genetic robustness.
We compared the enrichment of module pairs whose dependencies are removed by MGL to that of module pairs
between which new dependencies are added. Interestingly,
the former was smaller than the latter for 49 runs out of 50
as displayed in Fig. 6(b).

400
300
200
100
0
0

100
200
300
400
500
# GeneSets better detected by Toh

140

(b)

120
100
80
60
40
20
0
0

50
100
Toh superclusters: Best âˆ’log10p

Figure 5. (a) Each dot represents a run with a random k-means
starting point. For each of 50 runs, the number of GeneSets
more significantly overlapped with MGL super-modules (y-axis)
is compared to that with Toh super-clusters (x-axis). (b) Each dot
represents a GeneSet. For each GeneSet, the smallest enrichment
p-value achieved by Toh (x-axis) vs. MGL (y-axis) is compared.
We note that the plot for each run was consistent with the aggregated plot.
700

(a)

600
500
400
300
200
100
0
0

200
400
600
# GeneSets better detected by Toh

MGLâˆ’only dep. modules: Avg âˆ’log10p

MGL
Toh

# GeneSets better detected by MGL

CV test logâˆ’likelihood

x 10

# GeneSets better detected by MGL

6

âˆ’1.19

8

(b)

7.5
7
6.5
6
5.5
5
4.5

5
6
7
8
Tohâˆ’only dep. modules: Avg. âˆ’log10p

Figure 6. Each dot represents a run with a random k-means starting point. (a) For each of 50 runs, the number of GeneSets more
significantly overlapped with MGL modules (y-axis) is compared
to that with Toh clusters (x-axis).(b) For each of 50 runs, the average enrichment p-value for Toh-only dependent modules (x-axis)
is compared to MGL-only dependent modules (y-axis).

4.2.3. S URVIVAL PREDICTION USING LATENT
VARIABLES AS FEATURES

The latent variables could represent activity levels of pathways relevant to the disease process and clinical outcomes.

Efficient Dimensionality Reduction for High-Dimensional Network Estimation

We evaluated how well the inferred latent variables learned
from Tothill are predictive of survival time of ovarian cancer patients in TCGA and Denkert datasets. After learning
Toh and MGL on Tothill dataset, we trained the Cox regression model using the inferred latent variables as features in
Tothill dataset, and then tested the model on a separate test
dataset. In the test dataset, we computed the concordance
index (c-Index) which is considered a standard evaluation
metric estimating the accuracy of survival prediction based
on the â€˜censoredâ€™ survival data. Fig. 7 shows the c-Index
achieved for varying sparsity levels for the Cox regression
model (x-axis) on the two training-test dataset pairs. It
compares latent variables (modules) from MGL to clusters
from Toh and individual genes. In both of the settings, the
c-Index values for modules are larger than those for Toh
clusters or individual genes, for a wide range of sparsity
levels. The maximum c-Index for modules is also higher
than those for clusters and individual genes. The c-Index
values were averaged over 50 runs for MGL and Toh.
(a)

(b)

0.63
0.62

0.62
0.61

0.61
MGLglatentgvariables
Tohgclustergcentroids
Individualggenes

0.59
0.58

câˆ’Index

câˆ’Index

0.6

0.56

0.58

MGLglatentgvariables
Tohgclustergcentroids
Individualggenes

0.55
0.54
0

20

40
60
3gnonzerogcoef.

80

100

Immune system

3

1

2

4

6
Drug 5
metabolism

7

8

Mod

Detected Pathway

Mod

Detected Pathway

1

Cytokine receptor
interaction/chemo
kine signaling

5

Signaling by PDGF

2

Inflammatory
response

6

Drug metabolism

3

Natural killer cell

7

Mitotic cell cycle

4

Immune system
signaling

8

Oocyte meiosis
and cell cycle

Cell cycle

Figure 8. As small portion of the pathway structure identified by
MGL on Tothill dataset.

5. DISCUSSION
We proposed the module graphical lasso, a novel highdimensional GGM representation of conditional independencies among tightly coupled sets of variables (modules).
The MGL algorithm is a novel high-dimensional clustering algorithm that is a generalization of k-means clustering, with Mahalanobis distances between variables. The
full joint probability distribution function Eq. 3 defines a
non-Euclidean distance metric between the latent variables
L based on Î˜L .

0.6

0.59

0.57

myelogenous leukemia patients (Pietras et al., 2003).

0.57
0

20

40
60
#gnonzerogcoef.

80

100

Figure 7. Comparison among MGL latent variables (modules), kmeans clusters and individual genes based on survival prediction
performance. x-axis gives the number of nonzero coefficients
selected by penalized Cox regression model and y-axis gives cIndex values. Two pairs of training-test data are considered: (a)
Tothill-Denkert, (b) Tothill-TCGA.

4.2.4. I NTERESTING F INDINGS
A handful of modules identified by MGL are enriched
for processes relevant to tumor biology, drug metabolism,
and response to drug therapy. Fig. 8 shows a small portion of the module network learned on Tothill dataset. It
is a network among immune system, cell cycle and drug
metabolism processes. Edges between modules 1 through
4 indicate conditional dependencies among cytokines, inflammation, and immune signaling, which play important
roles in tumor biology (Coussens & Werb, 2002). There
are suggestive edges between module 4 and modules 7 and
8 (cell cycle modules), since innate immune response can
stimulate cell division in neoplastic cells. (Coussens &
Werb, 2002). Finally, module 5 is significantly enriched for
PDGF for signaling. PDGF receptor agonists, such as the
popular drug Gleevec, have succeeded in treating chronic

There are several possible extensions. First, MGL could
be extended to other graphical models, such as Markov
random fields, with novel distance metrics and clustering
properties. Second, the assumptions about relationships between latent and observed variables could be relaxed. For
instance, we could apply soft assignments of variables to
modules, and learn sub-networks within modules. Third,
we could add learning of module variances (Ïƒ) as an inference step to the MGL algorithm. And finally, we plan to
apply MGL to gene expression data across multiple healthy
and cancerous tissues to identify conserved and differential
latent molecular networks driving tumor biology.

Acknowledgments
The authors acknowledge funding from the following
sources: American Association of Univ. Women International Doctoral Fellowship to SC, NIH T32 HL 007312 to
BAL, and Univ. Washington Royalty Research Fund to SL.

References
Akavia, U.D., Litvin, O., Kim, J., Sanchez-Garcia, F.,
Kotliar, D., Causton, H.C., Pochanard, P., Mozes, E,
Garraway, L.A., and Peâ€™er, D. An integrated approach to
uncover drivers of cancer. Cell, 143(6):1005â€“17, 2010.
Ambroise, C., Chiquet, J., and Matias, C. Inferring sparse
gaussian graphical models with latent structure. Electron. J. Statist., 3:205â€“238, 2009.

Efficient Dimensionality Reduction for High-Dimensional Network Estimation

Banerjee, O., El Ghaoui, L., and dâ€™Aspremont, A. Model
selection through sparse maximum likelihood estimation
for multivariate Gaussian or binary data. JMLR, 9:485â€“
516, 2008.
Bast, R.C., Hennessy, B., and Mills, G.B. The biology of
ovarian cancer: new opportunities for translation. Nature
Reviews Cancer, 9(6):415â€“428, 2009.
Chandrasekaran, V., Parrilo, P.A., and Willsky, A.S. Latent
variable graphical model selection via convex optimization. The Annals of Statistics, 40:1935â€“1967, 2012.
Coussens, L.M. and Werb, Z. Inflammation and cancer.
Nature, 420(6917):860â€“867, 2002.
Dempster, A.P., Laird, N.M., and Rubin, D.B. Maximum
likelihood from incomplete data via the em algorithm.
Journal of the Royal Statistical Society, Series B, 39(1):
1â€“38, 1977.
Denkert, C., J., Budczies, S., Darb-Esfahani, Gyrffy, B.,
Sehouli, J., Knsgen, D., Zeillinger, R., Weichert, W.,
Noske, A., Buckendahl, A.C., Mller, B.M., Dietel, M.,
and Lage, H. A prognostic gene expression index in
ovarian cancer - validation across different independent
data sets. J. Pathol., 218(2):273â€“80, 2009.
Duchi, J., Gould, S., and Koller, D. Projected subgradient
methods for learning sparse gaussians. UAI, 2008.
Friedman, J., Hastie, T., and Tibshirani, R. Sparse inverse
covariance estimation with the graphical lasso. Biostatistics, 9:432â€“441, 2007.
Giraud, C. and Tsybakov, A.S. Discussion: Latent variable
graphical model selection via convex optimization. The
Annals of Statistics, 40(4):1984â€“1988, 2012.
Guo, J. and Wang, S. Modularized gaussian graphical
model. submitted to Computational Statistics and Data
Analysis, 2010.
He, Y., Kavukcuoglu, K., Qi, Y., and Park, H. Structured
latent factor analysis. NIPS, 2012.
Hsieh, Cho-Jui, Sustik, MaÌtyaÌs A., Dhillon, Inderjit S., and
Ravikumar, Pradeep K. Sparse inverse covariance matrix
estimation using quadratic approximation. NIPS, 2011.

Mardia, K.V., Kent, J., and Bibby, J.M. Multivariate Analysis. Academic Press, 1979.
Marlin, B.M. and Murphy, K. Sparse gaussian graphical
models with unknown block structure. ICML, 2009.
Marlin, B.M., Schmidt, M., and Murphy, K. Group sparse
priors for covariance estimation. UAI, 2009.
Pietras, K., Sjoblom, T., Rubin, K., Heldin, C.H., and Ostman, A. Pdgf receptors as cancer drug targets. Cancer
cell, 3:439â€“444, 2003.
Schmidt, M., van den Berg, E., Friedlander, M.P., and Murphy, K. Optimizing costly functions with simple constraints: A limited-memory projected quasi-newton algorithm. AISTATS, 2009.
TCGA, Cancer Genome Atlas Research Network. Integrated genomic analyses of ovarian carcinoma. Nature,
474(7353):609â€“15, 2012.
Tipping, M.E. and Bishop, C.M. Probabilistic principal
component analysis. Journal of the Royal Statistical Society, Series B, 61(3):611â€“622, 1999.
Toh, H. and Horimoto, K. Inference of a genetic network
by a combined approach of cluster analysis and graphical gaussian modeling. Bioinformatics, 18(2):287â€“297,
2002.
Tothill, R.W., Tinker, A.V., George, J., Brown, R., Fox,
S.B., Lade, S., Johnson, D.S., Trivett, M.K., Etemadmoghadam, D., Locandro, B., Traficante, N., Fereday,
S., Hung, J.A., Chiew, Y.E., Haviv, I., Group, Australian Ovarian Cancer Study, Gertig, D., DeFazio, A.,
and Bowtell, D.D. Novel molecular subtypes of serous
and endometrioid ovarian cancer linked to clinical outcome. Clin. Cancer Res., 14(16):5198â€“208, 2008.
Tseng, P. Convergence of a block coordinate descent
method for nondifferentiable minimization. Journal of
Optimization Theory and Applications, 109(3):475494,
2001.
Wang, Chengjing, Sun, Defeng, and Toh, Kim-Chuan.
Solving log-determinant optimization problems by a
newton-cg primal proximal point algorithm. SIAM J.
Optimization, (20):2994â€“3013, 2010.

Lauritzen, S.L. Graphical Models. Oxford Science Publications, 1996.

Yuan, M. and Lin, Y. Model selection and estimation in the
Gaussian graphical model. Biometrika, 94(10):19â€“35,
2007.

Liberzon, A., Subramanian, A., Pinchback, R., Thorvaldsdottir, H., Tamayo, P., and Mesirov, J.P. Molecular signatures database (msigdb) 3.0. Bioinformatics, 27(12):
1739â€“1740, 2011.

Zhao, Tuo, Liu, Han, Roeder, Kathryn, Lafferty, John,
and Wasserman, Larry. The huge package for highdimensional undirected graph estimation in r. JMLR,
(13):1059â€“1062, 2012.

